[
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\inference.py",
        "line_number": 11,
        "API": ".topk(",
        "context": [
            "from utils import AverageMeter\n",
            "\n",
            "\n",
            "def get_video_results(outputs, class_names, output_topk):\n",
            "    sorted_scores, locs = torch.topk(outputs,\n",
            "                                     k=min(output_topk, len(class_names)))\n",
            "\n",
            "    video_results = []\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\inference.py",
        "line_number": 17,
        "API": ".item(",
        "context": [
            "\n",
            "    video_results = []\n",
            "    for i in range(sorted_scores.size(0)):\n",
            "        video_results.append({\n",
            "            'label': class_names[locs[i].item()],\n",
            "            'score': sorted_scores[i].item()\n",
            "        })\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\inference.py",
        "line_number": 42,
        "API": ".softmax(",
        "context": [
            "            data_time.update(time.time() - end_time)\n",
            "\n",
            "            video_ids, segments = zip(*targets)\n",
            "            outputs = model(inputs)\n",
            "            outputs = F.softmax(outputs, dim=1).cpu()\n",
            "\n",
            "            for j in range(outputs.size(0)):\n",
            "                results['results'][video_ids[j]].append({\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\inference.py",
        "line_number": 55,
        "API": ".format(",
        "context": [
            "            end_time = time.time()\n",
            "\n",
            "            print('[{}/{}]\\t'\n",
            "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
            "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'.format(\n",
            "                      i + 1,\n",
            "                      len(data_loader),\n",
            "                      batch_time=batch_time,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\inference.py",
        "line_number": 67,
        "API": ".stack(",
        "context": [
            "        for video_id, video_results in results['results'].items():\n",
            "            video_outputs = [\n",
            "                segment_result['output'] for segment_result in video_results\n",
            "            ]\n",
            "            video_outputs = torch.stack(video_outputs)\n",
            "            average_scores = torch.mean(video_outputs, dim=0)\n",
            "            inference_results['results'][video_id] = get_video_results(\n",
            "                average_scores, class_names, output_topk)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\main.py",
        "line_number": 61,
        "API": ".format(",
        "context": [
            "\n",
            "    if opt.inference_batch_size == 0:\n",
            "        opt.inference_batch_size = opt.batch_size\n",
            "\n",
            "    opt.arch = '{}-{}'.format(opt.model, opt.model_depth)\n",
            "    opt.begin_epoch = 1\n",
            "    opt.mean, opt.std = get_mean_std(opt.value_scale, dataset=opt.mean_dataset)\n",
            "    opt.n_input_channels = 3\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\main.py",
        "line_number": 86,
        "API": ".format(",
        "context": [
            "    return opt\n",
            "\n",
            "\n",
            "def resume_model(resume_path, arch, model):\n",
            "    print('loading checkpoint {} model'.format(resume_path))\n",
            "    checkpoint = torch.load(resume_path, map_location='cpu')\n",
            "    assert arch == checkpoint['arch']\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\main.py",
        "line_number": 99,
        "API": ".format(",
        "context": [
            "    return model\n",
            "\n",
            "\n",
            "def resume_train_utils(resume_path, begin_epoch, optimizer, scheduler):\n",
            "    print('loading checkpoint {} train utils'.format(resume_path))\n",
            "    checkpoint = torch.load(resume_path, map_location='cpu')\n",
            "\n",
            "    begin_epoch = checkpoint['epoch'] + 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\main.py",
        "line_number": 314,
        "API": ".save(",
        "context": [
            "        'state_dict': model_state_dict,\n",
            "        'optimizer': optimizer.state_dict(),\n",
            "        'scheduler': scheduler.state_dict()\n",
            "    }\n",
            "    torch.save(save_states, save_file_path)\n",
            "\n",
            "\n",
            "def main_worker(index, opt):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\main.py",
        "line_number": 319,
        "API": ".seed(",
        "context": [
            "\n",
            "\n",
            "def main_worker(index, opt):\n",
            "    random.seed(opt.manual_seed)\n",
            "    np.random.seed(opt.manual_seed)\n",
            "    torch.manual_seed(opt.manual_seed)\n",
            "\n",
            "    if index >= 0 and opt.device.type == 'cuda':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\main.py",
        "line_number": 327,
        "API": ".init_process_group(",
        "context": [
            "        opt.device = torch.device(f'cuda:{index}')\n",
            "\n",
            "    if opt.distributed:\n",
            "        opt.dist_rank = opt.dist_rank * opt.ngpus_per_node + index\n",
            "        dist.init_process_group(backend='nccl',\n",
            "                                init_method=opt.dist_url,\n",
            "                                world_size=opt.world_size,\n",
            "                                rank=opt.dist_rank)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\main.py",
        "line_number": 355,
        "API": ".to(",
        "context": [
            "\n",
            "    if opt.is_master_node:\n",
            "        print(model)\n",
            "\n",
            "    criterion = CrossEntropyLoss().to(opt.device)\n",
            "\n",
            "    if not opt.no_train:\n",
            "        (train_loader, train_sampler, train_logger, train_batch_logger,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\main.py",
        "line_number": 389,
        "API": ".format(",
        "context": [
            "                        opt.device, current_lr, train_logger,\n",
            "                        train_batch_logger, tb_writer, opt.distributed)\n",
            "\n",
            "            if i % opt.checkpoint == 0 and opt.is_master_node:\n",
            "                save_file_path = opt.result_path / 'save_{}.pth'.format(i)\n",
            "                save_checkpoint(save_file_path, i, opt.arch, model, optimizer,\n",
            "                                scheduler)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\main.py",
        "line_number": 405,
        "API": ".format(",
        "context": [
            "            scheduler.step(prev_val_loss)\n",
            "\n",
            "    if opt.inference:\n",
            "        inference_loader, inference_class_names = get_inference_utils(opt)\n",
            "        inference_result_path = opt.result_path / '{}.json'.format(\n",
            "            opt.inference_subset)\n",
            "\n",
            "        inference.inference(inference_loader, model, inference_result_path,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\main.py",
        "line_number": 416,
        "API": ".device(",
        "context": [
            "\n",
            "if __name__ == '__main__':\n",
            "    opt = get_opt()\n",
            "\n",
            "    opt.device = torch.device('cpu' if opt.no_cuda else 'cuda')\n",
            "    if not opt.no_cuda:\n",
            "        cudnn.benchmark = True\n",
            "    if opt.accimage:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\model.py",
        "line_number": 7,
        "API": ".split(",
        "context": [
            "from models import resnet, resnet2p1d, pre_act_resnet, wide_resnet, resnext, densenet\n",
            "\n",
            "\n",
            "def get_module_name(name):\n",
            "    name = name.split('.')\n",
            "    if name[0] == 'module':\n",
            "        i = 1\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\model.py",
        "line_number": 98,
        "API": ".format(",
        "context": [
            "\n",
            "\n",
            "def load_pretrained_model(model, pretrain_path, model_name, n_finetune_classes):\n",
            "    if pretrain_path:\n",
            "        print('loading pretrained model {}'.format(pretrain_path))\n",
            "        pretrain = torch.load(pretrain_path, map_location='cpu')\n",
            "\n",
            "        model.load_state_dict(pretrain['state_dict'])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\model.py",
        "line_number": 116,
        "API": ".set_device(",
        "context": [
            "\n",
            "def make_data_parallel(model, is_distributed, device):\n",
            "    if is_distributed:\n",
            "        if device.type == 'cuda' and device.index is not None:\n",
            "            torch.cuda.set_device(device)\n",
            "            model.to(device)\n",
            "\n",
            "            model = nn.parallel.DistributedDataParallel(model,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\model.py",
        "line_number": 122,
        "API": ".to(",
        "context": [
            "\n",
            "            model = nn.parallel.DistributedDataParallel(model,\n",
            "                                                        device_ids=[device])\n",
            "        else:\n",
            "            model.to(device)\n",
            "            model = nn.parallel.DistributedDataParallel(model)\n",
            "    elif device.type == 'cuda':\n",
            "        model = nn.DataParallel(model, device_ids=None).cuda()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\spatial_transforms.py",
        "line_number": 105,
        "API": ".format(",
        "context": [
            "                0,\n",
            "                len(self.crop_positions) - 1)]\n",
            "\n",
            "    def __repr__(self):\n",
            "        return self.__class__.__name__ + '(size={0}, crop_position={1}, randomize={2})'.format(\n",
            "            self.size, self.crop_position, self.randomize)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\spatial_transforms.py",
        "line_number": 161,
        "API": ".format(",
        "context": [
            "\n",
            "        self.corner_crop = CornerCrop(None, crop_position)\n",
            "\n",
            "    def __repr__(self):\n",
            "        return self.__class__.__name__ + '(size={0}, scales={1}, interpolation={2})'.format(\n",
            "            self.size, self.scales, self.interpolation)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\temporal_transforms.py",
        "line_number": 107,
        "API": ".ceil(",
        "context": [
            "\n",
            "    def __call__(self, frame_indices):\n",
            "        n_frames = len(frame_indices)\n",
            "        stride = max(\n",
            "            1, math.ceil((n_frames - 1 - self.size) / (self.n_samples - 1)))\n",
            "\n",
            "        out = []\n",
            "        for begin_index in frame_indices[::stride]:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\training.py",
        "line_number": 22,
        "API": ".format(",
        "context": [
            "                epoch_logger,\n",
            "                batch_logger,\n",
            "                tb_writer=None,\n",
            "                distributed=False):\n",
            "    print('train at epoch {}'.format(epoch))\n",
            "\n",
            "    model.train()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\training.py",
        "line_number": 35,
        "API": ".to(",
        "context": [
            "    end_time = time.time()\n",
            "    for i, (inputs, targets) in enumerate(data_loader):\n",
            "        data_time.update(time.time() - end_time)\n",
            "\n",
            "        targets = targets.to(device, non_blocking=True)\n",
            "        outputs = model(inputs)\n",
            "        loss = criterion(outputs, targets)\n",
            "        acc = calculate_accuracy(outputs, targets)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\training.py",
        "line_number": 40,
        "API": ".size(",
        "context": [
            "        outputs = model(inputs)\n",
            "        loss = criterion(outputs, targets)\n",
            "        acc = calculate_accuracy(outputs, targets)\n",
            "\n",
            "        losses.update(loss.item(), inputs.size(0))\n",
            "        accuracies.update(acc, inputs.size(0))\n",
            "\n",
            "        optimizer.zero_grad()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\training.py",
        "line_number": 51,
        "API": ".log(",
        "context": [
            "        batch_time.update(time.time() - end_time)\n",
            "        end_time = time.time()\n",
            "\n",
            "        if batch_logger is not None:\n",
            "            batch_logger.log({\n",
            "                'epoch': epoch,\n",
            "                'batch': i + 1,\n",
            "                'iter': (epoch - 1) * len(data_loader) + (i + 1),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\training.py",
        "line_number": 64,
        "API": ".format(",
        "context": [
            "        print('Epoch: [{0}][{1}/{2}]\\t'\n",
            "              'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
            "              'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
            "              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
            "              'Acc {acc.val:.3f} ({acc.avg:.3f})'.format(epoch,\n",
            "                                                         i + 1,\n",
            "                                                         len(data_loader),\n",
            "                                                         batch_time=batch_time,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\training.py",
        "line_number": 86,
        "API": ".all_reduce(",
        "context": [
            "        acc_count = torch.tensor([accuracies.count],\n",
            "                                 dtype=torch.float32,\n",
            "                                 device=device)\n",
            "\n",
            "        dist.all_reduce(loss_sum, op=dist.ReduceOp.SUM)\n",
            "        dist.all_reduce(loss_count, op=dist.ReduceOp.SUM)\n",
            "        dist.all_reduce(acc_sum, op=dist.ReduceOp.SUM)\n",
            "        dist.all_reduce(acc_count, op=dist.ReduceOp.SUM)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\training.py",
        "line_number": 91,
        "API": ".item(",
        "context": [
            "        dist.all_reduce(loss_count, op=dist.ReduceOp.SUM)\n",
            "        dist.all_reduce(acc_sum, op=dist.ReduceOp.SUM)\n",
            "        dist.all_reduce(acc_count, op=dist.ReduceOp.SUM)\n",
            "\n",
            "        losses.avg = loss_sum.item() / loss_count.item()\n",
            "        accuracies.avg = acc_sum.item() / acc_count.item()\n",
            "\n",
            "    if epoch_logger is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\utils.py",
        "line_number": 47,
        "API": ".flush(",
        "context": [
            "            assert col in values\n",
            "            write_values.append(values[col])\n",
            "\n",
            "        self.logger.writerow(write_values)\n",
            "        self.log_file.flush()\n",
            "\n",
            "\n",
            "def calculate_accuracy(outputs, targets):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\utils.py",
        "line_number": 52,
        "API": ".size(",
        "context": [
            "\n",
            "\n",
            "def calculate_accuracy(outputs, targets):\n",
            "    with torch.no_grad():\n",
            "        batch_size = targets.size(0)\n",
            "\n",
            "        _, pred = outputs.topk(1, 1, largest=True, sorted=True)\n",
            "        pred = pred.t()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\utils.py",
        "line_number": 57,
        "API": ".sum(",
        "context": [
            "\n",
            "        _, pred = outputs.topk(1, 1, largest=True, sorted=True)\n",
            "        pred = pred.t()\n",
            "        correct = pred.eq(targets.view(1, -1))\n",
            "        n_correct_elems = correct.float().sum().item()\n",
            "\n",
            "        return n_correct_elems / batch_size\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\utils.py",
        "line_number": 64,
        "API": ".topk(",
        "context": [
            "\n",
            "\n",
            "def calculate_precision_and_recall(outputs, targets, pos_label=1):\n",
            "    with torch.no_grad():\n",
            "        _, pred = outputs.topk(1, 1, largest=True, sorted=True)\n",
            "        precision, recall, _, _ = precision_recall_fscore_support(\n",
            "            targets.view(-1, 1).cpu().numpy(),\n",
            "            pred.cpu().numpy())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\utils.py",
        "line_number": 73,
        "API": ".initial_seed(",
        "context": [
            "        return precision[pos_label], recall[pos_label]\n",
            "\n",
            "\n",
            "def worker_init_fn(worker_id):\n",
            "    torch_seed = torch.initial_seed()\n",
            "\n",
            "    random.seed(torch_seed + worker_id)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\utils.py",
        "line_number": 79,
        "API": ".seed(",
        "context": [
            "    random.seed(torch_seed + worker_id)\n",
            "\n",
            "    if torch_seed >= 2**32:\n",
            "        torch_seed = torch_seed % 2**32\n",
            "    np.random.seed(torch_seed + worker_id)\n",
            "\n",
            "\n",
            "def get_lr(optimizer):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\validation.py",
        "line_number": 18,
        "API": ".format(",
        "context": [
            "              device,\n",
            "              logger,\n",
            "              tb_writer=None,\n",
            "              distributed=False):\n",
            "    print('validation at epoch {}'.format(epoch))\n",
            "\n",
            "    model.eval()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\validation.py",
        "line_number": 33,
        "API": ".to(",
        "context": [
            "    with torch.no_grad():\n",
            "        for i, (inputs, targets) in enumerate(data_loader):\n",
            "            data_time.update(time.time() - end_time)\n",
            "\n",
            "            targets = targets.to(device, non_blocking=True)\n",
            "            outputs = model(inputs)\n",
            "            loss = criterion(outputs, targets)\n",
            "            acc = calculate_accuracy(outputs, targets)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\validation.py",
        "line_number": 38,
        "API": ".size(",
        "context": [
            "            outputs = model(inputs)\n",
            "            loss = criterion(outputs, targets)\n",
            "            acc = calculate_accuracy(outputs, targets)\n",
            "\n",
            "            losses.update(loss.item(), inputs.size(0))\n",
            "            accuracies.update(acc, inputs.size(0))\n",
            "\n",
            "            batch_time.update(time.time() - end_time)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\validation.py",
        "line_number": 48,
        "API": ".format(",
        "context": [
            "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
            "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
            "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
            "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
            "                  'Acc {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
            "                      epoch,\n",
            "                      i + 1,\n",
            "                      len(data_loader),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\validation.py",
        "line_number": 76,
        "API": ".item(",
        "context": [
            "        dist.all_reduce(loss_count, op=dist.ReduceOp.SUM)\n",
            "        dist.all_reduce(acc_sum, op=dist.ReduceOp.SUM)\n",
            "        dist.all_reduce(acc_count, op=dist.ReduceOp.SUM)\n",
            "\n",
            "        losses.avg = loss_sum.item() / loss_count.item()\n",
            "        accuracies.avg = acc_sum.item() / acc_count.item()\n",
            "\n",
            "    if logger is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\datasets\\activitynet.py",
        "line_number": 85,
        "API": ".load(",
        "context": [
            "\n",
            "    def __make_dataset(self, root_path, annotation_path, subset,\n",
            "                       video_path_formatter):\n",
            "        with annotation_path.open('r') as f:\n",
            "            data = json.load(f)\n",
            "        video_ids, annotations, fps_values = get_video_ids_annotations_and_fps(\n",
            "            data, subset)\n",
            "        class_to_idx = get_class_labels(data)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\datasets\\activitynet.py",
        "line_number": 96,
        "API": ".format(",
        "context": [
            "\n",
            "        dataset = []\n",
            "        for i in range(len(video_ids)):\n",
            "            if i % 1000 == 0:\n",
            "                print('dataset loading [{}/{}]'.format(i, len(video_ids)))\n",
            "\n",
            "            video_path = video_path_formatter(root_path, label, video_ids[i])\n",
            "            if not video_path.exists():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\datasets\\activitynet.py",
        "line_number": 105,
        "API": ".floor(",
        "context": [
            "\n",
            "            fps = fps_values[i]\n",
            "\n",
            "            for annotation in annotations[i]:\n",
            "                t_begin = math.floor(annotation['segment'][0] * fps) + 1\n",
            "                t_end = math.floor(annotation['segment'][1] * fps) + 1\n",
            "                n_video_frames = get_n_frames(video_path)\n",
            "                t_end = min(t_end, n_video_frames)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\datasets\\activitynet.py",
        "line_number": 132,
        "API": ".load(",
        "context": [
            "\n",
            "    def __make_untrimmed_dataset(self, root_path, annotation_path, subset,\n",
            "                                 video_path_formatter):\n",
            "        with annotation_path.open('r') as f:\n",
            "            data = json.load(f)\n",
            "        video_ids, annotations, fps_values = get_video_ids_annotations_and_fps(\n",
            "            data, subset)\n",
            "        class_to_idx = get_class_labels(data)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\datasets\\loader.py",
        "line_number": 12,
        "API": ".convert(",
        "context": [
            "    def __call__(self, path):\n",
            "        # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
            "        with path.open('rb') as f:\n",
            "            with Image.open(f) as img:\n",
            "                return img.convert('RGB')\n",
            "\n",
            "\n",
            "class ImageLoaderAccImage(object):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\datasets\\loader.py",
        "line_number": 77,
        "API": ".merge(",
        "context": [
            "                        Image.open(io.BytesIO(video_data[i]))\n",
            "                        for video_data in flow_data\n",
            "                    ]\n",
            "                    frame.append(frame[-1])  # add dummy data into third channel\n",
            "                    video.append(Image.merge('RGB', frame))\n",
            "\n",
            "        return video"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\datasets\\videodataset.py",
        "line_number": 68,
        "API": ".load(",
        "context": [
            "\n",
            "    def __make_dataset(self, root_path, annotation_path, subset,\n",
            "                       video_path_formatter):\n",
            "        with annotation_path.open('r') as f:\n",
            "            data = json.load(f)\n",
            "        video_ids, video_paths, annotations = get_database(\n",
            "            data, subset, root_path, video_path_formatter)\n",
            "        class_to_idx = get_class_labels(data)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\datasets\\videodataset.py",
        "line_number": 80,
        "API": ".format(",
        "context": [
            "        n_videos = len(video_ids)\n",
            "        dataset = []\n",
            "        for i in range(n_videos):\n",
            "            if i % (n_videos // 5) == 0:\n",
            "                print('dataset loading [{}/{}]'.format(i, len(video_ids)))\n",
            "\n",
            "            if 'label' in annotations[i]:\n",
            "                label = annotations[i]['label']\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\datasets\\videodataset.py",
        "line_number": 114,
        "API": ".stack(",
        "context": [
            "        clip = self.loader(path, frame_indices)\n",
            "        if self.spatial_transform is not None:\n",
            "            self.spatial_transform.randomize_parameters()\n",
            "            clip = [self.spatial_transform(img) for img in clip]\n",
            "        clip = torch.stack(clip, 0).permute(1, 0, 2, 3)\n",
            "\n",
            "        return clip\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\datasets\\videodataset_multiclips.py",
        "line_number": 35,
        "API": ".stack(",
        "context": [
            "            clip = self.loader(path, clip_frame_indices)\n",
            "            if self.spatial_transform is not None:\n",
            "                self.spatial_transform.randomize_parameters()\n",
            "                clip = [self.spatial_transform(img) for img in clip]\n",
            "            clips.append(torch.stack(clip, 0).permute(1, 0, 2, 3))\n",
            "            segments.append(\n",
            "                [min(clip_frame_indices),\n",
            "                 max(clip_frame_indices) + 1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\models\\densenet.py",
        "line_number": 36,
        "API": ".dropout(",
        "context": [
            "\n",
            "    def forward(self, x):\n",
            "        new_features = super().forward(x)\n",
            "        if self.drop_rate > 0:\n",
            "            new_features = F.dropout(new_features,\n",
            "                                     p=self.drop_rate,\n",
            "                                     training=self.training)\n",
            "        return torch.cat([x, new_features], 1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\models\\densenet.py",
        "line_number": 50,
        "API": ".format(",
        "context": [
            "        super().__init__()\n",
            "        for i in range(num_layers):\n",
            "            layer = _DenseLayer(num_input_features + i * growth_rate,\n",
            "                                growth_rate, bn_size, drop_rate)\n",
            "            self.add_module('denselayer{}'.format(i + 1), layer)\n",
            "\n",
            "\n",
            "class _Transition(nn.Sequential):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\models\\densenet.py",
        "line_number": 118,
        "API": ".format(",
        "context": [
            "                                num_input_features=num_features,\n",
            "                                bn_size=bn_size,\n",
            "                                growth_rate=growth_rate,\n",
            "                                drop_rate=drop_rate)\n",
            "            self.features.add_module('denseblock{}'.format(i + 1), block)\n",
            "            num_features = num_features + num_layers * growth_rate\n",
            "            if i != len(block_config) - 1:\n",
            "                trans = _Transition(num_input_features=num_features,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\models\\densenet.py",
        "line_number": 123,
        "API": ".format(",
        "context": [
            "            num_features = num_features + num_layers * growth_rate\n",
            "            if i != len(block_config) - 1:\n",
            "                trans = _Transition(num_input_features=num_features,\n",
            "                                    num_output_features=num_features // 2)\n",
            "                self.features.add_module('transition{}'.format(i + 1), trans)\n",
            "                num_features = num_features // 2\n",
            "\n",
            "        # Final batch norm\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\models\\densenet.py",
        "line_number": 152,
        "API": ".relu(",
        "context": [
            "                nn.init.constant_(m.bias, 0)\n",
            "\n",
            "    def forward(self, x):\n",
            "        features = self.features(x)\n",
            "        out = F.relu(features, inplace=True)\n",
            "        out = F.adaptive_avg_pool3d(out,\n",
            "                                    output_size=(1, 1,\n",
            "                                                 1)).view(features.size(0), -1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\models\\pre_act_resnet.py",
        "line_number": 25,
        "API": ".relu(",
        "context": [
            "    def forward(self, x):\n",
            "        residual = x\n",
            "\n",
            "        out = self.bn1(x)\n",
            "        out = self.relu(out)\n",
            "        out = self.conv1(out)\n",
            "\n",
            "        out = self.bn2(out)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\models\\pre_act_resnet.py",
        "line_number": 68,
        "API": ".relu(",
        "context": [
            "        out = self.relu(out)\n",
            "        out = self.conv2(out)\n",
            "\n",
            "        out = self.bn3(out)\n",
            "        out = self.relu(out)\n",
            "        out = self.conv3(out)\n",
            "\n",
            "        if self.downsample is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\models\\resnet.py",
        "line_number": 48,
        "API": ".relu(",
        "context": [
            "        residual = x\n",
            "\n",
            "        out = self.conv1(x)\n",
            "        out = self.bn1(out)\n",
            "        out = self.relu(out)\n",
            "\n",
            "        out = self.conv2(out)\n",
            "        out = self.bn2(out)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\models\\resnet.py",
        "line_number": 57,
        "API": ".relu(",
        "context": [
            "        if self.downsample is not None:\n",
            "            residual = self.downsample(x)\n",
            "\n",
            "        out += residual\n",
            "        out = self.relu(out)\n",
            "\n",
            "        return out\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\models\\resnet.py",
        "line_number": 162,
        "API": ".size(",
        "context": [
            "                nn.init.constant_(m.bias, 0)\n",
            "\n",
            "    def _downsample_basic_block(self, x, planes, stride):\n",
            "        out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
            "        zero_pads = torch.zeros(out.size(0), planes - out.size(1), out.size(2),\n",
            "                                out.size(3), out.size(4))\n",
            "        if isinstance(out.data, torch.cuda.FloatTensor):\n",
            "            zero_pads = zero_pads.cuda()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\models\\resnet.py",
        "line_number": 167,
        "API": ".cat(",
        "context": [
            "                                out.size(3), out.size(4))\n",
            "        if isinstance(out.data, torch.cuda.FloatTensor):\n",
            "            zero_pads = zero_pads.cuda()\n",
            "\n",
            "        out = torch.cat([out.data, zero_pads], dim=1)\n",
            "\n",
            "        return out\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\models\\resnet.py",
        "line_number": 198,
        "API": ".relu(",
        "context": [
            "\n",
            "    def forward(self, x):\n",
            "        x = self.conv1(x)\n",
            "        x = self.bn1(x)\n",
            "        x = self.relu(x)\n",
            "        if not self.no_max_pool:\n",
            "            x = self.maxpool(x)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\models\\resnet.py",
        "line_number": 209,
        "API": ".size(",
        "context": [
            "        x = self.layer4(x)\n",
            "\n",
            "        x = self.avgpool(x)\n",
            "\n",
            "        x = x.view(x.size(0), -1)\n",
            "        x = self.fc(x)\n",
            "\n",
            "        return x\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\models\\resnet2p1d.py",
        "line_number": 69,
        "API": ".relu(",
        "context": [
            "        residual = x\n",
            "\n",
            "        out = self.conv1_s(x)\n",
            "        out = self.bn1_s(out)\n",
            "        out = self.relu(out)\n",
            "        out = self.conv1_t(out)\n",
            "        out = self.bn1_t(out)\n",
            "        out = self.relu(out)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\models\\resnet2p1d.py",
        "line_number": 76,
        "API": ".relu(",
        "context": [
            "        out = self.relu(out)\n",
            "\n",
            "        out = self.conv2_s(out)\n",
            "        out = self.bn2_s(out)\n",
            "        out = self.relu(out)\n",
            "        out = self.conv2_t(out)\n",
            "        out = self.bn2_t(out)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\models\\resnet2p1d.py",
        "line_number": 117,
        "API": ".relu(",
        "context": [
            "        residual = x\n",
            "\n",
            "        out = self.conv1(x)\n",
            "        out = self.bn1(out)\n",
            "        out = self.relu(out)\n",
            "\n",
            "        out = self.conv2_s(out)\n",
            "        out = self.bn2_s(out)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\models\\resnet2p1d.py",
        "line_number": 124,
        "API": ".relu(",
        "context": [
            "        out = self.bn2_s(out)\n",
            "        out = self.relu(out)\n",
            "        out = self.conv2_t(out)\n",
            "        out = self.bn2_t(out)\n",
            "        out = self.relu(out)\n",
            "\n",
            "        out = self.conv3(out)\n",
            "        out = self.bn3(out)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\models\\resnet2p1d.py",
        "line_number": 246,
        "API": ".relu(",
        "context": [
            "\n",
            "    def forward(self, x):\n",
            "        x = self.conv1_s(x)\n",
            "        x = self.bn1_s(x)\n",
            "        x = self.relu(x)\n",
            "        x = self.conv1_t(x)\n",
            "        x = self.bn1_t(x)\n",
            "        x = self.relu(x)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\add_fps_into_activitynet_json.py",
        "line_number": 14,
        "API": ".load(",
        "context": [
            "    else:\n",
            "        dst_json_path = json_path\n",
            "\n",
            "    with json_path.open('r') as f:\n",
            "        json_data = json.load(f)\n",
            "\n",
            "    for video_file_path in sorted(video_dir_path.iterdir()):\n",
            "        file_name = video_file_path.name\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\add_fps_into_activitynet_json.py",
        "line_number": 27,
        "API": ".split(",
        "context": [
            "        p = subprocess.Popen(\n",
            "            ffprobe_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
            "        res = p.communicate()[1].decode('utf-8')\n",
            "\n",
            "        fps = float([x for x in res.split(',') if 'fps' in x][0].rstrip('fps'))\n",
            "        json_data['database'][name[2:]]['fps'] = fps\n",
            "\n",
            "    with dst_json_path.open('w') as f:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\eval_accuracy.py",
        "line_number": 16,
        "API": ".load(",
        "context": [
            "\n",
            "\n",
            "def load_ground_truth(ground_truth_path, subset):\n",
            "    with ground_truth_path.open('r') as f:\n",
            "        data = json.load(f)\n",
            "\n",
            "    class_labels_map = get_class_labels(data)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\eval_accuracy.py",
        "line_number": 32,
        "API": ".load(",
        "context": [
            "\n",
            "\n",
            "def load_result(result_path, top_k, class_labels_map):\n",
            "    with result_path.open('r') as f:\n",
            "        data = json.load(f)\n",
            "\n",
            "    result = {}\n",
            "    for video_id, v in data['results'].items():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\eval_accuracy.py",
        "line_number": 41,
        "API": ".sort(",
        "context": [
            "        for this_result in v:\n",
            "            label = class_labels_map[this_result['label']]\n",
            "            score = this_result['score']\n",
            "            labels_and_scores.append((label, score))\n",
            "        labels_and_scores.sort(key=lambda x: x[1], reverse=True)\n",
            "        result[video_id] = list(zip(*labels_and_scores[:top_k]))[0]\n",
            "    return result\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\eval_accuracy.py",
        "line_number": 56,
        "API": ".format(",
        "context": [
            "def evaluate(ground_truth_path, result_path, subset, top_k, ignore):\n",
            "    print('load ground truth')\n",
            "    ground_truth, class_labels_map = load_ground_truth(ground_truth_path,\n",
            "                                                       subset)\n",
            "    print('number of ground truth: {}'.format(len(ground_truth)))\n",
            "\n",
            "    print('load result')\n",
            "    result = load_result(result_path, top_k, class_labels_map)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\eval_accuracy.py",
        "line_number": 67,
        "API": ".format(",
        "context": [
            "    ground_truth = remove_nonexistent_ground_truth(ground_truth, result)\n",
            "    if ignore:\n",
            "        n_ground_truth = len(ground_truth)\n",
            "\n",
            "    print('calculate top-{} accuracy'.format(top_k))\n",
            "    correct = [1 if line[1] in result[line[0]] else 0 for line in ground_truth]\n",
            "    accuracy = sum(correct) / n_ground_truth\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\eval_accuracy.py",
        "line_number": 93,
        "API": ".format(",
        "context": [
            "    accuracy = evaluate(args.ground_truth_path, args.result_path, args.subset,\n",
            "                        args.k, args.ignore)\n",
            "\n",
            "    if args.save:\n",
            "        with (args.result_path.parent / 'top{}.txt'.format(\n",
            "                args.k)).open('w') as f:\n",
            "            f.write(str(accuracy))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\generate_video_hdf5.py",
        "line_number": 15,
        "API": ".split(",
        "context": [
            "        return\n",
            "\n",
            "    ffprobe_cmd = ('ffprobe -v error -select_streams v:0 '\n",
            "                   '-of default=noprint_wrappers=1:nokey=1 -show_entries '\n",
            "                   'stream=width,height,avg_frame_rate,duration').split()\n",
            "    ffprobe_cmd.append(str(video_file_path))\n",
            "\n",
            "    p = subprocess.run(ffprobe_cmd, capture_output=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\generate_video_hdf5.py",
        "line_number": 48,
        "API": ".glob(",
        "context": [
            "    try:\n",
            "        with h5py.File(hdf5_path, 'w') as f:\n",
            "            dtype = h5py.special_dtype(vlen='uint8')\n",
            "            video = f.create_dataset('video',\n",
            "                                     (len(list(dst_dir_path.glob('*.jpg'))),),\n",
            "                                     dtype=dtype)\n",
            "    except OSError as exc:\n",
            "        if 'errno = 36' in exc.args[0]:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\generate_video_hdf5.py",
        "line_number": 56,
        "API": ".glob(",
        "context": [
            "            hdf5_path = dst_dir_path.parent / f'{dst_dir_path.name[:250]}.hdf5'\n",
            "            with h5py.File(hdf5_path, 'w') as f:\n",
            "                dtype = h5py.special_dtype(vlen='uint8')\n",
            "                video = f.create_dataset(\n",
            "                    'video', (len(list(dst_dir_path.glob('*.jpg'))),),\n",
            "                    dtype=dtype)\n",
            "        else:\n",
            "            raise\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\generate_video_hdf5.py",
        "line_number": 61,
        "API": ".glob(",
        "context": [
            "                    dtype=dtype)\n",
            "        else:\n",
            "            raise\n",
            "\n",
            "    for i, file_path in enumerate(sorted(dst_dir_path.glob('*.jpg'))):\n",
            "        with file_path.open('rb') as f:\n",
            "            data = f.read()\n",
            "        with h5py.File(hdf5_path, 'r+') as f:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\generate_video_hdf5.py",
        "line_number": 68,
        "API": ".glob(",
        "context": [
            "        with h5py.File(hdf5_path, 'r+') as f:\n",
            "            video = f['video']\n",
            "            video[i] = np.frombuffer(data, dtype='uint8')\n",
            "\n",
            "    for file_path in dst_dir_path.glob('*.jpg'):\n",
            "        file_path.unlink()\n",
            "    dst_dir_path.rmdir()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\generate_video_jpgs.py",
        "line_number": 21,
        "API": ".split(",
        "context": [
            "    res = p.stdout.decode('utf-8').splitlines()\n",
            "    if len(res) < 4:\n",
            "        return\n",
            "\n",
            "    frame_rate = [float(r) for r in res[2].split('/')]\n",
            "    frame_rate = frame_rate[0] / frame_rate[1]\n",
            "    duration = float(res[3])\n",
            "    n_frames = int(frame_rate * duration)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\generate_video_jpgs.py",
        "line_number": 41,
        "API": ".format(",
        "context": [
            "    width = int(res[0])\n",
            "    height = int(res[1])\n",
            "\n",
            "    if width > height:\n",
            "        vf_param = 'scale=-1:{}'.format(size)\n",
            "    else:\n",
            "        vf_param = 'scale={}:-1'.format(size)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\generate_video_jpgs.py",
        "line_number": 46,
        "API": ".format(",
        "context": [
            "    else:\n",
            "        vf_param = 'scale={}:-1'.format(size)\n",
            "\n",
            "    if fps > 0:\n",
            "        vf_param += ',minterpolate={}'.format(fps)\n",
            "\n",
            "    ffmpeg_cmd = ['ffmpeg', '-i', str(video_file_path), '-vf', vf_param]\n",
            "    ffmpeg_cmd += ['-threads', '1', '{}/image_%05d.jpg'.format(dst_dir_path)]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\hmdb51_json.py",
        "line_number": 13,
        "API": ".format(",
        "context": [
            "def convert_csv_to_dict(csv_dir_path, split_index):\n",
            "    database = {}\n",
            "    for file_path in csv_dir_path.iterdir():\n",
            "        filename = file_path.name\n",
            "        if 'split{}'.format(split_index) not in filename:\n",
            "            continue\n",
            "\n",
            "        data = pd.read_csv(csv_dir_path / filename, delimiter=' ', header=None)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\hmdb51_json.py",
        "line_number": 28,
        "API": ".split(",
        "context": [
            "                subset = 'training'\n",
            "            elif row[1] == 2:\n",
            "                subset = 'validation'\n",
            "\n",
            "            keys.append(row[0].split('.')[0])\n",
            "            subsets.append(subset)\n",
            "\n",
            "        for i in range(len(keys)):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\hmdb51_json.py",
        "line_number": 35,
        "API": ".join(",
        "context": [
            "        for i in range(len(keys)):\n",
            "            key = keys[i]\n",
            "            database[key] = {}\n",
            "            database[key]['subset'] = subsets[i]\n",
            "            label = '_'.join(filename.split('_')[:-2])\n",
            "            database[key]['annotations'] = {'label': label}\n",
            "\n",
            "    return database\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\hmdb51_json.py",
        "line_number": 44,
        "API": ".join(",
        "context": [
            "\n",
            "def get_labels(csv_dir_path):\n",
            "    labels = []\n",
            "    for file_path in csv_dir_path.iterdir():\n",
            "        labels.append('_'.join(file_path.name.split('_')[:-2]))\n",
            "    return sorted(list(set(labels)))\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\hmdb51_json.py",
        "line_number": 91,
        "API": ".format(",
        "context": [
            "\n",
            "    args = parser.parse_args()\n",
            "\n",
            "    for split_index in range(1, 4):\n",
            "        dst_json_path = args.dst_dir_path / 'hmdb51_{}.json'.format(split_index)\n",
            "        convert_hmdb51_csv_to_json(args.dir_path, split_index, args.video_path,\n",
            "                                   dst_json_path)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\kinetics_json.py",
        "line_number": 37,
        "API": ".unique(",
        "context": [
            "\n",
            "\n",
            "def load_labels(train_csv_path):\n",
            "    data = pd.read_csv(train_csv_path)\n",
            "    return data['label'].unique().tolist()\n",
            "\n",
            "\n",
            "def convert_kinetics_csv_to_json(train_csv_path, val_csv_path, test_csv_path,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\kinetics_json.py",
        "line_number": 109,
        "API": ".format(",
        "context": [
            "\n",
            "    assert args.video_type in ['jpg', 'hdf5']\n",
            "\n",
            "    train_csv_path = (args.dir_path /\n",
            "                      'kinetics-{}_train.csv'.format(args.n_classes))\n",
            "    val_csv_path = (args.dir_path /\n",
            "                    'kinetics-{}_val.csv'.format(args.n_classes))\n",
            "    test_csv_path = (args.dir_path /\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\mit_json.py",
        "line_number": 15,
        "API": ".split(",
        "context": [
            "    keys = []\n",
            "    key_labels = []\n",
            "    if subset == 'testing':\n",
            "        for i in range(data.shape[0]):\n",
            "            basename = data.iloc[i, 0].split('/')\n",
            "            assert len(basename) == 1\n",
            "            basename = Path(basename[0]).stem\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\mit_json.py",
        "line_number": 22,
        "API": ".split(",
        "context": [
            "\n",
            "            keys.append(basename)\n",
            "    else:\n",
            "        for i in range(data.shape[0]):\n",
            "            basename = data.iloc[i, 0].split('/')\n",
            "            assert len(basename) == 2\n",
            "            basename = Path(basename[1]).stem\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\remove_dataparallel.py",
        "line_number": 13,
        "API": ".load(",
        "context": [
            "\n",
            "if args.dst_file_path is None:\n",
            "    args.dst_file_path = args.file_path\n",
            "\n",
            "x = torch.load(args.file_path)\n",
            "state_dict = x['state_dict']\n",
            "new_state_dict = OrderedDict()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\remove_dataparallel.py",
        "line_number": 18,
        "API": ".join(",
        "context": [
            "state_dict = x['state_dict']\n",
            "new_state_dict = OrderedDict()\n",
            "\n",
            "for k, v in state_dict.items():\n",
            "    new_k = '.'.join(k.split('.')[1:])\n",
            "    new_state_dict[new_k] = v\n",
            "\n",
            "x['state_dict'] = new_state_dict\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\remove_dataparallel.py",
        "line_number": 23,
        "API": ".save(",
        "context": [
            "    new_state_dict[new_k] = v\n",
            "\n",
            "x['state_dict'] = new_state_dict\n",
            "\n",
            "torch.save(x, args.dst_file_path)"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\ucf101_json.py",
        "line_number": 15,
        "API": ".split(",
        "context": [
            "    keys = []\n",
            "    key_labels = []\n",
            "    for i in range(data.shape[0]):\n",
            "        row = data.iloc[i, :]\n",
            "        slash_rows = data.iloc[i, 0].split('/')\n",
            "        class_name = slash_rows[0]\n",
            "        basename = slash_rows[1].split('.')[0]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3D-ResNets-PyTorch\\util_scripts\\ucf101_json.py",
        "line_number": 88,
        "API": ".format(",
        "context": [
            "    args = parser.parse_args()\n",
            "\n",
            "    for split_index in range(1, 4):\n",
            "        label_csv_path = args.dir_path / 'classInd.txt'\n",
            "        train_csv_path = args.dir_path / 'trainlist0{}.txt'.format(split_index)\n",
            "        val_csv_path = args.dir_path / 'testlist0{}.txt'.format(split_index)\n",
            "        dst_json_path = args.dst_path / 'ucf101_0{}.json'.format(split_index)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\benchmark.py",
        "line_number": 24,
        "API": ".load(",
        "context": [
            "\n",
            "def extract_param(checkpoint_fp, root='', filelists=None, arch='mobilenet_1', num_classes=62, device_ids=[0],\n",
            "                  batch_size=128, num_workers=4):\n",
            "    map_location = {f'cuda:{i}': 'cuda:0' for i in range(8)}\n",
            "    checkpoint = torch.load(checkpoint_fp, map_location=map_location)['state_dict']\n",
            "    torch.cuda.set_device(device_ids[0])\n",
            "    model = getattr(mobilenet_v1, arch)(num_classes=num_classes)\n",
            "    model = nn.DataParallel(model, device_ids=device_ids).cuda()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\benchmark.py",
        "line_number": 45,
        "API": ".flatten(",
        "context": [
            "            inputs = inputs.cuda()\n",
            "            output = model(inputs)\n",
            "\n",
            "            for i in range(output.shape[0]):\n",
            "                param_prediction = output[i].cpu().numpy().flatten()\n",
            "\n",
            "                outputs.append(param_prediction)\n",
            "        outputs = np.array(outputs, dtype=np.float32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\benchmark_aflw.py",
        "line_number": 9,
        "API": ".join(",
        "context": [
            "from math import sqrt\n",
            "from utils.io import _load\n",
            "\n",
            "d = 'test.configs'\n",
            "yaw_list = _load(osp.join(d, 'AFLW_GT_crop_yaws.npy'))\n",
            "roi_boxs = _load(osp.join(d, 'AFLW_GT_crop_roi_box.npy'))\n",
            "pts68_all = _load(osp.join(d, 'AFLW_GT_pts68.npy'))\n",
            "pts21_all = _load(osp.join(d, 'AFLW_GT_pts21.npy'))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\benchmark_aflw.py",
        "line_number": 16,
        "API": ".abs(",
        "context": [
            "pts21_all = _load(osp.join(d, 'AFLW_GT_pts21.npy'))\n",
            "\n",
            "\n",
            "def ana(nme_list):\n",
            "    yaw_list_abs = np.abs(yaw_list)\n",
            "    ind_yaw_1 = yaw_list_abs <= 30\n",
            "    ind_yaw_2 = np.bitwise_and(yaw_list_abs > 30, yaw_list_abs <= 60)\n",
            "    ind_yaw_3 = yaw_list_abs > 60\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\benchmark_aflw.py",
        "line_number": 25,
        "API": ".mean(",
        "context": [
            "    nme_1 = nme_list[ind_yaw_1]\n",
            "    nme_2 = nme_list[ind_yaw_2]\n",
            "    nme_3 = nme_list[ind_yaw_3]\n",
            "\n",
            "    mean_nme_1 = np.mean(nme_1) * 100\n",
            "    mean_nme_2 = np.mean(nme_2) * 100\n",
            "    mean_nme_3 = np.mean(nme_3) * 100\n",
            "    # mean_nme_all = np.mean(nme_list) * 100\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\benchmark_aflw.py",
        "line_number": 30,
        "API": ".std(",
        "context": [
            "    mean_nme_2 = np.mean(nme_2) * 100\n",
            "    mean_nme_3 = np.mean(nme_3) * 100\n",
            "    # mean_nme_all = np.mean(nme_list) * 100\n",
            "\n",
            "    std_nme_1 = np.std(nme_1) * 100\n",
            "    std_nme_2 = np.std(nme_2) * 100\n",
            "    std_nme_3 = np.std(nme_3) * 100\n",
            "    # std_nme_all = np.std(nme_list) * 100\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\benchmark_aflw.py",
        "line_number": 36,
        "API": ".mean(",
        "context": [
            "    std_nme_3 = np.std(nme_3) * 100\n",
            "    # std_nme_all = np.std(nme_list) * 100\n",
            "\n",
            "    mean_all = [mean_nme_1, mean_nme_2, mean_nme_3]\n",
            "    mean = np.mean(mean_all)\n",
            "    std = np.std(mean_all)\n",
            "\n",
            "    s1 = '[ 0, 30]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_1, std_nme_1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\benchmark_aflw.py",
        "line_number": 41,
        "API": ".format(",
        "context": [
            "    std = np.std(mean_all)\n",
            "\n",
            "    s1 = '[ 0, 30]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_1, std_nme_1)\n",
            "    s2 = '[30, 60]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_2, std_nme_2)\n",
            "    s3 = '[60, 90]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_3, std_nme_3)\n",
            "    # s4 = '[ 0, 90]\\tMean: \\x1b[31m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_all, std_nme_all)\n",
            "    s5 = '[ 0, 90]\\tMean: \\x1b[31m{:.3f}\\x1b[0m, Std: \\x1b[31m{:.3f}\\x1b[0m'.format(mean, std)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\benchmark_aflw.py",
        "line_number": 75,
        "API": ".zeros_like(",
        "context": [
            "        pts68_fit[0, :] = pts68_fit[0, :] * scale_x + sx\n",
            "        pts68_fit[1, :] = pts68_fit[1, :] * scale_y + sy\n",
            "\n",
            "        # pts68 -> pts21\n",
            "        pts21_est = np.zeros_like(pts21_gt, dtype=np.float32)\n",
            "        for i in range(21):\n",
            "            ind = ind_68to21[i]\n",
            "            tmp = np.mean(pts68_fit[:, ind], 1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\benchmark_aflw.py",
        "line_number": 82,
        "API": ".max(",
        "context": [
            "            tmp = np.mean(pts68_fit[:, ind], 1)\n",
            "            pts21_est[:, i] = tmp\n",
            "\n",
            "        # build bbox\n",
            "        minx, maxx = np.min(pts68_gt[0, :]), np.max(pts68_gt[0, :])\n",
            "        miny, maxy = np.min(pts68_gt[1, :]), np.max(pts68_gt[1, :])\n",
            "        llength = sqrt((maxx - minx) * (maxy - miny))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\benchmark_aflw.py",
        "line_number": 89,
        "API": ".sqrt(",
        "context": [
            "\n",
            "        # nme\n",
            "        pt_valid = (pts21_gt[0, :] != -1) & (pts21_gt[1, :] != -1)\n",
            "        dis = pts21_est[:, pt_valid] - pts21_gt[:, pt_valid]\n",
            "        dis = np.sqrt(np.sum(np.power(dis, 2), 0))\n",
            "        dis = np.mean(dis)\n",
            "        nme = dis / llength\n",
            "        nme_list.append(nme)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\benchmark_aflw.py",
        "line_number": 94,
        "API": ".array(",
        "context": [
            "        dis = np.mean(dis)\n",
            "        nme = dis / llength\n",
            "        nme_list.append(nme)\n",
            "\n",
            "    nme_list = np.array(nme_list, dtype=np.float32)\n",
            "    return nme_list\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\benchmark_aflw2000.py",
        "line_number": 18,
        "API": ".join(",
        "context": [
            "\n",
            "d = 'test.configs'\n",
            "\n",
            "# [1312, 383, 305], current version\n",
            "yaws_list = _load(osp.join(d, 'AFLW2000-3D.pose.npy'))\n",
            "\n",
            "# [1306, 462, 232], same as paper\n",
            "# yaws_list = _load(osp.join(d, 'AFLW2000-3D-new.pose.npy'))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\benchmark_aflw2000.py",
        "line_number": 24,
        "API": ".join(",
        "context": [
            "# [1306, 462, 232], same as paper\n",
            "# yaws_list = _load(osp.join(d, 'AFLW2000-3D-new.pose.npy'))\n",
            "\n",
            "# origin\n",
            "pts68_all_ori = _load(osp.join(d, 'AFLW2000-3D.pts68.npy'))\n",
            "\n",
            "# reannonated\n",
            "pts68_all_re = _load(osp.join(d, 'AFLW2000-3D-Reannotated.pts68.npy'))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\benchmark_aflw2000.py",
        "line_number": 32,
        "API": ".abs(",
        "context": [
            "roi_boxs = _load(osp.join(d, 'AFLW2000-3D_crop.roi_box.npy'))\n",
            "\n",
            "\n",
            "def ana(nme_list):\n",
            "    yaw_list_abs = np.abs(yaws_list)\n",
            "    ind_yaw_1 = yaw_list_abs <= 30\n",
            "    ind_yaw_2 = np.bitwise_and(yaw_list_abs > 30, yaw_list_abs <= 60)\n",
            "    ind_yaw_3 = yaw_list_abs > 60\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\benchmark_aflw2000.py",
        "line_number": 97,
        "API": ".max(",
        "context": [
            "        pts68_fit[0, :] = pts68_fit[0, :] * scale_x + sx\n",
            "        pts68_fit[1, :] = pts68_fit[1, :] * scale_y + sy\n",
            "\n",
            "        # build bbox\n",
            "        minx, maxx = np.min(pts68_gt[0, :]), np.max(pts68_gt[0, :])\n",
            "        miny, maxy = np.min(pts68_gt[1, :]), np.max(pts68_gt[1, :])\n",
            "        llength = sqrt((maxx - minx) * (maxy - miny))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\benchmark_aflw2000.py",
        "line_number": 103,
        "API": ".sqrt(",
        "context": [
            "        llength = sqrt((maxx - minx) * (maxy - miny))\n",
            "\n",
            "        #\n",
            "        dis = pts68_fit - pts68_gt[:2, :]\n",
            "        dis = np.sqrt(np.sum(np.power(dis, 2), 0))\n",
            "        dis = np.mean(dis)\n",
            "        nme = dis / llength\n",
            "        nme_list.append(nme)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\main.py",
        "line_number": 37,
        "API": ".load(",
        "context": [
            "    # 1. load pre-tained model\n",
            "    checkpoint_fp = 'models/phase1_wpdc_vdc.pth.tar'\n",
            "    arch = 'mobilenet_1'\n",
            "\n",
            "    checkpoint = torch.load(checkpoint_fp, map_location=lambda storage, loc: storage)['state_dict']\n",
            "    model = getattr(mobilenet_v1, arch)(num_classes=62)  # 62 = 12(pose) + 40(shape) +10(expression)\n",
            "\n",
            "    model_dict = model.state_dict()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\main.py",
        "line_number": 70,
        "API": ".strip(",
        "context": [
            "\n",
            "        if len(rects) == 0:\n",
            "            rects = dlib.rectangles()\n",
            "            rect_fp = img_fp + '.bbox'\n",
            "            lines = open(rect_fp).read().strip().split('\\n')[1:]\n",
            "            for l in lines:\n",
            "                l, r, t, b = [int(_) for _ in l.split(' ')[1:]]\n",
            "                rect = dlib.rectangle(l, r, t, b)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\main.py",
        "line_number": 87,
        "API": ".array(",
        "context": [
            "            # whether use dlib landmark to crop image, if not, use only face bbox to calc roi bbox for cropping\n",
            "            if args.dlib_landmark:\n",
            "                # - use landmark for cropping\n",
            "                pts = face_regressor(img_ori, rect).parts()\n",
            "                pts = np.array([[pt.x, pt.y] for pt in pts]).T\n",
            "                roi_box = parse_roi_box_from_landmark(pts)\n",
            "            else:\n",
            "                # - use detected face bbox\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\main.py",
        "line_number": 98,
        "API": ".unsqueeze(",
        "context": [
            "            img = crop_img(img_ori, roi_box)\n",
            "\n",
            "            # forward: one step\n",
            "            img = cv2.resize(img, dsize=(STD_SIZE, STD_SIZE), interpolation=cv2.INTER_LINEAR)\n",
            "            input = transform(img).unsqueeze(0)\n",
            "            with torch.no_grad():\n",
            "                if args.mode == 'gpu':\n",
            "                    input = input.cuda()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\main.py",
        "line_number": 103,
        "API": ".flatten(",
        "context": [
            "            with torch.no_grad():\n",
            "                if args.mode == 'gpu':\n",
            "                    input = input.cuda()\n",
            "                param = model(input)\n",
            "                param = param.squeeze().cpu().numpy().flatten().astype(np.float32)\n",
            "\n",
            "            # 68 pts\n",
            "            pts68 = predict_68pts(param, roi_box)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\main.py",
        "line_number": 113,
        "API": ".unsqueeze(",
        "context": [
            "            if args.bbox_init == 'two':\n",
            "                roi_box = parse_roi_box_from_landmark(pts68)\n",
            "                img_step2 = crop_img(img_ori, roi_box)\n",
            "                img_step2 = cv2.resize(img_step2, dsize=(STD_SIZE, STD_SIZE), interpolation=cv2.INTER_LINEAR)\n",
            "                input = transform(img_step2).unsqueeze(0)\n",
            "                with torch.no_grad():\n",
            "                    if args.mode == 'gpu':\n",
            "                        input = input.cuda()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\main.py",
        "line_number": 118,
        "API": ".flatten(",
        "context": [
            "                with torch.no_grad():\n",
            "                    if args.mode == 'gpu':\n",
            "                        input = input.cuda()\n",
            "                    param = model(input)\n",
            "                    param = param.squeeze().cpu().numpy().flatten().astype(np.float32)\n",
            "\n",
            "                pts68 = predict_68pts(param, roi_box)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\main.py",
        "line_number": 132,
        "API": ".format(",
        "context": [
            "            if args.dump_ply or args.dump_vertex or args.dump_depth or args.dump_pncc or args.dump_obj:\n",
            "                vertices = predict_dense(param, roi_box)\n",
            "                vertices_lst.append(vertices)\n",
            "            if args.dump_ply:\n",
            "                dump_to_ply(vertices, tri, '{}_{}.ply'.format(img_fp.replace(suffix, ''), ind))\n",
            "            if args.dump_vertex:\n",
            "                dump_vertex(vertices, '{}_{}.mat'.format(img_fp.replace(suffix, ''), ind))\n",
            "            if args.dump_pts:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\main.py",
        "line_number": 138,
        "API": ".format(",
        "context": [
            "                dump_vertex(vertices, '{}_{}.mat'.format(img_fp.replace(suffix, ''), ind))\n",
            "            if args.dump_pts:\n",
            "                wfp = '{}_{}.txt'.format(img_fp.replace(suffix, ''), ind)\n",
            "                np.savetxt(wfp, pts68, fmt='%.3f')\n",
            "                print('Save 68 3d landmarks to {}'.format(wfp))\n",
            "            if args.dump_roi_box:\n",
            "                wfp = '{}_{}.roibox'.format(img_fp.replace(suffix, ''), ind)\n",
            "                np.savetxt(wfp, roi_box, fmt='%.3f')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\main.py",
        "line_number": 144,
        "API": ".format(",
        "context": [
            "                wfp = '{}_{}.roibox'.format(img_fp.replace(suffix, ''), ind)\n",
            "                np.savetxt(wfp, roi_box, fmt='%.3f')\n",
            "                print('Save roi box to {}'.format(wfp))\n",
            "            if args.dump_paf:\n",
            "                wfp_paf = '{}_{}_paf.jpg'.format(img_fp.replace(suffix, ''), ind)\n",
            "                wfp_crop = '{}_{}_crop.jpg'.format(img_fp.replace(suffix, ''), ind)\n",
            "                paf_feature = gen_img_paf(img_crop=img, param=param, kernel_size=args.paf_size)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\main.py",
        "line_number": 150,
        "API": ".format(",
        "context": [
            "                paf_feature = gen_img_paf(img_crop=img, param=param, kernel_size=args.paf_size)\n",
            "\n",
            "                cv2.imwrite(wfp_paf, paf_feature)\n",
            "                cv2.imwrite(wfp_crop, img)\n",
            "                print('Dump to {} and {}'.format(wfp_crop, wfp_paf))\n",
            "            if args.dump_obj:\n",
            "                wfp = '{}_{}.obj'.format(img_fp.replace(suffix, ''), ind)\n",
            "                colors = get_colors(img_ori, vertices)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\main.py",
        "line_number": 155,
        "API": ".format(",
        "context": [
            "            if args.dump_obj:\n",
            "                wfp = '{}_{}.obj'.format(img_fp.replace(suffix, ''), ind)\n",
            "                colors = get_colors(img_ori, vertices)\n",
            "                write_obj_with_colors(wfp, vertices, tri, colors)\n",
            "                print('Dump obj with sampled texture to {}'.format(wfp))\n",
            "            ind += 1\n",
            "\n",
            "        if args.dump_pose:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\main.py",
        "line_number": 163,
        "API": ".format(",
        "context": [
            "            # P, pose = parse_pose(param)  # Camera matrix (without scale), and pose (yaw, pitch, roll, to verify)\n",
            "            img_pose = plot_pose_box(img_ori, Ps, pts_res)\n",
            "            wfp = img_fp.replace(suffix, '_pose.jpg')\n",
            "            cv2.imwrite(wfp, img_pose)\n",
            "            print('Dump to {}'.format(wfp))\n",
            "        if args.dump_depth:\n",
            "            wfp = img_fp.replace(suffix, '_depth.png')\n",
            "            # depths_img = get_depths_image(img_ori, vertices_lst, tri-1)  # python version\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\main.py",
        "line_number": 169,
        "API": ".format(",
        "context": [
            "            wfp = img_fp.replace(suffix, '_depth.png')\n",
            "            # depths_img = get_depths_image(img_ori, vertices_lst, tri-1)  # python version\n",
            "            depths_img = cget_depths_image(img_ori, vertices_lst, tri - 1)  # cython version\n",
            "            cv2.imwrite(wfp, depths_img)\n",
            "            print('Dump to {}'.format(wfp))\n",
            "        if args.dump_pncc:\n",
            "            wfp = img_fp.replace(suffix, '_pncc.png')\n",
            "            pncc_feature = cpncc(img_ori, vertices_lst, tri - 1)  # cython version\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\main.py",
        "line_number": 174,
        "API": ".format(",
        "context": [
            "        if args.dump_pncc:\n",
            "            wfp = img_fp.replace(suffix, '_pncc.png')\n",
            "            pncc_feature = cpncc(img_ori, vertices_lst, tri - 1)  # cython version\n",
            "            cv2.imwrite(wfp, pncc_feature[:, :, ::-1])  # cv2.imwrite will swap RGB -> BGR\n",
            "            print('Dump to {}'.format(wfp))\n",
            "        if args.dump_res:\n",
            "            draw_landmarks(img_ori, pts_res, wfp=img_fp.replace(suffix, '_3DDFA.jpg'), show_flg=args.show_flg)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\mobilenet_v1.py",
        "line_number": 36,
        "API": ".relu(",
        "context": [
            "\n",
            "    def forward(self, x):\n",
            "        out = self.conv_dw(x)\n",
            "        out = self.bn_dw(out)\n",
            "        out = self.relu(out)\n",
            "\n",
            "        out = self.conv_sep(out)\n",
            "        out = self.bn_sep(out)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\mobilenet_v1.py",
        "line_number": 88,
        "API": ".sqrt(",
        "context": [
            "\n",
            "        for m in self.modules():\n",
            "            if isinstance(m, nn.Conv2d):\n",
            "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
            "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
            "            elif isinstance(m, nn.BatchNorm2d):\n",
            "                m.weight.data.fill_(1)\n",
            "                m.bias.data.zero_()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\mobilenet_v1.py",
        "line_number": 96,
        "API": ".relu(",
        "context": [
            "\n",
            "    def forward(self, x):\n",
            "        x = self.conv1(x)\n",
            "        x = self.bn1(x)\n",
            "        x = self.relu(x)\n",
            "\n",
            "        x = self.dw2_1(x)\n",
            "        x = self.dw2_2(x)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\mobilenet_v1.py",
        "line_number": 113,
        "API": ".size(",
        "context": [
            "        x = self.dw5_6(x)\n",
            "        x = self.dw6(x)\n",
            "\n",
            "        x = self.avgpool(x)\n",
            "        x = x.view(x.size(0), -1)\n",
            "        x = self.fc(x)\n",
            "\n",
            "        return x\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\speed_cpu.py",
        "line_number": 23,
        "API": ".repeat(",
        "context": [
            "\n",
            "\n",
            "def main():\n",
            "    repeat, number = 5, 100\n",
            "    res = timeit.repeat(setup=SETUP_CODE,\n",
            "                        stmt=TEST_CODE,\n",
            "                        repeat=repeat,\n",
            "                        number=number)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\speed_cpu.py",
        "line_number": 29,
        "API": ".std(",
        "context": [
            "                        repeat=repeat,\n",
            "                        number=number)\n",
            "    res = np.array(res, dtype=np.float32)\n",
            "    res /= number\n",
            "    mean, var = np.mean(res), np.std(res)\n",
            "    print('Inference speed: {:.2f}\u00b1{:.2f} ms'.format(mean * 1000, var * 1000))\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\train.py",
        "line_number": 73,
        "API": ".split(",
        "context": [
            "    global args\n",
            "    args = parser.parse_args()\n",
            "\n",
            "    # some other operations\n",
            "    args.devices_id = [int(d) for d in args.devices_id.split(',')]\n",
            "    args.milestones = [int(m) for m in args.milestones.split(',')]\n",
            "\n",
            "    snapshot_dir = osp.split(args.snapshot)[0]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\train.py",
        "line_number": 83,
        "API": ".info(",
        "context": [
            "\n",
            "def print_args(args):\n",
            "    for arg in vars(args):\n",
            "        s = arg + ': ' + str(getattr(args, arg))\n",
            "        logging.info(s)\n",
            "\n",
            "\n",
            "def adjust_learning_rate(optimizer, epoch, milestones=None):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\train.py",
        "line_number": 108,
        "API": ".save(",
        "context": [
            "        param_group['lr'] = lr\n",
            "\n",
            "\n",
            "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
            "    torch.save(state, filename)\n",
            "    logging.info(f'Save checkpoint to {filename}')\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\train.py",
        "line_number": 129,
        "API": ".lower(",
        "context": [
            "        output = model(input)\n",
            "\n",
            "        data_time.update(time.time() - end)\n",
            "\n",
            "        if args.loss.lower() == 'vdc':\n",
            "            loss = criterion(output, target)\n",
            "        elif args.loss.lower() == 'wpdc':\n",
            "            loss = criterion(output, target)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\train.py",
        "line_number": 138,
        "API": ".size(",
        "context": [
            "            loss = criterion(output, target)\n",
            "        else:\n",
            "            raise Exception(f'Unknown loss {args.loss}')\n",
            "\n",
            "        losses.update(loss.item(), input.size(0))\n",
            "        # compute gradient and do SGD step\n",
            "        optimizer.zero_grad()\n",
            "        loss.backward()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\train.py",
        "line_number": 150,
        "API": ".info(",
        "context": [
            "        end = time.time()\n",
            "\n",
            "        # log\n",
            "        if i % args.print_freq == 0:\n",
            "            logging.info(f'Epoch: [{epoch}][{i}/{len(train_loader)}]\\t'\n",
            "                         f'LR: {lr:8f}\\t'\n",
            "                         f'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
            "                         # f'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\train.py",
        "line_number": 170,
        "API": ".item(",
        "context": [
            "            target = target.cuda(non_blocking=True)\n",
            "            output = model(input)\n",
            "\n",
            "            loss = criterion(output, target)\n",
            "            losses.append(loss.item())\n",
            "\n",
            "        elapse = time.time() - end\n",
            "        loss = np.mean(losses)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\train.py",
        "line_number": 197,
        "API": ".set_device(",
        "context": [
            "\n",
            "    # step1: define the model structure\n",
            "    model = getattr(mobilenet_v1, args.arch)(num_classes=args.num_classes)\n",
            "\n",
            "    torch.cuda.set_device(args.devices_id[0])  # fix bug for `ERROR: all tensors must be on devices[0]`\n",
            "\n",
            "    model = nn.DataParallel(model, device_ids=args.devices_id).cuda()  # -> GPU\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\train.py",
        "line_number": 203,
        "API": ".lower(",
        "context": [
            "    model = nn.DataParallel(model, device_ids=args.devices_id).cuda()  # -> GPU\n",
            "\n",
            "    # step2: optimization: loss and optimization method\n",
            "    # criterion = nn.MSELoss(size_average=args.size_average).cuda()\n",
            "    if args.loss.lower() == 'wpdc':\n",
            "        print(args.opt_style)\n",
            "        criterion = WPDCLoss(opt_style=args.opt_style).cuda()\n",
            "        logging.info('Use WPDC Loss')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\train.py",
        "line_number": 209,
        "API": ".info(",
        "context": [
            "        criterion = WPDCLoss(opt_style=args.opt_style).cuda()\n",
            "        logging.info('Use WPDC Loss')\n",
            "    elif args.loss.lower() == 'vdc':\n",
            "        criterion = VDCLoss(opt_style=args.opt_style).cuda()\n",
            "        logging.info('Use VDC Loss')\n",
            "    elif args.loss.lower() == 'pdc':\n",
            "        criterion = nn.MSELoss(size_average=args.size_average).cuda()\n",
            "        logging.info('Use PDC loss')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\train.py",
        "line_number": 224,
        "API": ".info(",
        "context": [
            "                                nesterov=True)\n",
            "    # step 2.1 resume\n",
            "    if args.resume:\n",
            "        if Path(args.resume).is_file():\n",
            "            logging.info(f'=> loading checkpoint {args.resume}')\n",
            "\n",
            "            checkpoint = torch.load(args.resume, map_location=lambda storage, loc: storage)['state_dict']\n",
            "            # checkpoint = torch.load(args.resume)['state_dict']\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\train.py",
        "line_number": 231,
        "API": ".info(",
        "context": [
            "            # checkpoint = torch.load(args.resume)['state_dict']\n",
            "            model.load_state_dict(checkpoint)\n",
            "\n",
            "        else:\n",
            "            logging.info(f'=> no checkpoint found at {args.resume}')\n",
            "\n",
            "    # step3: data\n",
            "    normalize = NormalizeGjz(mean=127.5, std=128)  # may need optimization\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\train.py",
        "line_number": 257,
        "API": ".info(",
        "context": [
            "\n",
            "    # step4: run\n",
            "    cudnn.benchmark = True\n",
            "    if args.test_initial:\n",
            "        logging.info('Testing from initial')\n",
            "        validate(val_loader, model, criterion, args.start_epoch)\n",
            "\n",
            "    for epoch in range(args.start_epoch, args.epochs + 1):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\vdc_loss.py",
        "line_number": 14,
        "API": ".view(",
        "context": [
            "\n",
            "def _parse_param_batch(param):\n",
            "    \"\"\"Work for both numpy and tensor\"\"\"\n",
            "    N = param.shape[0]\n",
            "    p_ = param[:, :12].view(N, 3, -1)\n",
            "    p = p_[:, :, :3]\n",
            "    offset = p_[:, :, -1].view(N, 3, 1)\n",
            "    alpha_shp = param[:, 12:52].view(N, -1, 1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\vdc_loss.py",
        "line_number": 59,
        "API": ".permute(",
        "context": [
            "\n",
            "        N = input.shape[0]\n",
            "        offset[:, -1] = offsetg[:, -1]\n",
            "        gt_vertex = pg @ (self.u + self.w_shp @ alpha_shpg + self.w_exp @ alpha_expg) \\\n",
            "            .view(N, -1, 3).permute(0, 2, 1) + offsetg\n",
            "        vertex = p @ (self.u + self.w_shp @ alpha_shp + self.w_exp @ alpha_exp) \\\n",
            "            .view(N, -1, 3).permute(0, 2, 1) + offset\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\vdc_loss.py",
        "line_number": 64,
        "API": ".mean(",
        "context": [
            "        vertex = p @ (self.u + self.w_shp @ alpha_shp + self.w_exp @ alpha_exp) \\\n",
            "            .view(N, -1, 3).permute(0, 2, 1) + offset\n",
            "\n",
            "        diff = (gt_vertex - vertex) ** 2\n",
            "        loss = torch.mean(diff)\n",
            "        return loss\n",
            "\n",
            "    def forward_resample(self, input, target, resample_num=132):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\vdc_loss.py",
        "line_number": 72,
        "API": ".reshape(",
        "context": [
            "        (p, offset, alpha_shp, alpha_exp), (pg, offsetg, alpha_shpg, alpha_expg) \\\n",
            "            = self.reconstruct_and_parse(input, target)\n",
            "\n",
            "        # resample index\n",
            "        index = torch.randperm(self.w_shp_length)[:resample_num].reshape(-1, 1)\n",
            "        keypoints_resample = torch.cat((3 * index, 3 * index + 1, 3 * index + 2), dim=1).view(-1).cuda()\n",
            "        keypoints_mix = torch.cat((self.keypoints, keypoints_resample))\n",
            "        w_shp_base = self.w_shp[keypoints_mix]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\vdc_loss.py",
        "line_number": 83,
        "API": ".permute(",
        "context": [
            "        offset[:, -1] = offsetg[:, -1]\n",
            "\n",
            "        N = input.shape[0]\n",
            "        gt_vertex = pg @ (u_base + w_shp_base @ alpha_shpg + w_exp_base @ alpha_expg) \\\n",
            "            .view(N, -1, 3).permute(0, 2, 1) + offsetg\n",
            "        vertex = p @ (u_base + w_shp_base @ alpha_shp + w_exp_base @ alpha_exp) \\\n",
            "            .view(N, -1, 3).permute(0, 2, 1) + offset\n",
            "        diff = (gt_vertex - vertex) ** 2\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\video_demo.py",
        "line_number": 37,
        "API": ".load(",
        "context": [
            "\n",
            "    tri = sio.loadmat('visualize/tri.mat')['tri']\n",
            "    transform = transforms.Compose([ToTensorGjz(), NormalizeGjz(mean=127.5, std=128)])\n",
            "\n",
            "    checkpoint = torch.load(checkpoint_fp, map_location=lambda storage, loc: storage)[\n",
            "        'state_dict'\n",
            "    ]\n",
            "    model = getattr(mobilenet_v1, arch)(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\video_demo.py",
        "line_number": 68,
        "API": ".array(",
        "context": [
            "        if len(last_frame_pts) == 0:\n",
            "            rects = face_detector(frame, 1)\n",
            "            for rect in rects:\n",
            "                pts = face_regressor(frame, rect).parts()\n",
            "                pts = np.array([[pt.x, pt.y] for pt in pts]).T\n",
            "                last_frame_pts.append(pts)\n",
            "\n",
            "        vertices_lst = []\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\video_demo.py",
        "line_number": 78,
        "API": ".unsqueeze(",
        "context": [
            "            img = crop_img(frame, roi_box)\n",
            "            img = cv2.resize(\n",
            "                img, dsize=(STD_SIZE, STD_SIZE), interpolation=cv2.INTER_LINEAR\n",
            "            )\n",
            "            input = transform(img).unsqueeze(0)\n",
            "            with torch.no_grad():\n",
            "                if args.mode == 'gpu':\n",
            "                    input = input.cuda()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\video_demo.py",
        "line_number": 83,
        "API": ".flatten(",
        "context": [
            "            with torch.no_grad():\n",
            "                if args.mode == 'gpu':\n",
            "                    input = input.cuda()\n",
            "                param = model(input)\n",
            "                param = param.squeeze().cpu().numpy().flatten().astype(np.float32)\n",
            "            pts68 = predict_68pts(param, roi_box)\n",
            "            vertex = predict_dense(param, roi_box)\n",
            "            lmk[:] = pts68[:2]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\visualize.py",
        "line_number": 31,
        "API": ".strip(",
        "context": [
            "\n",
            "def draw_landmarks():\n",
            "    filelists = 'test.data/AFLW2000-3D_crop.list'\n",
            "    root = 'AFLW-2000-3D/'\n",
            "    fns = open(filelists).read().strip().split('\\n')\n",
            "    params = _load('res/params_aflw2000.npy')\n",
            "\n",
            "    for i in range(2000):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\visualize.py",
        "line_number": 36,
        "API": ".join(",
        "context": [
            "    params = _load('res/params_aflw2000.npy')\n",
            "\n",
            "    for i in range(2000):\n",
            "        plt.close()\n",
            "        img_fp = osp.join(root, fns[i])\n",
            "        img = io.imread(img_fp)\n",
            "        lms = reconstruct_vertex(params[i], dense=False)\n",
            "        lms = convert_to_ori(lms, i)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\visualize.py",
        "line_number": 69,
        "API": ".scatter(",
        "context": [
            "        lms[1] = img.shape[1] - lms[1]\n",
            "        lms[2] = -lms[2]\n",
            "\n",
            "        # print(lms)\n",
            "        ax.scatter(lms[0], lms[2], lms[1], c=\"cyan\", alpha=1.0, edgecolor='b')\n",
            "\n",
            "        for ind in range(len(nums) - 1):\n",
            "            l, r = nums[ind], nums[ind + 1]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\visualize.py",
        "line_number": 85,
        "API": ".show(",
        "context": [
            "        ax.set_yticklabels([])\n",
            "        ax.set_zticklabels([])\n",
            "\n",
            "        plt.tight_layout()\n",
            "        # plt.show()\n",
            "\n",
            "        wfp = f'res/AFLW-2000-3D/{osp.basename(img_fp)}'\n",
            "        plt.savefig(wfp, dpi=200)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\visualize.py",
        "line_number": 94,
        "API": ".strip(",
        "context": [
            "\n",
            "def gen_3d_vertex():\n",
            "    filelists = 'test.data/AFLW2000-3D_crop.list'\n",
            "    root = 'AFLW-2000-3D/'\n",
            "    fns = open(filelists).read().strip().split('\\n')\n",
            "    params = _load('res/params_aflw2000.npy')\n",
            "\n",
            "    sel = ['00427', '00439', '00475', '00477', '00497', '00514', '00562', '00623', '01045', '01095', '01104', '01506',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\visualize.py",
        "line_number": 104,
        "API": ".join(",
        "context": [
            "    for i in range(2000):\n",
            "        fn = fns[i]\n",
            "        if fn in sel:\n",
            "            vertex = reconstruct_vertex(params[i], dense=True)\n",
            "            wfp = osp.join('res/AFLW-2000-3D_vertex/', fn.replace('.jpg', '.mat'))\n",
            "            print(wfp)\n",
            "            sio.savemat(wfp, {'vertex': vertex})\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\wpdc_loss.py",
        "line_number": 57,
        "API": ".reshape(",
        "context": [
            "        # resample index\n",
            "        if self.resample_num <= 0:\n",
            "            keypoints_mix = self.keypoints\n",
            "        else:\n",
            "            index = torch.randperm(self.w_shp_length)[:self.resample_num].reshape(-1, 1)\n",
            "            keypoints_resample = torch.cat((3 * index, 3 * index + 1, 3 * index + 2), dim=1).view(-1).cuda()\n",
            "            keypoints_mix = torch.cat((self.keypoints, keypoints_resample))\n",
            "        w_shp_base = self.w_shp[keypoints_mix]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\wpdc_loss.py",
        "line_number": 64,
        "API": ".clone(",
        "context": [
            "        w_shp_base = self.w_shp[keypoints_mix]\n",
            "        u_base = self.u[keypoints_mix]\n",
            "        w_exp_base = self.w_exp[keypoints_mix]\n",
            "\n",
            "        input = torch.tensor(input_.data.clone(), requires_grad=False)\n",
            "        target = torch.tensor(target_.data.clone(), requires_grad=False)\n",
            "\n",
            "        (p, offset, alpha_shp, alpha_exp), (pg, offsetg, alpha_shpg, alpha_expg) \\\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\wpdc_loss.py",
        "line_number": 77,
        "API": ".zeros_like(",
        "context": [
            "        N = input.shape[0]\n",
            "\n",
            "        offset[:, -1] = offsetg[:, -1]\n",
            "\n",
            "        weights = torch.zeros_like(input, dtype=torch.float)\n",
            "        tmpv = (u_base + w_shp_base @ alpha_shpg + w_exp_base @ alpha_expg).view(N, -1, 3).permute(0, 2, 1)\n",
            "\n",
            "        tmpv_norm = torch.norm(tmpv, dim=2)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\wpdc_loss.py",
        "line_number": 84,
        "API": ".abs(",
        "context": [
            "        tmpv_norm = torch.norm(tmpv, dim=2)\n",
            "        offset_norm = sqrt(w_shp_base.shape[0] // 3)\n",
            "\n",
            "        # for pose\n",
            "        param_diff_pose = torch.abs(input[:, :11] - target[:, :11])\n",
            "        for ind in range(11):\n",
            "            if ind in [0, 4, 8]:\n",
            "                weights[:, ind] = param_diff_pose[:, ind] * tmpv_norm[:, 0]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\wpdc_loss.py",
        "line_number": 98,
        "API": ".abs(",
        "context": [
            "\n",
            "        ## This is the optimizest version\n",
            "        # for shape_exp\n",
            "        magic_number = 0.00057339936  # scale\n",
            "        param_diff_shape_exp = torch.abs(input[:, 12:] - target[:, 12:])\n",
            "        # weights[:, 12:] = magic_number * param_diff_shape_exp * self.w_norm\n",
            "        w = torch.cat((w_shp_base, w_exp_base), dim=1)\n",
            "        w_norm = torch.norm(w, dim=0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\wpdc_loss.py",
        "line_number": 110,
        "API": ".max(",
        "context": [
            "        weights[:, :11] += eps\n",
            "        weights[:, 12:] += eps\n",
            "\n",
            "        # normalize the weights\n",
            "        maxes, _ = weights.max(dim=1)\n",
            "        maxes = maxes.view(-1, 1)\n",
            "        weights /= maxes\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\wpdc_loss.py",
        "line_number": 123,
        "API": ".mean(",
        "context": [
            "    def forward(self, input, target, weights_scale=10):\n",
            "        if self.opt_style == 'resample':\n",
            "            weights = self._calc_weights_resample(input, target)\n",
            "            loss = weights * (input - target) ** 2\n",
            "            return loss.mean()\n",
            "        else:\n",
            "            raise Exception(f'Unknown opt style: {self.opt_style}')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\c++\\convert_to_onnx.py",
        "line_number": 11,
        "API": ".load(",
        "context": [
            "def main():\n",
            "    # checkpoint_fp = 'weights/phase1_wpdc_vdc.pth.tar'\n",
            "    checkpoint_fp = 'weights/mb_1.p'\n",
            "    arch = 'mobilenet_1'\n",
            "    checkpoint = torch.load(checkpoint_fp, map_location=lambda storage, loc: storage)['state_dict']\n",
            "    model = getattr(mobilenet_v1, arch)(num_classes=62)  # 62 = 12(pose) + 40(shape) +10(expression)\n",
            "\n",
            "    model_dict = model.state_dict()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\c++\\convert_to_onnx.py",
        "line_number": 26,
        "API": ".randn(",
        "context": [
            "    model.load_state_dict(model_dict)\n",
            "\n",
            "    # conversion\n",
            "    batch_size = 1\n",
            "    dummy_input = torch.randn(batch_size, 3, 120, 120)\n",
            "    torch.onnx.export(model, dummy_input, checkpoint_fp.replace('.p', '.onnx'))\n",
            "    # torch.onnx.export(model, dummy_input, checkpoint_fp.replace('.pth.tar', '.onnx'))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\demo@obama\\convert_imgs_to_video.py",
        "line_number": 14,
        "API": ".join(",
        "context": [
            "def main():\n",
            "    assert len(sys.argv) >= 2\n",
            "    d = sys.argv[1]\n",
            "\n",
            "    fps = glob(osp.join(d, '*.jpg'))\n",
            "    fps = sorted(fps, key=lambda x: int(x.split('/')[-1].replace('.jpg', '')))\n",
            "\n",
            "    imgs = []\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\demo@obama\\rendering.py",
        "line_number": 29,
        "API": ".copy(",
        "context": [
            "\n",
            "\n",
            "def _to_ctype(arr):\n",
            "    if not arr.flags.c_contiguous:\n",
            "        return arr.copy(order='C')\n",
            "    return arr\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\demo@obama\\rendering.py",
        "line_number": 49,
        "API": ".format(",
        "context": [
            "        img = imageio.imread(img_fp).astype(np.float32) / 255.\n",
            "\n",
            "        # end = time.clock()\n",
            "        img_render = app(vertices, triangles, img)\n",
            "        # print('Elapse: {:.1f}ms'.format((time.clock() - end) * 1000))\n",
            "\n",
            "        img_wfp = osp.join(wd, osp.basename(img_fp))\n",
            "        imageio.imwrite(img_wfp, img_render)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\demo@obama\\rendering_demo.py",
        "line_number": 30,
        "API": ".show(",
        "context": [
            "    app = RenderPipeline(**cfg)\n",
            "    img_render = app(vertices, triangles, img)\n",
            "\n",
            "    plt.imshow(img_render)\n",
            "    plt.show()\n",
            "\n",
            "\n",
            "def main():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\cv_plot.py",
        "line_number": 13,
        "API": ".array(",
        "context": [
            "import cv2\n",
            "\n",
            "from utils.inference import calc_hypotenuse\n",
            "\n",
            "end_list = np.array([17, 22, 27, 42, 48, 31, 36, 68], dtype=np.int32) - 1\n",
            "\n",
            "\n",
            "def plot_kpt(image, kpt):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\cv_plot.py",
        "line_number": 22,
        "API": ".copy(",
        "context": [
            "    Args:\n",
            "        image: the input image\n",
            "        kpt: (68, 3).\n",
            "    '''\n",
            "    image = image.copy()\n",
            "    kpt = np.round(kpt).astype(np.int32)\n",
            "    for i in range(kpt.shape[0]):\n",
            "        st = kpt[i, :2]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\cv_plot.py",
        "line_number": 50,
        "API": ".reshape(",
        "context": [
            "    point_3d.append((-front_size, front_size, front_depth))\n",
            "    point_3d.append((front_size, front_size, front_depth))\n",
            "    point_3d.append((front_size, -front_size, front_depth))\n",
            "    point_3d.append((-front_size, -front_size, front_depth))\n",
            "    point_3d = np.array(point_3d, dtype=np.float).reshape(-1, 3)\n",
            "\n",
            "    return point_3d\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\cv_plot.py",
        "line_number": 62,
        "API": ".copy(",
        "context": [
            "        image: the input image\n",
            "        P: (3, 4). Affine Camera Matrix.\n",
            "        kpt: (2, 68) or (3, 68)\n",
            "    '''\n",
            "    image = image.copy()\n",
            "    if not isinstance(pts68s, list):\n",
            "        pts68s = [pts68s]\n",
            "    if not isinstance(Ps, list):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\cv_plot.py",
        "line_number": 74,
        "API": ".ones(",
        "context": [
            "        point_3d = build_camera_box(llength)\n",
            "        P = Ps[i]\n",
            "\n",
            "        # Map to 2d image points\n",
            "        point_3d_homo = np.hstack((point_3d, np.ones([point_3d.shape[0], 1])))  # n x 4\n",
            "        point_2d = point_3d_homo.dot(P.T)[:, :2]\n",
            "\n",
            "        point_2d[:, 1] = - point_2d[:, 1]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\cv_plot.py",
        "line_number": 79,
        "API": ".reshape(",
        "context": [
            "        point_2d = point_3d_homo.dot(P.T)[:, :2]\n",
            "\n",
            "        point_2d[:, 1] = - point_2d[:, 1]\n",
            "        point_2d[:, :2] = point_2d[:, :2] - np.mean(point_2d[:4, :2], 0) + np.mean(pts68[:2, :27], 1)\n",
            "        point_2d = np.int32(point_2d.reshape(-1, 2))\n",
            "\n",
            "        # Draw all the lines\n",
            "        cv2.polylines(image, [point_2d], True, color, line_width, cv2.LINE_AA)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\ddfa.py",
        "line_number": 18,
        "API": ".reshape(",
        "context": [
            "\n",
            "\n",
            "def _parse_param(param):\n",
            "    \"\"\"Work for both numpy and tensor\"\"\"\n",
            "    p_ = param[:12].reshape(3, -1)\n",
            "    p = p_[:, :3]\n",
            "    offset = p_[:, -1].reshape(3, 1)\n",
            "    alpha_shp = param[12:52].reshape(-1, 1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\ddfa.py",
        "line_number": 44,
        "API": ".reshape(",
        "context": [
            "\n",
            "    p, offset, alpha_shp, alpha_exp = _parse_param(param)\n",
            "\n",
            "    if dense:\n",
            "        vertex = p @ (u + w_shp @ alpha_shp + w_exp @ alpha_exp).reshape(3, -1, order='F') + offset\n",
            "\n",
            "        if transform:\n",
            "            # transform to image coordinate space\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\ddfa.py",
        "line_number": 51,
        "API": ".reshape(",
        "context": [
            "            # transform to image coordinate space\n",
            "            vertex[1, :] = std_size + 1 - vertex[1, :]\n",
            "    else:\n",
            "        \"\"\"For 68 pts\"\"\"\n",
            "        vertex = p @ (u_base + w_shp_base @ alpha_shp + w_exp_base @ alpha_exp).reshape(3, -1, order='F') + offset\n",
            "\n",
            "        if transform:\n",
            "            # transform to image coordinate space\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\ddfa.py",
        "line_number": 65,
        "API": ".lower(",
        "context": [
            "    return cv2.imread(path, cv2.IMREAD_COLOR)\n",
            "\n",
            "\n",
            "def str2bool(v):\n",
            "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
            "        return True\n",
            "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
            "        return False\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\ddfa.py",
        "line_number": 95,
        "API": ".transpose(",
        "context": [
            "\n",
            "class ToTensorGjz(object):\n",
            "    def __call__(self, pic):\n",
            "        if isinstance(pic, np.ndarray):\n",
            "            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n",
            "            return img.float()\n",
            "\n",
            "    def __repr__(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\ddfa.py",
        "line_number": 116,
        "API": ".strip(",
        "context": [
            "class DDFADataset(data.Dataset):\n",
            "    def __init__(self, root, filelists, param_fp, transform=None, **kargs):\n",
            "        self.root = root\n",
            "        self.transform = transform\n",
            "        self.lines = Path(filelists).read_text().strip().split('\\n')\n",
            "        self.params = _numpy_to_tensor(_load_cpu(param_fp))\n",
            "        self.img_loader = img_loader\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\ddfa.py",
        "line_number": 126,
        "API": ".join(",
        "context": [
            "\n",
            "        return target\n",
            "\n",
            "    def __getitem__(self, index):\n",
            "        path = osp.join(self.root, self.lines[index])\n",
            "        img = self.img_loader(path)\n",
            "\n",
            "        target = self._target_loader(index)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\ddfa.py",
        "line_number": 143,
        "API": ".strip(",
        "context": [
            "class DDFATestDataset(data.Dataset):\n",
            "    def __init__(self, filelists, root='', transform=None):\n",
            "        self.root = root\n",
            "        self.transform = transform\n",
            "        self.lines = Path(filelists).read_text().strip().split('\\n')\n",
            "\n",
            "    def __getitem__(self, index):\n",
            "        path = osp.join(self.root, self.lines[index])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\estimate_pose.py",
        "line_number": 14,
        "API": ".reshape(",
        "context": [
            "\n",
            "\n",
            "def parse_pose(param):\n",
            "    param = param * param_std + param_mean\n",
            "    Ps = param[:12].reshape(3, -1)  # camera matrix\n",
            "    # R = P[:, :3]\n",
            "    s, R, t3d = P2sRt(Ps)\n",
            "    P = np.concatenate((R, t3d.reshape(3, -1)), axis=1)  # without scale\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\estimate_pose.py",
        "line_number": 20,
        "API": ".reshape(",
        "context": [
            "    s, R, t3d = P2sRt(Ps)\n",
            "    P = np.concatenate((R, t3d.reshape(3, -1)), axis=1)  # without scale\n",
            "    # P = Ps / s\n",
            "    pose = matrix2angle(R)  # yaw, pitch, roll\n",
            "    # offset = p_[:, -1].reshape(3, 1)\n",
            "    return P, pose\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\estimate_pose.py",
        "line_number": 64,
        "API": ".norm(",
        "context": [
            "    '''\n",
            "    t3d = P[:, 3]\n",
            "    R1 = P[0:1, :3]\n",
            "    R2 = P[1:2, :3]\n",
            "    s = (np.linalg.norm(R1) + np.linalg.norm(R2)) / 2.0\n",
            "    r1 = R1 / np.linalg.norm(R1)\n",
            "    r2 = R2 / np.linalg.norm(R2)\n",
            "    r3 = np.cross(r1, r2)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\inference.py",
        "line_number": 25,
        "API": ".zeros(",
        "context": [
            "\n",
            "    sx, sy, ex, ey = [int(round(_)) for _ in roi_box]\n",
            "    dh, dw = ey - sy, ex - sx\n",
            "    if len(img.shape) == 3:\n",
            "        res = np.zeros((dh, dw, 3), dtype=np.uint8)\n",
            "    else:\n",
            "        res = np.zeros((dh, dw), dtype=np.uint8)\n",
            "    if sx < 0:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\inference.py",
        "line_number": 108,
        "API": ".format(",
        "context": [
            "    end_header\"\"\"\n",
            "\n",
            "    n_vertex = vertex.shape[1]\n",
            "    n_face = tri.shape[1]\n",
            "    header = header.format(n_vertex, n_face)\n",
            "\n",
            "    with open(wfp, 'w') as f:\n",
            "        f.write(header + '\\n')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\inference.py",
        "line_number": 114,
        "API": ".write(",
        "context": [
            "    with open(wfp, 'w') as f:\n",
            "        f.write(header + '\\n')\n",
            "        for i in range(n_vertex):\n",
            "            x, y, z = vertex[:, i]\n",
            "            f.write('{:.4f} {:.4f} {:.4f}\\n'.format(x, y, z))\n",
            "        for i in range(n_face):\n",
            "            idx1, idx2, idx3 = tri[:, i]\n",
            "            f.write('3 {} {} {}\\n'.format(idx1 - 1, idx2 - 1, idx3 - 1))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\inference.py",
        "line_number": 123,
        "API": ".format(",
        "context": [
            "\n",
            "\n",
            "def dump_vertex(vertex, wfp):\n",
            "    sio.savemat(wfp, {'vertex': vertex})\n",
            "    print('Dump to {}'.format(wfp))\n",
            "\n",
            "\n",
            "def _predict_vertices(param, roi_bbox, dense, transform=True):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\inference.py",
        "line_number": 166,
        "API": ".get(",
        "context": [
            "        elif style == 'fancy':\n",
            "            alpha = 0.8\n",
            "            markersize = 4\n",
            "            lw = 1.5\n",
            "            color = kwargs.get('color', 'w')\n",
            "            markeredgecolor = kwargs.get('markeredgecolor', 'black')\n",
            "\n",
            "            nums = [0, 17, 22, 27, 31, 36, 42, 48, 60, 68]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\inference.py",
        "line_number": 189,
        "API": ".format(",
        "context": [
            "                         markeredgecolor=markeredgecolor, alpha=alpha)\n",
            "\n",
            "    if wfp is not None:\n",
            "        plt.savefig(wfp, dpi=200)\n",
            "        print('Save visualization result to {}'.format(wfp))\n",
            "    if show_flg:\n",
            "        plt.show()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\inference.py",
        "line_number": 196,
        "API": ".maximum(",
        "context": [
            "\n",
            "\n",
            "def get_colors(image, vertices):\n",
            "    [h, w, _] = image.shape\n",
            "    vertices[0, :] = np.minimum(np.maximum(vertices[0, :], 0), w - 1)  # x\n",
            "    vertices[1, :] = np.minimum(np.maximum(vertices[1, :], 0), h - 1)  # y\n",
            "    ind = np.round(vertices).astype(np.int32)\n",
            "    colors = image[ind[1, :], ind[0, :], :]  # n x 3\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\inference.py",
        "line_number": 205,
        "API": ".copy(",
        "context": [
            "    return colors\n",
            "\n",
            "\n",
            "def write_obj_with_colors(obj_name, vertices, triangles, colors):\n",
            "    triangles = triangles.copy() # meshlab start with 1\n",
            "\n",
            "    if obj_name.split('.')[-1] != 'obj':\n",
            "        obj_name = obj_name + '.obj'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\inference.py",
        "line_number": 214,
        "API": ".format(",
        "context": [
            "    # write obj\n",
            "    with open(obj_name, 'w') as f:\n",
            "        # write vertices & colors\n",
            "        for i in range(vertices.shape[1]):\n",
            "            s = 'v {:.4f} {:.4f} {:.4f} {} {} {}\\n'.format(vertices[1, i], vertices[0, i], vertices[2, i], colors[i, 2],\n",
            "                                               colors[i, 1], colors[i, 0])\n",
            "            f.write(s)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\inference.py",
        "line_number": 220,
        "API": ".format(",
        "context": [
            "            f.write(s)\n",
            "\n",
            "        # write f: ver ind/ uv ind\n",
            "        for i in range(triangles.shape[1]):\n",
            "            s = 'f {} {} {}\\n'.format(triangles[0, i], triangles[1, i], triangles[2, i])\n",
            "            f.write(s)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\io.py",
        "line_number": 13,
        "API": ".system(",
        "context": [
            "\n",
            "def mkdir(d):\n",
            "    \"\"\"only works on *nix system\"\"\"\n",
            "    if not os.path.isdir(d) and not os.path.exists(d):\n",
            "        os.system('mkdir -p {}'.format(d))\n",
            "\n",
            "\n",
            "def _get_suffix(filename):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\io.py",
        "line_number": 27,
        "API": ".load(",
        "context": [
            "\n",
            "def _load(fp):\n",
            "    suffix = _get_suffix(fp)\n",
            "    if suffix == 'npy':\n",
            "        return np.load(fp)\n",
            "    elif suffix == 'pkl':\n",
            "        return pickle.load(open(fp, 'rb'))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\io.py",
        "line_number": 35,
        "API": ".save(",
        "context": [
            "\n",
            "def _dump(wfp, obj):\n",
            "    suffix = _get_suffix(wfp)\n",
            "    if suffix == 'npy':\n",
            "        np.save(wfp, obj)\n",
            "    elif suffix == 'pkl':\n",
            "        pickle.dump(obj, open(wfp, 'wb'))\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\io.py",
        "line_number": 43,
        "API": ".lower(",
        "context": [
            "        raise Exception('Unknown Type: {}'.format(suffix))\n",
            "\n",
            "\n",
            "def _load_tensor(fp, mode='cpu'):\n",
            "    if mode.lower() == 'cpu':\n",
            "        return torch.from_numpy(_load(fp))\n",
            "    elif mode.lower() == 'gpu':\n",
            "        return torch.from_numpy(_load(fp)).cuda()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\io.py",
        "line_number": 83,
        "API": ".flatten(",
        "context": [
            "        # flatten it, pay attention to index value\n",
            "        keypoints = model['keypoints'].astype(np.int32) - 1\n",
            "        keypoints = np.concatenate((3 * keypoints, 3 * keypoints + 1, 3 * keypoints + 2), axis=0)\n",
            "\n",
            "        model_new['keypoints'] = keypoints.T.flatten()\n",
            "\n",
            "        #\n",
            "        w = np.concatenate((w_shp, w_exp), axis=1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\io.py",
        "line_number": 88,
        "API": ".norm(",
        "context": [
            "\n",
            "        #\n",
            "        w = np.concatenate((w_shp, w_exp), axis=1)\n",
            "        w_base = w[keypoints]\n",
            "        w_norm = np.linalg.norm(w, axis=0)\n",
            "        w_base_norm = np.linalg.norm(w_base, axis=0)\n",
            "\n",
            "        dim = w_shp.shape[0] // 3\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\lighting.py",
        "line_number": 10,
        "API": ".sqrt(",
        "context": [
            "import numpy as np\n",
            "from utils import render\n",
            "from utils.cython import mesh_core_cython\n",
            "\n",
            "_norm = lambda arr: arr / np.sqrt(np.sum(arr ** 2, axis=1))[:, None]\n",
            "\n",
            "\n",
            "def norm_vertices(vertices):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\lighting.py",
        "line_number": 15,
        "API": ".max(",
        "context": [
            "\n",
            "\n",
            "def norm_vertices(vertices):\n",
            "    vertices -= vertices.min(0)[None, :]\n",
            "    vertices /= vertices.max()\n",
            "    vertices *= 2\n",
            "    vertices -= vertices.max(0)[None, :] / 2\n",
            "    return vertices\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\lighting.py",
        "line_number": 23,
        "API": ".array(",
        "context": [
            "\n",
            "\n",
            "def convert_type(obj):\n",
            "    if isinstance(obj, tuple) or isinstance(obj, list):\n",
            "        return np.array(obj, dtype=np.float32)[None, :]\n",
            "    return obj\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\lighting.py",
        "line_number": 29,
        "API": ".get(",
        "context": [
            "\n",
            "\n",
            "class RenderPipeline(object):\n",
            "    def __init__(self, **kwargs):\n",
            "        self.intensity_ambient = convert_type(kwargs.get('intensity_ambient', 0.3))\n",
            "        self.intensity_directional = convert_type(kwargs.get('intensity_directional', 0.6))\n",
            "        self.intensity_specular = convert_type(kwargs.get('intensity_specular', 0.9))\n",
            "        self.specular_exp = kwargs.get('specular_exp', 5)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\lighting.py",
        "line_number": 34,
        "API": ".get(",
        "context": [
            "        self.intensity_directional = convert_type(kwargs.get('intensity_directional', 0.6))\n",
            "        self.intensity_specular = convert_type(kwargs.get('intensity_specular', 0.9))\n",
            "        self.specular_exp = kwargs.get('specular_exp', 5)\n",
            "        self.color_ambient = convert_type(kwargs.get('color_ambient', (1, 1, 1)))\n",
            "        self.color_directional = convert_type(kwargs.get('color_directional', (1, 1, 1)))\n",
            "        self.light_pos = convert_type(kwargs.get('light_pos', (0, 0, 1)))\n",
            "        self.view_pos = convert_type(kwargs.get('view_pos', (0, 0, 1)))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\lighting.py",
        "line_number": 46,
        "API": ".zeros(",
        "context": [
            "        height, width = background.shape[:2]\n",
            "\n",
            "        # 1. compute triangle/face normals and vertex normals\n",
            "        # ## Old style: very slow\n",
            "        # normal = np.zeros((vertices.shape[0], 3), dtype=np.float32)\n",
            "        # # surface_count = np.zeros((vertices.shape[0], 1))\n",
            "        # for i in range(triangles.shape[0]):\n",
            "        #     i1, i2, i3 = triangles[i, :]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\lighting.py",
        "line_number": 51,
        "API": ".cross(",
        "context": [
            "        # # surface_count = np.zeros((vertices.shape[0], 1))\n",
            "        # for i in range(triangles.shape[0]):\n",
            "        #     i1, i2, i3 = triangles[i, :]\n",
            "        #     v1, v2, v3 = vertices[[i1, i2, i3], :]\n",
            "        #     surface_normal = np.cross(v2 - v1, v3 - v1)\n",
            "        #     normal[[i1, i2, i3], :] += surface_normal\n",
            "        #     # surface_count[[i1, i2, i3], :] += 1\n",
            "        #\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\lighting.py",
        "line_number": 56,
        "API": ".norm(",
        "context": [
            "        #     normal[[i1, i2, i3], :] += surface_normal\n",
            "        #     # surface_count[[i1, i2, i3], :] += 1\n",
            "        #\n",
            "        # # normal /= surface_count\n",
            "        # # normal /= np.linalg.norm(normal, axis=1, keepdims=True)\n",
            "        # normal = _norm(normal)\n",
            "\n",
            "        # Cython style\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\lighting.py",
        "line_number": 64,
        "API": ".zeros_like(",
        "context": [
            "        normal = np.zeros((vertices.shape[0], 3), dtype=np.float32)\n",
            "        mesh_core_cython.get_normal(normal, vertices, triangles, vertices.shape[0], triangles.shape[0])\n",
            "\n",
            "        # 2. lighting\n",
            "        color = np.zeros_like(vertices, dtype=np.float32)\n",
            "        # ambient component\n",
            "        if self.intensity_ambient > 0:\n",
            "            color += self.intensity_ambient * self.color_ambient\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\lighting.py",
        "line_number": 69,
        "API": ".copy(",
        "context": [
            "        # ambient component\n",
            "        if self.intensity_ambient > 0:\n",
            "            color += self.intensity_ambient * self.color_ambient\n",
            "\n",
            "        vertices_n = norm_vertices(vertices.copy())\n",
            "        if self.intensity_directional > 0:\n",
            "            # diffuse component\n",
            "            direction = _norm(self.light_pos - vertices_n)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\lighting.py",
        "line_number": 82,
        "API": ".sum(",
        "context": [
            "            # specular component\n",
            "            if self.intensity_specular > 0:\n",
            "                v2v = _norm(self.view_pos - vertices_n)\n",
            "                reflection = 2 * cos * normal - direction\n",
            "                spe = np.sum((v2v * reflection) ** self.specular_exp, axis=1)[:, None]\n",
            "                spe = np.where(cos != 0, np.clip(spe, 0, 1), np.zeros_like(spe))\n",
            "                color += self.intensity_specular * self.color_directional * np.clip(spe, 0, 1)\n",
            "        color = np.clip(color, 0, 1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\paf.py",
        "line_number": 12,
        "API": ".reshape(",
        "context": [
            "def reconstruct_paf_anchor(param, whitening=True):\n",
            "    if whitening:\n",
            "        param = param * param_std + param_mean\n",
            "    p, offset, alpha_shp, alpha_exp = _parse_param(param)\n",
            "    anchor = p @ (u_filter + w_filter @ alpha_shp + w_exp_filter @ alpha_exp).reshape(3, -1, order='F') + offset\n",
            "    anchor[1, :] = std_size + 1 - anchor[1, :]\n",
            "    return anchor[:2, :]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\paf.py",
        "line_number": 18,
        "API": ".zeros(",
        "context": [
            "    return anchor[:2, :]\n",
            "\n",
            "\n",
            "def gen_offsets(kernel_size):\n",
            "    offsets = np.zeros((2, kernel_size * kernel_size), dtype=np.int)\n",
            "    ind = 0\n",
            "    delta = (kernel_size - 1) // 2\n",
            "    for i in range(kernel_size):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\paf.py",
        "line_number": 37,
        "API": ".round(",
        "context": [
            "    img_crop: 120x120\n",
            "    kernel_size: kernel_size for convolution, should be even number like 3 or 5 or ...\n",
            "    \"\"\"\n",
            "    anchor = reconstruct_paf_anchor(param)\n",
            "    anchor = np.round(anchor).astype(np.int)\n",
            "    delta = (kernel_size - 1) // 2\n",
            "    anchor[anchor < delta] = delta\n",
            "    anchor[anchor >= std_size - delta - 1] = std_size - delta - 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\paf.py",
        "line_number": 42,
        "API": ".zeros(",
        "context": [
            "    delta = (kernel_size - 1) // 2\n",
            "    anchor[anchor < delta] = delta\n",
            "    anchor[anchor >= std_size - delta - 1] = std_size - delta - 1\n",
            "\n",
            "    img_paf = np.zeros((64 * kernel_size, 64 * kernel_size, 3), dtype=np.uint8)\n",
            "    offsets = gen_offsets(kernel_size)\n",
            "    for i in range(kernel_size * kernel_size):\n",
            "        ox, oy = offsets[:, i]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\paf.py",
        "line_number": 48,
        "API": ".reshape(",
        "context": [
            "    for i in range(kernel_size * kernel_size):\n",
            "        ox, oy = offsets[:, i]\n",
            "        index0 = anchor[0] + ox\n",
            "        index1 = anchor[1] + oy\n",
            "        p = img_crop[index1, index0].reshape(64, 64, 3).transpose(1, 0, 2)\n",
            "\n",
            "        img_paf[oy + delta::kernel_size, ox + delta::kernel_size] = p\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\params.py",
        "line_number": 9,
        "API": ".join(",
        "context": [
            "from .io import _load\n",
            "\n",
            "\n",
            "def make_abs_path(d):\n",
            "    return osp.join(osp.dirname(osp.realpath(__file__)), d)\n",
            "\n",
            "\n",
            "d = make_abs_path('../train.configs')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\params.py",
        "line_number": 14,
        "API": ".join(",
        "context": [
            "\n",
            "\n",
            "d = make_abs_path('../train.configs')\n",
            "keypoints = _load(osp.join(d, 'keypoints_sim.npy'))\n",
            "w_shp = _load(osp.join(d, 'w_shp_sim.npy'))\n",
            "w_exp = _load(osp.join(d, 'w_exp_sim.npy'))  # simplified version\n",
            "meta = _load(osp.join(d, 'param_whitening.pkl'))\n",
            "# param_mean and param_std are used for re-whitening\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\params.py",
        "line_number": 19,
        "API": ".get(",
        "context": [
            "w_exp = _load(osp.join(d, 'w_exp_sim.npy'))  # simplified version\n",
            "meta = _load(osp.join(d, 'param_whitening.pkl'))\n",
            "# param_mean and param_std are used for re-whitening\n",
            "param_mean = meta.get('param_mean')\n",
            "param_std = meta.get('param_std')\n",
            "u_shp = _load(osp.join(d, 'u_shp.npy'))\n",
            "u_exp = _load(osp.join(d, 'u_exp.npy'))\n",
            "u = u_shp + u_exp\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\params.py",
        "line_number": 25,
        "API": ".norm(",
        "context": [
            "u_exp = _load(osp.join(d, 'u_exp.npy'))\n",
            "u = u_shp + u_exp\n",
            "w = np.concatenate((w_shp, w_exp), axis=1)\n",
            "w_base = w[keypoints]\n",
            "w_norm = np.linalg.norm(w, axis=0)\n",
            "w_base_norm = np.linalg.norm(w_base, axis=0)\n",
            "\n",
            "# for inference\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\params.py",
        "line_number": 30,
        "API": ".reshape(",
        "context": [
            "w_base_norm = np.linalg.norm(w_base, axis=0)\n",
            "\n",
            "# for inference\n",
            "dim = w_shp.shape[0] // 3\n",
            "u_base = u[keypoints].reshape(-1, 1)\n",
            "w_shp_base = w_shp[keypoints]\n",
            "w_exp_base = w_exp[keypoints]\n",
            "std_size = 120\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\params.py",
        "line_number": 36,
        "API": ".join(",
        "context": [
            "w_exp_base = w_exp[keypoints]\n",
            "std_size = 120\n",
            "\n",
            "# for paf (pac)\n",
            "paf = _load(osp.join(d, 'Model_PAF.pkl'))\n",
            "u_filter = paf.get('mu_filter')\n",
            "w_filter = paf.get('w_filter')\n",
            "w_exp_filter = paf.get('w_exp_filter')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\params.py",
        "line_number": 42,
        "API": ".join(",
        "context": [
            "w_filter = paf.get('w_filter')\n",
            "w_exp_filter = paf.get('w_exp_filter')\n",
            "\n",
            "# pncc code (mean shape)\n",
            "pncc_code = _load(osp.join(d, 'pncc_code.npy'))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\render.py",
        "line_number": 62,
        "API": ".zeros(",
        "context": [
            "        h: height\n",
            "        w: width\n",
            "    \"\"\"\n",
            "    # initial\n",
            "    image = np.zeros((h, w, c))\n",
            "\n",
            "    depth_buffer = np.zeros([h, w]) - 999999.\n",
            "    # triangle depth: approximate the depth to the average value of z in each vertex(v0, v1, v2), since the vertices are closed to each other\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\render.py",
        "line_number": 73,
        "API": ".min(",
        "context": [
            "    for i in range(tri.shape[1]):\n",
            "        tri_idx = tri[:, i]  # 3 vertex indices\n",
            "\n",
            "        # the inner bounding box\n",
            "        umin = max(int(np.ceil(np.min(vertices[0, tri_idx]))), 0)\n",
            "        umax = min(int(np.floor(np.max(vertices[0, tri_idx]))), w - 1)\n",
            "\n",
            "        vmin = max(int(np.ceil(np.min(vertices[1, tri_idx]))), 0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\render.py",
        "line_number": 94,
        "API": ".zeros(",
        "context": [
            "def get_depths_image(img, vertices_lst, tri):\n",
            "    h, w = img.shape[:2]\n",
            "    c = 1\n",
            "\n",
            "    depths_img = np.zeros((h, w, c))\n",
            "    for i in range(len(vertices_lst)):\n",
            "        vertices = vertices_lst[i]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\render.py",
        "line_number": 106,
        "API": ".squeeze(",
        "context": [
            "        z = vertices[2:, :]\n",
            "        depth_img = render_colors(vertices.T, z.T, tri.T, h, w, 1)\n",
            "        depths_img[depth_img > 0] = depth_img[depth_img > 0]\n",
            "\n",
            "    depths_img = depths_img.squeeze() * 255\n",
            "    return depths_img\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\render.py",
        "line_number": 125,
        "API": ".zeros(",
        "context": [
            "        image: [h, w, c]. rendered image./rendering.\n",
            "    \"\"\"\n",
            "\n",
            "    if BG is None:\n",
            "        image = np.zeros((h, w, c), dtype=np.float32)\n",
            "    else:\n",
            "        assert BG.shape[0] == h and BG.shape[1] == w and BG.shape[2] == c\n",
            "        image = BG.astype(np.float32).copy(order='C')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\render.py",
        "line_number": 132,
        "API": ".copy(",
        "context": [
            "        image = BG.astype(np.float32).copy(order='C')\n",
            "    depth_buffer = np.zeros([h, w], dtype=np.float32, order='C') - 999999.\n",
            "\n",
            "    # to C order\n",
            "    vertices = vertices.astype(np.float32).copy(order='C')\n",
            "    triangles = triangles.astype(np.int32).copy(order='C')\n",
            "    colors = colors.astype(np.float32).copy(order='C')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\render.py",
        "line_number": 151,
        "API": ".zeros(",
        "context": [
            "    \"\"\"cython version for depth image render\"\"\"\n",
            "    h, w = img.shape[:2]\n",
            "    c = 1\n",
            "\n",
            "    depths_img = np.zeros((h, w, c))\n",
            "    for i in range(len(vertices_lst)):\n",
            "        vertices = vertices_lst[i]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\render.py",
        "line_number": 163,
        "API": ".squeeze(",
        "context": [
            "\n",
            "        depth_img = crender_colors(vertices.T, tri.T, z.T, h, w, 1)\n",
            "        depths_img[depth_img > 0] = depth_img[depth_img > 0]\n",
            "\n",
            "    depths_img = depths_img.squeeze() * 255\n",
            "    return depths_img\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\render.py",
        "line_number": 169,
        "API": ".zeros_like(",
        "context": [
            "\n",
            "\n",
            "def ncc(vertices):\n",
            "    ## simple version\n",
            "    # ncc_vertices = np.zeros_like(vertices)\n",
            "    # x = vertices[0, :]\n",
            "    # y = vertices[1, :]\n",
            "    # z = vertices[2, :]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\render.py",
        "line_number": 179,
        "API": ".min(",
        "context": [
            "    # ncc_vertices[1, :] = (y - min(y)) / (max(y) - min(y))\n",
            "    # ncc_vertices[2, :] = (z - min(z)) / (max(z) - min(z))\n",
            "\n",
            "    # matrix version\n",
            "    v_min = np.min(vertices, axis=1).reshape(-1, 1)\n",
            "    v_max = np.max(vertices, axis=1).reshape(-1, 1)\n",
            "    ncc_vertices = (vertices - v_min) / (v_max - v_min)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\render.py",
        "line_number": 191,
        "API": ".zeros(",
        "context": [
            "    \"\"\"cython version for PNCC render: original paper\"\"\"\n",
            "    h, w = img.shape[:2]\n",
            "    c = 3\n",
            "\n",
            "    pnccs_img = np.zeros((h, w, c))\n",
            "    for i in range(len(vertices_lst)):\n",
            "        vertices = vertices_lst[i]\n",
            "        pncc_img = crender_colors(vertices.T, tri.T, pncc_code.T, h, w, c)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\render.py",
        "line_number": 197,
        "API": ".squeeze(",
        "context": [
            "        vertices = vertices_lst[i]\n",
            "        pncc_img = crender_colors(vertices.T, tri.T, pncc_code.T, h, w, c)\n",
            "        pnccs_img[pncc_img > 0] = pncc_img[pncc_img > 0]\n",
            "\n",
            "    pnccs_img = pnccs_img.squeeze() * 255\n",
            "    return pnccs_img\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\render.py",
        "line_number": 206,
        "API": ".zeros(",
        "context": [
            "    \"\"\"cython version for PNCC render\"\"\"\n",
            "    h, w = img.shape[:2]\n",
            "    c = 3\n",
            "\n",
            "    pnccs_img = np.zeros((h, w, c))\n",
            "    for i in range(len(vertices_lst)):\n",
            "        vertices = vertices_lst[i]\n",
            "        ncc_vertices = ncc(vertices)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\3DDFA\\utils\\render.py",
        "line_number": 213,
        "API": ".squeeze(",
        "context": [
            "        ncc_vertices = ncc(vertices)\n",
            "        pncc_img = crender_colors(vertices.T, tri.T, ncc_vertices.T, h, w, c)\n",
            "        pnccs_img[pncc_img > 0] = pncc_img[pncc_img > 0]\n",
            "\n",
            "    pnccs_img = pnccs_img.squeeze() * 255\n",
            "    return pnccs_img\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\benchmarks\\big_model_inference.py",
        "line_number": 62,
        "API": ".get(",
        "context": [
            "    if args.model_name in DEFAULT_MODELS:\n",
            "        defaults = DEFAULT_MODELS[args.model_name]\n",
            "        args.model_name = defaults[\"model\"]\n",
            "        if args.tokenizer_name is None:\n",
            "            args.tokenizer_name = defaults.get(\"tokenizer\", args.model_name)\n",
            "        if args.is_causal is None:\n",
            "            args.is_causal = defaults[\"is_causal\"]\n",
            "        if args.model_revision is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\benchmarks\\big_model_inference.py",
        "line_number": 105,
        "API": ".join(",
        "context": [
            "    module_sizes = compute_module_sizes(model)\n",
            "    device_size = {v: 0 for v in model.hf_device_map.values()}\n",
            "    for module, device in model.hf_device_map.items():\n",
            "        device_size[device] += module_sizes[module]\n",
            "    message = \"\\n\".join([f\"- {device}: {size // 2**20}MiB\" for device, size in device_size.items()])\n",
            "    print(f\"\\nTheoretical use:\\n{message}\")\n",
            "\n",
            "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\benchmarks\\big_model_inference.py",
        "line_number": 115,
        "API": ".to(",
        "context": [
            "    generation_times = []\n",
            "    gen_tokens = []\n",
            "    texts_outs = []\n",
            "    for prompt in PROMPTS:\n",
            "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\n",
            "        tokens = inputs[\"input_ids\"][0].tolist()\n",
            "        before_generate = time.time()\n",
            "        outputs = model.generate(inputs[\"input_ids\"])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\benchmarks\\measures_util.py",
        "line_number": 31,
        "API": ".join(",
        "context": [
            "        self.thread.start()\n",
            "\n",
            "    def stop(self):\n",
            "        self.peak_monitoring = False\n",
            "        self.thread.join()\n",
            "        return self.cpu_memory_peak\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\benchmarks\\measures_util.py",
        "line_number": 43,
        "API": ".empty_cache(",
        "context": [
            "    # Time\n",
            "    measures = {\"time\": time.time()}\n",
            "\n",
            "    gc.collect()\n",
            "    torch.cuda.empty_cache()\n",
            "\n",
            "    # CPU mem\n",
            "    measures[\"cpu\"] = psutil.Process().memory_info().rss\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\benchmarks\\measures_util.py",
        "line_number": 62,
        "API": ".empty_cache(",
        "context": [
            "    # Time\n",
            "    measures = {\"time\": time.time() - start_measures[\"time\"]}\n",
            "\n",
            "    gc.collect()\n",
            "    torch.cuda.empty_cache()\n",
            "\n",
            "    # CPU mem\n",
            "    measures[\"cpu\"] = (psutil.Process().memory_info().rss - start_measures[\"cpu\"]) / 2**20\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\benchmarks\\measures_util.py",
        "line_number": 71,
        "API": ".max_memory_allocated(",
        "context": [
            "\n",
            "    # GPU mem\n",
            "    for i in range(torch.cuda.device_count()):\n",
            "        measures[str(i)] = (torch.cuda.memory_allocated(i) - start_measures[str(i)]) / 2**20\n",
            "        measures[f\"{i}-peak\"] = (torch.cuda.max_memory_allocated(i) - start_measures[str(i)]) / 2**20\n",
            "\n",
            "    return measures\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_cv_example.py",
        "line_number": 48,
        "API": ".split(",
        "context": [
            "\n",
            "\n",
            "# Function to get the label from the filename\n",
            "def extract_label(fname):\n",
            "    stem = fname.split(os.path.sep)[-1]\n",
            "    return re.search(r\"^(.*)_\\d+\\.jpg$\", stem).groups()[0]\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_cv_example.py",
        "line_number": 64,
        "API": ".convert(",
        "context": [
            "\n",
            "    def __getitem__(self, idx):\n",
            "        fname = self.file_names[idx]\n",
            "        raw_image = PIL.Image.open(fname)\n",
            "        image = raw_image.convert(\"RGB\")\n",
            "        if self.image_transform is not None:\n",
            "            image = self.image_transform(image)\n",
            "        label = extract_label(fname)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_cv_example.py",
        "line_number": 106,
        "API": ".split(",
        "context": [
            "        checkpointing_steps = None\n",
            "\n",
            "    # We need to initialize the trackers we use, and also store our configuration\n",
            "    if args.with_tracking:\n",
            "        run = os.path.split(__file__)[-1].split(\".\")[0]\n",
            "        accelerator.init_trackers(run, config)\n",
            "\n",
            "    # Grab all the image filenames\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_cv_example.py",
        "line_number": 115,
        "API": ".sort(",
        "context": [
            "\n",
            "    # Build the label correspondences\n",
            "    all_labels = [extract_label(fname) for fname in file_names]\n",
            "    id_to_label = list(set(all_labels))\n",
            "    id_to_label.sort()\n",
            "    label_to_id = {lbl: i for i, lbl in enumerate(id_to_label)}\n",
            "\n",
            "    # Set the seed before splitting the data.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_cv_example.py",
        "line_number": 120,
        "API": ".manual_seed(",
        "context": [
            "    label_to_id = {lbl: i for i, lbl in enumerate(id_to_label)}\n",
            "\n",
            "    # Set the seed before splitting the data.\n",
            "    np.random.seed(seed)\n",
            "    torch.manual_seed(seed)\n",
            "    torch.cuda.manual_seed_all(seed)\n",
            "\n",
            "    # Split our filenames between train and validation\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_cv_example.py",
        "line_number": 149,
        "API": ".to(",
        "context": [
            "\n",
            "    # We could avoid this line since the accelerator is set with `device_placement=True` (default value).\n",
            "    # Note that if you are placing tensors on devices manually, this line absolutely needs to be before the optimizer\n",
            "    # creation otherwise training will not work on TPU (`accelerate` will kindly throw an error to make us aware of that).\n",
            "    model = model.to(accelerator.device)\n",
            "\n",
            "    # Freezing the base model\n",
            "    for param in model.parameters():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_cv_example.py",
        "line_number": 158,
        "API": ".to(",
        "context": [
            "    for param in model.get_classifier().parameters():\n",
            "        param.requires_grad = True\n",
            "\n",
            "    # We normalize the batches of images to be a bit faster.\n",
            "    mean = torch.tensor(model.default_cfg[\"mean\"])[None, :, None, None].to(accelerator.device)\n",
            "    std = torch.tensor(model.default_cfg[\"std\"])[None, :, None, None].to(accelerator.device)\n",
            "\n",
            "    # Instantiate optimizer\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_cv_example.py",
        "line_number": 187,
        "API": ".sort(",
        "context": [
            "            path = os.path.basename(args.resume_from_checkpoint)\n",
            "        else:\n",
            "            # Get the most recent checkpoint\n",
            "            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n",
            "            dirs.sort(key=os.path.getctime)\n",
            "            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n",
            "        # Extract `epoch_{i}` or `step_{i}`\n",
            "        training_difference = os.path.splitext(path)[0]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_cv_example.py",
        "line_number": 214,
        "API": ".to(",
        "context": [
            "            # After the first iteration though, we need to go back to the original dataloader\n",
            "            active_dataloader = train_dataloader\n",
            "        for batch in active_dataloader:\n",
            "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
            "            batch = {k: v.to(accelerator.device) for k, v in batch.items()}\n",
            "            inputs = (batch[\"image\"] - mean) / std\n",
            "            outputs = model(inputs)\n",
            "            loss = torch.nn.functional.cross_entropy(outputs, batch[\"label\"])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_cv_example.py",
        "line_number": 221,
        "API": ".backward(",
        "context": [
            "            loss = torch.nn.functional.cross_entropy(outputs, batch[\"label\"])\n",
            "            # We keep track of the loss at each epoch\n",
            "            if args.with_tracking:\n",
            "                total_loss += loss.detach().float()\n",
            "            accelerator.backward(loss)\n",
            "            optimizer.step()\n",
            "            lr_scheduler.step()\n",
            "            optimizer.zero_grad()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_cv_example.py",
        "line_number": 230,
        "API": ".join(",
        "context": [
            "            if isinstance(checkpointing_steps, int):\n",
            "                output_dir = f\"step_{overall_step}\"\n",
            "                if overall_step % checkpointing_steps == 0:\n",
            "                    if args.output_dir is not None:\n",
            "                        output_dir = os.path.join(args.output_dir, output_dir)\n",
            "                    accelerator.save_state(output_dir)\n",
            "        model.eval()\n",
            "        accurate = 0\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_cv_example.py",
        "line_number": 237,
        "API": ".to(",
        "context": [
            "        accurate = 0\n",
            "        num_elems = 0\n",
            "        for step, batch in enumerate(eval_dataloader):\n",
            "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
            "            batch = {k: v.to(accelerator.device) for k, v in batch.items()}\n",
            "            inputs = (batch[\"image\"] - mean) / std\n",
            "            with torch.no_grad():\n",
            "                outputs = model(inputs)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_cv_example.py",
        "line_number": 245,
        "API": ".sum(",
        "context": [
            "            predictions = outputs.argmax(dim=-1)\n",
            "            predictions, references = accelerator.gather_for_metrics((predictions, batch[\"label\"]))\n",
            "            accurate_preds = predictions == references\n",
            "            num_elems += accurate_preds.shape[0]\n",
            "            accurate += accurate_preds.long().sum()\n",
            "\n",
            "        eval_metric = accurate.item() / num_elems\n",
            "        # Use accelerator.print to print only on the main process.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_cv_example.py",
        "line_number": 251,
        "API": ".log(",
        "context": [
            "        eval_metric = accurate.item() / num_elems\n",
            "        # Use accelerator.print to print only on the main process.\n",
            "        accelerator.print(f\"epoch {epoch}: {100 * eval_metric:.2f}\")\n",
            "        if args.with_tracking:\n",
            "            accelerator.log(\n",
            "                {\n",
            "                    \"accuracy\": 100 * eval_metric,\n",
            "                    \"train_loss\": total_loss.item() / len(train_dataloader),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_cv_example.py",
        "line_number": 262,
        "API": ".join(",
        "context": [
            "            )\n",
            "        if checkpointing_steps == \"epoch\":\n",
            "            output_dir = f\"epoch_{epoch}\"\n",
            "            if args.output_dir is not None:\n",
            "                output_dir = os.path.join(args.output_dir, output_dir)\n",
            "            accelerator.save_state(output_dir)\n",
            "\n",
            "    if args.with_tracking:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_nlp_example.py",
        "line_number": 78,
        "API": ".split(",
        "context": [
            "    batch_size = int(config[\"batch_size\"])\n",
            "\n",
            "    # We need to initialize the trackers we use, and also store our configuration\n",
            "    if args.with_tracking:\n",
            "        run = os.path.split(__file__)[-1].split(\".\")[0]\n",
            "        accelerator.init_trackers(run, config)\n",
            "\n",
            "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_nlp_example.py",
        "line_number": 83,
        "API": ".load(",
        "context": [
            "        accelerator.init_trackers(run, config)\n",
            "\n",
            "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
            "    datasets = load_dataset(\"glue\", \"mrpc\")\n",
            "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
            "\n",
            "    def tokenize_function(examples):\n",
            "        # max_length=None => use the model max length (it's actually the default)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_nlp_example.py",
        "line_number": 120,
        "API": ".pad(",
        "context": [
            "            pad_to_multiple_of = 8\n",
            "        else:\n",
            "            pad_to_multiple_of = None\n",
            "\n",
            "        return tokenizer.pad(\n",
            "            examples,\n",
            "            padding=\"longest\",\n",
            "            max_length=max_length,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_nlp_example.py",
        "line_number": 144,
        "API": ".to(",
        "context": [
            "\n",
            "    # We could avoid this line since the accelerator is set with `device_placement=True` (default value).\n",
            "    # Note that if you are placing tensors on devices manually, this line absolutely needs to be before the optimizer\n",
            "    # creation otherwise training will not work on TPU (`accelerate` will kindly throw an error to make us aware of that).\n",
            "    model = model.to(accelerator.device)\n",
            "\n",
            "    # Instantiate optimizer\n",
            "    optimizer = AdamW(params=model.parameters(), lr=lr)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_nlp_example.py",
        "line_number": 204,
        "API": ".to(",
        "context": [
            "            # After the first iteration though, we need to go back to the original dataloader\n",
            "            active_dataloader = train_dataloader\n",
            "        for step, batch in enumerate(active_dataloader):\n",
            "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
            "            batch.to(accelerator.device)\n",
            "            outputs = model(**batch)\n",
            "            loss = outputs.loss\n",
            "            loss = loss / gradient_accumulation_steps\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_nlp_example.py",
        "line_number": 211,
        "API": ".backward(",
        "context": [
            "            loss = loss / gradient_accumulation_steps\n",
            "            # We keep track of the loss at each epoch\n",
            "            if args.with_tracking:\n",
            "                total_loss += loss.detach().float()\n",
            "            accelerator.backward(loss)\n",
            "            if step % gradient_accumulation_steps == 0:\n",
            "                optimizer.step()\n",
            "                lr_scheduler.step()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_nlp_example.py",
        "line_number": 223,
        "API": ".join(",
        "context": [
            "            if isinstance(checkpointing_steps, int):\n",
            "                output_dir = f\"step_{overall_step}\"\n",
            "                if overall_step % checkpointing_steps == 0:\n",
            "                    if args.output_dir is not None:\n",
            "                        output_dir = os.path.join(args.output_dir, output_dir)\n",
            "                    accelerator.save_state(output_dir)\n",
            "\n",
            "        model.eval()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_nlp_example.py",
        "line_number": 229,
        "API": ".to(",
        "context": [
            "\n",
            "        model.eval()\n",
            "        for step, batch in enumerate(eval_dataloader):\n",
            "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
            "            batch.to(accelerator.device)\n",
            "            with torch.no_grad():\n",
            "                outputs = model(**batch)\n",
            "            predictions = outputs.logits.argmax(dim=-1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_nlp_example.py",
        "line_number": 243,
        "API": ".log(",
        "context": [
            "        eval_metric = metric.compute()\n",
            "        # Use accelerator.print to print only on the main process.\n",
            "        accelerator.print(f\"epoch {epoch}:\", eval_metric)\n",
            "        if args.with_tracking:\n",
            "            accelerator.log(\n",
            "                {\n",
            "                    \"accuracy\": eval_metric[\"accuracy\"],\n",
            "                    \"f1\": eval_metric[\"f1\"],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\complete_nlp_example.py",
        "line_number": 256,
        "API": ".join(",
        "context": [
            "\n",
            "        if checkpointing_steps == \"epoch\":\n",
            "            output_dir = f\"epoch_{epoch}\"\n",
            "            if args.output_dir is not None:\n",
            "                output_dir = os.path.join(args.output_dir, output_dir)\n",
            "            accelerator.save_state(output_dir)\n",
            "\n",
            "    if args.with_tracking:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\cv_example.py",
        "line_number": 87,
        "API": ".join(",
        "context": [
            "    if not isinstance(image_size, (list, tuple)):\n",
            "        image_size = (image_size, image_size)\n",
            "\n",
            "    # Grab all the image filenames\n",
            "    file_names = [os.path.join(args.data_dir, fname) for fname in os.listdir(args.data_dir) if fname.endswith(\".jpg\")]\n",
            "\n",
            "    # Build the label correspondences\n",
            "    all_labels = [extract_label(fname) for fname in file_names]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\cv_example.py",
        "line_number": 156,
        "API": ".to(",
        "context": [
            "    for epoch in range(num_epochs):\n",
            "        model.train()\n",
            "        for step, batch in enumerate(train_dataloader):\n",
            "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
            "            batch = {k: v.to(accelerator.device) for k, v in batch.items()}\n",
            "            inputs = (batch[\"image\"] - mean) / std\n",
            "            outputs = model(inputs)\n",
            "            loss = torch.nn.functional.cross_entropy(outputs, batch[\"label\"])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\cv_example.py",
        "line_number": 170,
        "API": ".to(",
        "context": [
            "        accurate = 0\n",
            "        num_elems = 0\n",
            "        for _, batch in enumerate(eval_dataloader):\n",
            "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
            "            batch = {k: v.to(accelerator.device) for k, v in batch.items()}\n",
            "            inputs = (batch[\"image\"] - mean) / std\n",
            "            with torch.no_grad():\n",
            "                outputs = model(inputs)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\multigpu_remote_launcher.py",
        "line_number": 45,
        "API": ".function(",
        "context": [
            "        \"scikit-learn\",\n",
            "        \"tensorboard\",\n",
            "        \"torch --upgrade --extra-index-url https://download.pytorch.org/whl/cu117\",\n",
            "    ]\n",
            "    launch_train_gpu = rh.function(fn=launch_train, system=gpu, reqs=reqs, name=\"train_bert_glue\")\n",
            "\n",
            "    # Define train args/config, run train function\n",
            "    train_args = argparse.Namespace(cpu=False, mixed_precision=\"fp16\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\nlp_example.py",
        "line_number": 122,
        "API": ".load(",
        "context": [
            "    num_epochs = int(config[\"num_epochs\"])\n",
            "    seed = int(config[\"seed\"])\n",
            "    batch_size = int(config[\"batch_size\"])\n",
            "\n",
            "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
            "\n",
            "    # If the batch size is too big we use gradient accumulation\n",
            "    gradient_accumulation_steps = 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\nlp_example.py",
        "line_number": 138,
        "API": ".to(",
        "context": [
            "\n",
            "    # We could avoid this line since the accelerator is set with `device_placement=True` (default value).\n",
            "    # Note that if you are placing tensors on devices manually, this line absolutely needs to be before the optimizer\n",
            "    # creation otherwise training will not work on TPU (`accelerate` will kindly throw an error to make us aware of that).\n",
            "    model = model.to(accelerator.device)\n",
            "    # Instantiate optimizer\n",
            "    optimizer = AdamW(params=model.parameters(), lr=lr)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\nlp_example.py",
        "line_number": 162,
        "API": ".to(",
        "context": [
            "    for epoch in range(num_epochs):\n",
            "        model.train()\n",
            "        for step, batch in enumerate(train_dataloader):\n",
            "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
            "            batch.to(accelerator.device)\n",
            "            outputs = model(**batch)\n",
            "            loss = outputs.loss\n",
            "            loss = loss / gradient_accumulation_steps\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\automatic_gradient_accumulation.py",
        "line_number": 94,
        "API": ".pad(",
        "context": [
            "            pad_to_multiple_of = 8\n",
            "        else:\n",
            "            pad_to_multiple_of = None\n",
            "\n",
            "        return tokenizer.pad(\n",
            "            examples,\n",
            "            padding=\"longest\",\n",
            "            pad_to_multiple_of=pad_to_multiple_of,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\automatic_gradient_accumulation.py",
        "line_number": 113,
        "API": ".get(",
        "context": [
            "    return train_dataloader, eval_dataloader\n",
            "\n",
            "\n",
            "# For testing only\n",
            "if os.environ.get(\"TESTING_MOCKED_DATALOADERS\", None) == \"1\":\n",
            "    from accelerate.test_utils.training import mocked_dataloaders\n",
            "\n",
            "    get_dataloaders = mocked_dataloaders  # noqa: F811\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\automatic_gradient_accumulation.py",
        "line_number": 121,
        "API": ".get(",
        "context": [
            "\n",
            "\n",
            "def training_function(config, args):\n",
            "    # For testing only\n",
            "    if os.environ.get(\"TESTING_MOCKED_DATALOADERS\", None) == \"1\":\n",
            "        config[\"num_epochs\"] = 2\n",
            "    # Initialize accelerator\n",
            "    accelerator = Accelerator(cpu=args.cpu, mixed_precision=args.mixed_precision)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\automatic_gradient_accumulation.py",
        "line_number": 131,
        "API": ".load(",
        "context": [
            "    num_epochs = int(config[\"num_epochs\"])\n",
            "    seed = int(config[\"seed\"])\n",
            "    observed_batch_size = int(config[\"batch_size\"])\n",
            "\n",
            "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
            "\n",
            "    # New Code #\n",
            "    # We use the `find_executable_batch_size` decorator, passing in the desired observed batch size\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\automatic_gradient_accumulation.py",
        "line_number": 163,
        "API": ".to(",
        "context": [
            "\n",
            "        # We could avoid this line since the accelerator is set with `device_placement=True` (default value).\n",
            "        # Note that if you are placing tensors on devices manually, this line absolutely needs to be before the optimizer\n",
            "        # creation otherwise training will not work on TPU (`accelerate` will kindly throw an error to make us aware of that).\n",
            "        model = model.to(accelerator.device)\n",
            "\n",
            "        # Instantiate optimizer\n",
            "        optimizer = AdamW(params=model.parameters(), lr=lr)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\automatic_gradient_accumulation.py",
        "line_number": 190,
        "API": ".to(",
        "context": [
            "            for step, batch in enumerate(train_dataloader):\n",
            "                # And perform gradient accumulation\n",
            "                with accelerator.accumulate(model):\n",
            "                    # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
            "                    batch.to(accelerator.device)\n",
            "                    outputs = model(**batch)\n",
            "                    loss = outputs.loss\n",
            "                    accelerator.backward(loss)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\automatic_gradient_accumulation.py",
        "line_number": 201,
        "API": ".to(",
        "context": [
            "\n",
            "            model.eval()\n",
            "            for step, batch in enumerate(eval_dataloader):\n",
            "                # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
            "                batch.to(accelerator.device)\n",
            "                with torch.no_grad():\n",
            "                    outputs = model(**batch)\n",
            "                predictions = outputs.logits.argmax(dim=-1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\checkpointing.py",
        "line_number": 152,
        "API": ".load(",
        "context": [
            "\n",
            "    set_seed(seed)\n",
            "\n",
            "    train_dataloader, eval_dataloader = get_dataloaders(accelerator, batch_size)\n",
            "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
            "\n",
            "    # If the batch size is too big we use gradient accumulation\n",
            "    gradient_accumulation_steps = 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\checkpointing.py",
        "line_number": 255,
        "API": ".to(",
        "context": [
            "\n",
            "        model.eval()\n",
            "        for step, batch in enumerate(eval_dataloader):\n",
            "            # We could avoid this line since we set the accelerator with `device_placement=True` (the default).\n",
            "            batch.to(accelerator.device)\n",
            "            with torch.no_grad():\n",
            "                outputs = model(**batch)\n",
            "            predictions = outputs.logits.argmax(dim=-1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\checkpointing.py",
        "line_number": 277,
        "API": ".join(",
        "context": [
            "        # If mixed precision was used, will also save a \"scalar.bin\" file\n",
            "        if checkpointing_steps == \"epoch\":\n",
            "            output_dir = f\"epoch_{epoch}\"\n",
            "            if args.output_dir is not None:\n",
            "                output_dir = os.path.join(args.output_dir, output_dir)\n",
            "            accelerator.save_state(output_dir)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\cross_validation.py",
        "line_number": 167,
        "API": ".split(",
        "context": [
            "    set_seed(seed)\n",
            "\n",
            "    # New Code #\n",
            "    # Create our folds:\n",
            "    folds = kfold.split(np.zeros(datasets[\"train\"].num_rows), datasets[\"train\"][\"label\"])\n",
            "    test_references = []\n",
            "    # Iterate over them\n",
            "    for i, (train_idxs, valid_idxs) in enumerate(folds):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\cross_validation.py",
        "line_number": 207,
        "API": ".to(",
        "context": [
            "        for epoch in range(num_epochs):\n",
            "            model.train()\n",
            "            for step, batch in enumerate(train_dataloader):\n",
            "                # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
            "                batch.to(accelerator.device)\n",
            "                outputs = model(**batch)\n",
            "                loss = outputs.loss\n",
            "                loss = loss / gradient_accumulation_steps\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\cross_validation.py",
        "line_number": 239,
        "API": ".to(",
        "context": [
            "        # We also run predictions on the test set at the very end\n",
            "        fold_predictions = []\n",
            "        for step, batch in enumerate(test_dataloader):\n",
            "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
            "            batch.to(accelerator.device)\n",
            "            with torch.no_grad():\n",
            "                outputs = model(**batch)\n",
            "            predictions = outputs.logits\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\cross_validation.py",
        "line_number": 249,
        "API": ".cat(",
        "context": [
            "            if i == 0:\n",
            "                # We need all of the test predictions\n",
            "                test_references.append(references.cpu())\n",
            "        # Use accelerator.print to print only on the main process.\n",
            "        test_predictions.append(torch.cat(fold_predictions, dim=0))\n",
            "        # We now need to release all our memory and get rid of the current model, optimizer, etc\n",
            "        accelerator.free_memory()\n",
            "    # New Code #\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\cross_validation.py",
        "line_number": 254,
        "API": ".cat(",
        "context": [
            "        # We now need to release all our memory and get rid of the current model, optimizer, etc\n",
            "        accelerator.free_memory()\n",
            "    # New Code #\n",
            "    # Finally we check the accuracy of our folded results:\n",
            "    test_references = torch.cat(test_references, dim=0)\n",
            "    preds = torch.stack(test_predictions, dim=0).sum(dim=0).div(int(args.num_folds)).argmax(dim=-1)\n",
            "    test_metric = metric.compute(predictions=preds, references=test_references)\n",
            "    accelerator.print(\"Average test metrics from all folds:\", test_metric)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 233,
        "API": ".split(",
        "context": [
            "    if args.dataset_name is None and args.train_file is None and args.validation_file is None:\n",
            "        raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
            "    else:\n",
            "        if args.train_file is not None:\n",
            "            extension = args.train_file.split(\".\")[-1]\n",
            "            assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, json or txt file.\"\n",
            "        if args.validation_file is not None:\n",
            "            extension = args.validation_file.split(\".\")[-1]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 260,
        "API": ".info(",
        "context": [
            "\n",
            "    success = model.save_checkpoint(checkpoint_folder, ckpt_id, checkpoint_state_dict)\n",
            "    status_msg = f\"checkpointing: checkpoint_folder={checkpoint_folder}, ckpt_id={ckpt_id}\"\n",
            "    if success:\n",
            "        logging.info(f\"Success {status_msg}\")\n",
            "    else:\n",
            "        logging.warning(f\"Failure {status_msg}\")\n",
            "    return\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 287,
        "API": ".repeat(",
        "context": [
            "        with torch.no_grad():\n",
            "            outputs = model(**batch)\n",
            "\n",
            "        loss = outputs.loss\n",
            "        losses.append(accelerator.gather_for_metrics(loss.repeat(args.per_device_eval_batch_size)))\n",
            "\n",
            "    losses = torch.cat(losses)\n",
            "    try:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 292,
        "API": ".exp(",
        "context": [
            "\n",
            "    losses = torch.cat(losses)\n",
            "    try:\n",
            "        eval_loss = torch.mean(losses)\n",
            "        perplexity = math.exp(eval_loss)\n",
            "    except OverflowError:\n",
            "        perplexity = float(\"inf\")\n",
            "    return perplexity, eval_loss\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 313,
        "API": ".info(",
        "context": [
            "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
            "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
            "        level=logging.INFO,\n",
            "    )\n",
            "    logger.info(accelerator.state, main_process_only=False)\n",
            "    if accelerator.is_local_main_process:\n",
            "        datasets.utils.logging.set_verbosity_warning()\n",
            "        transformers.utils.logging.set_verbosity_info()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 334,
        "API": ".join(",
        "context": [
            "            else:\n",
            "                repo_name = args.hub_model_id\n",
            "            repo = Repository(args.output_dir, clone_from=repo_name)\n",
            "\n",
            "            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n",
            "                if \"step_*\" not in gitignore:\n",
            "                    gitignore.write(\"step_*\\n\")\n",
            "                if \"epoch_*\" not in gitignore:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 340,
        "API": ".makedirs(",
        "context": [
            "                    gitignore.write(\"step_*\\n\")\n",
            "                if \"epoch_*\" not in gitignore:\n",
            "                    gitignore.write(\"epoch_*\\n\")\n",
            "        elif args.output_dir is not None:\n",
            "            os.makedirs(args.output_dir, exist_ok=True)\n",
            "    accelerator.wait_for_everyone()\n",
            "\n",
            "    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 373,
        "API": ".split(",
        "context": [
            "        if args.train_file is not None:\n",
            "            data_files[\"train\"] = args.train_file\n",
            "        if args.validation_file is not None:\n",
            "            data_files[\"validation\"] = args.validation_file\n",
            "        extension = args.train_file.split(\".\")[-1]\n",
            "        if extension == \"txt\":\n",
            "            extension = \"text\"\n",
            "            dataset_args[\"keep_linebreaks\"] = not args.no_keep_linebreaks\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 406,
        "API": ".warning(",
        "context": [
            "    elif args.model_name_or_path:\n",
            "        config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
            "    else:\n",
            "        config = CONFIG_MAPPING[args.model_type]()\n",
            "        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
            "\n",
            "    if args.tokenizer_name:\n",
            "        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=not args.use_slow_tokenizer)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 425,
        "API": ".info(",
        "context": [
            "            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
            "            config=config,\n",
            "        )\n",
            "    else:\n",
            "        logger.info(\"Training new model from scratch\")\n",
            "        model = AutoModelForCausalLM.from_config(config)\n",
            "\n",
            "    model.resize_token_embeddings(len(tokenizer))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 451,
        "API": ".warning(",
        "context": [
            "\n",
            "    if args.block_size is None:\n",
            "        block_size = tokenizer.model_max_length\n",
            "        if block_size > 1024:\n",
            "            logger.warning(\n",
            "                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
            "                \"Picking 1024 instead. You can change that default value by passing --block_size xxx.\"\n",
            "            )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 458,
        "API": ".warning(",
        "context": [
            "            )\n",
            "        block_size = 1024\n",
            "    else:\n",
            "        if args.block_size > tokenizer.model_max_length:\n",
            "            logger.warning(\n",
            "                f\"The block_size passed ({args.block_size}) is larger than the maximum length for the model\"\n",
            "                f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n",
            "            )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 478,
        "API": ".copy(",
        "context": [
            "        result = {\n",
            "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
            "            for k, t in concatenated_examples.items()\n",
            "        }\n",
            "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
            "        return result\n",
            "\n",
            "    # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 502,
        "API": ".info(",
        "context": [
            "    eval_dataset = lm_datasets[\"validation\"]\n",
            "\n",
            "    # Log a few random samples from the training set:\n",
            "    for index in random.sample(range(len(train_dataset)), 3):\n",
            "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
            "\n",
            "    # DataLoaders creation:\n",
            "    train_dataloader = DataLoader(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 548,
        "API": ".ceil(",
        "context": [
            "        args.gradient_accumulation_steps = accelerator.state.deepspeed_plugin.deepspeed_config[\n",
            "            \"gradient_accumulation_steps\"\n",
            "        ]\n",
            "\n",
            "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
            "    if args.max_train_steps is None:\n",
            "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 577,
        "API": ".ceil(",
        "context": [
            "        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
            "    )\n",
            "\n",
            "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
            "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
            "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
            "\n",
            "    # Figure out how many steps we should save the Accelerator states\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 599,
        "API": ".info(",
        "context": [
            "\n",
            "    # Train!\n",
            "    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
            "\n",
            "    logger.info(\"***** Running training *****\")\n",
            "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
            "    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
            "    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 604,
        "API": ".info(",
        "context": [
            "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
            "    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
            "    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
            "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
            "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
            "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
            "    # Only show the progress bar once on each machine.\n",
            "    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 643,
        "API": ".backward(",
        "context": [
            "            # We keep track of the loss at each epoch\n",
            "            if args.with_tracking:\n",
            "                total_loss += loss.detach().float()\n",
            "            loss = loss / args.gradient_accumulation_steps\n",
            "            accelerator.backward(loss)\n",
            "            if (step + 1) % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
            "                optimizer.step()\n",
            "                lr_scheduler.step()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 655,
        "API": ".join(",
        "context": [
            "            if isinstance(checkpointing_steps, int):\n",
            "                if completed_steps % checkpointing_steps == 0:\n",
            "                    output_dir = f\"step_{completed_steps }\"\n",
            "                    if args.output_dir is not None:\n",
            "                        output_dir = os.path.join(args.output_dir, output_dir)\n",
            "                    accelerator.save_state(output_dir)\n",
            "            if completed_steps >= args.max_train_steps:\n",
            "                break\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 661,
        "API": ".info(",
        "context": [
            "            if completed_steps >= args.max_train_steps:\n",
            "                break\n",
            "\n",
            "        perplexity, eval_loss = evaluate(args, model, eval_dataloader, accelerator, eval_dataset)\n",
            "        logger.info(f\"epoch {epoch}: perplexity: {perplexity} eval_loss: {eval_loss}\")\n",
            "\n",
            "        if args.with_tracking:\n",
            "            accelerator.log(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 668,
        "API": ".item(",
        "context": [
            "            accelerator.log(\n",
            "                {\n",
            "                    \"perplexity\": perplexity,\n",
            "                    \"eval_loss\": eval_loss,\n",
            "                    \"train_loss\": total_loss.item() / len(train_dataloader),\n",
            "                    \"epoch\": epoch,\n",
            "                    \"step\": completed_steps,\n",
            "                },\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 683,
        "API": ".join(",
        "context": [
            "        # New Code #\n",
            "        # Tracks the best checkpoint and best metric\n",
            "        if best_metric is None or best_metric > perplexity:\n",
            "            best_metric = perplexity\n",
            "            best_metric_checkpoint = os.path.join(args.output_dir, str(epoch))\n",
            "            accelerator.print(f\"New best metric: {best_metric} at epoch {epoch}\")\n",
            "            accelerator.print(f\"best_metric_checkpoint: {best_metric_checkpoint}\")\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 692,
        "API": ".join(",
        "context": [
            "    # Loads the best checkpoint after the training is finished\n",
            "    if args.load_best_model:\n",
            "        _, last_global_step = load_training_checkpoint(\n",
            "            model,\n",
            "            \"/\".join(best_metric_checkpoint.split(\"/\")[:-1]),\n",
            "            tag=best_metric_checkpoint.split(\"/\")[-1],\n",
            "            **{\"load_optimizer_states\": True, \"load_lr_scheduler_states\": True},\n",
            "        )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 700,
        "API": ".info(",
        "context": [
            "\n",
            "    # New Code #\n",
            "    # Evaluates using the best checkpoint\n",
            "    perplexity, eval_loss = evaluate(args, model, eval_dataloader, accelerator, eval_dataset)\n",
            "    logger.info(f\"Best model metrics: perplexity: {perplexity} eval_loss: {eval_loss}\")\n",
            "    if perplexity != best_metric:\n",
            "        raise AssertionError(\n",
            "            f\"Best metric {best_metric} does not match the metric {perplexity} of the loaded best model.\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\deepspeed_with_config_support.py",
        "line_number": 727,
        "API": ".join(",
        "context": [
            "            tokenizer.save_pretrained(args.output_dir)\n",
            "            if args.push_to_hub:\n",
            "                repo.push_to_hub(commit_message=\"End of training\", auto_lfs_prune=True)\n",
            "\n",
            "        with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n",
            "            json.dump({\"perplexity\": perplexity, \"eval_loss\": eval_loss.item()}, f)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\fsdp_with_peak_mem_tracking.py",
        "line_number": 70,
        "API": ".empty_cache(",
        "context": [
            "# This context manager is used to track the peak memory usage of the process\n",
            "class TorchTracemalloc:\n",
            "    def __enter__(self):\n",
            "        gc.collect()\n",
            "        torch.cuda.empty_cache()\n",
            "        torch.cuda.reset_max_memory_allocated()  # reset the peak gauge to zero\n",
            "        self.begin = torch.cuda.memory_allocated()\n",
            "        self.process = psutil.Process()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\fsdp_with_peak_mem_tracking.py",
        "line_number": 102,
        "API": ".empty_cache(",
        "context": [
            "    def __exit__(self, *exc):\n",
            "        self.peak_monitoring = False\n",
            "\n",
            "        gc.collect()\n",
            "        torch.cuda.empty_cache()\n",
            "        self.end = torch.cuda.memory_allocated()\n",
            "        self.peak = torch.cuda.max_memory_allocated()\n",
            "        self.used = b2mb(self.end - self.begin)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\fsdp_with_peak_mem_tracking.py",
        "line_number": 115,
        "API": ".get(",
        "context": [
            "        # print(f\"delta used/peak {self.used:4d}/{self.peaked:4d}\")\n",
            "\n",
            "\n",
            "# For testing only\n",
            "if os.environ.get(\"TESTING_MOCKED_DATALOADERS\", None) == \"1\":\n",
            "    from accelerate.test_utils.training import mocked_dataloaders\n",
            "\n",
            "    get_dataloaders = mocked_dataloaders  # noqa: F811\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\fsdp_with_peak_mem_tracking.py",
        "line_number": 123,
        "API": ".get(",
        "context": [
            "\n",
            "\n",
            "def training_function(config, args):\n",
            "    # For testing only\n",
            "    if os.environ.get(\"TESTING_MOCKED_DATALOADERS\", None) == \"1\":\n",
            "        config[\"num_epochs\"] = 2\n",
            "\n",
            "    # New Code #\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\fsdp_with_peak_mem_tracking.py",
        "line_number": 170,
        "API": ".load(",
        "context": [
            "        accelerator.init_trackers(\"fsdp_glue_no_trainer\", experiment_config)\n",
            "\n",
            "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
            "    datasets = load_dataset(\"glue\", \"mrpc\")\n",
            "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
            "\n",
            "    def tokenize_function(examples):\n",
            "        # max_length=None => use the model max length (it's actually the default)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\fsdp_with_peak_mem_tracking.py",
        "line_number": 296,
        "API": ".to(",
        "context": [
            "                if args.resume_from_checkpoint and epoch == 0:\n",
            "                    if resume_step is not None and step < resume_step:\n",
            "                        pass\n",
            "                # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
            "                batch.to(accelerator.device)\n",
            "                outputs = model(**batch)\n",
            "                loss = outputs.loss\n",
            "                loss = loss / gradient_accumulation_steps\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\fsdp_with_peak_mem_tracking.py",
        "line_number": 303,
        "API": ".backward(",
        "context": [
            "                loss = loss / gradient_accumulation_steps\n",
            "                # We keep track of the loss at each epoch\n",
            "                if args.with_tracking:\n",
            "                    total_loss += loss.detach().float()\n",
            "                accelerator.backward(loss)\n",
            "                if step % gradient_accumulation_steps == 0:\n",
            "                    optimizer.step()\n",
            "                    lr_scheduler.step()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\fsdp_with_peak_mem_tracking.py",
        "line_number": 316,
        "API": ".join(",
        "context": [
            "                if isinstance(checkpointing_steps, int):\n",
            "                    output_dir = f\"step_{overall_step}\"\n",
            "                    if overall_step % checkpointing_steps == 0:\n",
            "                        if args.output_dir is not None:\n",
            "                            output_dir = os.path.join(args.output_dir, output_dir)\n",
            "                        accelerator.save_state(output_dir)\n",
            "        # New Code #\n",
            "        # Printing the GPU memory usage details such as allocated memory, peak memory, and total memory usage\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\fsdp_with_peak_mem_tracking.py",
        "line_number": 321,
        "API": ".format(",
        "context": [
            "                        accelerator.save_state(output_dir)\n",
            "        # New Code #\n",
            "        # Printing the GPU memory usage details such as allocated memory, peak memory, and total memory usage\n",
            "        accelerator.print(\"Memory before entering the train : {}\".format(b2mb(tracemalloc.begin)))\n",
            "        accelerator.print(\"Memory consumed at the end of the train (end-begin): {}\".format(tracemalloc.used))\n",
            "        accelerator.print(\"Peak Memory consumed during the train (max-begin): {}\".format(tracemalloc.peaked))\n",
            "        accelerator.print(\n",
            "            \"Total Peak Memory consumed during the train (max): {}\".format(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\fsdp_with_peak_mem_tracking.py",
        "line_number": 330,
        "API": ".log(",
        "context": [
            "            )\n",
            "        )\n",
            "        # Logging the peak memory usage of the GPU to the tracker\n",
            "        if args.with_tracking:\n",
            "            accelerator.log(\n",
            "                {\n",
            "                    \"train_total_peak_memory\": tracemalloc.peaked + b2mb(tracemalloc.begin),\n",
            "                },\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\fsdp_with_peak_mem_tracking.py",
        "line_number": 343,
        "API": ".to(",
        "context": [
            "        with TorchTracemalloc() as tracemalloc:\n",
            "            model.eval()\n",
            "            for step, batch in enumerate(eval_dataloader):\n",
            "                # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
            "                batch.to(accelerator.device)\n",
            "                with torch.no_grad():\n",
            "                    outputs = model(**batch)\n",
            "                predictions = outputs.logits.argmax(dim=-1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\fsdp_with_peak_mem_tracking.py",
        "line_number": 357,
        "API": ".log(",
        "context": [
            "            eval_metric = metric.compute()\n",
            "            # Use accelerator.print to print only on the main process.\n",
            "            accelerator.print(f\"epoch {epoch}:\", eval_metric)\n",
            "            if args.with_tracking:\n",
            "                accelerator.log(\n",
            "                    {\n",
            "                        \"accuracy\": eval_metric[\"accuracy\"],\n",
            "                        \"f1\": eval_metric[\"f1\"],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\fsdp_with_peak_mem_tracking.py",
        "line_number": 369,
        "API": ".join(",
        "context": [
            "\n",
            "            if checkpointing_steps == \"epoch\":\n",
            "                output_dir = f\"epoch_{epoch}\"\n",
            "                if args.output_dir is not None:\n",
            "                    output_dir = os.path.join(args.output_dir, output_dir)\n",
            "                accelerator.save_state(output_dir)\n",
            "        # New Code #\n",
            "        # Printing the GPU memory usage details such as allocated memory, peak memory, and total memory usage\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\fsdp_with_peak_mem_tracking.py",
        "line_number": 374,
        "API": ".format(",
        "context": [
            "                accelerator.save_state(output_dir)\n",
            "        # New Code #\n",
            "        # Printing the GPU memory usage details such as allocated memory, peak memory, and total memory usage\n",
            "        accelerator.print(\"Memory before entering the eval : {}\".format(b2mb(tracemalloc.begin)))\n",
            "        accelerator.print(\"Memory consumed at the end of the eval (end-begin): {}\".format(tracemalloc.used))\n",
            "        accelerator.print(\"Peak Memory consumed during the eval (max-begin): {}\".format(tracemalloc.peaked))\n",
            "        accelerator.print(\n",
            "            \"Total Peak Memory consumed during the eval (max): {}\".format(tracemalloc.peaked + b2mb(tracemalloc.begin))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\fsdp_with_peak_mem_tracking.py",
        "line_number": 381,
        "API": ".log(",
        "context": [
            "            \"Total Peak Memory consumed during the eval (max): {}\".format(tracemalloc.peaked + b2mb(tracemalloc.begin))\n",
            "        )\n",
            "        # Logging the peak memory usage of the GPU to the tracker\n",
            "        if args.with_tracking:\n",
            "            accelerator.log(\n",
            "                {\n",
            "                    \"eval_total_peak_memory\": tracemalloc.peaked + b2mb(tracemalloc.begin),\n",
            "                },\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\gradient_accumulation.py",
        "line_number": 120,
        "API": ".get(",
        "context": [
            "\n",
            "\n",
            "def training_function(config, args):\n",
            "    # For testing only\n",
            "    if os.environ.get(\"TESTING_MOCKED_DATALOADERS\", None) == \"1\":\n",
            "        config[\"num_epochs\"] = 2\n",
            "    # New Code #\n",
            "    gradient_accumulation_steps = int(args.gradient_accumulation_steps)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\gradient_accumulation.py",
        "line_number": 138,
        "API": ".load(",
        "context": [
            "    num_epochs = int(config[\"num_epochs\"])\n",
            "    seed = int(config[\"seed\"])\n",
            "    batch_size = int(config[\"batch_size\"])\n",
            "\n",
            "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
            "\n",
            "    set_seed(seed)\n",
            "    train_dataloader, eval_dataloader = get_dataloaders(accelerator, batch_size)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\gradient_accumulation.py",
        "line_number": 172,
        "API": ".to(",
        "context": [
            "    for epoch in range(num_epochs):\n",
            "        model.train()\n",
            "        for step, batch in enumerate(train_dataloader):\n",
            "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
            "            batch.to(accelerator.device)\n",
            "            # New code #\n",
            "            # We use the new `accumulate` context manager to perform gradient accumulation\n",
            "            # We also currently do not support TPUs nor advise it as bugs were found on the XLA side when running our tests.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\gradient_accumulation.py",
        "line_number": 179,
        "API": ".backward(",
        "context": [
            "            # We also currently do not support TPUs nor advise it as bugs were found on the XLA side when running our tests.\n",
            "            with accelerator.accumulate(model):\n",
            "                output = model(**batch)\n",
            "                loss = output.loss\n",
            "                accelerator.backward(loss)\n",
            "                optimizer.step()\n",
            "                lr_scheduler.step()\n",
            "                optimizer.zero_grad()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\local_sgd.py",
        "line_number": 177,
        "API": ".to(",
        "context": [
            "            accelerator=accelerator, model=model, local_sgd_steps=local_sgd_steps, enabled=local_sgd_steps is not None\n",
            "        ) as local_sgd:\n",
            "            for step, batch in enumerate(train_dataloader):\n",
            "                # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
            "                batch.to(accelerator.device)\n",
            "                # New code #\n",
            "                # We use the new `accumulate` context manager to perform gradient accumulation\n",
            "                # We also currently do not support TPUs nor advise it as bugs were found on the XLA side when running our tests.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\local_sgd.py",
        "line_number": 184,
        "API": ".backward(",
        "context": [
            "                # We also currently do not support TPUs nor advise it as bugs were found on the XLA side when running our tests.\n",
            "                with accelerator.accumulate(model):\n",
            "                    output = model(**batch)\n",
            "                    loss = output.loss\n",
            "                    accelerator.backward(loss)\n",
            "                    optimizer.step()\n",
            "                    lr_scheduler.step()\n",
            "                    optimizer.zero_grad()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\megatron_lm_gpt_pretraining.py",
        "line_number": 481,
        "API": ".ceil(",
        "context": [
            "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
            "\n",
            "    # Scheduler and math around the number of training steps.\n",
            "    overrode_max_train_steps = False\n",
            "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
            "    if args.max_train_steps is None:\n",
            "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
            "        overrode_max_train_steps = True\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\megatron_lm_gpt_pretraining.py",
        "line_number": 512,
        "API": ".ceil(",
        "context": [
            "    if accelerator.distributed_type == DistributedType.TPU:\n",
            "        model.tie_weights()\n",
            "\n",
            "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
            "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
            "    if overrode_max_train_steps:\n",
            "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
            "    # Afterwards we recalculate our number of training epochs\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\megatron_lm_gpt_pretraining.py",
        "line_number": 542,
        "API": ".info(",
        "context": [
            "        total_batch_size = (\n",
            "            args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
            "        )\n",
            "\n",
            "    logger.info(\"***** Running training *****\")\n",
            "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
            "    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
            "    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\megatron_lm_gpt_pretraining.py",
        "line_number": 600,
        "API": ".backward(",
        "context": [
            "                loss = outputs.loss\n",
            "                # We keep track of the loss at each epoch\n",
            "                if args.with_tracking:\n",
            "                    total_loss += loss.detach().float()\n",
            "                accelerator.backward(loss)\n",
            "                optimizer.step()\n",
            "                lr_scheduler.step()\n",
            "                optimizer.zero_grad()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\megatron_lm_gpt_pretraining.py",
        "line_number": 631,
        "API": ".repeat(",
        "context": [
            "            # For Megatron-LM, the losses are already averaged across the data parallel group\n",
            "            if accelerator.distributed_type == DistributedType.MEGATRON_LM:\n",
            "                losses.append(loss)\n",
            "            else:\n",
            "                losses.append(accelerator.gather_for_metrics(loss.repeat(args.per_device_eval_batch_size)))\n",
            "        try:\n",
            "            if accelerator.distributed_type == DistributedType.MEGATRON_LM:\n",
            "                losses = torch.tensor(losses)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\megatron_lm_gpt_pretraining.py",
        "line_number": 636,
        "API": ".cat(",
        "context": [
            "        try:\n",
            "            if accelerator.distributed_type == DistributedType.MEGATRON_LM:\n",
            "                losses = torch.tensor(losses)\n",
            "            else:\n",
            "                losses = torch.cat(losses)\n",
            "            eval_loss = torch.mean(losses)\n",
            "            perplexity = math.exp(eval_loss)\n",
            "        except OverflowError:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\megatron_lm_gpt_pretraining.py",
        "line_number": 642,
        "API": ".info(",
        "context": [
            "            perplexity = math.exp(eval_loss)\n",
            "        except OverflowError:\n",
            "            perplexity = float(\"inf\")\n",
            "\n",
            "        logger.info(f\"epoch {epoch}: perplexity: {perplexity} eval_loss: {eval_loss}\")\n",
            "\n",
            "        if args.with_tracking:\n",
            "            accelerator.log(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\megatron_lm_gpt_pretraining.py",
        "line_number": 671,
        "API": ".join(",
        "context": [
            "\n",
            "        if args.checkpointing_steps == \"epoch\":\n",
            "            output_dir = f\"epoch_{epoch}\"\n",
            "            if args.output_dir is not None:\n",
            "                output_dir = os.path.join(args.output_dir, output_dir)\n",
            "            accelerator.save_state(output_dir)\n",
            "\n",
            "    # this is causing some issue with Megatron-LM when using `wandb` at the end of the main function.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\megatron_lm_gpt_pretraining.py",
        "line_number": 695,
        "API": ".join(",
        "context": [
            "            tokenizer.save_pretrained(args.output_dir)\n",
            "            if args.push_to_hub:\n",
            "                repo.push_to_hub(commit_message=\"End of training\", auto_lfs_prune=True)\n",
            "\n",
            "        with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n",
            "            json.dump({\"perplexity\": perplexity}, f)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\memory.py",
        "line_number": 135,
        "API": ".load(",
        "context": [
            "    num_epochs = int(config[\"num_epochs\"])\n",
            "    seed = int(config[\"seed\"])\n",
            "    batch_size = int(config[\"batch_size\"])\n",
            "\n",
            "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
            "\n",
            "    # New Code #\n",
            "    # We now can define an inner training loop function. It should take a batch size as the only parameter,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\memory.py",
        "line_number": 183,
        "API": ".to(",
        "context": [
            "        for epoch in range(num_epochs):\n",
            "            model.train()\n",
            "            for step, batch in enumerate(train_dataloader):\n",
            "                # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
            "                batch.to(accelerator.device)\n",
            "                outputs = model(**batch)\n",
            "                loss = outputs.loss\n",
            "                accelerator.backward(loss)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\multi_process_metrics.py",
        "line_number": 191,
        "API": ".to(",
        "context": [
            "        model.eval()\n",
            "        samples_seen = 0\n",
            "        for step, batch in enumerate(eval_dataloader):\n",
            "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
            "            batch.to(accelerator.device)\n",
            "            with torch.no_grad():\n",
            "                outputs = model(**batch)\n",
            "            predictions = outputs.logits.argmax(dim=-1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\tracking.py",
        "line_number": 125,
        "API": ".get(",
        "context": [
            "\n",
            "\n",
            "def training_function(config, args):\n",
            "    # For testing only\n",
            "    if os.environ.get(\"TESTING_MOCKED_DATALOADERS\", None) == \"1\":\n",
            "        config[\"num_epochs\"] = 2\n",
            "    # Initialize Accelerator\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\tracking.py",
        "line_number": 147,
        "API": ".load(",
        "context": [
            "    batch_size = int(config[\"batch_size\"])\n",
            "    set_seed(seed)\n",
            "\n",
            "    train_dataloader, eval_dataloader = get_dataloaders(accelerator, batch_size)\n",
            "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
            "\n",
            "    # If the batch size is too big we use gradient accumulation\n",
            "    gradient_accumulation_steps = 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\tracking.py",
        "line_number": 183,
        "API": ".split(",
        "context": [
            "\n",
            "    # New Code #\n",
            "    # We need to initialize the trackers we use. Overall configurations can also be stored\n",
            "    if args.with_tracking:\n",
            "        run = os.path.split(__file__)[-1].split(\".\")[0]\n",
            "        accelerator.init_trackers(run, config)\n",
            "\n",
            "    # Now we train the model\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\tracking.py",
        "line_number": 195,
        "API": ".to(",
        "context": [
            "        if args.with_tracking:\n",
            "            total_loss = 0\n",
            "        for step, batch in enumerate(train_dataloader):\n",
            "            # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
            "            batch.to(accelerator.device)\n",
            "            outputs = model(**batch)\n",
            "            loss = outputs.loss\n",
            "            # New Code #\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\tracking.py",
        "line_number": 202,
        "API": ".backward(",
        "context": [
            "            # New Code #\n",
            "            if args.with_tracking:\n",
            "                total_loss += loss.detach().float()\n",
            "            loss = loss / gradient_accumulation_steps\n",
            "            accelerator.backward(loss)\n",
            "            if step % gradient_accumulation_steps == 0:\n",
            "                optimizer.step()\n",
            "                lr_scheduler.step()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\examples\\by_feature\\tracking.py",
        "line_number": 229,
        "API": ".log(",
        "context": [
            "        # New Code #\n",
            "        # To actually log, we call `Accelerator.log`\n",
            "        # The values passed can be of `str`, `int`, `float` or `dict` of `str` to `float`/`int`\n",
            "        if args.with_tracking:\n",
            "            accelerator.log(\n",
            "                {\n",
            "                    \"accuracy\": eval_metric[\"accuracy\"],\n",
            "                    \"f1\": eval_metric[\"f1\"],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_1.py",
        "line_number": 22,
        "API": ".copy(",
        "context": [
            "    def construct(self):\n",
            "        mem = Rectangle(height=0.5,width=0.5)\n",
            "        fill = Rectangle(height=0.46,width=0.46).set_stroke(width=0)\n",
            "\n",
            "        cpu_left_col_base = [mem.copy() for i in range(6)]\n",
            "        cpu_right_col_base = [mem.copy() for i in range(6)]\n",
            "        cpu_left_col = VGroup(*cpu_left_col_base).arrange(UP, buff=0)\n",
            "        cpu_right_col = VGroup(*cpu_right_col_base).arrange(UP, buff=0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_1.py",
        "line_number": 30,
        "API": ".add(",
        "context": [
            "        cpu_rects = VGroup(cpu_left_col,cpu_right_col).arrange(RIGHT, buff=0)\n",
            "        cpu_text = Text(\"CPU\", font_size=24)\n",
            "        cpu = Group(cpu_rects,cpu_text).arrange(DOWN, buff=0.5, aligned_edge=DOWN)\n",
            "        cpu.move_to([-2.5,-.5,0])\n",
            "        self.add(cpu)\n",
            "\n",
            "        gpu_base = [mem.copy() for i in range(1)]\n",
            "        gpu_rect = VGroup(*gpu_base).arrange(UP,buff=0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_1.py",
        "line_number": 39,
        "API": ".add(",
        "context": [
            "        gpu = Group(gpu_rect,gpu_text).arrange(DOWN, buff=0.5, aligned_edge=DOWN)\n",
            "        gpu.align_to(cpu, DOWN)\n",
            "        gpu.set_x(gpu.get_x() - 1)\n",
            "        \n",
            "        self.add(gpu)\n",
            "\n",
            "        model_base = [mem.copy() for i in range(6)]\n",
            "        model_rect = VGroup(*model_base).arrange(RIGHT,buff=0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_1.py",
        "line_number": 77,
        "API": ".add(",
        "context": [
            "            Write(key_text),\n",
            "            Write(key)\n",
            "        )\n",
            "\n",
            "        self.add(model)\n",
            "        \n",
            "\n",
            "        cpu_targs = []\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_2.py",
        "line_number": 29,
        "API": ".add(",
        "context": [
            "        cpu_rects = VGroup(cpu_left_col,cpu_right_col).arrange(RIGHT, buff=0)\n",
            "        cpu_text = Text(\"CPU\", font_size=24)\n",
            "        cpu = Group(cpu_rects,cpu_text).arrange(DOWN, buff=0.5, aligned_edge=DOWN)\n",
            "        cpu.move_to([-2.5,-.5,0])\n",
            "        self.add(cpu)\n",
            "\n",
            "        gpu_base = [mem.copy() for i in range(4)]\n",
            "        gpu_rect = VGroup(*gpu_base).arrange(UP,buff=0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_2.py",
        "line_number": 36,
        "API": ".add(",
        "context": [
            "        gpu_rect = VGroup(*gpu_base).arrange(UP,buff=0)\n",
            "        gpu_text = Text(\"GPU\", font_size=24)\n",
            "        gpu = Group(gpu_rect,gpu_text).arrange(DOWN, buff=0.5, aligned_edge=DOWN)\n",
            "        gpu.move_to([-1,-1,0])\n",
            "        self.add(gpu)\n",
            "\n",
            "        model_base = [mem.copy() for i in range(6)]\n",
            "        model_rect = VGroup(*model_base).arrange(RIGHT,buff=0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_2.py",
        "line_number": 44,
        "API": ".add(",
        "context": [
            "\n",
            "        model_text = Text(\"Model\", font_size=24)\n",
            "        model = Group(model_rect,model_text).arrange(DOWN, buff=0.5, aligned_edge=DOWN)\n",
            "        model.move_to([3, -1., 0])\n",
            "        self.add(model)\n",
            "        \n",
            "        cpu_targs = []\n",
            "        for i,rect in enumerate(model_base):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_2.py",
        "line_number": 49,
        "API": ".copy(",
        "context": [
            "        \n",
            "        cpu_targs = []\n",
            "        for i,rect in enumerate(model_base):\n",
            "            rect.set_stroke(YELLOW)\n",
            "            # target = fill.copy().set_fill(YELLOW, opacity=0.7)\n",
            "            # target.move_to(rect)\n",
            "            # self.add(target)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_2.py",
        "line_number": 62,
        "API": ".add(",
        "context": [
            "            elif i == 3:\n",
            "                cpu_target.next_to(cpu_targs[0], direction=UP, buff=0.)\n",
            "            else:\n",
            "                cpu_target.next_to(cpu_targs[i-1], direction=RIGHT, buff=0.)\n",
            "            self.add(cpu_target)\n",
            "            cpu_targs.append(cpu_target)\n",
            "\n",
            "              \n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_2.py",
        "line_number": 67,
        "API": ".copy(",
        "context": [
            "            cpu_targs.append(cpu_target)\n",
            "\n",
            "              \n",
            "\n",
            "        checkpoint_base = [mem.copy() for i in range(6)]\n",
            "        checkpoint_rect = VGroup(*checkpoint_base).arrange(RIGHT,buff=0)\n",
            "\n",
            "        checkpoint_text = Text(\"Loaded Checkpoint\", font_size=24)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_2.py",
        "line_number": 84,
        "API": ".add(",
        "context": [
            "        )\n",
            "\n",
            "        key_text.move_to([-5, 2.4, 0])\n",
            "\n",
            "        self.add(key_text, key)\n",
            "\n",
            "        blue_text = MarkupText(\n",
            "            f\"<span fgcolor='{BLUE}'>\u25cf</span> Checkpoint\",\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_2.py",
        "line_number": 111,
        "API": ".copy(",
        "context": [
            "\n",
            "        first_animations = []\n",
            "        second_animations = []\n",
            "        for i,rect in enumerate(checkpoint_base):\n",
            "            target = fill.copy().set_fill(BLUE, opacity=0.7)\n",
            "            target.move_to(rect)\n",
            "            first_animations.append(GrowFromCenter(target, run_time=1))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_3.py",
        "line_number": 22,
        "API": ".copy(",
        "context": [
            "        mem = Rectangle(height=0.5,width=0.5)\n",
            "        meta_mem = Rectangle(height=0.25,width=0.25)\n",
            "        fill = Rectangle(height=0.46,width=0.46).set_stroke(width=0)\n",
            "\n",
            "        cpu_left_col_base = [mem.copy() for i in range(6)]\n",
            "        cpu_right_col_base = [mem.copy() for i in range(6)]\n",
            "        cpu_left_col = VGroup(*cpu_left_col_base).arrange(UP, buff=0)\n",
            "        cpu_right_col = VGroup(*cpu_right_col_base).arrange(UP, buff=0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_3.py",
        "line_number": 45,
        "API": ".add(",
        "context": [
            "\n",
            "        model_text = Text(\"Model\", font_size=24)\n",
            "        model = Group(model_rect,model_text).arrange(DOWN, buff=0.5, aligned_edge=DOWN)\n",
            "        model.move_to([3, -1., 0])\n",
            "        self.add(model)\n",
            "\n",
            "        model_arr = []\n",
            "        model_cpu_arr = []\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_3.py",
        "line_number": 63,
        "API": ".add(",
        "context": [
            "            elif i == 3:\n",
            "                cpu_target.next_to(model_cpu_arr[0], direction=UP, buff=0.)\n",
            "            else:\n",
            "                cpu_target.next_to(model_cpu_arr[i-1], direction=RIGHT, buff=0.)\n",
            "            self.add(cpu_target)\n",
            "            model_cpu_arr.append(cpu_target)\n",
            "\n",
            "        self.add(*model_arr, *model_cpu_arr, *model_meta_arr)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_3.py",
        "line_number": 68,
        "API": ".copy(",
        "context": [
            "            model_cpu_arr.append(cpu_target)\n",
            "\n",
            "        self.add(*model_arr, *model_cpu_arr, *model_meta_arr)\n",
            "\n",
            "        checkpoint_base = [mem.copy() for i in range(6)]\n",
            "        checkpoint_rect = VGroup(*checkpoint_base).arrange(RIGHT,buff=0)\n",
            "\n",
            "        checkpoint_text = Text(\"Loaded Checkpoint\", font_size=24)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_3.py",
        "line_number": 75,
        "API": ".add(",
        "context": [
            "        checkpoint_text = Text(\"Loaded Checkpoint\", font_size=24)\n",
            "        checkpoint = Group(checkpoint_rect,checkpoint_text).arrange(DOWN, buff=0.5, aligned_edge=DOWN)\n",
            "        checkpoint.move_to([3, .5, 0])\n",
            "            \n",
            "        self.add(checkpoint)\n",
            "\n",
            "        ckpt_arr = []\n",
            "        ckpt_cpu_arr = []\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_3.py",
        "line_number": 81,
        "API": ".copy(",
        "context": [
            "        ckpt_arr = []\n",
            "        ckpt_cpu_arr = []\n",
            "\n",
            "        for i,rect in enumerate(checkpoint_base):\n",
            "            target = fill.copy().set_fill(BLUE, opacity=0.7)\n",
            "            target.move_to(rect)\n",
            "            ckpt_arr.append(target)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_3.py",
        "line_number": 91,
        "API": ".add(",
        "context": [
            "                cpu_target.move_to(cpu_left_col_base[i+1])\n",
            "            else:\n",
            "                cpu_target.move_to(cpu_right_col_base[i-5])\n",
            "            ckpt_cpu_arr.append(cpu_target)\n",
            "        self.add(*ckpt_arr, *ckpt_cpu_arr)\n",
            "\n",
            "        key = Square(side_length=2.2)\n",
            "        key.move_to([-5, 2, 0])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_3.py",
        "line_number": 111,
        "API": ".add(",
        "context": [
            "            font_size=18,\n",
            "        )\n",
            "\n",
            "        blue_text.next_to(key_text, DOWN*2.4, aligned_edge=key_text.get_left())\n",
            "        self.add(blue_text)\n",
            "\n",
            "        step_3 = MarkupText(\n",
            "            f'Based on the passed in configuration, weights are stored in\\na variety of np.memmaps on disk or to a particular device.', \n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_3.py",
        "line_number": 119,
        "API": ".copy(",
        "context": [
            "            font_size=24\n",
            "        )\n",
            "        step_3.move_to([2, 2, 0])\n",
            "\n",
            "        disk_left_col_base = [meta_mem.copy() for i in range(6)]\n",
            "        disk_right_col_base = [meta_mem.copy() for i in range(6)]\n",
            "        disk_left_col = VGroup(*disk_left_col_base).arrange(UP, buff=0)\n",
            "        disk_right_col = VGroup(*disk_right_col_base).arrange(UP, buff=0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_3.py",
        "line_number": 135,
        "API": ".copy(",
        "context": [
            "        )\n",
            "\n",
            "        animations = []\n",
            "        for i,rect in enumerate(ckpt_cpu_arr):\n",
            "            target = rect.copy()\n",
            "            target.generate_target()\n",
            "            target.target.move_to(disk_left_col_base[i]).scale(0.5)\n",
            "            animations.append(MoveToTarget(target, run_time=1.5))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_4.py",
        "line_number": 22,
        "API": ".copy(",
        "context": [
            "        mem = Rectangle(height=0.5,width=0.5)\n",
            "        fill = Rectangle(height=0.46,width=0.46).set_stroke(width=0)\n",
            "        meta_mem = Rectangle(height=0.25,width=0.25)\n",
            "\n",
            "        cpu_left_col_base = [mem.copy() for i in range(6)]\n",
            "        cpu_right_col_base = [mem.copy() for i in range(6)]\n",
            "        cpu_left_col = VGroup(*cpu_left_col_base).arrange(UP, buff=0)\n",
            "        cpu_right_col = VGroup(*cpu_right_col_base).arrange(UP, buff=0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_4.py",
        "line_number": 45,
        "API": ".add(",
        "context": [
            "\n",
            "        model_text = Text(\"Model\", font_size=24)\n",
            "        model = Group(model_rect,model_text).arrange(DOWN, buff=0.5, aligned_edge=DOWN)\n",
            "        model.move_to([3, -1., 0])\n",
            "        self.add(model)\n",
            "\n",
            "        model_cpu_arr = []\n",
            "        model_meta_arr = []\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_4.py",
        "line_number": 62,
        "API": ".add(",
        "context": [
            "            elif i == 3:\n",
            "                cpu_target.next_to(model_cpu_arr[0], direction=UP, buff=0.)\n",
            "            else:\n",
            "                cpu_target.next_to(model_cpu_arr[i-1], direction=RIGHT, buff=0.)\n",
            "            self.add(cpu_target)\n",
            "            model_cpu_arr.append(cpu_target)\n",
            "\n",
            "        self.add(*model_cpu_arr, *model_meta_arr)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_4.py",
        "line_number": 67,
        "API": ".copy(",
        "context": [
            "            model_cpu_arr.append(cpu_target)\n",
            "\n",
            "        self.add(*model_cpu_arr, *model_meta_arr)\n",
            "\n",
            "        disk_left_col_base = [meta_mem.copy() for i in range(6)]\n",
            "        disk_right_col_base = [meta_mem.copy() for i in range(6)]\n",
            "        disk_left_col = VGroup(*disk_left_col_base).arrange(UP, buff=0)\n",
            "        disk_right_col = VGroup(*disk_right_col_base).arrange(UP, buff=0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_4.py",
        "line_number": 75,
        "API": ".add(",
        "context": [
            "        disk_rects = VGroup(disk_left_col,disk_right_col).arrange(RIGHT, buff=0)\n",
            "        disk_text = Text(\"Disk\", font_size=24)\n",
            "        disk = Group(disk_rects,disk_text).arrange(DOWN, buff=0.5, aligned_edge=DOWN)\n",
            "        disk.move_to([-4.,-1.25,0])\n",
            "        self.add(disk_text, disk_rects)\n",
            "\n",
            "        cpu_disk_arr = []\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_4.py",
        "line_number": 80,
        "API": ".copy(",
        "context": [
            "\n",
            "        cpu_disk_arr = []\n",
            "\n",
            "        for i in range(6):\n",
            "            target = fill.copy().set_fill(BLUE, opacity=0.8)\n",
            "            target.move_to(disk_left_col_base[i]).scale(0.5)\n",
            "            cpu_disk_arr.append(target)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_4.py",
        "line_number": 104,
        "API": ".add(",
        "context": [
            "            font_size=18,\n",
            "        )\n",
            "\n",
            "        blue_text.next_to(key_text, DOWN*2.4, aligned_edge=key_text.get_left())\n",
            "        self.add(blue_text)\n",
            "\n",
            "        step_5 = MarkupText(\n",
            "            f'The offloaded weights are all sent to the CPU.', \n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_4.py",
        "line_number": 116,
        "API": ".copy(",
        "context": [
            "        self.play(Write(step_5, run_time=3))\n",
            "\n",
            "        for i in range(6):\n",
            "            rect = cpu_disk_arr[i]\n",
            "            cp2 = rect.copy().set_fill(BLUE, opacity=0.8).scale(2.0)\n",
            "            cp2.generate_target()\n",
            "            cp2.target.move_to(model_base[i])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_5.py",
        "line_number": 23,
        "API": ".copy(",
        "context": [
            "        fill = Rectangle(height=0.46,width=0.46).set_stroke(width=0)\n",
            "\n",
            "        meta_mem = Rectangle(height=0.25,width=0.25)\n",
            "\n",
            "        cpu_left_col_base = [mem.copy() for i in range(6)]\n",
            "        cpu_right_col_base = [mem.copy() for i in range(6)]\n",
            "        cpu_left_col = VGroup(*cpu_left_col_base).arrange(UP, buff=0)\n",
            "        cpu_right_col = VGroup(*cpu_right_col_base).arrange(UP, buff=0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_5.py",
        "line_number": 52,
        "API": ".copy(",
        "context": [
            "        model_arr = []\n",
            "        model_cpu_arr = []\n",
            "        \n",
            "        for i,rect in enumerate(model_base):\n",
            "            target = fill.copy().set_fill(BLUE, opacity=0.8)\n",
            "            target.move_to(rect)\n",
            "            model_arr.append(target)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_5.py",
        "line_number": 60,
        "API": ".add(",
        "context": [
            "            cpu_target = Rectangle(height=0.46,width=0.46).set_stroke(width=0.).set_fill(BLUE, opacity=0.8)\n",
            "            cpu_target.move_to(cpu_left_col_base[i])\n",
            "            model_cpu_arr.append(cpu_target)\n",
            "\n",
            "        self.add(*model_arr, *model_cpu_arr)\n",
            "\n",
            "        disk_left_col_base = [meta_mem.copy() for i in range(6)]\n",
            "        disk_right_col_base = [meta_mem.copy() for i in range(6)]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_5.py",
        "line_number": 70,
        "API": ".add(",
        "context": [
            "        disk_rects = VGroup(disk_left_col,disk_right_col).arrange(RIGHT, buff=0)\n",
            "        disk_text = Text(\"Disk\", font_size=24)\n",
            "        disk = Group(disk_rects,disk_text).arrange(DOWN, buff=0.5, aligned_edge=DOWN)\n",
            "        disk.move_to([-4,-1.25,0])\n",
            "        self.add(disk_text, disk_rects)\n",
            "\n",
            "        key = Square(side_length=2.2)\n",
            "        key.move_to([-5, 2, 0])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_5.py",
        "line_number": 90,
        "API": ".add(",
        "context": [
            "            font_size=18,\n",
            "        )\n",
            "\n",
            "        blue_text.next_to(key_text, DOWN*2.4, aligned_edge=key_text.get_left())\n",
            "        self.add(blue_text)\n",
            "\n",
            "        step_6 = MarkupText(\n",
            "            f'Now watch as an input is passed through the model\\nand how the memory is utilized and handled.', \n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_5.py",
        "line_number": 140,
        "API": ".copy(",
        "context": [
            "        self.play(\n",
            "            MoveToTarget(model_cpu_arr[0])\n",
            "        )\n",
            "\n",
            "        a_c = a.copy()\n",
            "        for i in range(6):\n",
            "            a_c.next_to(model_arr[i].get_right()+0.02, UP, buff=0.2)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\manim_animations\\big_model_inference\\stage_5.py",
        "line_number": 201,
        "API": ".copy(",
        "context": [
            "                    MoveToTarget(model_cpu_arr[i])\n",
            "                )\n",
            "\n",
            "            a = a_c\n",
            "            a_c = a_c.copy()\n",
            "\n",
            "        input.generate_target()\n",
            "        input.target.next_to(model_base[-1], RIGHT+0.02, buff=.5)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 278,
        "API": ".get(",
        "context": [
            "        dynamo_plugin = TorchDynamoPlugin() if dynamo_backend is None else TorchDynamoPlugin(backend=dynamo_backend)\n",
            "\n",
            "        if deepspeed_plugin is None:  # init from env variables\n",
            "            deepspeed_plugin = (\n",
            "                DeepSpeedPlugin() if os.environ.get(\"ACCELERATE_USE_DEEPSPEED\", \"false\") == \"true\" else None\n",
            "            )\n",
            "        else:\n",
            "            assert isinstance(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 292,
        "API": ".get(",
        "context": [
            "            if compare_versions(\"deepspeed\", \"<\", \"0.9.3\"):\n",
            "                raise ImportError(\"DeepSpeed version must be >= 0.9.3. Please update DeepSpeed.\")\n",
            "\n",
            "            mixed_precision = (\n",
            "                os.environ.get(\"ACCELERATE_MIXED_PRECISION\", \"no\") if mixed_precision is None else mixed_precision\n",
            "            )\n",
            "            deepspeed_plugin.set_mixed_precision(mixed_precision)\n",
            "            deepspeed_plugin.set_deepspeed_weakref()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 297,
        "API": ".get(",
        "context": [
            "            )\n",
            "            deepspeed_plugin.set_mixed_precision(mixed_precision)\n",
            "            deepspeed_plugin.set_deepspeed_weakref()\n",
            "\n",
            "        if os.environ.get(\"ACCELERATE_USE_FSDP\", \"false\") == \"true\" or isinstance(\n",
            "            fsdp_plugin, FullyShardedDataParallelPlugin\n",
            "        ):\n",
            "            if is_torch_version(\"<\", FSDP_PYTORCH_VERSION):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 305,
        "API": ".get(",
        "context": [
            "                raise ValueError(f\"FSDP requires PyTorch >= {FSDP_PYTORCH_VERSION}\")\n",
            "\n",
            "        if fsdp_plugin is None:  # init from env variables\n",
            "            fsdp_plugin = (\n",
            "                FullyShardedDataParallelPlugin() if os.environ.get(\"ACCELERATE_USE_FSDP\", \"false\") == \"true\" else None\n",
            "            )\n",
            "        else:\n",
            "            if not isinstance(fsdp_plugin, FullyShardedDataParallelPlugin):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 314,
        "API": ".get(",
        "context": [
            "            os.environ[\"ACCELERATE_USE_FSDP\"] = \"true\"  # use FSDP if plugin is provided\n",
            "\n",
            "        if megatron_lm_plugin is None:  # init from env variables\n",
            "            megatron_lm_plugin = (\n",
            "                MegatronLMPlugin() if os.environ.get(\"ACCELERATE_USE_MEGATRON_LM\", \"false\") == \"true\" else None\n",
            "            )\n",
            "        else:\n",
            "            if not isinstance(megatron_lm_plugin, MegatronLMPlugin):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 370,
        "API": ".warn(",
        "context": [
            "        )\n",
            "\n",
            "        trackers = filter_trackers(log_with, self.logging_dir)\n",
            "        if len(trackers) < 1 and log_with is not None:\n",
            "            warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
            "        self.log_with = trackers\n",
            "\n",
            "        if (\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 417,
        "API": ".format(",
        "context": [
            "            and self.distributed_type not in (DistributedType.DEEPSPEED, DistributedType.MEGATRON_LM)\n",
            "        ):\n",
            "            self.native_amp = True\n",
            "            if self.device.type not in (\"cuda\", \"mps\", \"npu\"):\n",
            "                raise ValueError(err.format(mode=\"fp16\", requirement=\"a GPU\"))\n",
            "            kwargs = self.scaler_handler.to_kwargs() if self.scaler_handler is not None else {}\n",
            "            if self.distributed_type == DistributedType.FSDP:\n",
            "                from torch.distributed.fsdp.sharded_grad_scaler import ShardedGradScaler\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 437,
        "API": ".format(",
        "context": [
            "                self.native_amp = True\n",
            "            else:\n",
            "                self.native_amp = is_bf16_available(True)\n",
            "            if mixed_precision == \"bf16\" and not self.native_amp and not is_tpu_available():\n",
            "                raise ValueError(err.format(mode=\"bf16\", requirement=\"PyTorch >= 1.10 and a supported device.\"))\n",
            "\n",
            "        # Start of internal step tracking\n",
            "        self.step = 0\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 509,
        "API": ".warn(",
        "context": [
            "        return self.state.is_local_main_process\n",
            "\n",
            "    @property\n",
            "    def use_fp16(self):\n",
            "        warnings.warn(\n",
            "            \"The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use \"\n",
            "            \"`Accelerator.mixed_precision == 'fp16'` instead.\",\n",
            "            FutureWarning,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 537,
        "API": ".gather(",
        "context": [
            "            inputs (`list`, `tuple`, `torch.Tensor`, or `dict` of `list`/`tuple`/`torch.Tensor`):\n",
            "                The input to split between processes.\n",
            "            apply_padding (`bool`, `optional`, defaults to `False`):\n",
            "                Whether to apply padding by repeating the last element of the input so that all processes have the same\n",
            "                number of elements. Useful when trying to perform actions such as `Accelerator.gather()` on the outputs\n",
            "                or passing in less inputs than there are processes. If so, just remember to drop the padded elements\n",
            "                afterwards.\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 848,
        "API": ".backward(",
        "context": [
            "\n",
            "        >>> with accelerator.no_sync():\n",
            "        ...     outputs = model(input_a)\n",
            "        ...     loss = loss_func(outputs)\n",
            "        ...     accelerator.backward(loss)\n",
            "        ...     # No synchronization across processes, only accumulate gradients\n",
            "        >>> outputs = model(input_b)\n",
            "        >>> accelerator.backward(loss)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 887,
        "API": ".backward(",
        "context": [
            "\n",
            "                >>> with accelerator.no_sync():\n",
            "                ...     loss_a = loss_func(model(input_a))  # first forward pass\n",
            "                ...     loss_b = loss_func(model(input_b))  # second forward pass\n",
            "                >>> accelerator.backward(loss_a)  # No synchronization across processes, only accumulate gradients\n",
            "                >>> with accelerator.trigger_sync_in_backward(model):\n",
            "                ...     accelerator.backward(loss_b)  # Synchronization across all processes\n",
            "                >>> optimizer.step()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 959,
        "API": ".backward(",
        "context": [
            "        >>> for input, output in dataloader:\n",
            "        ...     with accelerator.accumulate(model):\n",
            "        ...         outputs = model(input)\n",
            "        ...         loss = loss_func(outputs)\n",
            "        ...         loss.backward()\n",
            "        ...         optimizer.step()\n",
            "        ...         scheduler.step()\n",
            "        ...         optimizer.zero_grad()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1014,
        "API": ".backward(",
        "context": [
            "        >>> with accelerator.join_uneven_inputs([ddp_model], even_batches=False):\n",
            "        ...     for input, output in dataloader:\n",
            "        ...         outputs = model(input)\n",
            "        ...         loss = loss_func(outputs)\n",
            "        ...         loss.backward()\n",
            "        ...         optimizer.step()\n",
            "        ...         optimizer.zero_grad()\n",
            "        ```\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1033,
        "API": ".warn(",
        "context": [
            "                    dl_even_batches_values.append((dl_idx, dl.batch_sampler.even_batches))\n",
            "                    dl.batch_sampler.even_batches = even_batches\n",
            "\n",
            "                if iterable_dl_seen:\n",
            "                    warnings.warn(\n",
            "                        \"Overridding even_batches is only supported for map-style datasets, yet some dataloaders given were iterable\"\n",
            "                    )\n",
            "            else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1050,
        "API": ".warn(",
        "context": [
            "                    self._dataloaders[dl_idx].batch_sampler.even_batches = even_batches_value\n",
            "        else:\n",
            "            # Even when disabled, Join expects models to subclass Joinable, so skip entirely for single process runs\n",
            "            if self.distributed_type != DistributedType.NO:\n",
            "                warnings.warn(\n",
            "                    \"Joining uneven inputs is only supported for multi-GPU training, as a result `join_uneven_inputs` will have no effect.\"\n",
            "                )\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1103,
        "API": ".warning(",
        "context": [
            "        intermediate_result = []\n",
            "        for obj in args:\n",
            "            if isinstance(obj, torch.optim.Optimizer):\n",
            "                if len(obj.param_groups) > 1:\n",
            "                    logger.warning(\n",
            "                        \"FSDP Warning: When using FSDP, several parameter groups will be conflated into \"\n",
            "                        \"a single one due to nested module wrapping and parameter flattening.\"\n",
            "                    )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1205,
        "API": ".warning(",
        "context": [
            "                    \"prepare must be called for all the models before optimizers are created. \"\n",
            "                    \"Then pass the optimizers to the prepare call in the same order as corresponding models.\"\n",
            "                )\n",
            "            elif model_count == 1 and optimizer_present:\n",
            "                logger.warning(\n",
            "                    \"FSDP Warning: When using FSDP, \"\n",
            "                    \"it is efficient and recommended to call prepare for the model before creating the optimizer\"\n",
            "                )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1229,
        "API": ".to(",
        "context": [
            "            if model_device is not None and optimizer_device is not None and model_device != optimizer_device:\n",
            "                raise ValueError(\n",
            "                    \"The model and the optimizer parameters are not on the same device, which probably means you \"\n",
            "                    \"created an optimizer around your model **before** putting on the device. Make sure the line \"\n",
            "                    \"model.to(device) is before the optimizer creation in your script or remove it entirely and use \"\n",
            "                    \"the flag default value for `device_placement` in your `Accelerator` to let it handle that \"\n",
            "                    \"part for you.\"\n",
            "                )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1326,
        "API": ".device(",
        "context": [
            "                )\n",
            "            current_device = list(model_devices)[0]\n",
            "            current_device_index = current_device.index if isinstance(current_device, torch.device) else current_device\n",
            "\n",
            "            if torch.device(current_device_index) != self.device:\n",
            "                # if on the first device (GPU 0) we don't care\n",
            "                if (self.device.index is not None) or (current_device_index != 0):\n",
            "                    raise ValueError(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1331,
        "API": ".current_device(",
        "context": [
            "                # if on the first device (GPU 0) we don't care\n",
            "                if (self.device.index is not None) or (current_device_index != 0):\n",
            "                    raise ValueError(\n",
            "                        \"You can't train a model that has been loaded in 8-bit precision on a different device than the one \"\n",
            "                        \"you're training on. Make sure you loaded the model on the correct device using for example `device_map={'':torch.cuda.current_device()}\"\n",
            "                        \"you're training on. Make sure you loaded the model on the correct device using for example `device_map={'':torch.cuda.current_device() or device_map={'':torch.xpu.current_device()}\"\n",
            "                    )\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1340,
        "API": ".to(",
        "context": [
            "                raise ValueError(\n",
            "                    \"You can't train a model that has been loaded in 8-bit precision with CPU or disk offload.\"\n",
            "                )\n",
            "        elif device_placement and not has_hf_device_map:\n",
            "            model = model.to(self.device)\n",
            "\n",
            "        if self.native_amp:\n",
            "            model._original_forward = model.forward\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1374,
        "API": ".warn(",
        "context": [
            "            fp8_enabled = cuda_device_capacity[0] >= 9 or (\n",
            "                cuda_device_capacity[0] == 8 and cuda_device_capacity[1] >= 9\n",
            "            )\n",
            "            if not fp8_enabled:\n",
            "                logger.warn(\n",
            "                    f\"The current device has compute capability of {cuda_device_capacity} which is \"\n",
            "                    \"insufficient for FP8 mixed precision training (requires a GPU Hopper/Ada Lovelace \"\n",
            "                    \"or higher, compute capability of 8.9 or higher). Will use FP16 instead.\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1420,
        "API": ".to(",
        "context": [
            "            elif self.distributed_type == DistributedType.MULTI_CPU:\n",
            "                kwargs = self.ddp_handler.to_kwargs() if self.ddp_handler is not None else {}\n",
            "                model = torch.nn.parallel.DistributedDataParallel(model, **kwargs)\n",
            "            elif self.distributed_type == DistributedType.TPU and self.state.fork_launched:\n",
            "                model = xmp.MpModelWrapper(model).to(self.device)\n",
            "        # torch.compile should be called last.\n",
            "        if self.state.dynamo_plugin.backend != DynamoBackend.NO:\n",
            "            if not is_torch_version(\">=\", \"2.0\"):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1425,
        "API": ".compile(",
        "context": [
            "        # torch.compile should be called last.\n",
            "        if self.state.dynamo_plugin.backend != DynamoBackend.NO:\n",
            "            if not is_torch_version(\">=\", \"2.0\"):\n",
            "                raise ValueError(\"Using `torch.compile` requires PyTorch 2.0 or higher.\")\n",
            "            model = torch.compile(model, **self.state.dynamo_plugin.to_kwargs())\n",
            "        return model\n",
            "\n",
            "    def _prepare_deepspeed(self, *args):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1457,
        "API": ".info(",
        "context": [
            "                )\n",
            "\n",
            "            batch_size_per_device = min(batch_sizes) if deepspeed_plugin.is_train_batch_min else max(batch_sizes)\n",
            "            if len(batch_sizes) > 1:\n",
            "                logger.info(\n",
            "                    \"Since you passed both train and evaluation dataloader, `is_train_batch_min` (here \"\n",
            "                    f\"{deepspeed_plugin.is_train_batch_min} will decide the `train_batch_size` ({batch_size_per_device}).\"\n",
            "                )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1466,
        "API": ".info(",
        "context": [
            "            batch_size_per_device = deepspeed_plugin.deepspeed_config[\"train_micro_batch_size_per_gpu\"]\n",
            "            result = [obj for obj in args]\n",
            "\n",
            "        if self.gradient_accumulation_steps != deepspeed_plugin.deepspeed_config[\"gradient_accumulation_steps\"]:\n",
            "            logger.info(\n",
            "                f\"Updating DeepSpeed's gradient accumulation steps to {self.gradient_accumulation_steps} from \"\n",
            "                f\"{deepspeed_plugin.deepspeed_config['gradient_accumulation_steps']}.\"\n",
            "            )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1562,
        "API": ".ceil(",
        "context": [
            "                    }\n",
            "                )\n",
            "                if scheduler.total_num_steps is not None:\n",
            "                    config_kwargs[\"scheduler.params.total_num_steps\"] = (\n",
            "                        math.ceil(scheduler.total_num_steps / self.num_processes)\n",
            "                        if not self.split_batches\n",
            "                        else scheduler.total_num_steps\n",
            "                    )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1573,
        "API": ".get(",
        "context": [
            "            if optimizer is not None:\n",
            "                if isinstance(optimizer, (DummyOptim)):\n",
            "                    kwargs[\"model_parameters\"] = optimizer.params\n",
            "                else:\n",
            "                    if self.deepspeed_config[\"zero_optimization\"].get(\"offload_optimizer\", {}).get(\n",
            "                        \"device\", \"none\"\n",
            "                    ) != \"none\" and self.deepspeed_config.get(\"zero_force_ds_cpu_optimizer\", True):\n",
            "                        from deepspeed.ops.adam import DeepSpeedCPUAdam\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1608,
        "API": ".backward(",
        "context": [
            "                elif (isinstance(result[i], (LRScheduler, DummyScheduler))) or (\n",
            "                    type(result[i]).__name__ in deepspeed.runtime.lr_schedules.VALID_LR_SCHEDULES\n",
            "                ):\n",
            "                    result[i] = scheduler\n",
            "            # pointing for deepspeed_engine_wrapped.backward()\n",
            "            self.deepspeed_engine_wrapped = DeepSpeedEngineWrapper(engine)\n",
            "            self._models.append(engine)\n",
            "            if optimizer is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1632,
        "API": ".info(",
        "context": [
            "                )\n",
            "\n",
            "            micro_batch_size = min(batch_sizes) if megatron_lm_plugin.is_train_batch_min else max(batch_sizes)\n",
            "            if len(batch_sizes) > 1:\n",
            "                logger.info(\n",
            "                    \"Since you passed both train and evaluation dataloader, `is_train_batch_min` (here \"\n",
            "                    f\"{megatron_lm_plugin.is_train_batch_min} will decide the `train_batch_size` ({micro_batch_size}).\"\n",
            "                )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1742,
        "API": ".to(",
        "context": [
            "                optimizer = obj\n",
            "        if optimizer is not None and model is not None:\n",
            "            dtype = torch.bfloat16 if self.state.mixed_precision == \"bf16\" else torch.float32\n",
            "            if self.device.type == \"xpu\" and is_xpu_available():\n",
            "                model = model.to(self.device)\n",
            "                model, optimizer = torch.xpu.optimize(\n",
            "                    model, optimizer=optimizer, dtype=dtype, inplace=True, level=\"O1\"\n",
            "                )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1792,
        "API": ".copy(",
        "context": [
            "            num_processes=self.num_processes,\n",
            "            process_index=self.process_index,\n",
            "            split_batches=self.split_batches,\n",
            "            put_on_device=device_placement,\n",
            "            rng_types=self.rng_types.copy(),\n",
            "            dispatch_batches=self.dispatch_batches,\n",
            "            even_batches=self.even_batches,\n",
            "        )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1878,
        "API": ".backward(",
        "context": [
            "        \"\"\"\n",
            "        Scales the gradients in accordance to the `GradientAccumulationPlugin` and calls the correct `backward()` based\n",
            "        on the configuration.\n",
            "\n",
            "        Should be used in lieu of `loss.backward()`.\n",
            "\n",
            "        Example:\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1888,
        "API": ".backward(",
        "context": [
            "\n",
            "        >>> accelerator = Accelerator(gradient_accumulation_steps=2)\n",
            "        >>> outputs = model(inputs)\n",
            "        >>> loss = loss_fn(outputs, labels)\n",
            "        >>> accelerator.backward(loss)\n",
            "        ```\n",
            "        \"\"\"\n",
            "        if self.distributed_type != DistributedType.DEEPSPEED:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1895,
        "API": ".backward(",
        "context": [
            "        if self.distributed_type != DistributedType.DEEPSPEED:\n",
            "            # deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`\n",
            "            loss = loss / self.gradient_accumulation_steps\n",
            "        if self.distributed_type == DistributedType.DEEPSPEED:\n",
            "            self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
            "        elif self.distributed_type == DistributedType.MEGATRON_LM:\n",
            "            return\n",
            "        elif self.scaler is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1901,
        "API": ".backward(",
        "context": [
            "            return\n",
            "        elif self.scaler is not None:\n",
            "            self.scaler.scale(loss).backward(**kwargs)\n",
            "        else:\n",
            "            loss.backward(**kwargs)\n",
            "\n",
            "    def unscale_gradients(self, optimizer=None):\n",
            "        \"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1923,
        "API": ".backward(",
        "context": [
            "        >>> accelerator = Accelerator()\n",
            "        >>> model, optimizer = accelerator.prepare(model, optimizer)\n",
            "        >>> outputs = model(inputs)\n",
            "        >>> loss = loss_fn(outputs, labels)\n",
            "        >>> accelerator.backward(loss)\n",
            "        >>> accelerator.unscale_gradients(optimizer=optimizer)\n",
            "        ```\n",
            "        \"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1957,
        "API": ".backward(",
        "context": [
            "        >>> for input, target in dataloader:\n",
            "        ...     optimizer.zero_grad()\n",
            "        ...     output = model(input)\n",
            "        ...     loss = loss_func(output, target)\n",
            "        ...     accelerator.backward(loss)\n",
            "        ...     if accelerator.sync_gradients:\n",
            "        ...         accelerator.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
            "        ...     optimizer.step()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1970,
        "API": ".backward(",
        "context": [
            "            for model in self._models:\n",
            "                if parameters == [p for p in model.parameters()]:\n",
            "                    return model.clip_grad_norm_(max_norm, norm_type)\n",
            "        elif self.distributed_type == DistributedType.DEEPSPEED:\n",
            "            # `accelerator.backward(loss)` is doing that automatically. Therefore, its implementation is not needed\n",
            "            # We cannot return the gradient norm because DeepSpeed does it.\n",
            "            return None\n",
            "        self.unscale_gradients()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 1992,
        "API": ".backward(",
        "context": [
            "        >>> for input, target in dataloader:\n",
            "        ...     optimizer.zero_grad()\n",
            "        ...     output = model(input)\n",
            "        ...     loss = loss_func(output, target)\n",
            "        ...     accelerator.backward(loss)\n",
            "        ...     if accelerator.sync_gradients:\n",
            "        ...         accelerator.clip_grad_value_(model.parameters(), clip_value)\n",
            "        ...     optimizer.step()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2028,
        "API": ".gather(",
        "context": [
            "        >>> from accelerate import Accelerator\n",
            "\n",
            "        >>> accelerator = Accelerator()\n",
            "        >>> process_tensor = torch.tensor([accelerator.process_index])\n",
            "        >>> gathered_tensor = accelerator.gather(process_tensor)\n",
            "        >>> gathered_tensor\n",
            "        tensor([0, 1, 2, 3])\n",
            "        ```\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2060,
        "API": ".gather(",
        "context": [
            "        >>> len(gathered_items)\n",
            "        9\n",
            "        ```\n",
            "        \"\"\"\n",
            "        tensor = self.gather(tensor)\n",
            "        if self.gradient_state.remainder == -1:\n",
            "            logger.info(\n",
            "                \"The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2106,
        "API": ".arange(",
        "context": [
            "        >>> import torch\n",
            "        >>> from accelerate import Accelerator\n",
            "\n",
            "        >>> accelerator = Accelerator()\n",
            "        >>> process_tensor = torch.arange(accelerator.num_processes) + 1 + (2 * accelerator.process_index)\n",
            "        >>> process_tensor = process_tensor.to(accelerator.device)\n",
            "        >>> reduced_tensor = accelerator.reduce(process_tensor, reduction=\"sum\")\n",
            "        >>> reduced_tensor\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2142,
        "API": ".to(",
        "context": [
            "        >>> import torch\n",
            "        >>> from accelerate import Accelerator\n",
            "\n",
            "        >>> accelerator = Accelerator()\n",
            "        >>> process_tensor = torch.arange(accelerator.process_index + 1).to(accelerator.device)\n",
            "        >>> padded_tensor = accelerator.pad_across_processes(process_tensor)\n",
            "        >>> padded_tensor.shape\n",
            "        torch.Size([2])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2247,
        "API": ".get(",
        "context": [
            "                tracker_init = LOGGER_TYPE_TO_CLASS[str(tracker)]\n",
            "                if getattr(tracker_init, \"requires_logging_directory\"):\n",
            "                    # We can skip this check since it was done in `__init__`\n",
            "                    self.trackers.append(\n",
            "                        tracker_init(project_name, self.logging_dir, **init_kwargs.get(str(tracker), {}))\n",
            "                    )\n",
            "                else:\n",
            "                    self.trackers.append(tracker_init(project_name, **init_kwargs.get(str(tracker), {})))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2311,
        "API": ".log(",
        "context": [
            "        >>> from accelerate import Accelerator\n",
            "\n",
            "        >>> accelerator = Accelerator(log_with=\"tensorboard\")\n",
            "        >>> accelerator.init_trackers(\"my_project\")\n",
            "        >>> accelerator.log({\"loss\": 0.5, \"accuracy\": 0.9})\n",
            "        ```\n",
            "        \"\"\"\n",
            "        for tracker in self.trackers:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2352,
        "API": ".save(",
        "context": [
            "        >>> from accelerate import Accelerator\n",
            "\n",
            "        >>> accelerator = Accelerator()\n",
            "        >>> arr = [0, 1, 2, 3]\n",
            "        >>> accelerator.save(arr, \"array.pkl\")\n",
            "        ```\n",
            "        \"\"\"\n",
            "        save(obj, f)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2401,
        "API": ".error(",
        "context": [
            "        if safe_serialization and not is_safetensors_available():\n",
            "            raise ImportError(\"`safe_serialization` requires the `safetensors library: `pip install safetensors`.\")\n",
            "\n",
            "        if os.path.isfile(save_directory):\n",
            "            logger.error(f\"Provided path ({save_directory}) should be a directory, not a file\")\n",
            "            return\n",
            "\n",
            "        os.makedirs(save_directory, exist_ok=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2433,
        "API": ".add(",
        "context": [
            "                    if name in state_dict:\n",
            "                        found += 1\n",
            "                        if found > 1:\n",
            "                            del state_dict[name]\n",
            "                            warn_names.add(name)\n",
            "            if len(warn_names) > 0:\n",
            "                logger.warning_once(\n",
            "                    f\"Removed shared tensor {warn_names} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\",\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2446,
        "API": ".join(",
        "context": [
            "        shards, index = shard_checkpoint(state_dict, max_shard_size=max_shard_size, weights_name=weights_name)\n",
            "\n",
            "        # Clean the folder from a previous save\n",
            "        for filename in os.listdir(save_directory):\n",
            "            full_filename = os.path.join(save_directory, filename)\n",
            "            # If we have a shard file that is not going to be replaced, we delete it, but only from the main process\n",
            "            # in distributed settings to avoid race conditions.\n",
            "            weights_no_suffix = weights_name.replace(\".bin\", \"\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2453,
        "API": ".compile(",
        "context": [
            "            weights_no_suffix = weights_name.replace(\".bin\", \"\")\n",
            "\n",
            "            # make sure that file to be deleted matches format of sharded file, e.g. pytorch_model-00001-of-00005\n",
            "            filename_no_suffix = filename.replace(\".bin\", \"\")\n",
            "            reg = re.compile(r\"(.*?)-\\d{5}-of-\\d{5}\")\n",
            "\n",
            "            if (\n",
            "                filename.startswith(weights_no_suffix)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2466,
        "API": ".join(",
        "context": [
            "                os.remove(full_filename)\n",
            "\n",
            "        # Save the model\n",
            "        for shard_file, shard in shards.items():\n",
            "            self.save(shard, os.path.join(save_directory, shard_file))\n",
            "\n",
            "        if index is None:\n",
            "            path_to_weights = os.path.join(save_directory, WEIGHTS_NAME)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2473,
        "API": ".join(",
        "context": [
            "            path_to_weights = os.path.join(save_directory, WEIGHTS_NAME)\n",
            "            logger.info(f\"Model weights saved in {path_to_weights}\")\n",
            "        else:\n",
            "            save_index_file = SAFE_WEIGHTS_INDEX_NAME if safe_serialization else WEIGHTS_INDEX_NAME\n",
            "            save_index_file = os.path.join(save_directory, save_index_file)\n",
            "            # Save the index as well\n",
            "            with open(save_index_file, \"w\", encoding=\"utf-8\") as f:\n",
            "                content = json.dumps(index, indent=2, sort_keys=True) + \"\\n\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2478,
        "API": ".info(",
        "context": [
            "            # Save the index as well\n",
            "            with open(save_index_file, \"w\", encoding=\"utf-8\") as f:\n",
            "                content = json.dumps(index, indent=2, sort_keys=True) + \"\\n\"\n",
            "                f.write(content)\n",
            "            logger.info(\n",
            "                f\"The model is bigger than the maximum size per checkpoint ({max_shard_size}) and is going to be \"\n",
            "                f\"split in {len(shards)} checkpoint shards. You can find where each parameters has been saved in the \"\n",
            "                f\"index located at {save_index_file}.\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2553,
        "API": ".join(",
        "context": [
            "        >>> accelerator.save_state(output_dir=\"my_checkpoint\")\n",
            "        ```\n",
            "        \"\"\"\n",
            "        if self.project_configuration.automatic_checkpoint_naming:\n",
            "            output_dir = os.path.join(self.project_dir, \"checkpoints\")\n",
            "        os.makedirs(output_dir, exist_ok=True)\n",
            "        if self.project_configuration.automatic_checkpoint_naming:\n",
            "            folders = [os.path.join(output_dir, folder) for folder in os.listdir(output_dir)]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2564,
        "API": ".sort(",
        "context": [
            "\n",
            "                def _inner(folder):\n",
            "                    return list(map(int, re.findall(r\"[\\/]?([0-9]+)(?=[^\\/]*$)\", folder)))[0]\n",
            "\n",
            "                folders.sort(key=_inner)\n",
            "                logger.warning(\n",
            "                    f\"Deleting {len(folders) + 1 - self.project_configuration.total_limit} checkpoints to make room for new checkpoint.\"\n",
            "                )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2570,
        "API": ".join(",
        "context": [
            "                    f\"Deleting {len(folders) + 1 - self.project_configuration.total_limit} checkpoints to make room for new checkpoint.\"\n",
            "                )\n",
            "                for folder in folders[: len(folders) + 1 - self.project_configuration.total_limit]:\n",
            "                    shutil.rmtree(folder)\n",
            "            output_dir = os.path.join(output_dir, f\"checkpoint_{self.save_iteration}\")\n",
            "            if os.path.exists(output_dir):\n",
            "                raise ValueError(\n",
            "                    f\"Checkpoint directory {output_dir} ({self.save_iteration}) already exists. Please manually override `self.save_iteration` with what iteration to start with.\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2575,
        "API": ".makedirs(",
        "context": [
            "            if os.path.exists(output_dir):\n",
            "                raise ValueError(\n",
            "                    f\"Checkpoint directory {output_dir} ({self.save_iteration}) already exists. Please manually override `self.save_iteration` with what iteration to start with.\"\n",
            "                )\n",
            "        os.makedirs(output_dir, exist_ok=True)\n",
            "        logger.info(f\"Saving current state to {output_dir}\")\n",
            "\n",
            "        if self.distributed_type == DistributedType.TPU:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2586,
        "API": ".info(",
        "context": [
            "        # Save the models taking care of FSDP and DeepSpeed nuances\n",
            "        weights = []\n",
            "        for i, model in enumerate(self._models):\n",
            "            if self.distributed_type == DistributedType.FSDP:\n",
            "                logger.info(\"Saving FSDP model\")\n",
            "                save_fsdp_model(self.state.fsdp_plugin, self, model, output_dir, i)\n",
            "                logger.info(f\"FSDP Model saved to output dir {output_dir}\")\n",
            "            elif self.distributed_type == DistributedType.DEEPSPEED:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2593,
        "API": ".join(",
        "context": [
            "            elif self.distributed_type == DistributedType.DEEPSPEED:\n",
            "                logger.info(\"Saving DeepSpeed Model and Optimizer\")\n",
            "                ckpt_id = f\"{MODEL_NAME}\" if i == 0 else f\"{MODEL_NAME}_{i}\"\n",
            "                model.save_checkpoint(output_dir, ckpt_id, **save_model_func_kwargs)\n",
            "                logger.info(f\"DeepSpeed Model and Optimizer saved to output dir {os.path.join(output_dir, ckpt_id)}\")\n",
            "            elif self.distributed_type == DistributedType.MEGATRON_LM:\n",
            "                logger.info(\"Saving Megatron-LM Model, Optimizer and Scheduler\")\n",
            "                model.save_checkpoint(output_dir)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2605,
        "API": ".info(",
        "context": [
            "        # Save the optimizers taking care of FSDP and DeepSpeed nuances\n",
            "        optimizers = []\n",
            "        if self.distributed_type == DistributedType.FSDP:\n",
            "            for opt in self._optimizers:\n",
            "                logger.info(\"Saving FSDP Optimizer\")\n",
            "                save_fsdp_optimizer(self.state.fsdp_plugin, self, opt, self._models[i], output_dir, i)\n",
            "                logger.info(f\"FSDP Optimizer saved to output dir {output_dir}\")\n",
            "        elif self.distributed_type not in [DistributedType.DEEPSPEED, DistributedType.MEGATRON_LM]:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2699,
        "API": ".info(",
        "context": [
            "        # Check if folder exists\n",
            "        input_dir = os.path.expanduser(input_dir)\n",
            "        if not os.path.isdir(input_dir):\n",
            "            raise ValueError(f\"Tried to find {input_dir} but folder does not exist\")\n",
            "        logger.info(f\"Loading states from {input_dir}\")\n",
            "\n",
            "        # Load the models taking care of FSDP and DeepSpeed nuances\n",
            "        models = []\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2705,
        "API": ".info(",
        "context": [
            "        # Load the models taking care of FSDP and DeepSpeed nuances\n",
            "        models = []\n",
            "        for i, model in enumerate(self._models):\n",
            "            if self.distributed_type == DistributedType.FSDP:\n",
            "                logger.info(\"Loading FSDP model\")\n",
            "                load_fsdp_model(self.state.fsdp_plugin, self, model, input_dir, i)\n",
            "                logger.info(f\"FSDP Model loaded from input dir {input_dir}\")\n",
            "            elif self.distributed_type == DistributedType.DEEPSPEED:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2712,
        "API": ".join(",
        "context": [
            "            elif self.distributed_type == DistributedType.DEEPSPEED:\n",
            "                logger.info(\"Loading DeepSpeed Model and Optimizer\")\n",
            "                ckpt_id = f\"{MODEL_NAME}\" if i == 0 else f\"{MODEL_NAME}_{i}\"\n",
            "                model.load_checkpoint(input_dir, ckpt_id, **load_model_func_kwargs)\n",
            "                logger.info(f\"DeepSpeed Model and Optimizer loaded from input dir {os.path.join(input_dir, ckpt_id)}\")\n",
            "            elif self.distributed_type == DistributedType.MEGATRON_LM:\n",
            "                logger.info(\"Loading Megatron-LM Model, Optimizer and Scheduler\")\n",
            "                model.load_checkpoint(input_dir)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2724,
        "API": ".info(",
        "context": [
            "        # Load the optimizers taking care of FSDP and DeepSpeed nuances\n",
            "        optimizers = []\n",
            "        if self.distributed_type == DistributedType.FSDP:\n",
            "            for i, opt in enumerate(self._optimizers):\n",
            "                logger.info(\"Loading FSDP Optimizer\")\n",
            "                load_fsdp_optimizer(self.state.fsdp_plugin, self, opt, self._models[i], input_dir, i)\n",
            "                logger.info(f\"FSDP Optimizer loaded from input dir {input_dir}\")\n",
            "        elif self.distributed_type not in [DistributedType.DEEPSPEED, DistributedType.MEGATRON_LM]:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2776,
        "API": ".info(",
        "context": [
            "            err += \"Please make sure to only load checkpoints from folders that were created with the same set of registered objects,\"\n",
            "            err += \"or avoid using `custom_checkpoint` in the filename for files in that same directory and load them in manually.\"\n",
            "            raise RuntimeError(err)\n",
            "        else:\n",
            "            logger.info(f\"Loading in {len(custom_checkpoints)} custom states\")\n",
            "            for index, obj in enumerate(self._custom_objects):\n",
            "                load_custom_state(obj, input_dir, index)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 2992,
        "API": ".backward(",
        "context": [
            "        >>> for input, target in skipped_dataloader:\n",
            "        ...     optimizer.zero_grad()\n",
            "        ...     output = model(input)\n",
            "        ...     loss = loss_func(output, target)\n",
            "        ...     accelerator.backward(loss)\n",
            "        ...     optimizer.step()\n",
            "\n",
            "        >>> # subsequent epochs\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\accelerator.py",
        "line_number": 3004,
        "API": ".info(",
        "context": [
            "        \"\"\"\n",
            "        return skip_first_batches(dataloader, num_batches=num_batches)\n",
            "\n",
            "    def __deepcopy__(self, memo):\n",
            "        logger.info(\"Deep copying the `Accelerator` object, note that this will point to the same original object.\")\n",
            "        return self\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\big_modeling.py",
        "line_number": 66,
        "API": ".to(",
        "context": [
            "\n",
            "    <Tip warning={true}>\n",
            "\n",
            "    Any model created under this context manager has no weights. As such you can't do something like\n",
            "    `model.to(some_device)` with it. To load weights inside your empty model, see [`load_checkpoint_and_dispatch`].\n",
            "\n",
            "    </Tip>\n",
            "    \"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\big_modeling.py",
        "line_number": 91,
        "API": ".device(",
        "context": [
            "    ```python\n",
            "    import torch.nn as nn\n",
            "    from accelerate import init_on_device\n",
            "\n",
            "    with init_on_device(device=torch.device(\"cuda\")):\n",
            "        tst = nn.Liner(100, 100)  # on `cuda` device\n",
            "    ```\n",
            "    \"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\big_modeling.py",
        "line_number": 104,
        "API": ".to(",
        "context": [
            "        old_register_parameter(module, name, param)\n",
            "        if param is not None:\n",
            "            param_cls = type(module._parameters[name])\n",
            "            kwargs = module._parameters[name].__dict__\n",
            "            module._parameters[name] = param_cls(module._parameters[name].to(device), **kwargs)\n",
            "\n",
            "    def register_empty_buffer(module, name, buffer, persistent=True):\n",
            "        old_register_buffer(module, name, buffer, persistent=persistent)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\big_modeling.py",
        "line_number": 109,
        "API": ".to(",
        "context": [
            "\n",
            "    def register_empty_buffer(module, name, buffer, persistent=True):\n",
            "        old_register_buffer(module, name, buffer, persistent=persistent)\n",
            "        if buffer is not None:\n",
            "            module._buffers[name] = module._buffers[name].to(device)\n",
            "\n",
            "    # Patch tensor creation\n",
            "    if include_buffers:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\big_modeling.py",
        "line_number": 173,
        "API": ".to(",
        "context": [
            "    \"\"\"\n",
            "    if execution_device is None:\n",
            "        execution_device = next(iter(model.parameters())).device\n",
            "    if state_dict is None:\n",
            "        state_dict = {n: p.to(\"cpu\") for n, p in model.state_dict().items()}\n",
            "\n",
            "    add_hook_to_module(model, AlignDevicesHook(io_same_device=True), append=True)\n",
            "    attach_align_device_hook(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\big_modeling.py",
        "line_number": 259,
        "API": ".join(",
        "context": [
            "            of the forward. This should only be used for classes that have submodules which are registered but not\n",
            "            called directly during the forward, for instance if a `dense` linear layer is registered, but at forward,\n",
            "            `dense.weight` and `dense.bias` are used in some operations instead of calling `dense` directly.\n",
            "    \"\"\"\n",
            "    if not os.path.isdir(offload_dir) or not os.path.isfile(os.path.join(offload_dir, \"index.json\")):\n",
            "        offload_state_dict(offload_dir, model.state_dict())\n",
            "    if execution_device is None:\n",
            "        execution_device = next(iter(model.parameters())).device\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\big_modeling.py",
        "line_number": 344,
        "API": ".join(",
        "context": [
            "        disk_modules = [name for name, device in device_map.items() if device == \"disk\"]\n",
            "        if offload_dir is None and offload_index is None and len(disk_modules) > 0:\n",
            "            raise ValueError(\n",
            "                \"We need an `offload_dir` to dispatch this model according to this `device_map`, the following submodules \"\n",
            "                f\"need to be offloaded: {', '.join(disk_modules)}.\"\n",
            "            )\n",
            "        if (\n",
            "            len(disk_modules) > 0\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\big_modeling.py",
        "line_number": 349,
        "API": ".join(",
        "context": [
            "            )\n",
            "        if (\n",
            "            len(disk_modules) > 0\n",
            "            and offload_index is None\n",
            "            and (not os.path.isdir(offload_dir) or not os.path.isfile(os.path.join(offload_dir, \"index.json\")))\n",
            "        ):\n",
            "            disk_state_dict = extract_submodules_state_dict(model.state_dict(), disk_modules)\n",
            "            offload_state_dict(offload_dir, disk_state_dict)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\big_modeling.py",
        "line_number": 384,
        "API": ".to(",
        "context": [
            "        retie_parameters(model, tied_params)\n",
            "    else:\n",
            "        device = list(device_map.values())[0]\n",
            "        if device != \"disk\":\n",
            "            model.to(device)\n",
            "        else:\n",
            "            raise ValueError(\n",
            "                \"You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\checkpointing.py",
        "line_number": 74,
        "API": ".join(",
        "context": [
            "    \"\"\"\n",
            "    # Model states\n",
            "    for i, state in enumerate(model_states):\n",
            "        weights_name = f\"{MODEL_NAME}.bin\" if i == 0 else f\"{MODEL_NAME}_{i}.bin\"\n",
            "        output_model_file = os.path.join(output_dir, weights_name)\n",
            "        save(state, output_model_file)\n",
            "        logger.info(f\"Model weights saved in {output_model_file}\")\n",
            "    # Optimizer states\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\checkpointing.py",
        "line_number": 81,
        "API": ".join(",
        "context": [
            "    # Optimizer states\n",
            "    for i, opt in enumerate(optimizers):\n",
            "        state = opt.state_dict()\n",
            "        optimizer_name = f\"{OPTIMIZER_NAME}.bin\" if i == 0 else f\"{OPTIMIZER_NAME}_{i}.bin\"\n",
            "        output_optimizer_file = os.path.join(output_dir, optimizer_name)\n",
            "        save(state, output_optimizer_file)\n",
            "        logger.info(f\"Optimizer state saved in {output_optimizer_file}\")\n",
            "    # Scheduler states\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\checkpointing.py",
        "line_number": 88,
        "API": ".join(",
        "context": [
            "    # Scheduler states\n",
            "    for i, scheduler in enumerate(schedulers):\n",
            "        state = scheduler.state_dict()\n",
            "        scheduler_name = f\"{SCHEDULER_NAME}.bin\" if i == 0 else f\"{SCHEDULER_NAME}_{i}.bin\"\n",
            "        output_scheduler_file = os.path.join(output_dir, scheduler_name)\n",
            "        save(state, output_scheduler_file)\n",
            "        logger.info(f\"Scheduler state saved in {output_scheduler_file}\")\n",
            "    # GradScaler state\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\checkpointing.py",
        "line_number": 94,
        "API": ".join(",
        "context": [
            "        logger.info(f\"Scheduler state saved in {output_scheduler_file}\")\n",
            "    # GradScaler state\n",
            "    if scaler is not None:\n",
            "        state = scaler.state_dict()\n",
            "        output_scaler_file = os.path.join(output_dir, SCALER_NAME)\n",
            "        torch.save(state, output_scaler_file)\n",
            "        logger.info(f\"Gradient scaler state saved in {output_scaler_file}\")\n",
            "    # Random number generator states\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\checkpointing.py",
        "line_number": 109,
        "API": ".join(",
        "context": [
            "    else:\n",
            "        states[\"torch_cuda_manual_seed\"] = torch.cuda.get_rng_state_all()\n",
            "    if is_tpu_available():\n",
            "        states[\"xm_seed\"] = xm.get_rng_state()\n",
            "    output_states_file = os.path.join(output_dir, states_name)\n",
            "    torch.save(states, output_states_file)\n",
            "    logger.info(f\"Random states saved in {output_states_file}\")\n",
            "    return output_dir\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\checkpointing.py",
        "line_number": 157,
        "API": ".join(",
        "context": [
            "        map_location = PartialState().device\n",
            "    # Model states\n",
            "    for i, model in enumerate(models):\n",
            "        weights_name = f\"{MODEL_NAME}.bin\" if i == 0 else f\"{MODEL_NAME}_{i}.bin\"\n",
            "        input_model_file = os.path.join(input_dir, weights_name)\n",
            "        models[i].load_state_dict(torch.load(input_model_file, map_location=map_location), **load_model_func_kwargs)\n",
            "    logger.info(\"All model weights loaded successfully\")\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\checkpointing.py",
        "line_number": 164,
        "API": ".join(",
        "context": [
            "\n",
            "    # Optimizer states\n",
            "    for i, opt in enumerate(optimizers):\n",
            "        optimizer_name = f\"{OPTIMIZER_NAME}.bin\" if i == 0 else f\"{OPTIMIZER_NAME}_{i}.bin\"\n",
            "        input_optimizer_file = os.path.join(input_dir, optimizer_name)\n",
            "        optimizer_state = torch.load(input_optimizer_file, map_location=map_location)\n",
            "        optimizers[i].load_state_dict(optimizer_state)\n",
            "    logger.info(\"All optimizer states loaded successfully\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\checkpointing.py",
        "line_number": 172,
        "API": ".join(",
        "context": [
            "\n",
            "    # Scheduler states\n",
            "    for i, scheduler in enumerate(schedulers):\n",
            "        scheduler_name = f\"{SCHEDULER_NAME}.bin\" if i == 0 else f\"{SCHEDULER_NAME}_{i}.bin\"\n",
            "        input_scheduler_file = os.path.join(input_dir, scheduler_name)\n",
            "        scheduler.load_state_dict(torch.load(input_scheduler_file))\n",
            "    logger.info(\"All scheduler states loaded successfully\")\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\checkpointing.py",
        "line_number": 178,
        "API": ".join(",
        "context": [
            "    logger.info(\"All scheduler states loaded successfully\")\n",
            "\n",
            "    # GradScaler state\n",
            "    if scaler is not None:\n",
            "        input_scaler_file = os.path.join(input_dir, SCALER_NAME)\n",
            "        scaler.load_state_dict(torch.load(input_scaler_file))\n",
            "        logger.info(\"GradScaler state loaded successfully\")\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\checkpointing.py",
        "line_number": 184,
        "API": ".join(",
        "context": [
            "        logger.info(\"GradScaler state loaded successfully\")\n",
            "\n",
            "    # Random states\n",
            "    try:\n",
            "        states = torch.load(os.path.join(input_dir, f\"{RNG_STATE_NAME}_{process_index}.pkl\"))\n",
            "        random.setstate(states[\"random_state\"])\n",
            "        np.random.set_state(states[\"numpy_random_seed\"])\n",
            "        torch.set_rng_state(states[\"torch_manual_seed\"])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\checkpointing.py",
        "line_number": 194,
        "API": ".info(",
        "context": [
            "        else:\n",
            "            torch.cuda.set_rng_state_all(states[\"torch_cuda_manual_seed\"])\n",
            "        if is_tpu_available():\n",
            "            xm.set_rng_state(states[\"xm_seed\"])\n",
            "        logger.info(\"All random states loaded successfully\")\n",
            "    except Exception:\n",
            "        logger.info(\"Could not load random states\")\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\checkpointing.py",
        "line_number": 205,
        "API": ".info(",
        "context": [
            "    Saves the state of `obj` to `{path}/custom_checkpoint_{index}.pkl`\n",
            "    \"\"\"\n",
            "    # Should this be the right way to get a qual_name type value from `obj`?\n",
            "    save_location = Path(path) / f\"custom_checkpoint_{index}.pkl\"\n",
            "    logger.info(f\"Saving the state of {get_pretty_name(obj)} to {save_location}\")\n",
            "    torch.save(obj.state_dict(), save_location)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\checkpointing.py",
        "line_number": 214,
        "API": ".info(",
        "context": [
            "    \"\"\"\n",
            "    Loads the state of `obj` at `{path}/custom_checkpoint_{index}.pkl`\n",
            "    \"\"\"\n",
            "    load_location = f\"{path}/custom_checkpoint_{index}.pkl\"\n",
            "    logger.info(f\"Loading the state of {get_pretty_name(obj)} from {load_location}\")\n",
            "    obj.load_state_dict(torch.load(load_location, map_location=\"cpu\"))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\data_loader.py",
        "line_number": 287,
        "API": ".copy(",
        "context": [
            "            if len(current_batch) == real_batch_size:\n",
            "                for i in process_slice:\n",
            "                    yield current_batch[i]\n",
            "                if first_batch is None:\n",
            "                    first_batch = current_batch.copy()\n",
            "                current_batch = []\n",
            "\n",
            "        # Finished if drop_last is True, otherwise complete the last batch with elements from the beginning.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\data_loader.py",
        "line_number": 293,
        "API": ".copy(",
        "context": [
            "\n",
            "        # Finished if drop_last is True, otherwise complete the last batch with elements from the beginning.\n",
            "        if not self.drop_last and len(current_batch) > 0:\n",
            "            if first_batch is None:\n",
            "                first_batch = current_batch.copy()\n",
            "            while len(current_batch) < real_batch_size:\n",
            "                current_batch += first_batch\n",
            "            for i in process_slice:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\data_loader.py",
        "line_number": 612,
        "API": ".ceil(",
        "context": [
            "            return whole_length\n",
            "        elif self._drop_last:\n",
            "            return whole_length // self.state.num_processes\n",
            "        else:\n",
            "            return math.ceil(whole_length / self.state.num_processes)\n",
            "\n",
            "    @property\n",
            "    def total_batch_size(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\hooks.py",
        "line_number": 259,
        "API": ".to(",
        "context": [
            "                name: param.device for name, param in named_module_tensors(module, recurse=self.place_submodules)\n",
            "            }\n",
            "            if self.weights_map is None:\n",
            "                self.weights_map = {\n",
            "                    name: param.to(\"cpu\")\n",
            "                    for name, param in named_module_tensors(\n",
            "                        module, include_buffers=self.offload_buffers, recurse=self.place_submodules\n",
            "                    )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\hooks.py",
        "line_number": 311,
        "API": ".device(",
        "context": [
            "\n",
            "    def detach_hook(self, module):\n",
            "        if self.offload:\n",
            "            for name, device in self.original_devices.items():\n",
            "                if device != torch.device(\"meta\"):\n",
            "                    set_module_tensor_to_device(module, name, device, value=self.weights_map.get(name, None))\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\hooks.py",
        "line_number": 530,
        "API": ".get(",
        "context": [
            "            preload_module_classes=preload_module_classes,\n",
            "            skip_keys=skip_keys,\n",
            "        )\n",
            "    elif module_name == \"\":\n",
            "        hook = AlignDevicesHook(execution_device=execution_device.get(\"\"), io_same_device=True, skip_keys=skip_keys)\n",
            "        add_hook_to_module(module, hook)\n",
            "\n",
            "    for child_name, child in module.named_children():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\hooks.py",
        "line_number": 572,
        "API": ".to(",
        "context": [
            "\n",
            "        self.execution_device = execution_device if execution_device is not None else PartialState().default_device\n",
            "\n",
            "    def init_hook(self, module):\n",
            "        return module.to(\"cpu\")\n",
            "\n",
            "    def pre_forward(self, module, *args, **kwargs):\n",
            "        if self.prev_module_hook is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\hooks.py",
        "line_number": 577,
        "API": ".to(",
        "context": [
            "\n",
            "    def pre_forward(self, module, *args, **kwargs):\n",
            "        if self.prev_module_hook is not None:\n",
            "            self.prev_module_hook.offload()\n",
            "        module.to(self.execution_device)\n",
            "        return send_to_device(args, self.execution_device), send_to_device(kwargs, self.execution_device)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\launchers.py",
        "line_number": 74,
        "API": ".lower(",
        "context": [
            "    elif \"IPython\" in sys.modules:\n",
            "        in_colab = \"google.colab\" in str(sys.modules[\"IPython\"].get_ipython())\n",
            "\n",
            "    try:\n",
            "        mixed_precision = PrecisionType(mixed_precision.lower())\n",
            "    except ValueError:\n",
            "        raise ValueError(\n",
            "            f\"Unknown mixed_precision mode: {args.mixed_precision.lower()}. Choose between {PrecisionType.list()}.\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\launchers.py",
        "line_number": 80,
        "API": ".get(",
        "context": [
            "        raise ValueError(\n",
            "            f\"Unknown mixed_precision mode: {args.mixed_precision.lower()}. Choose between {PrecisionType.list()}.\"\n",
            "        )\n",
            "\n",
            "    if (in_colab or in_kaggle) and (os.environ.get(\"TPU_NAME\", None) is not None):\n",
            "        # TPU launch\n",
            "        import torch_xla.distributed.xla_multiprocessing as xmp\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\launchers.py",
        "line_number": 98,
        "API": ".is_available(",
        "context": [
            "        print(f\"Launching a training on {num_processes} TPU cores.\")\n",
            "        xmp.spawn(launcher, args=args, nprocs=num_processes, start_method=\"fork\")\n",
            "    elif in_colab:\n",
            "        # No need for a distributed launch otherwise as it's either CPU or one GPU.\n",
            "        if torch.cuda.is_available():\n",
            "            print(\"Launching training on one GPU.\")\n",
            "        else:\n",
            "            print(\"Launching training on one CPU.\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\launchers.py",
        "line_number": 121,
        "API": ".is_initialized(",
        "context": [
            "                    \"inside your training function. Restart your notebook and make sure no cells initializes an \"\n",
            "                    \"`Accelerator`.\"\n",
            "                )\n",
            "\n",
            "            if torch.cuda.is_initialized():\n",
            "                raise ValueError(\n",
            "                    \"To launch a multi-GPU training from your notebook, you need to avoid running any instruction \"\n",
            "                    \"using `torch.cuda` in any cell. Restart your notebook and make sure no cells use any CUDA \"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\launchers.py",
        "line_number": 151,
        "API": ".is_available(",
        "context": [
            "            # No need for a distributed launch otherwise as it's either CPU, GPU or MPS.\n",
            "            if is_mps_available():\n",
            "                os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
            "                print(\"Launching training on MPS.\")\n",
            "            elif torch.cuda.is_available():\n",
            "                print(\"Launching training on one GPU.\")\n",
            "            else:\n",
            "                print(\"Launching training on CPU.\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\logging.py",
        "line_number": 59,
        "API": ".log(",
        "context": [
            "\n",
            "        if self.isEnabledFor(level):\n",
            "            if self._should_log(main_process_only):\n",
            "                msg, kwargs = self.process(msg, kwargs)\n",
            "                self.logger.log(level, msg, *args, **kwargs)\n",
            "\n",
            "            elif in_order:\n",
            "                state = PartialState()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\logging.py",
        "line_number": 66,
        "API": ".log(",
        "context": [
            "                state = PartialState()\n",
            "                for i in range(state.num_processes):\n",
            "                    if i == state.process_index:\n",
            "                        msg, kwargs = self.process(msg, kwargs)\n",
            "                        self.logger.log(level, msg, *args, **kwargs)\n",
            "                    state.wait_for_everyone()\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\logging.py",
        "line_number": 90,
        "API": ".info(",
        "context": [
            "    >>> from accelerate.logging import get_logger\n",
            "\n",
            "    >>> logger = get_logger(__name__)\n",
            "\n",
            "    >>> logger.info(\"My log\", main_process_only=False)\n",
            "    >>> logger.debug(\"My log\", main_process_only=True)\n",
            "\n",
            "    >>> logger = get_logger(__name__, log_level=\"DEBUG\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\logging.py",
        "line_number": 95,
        "API": ".debug(",
        "context": [
            "    >>> logger.debug(\"My log\", main_process_only=True)\n",
            "\n",
            "    >>> logger = get_logger(__name__, log_level=\"DEBUG\")\n",
            "    >>> logger.info(\"My log\")\n",
            "    >>> logger.debug(\"My second log\")\n",
            "\n",
            "    >>> from accelerate import Accelerator\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\logging.py",
        "line_number": 102,
        "API": ".info(",
        "context": [
            "\n",
            "    >>> accelerator = Accelerator()\n",
            "    >>> array = [\"a\", \"b\", \"c\", \"d\"]\n",
            "    >>> letter_at_rank = array[accelerator.process_index]\n",
            "    >>> logger.info(letter_at_rank, in_order=True)\n",
            "    ```\n",
            "    \"\"\"\n",
            "    if log_level is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\logging.py",
        "line_number": 109,
        "API": ".upper(",
        "context": [
            "    if log_level is None:\n",
            "        log_level = os.environ.get(\"ACCELERATE_LOG_LEVEL\", None)\n",
            "    logger = logging.getLogger(name)\n",
            "    if log_level is not None:\n",
            "        logger.setLevel(log_level.upper())\n",
            "        logger.root.setLevel(log_level.upper())\n",
            "    return MultiProcessAdapter(logger, {})\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\memory_utils.py",
        "line_number": 17,
        "API": ".warn(",
        "context": [
            "\n",
            "import warnings\n",
            "\n",
            "\n",
            "warnings.warn(\n",
            "    \"memory_utils has been reorganized to utils.memory. Import `find_executable_batchsize` from the main `__init__`: \"\n",
            "    \"`from accelerate import find_executable_batch_size` to avoid this warning.\",\n",
            "    FutureWarning,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\optimizer.py",
        "line_number": 33,
        "API": ".to(",
        "context": [
            "        return honor_type(state, (move_to_device(t, device) for t in state))\n",
            "    elif isinstance(state, dict):\n",
            "        return type(state)({k: move_to_device(v, device) for k, v in state.items()})\n",
            "    elif isinstance(state, torch.Tensor):\n",
            "        return state.to(device)\n",
            "    return state\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\optimizer.py",
        "line_number": 147,
        "API": ".get(",
        "context": [
            "                self.optimizer.step(closure)\n",
            "\n",
            "    def _switch_parameters(self, parameters_map):\n",
            "        for param_group in self.optimizer.param_groups:\n",
            "            param_group[\"params\"] = [parameters_map.get(p, p) for p in param_group[\"params\"]]\n",
            "\n",
            "    @property\n",
            "    def is_overflow(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\optimizer.py",
        "line_number": 152,
        "API": ".warn(",
        "context": [
            "\n",
            "    @property\n",
            "    def is_overflow(self):\n",
            "        \"\"\"Whether or not the optimizer step was done, or skipped because of gradient overflow.\"\"\"\n",
            "        warnings.warn(\n",
            "            \"The `is_overflow` property is deprecated and will be removed in version 1.0 of Accelerate use \"\n",
            "            \"`optimizer.step_was_skipped` instead.\",\n",
            "            FutureWarning,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 123,
        "API": ".get(",
        "context": [
            "        self.__dict__ = self._shared_state\n",
            "        if not self.initialized:\n",
            "            self._cpu = cpu\n",
            "            self.backend = None\n",
            "            env_device = os.environ.get(\"ACCELERATE_TORCH_DEVICE\", None)\n",
            "            self.device = torch.device(env_device) if env_device is not None else None\n",
            "            use_sagemaker_dp = kwargs.pop(\"_use_sagemaker_dp\", None)\n",
            "            if use_sagemaker_dp is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 128,
        "API": ".get(",
        "context": [
            "            self.device = torch.device(env_device) if env_device is not None else None\n",
            "            use_sagemaker_dp = kwargs.pop(\"_use_sagemaker_dp\", None)\n",
            "            if use_sagemaker_dp is None:\n",
            "                use_sagemaker_dp = (\n",
            "                    os.environ.get(\"ACCELERATE_USE_SAGEMAKER\", \"false\") == \"true\"\n",
            "                    and os.environ.get(\"ACCELERATE_SAGEMAKER_DISTRIBUTED_TYPE\") != SageMakerDistributedType.NO\n",
            "                )\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 134,
        "API": ".get(",
        "context": [
            "                )\n",
            "\n",
            "            if use_sagemaker_dp and not cpu:\n",
            "                if (\n",
            "                    os.environ.get(\"ACCELERATE_SAGEMAKER_DISTRIBUTED_TYPE\") == SageMakerDistributedType.DATA_PARALLEL\n",
            "                ) or use_sagemaker_dp:\n",
            "                    self.distributed_type = DistributedType.MULTI_GPU\n",
            "                    import smdistributed.dataparallel.torch.torch_smddp  # noqa\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 139,
        "API": ".is_initialized(",
        "context": [
            "                ) or use_sagemaker_dp:\n",
            "                    self.distributed_type = DistributedType.MULTI_GPU\n",
            "                    import smdistributed.dataparallel.torch.torch_smddp  # noqa\n",
            "\n",
            "                    if not torch.distributed.is_initialized():\n",
            "                        torch.distributed.init_process_group(backend=\"smddp\")\n",
            "                    self.backend = \"smddp\"\n",
            "                    self.num_processes = torch.distributed.get_world_size()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 144,
        "API": ".get(",
        "context": [
            "                        torch.distributed.init_process_group(backend=\"smddp\")\n",
            "                    self.backend = \"smddp\"\n",
            "                    self.num_processes = torch.distributed.get_world_size()\n",
            "                    self.process_index = torch.distributed.get_rank()\n",
            "                    self.local_process_index = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
            "                    if self.device is None:\n",
            "                        self.device = torch.device(\"cuda\", self.local_process_index)\n",
            "                    torch.cuda.set_device(self.device)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 155,
        "API": ".get(",
        "context": [
            "                self.process_index = xm.get_ordinal()\n",
            "                self.local_process_index = xm.get_local_ordinal()\n",
            "                self.device = xm.xla_device()\n",
            "            elif (\n",
            "                os.environ.get(\"ACCELERATE_USE_DEEPSPEED\", \"false\") == \"true\"\n",
            "                and int(os.environ.get(\"LOCAL_RANK\", -1)) != -1\n",
            "                and not cpu\n",
            "            ):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 163,
        "API": ".is_initialized(",
        "context": [
            "                assert (\n",
            "                    is_deepspeed_available()\n",
            "                ), \"DeepSpeed is not available => install it using `pip3 install deepspeed` or build it from source\"\n",
            "                self.distributed_type = DistributedType.DEEPSPEED\n",
            "                if not torch.distributed.is_initialized():\n",
            "                    from deepspeed import comm as dist\n",
            "\n",
            "                    # DeepSpeed always uses nccl\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 172,
        "API": ".get_rank(",
        "context": [
            "                    self.backend = \"nccl\"\n",
            "                    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "\n",
            "                self.num_processes = torch.distributed.get_world_size()\n",
            "                self.process_index = torch.distributed.get_rank()\n",
            "                self.local_process_index = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
            "                if self.device is None:\n",
            "                    if is_xpu_available():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 178,
        "API": ".set_device(",
        "context": [
            "                if self.device is None:\n",
            "                    if is_xpu_available():\n",
            "                        self.device = torch.device(\"xpu\", self.local_process_index)\n",
            "                        if self.device is not None:\n",
            "                            torch.xpu.set_device(self.device)\n",
            "                    else:\n",
            "                        self.device = torch.device(\"cuda\", self.local_process_index)\n",
            "                        if self.device is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 184,
        "API": ".get(",
        "context": [
            "                        self.device = torch.device(\"cuda\", self.local_process_index)\n",
            "                        if self.device is not None:\n",
            "                            torch.cuda.set_device(self.device)\n",
            "                self._mixed_precision = \"no\"  # deepspeed handles mixed_precision using deepspeed_config\n",
            "            elif int(os.environ.get(\"LOCAL_RANK\", -1)) != -1 and not cpu and torch.cuda.is_available():\n",
            "                self.distributed_type = DistributedType.MULTI_GPU\n",
            "                if not torch.distributed.is_initialized():\n",
            "                    self.backend = kwargs.pop(\"backend\", \"nccl\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 191,
        "API": ".init_process_group(",
        "context": [
            "                    self.backend = kwargs.pop(\"backend\", \"nccl\")\n",
            "                    # Special case for `TrainingArguments`, where `backend` will be `None`\n",
            "                    if self.backend is None:\n",
            "                        self.backend = \"nccl\"\n",
            "                    torch.distributed.init_process_group(backend=self.backend, **kwargs)\n",
            "                self.num_processes = torch.distributed.get_world_size()\n",
            "                self.process_index = torch.distributed.get_rank()\n",
            "                self.local_process_index = int(os.environ.get(\"LOCAL_RANK\", -1))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 196,
        "API": ".device(",
        "context": [
            "                self.num_processes = torch.distributed.get_world_size()\n",
            "                self.process_index = torch.distributed.get_rank()\n",
            "                self.local_process_index = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
            "                if self.device is None:\n",
            "                    self.device = torch.device(\"cuda\", self.local_process_index)\n",
            "                torch.cuda.set_device(self.device)\n",
            "            elif is_npu_available() and not cpu and int(os.environ.get(\"LOCAL_RANK\", -1)) != -1:\n",
            "                self.distributed_type = DistributedType.MULTI_NPU\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 204,
        "API": ".init_process_group(",
        "context": [
            "                if not torch.distributed.is_initialized():\n",
            "                    # Backend is not set by the user, we set it here\n",
            "                    kwargs.pop(\"backend\", None)\n",
            "                    self.backend = \"hccl\"\n",
            "                    torch.distributed.init_process_group(backend=self.backend, **kwargs)\n",
            "                self.num_processes = torch.distributed.get_world_size()\n",
            "                self.process_index = torch.distributed.get_rank()\n",
            "                self.local_process_index = int(os.environ.get(\"LOCAL_RANK\", -1))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 209,
        "API": ".device(",
        "context": [
            "                self.num_processes = torch.distributed.get_world_size()\n",
            "                self.process_index = torch.distributed.get_rank()\n",
            "                self.local_process_index = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
            "                if self.device is None:\n",
            "                    self.device = torch.device(\"npu\", self.local_process_index)\n",
            "                torch.npu.set_device(self.device)\n",
            "            elif get_int_from_env([\"PMI_SIZE\", \"OMPI_COMM_WORLD_SIZE\", \"MV2_COMM_WORLD_SIZE\", \"WORLD_SIZE\"], 1) > 1:\n",
            "                if not cpu and is_xpu_available():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 242,
        "API": ".get(",
        "context": [
            "                self.local_process_index = local_rank\n",
            "                os.environ[\"RANK\"] = str(rank)\n",
            "                os.environ[\"WORLD_SIZE\"] = str(size)\n",
            "                os.environ[\"LOCAL_RANK\"] = str(local_rank)\n",
            "                if not os.environ.get(\"MASTER_PORT\", None):\n",
            "                    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
            "                if not os.environ.get(\"MASTER_ADDR\", None):\n",
            "                    if local_size != size and backend != \"mpi\":\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 256,
        "API": ".cpu_count(",
        "context": [
            "                    and get_int_from_env([\"OMP_NUM_THREADS\", \"MKL_NUM_THREADS\"], 0) == 0\n",
            "                ):\n",
            "                    import psutil\n",
            "\n",
            "                    num_cpu_threads_per_process = int(psutil.cpu_count(logical=False) / local_size)\n",
            "                    if num_cpu_threads_per_process == 0:\n",
            "                        num_cpu_threads_per_process = 1\n",
            "                    torch.set_num_threads(num_cpu_threads_per_process)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 264,
        "API": ".is_initialized(",
        "context": [
            "                    warnings.warn(\n",
            "                        f\"OMP_NUM_THREADS/MKL_NUM_THREADS unset, we set it at {num_cpu_threads_per_process} to improve oob\"\n",
            "                        \" performance.\"\n",
            "                    )\n",
            "                if not torch.distributed.is_initialized():\n",
            "                    # Backend is not set by the user, we set it here\n",
            "                    kwargs.pop(\"backend\", None)\n",
            "                    self.backend = backend\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 270,
        "API": ".get_rank(",
        "context": [
            "                    kwargs.pop(\"backend\", None)\n",
            "                    self.backend = backend\n",
            "                    torch.distributed.init_process_group(self.backend, rank=rank, world_size=size, **kwargs)\n",
            "                self.num_processes = torch.distributed.get_world_size()\n",
            "                self.process_index = torch.distributed.get_rank()\n",
            "                if cpu:\n",
            "                    self.device = torch.device(\"cpu\")\n",
            "                elif is_xpu_available():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 275,
        "API": ".set_device(",
        "context": [
            "                if cpu:\n",
            "                    self.device = torch.device(\"cpu\")\n",
            "                elif is_xpu_available():\n",
            "                    self.device = torch.device(\"xpu\", self.local_process_index)\n",
            "                    torch.xpu.set_device(self.device)\n",
            "                else:\n",
            "                    self.device = self.default_device\n",
            "            else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 284,
        "API": ".device(",
        "context": [
            "                self.num_processes = 1\n",
            "                self.process_index = self.local_process_index = 0\n",
            "\n",
            "                if self.device is None:\n",
            "                    self.device = torch.device(\"cpu\") if cpu else self.default_device\n",
            "\n",
            "        self.fork_launched = parse_flag_from_env(\"FORK_LAUNCHED\", 0)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 367,
        "API": ".rendezvous(",
        "context": [
            "            DistributedType.FSDP,\n",
            "        ):\n",
            "            torch.distributed.barrier()\n",
            "        elif self.distributed_type == DistributedType.TPU:\n",
            "            xm.rendezvous(\"accelerate.utils.wait_for_everyone\")\n",
            "\n",
            "    def _goes_first(self, is_main: bool):\n",
            "        if not is_main:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 425,
        "API": ".ceil(",
        "context": [
            "        if isinstance(inputs, dict):\n",
            "            length = len(inputs[list(inputs.keys())[0]])\n",
            "            if not all(len(v) == length for v in inputs.values()):\n",
            "                raise ValueError(\"All values in the dictionary must have the same length\")\n",
            "        num_samples_per_process = math.ceil(len(inputs) / self.num_processes)\n",
            "        start_index = self.process_index * num_samples_per_process\n",
            "        end_index = start_index + num_samples_per_process\n",
            "        if (len(inputs) % self.num_processes != 0) and (self.process_index == self.num_processes - 1):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 664,
        "API": ".is_available(",
        "context": [
            "    @property\n",
            "    def default_device(self) -> torch.device:\n",
            "        \"\"\"\n",
            "        Returns the default device which is:\n",
            "        - MPS if `torch.backends.mps.is_available()` and `torch.backends.mps.is_built()` both return True.\n",
            "        - CUDA if `torch.cuda.is_available()`\n",
            "        - NPU if `is_npu_available()`\n",
            "        - CPU otherwise\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 671,
        "API": ".device(",
        "context": [
            "        - CPU otherwise\n",
            "        \"\"\"\n",
            "        if is_mps_available():\n",
            "            os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
            "            return torch.device(\"mps\")\n",
            "        elif torch.cuda.is_available():\n",
            "            return torch.device(\"cuda\")\n",
            "        elif is_xpu_available():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 677,
        "API": ".device(",
        "context": [
            "            return torch.device(\"cuda\")\n",
            "        elif is_xpu_available():\n",
            "            return torch.device(\"xpu:0\")\n",
            "        elif is_npu_available():\n",
            "            return torch.device(\"npu\")\n",
            "        else:\n",
            "            return torch.device(\"cpu\")\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 727,
        "API": ".lower(",
        "context": [
            "            self.deepspeed_plugin = None\n",
            "            mixed_precision = (\n",
            "                parse_choice_from_env(\"ACCELERATE_MIXED_PRECISION\", \"no\")\n",
            "                if mixed_precision is None\n",
            "                else mixed_precision.lower()\n",
            "            )\n",
            "            if mixed_precision == \"fp8\" and not is_fp8_available():\n",
            "                raise ValueError(\"Using `fp8` precision requires `transformer_engine` to be installed.\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 741,
        "API": ".get(",
        "context": [
            "            # deepspeed handles mixed_precision using deepspeed_config\n",
            "            self._mixed_precision = \"no\" if self.distributed_type == DistributedType.DEEPSPEED else mixed_precision\n",
            "            if self.distributed_type == DistributedType.TPU:\n",
            "                if mixed_precision == \"bf16\":\n",
            "                    if os.environ.get(\"ACCELERATE_DOWNCAST_BF16\"):\n",
            "                        os.environ[\"XLA_USE_BF16\"] = str(0)\n",
            "                        os.environ[\"XLA_DOWNCAST_BF16\"] = str(1)\n",
            "                        self.downcast_bfloat = True\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 749,
        "API": ".get(",
        "context": [
            "                    else:\n",
            "                        os.environ[\"XLA_USE_BF16\"] = str(1)\n",
            "                        os.environ[\"XLA_DOWNCAST_BF16\"] = str(0)\n",
            "                        self.downcast_bfloat = False\n",
            "            elif os.environ.get(\"ACCELERATE_USE_DEEPSPEED\", \"false\") == \"true\" and not cpu:\n",
            "                self.deepspeed_plugin = deepspeed_plugin\n",
            "            elif self.distributed_type == DistributedType.MULTI_GPU:\n",
            "                if os.environ.get(\"ACCELERATE_USE_FSDP\", \"false\") == \"true\":\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 757,
        "API": ".get(",
        "context": [
            "                    self.distributed_type = DistributedType.FSDP\n",
            "                    if self._mixed_precision != \"no\":\n",
            "                        fsdp_plugin.set_mixed_precision(self._mixed_precision)\n",
            "                    self.fsdp_plugin = fsdp_plugin\n",
            "                if os.environ.get(\"ACCELERATE_USE_MEGATRON_LM\", \"false\") == \"true\":\n",
            "                    self.distributed_type = DistributedType.MEGATRON_LM\n",
            "                    megatron_lm_plugin.set_mixed_precision(self._mixed_precision)\n",
            "                    self.megatron_lm_plugin = megatron_lm_plugin\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 790,
        "API": ".format(",
        "context": [
            "        \"Checks if a modification is trying to be made and the `AcceleratorState` has already been initialized\"\n",
            "        if self.initialized:\n",
            "            err = \"AcceleratorState has already been initialized and cannot be changed, restart your runtime completely and pass `{flag}` to `Accelerator()`.\"\n",
            "            if cpu and self.device.type != \"cpu\":\n",
            "                raise ValueError(err.format(flag=\"cpu=True\"))\n",
            "            if (\n",
            "                mixed_precision is not None\n",
            "                and mixed_precision != self._mixed_precision\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 796,
        "API": ".format(",
        "context": [
            "                mixed_precision is not None\n",
            "                and mixed_precision != self._mixed_precision\n",
            "                and self.distributed_type != DistributedType.DEEPSPEED\n",
            "            ):\n",
            "                raise ValueError(err.format(flag=f\"mixed_precision='{mixed_precision}'\"))\n",
            "\n",
            "    # For backward compatibility\n",
            "    @property\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 801,
        "API": ".warn(",
        "context": [
            "\n",
            "    # For backward compatibility\n",
            "    @property\n",
            "    def use_fp16(self):\n",
            "        warnings.warn(\n",
            "            \"The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use \"\n",
            "            \"`AcceleratorState.mixed_precision == 'fp16'` instead.\",\n",
            "            FutureWarning,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 812,
        "API": ".get(",
        "context": [
            "    @property\n",
            "    def mixed_precision(self):\n",
            "        if self.distributed_type == DistributedType.DEEPSPEED:\n",
            "            config = self.deepspeed_plugin.deepspeed_config\n",
            "            if config.get(\"fp16\", {}).get(\"enabled\", False):\n",
            "                mixed_precision = \"fp16\"\n",
            "            elif config.get(\"bf16\", {}).get(\"enabled\", False):\n",
            "                mixed_precision = \"bf16\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 958,
        "API": ".get(",
        "context": [
            "\n",
            "    @property\n",
            "    def num_steps(self) -> int:\n",
            "        \"Returns the number of steps to accumulate over\"\n",
            "        return self.plugin_kwargs.get(\"num_steps\", 1)\n",
            "\n",
            "    @property\n",
            "    def adjust_scheduler(self) -> bool:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 963,
        "API": ".get(",
        "context": [
            "\n",
            "    @property\n",
            "    def adjust_scheduler(self) -> bool:\n",
            "        \"Returns whether the scheduler should be adjusted\"\n",
            "        return self.plugin_kwargs.get(\"adjust_scheduler\", False)\n",
            "\n",
            "    @property\n",
            "    def sync_with_dataloader(self) -> bool:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\state.py",
        "line_number": 968,
        "API": ".get(",
        "context": [
            "\n",
            "    @property\n",
            "    def sync_with_dataloader(self) -> bool:\n",
            "        \"Returns whether the gradients should be synced at the end of the dataloader iteration and the number of total steps reset\"\n",
            "        return self.plugin_kwargs.get(\"sync_with_dataloader\", True)\n",
            "\n",
            "    @property\n",
            "    def initialized(self) -> bool:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 189,
        "API": ".join(",
        "context": [
            "    @on_main_process\n",
            "    def __init__(self, run_name: str, logging_dir: Union[str, os.PathLike], **kwargs):\n",
            "        super().__init__()\n",
            "        self.run_name = run_name\n",
            "        self.logging_dir = os.path.join(logging_dir, run_name)\n",
            "        self.writer = tensorboard.SummaryWriter(self.logging_dir, **kwargs)\n",
            "        logger.debug(f\"Initialized TensorBoard project {self.run_name} logging to {self.logging_dir}\")\n",
            "        logger.debug(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 212,
        "API": ".flush(",
        "context": [
            "                Values to be stored as initial hyperparameters as key-value pairs. The values need to have type `bool`,\n",
            "                `str`, `float`, `int`, or `None`.\n",
            "        \"\"\"\n",
            "        self.writer.add_hparams(values, metric_dict={})\n",
            "        self.writer.flush()\n",
            "        project_run_name = time.time()\n",
            "        dir_name = os.path.join(self.logging_dir, str(project_run_name))\n",
            "        os.makedirs(dir_name, exist_ok=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 220,
        "API": ".error(",
        "context": [
            "        with open(os.path.join(dir_name, \"hparams.yml\"), \"w\") as outfile:\n",
            "            try:\n",
            "                yaml.dump(values, outfile)\n",
            "            except yaml.representer.RepresenterError:\n",
            "                logger.error(\"Serialization to store hyperparameters failed\")\n",
            "                raise\n",
            "        logger.debug(\"Stored initial configuration hyperparameters to TensorBoard and hparams yaml file\")\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 247,
        "API": ".flush(",
        "context": [
            "            elif isinstance(v, str):\n",
            "                self.writer.add_text(k, v, global_step=step, **kwargs)\n",
            "            elif isinstance(v, dict):\n",
            "                self.writer.add_scalars(k, v, global_step=step, **kwargs)\n",
            "        self.writer.flush()\n",
            "        logger.debug(\"Successfully logged to TensorBoard\")\n",
            "\n",
            "    @on_main_process\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 265,
        "API": ".debug(",
        "context": [
            "                Additional key word arguments passed along to the `SummaryWriter.add_image` method.\n",
            "        \"\"\"\n",
            "        for k, v in values.items():\n",
            "            self.writer.add_images(k, v, global_step=step, **kwargs)\n",
            "        logger.debug(\"Successfully logged images to TensorBoard\")\n",
            "\n",
            "    @on_main_process\n",
            "    def finish(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 273,
        "API": ".debug(",
        "context": [
            "        \"\"\"\n",
            "        Closes `TensorBoard` writer\n",
            "        \"\"\"\n",
            "        self.writer.close()\n",
            "        logger.debug(\"TensorBoard writer closed\")\n",
            "\n",
            "\n",
            "class WandBTracker(GeneralTracker):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 295,
        "API": ".init(",
        "context": [
            "    @on_main_process\n",
            "    def __init__(self, run_name: str, **kwargs):\n",
            "        super().__init__()\n",
            "        self.run_name = run_name\n",
            "        self.run = wandb.init(project=self.run_name, **kwargs)\n",
            "        logger.debug(f\"Initialized WandB project {self.run_name}\")\n",
            "        logger.debug(\n",
            "            \"Make sure to log any initial configurations with `self.store_init_configuration` before training!\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 316,
        "API": ".debug(",
        "context": [
            "                Values to be stored as initial hyperparameters as key-value pairs. The values need to have type `bool`,\n",
            "                `str`, `float`, `int`, or `None`.\n",
            "        \"\"\"\n",
            "        wandb.config.update(values)\n",
            "        logger.debug(\"Stored initial configuration hyperparameters to WandB\")\n",
            "\n",
            "    @on_main_process\n",
            "    def log(self, values: dict, step: Optional[int] = None, **kwargs):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 332,
        "API": ".log(",
        "context": [
            "                The run step. If included, the log will be affiliated with this step.\n",
            "            kwargs:\n",
            "                Additional key word arguments passed along to the `wandb.log` method.\n",
            "        \"\"\"\n",
            "        self.run.log(values, step=step, **kwargs)\n",
            "        logger.debug(\"Successfully logged to WandB\")\n",
            "\n",
            "    @on_main_process\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 349,
        "API": ".log(",
        "context": [
            "            kwargs:\n",
            "                Additional key word arguments passed along to the `wandb.log` method.\n",
            "        \"\"\"\n",
            "        for k, v in values.items():\n",
            "            self.log({k: [wandb.Image(image) for image in v]}, step=step, **kwargs)\n",
            "        logger.debug(\"Successfully logged images to WandB\")\n",
            "\n",
            "    @on_main_process\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 380,
        "API": ".log(",
        "context": [
            "                The run step. If included, the log will be affiliated with this step.\n",
            "        \"\"\"\n",
            "\n",
            "        values = {table_name: wandb.Table(columns=columns, data=data, dataframe=dataframe)}\n",
            "        self.log(values, step=step, **kwargs)\n",
            "\n",
            "    @on_main_process\n",
            "    def finish(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 388,
        "API": ".debug(",
        "context": [
            "        \"\"\"\n",
            "        Closes `wandb` writer\n",
            "        \"\"\"\n",
            "        self.run.finish()\n",
            "        logger.debug(\"WandB run closed\")\n",
            "\n",
            "\n",
            "class CometMLTracker(GeneralTracker):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 412,
        "API": ".debug(",
        "context": [
            "    def __init__(self, run_name: str, **kwargs):\n",
            "        super().__init__()\n",
            "        self.run_name = run_name\n",
            "        self.writer = Experiment(project_name=run_name, **kwargs)\n",
            "        logger.debug(f\"Initialized CometML project {self.run_name}\")\n",
            "        logger.debug(\n",
            "            \"Make sure to log any initial configurations with `self.store_init_configuration` before training!\"\n",
            "        )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 432,
        "API": ".debug(",
        "context": [
            "                Values to be stored as initial hyperparameters as key-value pairs. The values need to have type `bool`,\n",
            "                `str`, `float`, `int`, or `None`.\n",
            "        \"\"\"\n",
            "        self.writer.log_parameters(values)\n",
            "        logger.debug(\"Stored initial configuration hyperparameters to CometML\")\n",
            "\n",
            "    @on_main_process\n",
            "    def log(self, values: dict, step: Optional[int] = None, **kwargs):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 458,
        "API": ".debug(",
        "context": [
            "            elif isinstance(v, str):\n",
            "                self.writer.log_other(k, v, **kwargs)\n",
            "            elif isinstance(v, dict):\n",
            "                self.writer.log_metrics(v, step=step, **kwargs)\n",
            "        logger.debug(\"Successfully logged to CometML\")\n",
            "\n",
            "    @on_main_process\n",
            "    def finish(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 466,
        "API": ".debug(",
        "context": [
            "        \"\"\"\n",
            "        Closes `comet-ml` writer\n",
            "        \"\"\"\n",
            "        self.writer.end()\n",
            "        logger.debug(\"CometML run closed\")\n",
            "\n",
            "\n",
            "class AimTracker(GeneralTracker):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 488,
        "API": ".debug(",
        "context": [
            "    def __init__(self, run_name: str, logging_dir: Optional[Union[str, os.PathLike]] = \".\", **kwargs):\n",
            "        self.run_name = run_name\n",
            "        self.writer = Run(repo=logging_dir, **kwargs)\n",
            "        self.writer.name = self.run_name\n",
            "        logger.debug(f\"Initialized Aim project {self.run_name}\")\n",
            "        logger.debug(\n",
            "            \"Make sure to log any initial configurations with `self.store_init_configuration` before training!\"\n",
            "        )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 575,
        "API": ".getenv(",
        "context": [
            "        nested_run: Optional[bool] = False,\n",
            "        run_name: Optional[str] = None,\n",
            "        description: Optional[str] = None,\n",
            "    ):\n",
            "        experiment_name = os.getenv(\"MLFLOW_EXPERIMENT_NAME\", experiment_name)\n",
            "        run_id = os.getenv(\"MLFLOW_RUN_ID\", run_id)\n",
            "        tags = os.getenv(\"MLFLOW_TAGS\", tags)\n",
            "        if isinstance(tags, str):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 581,
        "API": ".getenv(",
        "context": [
            "        tags = os.getenv(\"MLFLOW_TAGS\", tags)\n",
            "        if isinstance(tags, str):\n",
            "            tags = json.loads(tags)\n",
            "\n",
            "        nested_run = os.getenv(\"MLFLOW_NESTED_RUN\", nested_run)\n",
            "\n",
            "        exps = mlflow.search_experiments(filter_string=f\"name = '{experiment_name}'\")\n",
            "        if len(exps) > 0:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 586,
        "API": ".warning(",
        "context": [
            "\n",
            "        exps = mlflow.search_experiments(filter_string=f\"name = '{experiment_name}'\")\n",
            "        if len(exps) > 0:\n",
            "            if len(exps) > 1:\n",
            "                logger.warning(\"Multiple experiments with the same name found. Using first one.\")\n",
            "            experiment_id = exps[0].experiment_id\n",
            "        else:\n",
            "            experiment_id = mlflow.create_experiment(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 604,
        "API": ".debug(",
        "context": [
            "            tags=tags,\n",
            "            description=description,\n",
            "        )\n",
            "\n",
            "        logger.debug(f\"Initialized mlflow experiment {experiment_name}\")\n",
            "        logger.debug(\n",
            "            \"Make sure to log any initial configurations with `self.store_init_configuration` before training!\"\n",
            "        )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 626,
        "API": ".warning(",
        "context": [
            "\n",
            "        for name, value in list(values.items()):\n",
            "            # internally, all values are converted to str in MLflow\n",
            "            if len(str(value)) > mlflow.utils.validation.MAX_PARAM_VAL_LENGTH:\n",
            "                logger.warning(\n",
            "                    f'Trainer is attempting to log a value of \"{value}\" for key \"{name}\" as a parameter. MLflow\\'s'\n",
            "                    f\" log_param() only accepts values no longer than {mlflow.utils.validation.MAX_PARAM_VAL_LENGTH} characters so we dropped this attribute.\"\n",
            "                )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 638,
        "API": ".debug(",
        "context": [
            "        # MLflow cannot log more than 100 values in one go, so we have to split it\n",
            "        for i in range(0, len(values_list), mlflow.utils.validation.MAX_PARAMS_TAGS_PER_BATCH):\n",
            "            mlflow.log_params(dict(values_list[i : i + mlflow.utils.validation.MAX_PARAMS_TAGS_PER_BATCH]))\n",
            "\n",
            "        logger.debug(\"Stored initial configuration hyperparameters to MLflow\")\n",
            "\n",
            "    @on_main_process\n",
            "    def log(self, values: dict, step: Optional[int]):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 656,
        "API": ".warning(",
        "context": [
            "        for k, v in values.items():\n",
            "            if isinstance(v, (int, float)):\n",
            "                metrics[k] = v\n",
            "            else:\n",
            "                logger.warning(\n",
            "                    f'MLflowTracker is attempting to log a value of \"{v}\" of type {type(v)} for key \"{k}\" as a metric. '\n",
            "                    \"MLflow's log_metric() only accepts float and int types so we dropped this attribute.\"\n",
            "                )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 662,
        "API": ".debug(",
        "context": [
            "                    \"MLflow's log_metric() only accepts float and int types so we dropped this attribute.\"\n",
            "                )\n",
            "\n",
            "        mlflow.log_metrics(metrics, step=step)\n",
            "        logger.debug(\"Successfully logged to mlflow\")\n",
            "\n",
            "    @on_main_process\n",
            "    def finish(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\tracking.py",
        "line_number": 729,
        "API": ".debug(",
        "context": [
            "                                        f\"Logging with `{log_type}` requires a `logging_dir` to be passed in.\"\n",
            "                                    )\n",
            "                            loggers.append(log_type)\n",
            "                        else:\n",
            "                            logger.debug(f\"Tried adding logger {log_type}, but package is unavailable in the system.\")\n",
            "\n",
            "    return loggers\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\env.py",
        "line_number": 47,
        "API": ".is_available(",
        "context": [
            "\n",
            "\n",
            "def env_command(args):\n",
            "    pt_version = torch.__version__\n",
            "    pt_cuda_available = torch.cuda.is_available()\n",
            "    pt_xpu_available = is_xpu_available()\n",
            "    pt_npu_available = is_npu_available()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\env.py",
        "line_number": 70,
        "API": ".join(",
        "context": [
            "    if pt_cuda_available:\n",
            "        info[\"GPU type\"] = torch.cuda.get_device_name()\n",
            "\n",
            "    print(\"\\nCopy-and-paste the text below in your GitHub issue\\n\")\n",
            "    print(\"\\n\".join([f\"- {prop}: {val}\" for prop, val in info.items()]))\n",
            "\n",
            "    print(\"- `Accelerate` default config:\" if args.config_file is None else \"- `Accelerate` config passed:\")\n",
            "    accelerate_config_str = (\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\launch.py",
        "line_number": 197,
        "API": ".lower(",
        "context": [
            "    # Dynamo arguments\n",
            "    resource_args.add_argument(\n",
            "        \"--dynamo_backend\",\n",
            "        type=str,\n",
            "        choices=[\"no\"] + [b.lower() for b in DYNAMO_BACKENDS],\n",
            "        help=\"Choose a backend to optimize your training with dynamo, see more at \"\n",
            "        \"https://github.com/pytorch/torchdynamo.\",\n",
            "    )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\launch.py",
        "line_number": 668,
        "API": ".write(",
        "context": [
            "        with open(\".deepspeed_env\", \"a\") as f:\n",
            "            for key, value in current_env.items():\n",
            "                if \";\" in value or \" \" in value:\n",
            "                    continue\n",
            "                f.write(f\"{key}={value}\\n\")\n",
            "\n",
            "        process = subprocess.Popen(cmd, env=current_env)\n",
            "        process.wait()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\launch.py",
        "line_number": 849,
        "API": ".split(",
        "context": [
            "\n",
            "        if args.multi_gpu and args.num_machines is None:\n",
            "            args.num_machines = defaults.num_machines\n",
            "\n",
            "        if len(args.gpu_ids.split(\",\")) < 2 and (args.gpu_ids != \"all\") and args.multi_gpu and args.num_machines <= 1:\n",
            "            raise ValueError(\n",
            "                \"Less than two GPU ids were configured and tried to run on on multiple GPUs. \"\n",
            "                \"Please ensure at least two are specified for `--gpu_ids`, or use `--gpu_ids='all'`.\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\launch.py",
        "line_number": 888,
        "API": ".is_available(",
        "context": [
            "                mp_from_config_flag = True\n",
            "        else:\n",
            "            native_amp = False\n",
            "            err = \"{mode} mixed precision requires {requirement}\"\n",
            "            if args.use_cpu or (args.use_xpu and torch.xpu.is_available()):\n",
            "                native_amp = is_torch_version(\">=\", \"1.10\")\n",
            "            else:\n",
            "                native_amp = is_bf16_available(True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\launch.py",
        "line_number": 893,
        "API": ".format(",
        "context": [
            "                native_amp = is_torch_version(\">=\", \"1.10\")\n",
            "            else:\n",
            "                native_amp = is_bf16_available(True)\n",
            "            if args.mixed_precision == \"bf16\" and not native_amp and not (args.tpu and is_tpu_available()):\n",
            "                raise ValueError(err.format(mode=\"bf16\", requirement=\"PyTorch >= 1.10 and a supported device.\"))\n",
            "\n",
            "        # Silently set the default here\n",
            "        if args.dynamo_backend is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\launch.py",
        "line_number": 938,
        "API": ".cpu_count(",
        "context": [
            "        if args.use_cpu and args.num_processes >= 1:\n",
            "            local_size = get_int_from_env(\n",
            "                [\"MPI_LOCALNRANKS\", \"OMPI_COMM_WORLD_LOCAL_SIZE\", \"MV2_COMM_WORLD_LOCAL_SIZE\"], 1\n",
            "            )\n",
            "            threads_per_process = int(psutil.cpu_count(logical=False) / local_size)\n",
            "            if threads_per_process > 1:\n",
            "                args.num_cpu_threads_per_process = threads_per_process\n",
            "                warned.append(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\launch.py",
        "line_number": 947,
        "API": ".join(",
        "context": [
            "                )\n",
            "\n",
            "    if any(warned):\n",
            "        message = \"The following values were not passed to `accelerate launch` and had defaults used instead:\\n\"\n",
            "        message += \"\\n\".join(warned)\n",
            "        message += (\n",
            "            \"\\nTo avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\"\n",
            "        )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\launch.py",
        "line_number": 962,
        "API": ".join(",
        "context": [
            "    if args.use_deepspeed and not args.cpu:\n",
            "        args.deepspeed_fields_from_accelerate_config = list(defaults.deepspeed_config.keys()) if defaults else []\n",
            "        if mp_from_config_flag:\n",
            "            args.deepspeed_fields_from_accelerate_config.append(\"mixed_precision\")\n",
            "        args.deepspeed_fields_from_accelerate_config = \",\".join(args.deepspeed_fields_from_accelerate_config)\n",
            "        deepspeed_launcher(args)\n",
            "    elif args.use_fsdp and not args.cpu:\n",
            "        multi_gpu_launcher(args)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\tpu.py",
        "line_number": 125,
        "API": ".join(",
        "context": [
            "    new_cmd = [\"cd /usr/share\"]\n",
            "    if args.install_accelerate:\n",
            "        new_cmd += [f\"pip install {args.accelerate_version}\"]\n",
            "    new_cmd += args.command\n",
            "    args.command = \"; \".join(new_cmd)\n",
            "\n",
            "    # Then send it to gcloud\n",
            "    # Eventually try to use google-api-core to do this instead of subprocess\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\tpu.py",
        "line_number": 146,
        "API": ".join(",
        "context": [
            "        \"--worker\",\n",
            "        \"all\",\n",
            "    ]\n",
            "    if args.debug:\n",
            "        print(f\"Running {' '.join(cmd)}\")\n",
            "        return\n",
            "    subprocess.run(cmd)\n",
            "    print(\"Successfully setup pod.\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\config\\cluster.py",
        "line_number": 140,
        "API": ".lower(",
        "context": [
            "    if use_dynamo:\n",
            "        prefix = \"dynamo_\"\n",
            "        dynamo_config[prefix + \"backend\"] = _ask_options(\n",
            "            \"Which dynamo backend would you like to use?\",\n",
            "            [x.lower() for x in DYNAMO_BACKENDS],\n",
            "            _convert_dynamo_backend,\n",
            "            default=2,\n",
            "        )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\config\\cluster.py",
        "line_number": 458,
        "API": ".split(",
        "context": [
            "        DistributedType.MULTI_GPU,\n",
            "        DistributedType.MULTI_NPU,\n",
            "        DistributedType.TPU,\n",
            "    ]:\n",
            "        machine_type = str(distributed_type).split(\".\")[1].replace(\"MULTI_\", \"\")\n",
            "        if machine_type == \"TPU\":\n",
            "            machine_type += \" cores\"\n",
            "        else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\config\\cluster.py",
        "line_number": 564,
        "API": ".split(",
        "context": [
            "                        )\n",
            "            tpu_vm = _ask_field(\n",
            "                \"If not using an instance group, what are the names of the Compute VM instances to be used, seperated by a comma: \",\n",
            "                default=\"\",\n",
            "            ).split(\",\")\n",
            "            tpu_env = _ask_field(\n",
            "                \"What environment variables do you wish to set in each pod, seperated by a comma: \",\n",
            "                default=\"\",\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\config\\config.py",
        "line_number": 71,
        "API": ".makedirs(",
        "context": [
            "    if args.config_file is not None:\n",
            "        config_file = args.config_file\n",
            "    else:\n",
            "        if not os.path.isdir(cache_dir):\n",
            "            os.makedirs(cache_dir)\n",
            "        config_file = default_yaml_config_file\n",
            "\n",
            "    if config_file.endswith(\".json\"):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\config\\config_args.py",
        "line_number": 29,
        "API": ".getenv(",
        "context": [
            "from ...utils.constants import SAGEMAKER_PYTHON_VERSION, SAGEMAKER_PYTORCH_VERSION, SAGEMAKER_TRANSFORMERS_VERSION\n",
            "\n",
            "\n",
            "hf_cache_home = os.path.expanduser(\n",
            "    os.getenv(\"HF_HOME\", os.path.join(os.getenv(\"XDG_CACHE_HOME\", \"~/.cache\"), \"huggingface\"))\n",
            ")\n",
            "cache_dir = os.path.join(hf_cache_home, \"accelerate\")\n",
            "default_json_config_file = os.path.join(cache_dir, \"default_config.yaml\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\config\\config_args.py",
        "line_number": 56,
        "API": ".get(",
        "context": [
            "        config_file = default_config_file\n",
            "    with open(config_file, \"r\", encoding=\"utf-8\") as f:\n",
            "        if config_file.endswith(\".json\"):\n",
            "            if (\n",
            "                json.load(f).get(\"compute_environment\", ComputeEnvironment.LOCAL_MACHINE)\n",
            "                == ComputeEnvironment.LOCAL_MACHINE\n",
            "            ):\n",
            "                config_class = ClusterConfig\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\config\\config_args.py",
        "line_number": 65,
        "API": ".get(",
        "context": [
            "                config_class = SageMakerConfig\n",
            "            return config_class.from_json_file(json_file=config_file)\n",
            "        else:\n",
            "            if (\n",
            "                yaml.safe_load(f).get(\"compute_environment\", ComputeEnvironment.LOCAL_MACHINE)\n",
            "                == ComputeEnvironment.LOCAL_MACHINE\n",
            "            ):\n",
            "                config_class = ClusterConfig\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\config\\config_args.py",
        "line_number": 96,
        "API": ".load(",
        "context": [
            "    @classmethod\n",
            "    def from_json_file(cls, json_file=None):\n",
            "        json_file = default_json_config_file if json_file is None else json_file\n",
            "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
            "            config_dict = json.load(f)\n",
            "        if \"compute_environment\" not in config_dict:\n",
            "            config_dict[\"compute_environment\"] = ComputeEnvironment.LOCAL_MACHINE\n",
            "        if \"mixed_precision\" not in config_dict:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\config\\config_args.py",
        "line_number": 113,
        "API": ".write(",
        "context": [
            "\n",
            "    def to_json_file(self, json_file):\n",
            "        with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
            "            content = json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
            "            f.write(content)\n",
            "\n",
            "    @classmethod\n",
            "    def from_yaml_file(cls, yaml_file=None):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\config\\config_utils.py",
        "line_number": 87,
        "API": ".lower(",
        "context": [
            "    return SageMakerDistributedType([\"NO\", \"DATA_PARALLEL\", \"MODEL_PARALLEL\"][value])\n",
            "\n",
            "\n",
            "def _convert_yes_no_to_bool(value):\n",
            "    return {\"yes\": True, \"no\": False}[value.lower()]\n",
            "\n",
            "\n",
            "class SubcommandHelpFormatter(argparse.RawDescriptionHelpFormatter):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\config\\default.py",
        "line_number": 50,
        "API": ".lower(",
        "context": [
            "        print(\n",
            "            f\"Configuration already exists at {save_location}, will not override. Run `accelerate config` manually or pass a different `save_location`.\"\n",
            "        )\n",
            "        return False\n",
            "    mixed_precision = mixed_precision.lower()\n",
            "    if mixed_precision not in [\"no\", \"fp16\", \"bf16\", \"fp8\"]:\n",
            "        raise ValueError(\n",
            "            f\"`mixed_precision` should be one of 'no', 'fp16', 'bf16', or 'fp8'. Received {mixed_precision}\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\config\\default.py",
        "line_number": 59,
        "API": ".is_available(",
        "context": [
            "    config = {\n",
            "        \"compute_environment\": \"LOCAL_MACHINE\",\n",
            "        \"mixed_precision\": mixed_precision,\n",
            "    }\n",
            "    if torch.cuda.is_available():\n",
            "        num_gpus = torch.cuda.device_count()\n",
            "        config[\"num_processes\"] = num_gpus\n",
            "        config[\"use_cpu\"] = False\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\config\\sagemaker.py",
        "line_number": 140,
        "API": ".lower(",
        "context": [
            "        error_message=\"Please enter yes or no.\",\n",
            "    )\n",
            "    docker_image = None\n",
            "    if is_custom_docker_image:\n",
            "        docker_image = _ask_field(\"Enter your Docker image: \", lambda x: str(x).lower())\n",
            "\n",
            "    is_sagemaker_inputs_enabled = _ask_field(\n",
            "        \"Do you want to provide SageMaker input channels with data locations? [yes/NO]: \",\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\config\\sagemaker.py",
        "line_number": 152,
        "API": ".lower(",
        "context": [
            "    sagemaker_inputs_file = None\n",
            "    if is_sagemaker_inputs_enabled:\n",
            "        sagemaker_inputs_file = _ask_field(\n",
            "            \"Enter the path to the SageMaker inputs TSV file with columns (channel_name, data_location): \",\n",
            "            lambda x: str(x).lower(),\n",
            "        )\n",
            "\n",
            "    is_sagemaker_metrics_enabled = _ask_field(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\config\\sagemaker.py",
        "line_number": 165,
        "API": ".lower(",
        "context": [
            "    sagemaker_metrics_file = None\n",
            "    if is_sagemaker_metrics_enabled:\n",
            "        sagemaker_metrics_file = _ask_field(\n",
            "            \"Enter the path to the SageMaker metrics TSV file with columns (metric_name, metric_regex): \",\n",
            "            lambda x: str(x).lower(),\n",
            "        )\n",
            "\n",
            "    distributed_type = _ask_options(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\config\\sagemaker.py",
        "line_number": 221,
        "API": ".lower(",
        "context": [
            "            ec2_instance_query, SAGEMAKER_PARALLEL_EC2_INSTANCES, lambda x: SAGEMAKER_PARALLEL_EC2_INSTANCES[int(x)]\n",
            "        )\n",
            "    else:\n",
            "        ec2_instance_query += \"? [ml.p3.2xlarge]:\"\n",
            "        ec2_instance_type = _ask_field(ec2_instance_query, lambda x: str(x).lower(), default=\"ml.p3.2xlarge\")\n",
            "\n",
            "    num_machines = 1\n",
            "    if distributed_type in (SageMakerDistributedType.DATA_PARALLEL, SageMakerDistributedType.MODEL_PARALLEL):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\menu\\cursor.py",
        "line_number": 41,
        "API": ".write(",
        "context": [
            "        ctypes.windll.kernel32.GetConsoleCursorInfo(handle, ctypes.byref(ci))\n",
            "        ci.visible = False\n",
            "        ctypes.windll.kernel32.SetConsoleCursorInfo(handle, ctypes.byref(ci))\n",
            "    elif os.name == \"posix\":\n",
            "        sys.stdout.write(\"\\033[?25l\")\n",
            "        sys.stdout.flush()\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\menu\\cursor.py",
        "line_number": 53,
        "API": ".write(",
        "context": [
            "        ctypes.windll.kernel32.GetConsoleCursorInfo(handle, ctypes.byref(ci))\n",
            "        ci.visible = True\n",
            "        ctypes.windll.kernel32.SetConsoleCursorInfo(handle, ctypes.byref(ci))\n",
            "    elif os.name == \"posix\":\n",
            "        sys.stdout.write(\"\\033[?25h\")\n",
            "        sys.stdout.flush()\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\menu\\helpers.py",
        "line_number": 35,
        "API": ".write(",
        "context": [
            "    DOWN = 1\n",
            "\n",
            "\n",
            "def forceWrite(content, end=\"\"):\n",
            "    sys.stdout.write(str(content) + end)\n",
            "    sys.stdout.flush()\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\menu\\helpers.py",
        "line_number": 48,
        "API": ".upper(",
        "context": [
            "    forceWrite(\"\\r\")\n",
            "\n",
            "\n",
            "def move_cursor(num_lines: int, direction: str):\n",
            "    forceWrite(f\"\\033[{num_lines}{CURSOR_TO_CHAR[direction.upper()]}\")\n",
            "\n",
            "\n",
            "def clear_line():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\menu\\input.py",
        "line_number": 75,
        "API": ".get(",
        "context": [
            "        \"Finds and returns the selected character if it exists in the handler\"\n",
            "        char = get_character()\n",
            "        if char != KEYMAP[\"undefined\"]:\n",
            "            char = ord(char)\n",
            "        handler = cls.key_handler.get(char)\n",
            "        if handler:\n",
            "            cls.current_selection = char\n",
            "            return handler(cls)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\menu\\input.py",
        "line_number": 85,
        "API": ".copy(",
        "context": [
            "\n",
            "\n",
            "def register(cls):\n",
            "    \"\"\"Adds KeyHandler metaclass to the class\"\"\"\n",
            "    return KeyHandler(cls.__name__, cls.__bases__, cls.__dict__.copy())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\commands\\menu\\selection_menu.py",
        "line_number": 131,
        "API": ".input(",
        "context": [
            "        with cursor.hide():\n",
            "            while True:\n",
            "                if in_colab:\n",
            "                    try:\n",
            "                        choice = int(builtins.input())\n",
            "                    except ValueError:\n",
            "                        choice = default_choice\n",
            "                else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\test_utils\\examples.py",
        "line_number": 88,
        "API": ".join(",
        "context": [
            "            `complete_nlp_example.py`, the template script should be included here. Such as `examples/cv_example.py`\n",
            "    \"\"\"\n",
            "    with open(base_filename, \"r\") as f:\n",
            "        base_file_contents = f.readlines()\n",
            "    with open(os.path.abspath(os.path.join(\"examples\", \"nlp_example.py\")), \"r\") as f:\n",
            "        full_file_contents = f.readlines()\n",
            "    with open(feature_filename, \"r\") as f:\n",
            "        feature_file_contents = f.readlines()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\test_utils\\training.py",
        "line_number": 25,
        "API": ".normal(",
        "context": [
            "class RegressionDataset:\n",
            "    def __init__(self, a=2, b=3, length=64, seed=None):\n",
            "        rng = np.random.default_rng(seed)\n",
            "        self.length = length\n",
            "        self.x = rng.normal(size=(length,)).astype(np.float32)\n",
            "        self.y = a * self.x + b + rng.normal(scale=0.1, size=(length,)).astype(np.float32)\n",
            "\n",
            "    def __len__(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\test_utils\\training.py",
        "line_number": 70,
        "API": ".unique(",
        "context": [
            "\n",
            "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
            "    data_files = {\"train\": \"tests/test_samples/MRPC/train.csv\", \"validation\": \"tests/test_samples/MRPC/dev.csv\"}\n",
            "    datasets = load_dataset(\"csv\", data_files=data_files)\n",
            "    label_list = datasets[\"train\"].unique(\"label\")\n",
            "\n",
            "    label_to_id = {v: i for i, v in enumerate(label_list)}\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\test_utils\\training.py",
        "line_number": 93,
        "API": ".pad(",
        "context": [
            "\n",
            "    def collate_fn(examples):\n",
            "        # On TPU it's best to pad everything to the same length or training will be very slow.\n",
            "        if accelerator.distributed_type == DistributedType.TPU:\n",
            "            return tokenizer.pad(examples, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
            "        return tokenizer.pad(examples, padding=\"longest\", return_tensors=\"pt\")\n",
            "\n",
            "    # Instantiate dataloaders.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\bnb.py",
        "line_number": 134,
        "API": ".warning(",
        "context": [
            "\n",
            "    model_device = get_parameter_device(model)\n",
            "    if model_device.type != \"meta\":\n",
            "        # quantization of an already loaded model\n",
            "        logger.warning(\n",
            "            \"It is not recommended to quantize a loaded model. \"\n",
            "            \"The model should be instantiated under the `init_empty_weights` context manager.\"\n",
            "        )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\bnb.py",
        "line_number": 143,
        "API": ".to(",
        "context": [
            "        # convert param to the right dtype\n",
            "        dtype = bnb_quantization_config.torch_dtype\n",
            "        for name, param in model.state_dict().items():\n",
            "            if any(module_to_keep_in_fp32 in name for module_to_keep_in_fp32 in keep_in_fp32_modules):\n",
            "                param.to(torch.float32)\n",
            "                if param.dtype != torch.float32:\n",
            "                    name = name.replace(\".weight\", \"\").replace(\".bias\", \"\")\n",
            "                    param = getattr(model, name, None)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\bnb.py",
        "line_number": 148,
        "API": ".to(",
        "context": [
            "                if param.dtype != torch.float32:\n",
            "                    name = name.replace(\".weight\", \"\").replace(\".bias\", \"\")\n",
            "                    param = getattr(model, name, None)\n",
            "                    if param is not None:\n",
            "                        param.to(torch.float32)\n",
            "            elif torch.is_floating_point(param):\n",
            "                param.to(dtype)\n",
            "        if model_device.type == \"cuda\":\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\bnb.py",
        "line_number": 153,
        "API": ".current_device(",
        "context": [
            "            elif torch.is_floating_point(param):\n",
            "                param.to(dtype)\n",
            "        if model_device.type == \"cuda\":\n",
            "            # move everything to cpu in the first place because we can't do quantization if the weights are already on cuda\n",
            "            model.cuda(torch.cuda.current_device())\n",
            "            torch.cuda.empty_cache()\n",
            "        elif torch.cuda.is_available():\n",
            "            model.to(torch.cuda.current_device())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\bnb.py",
        "line_number": 159,
        "API": ".info(",
        "context": [
            "        elif torch.cuda.is_available():\n",
            "            model.to(torch.cuda.current_device())\n",
            "        else:\n",
            "            raise RuntimeError(\"No GPU found. A GPU is needed for quantization.\")\n",
            "        logger.info(\n",
            "            f\"The model device type is {model_device.type}. However, cuda is needed for quantization.\"\n",
            "            \"We move the model to cuda.\"\n",
            "        )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\bnb.py",
        "line_number": 205,
        "API": ".is_available(",
        "context": [
            "def get_quantized_model_device_map(\n",
            "    model, bnb_quantization_config, device_map=None, max_memory=None, no_split_module_classes=None\n",
            "):\n",
            "    if device_map is None:\n",
            "        if torch.cuda.is_available():\n",
            "            device_map = {\"\": torch.cuda.current_device()}\n",
            "        else:\n",
            "            raise RuntimeError(\"No GPU found. A GPU is needed for quantization.\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\bnb.py",
        "line_number": 272,
        "API": ".info(",
        "context": [
            "                        for more details.\n",
            "                        \"\"\"\n",
            "                    )\n",
            "                else:\n",
            "                    logger.info(\n",
            "                        \"Some modules are are offloaded to the CPU or the disk. Note that these modules will be converted to 8-bit\"\n",
            "                    )\n",
            "        del device_map_without_some_modules\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\bnb.py",
        "line_number": 302,
        "API": ".warning(",
        "context": [
            "    model, has_been_replaced = _replace_with_bnb_layers(\n",
            "        model, bnb_quantization_config, modules_to_not_convert, current_key_name\n",
            "    )\n",
            "    if not has_been_replaced:\n",
            "        logger.warning(\n",
            "            \"You are loading your model in 8bit or 4bit but no linear modules were found in your model.\"\n",
            "            \" this can happen for some architectures such as gpt2 that uses Conv1D instead of Linear layers.\"\n",
            "            \" Please double check your model architecture, or submit an issue on github if you think this is\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\bnb.py",
        "line_number": 329,
        "API": ".join(",
        "context": [
            "            current_key_name = []\n",
            "        current_key_name.append(name)\n",
            "        if isinstance(module, nn.Linear) and name not in modules_to_not_convert:\n",
            "            # Check if the current key is not in the `modules_to_not_convert`\n",
            "            current_key_name_str = \".\".join(current_key_name)\n",
            "            proceed = True\n",
            "            for key in modules_to_not_convert:\n",
            "                if (\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\bnb.py",
        "line_number": 445,
        "API": ".split(",
        "context": [
            "        set_module_tensor_to_device(model, param_name, 0, dtype=new_dtype, value=param)\n",
            "        tensor_name = param_name\n",
            "        module = model\n",
            "        if \".\" in tensor_name:\n",
            "            splits = tensor_name.split(\".\")\n",
            "            for split in splits[:-1]:\n",
            "                new_module = getattr(module, split)\n",
            "                if new_module is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\bnb.py",
        "line_number": 466,
        "API": ".empty(",
        "context": [
            "    else:\n",
            "        offload_weight(param, param_name, offload_folder, index=offload_index)\n",
            "        offload_weight(fp16_statistics, param_name.replace(\"weight\", \"SCB\"), offload_folder, index=offload_index)\n",
            "\n",
            "    set_module_tensor_to_device(model, param_name, \"meta\", dtype=new_dtype, value=torch.empty(*param.size()))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 168,
        "API": ".upper(",
        "context": [
            "    amax_compute_algo: str = \"most_recent\"\n",
            "    override_linear_precision: Tuple[bool, bool, bool] = (False, False, False)\n",
            "\n",
            "    def __post_init__(self):\n",
            "        self.fp8_format = self.fp8_format.upper()\n",
            "        if self.fp8_format not in [\"E4M3\", \"HYBRID\"]:\n",
            "            raise ValueError(\"`fp8_format` must be 'E4M3' or 'HYBRID'.\")\n",
            "        if self.amax_compute_algo not in [\"max\", \"most_recent\"]:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 429,
        "API": ".lower(",
        "context": [
            "    \"\"\"\n",
            "\n",
            "    backend: DynamoBackend = field(\n",
            "        default=None,\n",
            "        metadata={\"help\": f\"Possible options are {[b.value.lower() for b in DynamoBackend]}\"},\n",
            "    )\n",
            "    mode: str = field(\n",
            "        default=None, metadata={\"help\": \"Possible options are 'default', 'reduce-overhead' or 'max-autotune'\"}\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 437,
        "API": ".compile(",
        "context": [
            "    )\n",
            "    fullgraph: bool = field(default=None, metadata={\"help\": \"Whether it is ok to break model into several subgraphs\"})\n",
            "    dynamic: bool = field(default=None, metadata={\"help\": \"Whether to use dynamic shape for tracing\"})\n",
            "    options: Any = field(default=None, metadata={\"help\": \"A dictionary of options to pass to the backend.\"})\n",
            "    disable: bool = field(default=False, metadata={\"help\": \"Turn torch.compile() into a no-op for testing\"})\n",
            "\n",
            "    def __post_init__(self):\n",
            "        prefix = \"ACCELERATE_DYNAMO_\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 442,
        "API": ".get(",
        "context": [
            "\n",
            "    def __post_init__(self):\n",
            "        prefix = \"ACCELERATE_DYNAMO_\"\n",
            "        if self.backend is None:\n",
            "            self.backend = os.environ.get(prefix + \"BACKEND\", \"no\")\n",
            "        self.backend = DynamoBackend(self.backend.upper())\n",
            "        if self.mode is None:\n",
            "            self.mode = os.environ.get(prefix + \"MODE\", \"default\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 447,
        "API": ".get(",
        "context": [
            "        self.backend = DynamoBackend(self.backend.upper())\n",
            "        if self.mode is None:\n",
            "            self.mode = os.environ.get(prefix + \"MODE\", \"default\")\n",
            "        if self.fullgraph is None:\n",
            "            self.fullgraph = strtobool(os.environ.get(prefix + \"USE_FULLGRAPH\", \"False\")) == 1\n",
            "        if self.dynamic is None:\n",
            "            self.dynamic = strtobool(os.environ.get(prefix + \"USE_DYNAMIC\", \"False\")) == 1\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 453,
        "API": ".lower(",
        "context": [
            "            self.dynamic = strtobool(os.environ.get(prefix + \"USE_DYNAMIC\", \"False\")) == 1\n",
            "\n",
            "    def to_dict(self):\n",
            "        dynamo_config = copy.deepcopy(self.__dict__)\n",
            "        dynamo_config[\"backend\"] = dynamo_config[\"backend\"].value.lower()\n",
            "        return dynamo_config\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 513,
        "API": ".get(",
        "context": [
            "    def __post_init__(self):\n",
            "        from .deepspeed import HfDeepSpeedConfig\n",
            "\n",
            "        if self.gradient_accumulation_steps is None:\n",
            "            self.gradient_accumulation_steps = int(os.environ.get(\"ACCELERATE_GRADIENT_ACCUMULATION_STEPS\", 1))\n",
            "\n",
            "        if self.gradient_clipping is None:\n",
            "            gradient_clipping = os.environ.get(\"ACCELERATE_GRADIENT_CLIPPING\", \"none\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 521,
        "API": ".get(",
        "context": [
            "            if gradient_clipping != \"none\":\n",
            "                self.gradient_clipping = float(gradient_clipping)\n",
            "\n",
            "        if self.zero_stage is None:\n",
            "            self.zero_stage = int(os.environ.get(\"ACCELERATE_DEEPSPEED_ZERO_STAGE\", 2))\n",
            "\n",
            "        if self.offload_optimizer_device is None:\n",
            "            self.offload_optimizer_device = os.environ.get(\"ACCELERATE_DEEPSPEED_OFFLOAD_OPTIMIZER_DEVICE\", \"none\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 527,
        "API": ".get(",
        "context": [
            "        if self.offload_optimizer_device is None:\n",
            "            self.offload_optimizer_device = os.environ.get(\"ACCELERATE_DEEPSPEED_OFFLOAD_OPTIMIZER_DEVICE\", \"none\")\n",
            "\n",
            "        if self.offload_param_device is None:\n",
            "            self.offload_param_device = os.environ.get(\"ACCELERATE_DEEPSPEED_OFFLOAD_PARAM_DEVICE\", \"none\")\n",
            "\n",
            "        if self.offload_optimizer_nvme_path is None:\n",
            "            self.offload_optimizer_nvme_path = os.environ.get(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 535,
        "API": ".get(",
        "context": [
            "                \"ACCELERATE_DEEPSPEED_OFFLOAD_OPTIMIZER_NVME_PATH\", \"none\"\n",
            "            )\n",
            "\n",
            "        if self.offload_param_nvme_path is None:\n",
            "            self.offload_param_nvme_path = os.environ.get(\"ACCELERATE_DEEPSPEED_OFFLOAD_PARAM_NVME_PATH\", \"none\")\n",
            "\n",
            "        if self.zero3_save_16bit_model is None:\n",
            "            self.zero3_save_16bit_model = (\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 543,
        "API": ".get(",
        "context": [
            "                os.environ.get(\"ACCELERATE_DEEPSPEED_ZERO3_SAVE_16BIT_MODEL\", \"false\") == \"true\"\n",
            "            )\n",
            "\n",
            "        if self.hf_ds_config is None:\n",
            "            self.hf_ds_config = os.environ.get(\"ACCELERATE_DEEPSPEED_CONFIG_FILE\", \"none\")\n",
            "        if (\n",
            "            isinstance(self.hf_ds_config, dict)\n",
            "            or (isinstance(self.hf_ds_config, str) and self.hf_ds_config != \"none\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 606,
        "API": ".get(",
        "context": [
            "        self.deepspeed_config = self.hf_ds_config.config\n",
            "        self.deepspeed_config[\"steps_per_print\"] = float(\"inf\")  # this will stop deepspeed from logging @ stdout\n",
            "        if self.zero3_init_flag is None:\n",
            "            self.zero3_init_flag = (\n",
            "                strtobool(os.environ.get(\"ACCELERATE_DEEPSPEED_ZERO3_INIT\", str(self.hf_ds_config.is_zero3()))) == 1\n",
            "            )\n",
            "        if self.zero3_init_flag and not self.hf_ds_config.is_zero3():\n",
            "            warnings.warn(\"DeepSpeed Zero3 Init flag is only applicable for ZeRO Stage 3. Setting it to False.\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 618,
        "API": ".get(",
        "context": [
            "        config, ds_key = self.hf_ds_config.find_config_node(ds_key_long)\n",
            "        if config is None:\n",
            "            return\n",
            "\n",
            "        if config.get(ds_key) == \"auto\":\n",
            "            if ds_key_long in kwargs:\n",
            "                config[ds_key] = kwargs[ds_key_long]\n",
            "                return\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 632,
        "API": ".get(",
        "context": [
            "\n",
            "        if not must_match:\n",
            "            return\n",
            "\n",
            "        ds_val = config.get(ds_key)\n",
            "        if ds_val is not None and ds_key_long in kwargs:\n",
            "            if ds_val != kwargs[ds_key_long]:\n",
            "                mismatches.append(f\"- ds {ds_key_long}={ds_val} vs arg {ds_key_long}={kwargs[ds_key_long]}\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 650,
        "API": ".join(",
        "context": [
            "                )\n",
            "            else:\n",
            "                self.fill_match(prefix + key, mismatches, must_match=must_match, **kwargs)\n",
            "        if len(mismatches) > 0 and prefix == \"\":\n",
            "            mismatches_msg = \"\\n\".join(mismatches)\n",
            "            raise ValueError(\n",
            "                \"Please correct the following DeepSpeed config values that mismatch kwargs \"\n",
            "                f\" values:\\n{mismatches_msg}\\nThe easiest method is to set these DeepSpeed config values to 'auto'.\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 671,
        "API": ".get(",
        "context": [
            "                ds_config[\"bf16\"] = {\"enabled\": True}\n",
            "\n",
            "        if mixed_precision != \"no\":\n",
            "            diff_dtype = \"bf16\" if mixed_precision == \"fp16\" else \"fp16\"\n",
            "            if str(ds_config.get(diff_dtype, {}).get(\"enabled\", \"False\")).lower() == \"true\":\n",
            "                raise ValueError(\n",
            "                    f\"`--mixed_precision` arg cannot be set to `{mixed_precision}` when `{diff_dtype}` is set in the DeepSpeed config file.\"\n",
            "                )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 735,
        "API": ".lower(",
        "context": [
            "            \"ACCELERATE_DEEPSPEED_ZERO3_SAVE_16BIT_MODEL\",\n",
            "            \"ACCELERATE_MIXED_PRECISION\",\n",
            "        ]\n",
            "        env_variable_names_to_ignore = [\n",
            "            name.replace(\"ACCELERATE_\", \"\").replace(\"DEEPSPEED_\", \"\").lower() for name in env_variable_names_to_ignore\n",
            "        ]\n",
            "\n",
            "        deepspeed_fields_from_accelerate_config = os.environ.get(\"ACCELERATE_CONFIG_DS_FIELDS\", \"\").split(\",\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 865,
        "API": ".get(",
        "context": [
            "        )\n",
            "\n",
            "        prefix = \"FSDP_\"\n",
            "        if self.sharding_strategy is None:\n",
            "            self.sharding_strategy = ShardingStrategy(int(os.environ.get(prefix + \"SHARDING_STRATEGY\", 1)))\n",
            "\n",
            "        if self.cpu_offload is None:\n",
            "            if strtobool(os.environ.get(prefix + \"OFFLOAD_PARAMS\", \"False\")) == 1:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 874,
        "API": ".get(",
        "context": [
            "            else:\n",
            "                self.cpu_offload = CPUOffload(offload_params=False)\n",
            "\n",
            "        if self.backward_prefetch is None:\n",
            "            prefetch_policy = os.environ.get(prefix + \"BACKWARD_PREFETCH\", \"NO_PREFETCH\")\n",
            "            if prefetch_policy != FSDP_BACKWARD_PREFETCH[-1]:\n",
            "                self.backward_prefetch = BackwardPrefetch(FSDP_BACKWARD_PREFETCH.index(prefetch_policy) + 1)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 879,
        "API": ".get(",
        "context": [
            "            if prefetch_policy != FSDP_BACKWARD_PREFETCH[-1]:\n",
            "                self.backward_prefetch = BackwardPrefetch(FSDP_BACKWARD_PREFETCH.index(prefetch_policy) + 1)\n",
            "\n",
            "        if self.state_dict_type is None:\n",
            "            state_dict_type_policy = os.environ.get(prefix + \"STATE_DICT_TYPE\", \"FULL_STATE_DICT\")\n",
            "            self.state_dict_type = StateDictType(FSDP_STATE_DICT_TYPE.index(state_dict_type_policy) + 1)\n",
            "\n",
            "            if self.state_dict_type == StateDictType.FULL_STATE_DICT:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 888,
        "API": ".get(",
        "context": [
            "                    self.state_dict_config = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n",
            "                if self.optim_state_dict_config is None:\n",
            "                    self.optim_state_dict_config = FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=True)\n",
            "\n",
            "        self.use_orig_params = strtobool(os.environ.get(prefix + \"USE_ORIG_PARAMS\", \"False\")) == 1\n",
            "        self.sync_module_states = strtobool(os.environ.get(prefix + \"SYNC_MODULE_STATES\", \"False\")) == 1\n",
            "        self.forward_prefetch = strtobool(os.environ.get(prefix + \"FORWARD_PREFETCH\", \"False\")) == 1\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 916,
        "API": ".get(",
        "context": [
            "    def set_auto_wrap_policy(self, model):\n",
            "        from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy, transformer_auto_wrap_policy\n",
            "\n",
            "        if self.auto_wrap_policy is None:\n",
            "            auto_wrap_policy = os.environ.get(\"FSDP_AUTO_WRAP_POLICY\", \"NO_WRAP\")\n",
            "            if auto_wrap_policy == FSDP_AUTO_WRAP_POLICY[0]:\n",
            "                transformer_cls_names_to_wrap = os.environ.get(\"FSDP_TRANSFORMER_CLS_TO_WRAP\", \"\").split(\",\")\n",
            "                transformer_cls_to_wrap = set()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 925,
        "API": ".add(",
        "context": [
            "                    transformer_cls = FullyShardedDataParallelPlugin.get_module_class_from_name(model, layer_class)\n",
            "                    if transformer_cls is None:\n",
            "                        raise Exception(\"Could not find the transformer layer class to wrap in the model.\")\n",
            "                    else:\n",
            "                        transformer_cls_to_wrap.add(transformer_cls)\n",
            "\n",
            "                self.auto_wrap_policy = functools.partial(\n",
            "                    transformer_auto_wrap_policy,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 933,
        "API": ".get(",
        "context": [
            "                    # Transformer layer class to wrap\n",
            "                    transformer_layer_cls=transformer_cls_to_wrap,\n",
            "                )\n",
            "            elif auto_wrap_policy == FSDP_AUTO_WRAP_POLICY[1]:\n",
            "                min_num_params = int(os.environ.get(\"FSDP_MIN_NUM_PARAMS\", 0))\n",
            "                if min_num_params > 0:\n",
            "                    self.auto_wrap_policy = functools.partial(\n",
            "                        size_based_auto_wrap_policy, min_num_params=min_num_params\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 1115,
        "API": ".get(",
        "context": [
            "\n",
            "    def __post_init__(self):\n",
            "        prefix = \"MEGATRON_LM_\"\n",
            "        if self.tp_degree is None:\n",
            "            self.tp_degree = int(os.environ.get(prefix + \"TP_DEGREE\", 1))\n",
            "        if self.pp_degree is None:\n",
            "            self.pp_degree = int(os.environ.get(prefix + \"PP_DEGREE\", 1))\n",
            "        if self.num_micro_batches is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 1121,
        "API": ".get(",
        "context": [
            "            self.pp_degree = int(os.environ.get(prefix + \"PP_DEGREE\", 1))\n",
            "        if self.num_micro_batches is None:\n",
            "            self.num_micro_batches = int(os.environ.get(prefix + \"NUM_MICRO_BATCHES\", 1))\n",
            "        if self.gradient_clipping is None:\n",
            "            self.gradient_clipping = float(os.environ.get(prefix + \"GRADIENT_CLIPPING\", 1.0))\n",
            "        if self.recompute_activation is None:\n",
            "            self.recompute_activation = strtobool(os.environ.get(prefix + \"RECOMPUTE_ACTIVATION\", \"False\")) == 1\n",
            "        if self.use_distributed_optimizer is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 1126,
        "API": ".get(",
        "context": [
            "        if self.recompute_activation is None:\n",
            "            self.recompute_activation = strtobool(os.environ.get(prefix + \"RECOMPUTE_ACTIVATION\", \"False\")) == 1\n",
            "        if self.use_distributed_optimizer is None:\n",
            "            self.use_distributed_optimizer = (\n",
            "                strtobool(os.environ.get(prefix + \"USE_DISTRIBUTED_OPTIMIZER\", \"False\")) == 1\n",
            "            )\n",
            "        if self.sequence_parallelism is None:\n",
            "            self.sequence_parallelism = strtobool(os.environ.get(prefix + \"SEQUENCE_PARALLELISM\", \"False\")) == 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 1172,
        "API": ".lower(",
        "context": [
            "\n",
            "    def set_network_size_args(self, model, batch_data=None):\n",
            "        # Check if the model is either BERT, GPT or T5 else raise error\n",
            "        # set 'num_layers', 'hidden_size', 'num_attention_heads', 'max_position_embeddings'\n",
            "        if \"megatron-bert\" in model.config.model_type.lower():\n",
            "            model_type_name = \"bert\"\n",
            "            num_layers = model.config.num_hidden_layers\n",
            "            hidden_size = model.config.hidden_size\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 1180,
        "API": ".lower(",
        "context": [
            "            num_attention_heads = model.config.num_attention_heads\n",
            "            max_position_embeddings = model.config.max_position_embeddings\n",
            "            num_labels = model.config.num_labels\n",
            "            orig_vocab_size = model.config.vocab_size\n",
            "            if \"maskedlm\" in model.__class__.__name__.lower():\n",
            "                pretraining_flag = True\n",
            "            if self.seq_length is not None:\n",
            "                if self.encoder_seq_length is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 1193,
        "API": ".lower(",
        "context": [
            "                self.seq_length = batch_data[\"input_ids\"].shape[1]\n",
            "            else:\n",
            "                self.seq_length = max_position_embeddings\n",
            "            self.megatron_lm_default_args[\"seq_length\"] = self.seq_length\n",
            "        elif \"gpt2\" in model.config.model_type.lower():\n",
            "            model_type_name = \"gpt\"\n",
            "            num_layers = model.config.n_layer\n",
            "            hidden_size = model.config.n_embd\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 1203,
        "API": ".warn(",
        "context": [
            "            orig_vocab_size = model.config.vocab_size\n",
            "            pretraining_flag = True\n",
            "            if self.seq_length is not None:\n",
            "                if self.decoder_seq_length is not None:\n",
            "                    warnings.warn(\"Both `seq_length` and `decoder_seq_length` are set. Using `decoder_seq_length`.\")\n",
            "                self.seq_length = self.decoder_seq_length\n",
            "            elif self.decoder_seq_length is not None:\n",
            "                self.seq_length = self.decoder_seq_length\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 1214,
        "API": ".lower(",
        "context": [
            "                self.seq_length = max_position_embeddings\n",
            "            self.megatron_lm_default_args[\"seq_length\"] = self.seq_length\n",
            "            self.megatron_lm_default_args[\"return_logits\"] = self.return_logits\n",
            "            self.megatron_lm_default_args[\"tokenizer_type\"] = \"GPT2BPETokenizer\"\n",
            "        elif \"t5\" in model.config.model_type.lower():\n",
            "            model_type_name = \"t5\"\n",
            "            num_layers = model.config.num_layers\n",
            "            hidden_size = model.config.d_model\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 1269,
        "API": ".lower(",
        "context": [
            "        self.megatron_lm_default_args[\"micro_batch_size\"] = self.micro_batch_size\n",
            "        self.megatron_lm_default_args[\"global_batch_size\"] = self.global_batch_size\n",
            "\n",
            "    def set_optimizer_type(self, optimizer):\n",
            "        optimizer_name = optimizer.__class__.__name__.lower()\n",
            "        if \"adam\" in optimizer_name:\n",
            "            self.megatron_lm_default_args[\"optimizer\"] = \"adam\"\n",
            "            self.megatron_lm_default_args[\"adam_beta1\"] = optimizer.defaults[\"betas\"][0]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 1289,
        "API": ".warn(",
        "context": [
            "        if self.train_iters is None:\n",
            "            self.train_iters = scheduler.total_num_steps // self.megatron_lm_default_args[\"data_parallel_size\"]\n",
            "            if self.train_samples is not None:\n",
            "                self.train_samples = None\n",
            "                warnings.warn(\n",
            "                    \"Ignoring `train_samples` as `train_iters` based on scheduler is being used for training.\"\n",
            "                )\n",
            "        if self.lr_warmup_iters is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 1295,
        "API": ".warn(",
        "context": [
            "                )\n",
            "        if self.lr_warmup_iters is None:\n",
            "            self.lr_warmup_iters = scheduler.warmup_num_steps // self.megatron_lm_default_args[\"data_parallel_size\"]\n",
            "            if self.lr_warmup_samples is not None:\n",
            "                warnings.warn(\n",
            "                    \"Ignoring `lr_warmup_samples` as `lr_warmup_iters` based on scheduler is being used for training.\"\n",
            "                )\n",
            "            self.lr_warmup_samples = 0\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\dataclasses.py",
        "line_number": 1437,
        "API": ".warn(",
        "context": [
            "        if self.load_in_8bit:\n",
            "            self.target_dtype = torch.int8\n",
            "\n",
            "        if self.load_in_4bit and self.llm_int8_threshold != 6.0:\n",
            "            warnings.warn(\"llm_int8_threshold can only be used for model loaded in 8bit\")\n",
            "\n",
            "        if isinstance(self.torch_dtype, str):\n",
            "            if self.torch_dtype == \"fp32\":\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\deepspeed.py",
        "line_number": 48,
        "API": ".load(",
        "context": [
            "            # modified it, it will not be accepted here again, since `auto` values would have been overridden\n",
            "            config = deepcopy(config_file_or_dict)\n",
            "        elif os.path.exists(config_file_or_dict):\n",
            "            with io.open(config_file_or_dict, \"r\", encoding=\"utf-8\") as f:\n",
            "                config = json.load(f)\n",
            "        else:\n",
            "            try:\n",
            "                config_decoded = base64.urlsafe_b64decode(config_file_or_dict).decode(\"utf-8\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\deepspeed.py",
        "line_number": 85,
        "API": ".split(",
        "context": [
            "    def find_config_node(self, ds_key_long):\n",
            "        config = self.config\n",
            "\n",
            "        # find the config node of interest if it exists\n",
            "        nodes = ds_key_long.split(\".\")\n",
            "        ds_key = nodes.pop()\n",
            "        for node in nodes:\n",
            "            config = config.get(node)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\deepspeed.py",
        "line_number": 101,
        "API": ".get(",
        "context": [
            "        \"\"\"\n",
            "        config, ds_key = self.find_config_node(ds_key_long)\n",
            "        if config is None:\n",
            "            return default\n",
            "        return config.get(ds_key, default)\n",
            "\n",
            "    def del_config_sub_tree(self, ds_key_long, must_exist=False):\n",
            "        \"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\deepspeed.py",
        "line_number": 112,
        "API": ".split(",
        "context": [
            "        \"\"\"\n",
            "        config = self.config\n",
            "\n",
            "        # find the config node of interest if it exists\n",
            "        nodes = ds_key_long.split(\".\")\n",
            "        for node in nodes:\n",
            "            parent_config = config\n",
            "            config = config.get(node)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\deepspeed.py",
        "line_number": 166,
        "API": ".backward(",
        "context": [
            "        self.engine = engine\n",
            "\n",
            "    def backward(self, loss, **kwargs):\n",
            "        # runs backpropagation and handles mixed precision\n",
            "        self.engine.backward(loss, **kwargs)\n",
            "\n",
            "        # Deepspeed's `engine.step` performs the following operations:\n",
            "        # - gradient accumulation check\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\deepspeed.py",
        "line_number": 195,
        "API": ".backward(",
        "context": [
            "        super().__init__(optimizer, device_placement=False, scaler=None)\n",
            "        self.__has_overflow__ = hasattr(self.optimizer, \"overflow\")\n",
            "\n",
            "    def zero_grad(self, set_to_none=None):\n",
            "        pass  # `accelerator.backward(loss)` is doing that automatically. Therefore, its implementation is not needed\n",
            "\n",
            "    def step(self):\n",
            "        pass  # `accelerator.backward(loss)` is doing that automatically. Therefore, its implementation is not needed\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\deepspeed.py",
        "line_number": 222,
        "API": ".backward(",
        "context": [
            "    def __init__(self, scheduler, optimizers):\n",
            "        super().__init__(scheduler, optimizers)\n",
            "\n",
            "    def step(self):\n",
            "        pass  # `accelerator.backward(loss)` is doing that automatically. Therefore, its implementation is not needed\n",
            "\n",
            "\n",
            "class DummyOptim:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\environment.py",
        "line_number": 21,
        "API": ".get(",
        "context": [
            "\n",
            "def get_int_from_env(env_keys, default):\n",
            "    \"\"\"Returns the first positive env value found in the `env_keys` list or the default.\"\"\"\n",
            "    for e in env_keys:\n",
            "        val = int(os.environ.get(e, -1))\n",
            "        if val >= 0:\n",
            "            return val\n",
            "    return default\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\environment.py",
        "line_number": 29,
        "API": ".get(",
        "context": [
            "\n",
            "\n",
            "def parse_flag_from_env(key, default=False):\n",
            "    \"\"\"Returns truthy value for `key` from the env if available else the default.\"\"\"\n",
            "    value = os.environ.get(key, str(default))\n",
            "    return strtobool(value) == 1  # As its name indicates `strtobool` actually returns an int...\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\environment.py",
        "line_number": 34,
        "API": ".get(",
        "context": [
            "    return strtobool(value) == 1  # As its name indicates `strtobool` actually returns an int...\n",
            "\n",
            "\n",
            "def parse_choice_from_env(key, default=\"no\"):\n",
            "    value = os.environ.get(key, str(default))\n",
            "    return value\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\fsdp_utils.py",
        "line_number": 34,
        "API": ".makedirs(",
        "context": [
            "logger = get_logger(__name__)\n",
            "\n",
            "\n",
            "def save_fsdp_model(fsdp_plugin, accelerator, model, output_dir, model_index=0):\n",
            "    os.makedirs(output_dir, exist_ok=True)\n",
            "    with FSDP.state_dict_type(\n",
            "        model, fsdp_plugin.state_dict_type, fsdp_plugin.state_dict_config, fsdp_plugin.optim_state_dict_config\n",
            "    ):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\fsdp_utils.py",
        "line_number": 41,
        "API": ".join(",
        "context": [
            "    ):\n",
            "        state_dict = model.state_dict()\n",
            "        if fsdp_plugin.state_dict_type == StateDictType.FULL_STATE_DICT:\n",
            "            weights_name = f\"{MODEL_NAME}.bin\" if model_index == 0 else f\"{MODEL_NAME}_{model_index}.bin\"\n",
            "            output_model_file = os.path.join(output_dir, weights_name)\n",
            "            if accelerator.process_index == 0:\n",
            "                logger.info(f\"Saving model to {output_model_file}\")\n",
            "                torch.save(state_dict, output_model_file)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\fsdp_utils.py",
        "line_number": 52,
        "API": ".join(",
        "context": [
            "                f\"{MODEL_NAME}_rank{accelerator.process_index}.bin\"\n",
            "                if model_index == 0\n",
            "                else f\"{MODEL_NAME}_{model_index}_rank{accelerator.process_index}.bin\"\n",
            "            )\n",
            "            output_model_file = os.path.join(output_dir, weights_name)\n",
            "            logger.info(f\"Saving model to {output_model_file}\")\n",
            "            torch.save(state_dict, output_model_file)\n",
            "            logger.info(f\"Model saved to {output_model_file}\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\fsdp_utils.py",
        "line_number": 57,
        "API": ".join(",
        "context": [
            "            logger.info(f\"Saving model to {output_model_file}\")\n",
            "            torch.save(state_dict, output_model_file)\n",
            "            logger.info(f\"Model saved to {output_model_file}\")\n",
            "        elif fsdp_plugin.state_dict_type == StateDictType.SHARDED_STATE_DICT:\n",
            "            ckpt_dir = os.path.join(output_dir, f\"{MODEL_NAME}_{model_index}\")\n",
            "            os.makedirs(ckpt_dir, exist_ok=True)\n",
            "            logger.info(f\"Saving model to {ckpt_dir}\")\n",
            "            state_dict = {\"model\": state_dict}\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\fsdp_utils.py",
        "line_number": 67,
        "API": ".info(",
        "context": [
            "                state_dict=state_dict,\n",
            "                storage_writer=dist_cp.FileSystemWriter(ckpt_dir),\n",
            "                planner=DefaultSavePlanner(),\n",
            "            )\n",
            "            logger.info(f\"Model saved to {ckpt_dir}\")\n",
            "\n",
            "\n",
            "def load_fsdp_model(fsdp_plugin, accelerator, model, input_dir, model_index=0):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\fsdp_utils.py",
        "line_number": 84,
        "API": ".join(",
        "context": [
            "                        \"initializing FSDP object\"\n",
            "                    )\n",
            "                return\n",
            "            weights_name = f\"{MODEL_NAME}.bin\" if model_index == 0 else f\"{MODEL_NAME}_{model_index}.bin\"\n",
            "            input_model_file = os.path.join(input_dir, weights_name)\n",
            "            logger.info(f\"Loading model from {input_model_file}\")\n",
            "            state_dict = torch.load(input_model_file)\n",
            "            logger.info(f\"Model loaded from {input_model_file}\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\fsdp_utils.py",
        "line_number": 94,
        "API": ".join(",
        "context": [
            "                f\"{MODEL_NAME}_rank{accelerator.process_index}.bin\"\n",
            "                if model_index == 0\n",
            "                else f\"{MODEL_NAME}_{model_index}_rank{accelerator.process_index}.bin\"\n",
            "            )\n",
            "            input_model_file = os.path.join(input_dir, weights_name)\n",
            "            logger.info(f\"Loading model from {input_model_file}\")\n",
            "            state_dict = torch.load(input_model_file)\n",
            "            logger.info(f\"Model loaded from {input_model_file}\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\fsdp_utils.py",
        "line_number": 100,
        "API": ".join(",
        "context": [
            "            state_dict = torch.load(input_model_file)\n",
            "            logger.info(f\"Model loaded from {input_model_file}\")\n",
            "        elif fsdp_plugin.state_dict_type == StateDictType.SHARDED_STATE_DICT:\n",
            "            ckpt_dir = (\n",
            "                os.path.join(input_dir, f\"{MODEL_NAME}_{model_index}\")\n",
            "                if f\"{MODEL_NAME}\" not in input_dir\n",
            "                else input_dir\n",
            "            )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\fsdp_utils.py",
        "line_number": 112,
        "API": ".info(",
        "context": [
            "                storage_reader=dist_cp.FileSystemReader(ckpt_dir),\n",
            "                planner=DefaultLoadPlanner(),\n",
            "            )\n",
            "            state_dict = state_dict[\"model\"]\n",
            "            logger.info(f\"Model loaded from {ckpt_dir}\")\n",
            "        model.load_state_dict(state_dict)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\fsdp_utils.py",
        "line_number": 117,
        "API": ".makedirs(",
        "context": [
            "        model.load_state_dict(state_dict)\n",
            "\n",
            "\n",
            "def save_fsdp_optimizer(fsdp_plugin, accelerator, optimizer, model, output_dir, optimizer_index=0):\n",
            "    os.makedirs(output_dir, exist_ok=True)\n",
            "    with FSDP.state_dict_type(\n",
            "        model, fsdp_plugin.state_dict_type, fsdp_plugin.state_dict_config, fsdp_plugin.optim_state_dict_config\n",
            "    ):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\fsdp_utils.py",
        "line_number": 127,
        "API": ".join(",
        "context": [
            "            if accelerator.process_index == 0:\n",
            "                optim_state_name = (\n",
            "                    f\"{OPTIMIZER_NAME}.bin\" if optimizer_index == 0 else f\"{OPTIMIZER_NAME}_{optimizer_index}.bin\"\n",
            "                )\n",
            "                output_optimizer_file = os.path.join(output_dir, optim_state_name)\n",
            "                logger.info(f\"Saving Optimizer state to {output_optimizer_file}\")\n",
            "                torch.save(optim_state, output_optimizer_file)\n",
            "                logger.info(f\"Optimizer state saved in {output_optimizer_file}\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\fsdp_utils.py",
        "line_number": 132,
        "API": ".join(",
        "context": [
            "                logger.info(f\"Saving Optimizer state to {output_optimizer_file}\")\n",
            "                torch.save(optim_state, output_optimizer_file)\n",
            "                logger.info(f\"Optimizer state saved in {output_optimizer_file}\")\n",
            "        else:\n",
            "            ckpt_dir = os.path.join(output_dir, f\"{OPTIMIZER_NAME}_{optimizer_index}\")\n",
            "            os.makedirs(ckpt_dir, exist_ok=True)\n",
            "            logger.info(f\"Saving Optimizer state to {ckpt_dir}\")\n",
            "            dist_cp.save_state_dict(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\fsdp_utils.py",
        "line_number": 140,
        "API": ".info(",
        "context": [
            "                state_dict={\"optimizer\": optim_state},\n",
            "                storage_writer=dist_cp.FileSystemWriter(ckpt_dir),\n",
            "                planner=DefaultSavePlanner(),\n",
            "            )\n",
            "            logger.info(f\"Optimizer state saved in {ckpt_dir}\")\n",
            "\n",
            "\n",
            "def load_fsdp_optimizer(fsdp_plugin, accelerator, optimizer, model, input_dir, optimizer_index=0):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\fsdp_utils.py",
        "line_number": 156,
        "API": ".join(",
        "context": [
            "            # if accelerator.process_index == 0 or not fsdp_plugin.optim_state_dict_config.rank0_only:\n",
            "            optimizer_name = (\n",
            "                f\"{OPTIMIZER_NAME}.bin\" if optimizer_index == 0 else f\"{OPTIMIZER_NAME}_{optimizer_index}.bin\"\n",
            "            )\n",
            "            input_optimizer_file = os.path.join(input_dir, optimizer_name)\n",
            "            logger.info(f\"Loading Optimizer state from {input_optimizer_file}\")\n",
            "            optim_state = torch.load(input_optimizer_file)\n",
            "            logger.info(f\"Optimizer state loaded from {input_optimizer_file}\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\fsdp_utils.py",
        "line_number": 162,
        "API": ".join(",
        "context": [
            "            optim_state = torch.load(input_optimizer_file)\n",
            "            logger.info(f\"Optimizer state loaded from {input_optimizer_file}\")\n",
            "        else:\n",
            "            ckpt_dir = (\n",
            "                os.path.join(input_dir, f\"{OPTIMIZER_NAME}_{optimizer_index}\")\n",
            "                if f\"{OPTIMIZER_NAME}\" not in input_dir\n",
            "                else input_dir\n",
            "            )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\fsdp_utils.py",
        "line_number": 173,
        "API": ".info(",
        "context": [
            "                optimizer_key=\"optimizer\",\n",
            "                storage_reader=dist_cp.FileSystemReader(ckpt_dir),\n",
            "            )\n",
            "            optim_state = optim_state[\"optimizer\"]\n",
            "            logger.info(f\"Optimizer loaded from {ckpt_dir}\")\n",
            "        flattened_osd = FSDP.optim_state_dict_to_load(optim_state, model, optimizer)\n",
            "        optimizer.load_state_dict(flattened_osd)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\imports.py",
        "line_number": 38,
        "API": ".is_available(",
        "context": [
            "    _tpu_available = False\n",
            "\n",
            "\n",
            "# Cache this result has it's a C FFI call which can be pretty time-consuming\n",
            "_torch_distributed_available = torch.distributed.is_available()\n",
            "\n",
            "\n",
            "def _is_package_available(pkg_name):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\imports.py",
        "line_number": 72,
        "API": ".version(",
        "context": [
            "    )\n",
            "\n",
            "\n",
            "def get_ccl_version():\n",
            "    return importlib.metadata.version(\"oneccl_bind_pt\")\n",
            "\n",
            "\n",
            "def is_fp8_available():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\imports.py",
        "line_number": 83,
        "API": ".is_available(",
        "context": [
            "@lru_cache\n",
            "def is_tpu_available(check_device=True):\n",
            "    \"Checks if `torch_xla` is installed and potentially if a TPU is in the environment\"\n",
            "    # Due to bugs on the amp series GPUs, we disable torch-xla on them\n",
            "    if torch.cuda.is_available():\n",
            "        return False\n",
            "    if _tpu_available and check_device:\n",
            "        try:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\imports.py",
        "line_number": 103,
        "API": ".is_available(",
        "context": [
            "def is_bf16_available(ignore_tpu=False):\n",
            "    \"Checks if bf16 is supported, optionally ignoring the TPU\"\n",
            "    if is_tpu_available():\n",
            "        return not ignore_tpu\n",
            "    if torch.cuda.is_available():\n",
            "        return torch.cuda.is_bf16_supported()\n",
            "    if is_npu_available():\n",
            "        return False\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\imports.py",
        "line_number": 113,
        "API": ".version(",
        "context": [
            "\n",
            "def is_4bit_bnb_available():\n",
            "    package_exists = _is_package_available(\"bitsandbytes\")\n",
            "    if package_exists:\n",
            "        bnb_version = version.parse(importlib.metadata.version(\"bitsandbytes\"))\n",
            "        return compare_versions(bnb_version, \">=\", \"0.39.0\")\n",
            "    return False\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\imports.py",
        "line_number": 121,
        "API": ".version(",
        "context": [
            "\n",
            "def is_8bit_bnb_available():\n",
            "    package_exists = _is_package_available(\"bitsandbytes\")\n",
            "    if package_exists:\n",
            "        bnb_version = version.parse(importlib.metadata.version(\"bitsandbytes\"))\n",
            "        return compare_versions(bnb_version, \">=\", \"0.37.2\")\n",
            "    return False\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\imports.py",
        "line_number": 131,
        "API": ".get(",
        "context": [
            "    return _is_package_available(\"bitsandbytes\")\n",
            "\n",
            "\n",
            "def is_megatron_lm_available():\n",
            "    if strtobool(os.environ.get(\"ACCELERATE_USE_MEGATRON_LM\", \"False\")) == 1:\n",
            "        package_exists = importlib.util.find_spec(\"megatron\") is not None\n",
            "        if package_exists:\n",
            "            try:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\imports.py",
        "line_number": 138,
        "API": ".warn(",
        "context": [
            "            try:\n",
            "                megatron_version = parse(importlib.metadata.version(\"megatron-lm\"))\n",
            "                return compare_versions(megatron_version, \">=\", \"2.2.0\")\n",
            "            except Exception as e:\n",
            "                warnings.warn(f\"Parse Megatron version failed. Exception:{e}\")\n",
            "                return False\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\imports.py",
        "line_number": 177,
        "API": ".warn(",
        "context": [
            "\n",
            "def is_rich_available():\n",
            "    if _is_package_available(\"rich\"):\n",
            "        if \"ACCELERATE_DISABLE_RICH\" in os.environ:\n",
            "            warnings.warn(\n",
            "                \"`ACCELERATE_DISABLE_RICH` is deprecated and will be removed in v0.22.0 and deactivated by default. Please use `ACCELERATE_ENABLE_RICH` if you wish to use `rich`.\"\n",
            "            )\n",
            "            return not parse_flag_from_env(\"ACCELERATE_DISABLE_RICH\", False)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\imports.py",
        "line_number": 198,
        "API": ".is_available(",
        "context": [
            "    return _is_package_available(\"mlflow\")\n",
            "\n",
            "\n",
            "def is_mps_available():\n",
            "    return is_torch_version(\">=\", \"1.12\") and torch.backends.mps.is_available() and torch.backends.mps.is_built()\n",
            "\n",
            "\n",
            "def is_ipex_available():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\imports.py",
        "line_number": 205,
        "API": ".version(",
        "context": [
            "def is_ipex_available():\n",
            "    def get_major_and_minor_from_version(full_version):\n",
            "        return str(version.parse(full_version).major) + \".\" + str(version.parse(full_version).minor)\n",
            "\n",
            "    _torch_version = importlib.metadata.version(\"torch\")\n",
            "    if importlib.util.find_spec(\"intel_extension_for_pytorch\") is None:\n",
            "        return False\n",
            "    _ipex_version = \"N/A\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\imports.py",
        "line_number": 210,
        "API": ".version(",
        "context": [
            "    if importlib.util.find_spec(\"intel_extension_for_pytorch\") is None:\n",
            "        return False\n",
            "    _ipex_version = \"N/A\"\n",
            "    try:\n",
            "        _ipex_version = importlib.metadata.version(\"intel_extension_for_pytorch\")\n",
            "    except importlib.metadata.PackageNotFoundError:\n",
            "        return False\n",
            "    torch_major_and_minor = get_major_and_minor_from_version(_torch_version)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\imports.py",
        "line_number": 216,
        "API": ".warn(",
        "context": [
            "        return False\n",
            "    torch_major_and_minor = get_major_and_minor_from_version(_torch_version)\n",
            "    ipex_major_and_minor = get_major_and_minor_from_version(_ipex_version)\n",
            "    if torch_major_and_minor != ipex_major_and_minor:\n",
            "        warnings.warn(\n",
            "            f\"Intel Extension for PyTorch {ipex_major_and_minor} needs to work with PyTorch {ipex_major_and_minor}.*,\"\n",
            "            f\" but PyTorch {_torch_version} is found. Please switch to the matching version and run again.\"\n",
            "        )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\imports.py",
        "line_number": 237,
        "API": ".is_available(",
        "context": [
            "    if check_device:\n",
            "        try:\n",
            "            # Will raise a RuntimeError if no NPU is found\n",
            "            _ = torch.npu.device_count()\n",
            "            return torch.npu.is_available()\n",
            "        except RuntimeError:\n",
            "            return False\n",
            "    return hasattr(torch, \"npu\") and torch.npu.is_available()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\imports.py",
        "line_number": 263,
        "API": ".is_available(",
        "context": [
            "    if check_device:\n",
            "        try:\n",
            "            # Will raise a RuntimeError if no XPU  is found\n",
            "            _ = torch.xpu.device_count()\n",
            "            return torch.xpu.is_available()\n",
            "        except RuntimeError:\n",
            "            return False\n",
            "    return hasattr(torch, \"xpu\") and torch.xpu.is_available()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\launch.py",
        "line_number": 61,
        "API": ".copy(",
        "context": [
            "            cmd.append(\"-m\")\n",
            "    cmd.append(args.training_script)\n",
            "    cmd.extend(args.training_script_args)\n",
            "\n",
            "    current_env = os.environ.copy()\n",
            "    current_env[\"ACCELERATE_USE_CPU\"] = str(args.cpu or args.use_cpu)\n",
            "    if args.gpu_ids != \"all\" and args.gpu_ids is not None:\n",
            "        if is_xpu_available():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\launch.py",
        "line_number": 78,
        "API": ".lower(",
        "context": [
            "        current_env[\"MASTER_ADDR\"] = args.main_process_ip if args.main_process_ip is not None else \"127.0.0.1\"\n",
            "        current_env[\"MASTER_PORT\"] = str(args.main_process_port) if args.main_process_port is not None else \"29500\"\n",
            "\n",
            "    try:\n",
            "        mixed_precision = PrecisionType(args.mixed_precision.lower())\n",
            "    except ValueError:\n",
            "        raise ValueError(\n",
            "            f\"Unknown mixed_precision mode: {args.mixed_precision.lower()}. Choose between {PrecisionType.list()}.\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\launch.py",
        "line_number": 87,
        "API": ".upper(",
        "context": [
            "\n",
            "    current_env[\"ACCELERATE_MIXED_PRECISION\"] = str(mixed_precision)\n",
            "\n",
            "    try:\n",
            "        dynamo_backend = DynamoBackend(args.dynamo_backend.upper())\n",
            "    except ValueError:\n",
            "        raise ValueError(f\"Unknown dynamo backend: {args.dynamo_backend.upper()}. Choose between {DYNAMO_BACKENDS}.\")\n",
            "    current_env[\"ACCELERATE_DYNAMO_BACKEND\"] = dynamo_backend.value\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\launch.py",
        "line_number": 97,
        "API": ".lower(",
        "context": [
            "    current_env[\"ACCELERATE_DYNAMO_USE_DYNAMIC\"] = str(args.dynamo_use_dynamic)\n",
            "\n",
            "    current_env[\"OMP_NUM_THREADS\"] = str(args.num_cpu_threads_per_process)\n",
            "    if is_ipex_available():\n",
            "        current_env[\"ACCELERATE_USE_IPEX\"] = str(args.ipex).lower()\n",
            "        current_env[\"ACCELERATE_USE_XPU\"] = str(args.use_xpu).lower()\n",
            "    return cmd, current_env\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\launch.py",
        "line_number": 141,
        "API": ".copy(",
        "context": [
            "        setattr(args, \"module\", True)\n",
            "    elif args.no_python:\n",
            "        setattr(args, \"no_python\", True)\n",
            "\n",
            "    current_env = os.environ.copy()\n",
            "    gpu_ids = getattr(args, \"gpu_ids\", \"all\")\n",
            "    if gpu_ids != \"all\" and args.gpu_ids is not None:\n",
            "        if not is_xpu_available():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\launch.py",
        "line_number": 150,
        "API": ".lower(",
        "context": [
            "        elif is_npu_available():\n",
            "            current_env[\"ASCEND_RT_VISIBLE_DEVICES\"] = gpu_ids\n",
            "        else:\n",
            "            current_env[\"CUDA_VISIBLE_DEVICES\"] = gpu_ids\n",
            "    mixed_precision = args.mixed_precision.lower()\n",
            "    try:\n",
            "        mixed_precision = PrecisionType(mixed_precision)\n",
            "    except ValueError:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\launch.py",
        "line_number": 170,
        "API": ".lower(",
        "context": [
            "\n",
            "    if args.use_fsdp:\n",
            "        current_env[\"ACCELERATE_USE_FSDP\"] = \"true\"\n",
            "        current_env[\"FSDP_SHARDING_STRATEGY\"] = str(args.fsdp_sharding_strategy)\n",
            "        current_env[\"FSDP_OFFLOAD_PARAMS\"] = str(args.fsdp_offload_params).lower()\n",
            "        current_env[\"FSDP_MIN_NUM_PARAMS\"] = str(args.fsdp_min_num_params)\n",
            "        if args.fsdp_auto_wrap_policy is not None:\n",
            "            current_env[\"FSDP_AUTO_WRAP_POLICY\"] = str(args.fsdp_auto_wrap_policy)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\launch.py",
        "line_number": 180,
        "API": ".lower(",
        "context": [
            "        if args.fsdp_backward_prefetch_policy is not None:\n",
            "            current_env[\"FSDP_BACKWARD_PREFETCH\"] = str(args.fsdp_backward_prefetch_policy)\n",
            "        if args.fsdp_state_dict_type is not None:\n",
            "            current_env[\"FSDP_STATE_DICT_TYPE\"] = str(args.fsdp_state_dict_type)\n",
            "        current_env[\"FSDP_FORWARD_PREFETCH\"] = str(args.fsdp_forward_prefetch).lower()\n",
            "        current_env[\"FSDP_USE_ORIG_PARAMS\"] = str(args.fsdp_use_orig_params).lower()\n",
            "        current_env[\"FSDP_SYNC_MODULE_STATES\"] = str(args.fsdp_sync_module_states).lower()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\launch.py",
        "line_number": 285,
        "API": ".lower(",
        "context": [
            "            current_env[\"CUDA_VISIBLE_DEVICES\"] = gpu_ids\n",
            "        else:\n",
            "            current_env[\"ZE_AFFINITY_MASK\"] = gpu_ids\n",
            "    try:\n",
            "        mixed_precision = PrecisionType(args.mixed_precision.lower())\n",
            "    except ValueError:\n",
            "        raise ValueError(\n",
            "            f\"Unknown mixed_precision mode: {args.mixed_precision.lower()}. Choose between {PrecisionType.list()}.\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\launch.py",
        "line_number": 293,
        "API": ".lower(",
        "context": [
            "        )\n",
            "\n",
            "    current_env[\"PYTHONPATH\"] = env_var_path_add(\"PYTHONPATH\", os.path.abspath(\".\"))\n",
            "    current_env[\"ACCELERATE_MIXED_PRECISION\"] = str(mixed_precision)\n",
            "    current_env[\"ACCELERATE_CONFIG_DS_FIELDS\"] = str(args.deepspeed_fields_from_accelerate_config).lower()\n",
            "    current_env[\"ACCELERATE_USE_DEEPSPEED\"] = \"true\"\n",
            "    if args.zero_stage is not None:\n",
            "        current_env[\"ACCELERATE_DEEPSPEED_ZERO_STAGE\"] = str(args.zero_stage)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\launch.py",
        "line_number": 300,
        "API": ".lower(",
        "context": [
            "        current_env[\"ACCELERATE_DEEPSPEED_ZERO_STAGE\"] = str(args.zero_stage)\n",
            "    if args.gradient_accumulation_steps is not None:\n",
            "        current_env[\"ACCELERATE_GRADIENT_ACCUMULATION_STEPS\"] = str(args.gradient_accumulation_steps)\n",
            "    if args.gradient_clipping is not None:\n",
            "        current_env[\"ACCELERATE_GRADIENT_CLIPPING\"] = str(args.gradient_clipping).lower()\n",
            "    if args.offload_optimizer_device is not None:\n",
            "        current_env[\"ACCELERATE_DEEPSPEED_OFFLOAD_OPTIMIZER_DEVICE\"] = str(args.offload_optimizer_device).lower()\n",
            "    if args.offload_param_device is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\launch.py",
        "line_number": 306,
        "API": ".lower(",
        "context": [
            "        current_env[\"ACCELERATE_DEEPSPEED_OFFLOAD_OPTIMIZER_DEVICE\"] = str(args.offload_optimizer_device).lower()\n",
            "    if args.offload_param_device is not None:\n",
            "        current_env[\"ACCELERATE_DEEPSPEED_OFFLOAD_PARAM_DEVICE\"] = str(args.offload_param_device).lower()\n",
            "    if args.zero3_init_flag is not None:\n",
            "        current_env[\"ACCELERATE_DEEPSPEED_ZERO3_INIT\"] = str(args.zero3_init_flag).lower()\n",
            "    if args.zero3_save_16bit_model is not None:\n",
            "        current_env[\"ACCELERATE_DEEPSPEED_ZERO3_SAVE_16BIT_MODEL\"] = str(args.zero3_save_16bit_model).lower()\n",
            "    if args.deepspeed_config_file is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\launch.py",
        "line_number": 404,
        "API": ".lower(",
        "context": [
            "    print(\"Converting Arguments to Hyperparameters\")\n",
            "    hyperparameters = _convert_nargs_to_dict(args.training_script_args)\n",
            "\n",
            "    try:\n",
            "        mixed_precision = PrecisionType(args.mixed_precision.lower())\n",
            "    except ValueError:\n",
            "        raise ValueError(\n",
            "            f\"Unknown mixed_precision mode: {args.mixed_precision.lower()}. Choose between {PrecisionType.list()}.\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\launch.py",
        "line_number": 411,
        "API": ".upper(",
        "context": [
            "            f\"Unknown mixed_precision mode: {args.mixed_precision.lower()}. Choose between {PrecisionType.list()}.\"\n",
            "        )\n",
            "\n",
            "    try:\n",
            "        dynamo_backend = DynamoBackend(args.dynamo_backend.upper())\n",
            "    except ValueError:\n",
            "        raise ValueError(f\"Unknown dynamo backend: {args.dynamo_backend.upper()}. Choose between {DYNAMO_BACKENDS}.\")\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\launch.py",
        "line_number": 439,
        "API": ".split(",
        "context": [
            "        with open(sagemaker_config.sagemaker_inputs_file) as file:\n",
            "            for i, line in enumerate(file):\n",
            "                if i == 0:\n",
            "                    continue\n",
            "                l = line.split(\"\\t\")\n",
            "                sagemaker_inputs[l[0]] = l[1].strip()\n",
            "        print(f\"Loaded SageMaker Inputs: {sagemaker_inputs}\")\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\launch.py",
        "line_number": 452,
        "API": ".split(",
        "context": [
            "        with open(sagemaker_config.sagemaker_metrics_file) as file:\n",
            "            for i, line in enumerate(file):\n",
            "                if i == 0:\n",
            "                    continue\n",
            "                l = line.split(\"\\t\")\n",
            "                metric_dict = {\n",
            "                    \"Name\": l[0],\n",
            "                    \"Regex\": l[1].strip(),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\launch.py",
        "line_number": 490,
        "API": ".get(",
        "context": [
            "    \"\"\"\n",
            "    Extends a path-based environment variable's value with a new path and returns the updated value. It's up to the\n",
            "    caller to set it in os.environ.\n",
            "    \"\"\"\n",
            "    paths = [p for p in os.environ.get(env_var_name, \"\").split(\":\") if len(p) > 0]\n",
            "    paths.append(str(path_to_add))\n",
            "    return \":\".join(paths)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\launch.py",
        "line_number": 515,
        "API": ".get(",
        "context": [
            "        self.debug = debug\n",
            "\n",
            "    def __call__(self, index, *args):\n",
            "        if self.debug:\n",
            "            world_size = int(os.environ.get(\"WORLD_SIZE\"))\n",
            "            rdv_file = os.environ.get(\"ACCELERATE_DEBUG_RDV_FILE\")\n",
            "            torch.distributed.init_process_group(\n",
            "                \"gloo\",\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 239,
        "API": ".format(",
        "context": [
            "                data_parallel_size=mpu.get_data_parallel_world_size(),\n",
            "                data_sharding=args.data_sharding,\n",
            "            )\n",
            "        else:\n",
            "            raise Exception(\"{} dataloader type is not supported.\".format(args.dataloader_type))\n",
            "\n",
            "        # Torch dataloader.\n",
            "        return torch.utils.data.DataLoader(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 283,
        "API": ".format(",
        "context": [
            "                eval_iters * args.global_batch_size,\n",
            "                test_iters * args.global_batch_size,\n",
            "            ]\n",
            "            print_rank_0(\" > datasets target sizes (minimum size):\")\n",
            "            print_rank_0(\"    train:      {}\".format(train_val_test_num_samples[0]))\n",
            "            print_rank_0(\"    validation: {}\".format(train_val_test_num_samples[1]))\n",
            "            print_rank_0(\"    test:       {}\".format(train_val_test_num_samples[2]))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 306,
        "API": ".broadcast(",
        "context": [
            "        else:\n",
            "            flags = torch.cuda.LongTensor([0, 0, 0])\n",
            "\n",
            "        # Broadcast num tokens.\n",
            "        torch.distributed.broadcast(\n",
            "            flags, mpu.get_tensor_model_parallel_src_rank(), group=mpu.get_tensor_model_parallel_group()\n",
            "        )\n",
            "        args.do_train = flags[0].item()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 311,
        "API": ".item(",
        "context": [
            "            flags, mpu.get_tensor_model_parallel_src_rank(), group=mpu.get_tensor_model_parallel_group()\n",
            "        )\n",
            "        args.do_train = flags[0].item()\n",
            "        args.do_valid = flags[1].item()\n",
            "        args.do_test = flags[2].item()\n",
            "\n",
            "        # Build iterators.\n",
            "        dl_type = args.dataloader_type\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 368,
        "API": ".copy(",
        "context": [
            "            num_processes=mpu.get_data_parallel_world_size(),\n",
            "            process_index=mpu.get_data_parallel_rank(),\n",
            "            split_batches=accelerator.split_batches,\n",
            "            put_on_device=True,\n",
            "            rng_types=accelerator.rng_types.copy(),\n",
            "            dispatch_batches=accelerator.dispatch_batches,\n",
            "        )\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 513,
        "API": ".current_device(",
        "context": [
            "\n",
            "        def get_batch_transformer(data_iterator):\n",
            "            \"\"\"Build the batch.\"\"\"\n",
            "            data = next(data_iterator)\n",
            "            data = send_to_device(data, torch.cuda.current_device())\n",
            "\n",
            "            # Unpack.\n",
            "            tokens = data[\"input_ids\"].long()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 524,
        "API": ".to(",
        "context": [
            "            else:\n",
            "                types = None\n",
            "            if \"labels\" in data:\n",
            "                lm_labels = data[\"labels\"].long()\n",
            "                loss_mask = (data[\"labels\"] != -100).to(torch.float)\n",
            "            else:\n",
            "                lm_labels = None\n",
            "                loss_mask = None\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 546,
        "API": ".reshape(",
        "context": [
            "            lm_loss_, sop_logits = output_tensor\n",
            "\n",
            "            lm_loss_ = lm_loss_.float()\n",
            "            loss_mask = loss_mask.float()\n",
            "            lm_loss = torch.sum(lm_loss_.view(-1) * loss_mask.reshape(-1)) / loss_mask.sum()\n",
            "\n",
            "            if sop_logits is not None:\n",
            "                sop_loss = F.cross_entropy(sop_logits.view(-1, 2).float(), sentence_order.view(-1), ignore_index=-1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 564,
        "API": ".view(",
        "context": [
            "        def loss_func_finetune(labels, logits):\n",
            "            if num_labels == 1:\n",
            "                #  We are doing regression\n",
            "                loss_fct = MSELoss()\n",
            "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
            "            elif self.num_labels > 1 and (labels.dtype in (torch.long, torch.int)):\n",
            "                loss_fct = CrossEntropyLoss()\n",
            "                loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 637,
        "API": ".contiguous(",
        "context": [
            "            data_b = mpu.broadcast_data(keys, data, datatype)\n",
            "\n",
            "            # Unpack.\n",
            "            tokens_ = data_b[\"text\"].long()\n",
            "            labels = tokens_[:, 1:].contiguous()\n",
            "            tokens = tokens_[:, :-1].contiguous()\n",
            "\n",
            "            # Get the masks and postition ids.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 650,
        "API": ".current_device(",
        "context": [
            "\n",
            "        def get_batch_transformer(data_iterator):\n",
            "            data = next(data_iterator)\n",
            "            data = {\"input_ids\": data[\"input_ids\"]}\n",
            "            data = send_to_device(data, torch.cuda.current_device())\n",
            "\n",
            "            tokens_ = data[\"input_ids\"].long()\n",
            "            padding = torch.zeros((tokens_.shape[0], 1), dtype=tokens_.dtype, device=tokens_.device) + self.eod_token\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 655,
        "API": ".contiguous(",
        "context": [
            "\n",
            "            tokens_ = data[\"input_ids\"].long()\n",
            "            padding = torch.zeros((tokens_.shape[0], 1), dtype=tokens_.dtype, device=tokens_.device) + self.eod_token\n",
            "            tokens_ = torch.concat([tokens_, padding], dim=1)\n",
            "            labels = tokens_[:, 1:].contiguous()\n",
            "            tokens = tokens_[:, :-1].contiguous()\n",
            "            # Get the masks and postition ids.\n",
            "            attention_mask, loss_mask, position_ids = get_ltor_masks_and_position_ids(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 677,
        "API": ".view(",
        "context": [
            "                losses, logits = output_tensor\n",
            "            else:\n",
            "                losses = output_tensor\n",
            "            losses = losses.float()\n",
            "            loss_mask = loss_mask.view(-1).float()\n",
            "            loss = torch.sum(losses.view(-1) * loss_mask) / loss_mask.sum()\n",
            "\n",
            "            # Reduce loss for logging.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 724,
        "API": ".unsqueeze(",
        "context": [
            "    @staticmethod\n",
            "    def attn_mask_postprocess(attention_mask):\n",
            "        # We create a 3D attention mask from a 2D tensor mask.\n",
            "        # [b, 1, s]\n",
            "        attention_mask_b1s = attention_mask.unsqueeze(1)\n",
            "        # [b, s, 1]\n",
            "        attention_mask_bs1 = attention_mask.unsqueeze(2)\n",
            "        # [b, s, s]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 735,
        "API": ".tril(",
        "context": [
            "        return extended_attention_mask\n",
            "\n",
            "    @staticmethod\n",
            "    def get_decoder_mask(seq_length, device):\n",
            "        attention_mask = torch.tril(torch.ones((1, seq_length, seq_length), device=device))\n",
            "        attention_mask = attention_mask < 0.5\n",
            "        return attention_mask\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 744,
        "API": ".unsqueeze(",
        "context": [
            "    def get_enc_dec_mask(attention_mask, dec_seq_length, device):\n",
            "        batch_size, _ = attention_mask.shape\n",
            "        # We create a 3D attention mask from a 2D tensor mask.\n",
            "        # [b, 1, s]\n",
            "        attention_mask_b1s = attention_mask.unsqueeze(1)\n",
            "        # [b, s, 1]\n",
            "        attention_mask_bs1 = torch.ones((batch_size, dec_seq_length, 1), device=device)\n",
            "        attention_mask_bss = attention_mask_bs1 * attention_mask_b1s\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 780,
        "API": ".current_device(",
        "context": [
            "\n",
            "        def get_batch_transformer(data_iterator):\n",
            "            \"\"\"Build the batch.\"\"\"\n",
            "            data = next(data_iterator)\n",
            "            data = send_to_device(data, torch.cuda.current_device())\n",
            "\n",
            "            tokens_enc = data[\"input_ids\"].long()\n",
            "            labels = data[\"labels\"].long()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 788,
        "API": ".new_zeros(",
        "context": [
            "            loss_mask = (labels != -100).to(torch.float)\n",
            "            if \"decoder_input_ids\" in data:\n",
            "                tokens_dec = data[\"decoder_input_ids\"].long()\n",
            "            else:\n",
            "                tokens_dec = labels.new_zeros(labels.shape, device=labels.device, dtype=torch.long)\n",
            "                tokens_dec[..., 1:] = labels[..., :-1].clone()\n",
            "                tokens_dec[..., 0] = 0\n",
            "                tokens_dec.masked_fill_(tokens_dec == -100, 0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 808,
        "API": ".reshape(",
        "context": [
            "\n",
            "    def get_loss_func(self):\n",
            "        def loss_func(loss_mask, output_tensor):\n",
            "            lm_loss_ = output_tensor.float()\n",
            "            lm_loss = torch.sum(lm_loss_.view(-1) * loss_mask.reshape(-1)) / loss_mask.sum()\n",
            "\n",
            "            loss = lm_loss\n",
            "            averaged_losses = average_losses_across_data_parallel_group([lm_loss])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 837,
        "API": ".is_available(",
        "context": [
            "\n",
            "# intialize megatron setup\n",
            "def initialize(accelerator, extra_args_provider=None, args_defaults={}):\n",
            "    accelerator.print(\"Initializing Megatron-LM\")\n",
            "    assert torch.cuda.is_available(), \"Megatron requires CUDA.\"\n",
            "\n",
            "    # Parse arguments\n",
            "    args = parse_args(extra_args_provider, ignore_unknown_args=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 848,
        "API": ".format(",
        "context": [
            "        if getattr(args, key, None) is not None:\n",
            "            if args.rank == 0:\n",
            "                print(\n",
            "                    \"WARNING: overriding default arguments for {key}:{v} \\\n",
            "                        with {key}:{v2}\".format(\n",
            "                        key=key, v=getattr(args, key), v2=value\n",
            "                    ),\n",
            "                    flush=True,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 855,
        "API": ".get(",
        "context": [
            "                    flush=True,\n",
            "                )\n",
            "        setattr(args, key, value)\n",
            "\n",
            "    if args.use_checkpoint_args or args_defaults.get(\"use_checkpoint_args\", False):\n",
            "        assert args.load is not None, \"--use-checkpoints-args requires --load argument\"\n",
            "        load_args_from_checkpoint(args)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 870,
        "API": ".get_rank(",
        "context": [
            "    def finish_mpu_init():\n",
            "        args = get_args()\n",
            "        # Pytorch distributed.\n",
            "        device_count = torch.cuda.device_count()\n",
            "        args.rank = torch.distributed.get_rank()\n",
            "        args.world_size = torch.distributed.get_world_size()\n",
            "        if device_count > 0:\n",
            "            device = args.rank % device_count\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 893,
        "API": ".format(",
        "context": [
            "                )\n",
            "\n",
            "        # Random seeds for reproducibility.\n",
            "        if args.rank == 0:\n",
            "            print(\"> setting random seeds to {} ...\".format(args.seed))\n",
            "        _set_random_seed(args.seed, args.data_parallel_random_init)\n",
            "\n",
            "    args = get_args()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 1019,
        "API": ".empty_cache(",
        "context": [
            "        )\n",
            "\n",
            "        # Empty unused memory.\n",
            "        if args.empty_unused_memory_level >= 1:\n",
            "            torch.cuda.empty_cache()\n",
            "\n",
            "        # Reduce gradients.\n",
            "        timers(\"backward-reduce-model-grads\").start()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 1050,
        "API": ".empty_cache(",
        "context": [
            "        self.optimizer.skipped_iter = not update_successful\n",
            "\n",
            "        # Empty unused memory.\n",
            "        if args.empty_unused_memory_level >= 2:\n",
            "            torch.cuda.empty_cache()\n",
            "\n",
            "        args.consumed_train_samples += (\n",
            "            mpu.get_data_parallel_world_size() * args.micro_batch_size * get_num_microbatches()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 1064,
        "API": ".concat(",
        "context": [
            "                losses_reduced_for_key = [x[key] for x in losses_reduced]\n",
            "                if len(losses_reduced_for_key[0].shape) == 0:\n",
            "                    loss_reduced[key] = sum(losses_reduced_for_key) / len(losses_reduced_for_key)\n",
            "                else:\n",
            "                    loss_reduced[key] = torch.concat(losses_reduced_for_key)\n",
            "            return loss_reduced, skipped_iter, grad_norm, num_zeros_in_grad\n",
            "        return {}, skipped_iter, grad_norm, num_zeros_in_grad\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 1101,
        "API": ".empty_cache(",
        "context": [
            "            forward_only=True,\n",
            "        )\n",
            "        # Empty unused memory\n",
            "        if args.empty_unused_memory_level >= 1:\n",
            "            torch.cuda.empty_cache()\n",
            "\n",
            "        args.consumed_valid_samples += (\n",
            "            mpu.get_data_parallel_world_size() * args.micro_batch_size * get_num_microbatches()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 1115,
        "API": ".concat(",
        "context": [
            "                losses_reduced_for_key = [x[key] for x in loss_dicts]\n",
            "                if len(losses_reduced_for_key[0].shape) == 0:\n",
            "                    loss_reduced[key] = sum(losses_reduced_for_key) / len(losses_reduced_for_key)\n",
            "                else:\n",
            "                    loss_reduced[key] = torch.concat(losses_reduced_for_key)\n",
            "            return loss_reduced\n",
            "        else:\n",
            "            return {}\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 1141,
        "API": ".item(",
        "context": [
            "            loss_dict, skipped_iter, grad_norm, num_zeros_in_grad = self.train_step(**batch_data)\n",
            "            self.iteration += 1\n",
            "            if args.tensorboard_dir is not None:\n",
            "                # Logging.\n",
            "                loss_scale = self.optimizer.get_loss_scale().item()\n",
            "                params_norm = None\n",
            "                if args.log_params_norm:\n",
            "                    params_norm = calc_params_l2_norm(self.model)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 1162,
        "API": ".get(",
        "context": [
            "            loss_dict = self.eval_step(**batch_data)\n",
            "            if args.tensorboard_dir is not None:\n",
            "                for key in loss_dict:\n",
            "                    self.eval_total_loss_dict[key] = (\n",
            "                        self.eval_total_loss_dict.get(key, torch.cuda.FloatTensor([0.0])) + loss_dict[key]\n",
            "                    )\n",
            "                    self.eval_total_loss_dict[key + \"_num_iters\"] = self.eval_total_loss_dict.get(\n",
            "                        key + \"_num_iters\", torch.cuda.FloatTensor([0.0])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 1193,
        "API": ".exp(",
        "context": [
            "            if key.endswith(\"_num_iters\"):\n",
            "                continue\n",
            "            value = self.eval_total_loss_dict[key] / self.eval_total_loss_dict[key + \"_num_iters\"]\n",
            "            string += f\"{key} value: {value} | \"\n",
            "            ppl = math.exp(min(20, value.item()))\n",
            "            if args.pretraining_flag:\n",
            "                string += f\"{key} PPL: {ppl} | \"\n",
            "            if writer:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 1298,
        "API": ".get(",
        "context": [
            "        else:\n",
            "            if not (0.0 <= top_p <= 1.0):\n",
            "                raise ValueError(\"top_p must be less than or equal to 1.0\")\n",
            "\n",
            "        top_p_decay = kwargs.get(\"top_p_decay\", 0.0)\n",
            "        if not (0.0 <= top_p_decay <= 1.0):\n",
            "            raise ValueError(\"top_p_decay must be less than or equal to 1.0\")\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 1306,
        "API": ".get(",
        "context": [
            "        top_p_bound = kwargs.get(\"top_p_bound\", 0.0)\n",
            "        if not (0.0 <= top_p_bound <= 1.0):\n",
            "            raise ValueError(\"top_p_bound must be less than or equal to 1.0\")\n",
            "\n",
            "        add_BOS = kwargs.get(\"add_BOS\", False)\n",
            "        if not (isinstance(add_BOS, bool)):\n",
            "            raise ValueError(\"add_BOS must be a boolean\")\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 1321,
        "API": ".get(",
        "context": [
            "                return \"When doing beam_search, batch size must be 1\"\n",
            "\n",
            "        tokenizer = get_tokenizer()\n",
            "\n",
            "        stop_token = kwargs.get(\"stop_token\", tokenizer.eod)\n",
            "        if stop_token is not None:\n",
            "            if not isinstance(stop_token, int):\n",
            "                raise ValueError(\"stop_token must be an integer\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 1332,
        "API": ".get_rank(",
        "context": [
            "\n",
            "        sizes_list = None\n",
            "        prompts_tokens_tensor = None\n",
            "        prompts_length_tensor = None\n",
            "        if torch.distributed.get_rank() == 0:\n",
            "            # Get the prompts length.\n",
            "            if attention_mask is None:\n",
            "                prompts_length_tensor = torch.cuda.LongTensor([inputs.shape[1]] * inputs.shape[0])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 1337,
        "API": ".sum(",
        "context": [
            "            # Get the prompts length.\n",
            "            if attention_mask is None:\n",
            "                prompts_length_tensor = torch.cuda.LongTensor([inputs.shape[1]] * inputs.shape[0])\n",
            "            else:\n",
            "                prompts_length_tensor = attention_mask.sum(axis=-1).cuda()\n",
            "\n",
            "            if max_new_tokens is None:\n",
            "                max_new_tokens = max_length - inputs.shape[1]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 1347,
        "API": ".ceil(",
        "context": [
            "\n",
            "            if add_BOS:\n",
            "                max_length = max_new_tokens + inputs.shape[1] + 1\n",
            "                # making sure that `max_length` is a multiple of 4 to leverage fused kernels\n",
            "                max_length = 4 * math.ceil(max_length / 4)\n",
            "                max_new_tokens = max_length - (inputs.shape[1] + 1)\n",
            "                padding = torch.cuda.LongTensor([[tokenizer.eod] * max_new_tokens] * inputs.shape[0])\n",
            "                prompts_tokens_tensor = torch.concat(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 1356,
        "API": ".ceil(",
        "context": [
            "                )\n",
            "            else:\n",
            "                # making sure that `max_length` is a multiple of 4 to leverage fused kernels\n",
            "                max_length = max_new_tokens + inputs.shape[1]\n",
            "                max_length = 4 * math.ceil(max_length / 4)\n",
            "                max_new_tokens = max_length - inputs.shape[1]\n",
            "                padding = torch.cuda.LongTensor([[tokenizer.eod] * max_new_tokens] * inputs.shape[0])\n",
            "                prompts_tokens_tensor = torch.concat([inputs.cuda(), padding], axis=-1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 1363,
        "API": ".size(",
        "context": [
            "                prompts_tokens_tensor = torch.concat([inputs.cuda(), padding], axis=-1)\n",
            "\n",
            "            # We need the sizes of these tensors for the boradcast\n",
            "            sizes_list = [\n",
            "                prompts_tokens_tensor.size(0),  # Batch size\n",
            "                prompts_tokens_tensor.size(1),\n",
            "            ]  # Sequence lenght\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 1377,
        "API": ".get(",
        "context": [
            "        context_tokens_tensor = broadcast_tensor(sizes, torch.int64, tensor=prompts_tokens_tensor, rank=0)\n",
            "        context_length_tensor = broadcast_tensor(sizes[0], torch.int64, tensor=prompts_length_tensor, rank=0)\n",
            "\n",
            "        # Run the inference\n",
            "        random_seed = kwargs.get(\"random_seed\", 0)\n",
            "        torch.random.manual_seed(random_seed)\n",
            "        unwrapped_model = unwrap_model(self.base_model, (torchDDP, LocalDDP, Float16Module))\n",
            "        if beam_width is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 1430,
        "API": ".clone(",
        "context": [
            "    \"\"\"\n",
            "\n",
            "    def _gpu_gather_one(tensor):\n",
            "        if tensor.ndim == 0:\n",
            "            tensor = tensor.clone()[None]\n",
            "        output_tensors = [\n",
            "            torch.empty_like(tensor)\n",
            "            for _ in range(torch.distributed.get_world_size(group=mpu.get_data_parallel_group()))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\megatron_lm.py",
        "line_number": 1435,
        "API": ".all_gather(",
        "context": [
            "        output_tensors = [\n",
            "            torch.empty_like(tensor)\n",
            "            for _ in range(torch.distributed.get_world_size(group=mpu.get_data_parallel_group()))\n",
            "        ]\n",
            "        torch.distributed.all_gather(output_tensors, tensor, group=mpu.get_data_parallel_group())\n",
            "        return torch.cat(output_tensors, dim=0)\n",
            "\n",
            "    return recursively_apply(_gpu_gather_one, tensor, error_on_other_type=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\memory.py",
        "line_number": 30,
        "API": ".empty_cache(",
        "context": [
            "\n",
            "\n",
            "def release_memory(*objects):\n",
            "    \"\"\"\n",
            "    Releases memory from `objects` by setting them to `None` and calls `gc.collect()` and `torch.cuda.empty_cache()`.\n",
            "    Returned objects should be reassigned to the same variables.\n",
            "\n",
            "    Args:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\memory.py",
        "line_number": 45,
        "API": ".ones(",
        "context": [
            "        ```python\n",
            "        >>> import torch\n",
            "        >>> from accelerate.utils import release_memory\n",
            "\n",
            "        >>> a = torch.ones(1000, 1000).cuda()\n",
            "        >>> b = torch.ones(1000, 1000).cuda()\n",
            "        >>> a, b = release_memory(a, b)\n",
            "        ```\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\memory.py",
        "line_number": 56,
        "API": ".empty_cache(",
        "context": [
            "    for i in range(len(objects)):\n",
            "        objects[i] = None\n",
            "    gc.collect()\n",
            "    if is_xpu_available():\n",
            "        torch.xpu.empty_cache()\n",
            "    elif is_npu_available():\n",
            "        torch.npu.empty_cache()\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\memory.py",
        "line_number": 118,
        "API": ".empty_cache(",
        "context": [
            "    def decorator(*args, **kwargs):\n",
            "        nonlocal batch_size\n",
            "        gc.collect()\n",
            "        if is_xpu_available():\n",
            "            torch.xpu.empty_cache()\n",
            "        elif is_npu_available():\n",
            "            torch.npu.empty_cache()\n",
            "        else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\memory.py",
        "line_number": 126,
        "API": ".join(",
        "context": [
            "            torch.cuda.empty_cache()\n",
            "        params = list(inspect.signature(function).parameters.keys())\n",
            "        # Guard against user error\n",
            "        if len(params) < (len(args) + 1):\n",
            "            arg_str = \", \".join([f\"{arg}={value}\" for arg, value in zip(params[1:], args[1:])])\n",
            "            raise TypeError(\n",
            "                f\"Batch size was passed into `{function.__name__}` as the first argument when called.\"\n",
            "                f\"Remove this as the decorator already does so: `{function.__name__}({arg_str})`\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\memory.py",
        "line_number": 140,
        "API": ".empty_cache(",
        "context": [
            "            except Exception as e:\n",
            "                if should_reduce_batch_size(e):\n",
            "                    gc.collect()\n",
            "                    if is_xpu_available():\n",
            "                        torch.xpu.empty_cache()\n",
            "                    elif is_npu_available():\n",
            "                        torch.npu.empty_cache()\n",
            "                    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 63,
        "API": ".upper(",
        "context": [
            "    ```\n",
            "    \"\"\"\n",
            "    if isinstance(size, int):\n",
            "        return size\n",
            "    if size.upper().endswith(\"GIB\"):\n",
            "        return int(size[:-3]) * (2**30)\n",
            "    if size.upper().endswith(\"MIB\"):\n",
            "        return int(size[:-3]) * (2**20)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 69,
        "API": ".upper(",
        "context": [
            "    if size.upper().endswith(\"MIB\"):\n",
            "        return int(size[:-3]) * (2**20)\n",
            "    if size.upper().endswith(\"KIB\"):\n",
            "        return int(size[:-3]) * (2**10)\n",
            "    if size.upper().endswith(\"GB\"):\n",
            "        int_size = int(size[:-2]) * (10**9)\n",
            "        return int_size // 8 if size.endswith(\"b\") else int_size\n",
            "    if size.upper().endswith(\"MB\"):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 75,
        "API": ".upper(",
        "context": [
            "        return int_size // 8 if size.endswith(\"b\") else int_size\n",
            "    if size.upper().endswith(\"MB\"):\n",
            "        int_size = int(size[:-2]) * (10**6)\n",
            "        return int_size // 8 if size.endswith(\"b\") else int_size\n",
            "    if size.upper().endswith(\"KB\"):\n",
            "        int_size = int(size[:-2]) * (10**3)\n",
            "        return int_size // 8 if size.endswith(\"b\") else int_size\n",
            "    raise ValueError(\"`size` is not in a valid format. Use an integer followed by the unit, e.g., '5GB'.\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 131,
        "API": ".size(",
        "context": [
            "    except Exception:\n",
            "        # Fallback for torch==1.10\n",
            "        try:\n",
            "            storage_ptr = tensor.storage().data_ptr()\n",
            "            storage_size = tensor.storage().size() * _SIZE[tensor.dtype]\n",
            "        except NotImplementedError:\n",
            "            # Fallback for meta storage\n",
            "            storage_ptr = 0\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 233,
        "API": ".to(",
        "context": [
            "    fp16_statistics: Optional[torch.HalfTensor] = None,\n",
            "):\n",
            "    \"\"\"\n",
            "    A helper function to set a given tensor (parameter of buffer) of a module on a specific device (note that doing\n",
            "    `param.to(device)` creates a new tensor not linked to the parameter, which is why we need this function).\n",
            "\n",
            "    Args:\n",
            "        module (`torch.nn.Module`):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 252,
        "API": ".split(",
        "context": [
            "            The list of fp16 statistics to set on the module, used for 8 bit model serialization.\n",
            "    \"\"\"\n",
            "    # Recurse if needed\n",
            "    if \".\" in tensor_name:\n",
            "        splits = tensor_name.split(\".\")\n",
            "        for split in splits[:-1]:\n",
            "            new_module = getattr(module, split)\n",
            "            if new_module is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 265,
        "API": ".device(",
        "context": [
            "        raise ValueError(f\"{module} does not have a parameter or a buffer named {tensor_name}.\")\n",
            "    is_buffer = tensor_name in module._buffers\n",
            "    old_value = getattr(module, tensor_name)\n",
            "\n",
            "    if old_value.device == torch.device(\"meta\") and device not in [\"meta\", torch.device(\"meta\")] and value is None:\n",
            "        raise ValueError(f\"{tensor_name} is on the meta device, we need a `value` to put in on {device}.\")\n",
            "\n",
            "    if value is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 276,
        "API": ".to(",
        "context": [
            "            )\n",
            "\n",
            "        if dtype is None:\n",
            "            # For compatibility with PyTorch load_state_dict which converts state dict dtype to existing dtype in model\n",
            "            value = value.to(old_value.dtype)\n",
            "        elif not str(value.dtype).startswith((\"torch.uint\", \"torch.int\", \"torch.bool\")):\n",
            "            value = value.to(dtype)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 290,
        "API": ".device(",
        "context": [
            "        # # fix the case where the device is meta, we don't want to put it on cpu because there is no data =0\n",
            "        if (\n",
            "            param is not None\n",
            "            and param.device.type != \"cuda\"\n",
            "            and torch.device(device).type == \"cuda\"\n",
            "            and param_cls.__name__ in [\"Int8Params\", \"FP4Params\"]\n",
            "        ):\n",
            "            device_quantization = device\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 296,
        "API": ".to(",
        "context": [
            "        ):\n",
            "            device_quantization = device\n",
            "            device = \"cpu\"\n",
            "        if value is None:\n",
            "            new_value = old_value.to(device)\n",
            "            if dtype is not None and device in [\"meta\", torch.device(\"meta\")]:\n",
            "                new_value = new_value.to(dtype)\n",
            "                if not is_buffer:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 302,
        "API": ".to(",
        "context": [
            "                new_value = new_value.to(dtype)\n",
            "                if not is_buffer:\n",
            "                    module._parameters[tensor_name] = param_cls(new_value, requires_grad=old_value.requires_grad)\n",
            "        elif isinstance(value, torch.Tensor):\n",
            "            new_value = value.to(device)\n",
            "        else:\n",
            "            new_value = torch.tensor(value, device=device)\n",
            "        if device_quantization is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 309,
        "API": ".device(",
        "context": [
            "        if device_quantization is not None:\n",
            "            device = device_quantization\n",
            "        if is_buffer:\n",
            "            module._buffers[tensor_name] = new_value\n",
            "        elif value is not None or torch.device(device) != module._parameters[tensor_name].device:\n",
            "            param_cls = type(module._parameters[tensor_name])\n",
            "            kwargs = module._parameters[tensor_name].__dict__\n",
            "            if param_cls.__name__ in [\"Int8Params\", \"FP4Params\"]:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 315,
        "API": ".to(",
        "context": [
            "            kwargs = module._parameters[tensor_name].__dict__\n",
            "            if param_cls.__name__ in [\"Int8Params\", \"FP4Params\"]:\n",
            "                if param_cls.__name__ == \"Int8Params\" and new_value.dtype == torch.float32:\n",
            "                    # downcast to fp16 if any - needed for 8bit serialization\n",
            "                    new_value = new_value.to(torch.float16)\n",
            "                # quantize module that are going to stay on the cpu so that we offload quantized weights\n",
            "                if device == \"cpu\" and param_cls.__name__ == \"Int8Params\":\n",
            "                    new_value = param_cls(new_value, requires_grad=old_value.requires_grad, **kwargs).to(0).to(\"cpu\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 320,
        "API": ".to(",
        "context": [
            "                # quantize module that are going to stay on the cpu so that we offload quantized weights\n",
            "                if device == \"cpu\" and param_cls.__name__ == \"Int8Params\":\n",
            "                    new_value = param_cls(new_value, requires_grad=old_value.requires_grad, **kwargs).to(0).to(\"cpu\")\n",
            "                    new_value.CB = new_value.CB.to(\"cpu\")\n",
            "                    new_value.SCB = new_value.SCB.to(\"cpu\")\n",
            "                else:\n",
            "                    new_value = param_cls(new_value, requires_grad=old_value.requires_grad, **kwargs).to(device)\n",
            "            else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 327,
        "API": ".to(",
        "context": [
            "            else:\n",
            "                new_value = param_cls(new_value, requires_grad=old_value.requires_grad).to(device)\n",
            "            module._parameters[tensor_name] = new_value\n",
            "            if fp16_statistics is not None:\n",
            "                setattr(module._parameters[tensor_name], \"SCB\", fp16_statistics.to(device))\n",
            "                del fp16_statistics\n",
            "            # as we put the weight to meta, it doesn't have SCB attr anymore. make sure that it is not a meta weight\n",
            "            if (\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 336,
        "API": ".device(",
        "context": [
            "                and getattr(module.weight, \"SCB\", None) is None\n",
            "                and str(module.weight.device) != \"meta\"\n",
            "            ):\n",
            "                # quantize only if necessary\n",
            "                device_index = torch.device(device).index if torch.device(device).type == \"cuda\" else None\n",
            "                if not getattr(module.weight, \"SCB\", None) and device_index is not None:\n",
            "                    if module.bias is not None and module.bias.device.type != \"meta\":\n",
            "                        # if a bias exists, we need to wait until the bias is set on the correct device\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 346,
        "API": ".device(",
        "context": [
            "                        # if no bias exists, we can quantize right away\n",
            "                        module = module.cuda(device_index)\n",
            "            elif module.__class__.__name__ == \"Linear4bit\" and getattr(module.weight, \"quant_state\", None) is None:\n",
            "                # quantize only if necessary\n",
            "                device_index = torch.device(device).index if torch.device(device).type == \"cuda\" else None\n",
            "                if not getattr(module.weight, \"quant_state\", None) and device_index is not None:\n",
            "                    module.weight = module.weight.cuda(device_index)\n",
            "    # clean pre and post foward hook\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 423,
        "API": ".join(",
        "context": [
            "\n",
            "def _get_param_device(param, device_map):\n",
            "    if param in device_map:\n",
            "        return device_map[param]\n",
            "    parent_param = \".\".join(param.split(\".\")[:-1])\n",
            "    if parent_param == param:\n",
            "        raise ValueError(f\"The `device_map` does not contain the module {param}.\")\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 447,
        "API": ".warn(",
        "context": [
            "        tie_param_devices = {}\n",
            "        for param in tie_param:\n",
            "            tie_param_devices[param] = _get_param_device(param, device_map)\n",
            "        if len(set(tie_param_devices.values())) > 1:\n",
            "            logger.warn(\n",
            "                f\"Tied parameters are on different devices: {tie_param_devices}. \"\n",
            "                \"Please modify your custom device map or set `device_map='auto'`. \"\n",
            "            )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 483,
        "API": ".get(",
        "context": [
            "    [['linear1.weight', 'linear2.weight']]\n",
            "    ```\n",
            "    \"\"\"\n",
            "    # Initialize result and named_parameters before recursing.\n",
            "    named_parameters = kwargs.get(\"named_parameters\", None)\n",
            "    prefix = kwargs.get(\"prefix\", \"\")\n",
            "    result = kwargs.get(\"result\", {})\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 526,
        "API": ".split(",
        "context": [
            "        param_to_tie = None\n",
            "        # First iteration of the loop will set param_to_tie, next ones will tie it to the others\n",
            "        for param_name in tied_group:\n",
            "            module = model\n",
            "            splits = param_name.split(\".\")\n",
            "            for split in splits[:-1]:\n",
            "                module = getattr(module, split)\n",
            "            if param_to_tie is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 568,
        "API": ".split(",
        "context": [
            "        elif dtype is None:\n",
            "            size = tensor.numel() * dtype_byte_size(tensor.dtype)\n",
            "        else:\n",
            "            size = tensor.numel() * min(dtype_size, dtype_byte_size(tensor.dtype))\n",
            "        name_parts = name.split(\".\")\n",
            "        for idx in range(len(name_parts) + 1):\n",
            "            module_sizes[\".\".join(name_parts[:idx])] += size\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 597,
        "API": ".copy(",
        "context": [
            "        `Tuple[int, List[str]]`: The maximum size of a layer with the list of layer names realizing that maximum size.\n",
            "    \"\"\"\n",
            "    max_size = 0\n",
            "    layer_names = []\n",
            "    modules_to_treat = modules.copy()\n",
            "    while len(modules_to_treat) > 0:\n",
            "        module_name, module = modules_to_treat.pop(0)\n",
            "        modules_children = list(module.named_children()) if isinstance(module, torch.nn.Module) else []\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 621,
        "API": ".is_available(",
        "context": [
            "    \"\"\"\n",
            "    import psutil\n",
            "\n",
            "    if max_memory is None:\n",
            "        if not (torch.cuda.is_available() or is_xpu_available()):\n",
            "            max_memory = {}\n",
            "\n",
            "        else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 632,
        "API": ".device(",
        "context": [
            "                    _ = torch.tensor([0], device=i)\n",
            "                max_memory = {i: torch.cuda.mem_get_info(i)[0] for i in range(torch.cuda.device_count())}\n",
            "            else:\n",
            "                for i in range(torch.xpu.device_count()):\n",
            "                    _ = torch.tensor(0, device=torch.device(\"xpu\", i))\n",
            "                max_memory = {i: torch.xpu.max_memory_allocated(i) for i in range(torch.xpu.device_count())}\n",
            "        # allocate everything in the mps device as the RAM is shared\n",
            "        if is_mps_available():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 661,
        "API": ".split(",
        "context": [
            "        device_map[module_name] = values[0]\n",
            "\n",
            "    # Recurse over the children\n",
            "    children_modules = [k for k in device_map.keys() if k.startswith(prefix) and len(k) > len(module_name)]\n",
            "    idx = len(module_name.split(\".\")) + 1 if len(module_name) > 0 else 1\n",
            "    children_modules = set(\".\".join(k.split(\".\")[:idx]) for k in children_modules)\n",
            "    for child in children_modules:\n",
            "        clean_device_map(device_map, module_name=child)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 692,
        "API": ".join(",
        "context": [
            "        fp16_statistics = None\n",
            "        if \"weight\" in param_name and param_name.replace(\"weight\", \"SCB\") in index.keys():\n",
            "            weight_name = param_name.replace(\"weight\", \"SCB\")\n",
            "            fp16_statistics = load_offloaded_weight(\n",
            "                os.path.join(offload_folder, f\"{weight_name}.dat\"), index[weight_name]\n",
            "            )\n",
            "        tensor_file = os.path.join(offload_folder, f\"{param_name}.dat\")\n",
            "        weight = load_offloaded_weight(tensor_file, metadata)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 737,
        "API": ".is_available(",
        "context": [
            "    \"\"\"\n",
            "    # Get default / clean up max_memory\n",
            "    max_memory = get_max_memory(max_memory)\n",
            "\n",
            "    if not (torch.cuda.is_available() or is_xpu_available()) or is_mps_available():\n",
            "        return max_memory\n",
            "\n",
            "    if not is_xpu_available():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 747,
        "API": ".get_device_properties(",
        "context": [
            "        num_devices = len(\n",
            "            [\n",
            "                d\n",
            "                for d in max_memory\n",
            "                if (torch.device(d).type == \"xpu\" or torch.xpu.get_device_properties(d).dev_type == \"gpu\")\n",
            "                and max_memory[d] > 0\n",
            "            ]\n",
            "        )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 776,
        "API": ".split(",
        "context": [
            "        for name, size in module_sizes.items():\n",
            "            if name == \"\":\n",
            "                continue\n",
            "            submodule = model\n",
            "            for submodule_name in name.split(\".\"):\n",
            "                submodule = getattr(submodule, submodule_name)\n",
            "            class_name = submodule.__class__.__name__\n",
            "            if class_name in no_split_module_classes and class_name not in no_split_children:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 875,
        "API": ".warn(",
        "context": [
            "    module_sizes = compute_module_sizes(model, dtype=dtype, special_dtypes=special_dtypes)\n",
            "    tied_parameters = find_tied_parameters(model)\n",
            "\n",
            "    if check_tied_parameters_in_config(model) and len(tied_parameters) == 0:\n",
            "        logger.warn(\n",
            "            \"The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\"\n",
            "        )\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 1070,
        "API": ".join(",
        "context": [
            "                for name in all_model_tensors\n",
            "                if not name == module_name and not name.startswith(module_name + \".\")\n",
            "            ]\n",
            "    if len(all_model_tensors) > 0:\n",
            "        non_covered_params = \", \".join(all_model_tensors)\n",
            "        raise ValueError(\n",
            "            f\"The device_map provided does not give any device for the following parameters: {non_covered_params}\"\n",
            "        )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 1097,
        "API": ".warn(",
        "context": [
            "            metadata = f.metadata()\n",
            "            weight_names = f.keys()\n",
            "\n",
            "        if metadata is None:\n",
            "            logger.warn(\n",
            "                f\"The safetensors archive passed at {checkpoint_file} does not contain metadata. \"\n",
            "                \"Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\"\n",
            "            )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 1103,
        "API": ".get(",
        "context": [
            "                \"Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\"\n",
            "            )\n",
            "            metadata = {\"format\": \"pt\"}\n",
            "\n",
            "        if metadata.get(\"format\") not in [\"pt\", \"tf\", \"flax\"]:\n",
            "            raise OSError(\n",
            "                f\"The safetensors archive passed at {checkpoint_file} does not contain the valid metadata. Make sure \"\n",
            "                \"you save your model with the `save_pretrained` method.\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 1157,
        "API": ".load(",
        "context": [
            "                progress_bar.close()\n",
            "\n",
            "            return tensors\n",
            "    else:\n",
            "        return torch.load(checkpoint_file, map_location=torch.device(\"cpu\"))\n",
            "\n",
            "\n",
            "def load_checkpoint_in_model(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 1215,
        "API": ".warn(",
        "context": [
            "\n",
            "    tied_params = find_tied_parameters(model)\n",
            "\n",
            "    if check_tied_parameters_in_config(model) and len(tied_params) == 0:\n",
            "        logger.warn(\n",
            "            \"The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\"\n",
            "        )\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 1226,
        "API": ".makedirs(",
        "context": [
            "        raise ValueError(\n",
            "            \"At least one of the model submodule will be offloaded to disk, please pass along an `offload_folder`.\"\n",
            "        )\n",
            "    elif offload_folder is not None and device_map is not None and \"disk\" in device_map.values():\n",
            "        os.makedirs(offload_folder, exist_ok=True)\n",
            "\n",
            "    if isinstance(dtype, str):\n",
            "        # We accept \"torch.float16\" or just \"float16\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 1244,
        "API": ".join(",
        "context": [
            "    elif os.path.isdir(checkpoint):\n",
            "        # check if the whole state dict is present\n",
            "        potential_state = [f for f in os.listdir(checkpoint) if f == WEIGHTS_NAME]\n",
            "        if len(potential_state) == 1:\n",
            "            checkpoint_files = [os.path.join(checkpoint, potential_state[0])]\n",
            "        else:\n",
            "            # otherwise check for sharded checkpoints\n",
            "            potential_index = [f for f in os.listdir(checkpoint) if f.endswith(\".index.json\")]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 1253,
        "API": ".join(",
        "context": [
            "                raise ValueError(\n",
            "                    f\"{checkpoint} is not a folder containing a `.index.json` file or a {WEIGHTS_NAME} file\"\n",
            "                )\n",
            "            elif len(potential_index) == 1:\n",
            "                index_filename = os.path.join(checkpoint, potential_index[0])\n",
            "            else:\n",
            "                raise ValueError(\n",
            "                    f\"{checkpoint} containing more than one `.index.json` file, delete the irrelevant ones.\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 1265,
        "API": ".split(",
        "context": [
            "            f\"checkpoint, or a folder containing a sharded checkpoint or the whole state dict, but got {checkpoint}.\"\n",
            "        )\n",
            "\n",
            "    if index_filename is not None:\n",
            "        checkpoint_folder = os.path.split(index_filename)[0]\n",
            "        with open(index_filename, \"r\") as f:\n",
            "            index = json.loads(f.read())\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 1272,
        "API": ".join(",
        "context": [
            "\n",
            "        if \"weight_map\" in index:\n",
            "            index = index[\"weight_map\"]\n",
            "        checkpoint_files = sorted(list(set(index.values())))\n",
            "        checkpoint_files = [os.path.join(checkpoint_folder, f) for f in checkpoint_files]\n",
            "\n",
            "    # Logic for missing/unexepected keys goes here.\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 1278,
        "API": ".mkdtemp(",
        "context": [
            "    # Logic for missing/unexepected keys goes here.\n",
            "\n",
            "    offload_index = {}\n",
            "    if offload_state_dict:\n",
            "        state_dict_folder = tempfile.mkdtemp()\n",
            "        state_dict_index = {}\n",
            "\n",
            "    buffer_names = [name for name, _ in model.named_buffers()]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\modeling.py",
        "line_number": 1295,
        "API": ".join(",
        "context": [
            "\n",
            "                module_name = param_name\n",
            "\n",
            "                while len(module_name) > 0 and module_name not in device_map:\n",
            "                    module_name = \".\".join(module_name.split(\".\")[:-1])\n",
            "                if module_name == \"\" and \"\" not in device_map:\n",
            "                    # TODO: group all errors and raise at the end.\n",
            "                    raise ValueError(f\"{param_name} doesn't have any device set.\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\offload.py",
        "line_number": 30,
        "API": ".view(",
        "context": [
            "    dtype = None\n",
            "    # Check the string instead of the dtype to be compatible with versions of PyTorch that don't have bfloat16.\n",
            "    if str(weight.dtype) == \"torch.bfloat16\":\n",
            "        # Need to reinterpret the underlined data as int16 since NumPy does not handle bfloat16s.\n",
            "        weight = weight.view(torch.int16)\n",
            "        dtype = \"bfloat16\"\n",
            "    array = weight.cpu().numpy()\n",
            "    tensor_file = os.path.join(offload_folder, f\"{weight_name}.dat\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\offload.py",
        "line_number": 42,
        "API": ".flush(",
        "context": [
            "    if array.ndim == 0:\n",
            "        array = array[None]\n",
            "    file_array = np.memmap(tensor_file, dtype=array.dtype, mode=\"w+\", shape=array.shape)\n",
            "    file_array[:] = array[:]\n",
            "    file_array.flush()\n",
            "    return index\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\offload.py",
        "line_number": 63,
        "API": ".view(",
        "context": [
            "    if len(weight_info[\"shape\"]) == 0:\n",
            "        weight = weight[0]\n",
            "    weight = torch.tensor(weight)\n",
            "    if weight_info[\"dtype\"] == \"bfloat16\":\n",
            "        weight = weight.view(torch.bfloat16)\n",
            "\n",
            "    return weight\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\offload.py",
        "line_number": 73,
        "API": ".join(",
        "context": [
            "    if index is None or len(index) == 0:\n",
            "        # Nothing to save\n",
            "        return\n",
            "\n",
            "    offload_index_file = os.path.join(offload_folder, \"index.json\")\n",
            "    if os.path.isfile(offload_index_file):\n",
            "        with open(offload_index_file, \"r\", encoding=\"utf-8\") as f:\n",
            "            current_index = json.load(f)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\offload.py",
        "line_number": 95,
        "API": ".makedirs(",
        "context": [
            "            The directory in which to offload the state dict.\n",
            "        state_dict (`Dict[str, torch.Tensor]`):\n",
            "            The dictionary of tensors to offload.\n",
            "    \"\"\"\n",
            "    os.makedirs(save_dir, exist_ok=True)\n",
            "    index = {}\n",
            "    for name, parameter in state_dict.items():\n",
            "        index = offload_weight(parameter, name, save_dir, index=index)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\offload.py",
        "line_number": 154,
        "API": ".join(",
        "context": [
            "\n",
            "        self.state_dict = {} if state_dict is None else state_dict\n",
            "        self.save_folder = save_folder\n",
            "        if index is None and save_folder is not None:\n",
            "            with open(os.path.join(save_folder, \"index.json\")) as f:\n",
            "                index = json.load(f)\n",
            "        self.index = {} if index is None else index\n",
            "        self.all_keys = list(self.state_dict.keys())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\offload.py",
        "line_number": 166,
        "API": ".get(",
        "context": [
            "        # State dict gets priority\n",
            "        if key in self.state_dict:\n",
            "            return self.state_dict[key]\n",
            "        weight_info = self.index[key]\n",
            "        if weight_info.get(\"safetensors_file\") is not None:\n",
            "            if not is_safetensors_available():\n",
            "                raise ImportError(\"These offloaded weights require the use of safetensors: `pip install safetensors`.\")\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\offload.py",
        "line_number": 174,
        "API": ".get(",
        "context": [
            "            from safetensors import safe_open\n",
            "\n",
            "            device = \"cpu\" if self.device is None else self.device\n",
            "            with safe_open(weight_info[\"safetensors_file\"], framework=\"pt\", device=device) as f:\n",
            "                tensor = f.get_tensor(weight_info.get(\"weight_name\", key))\n",
            "\n",
            "            if \"dtype\" in weight_info:\n",
            "                return tensor.to(getattr(torch, weight_info[\"dtype\"]))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\offload.py",
        "line_number": 181,
        "API": ".join(",
        "context": [
            "                return tensor.to(getattr(torch, weight_info[\"dtype\"]))\n",
            "            else:\n",
            "                return tensor\n",
            "\n",
            "        weight_file = os.path.join(self.save_folder, f\"{key}.dat\")\n",
            "        return load_offloaded_weight(weight_file, weight_info)\n",
            "\n",
            "    def __iter__(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\operations.py",
        "line_number": 166,
        "API": ".to(",
        "context": [
            "            }\n",
            "        )\n",
            "    elif hasattr(tensor, \"to\"):\n",
            "        try:\n",
            "            return tensor.to(device, non_blocking=non_blocking)\n",
            "        except TypeError:  # .to() doesn't accept non_blocking as kwarg\n",
            "            return tensor.to(device)\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\operations.py",
        "line_number": 200,
        "API": ".empty(",
        "context": [
            "        The same data structure as `data` with tensors instead of [`~utils.TensorInformation`].\n",
            "    \"\"\"\n",
            "\n",
            "    def _initialize_tensor(tensor_info):\n",
            "        return torch.empty(*tensor_info.shape, dtype=tensor_info.dtype)\n",
            "\n",
            "    return recursively_apply(_initialize_tensor, data_structure, test_type=is_tensor_information)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\operations.py",
        "line_number": 242,
        "API": ".to(",
        "context": [
            "        if tensor.dtype == torch.bfloat16:\n",
            "            # As of Numpy 1.21.4, NumPy does not support bfloat16 (see\n",
            "            # https://github.com/numpy/numpy/blob/a47ecdea856986cd60eabbd53265c2ca5916ad5d/doc/source/user/basics.types.rst ).\n",
            "            # Until Numpy adds bfloat16, we must convert float32.\n",
            "            tensor = tensor.to(torch.float32)\n",
            "        return tensor.tolist()\n",
            "\n",
            "    return recursively_apply(_convert_to_list, data)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\operations.py",
        "line_number": 251,
        "API": ".clone(",
        "context": [
            "\n",
            "def _tpu_gather(tensor):\n",
            "    def _tpu_gather_one(tensor):\n",
            "        if tensor.ndim == 0:\n",
            "            tensor = tensor.clone()[None]\n",
            "\n",
            "        return xm.all_gather(tensor)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\operations.py",
        "line_number": 263,
        "API": ".clone(",
        "context": [
            "\n",
            "def _gpu_gather(tensor):\n",
            "    def _gpu_gather_one(tensor):\n",
            "        if tensor.ndim == 0:\n",
            "            tensor = tensor.clone()[None]\n",
            "        output_tensors = [torch.empty_like(tensor) for _ in range(torch.distributed.get_world_size())]\n",
            "        torch.distributed.all_gather(output_tensors, tensor)\n",
            "        return torch.cat(output_tensors, dim=0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\operations.py",
        "line_number": 336,
        "API": ".broadcast(",
        "context": [
            "\n",
            "\n",
            "def _gpu_broadcast(data, src=0):\n",
            "    def _gpu_broadcast_one(tensor, src=0):\n",
            "        torch.distributed.broadcast(tensor, src=src)\n",
            "        return tensor\n",
            "\n",
            "    return recursively_apply(_gpu_broadcast_one, data, error_on_other_type=True, src=src)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\operations.py",
        "line_number": 443,
        "API": ".cat(",
        "context": [
            "    elif isinstance(data[0], Mapping):\n",
            "        return type(data[0])({k: concatenate([d[k] for d in data], dim=dim) for k in data[0].keys()})\n",
            "    elif not isinstance(data[0], torch.Tensor):\n",
            "        raise TypeError(f\"Can only concatenate tensors but got {type(data[0])}\")\n",
            "    return torch.cat(data, dim=dim)\n",
            "\n",
            "\n",
            "def pad_across_processes(tensor, dim=0, pad_index=0, pad_first=False):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\operations.py",
        "line_number": 477,
        "API": ".new_zeros(",
        "context": [
            "\n",
            "        old_size = tensor.shape\n",
            "        new_size = list(old_size)\n",
            "        new_size[dim] = max_size\n",
            "        new_tensor = tensor.new_zeros(tuple(new_size)) + pad_index\n",
            "        if pad_first:\n",
            "            indices = tuple(\n",
            "                slice(max_size - old_size[dim], max_size) if i == dim else slice(None) for i in range(len(new_size))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\operations.py",
        "line_number": 509,
        "API": ".clone(",
        "context": [
            "    \"\"\"\n",
            "\n",
            "    def _reduce_across_processes(tensor, reduction=\"mean\"):\n",
            "        state = PartialState()\n",
            "        cloned_tensor = tensor.clone()\n",
            "        if state.distributed_type == DistributedType.NO:\n",
            "            return cloned_tensor\n",
            "        if state.distributed_type == DistributedType.TPU:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\operations.py",
        "line_number": 515,
        "API": ".all_reduce(",
        "context": [
            "            return cloned_tensor\n",
            "        if state.distributed_type == DistributedType.TPU:\n",
            "            xm.all_reduce(\"sum\", cloned_tensor)\n",
            "        elif state.distributed_type.value in CUDA_DISTRIBUTED_TYPES:\n",
            "            torch.distributed.all_reduce(cloned_tensor, ReduceOp.SUM)\n",
            "        elif state.distributed_type.value in DistributedType.MULTI_NPU:\n",
            "            torch.distributed.all_reduce(cloned_tensor, ReduceOp.SUM)\n",
            "        elif state.distributed_type.value in DistributedType.MULTI_XPU:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\operations.py",
        "line_number": 521,
        "API": ".all_reduce(",
        "context": [
            "            torch.distributed.all_reduce(cloned_tensor, ReduceOp.SUM)\n",
            "        elif state.distributed_type.value in DistributedType.MULTI_XPU:\n",
            "            torch.distributed.all_reduce(cloned_tensor, ReduceOp.SUM)\n",
            "        elif state.distributed_type == DistributedType.MULTI_CPU:\n",
            "            torch.distributed.all_reduce(cloned_tensor, ReduceOp.SUM)\n",
            "        if reduction == \"mean\":\n",
            "            cloned_tensor /= state.num_processes\n",
            "        return cloned_tensor\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\other.py",
        "line_number": 37,
        "API": ".compile(",
        "context": [
            "\n",
            "\n",
            "def is_compiled_module(module):\n",
            "    \"\"\"\n",
            "    Check whether the module was compiled with torch.compile()\n",
            "    \"\"\"\n",
            "    if is_torch_version(\"<\", \"2.0.0\") or not hasattr(torch, \"_dynamo\"):\n",
            "        return False\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\other.py",
        "line_number": 104,
        "API": ".save(",
        "context": [
            "\n",
            "\n",
            "def save(obj, f):\n",
            "    \"\"\"\n",
            "    Save the data to disk. Use in place of `torch.save()`.\n",
            "\n",
            "    Args:\n",
            "        obj: The data to save\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\other.py",
        "line_number": 111,
        "API": ".save(",
        "context": [
            "        obj: The data to save\n",
            "        f: The file (or file-like object) to use to save the data\n",
            "    \"\"\"\n",
            "    if PartialState().distributed_type == DistributedType.TPU:\n",
            "        xm.save(obj, f)\n",
            "    elif PartialState().local_process_index == 0:\n",
            "        torch.save(obj, f)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\other.py",
        "line_number": 135,
        "API": ".upper(",
        "context": [
            "    >>> print(os.environ[\"FOO\"])  # raises KeyError\n",
            "    ```\n",
            "    \"\"\"\n",
            "    for key, value in kwargs.items():\n",
            "        os.environ[key.upper()] = str(value)\n",
            "\n",
            "    yield\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\other.py",
        "line_number": 140,
        "API": ".upper(",
        "context": [
            "\n",
            "    yield\n",
            "\n",
            "    for key in kwargs:\n",
            "        if key.upper() in os.environ:\n",
            "            del os.environ[key.upper()]\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\random.py",
        "line_number": 42,
        "API": ".seed(",
        "context": [
            "            Whether to differ the seed on each device slightly with `self.process_index`.\n",
            "    \"\"\"\n",
            "    if device_specific:\n",
            "        seed += AcceleratorState().process_index\n",
            "    random.seed(seed)\n",
            "    np.random.seed(seed)\n",
            "    torch.manual_seed(seed)\n",
            "    if is_xpu_available():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\random.py",
        "line_number": 48,
        "API": ".manual_seed_all(",
        "context": [
            "    torch.manual_seed(seed)\n",
            "    if is_xpu_available():\n",
            "        torch.xpu.manual_seed_all(seed)\n",
            "    elif is_npu_available():\n",
            "        torch.npu.manual_seed_all(seed)\n",
            "    else:\n",
            "        torch.cuda.manual_seed_all(seed)\n",
            "    # ^^ safe to call this function even if cuda is not available\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\random.py",
        "line_number": 78,
        "API": ".to(",
        "context": [
            "\n",
            "    # Broadcast the rng state from device 0 to other devices\n",
            "    state = AcceleratorState()\n",
            "    if state.distributed_type == DistributedType.TPU:\n",
            "        rng_state = rng_state.to(xm.xla_device())\n",
            "        xm.collective_broadcast([rng_state])\n",
            "        xm.mark_step()\n",
            "        rng_state = rng_state.cpu()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\random.py",
        "line_number": 87,
        "API": ".to(",
        "context": [
            "        state.distributed_type in CUDA_DISTRIBUTED_TYPES\n",
            "        or state.distributed_type == DistributedType.MULTI_NPU\n",
            "        or state.distributed_type == DistributedType.MULTI_XPU\n",
            "    ):\n",
            "        rng_state = rng_state.to(state.device)\n",
            "        torch.distributed.broadcast(rng_state, 0)\n",
            "        rng_state = rng_state.cpu()\n",
            "    elif state.distributed_type == DistributedType.MULTI_CPU:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\random.py",
        "line_number": 103,
        "API": ".item(",
        "context": [
            "        torch.npu.set_rng_state(rng_state)\n",
            "    elif rng_type == RNGType.XPU:\n",
            "        torch.xpu.set_rng_state(rng_state)\n",
            "    elif rng_type == RNGType.XLA:\n",
            "        xm.set_rng_state(rng_state.item())\n",
            "    elif rng_type == RNGType.GENERATOR:\n",
            "        generator.set_state(rng_state)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\torch_xla.py",
        "line_number": 44,
        "API": ".version(",
        "context": [
            "        if upgrade:\n",
            "            torch_install_cmd = [\"pip\", \"install\", \"-U\", \"torch\"]\n",
            "            subprocess.run(torch_install_cmd, check=True)\n",
            "        # get the current version of torch\n",
            "        torch_version = importlib.metadata.version(\"torch\")\n",
            "        torch_version_trunc = torch_version[: torch_version.rindex(\".\")]\n",
            "        xla_wheel = f\"https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-{torch_version_trunc}-cp37-cp37m-linux_x86_64.whl\"\n",
            "        xla_install_cmd = [\"pip\", \"install\", xla_wheel]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\transformer_engine.py",
        "line_number": 38,
        "API": ".clone(",
        "context": [
            "            has_bias = module.bias is not None\n",
            "            te_module = te.Linear(\n",
            "                module.in_features, module.out_features, bias=has_bias, params_dtype=module.weight.dtype\n",
            "            )\n",
            "            te_module.weight.data = module.weight.data.clone()\n",
            "            if has_bias:\n",
            "                te_module.bias.data = module.bias.data.clone()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\transformer_engine.py",
        "line_number": 45,
        "API": ".clone(",
        "context": [
            "\n",
            "            setattr(model, name, te_module)\n",
            "        elif isinstance(module, nn.LayerNorm) and to_transformer_engine and _convert_ln:\n",
            "            te_module = te.LayerNorm(module.normalized_shape[0], eps=module.eps, params_dtype=module.weight.dtype)\n",
            "            te_module.weight.data = module.weight.data.clone()\n",
            "            te_module.bias.data = module.bias.data.clone()\n",
            "\n",
            "            setattr(model, name, te_module)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\transformer_engine.py",
        "line_number": 54,
        "API": ".clone(",
        "context": [
            "            has_bias = module.bias is not None\n",
            "            new_module = nn.Linear(\n",
            "                module.in_features, module.out_features, bias=has_bias, params_dtype=module.weight.dtype\n",
            "            )\n",
            "            new_module.weight.data = module.weight.data.clone()\n",
            "            if has_bias:\n",
            "                new_module.bias.data = module.bias.data.clone()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\transformer_engine.py",
        "line_number": 61,
        "API": ".clone(",
        "context": [
            "\n",
            "            setattr(model, name, new_module)\n",
            "        elif isinstance(module, te.LayerNorm) and not to_transformer_engine and _convert_ln:\n",
            "            new_module = nn.LayerNorm(module.normalized_shape[0], eps=module.eps, params_dtype=module.weight.dtype)\n",
            "            new_module.weight.data = module.weight.data.clone()\n",
            "            new_module.bias.data = module.bias.data.clone()\n",
            "\n",
            "            setattr(model, name, new_module)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\versions.py",
        "line_number": 22,
        "API": ".version(",
        "context": [
            "\n",
            "from .constants import STR_OPERATION_TO_FUNC\n",
            "\n",
            "\n",
            "torch_version = parse(importlib.metadata.version(\"torch\"))\n",
            "\n",
            "\n",
            "def compare_versions(library_or_version: Union[str, Version], operation: str, requirement_version: str):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\src\\accelerate\\utils\\versions.py",
        "line_number": 41,
        "API": ".version(",
        "context": [
            "    if operation not in STR_OPERATION_TO_FUNC.keys():\n",
            "        raise ValueError(f\"`operation` must be one of {list(STR_OPERATION_TO_FUNC.keys())}, received {operation}\")\n",
            "    operation = STR_OPERATION_TO_FUNC[operation]\n",
            "    if isinstance(library_or_version, str):\n",
            "        library_or_version = parse(importlib.metadata.version(library_or_version))\n",
            "    return operation(library_or_version, parse(requirement_version))\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\utils\\log_reports.py",
        "line_number": 30,
        "API": ".get(",
        "context": [
            "    {\n",
            "        \"type\": \"header\",\n",
            "        \"text\": {\n",
            "            \"type\": \"plain_text\",\n",
            "            \"text\": f\"\ud83e\udd17 Accelerate nightly {os.environ.get('TEST_TYPE', '')} test results\",\n",
            "            \"emoji\": True,\n",
            "        },\n",
            "    }\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\utils\\log_reports.py",
        "line_number": 37,
        "API": ".glob(",
        "context": [
            "    }\n",
            "]\n",
            "\n",
            "total_num_failed = 0\n",
            "for log in Path().glob(\"*.log\"):\n",
            "    section_num_failed = 0\n",
            "    with open(log, \"r\") as f:\n",
            "        for line in f:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\utils\\log_reports.py",
        "line_number": 42,
        "API": ".get(",
        "context": [
            "    section_num_failed = 0\n",
            "    with open(log, \"r\") as f:\n",
            "        for line in f:\n",
            "            line = json.loads(line)\n",
            "            if line.get(\"nodeid\", \"\") != \"\":\n",
            "                test = line[\"nodeid\"]\n",
            "                if line.get(\"duration\", None) is not None:\n",
            "                    duration = f'{line[\"duration\"]:.4f}'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\utils\\log_reports.py",
        "line_number": 48,
        "API": ".split(",
        "context": [
            "                if line.get(\"duration\", None) is not None:\n",
            "                    duration = f'{line[\"duration\"]:.4f}'\n",
            "                    if line.get(\"outcome\", \"\") == \"failed\":\n",
            "                        section_num_failed += 1\n",
            "                        failed.append([test, duration, log.name.split(\"_\")[0]])\n",
            "                        total_num_failed += 1\n",
            "    group_info.append([str(log), section_num_failed, failed])\n",
            "    failed = []\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\utils\\log_reports.py",
        "line_number": 66,
        "API": ".split(",
        "context": [
            "                message += f\"*{name[1:]}: {num_failed} failed tests*\\n\"\n",
            "            failed_table = []\n",
            "            files2failed = {}\n",
            "            for test in failed_tests:\n",
            "                data = test[0].split(\"::\")\n",
            "                data[0] = data[0].split(\"/\")[-1]\n",
            "                if data[0] not in files2failed:\n",
            "                    files2failed[data[0]] = [data[1:]]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\utils\\log_reports.py",
        "line_number": 99,
        "API": ".get(",
        "context": [
            "    message = \"No failed tests! \ud83e\udd17\"\n",
            "    print(f\"## {message}\")\n",
            "    payload.append(no_error_payload)\n",
            "\n",
            "if os.environ.get(\"TEST_TYPE\", \"\") != \"\":\n",
            "    from slack_sdk import WebClient\n",
            "\n",
            "    client = WebClient(token=os.environ[\"SLACK_API_TOKEN\"])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\utils\\log_reports.py",
        "line_number": 134,
        "API": ".get(",
        "context": [
            "            \"type\": \"context\",\n",
            "            \"elements\": [\n",
            "                {\n",
            "                    \"type\": \"plain_text\",\n",
            "                    \"text\": f\"Nightly {os.environ.get('TEST_TYPE')} test results for {date.today()}\",\n",
            "                }\n",
            "            ],\n",
            "        }\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\utils\\stale.py",
        "line_number": 46,
        "API": ".lower(",
        "context": [
            "            last_comment is not None\n",
            "            and last_comment.user.login == \"github-actions[bot]\"\n",
            "            and days_since_updated > 7\n",
            "            and days_since_creation >= 30\n",
            "            and not any(label.name.lower() in LABELS_TO_EXEMPT for label in issue.get_labels())\n",
            "        ):\n",
            "            # Close issue since it has been 7 days of inactivity since bot mention.\n",
            "            issue.edit(state=\"closed\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\accelerate\\utils\\stale.py",
        "line_number": 53,
        "API": ".lower(",
        "context": [
            "            issue.edit(state=\"closed\")\n",
            "        elif (\n",
            "            days_since_updated > 23\n",
            "            and days_since_creation >= 30\n",
            "            and not any(label.name.lower() in LABELS_TO_EXEMPT for label in issue.get_labels())\n",
            "        ):\n",
            "            # Add stale comment\n",
            "            issue.create_comment(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\align_faces.py",
        "line_number": 26,
        "API": ".array(",
        "context": [
            "\n",
            "\n",
            "def align(img, parts, dst_dir='realign1024x1024', output_size=1024, transform_size=4096, item_idx=0, enable_padding=True):\n",
            "    # Parse landmarks.\n",
            "    lm = np.array(parts)\n",
            "    lm_chin          = lm[0: 17]  # left-right\n",
            "    lm_eyebrow_left = lm[17: 22]  # left-right\n",
            "    lm_eyebrow_right = lm[22: 27]  # left-right\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\align_faces.py",
        "line_number": 38,
        "API": ".mean(",
        "context": [
            "    lm_mouth_outer = lm[48: 60]  # left-clockwise\n",
            "    lm_mouth_inner = lm[60: 68]  # left-clockwise\n",
            "\n",
            "    # Calculate auxiliary vectors.\n",
            "    eye_left = np.mean(lm_eye_left, axis=0)\n",
            "    eye_right = np.mean(lm_eye_right, axis=0)\n",
            "    eye_avg = (eye_left + eye_right) * 0.5\n",
            "    eye_to_eye = eye_right - eye_left\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\align_faces.py",
        "line_number": 60,
        "API": ".stack(",
        "context": [
            "    y = np.flipud(x) * [-1, 1]\n",
            "\n",
            "    if use_1024:\n",
            "        c = eye_avg + eye_to_mouth * 0.1\n",
            "        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
            "    else:\n",
            "        c = eye_avg + eye_to_mouth * 0.317\n",
            "        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\align_faces.py",
        "line_number": 70,
        "API": ".floor(",
        "context": [
            "\n",
            "    img = Image.fromarray(img)\n",
            "\n",
            "    # Shrink.\n",
            "    shrink = int(np.floor(qsize / output_size * 0.5))\n",
            "    if shrink > 1:\n",
            "        rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n",
            "        img = img.resize(rsize, PIL.Image.ANTIALIAS)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\align_faces.py",
        "line_number": 79,
        "API": ".floor(",
        "context": [
            "        qsize /= shrink\n",
            "\n",
            "    # Crop.\n",
            "    border = max(int(np.rint(qsize * 0.1)), 3)\n",
            "    crop = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),\n",
            "            int(np.ceil(max(quad[:, 1]))))\n",
            "    crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]),\n",
            "            min(crop[3] + border, img.size[1]))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\align_faces.py",
        "line_number": 88,
        "API": ".floor(",
        "context": [
            "        img = img.crop(crop)\n",
            "        quad -= crop[0:2]\n",
            "\n",
            "    # Pad.\n",
            "    pad = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),\n",
            "           int(np.ceil(max(quad[:, 1]))))\n",
            "    pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0),\n",
            "           max(pad[3] - img.size[1] + border, 0))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\align_faces.py",
        "line_number": 93,
        "API": ".maximum(",
        "context": [
            "           int(np.ceil(max(quad[:, 1]))))\n",
            "    pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0),\n",
            "           max(pad[3] - img.size[1] + border, 0))\n",
            "    if enable_padding and max(pad) > border - 4:\n",
            "        pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n",
            "        img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
            "        h, w, _ = img.shape\n",
            "        y, x, _ = np.ogrid[:h, :w, :1]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\align_faces.py",
        "line_number": 98,
        "API": ".minimum(",
        "context": [
            "        img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
            "        h, w, _ = img.shape\n",
            "        y, x, _ = np.ogrid[:h, :w, :1]\n",
            "        mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w - 1 - x) / pad[2]),\n",
            "                          1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h - 1 - y) / pad[3]))\n",
            "        blur = qsize * 0.02\n",
            "        img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
            "        img += (np.median(img, axis=(0, 1)) - img) * np.clip(mask, 0.0, 1.0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\align_faces.py",
        "line_number": 106,
        "API": ".flatten(",
        "context": [
            "        img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n",
            "        quad += pad[:2]\n",
            "\n",
            "    # Transform.\n",
            "    img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n",
            "    if output_size < transform_size:\n",
            "        img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\align_faces.py",
        "line_number": 112,
        "API": ".makedirs(",
        "context": [
            "        img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n",
            "\n",
            "    # Save aligned image.\n",
            "    dst_subdir = dst_dir\n",
            "    os.makedirs(dst_subdir, exist_ok=True)\n",
            "    img.save(os.path.join(dst_subdir, '%05d.png' % item_idx))\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\align_faces.py",
        "line_number": 129,
        "API": ".format(",
        "context": [
            "    if img.shape[2] == 4:\n",
            "        img = img[:, :, :3]\n",
            "\n",
            "    dets = detector(img, 0)\n",
            "    print(\"Number of faces detected: {}\".format(len(dets)))\n",
            "\n",
            "    for i, d in enumerate(dets):\n",
            "        print(\"Detection {}: Left: {} Top: {} Right: {} Bottom: {}\".format(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\checkpointer.py",
        "line_number": 61,
        "API": ".join(",
        "context": [
            "        data.update(kwargs)\n",
            "\n",
            "        @utils.async_func\n",
            "        def save_data():\n",
            "            save_file = os.path.join(self.cfg.OUTPUT_DIR, \"%s.pth\" % _name)\n",
            "            self.logger.info(\"Saving checkpoint to %s\" % save_file)\n",
            "            torch.save(data, save_file)\n",
            "            self.tag_last_checkpoint(save_file)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\checkpointer.py",
        "line_number": 69,
        "API": ".join(",
        "context": [
            "\n",
            "        return save_data()\n",
            "\n",
            "    def load(self, ignore_last_checkpoint=False, file_name=None):\n",
            "        save_file = os.path.join(self.cfg.OUTPUT_DIR, \"last_checkpoint\")\n",
            "        try:\n",
            "            with open(save_file, \"r\") as last_checkpoint:\n",
            "                f = last_checkpoint.read().strip()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\checkpointer.py",
        "line_number": 74,
        "API": ".info(",
        "context": [
            "        try:\n",
            "            with open(save_file, \"r\") as last_checkpoint:\n",
            "                f = last_checkpoint.read().strip()\n",
            "        except IOError:\n",
            "            self.logger.info(\"No checkpoint found. Initializing model from scratch\")\n",
            "            if file_name is None:\n",
            "                return {}\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\checkpointer.py",
        "line_number": 79,
        "API": ".info(",
        "context": [
            "            if file_name is None:\n",
            "                return {}\n",
            "\n",
            "        if ignore_last_checkpoint:\n",
            "            self.logger.info(\"Forced to Initialize model from scratch\")\n",
            "            return {}\n",
            "        if file_name is not None:\n",
            "            f = file_name\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\checkpointer.py",
        "line_number": 84,
        "API": ".info(",
        "context": [
            "            return {}\n",
            "        if file_name is not None:\n",
            "            f = file_name\n",
            "\n",
            "        self.logger.info(\"Loading checkpoint from {}\".format(f))\n",
            "        checkpoint = torch.load(f, map_location=torch.device(\"cpu\"))\n",
            "        for name, model in self.models.items():\n",
            "            if name in checkpoint[\"models\"]:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\checkpointer.py",
        "line_number": 93,
        "API": ".warning(",
        "context": [
            "                    model_dict = checkpoint[\"models\"].pop(name)\n",
            "                    if model_dict is not None:\n",
            "                        self.models[name].load_state_dict(model_dict, strict=False)\n",
            "                    else:\n",
            "                        self.logger.warning(\"State dict for model \\\"%s\\\" is None \" % name)\n",
            "                except RuntimeError as e:\n",
            "                    self.logger.warning('%s\\nFailed to load: %s\\n%s' % ('!' * 160, name, '!' * 160))\n",
            "                    self.logger.warning('\\nFailed to load: %s' % str(e))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\checkpointer.py",
        "line_number": 98,
        "API": ".warning(",
        "context": [
            "                except RuntimeError as e:\n",
            "                    self.logger.warning('%s\\nFailed to load: %s\\n%s' % ('!' * 160, name, '!' * 160))\n",
            "                    self.logger.warning('\\nFailed to load: %s' % str(e))\n",
            "            else:\n",
            "                self.logger.warning(\"No state dict for model: %s\" % name)\n",
            "        checkpoint.pop('models')\n",
            "        if \"auxiliary\" in checkpoint and self.auxiliary:\n",
            "            self.logger.info(\"Loading auxiliary from {}\".format(f))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\checkpointer.py",
        "line_number": 111,
        "API": ".warning(",
        "context": [
            "                        self.auxiliary[name].load_state_dict(checkpoint[\"optimizers\"].pop(name))\n",
            "                    if name in checkpoint:\n",
            "                        self.auxiliary[name].load_state_dict(checkpoint.pop(name))\n",
            "                except (IndexError, ValueError):\n",
            "                    self.logger.warning('%s\\nFailed to load: %s\\n%s' % ('!' * 160, name, '!' * 160))\n",
            "            checkpoint.pop('auxiliary')\n",
            "\n",
            "        return checkpoint\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\checkpointer.py",
        "line_number": 117,
        "API": ".join(",
        "context": [
            "\n",
            "        return checkpoint\n",
            "\n",
            "    def tag_last_checkpoint(self, last_filename):\n",
            "        save_file = os.path.join(self.cfg.OUTPUT_DIR, \"last_checkpoint\")\n",
            "        with open(save_file, \"w\") as f:\n",
            "            f.write(last_filename)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\custom_adam.py",
        "line_number": 26,
        "API": ".format(",
        "context": [
            "    def __init__(self, params, lr=1e-3, betas=(0.0, 0.99), eps=1e-8,\n",
            "                 weight_decay=0):\n",
            "        beta_2 = betas[1]\n",
            "        if not 0.0 <= lr:\n",
            "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
            "        if not 0.0 <= eps:\n",
            "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
            "        if not 0.0 == betas[0]:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\custom_adam.py",
        "line_number": 32,
        "API": ".format(",
        "context": [
            "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
            "        if not 0.0 == betas[0]:\n",
            "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
            "        if not 0.0 <= beta_2 < 1.0:\n",
            "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(beta_2))\n",
            "        defaults = dict(lr=lr, beta_2=beta_2, eps=eps,\n",
            "                        weight_decay=weight_decay)\n",
            "        super(LREQAdam, self).__init__(params, defaults)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\custom_adam.py",
        "line_number": 65,
        "API": ".zeros_like(",
        "context": [
            "                # State initialization\n",
            "                if len(state) == 0:\n",
            "                    state['step'] = 0\n",
            "                    # Exponential moving average of gradient values\n",
            "                    # state['exp_avg'] = torch.zeros_like(p.data)\n",
            "                    # Exponential moving average of squared gradient values\n",
            "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\custom_adam.py",
        "line_number": 81,
        "API": ".sqrt(",
        "context": [
            "\n",
            "                # Decay the first and second moment running average coefficient\n",
            "                # exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
            "                exp_avg_sq.mul_(beta_2).addcmul_(1 - beta_2, grad, grad)\n",
            "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
            "\n",
            "                # bias_correction1 = 1 - beta1 ** state['step'] # 1\n",
            "                bias_correction2 = 1 - beta_2 ** state['step']\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\custom_adam.py",
        "line_number": 86,
        "API": ".sqrt(",
        "context": [
            "\n",
            "                # bias_correction1 = 1 - beta1 ** state['step'] # 1\n",
            "                bias_correction2 = 1 - beta_2 ** state['step']\n",
            "                # step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
            "                step_size = group['lr'] * math.sqrt(bias_correction2)\n",
            "\n",
            "                # p.data.addcdiv_(-step_size, exp_avg, denom)\n",
            "                if hasattr(p, 'lr_equalization_coef'):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataloader.py",
        "line_number": 26,
        "API": ".device(",
        "context": [
            "import torch.utils.data\n",
            "import time\n",
            "import math\n",
            "\n",
            "cpu = torch.device('cpu')\n",
            "\n",
            "\n",
            "class TFRecordsDataset:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataloader.py",
        "line_number": 97,
        "API": ".info(",
        "context": [
            "        if self.seed is None:\n",
            "            seed = np.uint64(time.time() * 1000)\n",
            "        else:\n",
            "            seed = self.seed\n",
            "            self.logger.info('!' * 80)\n",
            "            self.logger.info('! Seed is used for to shuffle data in TFRecordsDataset!')\n",
            "            self.logger.info('!' * 80)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataloader.py",
        "line_number": 112,
        "API": ".device(",
        "context": [
            "\n",
            "\n",
            "def make_dataloader(cfg, logger, dataset, GPU_batch_size, local_rank, numpy=False):\n",
            "    class BatchCollator(object):\n",
            "        def __init__(self, device=torch.device(\"cpu\")):\n",
            "            self.device = device\n",
            "            self.flip = cfg.DATASET.FLIP_IMAGES\n",
            "            self.numpy = numpy\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataloader.py",
        "line_number": 122,
        "API": ".array(",
        "context": [
            "            with torch.no_grad():\n",
            "                x, = batch\n",
            "                if self.flip:\n",
            "                    flips = [(slice(None, None, None), slice(None, None, None), slice(None, None, random.choice([-1, None]))) for _ in range(x.shape[0])]\n",
            "                    x = np.array([img[flip] for img, flip in zip(x, flips)])\n",
            "                if self.numpy:\n",
            "                    return x\n",
            "                x = torch.tensor(x, requires_grad=True, device=torch.device(self.device), dtype=torch.float32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataloader.py",
        "line_number": 135,
        "API": ".device(",
        "context": [
            "\n",
            "\n",
            "def make_dataloader_y(cfg, logger, dataset, GPU_batch_size, local_rank):\n",
            "    class BatchCollator(object):\n",
            "        def __init__(self, device=torch.device(\"cpu\")):\n",
            "            self.device = device\n",
            "            self.flip = cfg.DATASET.FLIP_IMAGES\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataloader.py",
        "line_number": 144,
        "API": ".array(",
        "context": [
            "            with torch.no_grad():\n",
            "                x, y = batch\n",
            "                if self.flip:\n",
            "                    flips = [(slice(None, None, None), slice(None, None, None), slice(None, None, random.choice([-1, None]))) for _ in range(x.shape[0])]\n",
            "                    x = np.array([img[flip] for img, flip in zip(x, flips)])\n",
            "                x = torch.tensor(x, requires_grad=True, device=torch.device(self.device), dtype=torch.float32)\n",
            "                return x, y\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataloader.py",
        "line_number": 236,
        "API": ".device(",
        "context": [
            "\n",
            "\n",
            "def make_imagenet_dataloader(cfg, logger, dataset, GPU_batch_size, target_size, local_rank, do_random_crops=True):\n",
            "    class BatchCollator(object):\n",
            "        def __init__(self, device=torch.device(\"cpu\")):\n",
            "            self.device = device\n",
            "            self.flip = cfg.DATASET.FLIP_IMAGES\n",
            "            self.size = target_size\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataloader.py",
        "line_number": 257,
        "API": ".stack(",
        "context": [
            "                        offx = np.random.randint(deltax + 1)\n",
            "                        offy = np.random.randint(deltay + 1)\n",
            "                        im = im[:, offy:offy + self.size, offx:offx + self.size]\n",
            "                        images.append(im)\n",
            "                    x = np.stack(images)\n",
            "\n",
            "                if self.flip:\n",
            "                    flips = [(slice(None, None, None), slice(None, None, None), slice(None, None, random.choice([-1, None]))) for _ in range(x.shape[0])]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataloader.py",
        "line_number": 262,
        "API": ".device(",
        "context": [
            "\n",
            "                if self.flip:\n",
            "                    flips = [(slice(None, None, None), slice(None, None, None), slice(None, None, random.choice([-1, None]))) for _ in range(x.shape[0])]\n",
            "                    x = np.array([img[flip] for img, flip in zip(x, flips)])\n",
            "                x = torch.tensor(x, requires_grad=True, device=torch.device(self.device), dtype=torch.float32)\n",
            "\n",
            "                return x\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataloader.py",
        "line_number": 273,
        "API": ".device(",
        "context": [
            "\n",
            "\n",
            "def make_imagenet_dataloader_y(cfg, logger, dataset, GPU_batch_size, target_size, local_rank, do_random_crops=True):\n",
            "    class BatchCollator(object):\n",
            "        def __init__(self, device=torch.device(\"cpu\")):\n",
            "            self.device = device\n",
            "            self.flip = cfg.DATASET.FLIP_IMAGES\n",
            "            self.size = target_size\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataloader.py",
        "line_number": 294,
        "API": ".stack(",
        "context": [
            "                        offx = np.random.randint(deltax + 1)\n",
            "                        offy = np.random.randint(deltay + 1)\n",
            "                        im = im[:, offy:offy+self.size, offx:offx+self.size]\n",
            "                        images.append(im)\n",
            "                    x = np.stack(images)\n",
            "\n",
            "                if self.flip:\n",
            "                    flips = [(slice(None, None, None), slice(None, None, None), slice(None, None, random.choice([-1, None]))) for _ in range(x.shape[0])]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataloader.py",
        "line_number": 299,
        "API": ".device(",
        "context": [
            "\n",
            "                if self.flip:\n",
            "                    flips = [(slice(None, None, None), slice(None, None, None), slice(None, None, random.choice([-1, None]))) for _ in range(x.shape[0])]\n",
            "                    x = np.array([img[flip] for img, flip in zip(x, flips)])\n",
            "                x = torch.tensor(x, requires_grad=True, device=torch.device(self.device), dtype=torch.float32)\n",
            "                return x, y\n",
            "\n",
            "    batches = db.data_loader(iter(dataset), BatchCollator(local_rank), len(dataset) // GPU_batch_size)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\defaults.py",
        "line_number": 82,
        "API": ".clone(",
        "context": [
            "_C.TRAIN.LEARNING_RATES = [0.002]\n",
            "\n",
            "\n",
            "def get_cfg_defaults():\n",
            "    return _C.clone()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\interactive_demo.py",
        "line_number": 46,
        "API": ".set_device(",
        "context": [
            "          ]\n",
            "\n",
            "\n",
            "def sample(cfg, logger):\n",
            "    torch.cuda.set_device(0)\n",
            "    model = Model(\n",
            "        startf=cfg.MODEL.START_CHANNEL_COUNT,\n",
            "        layer_count=cfg.MODEL.LAYER_COUNT,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\interactive_demo.py",
        "line_number": 68,
        "API": ".info(",
        "context": [
            "    mapping_tl = model.mapping_d\n",
            "    mapping_fl = model.mapping_f\n",
            "    dlatent_avg = model.dlatent_avg\n",
            "\n",
            "    logger.info(\"Trainable parameters generator:\")\n",
            "    count_parameters(decoder)\n",
            "\n",
            "    logger.info(\"Trainable parameters discriminator:\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\interactive_demo.py",
        "line_number": 91,
        "API": ".load(",
        "context": [
            "                                {},\n",
            "                                logger=logger,\n",
            "                                save=False)\n",
            "\n",
            "    extra_checkpoint_data = checkpointer.load()\n",
            "\n",
            "    model.eval()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\interactive_demo.py",
        "line_number": 99,
        "API": ".repeat(",
        "context": [
            "    layer_count = cfg.MODEL.LAYER_COUNT\n",
            "\n",
            "    def encode(x):\n",
            "        Z, _ = model.encode(x, layer_count - 1, 1)\n",
            "        Z = Z.repeat(1, model.mapping_f.num_layers, 1)\n",
            "        return Z\n",
            "\n",
            "    def decode(x):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\interactive_demo.py",
        "line_number": 104,
        "API": ".ones(",
        "context": [
            "        return Z\n",
            "\n",
            "    def decode(x):\n",
            "        layer_idx = torch.arange(2 * layer_count)[np.newaxis, :, np.newaxis]\n",
            "        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n",
            "        coefs = torch.where(layer_idx < model.truncation_cutoff, ones, ones)\n",
            "        # x = torch.lerp(model.dlatent_avg.buff.data, x, coefs)\n",
            "        return model.decoder(x, layer_count - 1, 1, noise=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\interactive_demo.py",
        "line_number": 112,
        "API": ".sort(",
        "context": [
            "\n",
            "    path = 'dataset_samples/faces/realign1024x1024'\n",
            "\n",
            "    paths = list(os.listdir(path))\n",
            "    paths.sort()\n",
            "    paths_backup = paths[:]\n",
            "    randomize = bimpy.Bool(True)\n",
            "    current_file = bimpy.String(\"\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\interactive_demo.py",
        "line_number": 121,
        "API": ".load(",
        "context": [
            "    ctx = bimpy.Context()\n",
            "\n",
            "    attribute_values = [bimpy.Float(0) for i in indices]\n",
            "\n",
            "    W = [torch.tensor(np.load(\"principal_directions/direction_%d.npy\" % i), dtype=torch.float32) for i in indices]\n",
            "\n",
            "    rnd = np.random.RandomState(5)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\interactive_demo.py",
        "line_number": 134,
        "API": ".transpose(",
        "context": [
            "            paths.extend(paths_backup)\n",
            "\n",
            "        if img.shape[2] == 4:\n",
            "            img = img[:, :, :3]\n",
            "        im = img.transpose((2, 0, 1))\n",
            "        x = torch.tensor(np.asarray(im, dtype=np.float32), device='cpu', requires_grad=True).cuda() / 127.5 - 1.\n",
            "        if x.shape[0] == 4:\n",
            "            x = x[:3]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\interactive_demo.py",
        "line_number": 145,
        "API": ".transpose(",
        "context": [
            "            x = F.avg_pool2d(x, 2, 2)\n",
            "        if x.shape[2] != needed_resolution:\n",
            "            x = F.adaptive_avg_pool2d(x, (needed_resolution, needed_resolution))\n",
            "\n",
            "        img_src = ((x * 0.5 + 0.5) * 255).type(torch.long).clamp(0, 255).cpu().type(torch.uint8).transpose(0, 2).transpose(0, 1).numpy()\n",
            "\n",
            "        latents_original = encode(x[None, ...].cuda())\n",
            "        latents = latents_original[0, 0].clone()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\interactive_demo.py",
        "line_number": 152,
        "API": ".sum(",
        "context": [
            "        latents = latents_original[0, 0].clone()\n",
            "        latents -= model.dlatent_avg.buff.data[0]\n",
            "\n",
            "        for v, w in zip(attribute_values, W):\n",
            "            v.value = (latents * w).sum()\n",
            "\n",
            "        for v, w in zip(attribute_values, W):\n",
            "            latents = latents - v.value * w\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\interactive_demo.py",
        "line_number": 160,
        "API": ".randn(",
        "context": [
            "\n",
            "        return latents, latents_original, img_src\n",
            "\n",
            "    def loadRandom():\n",
            "        latents = rnd.randn(1, cfg.MODEL.LATENT_SPACE_SIZE)\n",
            "        lat = torch.tensor(latents).float().cuda()\n",
            "        dlat = mapping_fl(lat)\n",
            "        layer_idx = torch.arange(2 * layer_count)[np.newaxis, :, np.newaxis]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\interactive_demo.py",
        "line_number": 165,
        "API": ".where(",
        "context": [
            "        lat = torch.tensor(latents).float().cuda()\n",
            "        dlat = mapping_fl(lat)\n",
            "        layer_idx = torch.arange(2 * layer_count)[np.newaxis, :, np.newaxis]\n",
            "        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n",
            "        coefs = torch.where(layer_idx < model.truncation_cutoff, ones, ones)\n",
            "        dlat = torch.lerp(model.dlatent_avg.buff.data, dlat, coefs)\n",
            "        x = decode(dlat)[0]\n",
            "        img_src = ((x * 0.5 + 0.5) * 255).type(torch.long).clamp(0, 255).cpu().type(torch.uint8).transpose(0, 2).transpose(0, 1).numpy()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\interactive_demo.py",
        "line_number": 170,
        "API": ".clone(",
        "context": [
            "        dlat = torch.lerp(model.dlatent_avg.buff.data, dlat, coefs)\n",
            "        x = decode(dlat)[0]\n",
            "        img_src = ((x * 0.5 + 0.5) * 255).type(torch.long).clamp(0, 255).cpu().type(torch.uint8).transpose(0, 2).transpose(0, 1).numpy()\n",
            "        latents_original = dlat\n",
            "        latents = latents_original[0, 0].clone()\n",
            "        latents -= model.dlatent_avg.buff.data[0]\n",
            "\n",
            "        for v, w in zip(attribute_values, W):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\interactive_demo.py",
        "line_number": 183,
        "API": ".init(",
        "context": [
            "        return latents, latents_original, img_src\n",
            "\n",
            "    latents, latents_original, img_src = loadNext()\n",
            "\n",
            "    ctx.init(1800, 1600, \"Styles\")\n",
            "\n",
            "    def update_image(w, latents_original):\n",
            "        with torch.no_grad():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\interactive_demo.py",
        "line_number": 188,
        "API": ".repeat(",
        "context": [
            "\n",
            "    def update_image(w, latents_original):\n",
            "        with torch.no_grad():\n",
            "            w = w + model.dlatent_avg.buff.data[0]\n",
            "            w = w[None, None, ...].repeat(1, model.mapping_f.num_layers, 1)\n",
            "\n",
            "            layer_idx = torch.arange(model.mapping_f.num_layers)[np.newaxis, :, np.newaxis]\n",
            "            cur_layers = (7 + 1) * 2\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\interactive_demo.py",
        "line_number": 193,
        "API": ".where(",
        "context": [
            "\n",
            "            layer_idx = torch.arange(model.mapping_f.num_layers)[np.newaxis, :, np.newaxis]\n",
            "            cur_layers = (7 + 1) * 2\n",
            "            mixing_cutoff = cur_layers\n",
            "            styles = torch.where(layer_idx < mixing_cutoff, w, latents_original)\n",
            "\n",
            "            x_rec = decode(styles)\n",
            "            resultsample = ((x_rec * 0.5 + 0.5) * 255).type(torch.long).clamp(0, 255)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\interactive_demo.py",
        "line_number": 198,
        "API": ".transpose(",
        "context": [
            "\n",
            "            x_rec = decode(styles)\n",
            "            resultsample = ((x_rec * 0.5 + 0.5) * 255).type(torch.long).clamp(0, 255)\n",
            "            resultsample = resultsample.cpu()[0, :, :, :]\n",
            "            return resultsample.type(torch.uint8).transpose(0, 2).transpose(0, 1)\n",
            "\n",
            "    im_size = 2 ** (cfg.MODEL.LAYER_COUNT + 1)\n",
            "    im = update_image(latents, latents_original)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\interactive_demo.py",
        "line_number": 221,
        "API": ".image(",
        "context": [
            "\n",
            "            bimpy.begin(\"Principal directions\")\n",
            "            bimpy.columns(2)\n",
            "            bimpy.set_column_width(0, im_size + 20)\n",
            "            bimpy.image(im)\n",
            "            bimpy.next_column()\n",
            "\n",
            "            for v, label in zip(attribute_values, labels):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\interactive_demo.py",
        "line_number": 232,
        "API": ".manual_seed(",
        "context": [
            "\n",
            "            if randomize.value:\n",
            "                seed += 1\n",
            "\n",
            "            torch.manual_seed(seed)\n",
            "\n",
            "            if bimpy.button('Next'):\n",
            "                latents, latents_original, img_src = loadNext()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\launcher.py",
        "line_number": 28,
        "API": ".init_process_group(",
        "context": [
            "\n",
            "def setup(rank, world_size):\n",
            "    os.environ['MASTER_ADDR'] = 'localhost'\n",
            "    os.environ['MASTER_PORT'] = '12355'\n",
            "    distributed.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
            "\n",
            "\n",
            "def cleanup():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\launcher.py",
        "line_number": 39,
        "API": ".set_device(",
        "context": [
            "def _run(rank, world_size, fn, defaults, write_log, no_cuda, args):\n",
            "    if world_size > 1:\n",
            "        setup(rank, world_size)\n",
            "    if not no_cuda:\n",
            "        torch.cuda.set_device(rank)\n",
            "\n",
            "    cfg = defaults\n",
            "    config_file = args.config_file\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\launcher.py",
        "line_number": 45,
        "API": ".join(",
        "context": [
            "    cfg = defaults\n",
            "    config_file = args.config_file\n",
            "    if len(os.path.splitext(config_file)[1]) == 0:\n",
            "        config_file += '.yaml'\n",
            "    if not os.path.exists(config_file) and os.path.exists(os.path.join('configs', config_file)):\n",
            "        config_file = os.path.join('configs', config_file)\n",
            "    cfg.merge_from_file(config_file)\n",
            "    cfg.merge_from_list(args.opts)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\launcher.py",
        "line_number": 55,
        "API": ".makedirs(",
        "context": [
            "    logger = logging.getLogger(\"logger\")\n",
            "    logger.setLevel(logging.DEBUG)\n",
            "\n",
            "    output_dir = cfg.OUTPUT_DIR\n",
            "    os.makedirs(output_dir, exist_ok=True)\n",
            "\n",
            "    if rank == 0:\n",
            "        ch = logging.StreamHandler(stream=sys.stdout)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\launcher.py",
        "line_number": 65,
        "API": ".join(",
        "context": [
            "        ch.setFormatter(formatter)\n",
            "        logger.addHandler(ch)\n",
            "\n",
            "        if write_log:\n",
            "            filepath = os.path.join(output_dir, 'log.txt')\n",
            "            if isinstance(write_log, str):\n",
            "                filepath = write_log\n",
            "            fh = logging.FileHandler(filepath)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\launcher.py",
        "line_number": 73,
        "API": ".info(",
        "context": [
            "            fh.setLevel(logging.DEBUG)\n",
            "            fh.setFormatter(formatter)\n",
            "            logger.addHandler(fh)\n",
            "\n",
            "    logger.info(args)\n",
            "\n",
            "    logger.info(\"World size: {}\".format(world_size))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\launcher.py",
        "line_number": 80,
        "API": ".info(",
        "context": [
            "\n",
            "    logger.info(\"Loaded configuration file {}\".format(config_file))\n",
            "    with open(config_file, \"r\") as cf:\n",
            "        config_str = \"\\n\" + cf.read()\n",
            "        logger.info(config_str)\n",
            "    logger.info(\"Running with config:\\n{}\".format(cfg))\n",
            "\n",
            "    if not no_cuda:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\launcher.py",
        "line_number": 85,
        "API": ".current_device(",
        "context": [
            "    logger.info(\"Running with config:\\n{}\".format(cfg))\n",
            "\n",
            "    if not no_cuda:\n",
            "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
            "        device = torch.cuda.current_device()\n",
            "        print(\"Running on \", torch.cuda.get_device_name(device))\n",
            "\n",
            "    args.distributed = world_size > 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\launcher.py",
        "line_number": 118,
        "API": ".cpu_count(",
        "context": [
            "        nargs=argparse.REMAINDER,\n",
            "    )\n",
            "\n",
            "    import multiprocessing\n",
            "    cpu_count = multiprocessing.cpu_count()\n",
            "    os.environ[\"OMP_NUM_THREADS\"] = str(max(1, int(cpu_count / world_size)))\n",
            "    del multiprocessing\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\lod_driver.py",
        "line_number": 66,
        "API": ".sin(",
        "context": [
            "        if self.cfg.TRAIN.EPOCHS_PER_LOD == 0:\n",
            "            return 1\n",
            "        blend_factor = float((self.current_epoch % self.cfg.TRAIN.EPOCHS_PER_LOD) * self.dataset_size + self.iteration)\n",
            "        blend_factor /= float(self.cfg.TRAIN.EPOCHS_PER_LOD // 2 * self.dataset_size)\n",
            "        blend_factor = math.sin(blend_factor * math.pi - 0.5 * math.pi) * 0.5 + 0.5\n",
            "\n",
            "        if not self.in_transition:\n",
            "            blend_factor = 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\lod_driver.py",
        "line_number": 104,
        "API": ".info(",
        "context": [
            "\n",
            "        new_lod = min(self.cfg.MODEL.LAYER_COUNT - 1, epoch // self.cfg.TRAIN.EPOCHS_PER_LOD)\n",
            "        if new_lod != self.lod:\n",
            "            self.lod = new_lod\n",
            "            self.logger.info(\"#\" * 80)\n",
            "            self.logger.info(\"# Switching LOD to %d\" % self.lod)\n",
            "            self.logger.info(\"# Starting transition\")\n",
            "            self.logger.info(\"#\" * 80)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\lod_driver.py",
        "line_number": 118,
        "API": ".info(",
        "context": [
            "        new_in_transition = is_in_first_half_of_cycle and is_growing\n",
            "\n",
            "        if new_in_transition != self.in_transition:\n",
            "            self.in_transition = new_in_transition\n",
            "            self.logger.info(\"#\" * 80)\n",
            "            self.logger.info(\"# Transition ended\")\n",
            "            self.logger.info(\"#\" * 80)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\losses.py",
        "line_number": 24,
        "API": ".exp(",
        "context": [
            "           'discriminator_gradient_penalty', 'generator_logistic_non_saturating']\n",
            "\n",
            "\n",
            "def kl(mu, log_var):\n",
            "    return -0.5 * torch.mean(torch.mean(1 + log_var - mu.pow(2) - log_var.exp(), 1))\n",
            "\n",
            "\n",
            "def reconstruction(recon_x, x, lod=None):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\losses.py",
        "line_number": 35,
        "API": ".sum(",
        "context": [
            "def discriminator_logistic_simple_gp(d_result_fake, d_result_real, reals, r1_gamma=10.0):\n",
            "    loss = (F.softplus(d_result_fake) + F.softplus(-d_result_real))\n",
            "\n",
            "    if r1_gamma != 0.0:\n",
            "        real_loss = d_result_real.sum()\n",
            "        real_grads = torch.autograd.grad(real_loss, reals, create_graph=True, retain_graph=True)[0]\n",
            "        r1_penalty = torch.sum(real_grads.pow(2.0), dim=[1, 2, 3])\n",
            "        loss = loss + r1_penalty * (r1_gamma * 0.5)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\losses.py",
        "line_number": 43,
        "API": ".sum(",
        "context": [
            "    return loss.mean()\n",
            "\n",
            "\n",
            "def discriminator_gradient_penalty(d_result_real, reals, r1_gamma=10.0):\n",
            "    real_loss = d_result_real.sum()\n",
            "    real_grads = torch.autograd.grad(real_loss, reals, create_graph=True, retain_graph=True)[0]\n",
            "    r1_penalty = torch.sum(real_grads.pow(2.0), dim=[1, 2, 3])\n",
            "    loss = r1_penalty * (r1_gamma * 0.5)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\losses.py",
        "line_number": 51,
        "API": ".mean(",
        "context": [
            "    return loss.mean()\n",
            "\n",
            "\n",
            "def generator_logistic_non_saturating(d_result_fake):\n",
            "    return F.softplus(-d_result_fake).mean()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\lreq.py",
        "line_number": 52,
        "API": ".sqrt(",
        "context": [
            "    return tuple([x for _ in range(n)])\n",
            "\n",
            "\n",
            "class Linear(nn.Module):\n",
            "    def __init__(self, in_features, out_features, bias=True, gain=np.sqrt(2.0), lrmul=1.0, implicit_lreq=use_implicit_lreq):\n",
            "        super(Linear, self).__init__()\n",
            "        self.in_features = in_features\n",
            "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\lreq.py",
        "line_number": 67,
        "API": ".sqrt(",
        "context": [
            "        self.implicit_lreq = implicit_lreq\n",
            "        self.reset_parameters()\n",
            "\n",
            "    def reset_parameters(self):\n",
            "        self.std = self.gain / np.sqrt(self.in_features) * self.lrmul\n",
            "        if not self.implicit_lreq:\n",
            "            init.normal_(self.weight, mean=0, std=1.0 / self.lrmul)\n",
            "        else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\lreq.py",
        "line_number": 92,
        "API": ".sqrt(",
        "context": [
            "\n",
            "\n",
            "class Conv2d(nn.Module):\n",
            "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, dilation=1,\n",
            "                 groups=1, bias=True, gain=np.sqrt(2.0), transpose=False, transform_kernel=False, lrmul=1.0,\n",
            "                 implicit_lreq=use_implicit_lreq):\n",
            "        super(Conv2d, self).__init__()\n",
            "        if in_channels % groups != 0:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\lreq.py",
        "line_number": 110,
        "API": ".prod(",
        "context": [
            "        self.groups = groups\n",
            "        self.gain = gain\n",
            "        self.lrmul = lrmul\n",
            "        self.transpose = transpose\n",
            "        self.fan_in = np.prod(self.kernel_size) * in_channels // groups\n",
            "        self.transform_kernel = transform_kernel\n",
            "        if transpose:\n",
            "            self.weight = Parameter(torch.Tensor(in_channels, out_channels // groups, *self.kernel_size))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\lreq.py",
        "line_number": 125,
        "API": ".sqrt(",
        "context": [
            "        self.implicit_lreq = implicit_lreq\n",
            "        self.reset_parameters()\n",
            "\n",
            "    def reset_parameters(self):\n",
            "        self.std = self.gain / np.sqrt(self.fan_in) * self.lrmul\n",
            "        if not self.implicit_lreq:\n",
            "            init.normal_(self.weight, mean=0, std=1.0 / self.lrmul)\n",
            "        else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\lreq.py",
        "line_number": 142,
        "API": ".pad(",
        "context": [
            "    def forward(self, x):\n",
            "        if self.transpose:\n",
            "            w = self.weight\n",
            "            if self.transform_kernel:\n",
            "                w = F.pad(w, (1, 1, 1, 1), mode='constant')\n",
            "                w = w[:, :, 1:, 1:] + w[:, :, :-1, 1:] + w[:, :, 1:, :-1] + w[:, :, :-1, :-1]\n",
            "            if not self.implicit_lreq:\n",
            "                bias = self.bias\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\lreq.py",
        "line_number": 158,
        "API": ".pad(",
        "context": [
            "                                          groups=self.groups)\n",
            "        else:\n",
            "            w = self.weight\n",
            "            if self.transform_kernel:\n",
            "                w = F.pad(w, (1, 1, 1, 1), mode='constant')\n",
            "                w = (w[:, :, 1:, 1:] + w[:, :, :-1, 1:] + w[:, :, 1:, :-1] + w[:, :, :-1, :-1]) * 0.25\n",
            "            if not self.implicit_lreq:\n",
            "                bias = self.bias\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\lreq.py",
        "line_number": 164,
        "API": ".conv2d(",
        "context": [
            "            if not self.implicit_lreq:\n",
            "                bias = self.bias\n",
            "                if bias is not None:\n",
            "                    bias = bias * self.lrmul\n",
            "                return F.conv2d(x, w * self.std, bias, stride=self.stride, padding=self.padding,\n",
            "                                dilation=self.dilation, groups=self.groups)\n",
            "            else:\n",
            "                return F.conv2d(x, w, self.bias, stride=self.stride, padding=self.padding,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\lreq.py",
        "line_number": 173,
        "API": ".sqrt(",
        "context": [
            "\n",
            "\n",
            "class ConvTranspose2d(Conv2d):\n",
            "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, dilation=1,\n",
            "                 groups=1, bias=True, gain=np.sqrt(2.0), transform_kernel=False, lrmul=1.0, implicit_lreq=use_implicit_lreq):\n",
            "        super(ConvTranspose2d, self).__init__(in_channels=in_channels,\n",
            "                                              out_channels=out_channels,\n",
            "                                              kernel_size=kernel_size,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\lreq.py",
        "line_number": 192,
        "API": ".sqrt(",
        "context": [
            "\n",
            "\n",
            "class SeparableConv2d(nn.Module):\n",
            "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, dilation=1,\n",
            "                 bias=True, gain=np.sqrt(2.0), transpose=False):\n",
            "        super(SeparableConv2d, self).__init__()\n",
            "        self.spatial_conv = Conv2d(in_channels, in_channels, kernel_size, stride, padding, output_padding, dilation,\n",
            "                                   in_channels, False, 1, transpose)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\lreq.py",
        "line_number": 204,
        "API": ".sqrt(",
        "context": [
            "\n",
            "\n",
            "class SeparableConvTranspose2d(Conv2d):\n",
            "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, dilation=1,\n",
            "                 bias=True, gain=np.sqrt(2.0)):\n",
            "        super(SeparableConvTranspose2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding,\n",
            "                                              output_padding, dilation, bias, gain, True)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\model.py",
        "line_number": 24,
        "API": ".zeros(",
        "context": [
            "\n",
            "class DLatent(nn.Module):\n",
            "    def __init__(self, dlatent_size, layer_count):\n",
            "        super(DLatent, self).__init__()\n",
            "        buffer = torch.zeros(layer_count, dlatent_size, dtype=torch.float32)\n",
            "        self.register_buffer('buff', buffer)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\model.py",
        "line_number": 73,
        "API": ".randn(",
        "context": [
            "        self.truncation_cutoff = truncation_cutoff\n",
            "\n",
            "    def generate(self, lod, blend_factor, z=None, count=32, mixing=True, noise=True, return_styles=False, no_truncation=False):\n",
            "        if z is None:\n",
            "            z = torch.randn(count, self.latent_size)\n",
            "        styles = self.mapping_f(z)[:, 0]\n",
            "        s = styles.view(styles.shape[0], 1, styles.shape[1])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\model.py",
        "line_number": 81,
        "API": ".mean(",
        "context": [
            "        styles = s.repeat(1, self.mapping_f.num_layers, 1)\n",
            "\n",
            "        if self.dlatent_avg_beta is not None:\n",
            "            with torch.no_grad():\n",
            "                batch_avg = styles.mean(dim=0)\n",
            "                self.dlatent_avg.buff.data.lerp_(batch_avg.data, 1.0 - self.dlatent_avg_beta)\n",
            "\n",
            "        if mixing and self.style_mixing_prob is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\model.py",
        "line_number": 86,
        "API": ".randn(",
        "context": [
            "                self.dlatent_avg.buff.data.lerp_(batch_avg.data, 1.0 - self.dlatent_avg_beta)\n",
            "\n",
            "        if mixing and self.style_mixing_prob is not None:\n",
            "            if random.random() < self.style_mixing_prob:\n",
            "                z2 = torch.randn(count, self.latent_size)\n",
            "                styles2 = self.mapping_f(z2)[:, 0]\n",
            "                styles2 = styles2.view(styles2.shape[0], 1, styles2.shape[1]).repeat(1, self.mapping_f.num_layers, 1)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\model.py",
        "line_number": 93,
        "API": ".where(",
        "context": [
            "\n",
            "                layer_idx = torch.arange(self.mapping_f.num_layers)[np.newaxis, :, np.newaxis]\n",
            "                cur_layers = (lod + 1) * 2\n",
            "                mixing_cutoff = random.randint(1, cur_layers)\n",
            "                styles = torch.where(layer_idx < mixing_cutoff, styles, styles2)\n",
            "\n",
            "        if (self.truncation_psi is not None) and not no_truncation:\n",
            "            layer_idx = torch.arange(self.mapping_f.num_layers)[np.newaxis, :, np.newaxis]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\model.py",
        "line_number": 98,
        "API": ".where(",
        "context": [
            "\n",
            "        if (self.truncation_psi is not None) and not no_truncation:\n",
            "            layer_idx = torch.arange(self.mapping_f.num_layers)[np.newaxis, :, np.newaxis]\n",
            "            ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n",
            "            coefs = torch.where(layer_idx < self.truncation_cutoff, self.truncation_psi * ones, ones)\n",
            "            styles = torch.lerp(self.dlatent_avg.buff.data, styles, coefs)\n",
            "\n",
            "        rec = self.decoder.forward(styles, lod, blend_factor, noise)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\model.py",
        "line_number": 116,
        "API": ".randn(",
        "context": [
            "    def forward(self, x, lod, blend_factor, d_train, ae):\n",
            "        if ae:\n",
            "            self.encoder.requires_grad_(True)\n",
            "\n",
            "            z = torch.randn(x.shape[0], self.latent_size)\n",
            "            s, rec = self.generate(lod, blend_factor, z=z, mixing=False, noise=True, return_styles=True)\n",
            "\n",
            "            Z, d_result_real = self.encode(rec, lod, blend_factor)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\model.py",
        "line_number": 124,
        "API": ".mean(",
        "context": [
            "\n",
            "            assert Z.shape == s.shape\n",
            "\n",
            "            if self.z_regression:\n",
            "                Lae = torch.mean(((Z[:, 0] - z)**2))\n",
            "            else:\n",
            "                Lae = torch.mean(((Z - s.detach())**2))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\model.py",
        "line_number": 144,
        "API": ".randn(",
        "context": [
            "            loss_d = losses.discriminator_logistic_simple_gp(d_result_fake, d_result_real, x)\n",
            "            return loss_d\n",
            "        else:\n",
            "            with torch.no_grad():\n",
            "                z = torch.randn(x.shape[0], self.latent_size)\n",
            "\n",
            "            self.encoder.requires_grad_(False)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\model.py",
        "line_number": 196,
        "API": ".view(",
        "context": [
            "        self.truncation_cutoff = truncation_cutoff\n",
            "\n",
            "    def generate(self, lod, blend_factor, z=None):\n",
            "        styles = self.mapping_f(z)[:, 0]\n",
            "        s = styles.view(styles.shape[0], 1, styles.shape[1])\n",
            "\n",
            "        styles = s.repeat(1, self.mapping_f.num_layers, 1)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\model.py",
        "line_number": 201,
        "API": ".ones(",
        "context": [
            "\n",
            "        styles = s.repeat(1, self.mapping_f.num_layers, 1)\n",
            "\n",
            "        layer_idx = torch.arange(self.mapping_f.num_layers)[np.newaxis, :, np.newaxis]\n",
            "        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n",
            "        coefs = torch.where(layer_idx < self.truncation_cutoff, self.truncation_psi * ones, ones)\n",
            "        styles = torch.lerp(self.dlatent_avg.buff.data, styles, coefs)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 28,
        "API": ".rsqrt(",
        "context": [
            "from registry import *\n",
            "\n",
            "\n",
            "def pixel_norm(x, epsilon=1e-8):\n",
            "    return x * torch.rsqrt(torch.mean(x.pow(2.0), dim=1, keepdim=True) + epsilon)\n",
            "\n",
            "\n",
            "def style_mod(x, style):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 38,
        "API": ".reshape(",
        "context": [
            "\n",
            "\n",
            "def upscale2d(x, factor=2):\n",
            "    s = x.shape\n",
            "    x = torch.reshape(x, [-1, s[1], s[2], 1, s[3], 1])\n",
            "    x = x.repeat(1, 1, 1, factor, 1, factor)\n",
            "    x = torch.reshape(x, [-1, s[1], s[2] * factor, s[3] * factor])\n",
            "    return x\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 51,
        "API": ".array(",
        "context": [
            "\n",
            "class Blur(nn.Module):\n",
            "    def __init__(self, channels):\n",
            "        super(Blur, self).__init__()\n",
            "        f = np.array([1, 2, 1], dtype=np.float32)\n",
            "        f = f[:, np.newaxis] * f[np.newaxis, :]\n",
            "        f /= np.sum(f)\n",
            "        kernel = torch.Tensor(f).view(1, 1, 3, 3).repeat(channels, 1, 1, 1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 59,
        "API": ".conv2d(",
        "context": [
            "        self.register_buffer('weight', kernel)\n",
            "        self.groups = channels\n",
            "\n",
            "    def forward(self, x):\n",
            "        return F.conv2d(x, weight=self.weight, groups=self.groups, padding=1)\n",
            "\n",
            "\n",
            "class EncodeBlock(nn.Module):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 95,
        "API": ".mean(",
        "context": [
            "    def forward(self, x):\n",
            "        x = self.conv_1(x) + self.bias_1\n",
            "        x = F.leaky_relu(x, 0.2)\n",
            "\n",
            "        m = torch.mean(x, dim=[2, 3], keepdim=True)\n",
            "        std = torch.sqrt(torch.mean((x - m) ** 2, dim=[2, 3], keepdim=True))\n",
            "        style_1 = torch.cat((m, std), dim=1)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 102,
        "API": ".view(",
        "context": [
            "\n",
            "        x = self.instance_norm_1(x)\n",
            "\n",
            "        if self.last:\n",
            "            x = self.dense(x.view(x.shape[0], -1))\n",
            "\n",
            "            x = F.leaky_relu(x, 0.2)\n",
            "            w1 = self.style_1(style_1.view(style_1.shape[0], style_1.shape[1]))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 115,
        "API": ".mean(",
        "context": [
            "            x = x + self.bias_2\n",
            "\n",
            "            x = F.leaky_relu(x, 0.2)\n",
            "\n",
            "            m = torch.mean(x, dim=[2, 3], keepdim=True)\n",
            "            std = torch.sqrt(torch.mean((x - m) ** 2, dim=[2, 3], keepdim=True))\n",
            "            style_2 = torch.cat((m, std), dim=1)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 121,
        "API": ".view(",
        "context": [
            "            style_2 = torch.cat((m, std), dim=1)\n",
            "\n",
            "            x = self.instance_norm_2(x)\n",
            "\n",
            "            w1 = self.style_1(style_1.view(style_1.shape[0], style_1.shape[1]))\n",
            "            w2 = self.style_2(style_2.view(style_2.shape[0], style_2.shape[1]))\n",
            "\n",
            "        return x, w1, w2\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 158,
        "API": ".view(",
        "context": [
            "        x = self.conv_1(x) + self.bias_1\n",
            "        x = F.leaky_relu(x, 0.2)\n",
            "\n",
            "        if self.dense_:\n",
            "            x = self.dense(x.view(x.shape[0], -1))\n",
            "        else:\n",
            "            x = self.conv_2(self.blur(x))\n",
            "            if not self.fused_scale:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 212,
        "API": ".randn(",
        "context": [
            "\n",
            "        if noise:\n",
            "            if noise == 'batch_constant':\n",
            "                x = torch.addcmul(x, value=1.0, tensor1=self.noise_weight_1,\n",
            "                                  tensor2=torch.randn([1, 1, x.shape[2], x.shape[3]]))\n",
            "            else:\n",
            "                x = torch.addcmul(x, value=1.0, tensor1=self.noise_weight_1,\n",
            "                                  tensor2=torch.randn([x.shape[0], 1, x.shape[2], x.shape[3]]))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 217,
        "API": ".pow(",
        "context": [
            "            else:\n",
            "                x = torch.addcmul(x, value=1.0, tensor1=self.noise_weight_1,\n",
            "                                  tensor2=torch.randn([x.shape[0], 1, x.shape[2], x.shape[3]]))\n",
            "        else:\n",
            "            s = math.pow(self.layer + 1, 0.5)\n",
            "            x = x + s * torch.exp(-x * x / (2.0 * s * s)) / math.sqrt(2 * math.pi) * 0.8\n",
            "        x = x + self.bias_1\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 232,
        "API": ".randn(",
        "context": [
            "\n",
            "        if noise:\n",
            "            if noise == 'batch_constant':\n",
            "                x = torch.addcmul(x, value=1.0, tensor1=self.noise_weight_2,\n",
            "                                  tensor2=torch.randn([1, 1, x.shape[2], x.shape[3]]))\n",
            "            else:\n",
            "                x = torch.addcmul(x, value=1.0, tensor1=self.noise_weight_2,\n",
            "                                  tensor2=torch.randn([x.shape[0], 1, x.shape[2], x.shape[3]]))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 237,
        "API": ".pow(",
        "context": [
            "            else:\n",
            "                x = torch.addcmul(x, value=1.0, tensor1=self.noise_weight_2,\n",
            "                                  tensor2=torch.randn([x.shape[0], 1, x.shape[2], x.shape[3]]))\n",
            "        else:\n",
            "            s = math.pow(self.layer + 1, 0.5)\n",
            "            x = x + s * torch.exp(-x * x / (2.0 * s * s)) / math.sqrt(2 * math.pi) * 0.8\n",
            "\n",
            "        x = x + self.bias_2\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 308,
        "API": ".zeros(",
        "context": [
            "            inputs = outputs\n",
            "            mul *= 2\n",
            "\n",
            "    def encode(self, x, lod):\n",
            "        styles = torch.zeros(x.shape[0], 1, self.latent_size)\n",
            "\n",
            "        x = self.from_rgb[self.layer_count - lod - 1](x)\n",
            "        x = F.leaky_relu(x, 0.2)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 321,
        "API": ".zeros(",
        "context": [
            "        return styles\n",
            "\n",
            "    def encode2(self, x, lod, blend):\n",
            "        x_orig = x\n",
            "        styles = torch.zeros(x.shape[0], 1, self.latent_size)\n",
            "\n",
            "        x = self.from_rgb[self.layer_count - lod - 1](x)\n",
            "        x = F.leaky_relu(x, 0.2)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 349,
        "API": ".std(",
        "context": [
            "        else:\n",
            "            return self.encode2(x, lod, blend)\n",
            "\n",
            "    def get_statistics(self, lod):\n",
            "        rgb_std = self.from_rgb[self.layer_count - lod - 1].from_rgb.weight.std().item()\n",
            "        rgb_std_c = self.from_rgb[self.layer_count - lod - 1].from_rgb.std\n",
            "\n",
            "        layers = []\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 354,
        "API": ".std(",
        "context": [
            "        rgb_std_c = self.from_rgb[self.layer_count - lod - 1].from_rgb.std\n",
            "\n",
            "        layers = []\n",
            "        for i in range(self.layer_count - lod - 1, self.layer_count):\n",
            "            conv_1 = self.encode_block[i].conv_1.weight.std().item()\n",
            "            conv_1_c = self.encode_block[i].conv_1.std\n",
            "            conv_2 = self.encode_block[i].conv_2.weight.std().item()\n",
            "            conv_2_c = self.encode_block[i].conv_2.std\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 398,
        "API": ".zeros(",
        "context": [
            "\n",
            "        self.fc2 = ln.Linear(inputs, 1, gain=1)\n",
            "\n",
            "    def encode(self, x, lod):\n",
            "        styles = torch.zeros(x.shape[0], 1, self.latent_size)\n",
            "\n",
            "        x = self.from_rgb[self.layer_count - lod - 1](x)\n",
            "        x = F.leaky_relu(x, 0.2)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 411,
        "API": ".zeros(",
        "context": [
            "        return styles, self.fc2(x)\n",
            "\n",
            "    def encode2(self, x, lod, blend):\n",
            "        x_orig = x\n",
            "        styles = torch.zeros(x.shape[0], 1, self.latent_size)\n",
            "\n",
            "        x = self.from_rgb[self.layer_count - lod - 1](x)\n",
            "        x = F.leaky_relu(x, 0.2)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 581,
        "API": ".view(",
        "context": [
            "\n",
            "        for i in range(self.layer_count - lod - 1, self.layer_count):\n",
            "            x = self.encode_block[i](x)\n",
            "\n",
            "        return self.fc2(x).view(x.shape[0], 1, x.shape[1])\n",
            "\n",
            "    def encode2(self, x, lod, blend):\n",
            "        x_orig = x\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 599,
        "API": ".view(",
        "context": [
            "\n",
            "        for i in range(self.layer_count - (lod - 1) - 1, self.layer_count):\n",
            "            x = self.encode_block[i](x)\n",
            "\n",
            "        return self.fc2(x).view(x.shape[0], 1, x.shape[1])\n",
            "\n",
            "    def forward(self, x, lod, blend):\n",
            "        if blend == 1:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 756,
        "API": ".std(",
        "context": [
            "        else:\n",
            "            return self.decode2(styles, lod, blend, noise)\n",
            "\n",
            "    def get_statistics(self, lod):\n",
            "        rgb_std = self.to_rgb[lod].to_rgb.weight.std().item()\n",
            "        rgb_std_c = self.to_rgb[lod].to_rgb.std\n",
            "\n",
            "        layers = []\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 764,
        "API": ".std(",
        "context": [
            "        for i in range(lod + 1):\n",
            "            conv_1 = 1.0\n",
            "            conv_1_c = 1.0\n",
            "            if i != 0:\n",
            "                conv_1 = self.decode_block[i].conv_1.weight.std().item()\n",
            "                conv_1_c = self.decode_block[i].conv_1.std\n",
            "            conv_2 = self.decode_block[i].conv_2.weight.std().item()\n",
            "            conv_2_c = self.decode_block[i].conv_2.std\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 776,
        "API": ".cat(",
        "context": [
            "def minibatch_stddev_layer(x, group_size=4):\n",
            "    group_size = min(group_size, x.shape[0])\n",
            "    size = x.shape[0]\n",
            "    if x.shape[0] % group_size != 0:\n",
            "        x = torch.cat([x, x[:(group_size - (x.shape[0] % group_size)) % group_size]])\n",
            "    y = x.view(group_size, -1, x.shape[1], x.shape[2], x.shape[3])\n",
            "    y = y - y.mean(dim=0, keepdim=True)\n",
            "    y = torch.sqrt((y ** 2).mean(dim=0) + 1e-8).mean(dim=[1, 2, 3], keepdim=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 781,
        "API": ".cat(",
        "context": [
            "    y = x.view(group_size, -1, x.shape[1], x.shape[2], x.shape[3])\n",
            "    y = y - y.mean(dim=0, keepdim=True)\n",
            "    y = torch.sqrt((y ** 2).mean(dim=0) + 1e-8).mean(dim=[1, 2, 3], keepdim=True)\n",
            "    y = y.repeat(group_size, 1, x.shape[2], x.shape[3])\n",
            "    return torch.cat([x, y], dim=1)[:size]\n",
            "\n",
            "\n",
            "image_size = 64\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 827,
        "API": ".main(",
        "context": [
            "            # state size. (nc) x 64 x 64\n",
            "        )\n",
            "\n",
            "    def forward(self, x):\n",
            "        return self.main(x.view(x.shape[0], nz, 1, 1))\n",
            "\n",
            "\n",
            "@ENCODERS.register(\"DCGANEncoder\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 856,
        "API": ".main(",
        "context": [
            "            nn.LeakyReLU(0.01),\n",
            "        )\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = self.main(x)\n",
            "        return x.view(x.shape[0], x.shape[1])\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 890,
        "API": ".repeat(",
        "context": [
            "\n",
            "        for i in range(self.mapping_layers):\n",
            "            x = getattr(self, \"block_%d\" % (i + 1))(x)\n",
            "\n",
            "        return x.view(x.shape[0], 1, x.shape[1]).repeat(1, self.num_layers, 1)\n",
            "\n",
            "\n",
            "# Used in default configuration. The D network\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 955,
        "API": ".repeat(",
        "context": [
            "\n",
            "        for i in range(self.mapping_layers):\n",
            "            x = self.map_blocks[i](x)\n",
            "\n",
            "        return x.view(x.shape[0], 1, x.shape[1]).repeat(1, self.num_layers, 1)\n",
            "\n",
            "\n",
            "@ENCODERS.register(\"EncoderFC\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 974,
        "API": ".view(",
        "context": [
            "        self.fc_3 = ln.Linear(1024, latent_size)\n",
            "\n",
            "    def encode(self, x, lod):\n",
            "        x = F.interpolate(x, 28)\n",
            "        x = x.view(x.shape[0], 28 * 28)\n",
            "\n",
            "        x = self.fc_1(x)\n",
            "        x = F.leaky_relu(x, 0.2)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 1008,
        "API": ".view(",
        "context": [
            "\n",
            "    def decode(self, x, lod, blend_factor, noise):\n",
            "        if len(x.shape) == 3:\n",
            "            x = x[:, 0]  # no styles\n",
            "        x.view(x.shape[0], self.latent_size)\n",
            "\n",
            "        x = self.fc_1(x)\n",
            "        x = F.leaky_relu(x, 0.2)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\net.py",
        "line_number": 1016,
        "API": ".view(",
        "context": [
            "        x = self.fc_2(x)\n",
            "        x = F.leaky_relu(x, 0.2)\n",
            "        x = self.fc_3(x)\n",
            "\n",
            "        x = x.view(x.shape[0], 1, 28, 28)\n",
            "        x = F.interpolate(x, 2 ** (2 + lod))\n",
            "        return x\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\scheduler.py",
        "line_number": 38,
        "API": ".format(",
        "context": [
            "\n",
            "        self.last_epoch = last_epoch\n",
            "\n",
            "        if not isinstance(optimizer, torch.optim.Optimizer):\n",
            "            raise TypeError('{} is not an Optimizer'.format(\n",
            "                type(optimizer).__name__))\n",
            "        self.optimizer = optimizer\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\scheduler.py",
        "line_number": 112,
        "API": ".all(",
        "context": [
            "        for k, x in self.schedulers.items():\n",
            "            x.load_state_dict(state_dict[k])\n",
            "\n",
            "        last_epochs = [x.last_epoch for k, x in self.schedulers.items()]\n",
            "        assert np.all(np.asarray(last_epochs) == last_epochs[0])\n",
            "        self.last_epoch = last_epochs[0]\n",
            "\n",
            "    def start_epoch(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\tracker.py",
        "line_number": 49,
        "API": ".unsqueeze(",
        "context": [
            "        self.values = []\n",
            "\n",
            "    def __iadd__(self, value):\n",
            "        with torch.no_grad():\n",
            "            self.values.append(value.detach().cpu().unsqueeze(0))\n",
            "            return self\n",
            "\n",
            "    def reset(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\tracker.py",
        "line_number": 59,
        "API": ".mean(",
        "context": [
            "    def mean(self):\n",
            "        with torch.no_grad():\n",
            "            if len(self.values) == 0:\n",
            "                return 0.0\n",
            "            return float(torch.cat(self.values).mean().item())\n",
            "\n",
            "\n",
            "class LossTracker:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\tracker.py",
        "line_number": 72,
        "API": ".add(",
        "context": [
            "\n",
            "    def update(self, d):\n",
            "        for k, v in d.items():\n",
            "            if k not in self.tracks:\n",
            "                self.add(k, isinstance(v, torch.Tensor))\n",
            "            self.tracks[k] += v\n",
            "\n",
            "    def add(self, name, pytorch=True):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\tracker.py",
        "line_number": 91,
        "API": ".mean(",
        "context": [
            "\n",
            "        for key in self.means_over_epochs.keys():\n",
            "            if key in self.tracks:\n",
            "                value = self.tracks[key]\n",
            "                self.means_over_epochs[key].append(value.mean())\n",
            "                value.reset()\n",
            "            else:\n",
            "                self.means_over_epochs[key].append(None)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\tracker.py",
        "line_number": 96,
        "API": ".join(",
        "context": [
            "                value.reset()\n",
            "            else:\n",
            "                self.means_over_epochs[key].append(None)\n",
            "\n",
            "        with open(os.path.join(self.output_folder, 'log.csv'), mode='w') as csv_file:\n",
            "            fieldnames = ['epoch'] + list(self.tracks.keys())\n",
            "            writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
            "            writer.writerow(fieldnames)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\tracker.py",
        "line_number": 106,
        "API": ".mean(",
        "context": [
            "\n",
            "    def __str__(self):\n",
            "        result = \"\"\n",
            "        for key, value in self.tracks.items():\n",
            "            result += \"%s: %.7f, \" % (key, value.mean())\n",
            "        return result[:-2]\n",
            "\n",
            "    def plot(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\tracker.py",
        "line_number": 126,
        "API": ".join(",
        "context": [
            "        ax.legend(loc=4)\n",
            "        ax.grid(True)\n",
            "        fig.tight_layout()\n",
            "\n",
            "        fig.savefig(os.path.join(self.output_folder, 'plot.png'))\n",
            "        fig.clf()\n",
            "        plt.close()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\train_alae.py",
        "line_number": 36,
        "API": ".makedirs(",
        "context": [
            "from PIL import Image\n",
            "\n",
            "\n",
            "def save_sample(lod2batch, tracker, sample, samplez, x, logger, model, cmodel, cfg, encoder_optimizer, decoder_optimizer):\n",
            "    os.makedirs('results', exist_ok=True)\n",
            "\n",
            "    logger.info('\\n[%d/%d] - ptime: %.2f, %s, blend: %.3f, lr: %.12f,  %.12f, max mem: %f\",' % (\n",
            "        (lod2batch.current_epoch + 1), cfg.TRAIN.TRAIN_EPOCHS, lod2batch.per_epoch_ptime, str(tracker),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\train_alae.py",
        "line_number": 42,
        "API": ".max_memory_allocated(",
        "context": [
            "    logger.info('\\n[%d/%d] - ptime: %.2f, %s, blend: %.3f, lr: %.12f,  %.12f, max mem: %f\",' % (\n",
            "        (lod2batch.current_epoch + 1), cfg.TRAIN.TRAIN_EPOCHS, lod2batch.per_epoch_ptime, str(tracker),\n",
            "        lod2batch.get_blend_factor(),\n",
            "        encoder_optimizer.param_groups[0]['lr'], decoder_optimizer.param_groups[0]['lr'],\n",
            "        torch.cuda.max_memory_allocated() / 1024.0 / 1024.0))\n",
            "\n",
            "    with torch.no_grad():\n",
            "        model.eval()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\train_alae.py",
        "line_number": 68,
        "API": ".repeat(",
        "context": [
            "\n",
            "        if cfg.MODEL.Z_REGRESSION:\n",
            "            Z = model.mapping_f(Z[:, 0])\n",
            "        else:\n",
            "            Z = Z.repeat(1, model.mapping_f.num_layers, 1)\n",
            "\n",
            "        rec1 = model.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n",
            "        rec2 = cmodel.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\train_alae.py",
        "line_number": 79,
        "API": ".cat(",
        "context": [
            "\n",
            "        Z = cmodel.mapping_f(samplez)\n",
            "        cg_rec = cmodel.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n",
            "\n",
            "        resultsample = torch.cat([sample_in, rec1, rec2, g_rec, cg_rec], dim=0)\n",
            "\n",
            "        @utils.async_func\n",
            "        def save_pic(x_rec):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\train_alae.py",
        "line_number": 88,
        "API": ".join(",
        "context": [
            "            tracker.plot()\n",
            "\n",
            "            result_sample = x_rec * 0.5 + 0.5\n",
            "            result_sample = result_sample.cpu()\n",
            "            f = os.path.join(cfg.OUTPUT_DIR,\n",
            "                             'sample_%d_%d.jpg' % (\n",
            "                                 lod2batch.current_epoch + 1,\n",
            "                                 lod2batch.iteration // 1000)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\train_alae.py",
        "line_number": 100,
        "API": ".set_device(",
        "context": [
            "        save_pic(resultsample)\n",
            "\n",
            "\n",
            "def train(cfg, logger, local_rank, world_size, distributed):\n",
            "    torch.cuda.set_device(local_rank)\n",
            "    model = Model(\n",
            "        startf=cfg.MODEL.START_CHANNEL_COUNT,\n",
            "        layer_count=cfg.MODEL.LAYER_COUNT,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\train_alae.py",
        "line_number": 156,
        "API": ".info(",
        "context": [
            "        mapping_d = model.mapping_d\n",
            "        mapping_f = model.mapping_f\n",
            "        dlatent_avg = model.dlatent_avg\n",
            "\n",
            "    count_param_override.print = lambda a: logger.info(a)\n",
            "\n",
            "    logger.info(\"Trainable parameters generator:\")\n",
            "    count_parameters(decoder)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\train_alae.py",
        "line_number": 161,
        "API": ".info(",
        "context": [
            "\n",
            "    logger.info(\"Trainable parameters generator:\")\n",
            "    count_parameters(decoder)\n",
            "\n",
            "    logger.info(\"Trainable parameters discriminator:\")\n",
            "    count_parameters(encoder)\n",
            "\n",
            "    arguments = dict()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\train_alae.py",
        "line_number": 212,
        "API": ".load(",
        "context": [
            "                                },\n",
            "                                logger=logger,\n",
            "                                save=local_rank == 0)\n",
            "\n",
            "    extra_checkpoint_data = checkpointer.load()\n",
            "    logger.info(\"Starting from epoch: %d\" % (scheduler.start_epoch()))\n",
            "\n",
            "    arguments.update(extra_checkpoint_data)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\train_alae.py",
        "line_number": 222,
        "API": ".randn(",
        "context": [
            "\n",
            "    dataset = TFRecordsDataset(cfg, logger, rank=local_rank, world_size=world_size, buffer_size_mb=1024, channels=cfg.MODEL.CHANNELS)\n",
            "\n",
            "    rnd = np.random.RandomState(3456)\n",
            "    latents = rnd.randn(32, cfg.MODEL.LATENT_SPACE_SIZE)\n",
            "    samplez = torch.tensor(latents).float().cuda()\n",
            "\n",
            "    lod2batch = lod_driver.LODDriver(cfg, logger, world_size, dataset_size=len(dataset) * world_size)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\train_alae.py",
        "line_number": 232,
        "API": ".join(",
        "context": [
            "        path = cfg.DATASET.SAMPLES_PATH\n",
            "        src = []\n",
            "        with torch.no_grad():\n",
            "            for filename in list(os.listdir(path))[:32]:\n",
            "                img = np.asarray(Image.open(os.path.join(path, filename)))\n",
            "                if img.shape[2] == 4:\n",
            "                    img = img[:, :, :3]\n",
            "                im = img.transpose((2, 0, 1))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\train_alae.py",
        "line_number": 240,
        "API": ".stack(",
        "context": [
            "                x = torch.tensor(np.asarray(im, dtype=np.float32), requires_grad=True).cuda() / 127.5 - 1.\n",
            "                if x.shape[0] == 4:\n",
            "                    x = x[:3]\n",
            "                src.append(x)\n",
            "            sample = torch.stack(src)\n",
            "    else:\n",
            "        dataset.reset(cfg.DATASET.MAX_RESOLUTION_LEVEL, 32)\n",
            "        sample = next(make_dataloader(cfg, logger, dataset, 32, local_rank))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\train_alae.py",
        "line_number": 252,
        "API": ".info(",
        "context": [
            "    for epoch in range(scheduler.start_epoch(), cfg.TRAIN.TRAIN_EPOCHS):\n",
            "        model.train()\n",
            "        lod2batch.set_epoch(epoch, [encoder_optimizer, decoder_optimizer])\n",
            "\n",
            "        logger.info(\"Batch size: %d, Batch size per GPU: %d, LOD: %d - %dx%d, blend: %.3f, dataset size: %d\" % (\n",
            "                                                                lod2batch.get_batch_size(),\n",
            "                                                                lod2batch.get_per_GPU_batch_size(),\n",
            "                                                                lod2batch.lod,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\train_alae.py",
        "line_number": 278,
        "API": ".permute(",
        "context": [
            "            with torch.no_grad():\n",
            "                if x_orig.shape[0] != lod2batch.get_per_GPU_batch_size():\n",
            "                    continue\n",
            "                if need_permute:\n",
            "                    x_orig = x_orig.permute(0, 3, 1, 2)\n",
            "                x_orig = (x_orig / 127.5 - 1.)\n",
            "\n",
            "                blend_factor = lod2batch.get_blend_factor()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\train_alae.py",
        "line_number": 297,
        "API": ".backward(",
        "context": [
            "\n",
            "            encoder_optimizer.zero_grad()\n",
            "            loss_d = model(x, lod2batch.lod, blend_factor, d_train=True, ae=False)\n",
            "            tracker.update(dict(loss_d=loss_d))\n",
            "            loss_d.backward()\n",
            "            encoder_optimizer.step()\n",
            "\n",
            "            decoder_optimizer.zero_grad()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\train_alae.py",
        "line_number": 303,
        "API": ".backward(",
        "context": [
            "\n",
            "            decoder_optimizer.zero_grad()\n",
            "            loss_g = model(x, lod2batch.lod, blend_factor, d_train=False, ae=False)\n",
            "            tracker.update(dict(loss_g=loss_g))\n",
            "            loss_g.backward()\n",
            "            decoder_optimizer.step()\n",
            "\n",
            "            encoder_optimizer.zero_grad()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\train_alae.py",
        "line_number": 310,
        "API": ".backward(",
        "context": [
            "            encoder_optimizer.zero_grad()\n",
            "            decoder_optimizer.zero_grad()\n",
            "            lae = model(x, lod2batch.lod, blend_factor, d_train=True, ae=True)\n",
            "            tracker.update(dict(lae=lae))\n",
            "            lae.backward()\n",
            "            encoder_optimizer.step()\n",
            "            decoder_optimizer.step()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\train_alae.py",
        "line_number": 325,
        "API": ".save(",
        "context": [
            "            lod_for_saving_model = lod2batch.lod\n",
            "            lod2batch.step()\n",
            "            if local_rank == 0:\n",
            "                if lod2batch.is_time_to_save():\n",
            "                    checkpointer.save(\"model_tmp_intermediate_lod%d\" % lod_for_saving_model)\n",
            "                if lod2batch.is_time_to_report():\n",
            "                    save_sample(lod2batch, tracker, sample, samplez, x, logger, model_s,\n",
            "                                model.module if hasattr(model, \"module\") else model, cfg, encoder_optimizer,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\train_alae.py",
        "line_number": 334,
        "API": ".save(",
        "context": [
            "\n",
            "        scheduler.step()\n",
            "\n",
            "        if local_rank == 0:\n",
            "            checkpointer.save(\"model_tmp_lod%d\" % lod_for_saving_model)\n",
            "            save_sample(lod2batch, tracker, sample, samplez, x, logger, model_s,\n",
            "                        model.module if hasattr(model, \"module\") else model, cfg, encoder_optimizer, decoder_optimizer)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\train_alae.py",
        "line_number": 340,
        "API": ".save(",
        "context": [
            "                        model.module if hasattr(model, \"module\") else model, cfg, encoder_optimizer, decoder_optimizer)\n",
            "\n",
            "    logger.info(\"Training finish!... save training results\")\n",
            "    if local_rank == 0:\n",
            "        checkpointer.save(\"model_final\").wait()\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\utils.py",
        "line_number": 31,
        "API": ".join(",
        "context": [
            "\n",
            "    def __call__(self, *args, **kwargs):\n",
            "        m = hashlib.sha256()\n",
            "        m.update(pickle.dumps((self.function.__name__, args, frozenset(kwargs.items()))))\n",
            "        output_path = os.path.join('.cache', \"%s_%s\" % (m.hexdigest(), self.pickle_name))\n",
            "        try:\n",
            "            with open(output_path, 'rb') as f:\n",
            "                data = pickle.load(f)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\utils.py",
        "line_number": 36,
        "API": ".function(",
        "context": [
            "        try:\n",
            "            with open(output_path, 'rb') as f:\n",
            "                data = pickle.load(f)\n",
            "        except (FileNotFoundError, pickle.PickleError):\n",
            "            data = self.function(*args, **kwargs)\n",
            "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
            "            with open(output_path, 'wb') as f:\n",
            "                pickle.dump(data, f)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\utils.py",
        "line_number": 45,
        "API": ".save(",
        "context": [
            "\n",
            "\n",
            "def save_model(x, name):\n",
            "    if isinstance(x, nn.DataParallel):\n",
            "        torch.save(x.module.state_dict(), name)\n",
            "    else:\n",
            "        torch.save(x.state_dict(), name)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\utils.py",
        "line_number": 62,
        "API": ".join(",
        "context": [
            "        self.Thread.start()\n",
            "        return self\n",
            "\n",
            "    def wait(self, timeout=None):\n",
            "        self.Thread.join(timeout)\n",
            "        if self.Thread.isAlive():\n",
            "            raise TimeoutError\n",
            "        else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_celeba_hq_tfrecords.py",
        "line_number": 27,
        "API": ".makedirs(",
        "context": [
            "        directory = os.path.dirname(cfg.DATASET.PATH)\n",
            "    else:\n",
            "        directory = os.path.dirname(cfg.DATASET.PATH_TEST)\n",
            "\n",
            "    os.makedirs(directory, exist_ok=True)\n",
            "\n",
            "    images = []\n",
            "    # The official way of generating CelebA-HQ can be challenging.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_celeba_hq_tfrecords.py",
        "line_number": 46,
        "API": ".seed(",
        "context": [
            "\n",
            "    count = len(images)\n",
            "    print(\"Count: %d\" % count)\n",
            "\n",
            "    random.seed(0)\n",
            "    random.shuffle(images)\n",
            "\n",
            "    folds = cfg.DATASET.PART_COUNT\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_celeba_hq_tfrecords.py",
        "line_number": 66,
        "API": ".makedirs(",
        "context": [
            "        writers = {}\n",
            "        for lod in range(cfg.DATASET.MAX_RESOLUTION_LEVEL, 1, -1):\n",
            "            tfr_opt = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.NONE)\n",
            "            part_path = path % (lod, i)\n",
            "            os.makedirs(os.path.dirname(part_path), exist_ok=True)\n",
            "            tfr_writer = tf.python_io.TFRecordWriter(part_path, tfr_opt)\n",
            "            writers[lod] = tfr_writer\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_celeba_hq_tfrecords.py",
        "line_number": 71,
        "API": ".join(",
        "context": [
            "            tfr_writer = tf.python_io.TFRecordWriter(part_path, tfr_opt)\n",
            "            writers[lod] = tfr_writer\n",
            "\n",
            "        for label, filename in tqdm.tqdm(celeba_folds[i]):\n",
            "            img = np.asarray(Image.open(os.path.join(source_path, filename)))\n",
            "            img = img.transpose((2, 0, 1))\n",
            "            for lod in range(cfg.DATASET.MAX_RESOLUTION_LEVEL, 1, -1):\n",
            "                ex = tf.train.Example(features=tf.train.Features(feature={\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_celeba_hq_tfrecords.py",
        "line_number": 78,
        "API": ".write(",
        "context": [
            "                ex = tf.train.Example(features=tf.train.Features(feature={\n",
            "                    'shape': tf.train.Feature(int64_list=tf.train.Int64List(value=img.shape)),\n",
            "                    'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n",
            "                    'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.tostring()]))}))\n",
            "                writers[lod].write(ex.SerializeToString())\n",
            "\n",
            "                image = torch.tensor(np.asarray(img, dtype=np.float32)).view(1, 3, img.shape[1], img.shape[2])\n",
            "                image_down = F.avg_pool2d(image, 2, 2).clamp_(0, 255).to('cpu', torch.uint8).view(3, image.shape[2] // 2, image.shape[3] // 2).numpy()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_celeba_hq_tfrecords.py",
        "line_number": 112,
        "API": ".makedirs(",
        "context": [
            "    logger = logging.getLogger(\"logger\")\n",
            "    logger.setLevel(logging.DEBUG)\n",
            "\n",
            "    output_dir = cfg.OUTPUT_DIR\n",
            "    os.makedirs(output_dir, exist_ok=True)\n",
            "\n",
            "    ch = logging.StreamHandler(stream=sys.stdout)\n",
            "    ch.setLevel(logging.DEBUG)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_celeba_hq_tfrecords.py",
        "line_number": 119,
        "API": ".info(",
        "context": [
            "    ch.setLevel(logging.DEBUG)\n",
            "    formatter = logging.Formatter(\"%(asctime)s %(name)s %(levelname)s: %(message)s\")\n",
            "    ch.setFormatter(formatter)\n",
            "    logger.addHandler(ch)\n",
            "    logger.info(args)\n",
            "\n",
            "    logger.info(\"Loaded configuration file {}\".format(args.config_file))\n",
            "    with open(args.config_file, \"r\") as cf:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_celeba_hq_tfrecords.py",
        "line_number": 124,
        "API": ".info(",
        "context": [
            "\n",
            "    logger.info(\"Loaded configuration file {}\".format(args.config_file))\n",
            "    with open(args.config_file, \"r\") as cf:\n",
            "        config_str = \"\\n\" + cf.read()\n",
            "        logger.info(config_str)\n",
            "    logger.info(\"Running with config:\\n{}\".format(cfg))\n",
            "\n",
            "    prepare_celeba(cfg, logger, True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_celeba_tfrecords.py",
        "line_number": 40,
        "API": ".split(",
        "context": [
            "        directory = os.path.dirname(cfg.DATASET.PATH_TEST)\n",
            "\n",
            "    with open(\"/data/datasets/CelebA/Eval/list_eval_partition.txt\") as f:\n",
            "        lineList = f.readlines()\n",
            "    lineList = [x[:-1].split(' ') for x in lineList]\n",
            "\n",
            "    split_map = {}\n",
            "    for x in lineList:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_celeba_tfrecords.py",
        "line_number": 46,
        "API": ".makedirs(",
        "context": [
            "    split_map = {}\n",
            "    for x in lineList:\n",
            "        split_map[int(x[0][:-4])] = int(x[1])\n",
            "\n",
            "    os.makedirs(directory, exist_ok=True)\n",
            "\n",
            "    corrupted = [\n",
            "        '195995.jpg',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_celeba_tfrecords.py",
        "line_number": 74,
        "API": ".array(",
        "context": [
            "            crop_w = crop_h # the width and height after cropped\n",
            "        h, w = x.shape[:2]\n",
            "        j = int(round((h - crop_h)/2.)) + 15\n",
            "        i = int(round((w - crop_w)/2.))\n",
            "        return np.array(Image.fromarray(x[j:j+crop_h, i:i+crop_w]).resize([resize_w, resize_w]))\n",
            "\n",
            "    archive = zipfile.ZipFile(os.path.join(directory, '/data/datasets/CelebA/Img/img_align_celeba.zip'), 'r')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_celeba_tfrecords.py",
        "line_number": 92,
        "API": ".seed(",
        "context": [
            "    print(\"Count: %d\" % count)\n",
            "\n",
            "    names = [x for x in names if x[-10:] not in corrupted]\n",
            "\n",
            "    random.seed(0)\n",
            "    random.shuffle(names)\n",
            "\n",
            "    folds = cfg.DATASET.PART_COUNT\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_celeba_tfrecords.py",
        "line_number": 108,
        "API": ".split(",
        "context": [
            "        # 000002.jpg 2937\n",
            "        with open(\"/data/datasets/CelebA/Anno/identity_CelebA.txt\") as f:\n",
            "            lineList = f.readlines()\n",
            "\n",
            "        lineList = [x[:-1].split(' ') for x in lineList]\n",
            "\n",
            "        identity_map = {}\n",
            "        for x in lineList:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_celeba_tfrecords.py",
        "line_number": 114,
        "API": ".split(",
        "context": [
            "        identity_map = {}\n",
            "        for x in lineList:\n",
            "            identity_map[x[0]] = int(x[1])\n",
            "\n",
            "        names = [(identity_map[x.split('/')[1]], x) for x in names]\n",
            "\n",
            "        class_bins = {}\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_celeba_tfrecords.py",
        "line_number": 161,
        "API": ".transpose(",
        "context": [
            "        images = []\n",
            "        for x in tqdm.tqdm(celeba_folds[i]):\n",
            "            imgfile = archive.open(x)\n",
            "            image = center_crop(imageio.imread(imgfile.read()))\n",
            "            images.append((int(x[:-4][-6:]), image.transpose((2, 0, 1))))\n",
            "\n",
            "        tfr_opt = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.NONE)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_celeba_tfrecords.py",
        "line_number": 179,
        "API": ".write(",
        "context": [
            "            ex = tf.train.Example(features=tf.train.Features(feature={\n",
            "                'shape': tf.train.Feature(int64_list=tf.train.Int64List(value=image.shape)),\n",
            "                'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n",
            "                'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image.tostring()]))}))\n",
            "            tfr_writer.write(ex.SerializeToString())\n",
            "        tfr_writer.close()\n",
            "\n",
            "        for j in range(5):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_celeba_tfrecords.py",
        "line_number": 188,
        "API": ".view(",
        "context": [
            "\n",
            "            for label, image in tqdm.tqdm(images):\n",
            "                h = image.shape[1]\n",
            "                w = image.shape[2]\n",
            "                image = torch.tensor(np.asarray(image, dtype=np.float32)).view(1, 3, h, w)\n",
            "\n",
            "                image_down = F.avg_pool2d(image, 2, 2).clamp_(0, 255).to('cpu', torch.uint8)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_celeba_tfrecords.py",
        "line_number": 206,
        "API": ".write(",
        "context": [
            "                ex = tf.train.Example(features=tf.train.Features(feature={\n",
            "                    'shape': tf.train.Feature(int64_list=tf.train.Int64List(value=image.shape)),\n",
            "                    'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n",
            "                    'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image.tostring()]))}))\n",
            "                tfr_writer.write(ex.SerializeToString())\n",
            "            tfr_writer.close()\n",
            "\n",
            "            images = images_down\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_imagenet.py",
        "line_number": 42,
        "API": ".makedirs(",
        "context": [
            "    writers = {}\n",
            "    for lod in range(8, 1, -1):\n",
            "        tfr_opt = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.NONE)\n",
            "        part_path = path % (lod, i)\n",
            "        os.makedirs(os.path.dirname(part_path), exist_ok=True)\n",
            "        tfr_writer = tf.python_io.TFRecordWriter(part_path, tfr_opt)\n",
            "        writers[lod] = tfr_writer\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_imagenet.py",
        "line_number": 47,
        "API": ".join(",
        "context": [
            "        tfr_writer = tf.python_io.TFRecordWriter(part_path, tfr_opt)\n",
            "        writers[lod] = tfr_writer\n",
            "\n",
            "    for s, image in image_folds[i]:\n",
            "        im = os.path.join(train_root, s, image)\n",
            "        img = Image.open(im)\n",
            "        if fixed:\n",
            "            img = F.resize(img, 288)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_imagenet.py",
        "line_number": 57,
        "API": ".tile(",
        "context": [
            "            img = F.resize(img, 288)\n",
            "            img = F.center_crop(img, 288)\n",
            "        img = np.asarray(img)\n",
            "        if len(img.shape) == 2:\n",
            "            img = np.tile(img[:, :, None], (1, 1, 3))\n",
            "        img = img.transpose((2, 0, 1))\n",
            "        if img.shape[0] > 3:\n",
            "            img = img[:3]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_imagenet.py",
        "line_number": 67,
        "API": ".write(",
        "context": [
            "            ex = tf.train.Example(features=tf.train.Features(feature={\n",
            "                'shape': tf.train.Feature(int64_list=tf.train.Int64List(value=img.shape)),\n",
            "                'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[wnid_to_indx[s]])),\n",
            "                'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.tostring()]))}))\n",
            "            writers[lod].write(ex.SerializeToString())\n",
            "\n",
            "            image = torch.tensor(np.asarray(img, dtype=np.float32)).view(1, 3, img.shape[1], img.shape[2])\n",
            "            image_down = avg_pool2d(image, 2, 2).clamp_(0, 255).to('cpu', torch.uint8).view(3, image.shape[2] // 2,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_imagenet.py",
        "line_number": 80,
        "API": ".join(",
        "context": [
            "        writers[lod].close()\n",
            "\n",
            "\n",
            "def parse_meta_mat(devkit_root):\n",
            "    metafile = os.path.join(devkit_root, \"data\", \"meta.mat\")\n",
            "    meta = sio.loadmat(metafile, squeeze_me=True)['synsets']\n",
            "    nums_children = list(zip(*meta))[4]\n",
            "    meta = [meta[idx] for idx, num_children in enumerate(nums_children)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_imagenet.py",
        "line_number": 86,
        "API": ".split(",
        "context": [
            "    nums_children = list(zip(*meta))[4]\n",
            "    meta = [meta[idx] for idx, num_children in enumerate(nums_children)\n",
            "            if num_children == 0]\n",
            "    idcs, wnids, classes = list(zip(*meta))[:3]\n",
            "    classes = [tuple(clss.split(', ')) for clss in classes]\n",
            "    idx_to_wnid = {idx: wnid for idx, wnid in zip(idcs, wnids)}\n",
            "    wnid_to_classes = {wnid: clss for wnid, clss in zip(wnids, classes)}\n",
            "    return idx_to_wnid, wnid_to_classes\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_imagenet.py",
        "line_number": 93,
        "API": ".join(",
        "context": [
            "    return idx_to_wnid, wnid_to_classes\n",
            "\n",
            "\n",
            "def parse_val_groundtruth_txt(devkit_root):\n",
            "    file = os.path.join(devkit_root, \"data\",\n",
            "                        \"ILSVRC2012_validation_ground_truth.txt\")\n",
            "    with open(file, 'r') as txtfh:\n",
            "        val_idcs = txtfh.readlines()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_imagenet.py",
        "line_number": 105,
        "API": ".join(",
        "context": [
            "def get_names(train_root):\n",
            "    names = []\n",
            "    sets = os.listdir(train_root)\n",
            "    for s in sets:\n",
            "        images = os.listdir(os.path.join(train_root, s))\n",
            "        names += [(s, im) for im in images]\n",
            "    return names\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_imagenet.py",
        "line_number": 123,
        "API": ".join(",
        "context": [
            "        print(\"%d - %s\" % (i, c))\n",
            "\n",
            "    wnid_to_indx = dict([(v, k - 1) for k, v in idx_to_wnid.items()])\n",
            "\n",
            "    torch.save((wnid_to_classes, val_wnids), os.path.join(\"\", \"meta\"))\n",
            "\n",
            "    train_root = \"/data/datasets/ImageNet_bak/raw-data/train\"\n",
            "    validation_root = \"/data/datasets/ImageNet_bak/raw-data/validation\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_imagenet.py",
        "line_number": 129,
        "API": ".info(",
        "context": [
            "    train_root = \"/data/datasets/ImageNet_bak/raw-data/train\"\n",
            "    validation_root = \"/data/datasets/ImageNet_bak/raw-data/validation\"\n",
            "\n",
            "    ###\n",
            "    logger.info(\"Savingexamples\")\n",
            "\n",
            "    path = 'dataset_samples/imagenet256x256'\n",
            "    os.makedirs(path, exist_ok=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_imagenet.py",
        "line_number": 137,
        "API": ".join(",
        "context": [
            "    k = 0\n",
            "    names = get_names(train_root)\n",
            "    random.shuffle(names)\n",
            "    for s, image in names:\n",
            "        im = os.path.join(train_root, s, image)\n",
            "        img = Image.open(im)\n",
            "        img = F.resize(img, 288)\n",
            "        img = F.center_crop(img, 256)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_imagenet.py",
        "line_number": 143,
        "API": ".tile(",
        "context": [
            "        img = F.resize(img, 288)\n",
            "        img = F.center_crop(img, 256)\n",
            "        img = np.asarray(img)\n",
            "        if len(img.shape) == 2:\n",
            "            img = np.tile(img[:, :, None], (1, 1, 3))\n",
            "        img = img.transpose((2, 0, 1))\n",
            "        if img.shape[0] > 3:\n",
            "            img = img[:3]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_imagenet.py",
        "line_number": 149,
        "API": ".save(",
        "context": [
            "        if img.shape[0] > 3:\n",
            "            img = img[:3]\n",
            "        img = img.transpose((1, 2, 0))\n",
            "        img = Image.fromarray(img)\n",
            "        img.save(path + '/' + str(k) + \".png\")\n",
            "        k += 1\n",
            "        if k == 2000:\n",
            "            break\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_imagenet.py",
        "line_number": 157,
        "API": ".seed(",
        "context": [
            "    ###\n",
            "    exit()\n",
            "\n",
            "    if True:\n",
            "        random.seed(0)\n",
            "\n",
            "        names = get_names(train_root)\n",
            "        random.shuffle(names)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_imagenet.py",
        "line_number": 176,
        "API": ".join(",
        "context": [
            "            thread.start()\n",
            "            threads.append(thread)\n",
            "\n",
            "        for i in range(folds):\n",
            "            threads[i].join()\n",
            "    if False:\n",
            "        random.seed(0)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_imagenet.py",
        "line_number": 197,
        "API": ".join(",
        "context": [
            "            thread.start()\n",
            "            threads.append(thread)\n",
            "\n",
            "        for i in range(folds):\n",
            "            threads[i].join()\n",
            "\n",
            "    print(idx_to_wnid, wnid_to_classes)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_imagenet.py",
        "line_number": 240,
        "API": ".info(",
        "context": [
            "\n",
            "    logger.info(\"Loaded configuration file {}\".format(args.config_file))\n",
            "    with open(args.config_file, \"r\") as cf:\n",
            "        config_str = \"\\n\" + cf.read()\n",
            "        logger.info(config_str)\n",
            "    logger.info(\"Running with config:\\n{}\".format(cfg))\n",
            "\n",
            "    prepare_imagenet(cfg, logger)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_mnist_tfrecords.py",
        "line_number": 39,
        "API": ".pad(",
        "context": [
            "    else:\n",
            "        mnist_images = mnist_images[50000:]\n",
            "        mnist_labels = mnist_labels[50000:]\n",
            "\n",
            "    mnist_images = F.pad(torch.tensor(mnist_images).view(mnist_images.shape[0], 1, 28, 28), (2, 2, 2, 2)).detach().cpu().numpy()\n",
            "    # mnist_images = torch.tensor(mnist_images).view(mnist_images.shape[0], 1, 28, 28).detach().cpu().numpy()\n",
            "\n",
            "    if train:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_mnist_tfrecords.py",
        "line_number": 49,
        "API": ".makedirs(",
        "context": [
            "        path = cfg.DATASET.PATH_TEST\n",
            "\n",
            "    directory = os.path.dirname(path)\n",
            "\n",
            "    os.makedirs(directory, exist_ok=True)\n",
            "\n",
            "    folds = cfg.DATASET.PART_COUNT\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_mnist_tfrecords.py",
        "line_number": 77,
        "API": ".write(",
        "context": [
            "            ex = tf.train.Example(features=tf.train.Features(feature={\n",
            "                'shape': tf.train.Feature(int64_list=tf.train.Int64List(value=image.shape)),\n",
            "                'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n",
            "                'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image.tostring()]))}))\n",
            "            tfr_writer.write(ex.SerializeToString())\n",
            "        tfr_writer.close()\n",
            "\n",
            "        if train:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_mnist_tfrecords.py",
        "line_number": 87,
        "API": ".view(",
        "context": [
            "\n",
            "                for image, label in zip(images, labels):\n",
            "                    h = image.shape[1]\n",
            "                    w = image.shape[2]\n",
            "                    image = torch.tensor(np.asarray(image, dtype=np.float32)).view(1, 1, h, w)\n",
            "\n",
            "                    image_down = F.avg_pool2d(image, 2, 2).clamp_(0, 255).to('cpu', torch.uint8)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_mnist_tfrecords.py",
        "line_number": 101,
        "API": ".write(",
        "context": [
            "                    ex = tf.train.Example(features=tf.train.Features(feature={\n",
            "                        'shape': tf.train.Feature(int64_list=tf.train.Int64List(value=image.shape)),\n",
            "                        'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n",
            "                        'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image.tostring()]))}))\n",
            "                    tfr_writer.write(ex.SerializeToString())\n",
            "                tfr_writer.close()\n",
            "\n",
            "                images = images_down\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_mnist_tfrecords.py",
        "line_number": 145,
        "API": ".info(",
        "context": [
            "\n",
            "    logger.info(\"Loaded configuration file {}\".format(args.config_file))\n",
            "    with open(args.config_file, \"r\") as cf:\n",
            "        config_str = \"\\n\" + cf.read()\n",
            "        logger.info(config_str)\n",
            "    logger.info(\"Running with config:\\n{}\".format(cfg))\n",
            "\n",
            "    random.seed(0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_mnist_tfrecords.py",
        "line_number": 154,
        "API": ".stack(",
        "context": [
            "    dlutils.download.mnist()\n",
            "    mnist = dlutils.reader.Mnist('mnist', train=True, test=False).items\n",
            "    random.shuffle(mnist)\n",
            "\n",
            "    mnist_images = np.stack([x[1] for x in mnist])\n",
            "    mnist_labels = np.stack([x[0] for x in mnist])\n",
            "\n",
            "    prepare_mnist(cfg, logger, mnist_images, mnist_labels, train=False)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_svhn_tfrecords.py",
        "line_number": 32,
        "API": ".view(",
        "context": [
            "\n",
            "def prepare_mnist(cfg, logger, mnist_images, mnist_labels, train):\n",
            "    im_size = 32\n",
            "\n",
            "    mnist_images = torch.tensor(mnist_images).view(mnist_images.shape[0], 3, 32, 32).detach().cpu().numpy()\n",
            "    # mnist_images = torch.tensor(mnist_images).view(mnist_images.shape[0], 1, 28, 28).detach().cpu().numpy()\n",
            "\n",
            "    if train:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_svhn_tfrecords.py",
        "line_number": 70,
        "API": ".write(",
        "context": [
            "            ex = tf.train.Example(features=tf.train.Features(feature={\n",
            "                'shape': tf.train.Feature(int64_list=tf.train.Int64List(value=image.shape)),\n",
            "                'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n",
            "                'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image.tostring()]))}))\n",
            "            tfr_writer.write(ex.SerializeToString())\n",
            "        tfr_writer.close()\n",
            "\n",
            "        if True:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_svhn_tfrecords.py",
        "line_number": 80,
        "API": ".view(",
        "context": [
            "\n",
            "                for image, label in zip(images, labels):\n",
            "                    h = image.shape[1]\n",
            "                    w = image.shape[2]\n",
            "                    image = torch.tensor(np.asarray(image, dtype=np.float32)).view(1, 3, h, w)\n",
            "\n",
            "                    image_down = F.avg_pool2d(image, 2, 2).clamp_(0, 255).to('cpu', torch.uint8)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_svhn_tfrecords.py",
        "line_number": 143,
        "API": ".makedirs(",
        "context": [
            "    logger.info(\"Running with config:\\n{}\".format(cfg))\n",
            "\n",
            "    random.seed(0)\n",
            "\n",
            "    os.makedirs(\"SVHN\", exist_ok=True)\n",
            "    train = list(SVHN('.', split='train', download=True))\n",
            "    test = list(SVHN('.', split='test', download=True))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_svhn_tfrecords.py",
        "line_number": 149,
        "API": ".stack(",
        "context": [
            "    test = list(SVHN('.', split='test', download=True))\n",
            "\n",
            "    random.shuffle(train)\n",
            "\n",
            "    svhn_images = np.stack([np.transpose(x[0], (2, 0, 1)) for x in train])\n",
            "    svhn_labels = np.stack([x[1] for x in train])\n",
            "\n",
            "    prepare_mnist(cfg, logger, svhn_images, svhn_labels, train=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\prepare_svhn_tfrecords.py",
        "line_number": 154,
        "API": ".stack(",
        "context": [
            "    svhn_labels = np.stack([x[1] for x in train])\n",
            "\n",
            "    prepare_mnist(cfg, logger, svhn_images, svhn_labels, train=True)\n",
            "\n",
            "    svhn_images = np.stack([np.transpose(x[0], (2, 0, 1)) for x in test])\n",
            "    svhn_labels = np.stack([x[1] for x in test])\n",
            "\n",
            "    prepare_mnist(cfg, logger, svhn_images, svhn_labels, train=False)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\split_tfrecords_bedroom.py",
        "line_number": 33,
        "API": ".info(",
        "context": [
            "    ffhq_size = cfg.DATASET.SIZE\n",
            "\n",
            "    part_size = ffhq_size // cfg.DATASET.PART_COUNT\n",
            "\n",
            "    logger.info(\"Splitting into % size parts\" % part_size)\n",
            "\n",
            "    chunk_size = 1024\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\split_tfrecords_bedroom.py",
        "line_number": 40,
        "API": ".batch(",
        "context": [
            "\n",
            "    # # Commented code is for saving out samples of bedroom dataset\n",
            "    # with tf.Graph().as_default(), tf.Session() as sess:\n",
            "    #     ds = tf.data.TFRecordDataset(tfrecord_path % 8)\n",
            "    #     batch = ds.batch(256).make_one_shot_iterator().get_next()\n",
            "    #\n",
            "    #     features = {\n",
            "    #         # 'shape': db.FixedLenFeature([3], db.int64),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\split_tfrecords_bedroom.py",
        "line_number": 49,
        "API": ".makedirs(",
        "context": [
            "    #     }\n",
            "    #     parser = db.RecordParser(features, False)\n",
            "    #     try:\n",
            "    #         path = 'dataset_samples/bedroom256x256'\n",
            "    #         os.makedirs(path, exist_ok=True)\n",
            "    #         records = sess.run(batch)\n",
            "    #         k = 0\n",
            "    #         for record in records:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\split_tfrecords_bedroom.py",
        "line_number": 54,
        "API": ".transpose(",
        "context": [
            "    #         records = sess.run(batch)\n",
            "    #         k = 0\n",
            "    #         for record in records:\n",
            "    #             im = parser.parse_single_example(record)[0]\n",
            "    #             im = im.transpose((1, 2, 0))\n",
            "    #             image = Image.fromarray(im)\n",
            "    #             image.save(path + '/' + str(k) + \".png\")\n",
            "    #             k += 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\split_tfrecords_bedroom.py",
        "line_number": 66,
        "API": ".batch(",
        "context": [
            "    for i in range(0, cfg.DATASET.MAX_RESOLUTION_LEVEL + 1):\n",
            "        part_num = 0\n",
            "        with tf.Graph().as_default(), tf.Session() as sess:\n",
            "            ds = tf.data.TFRecordDataset(tfrecord_path % i)\n",
            "            batch = ds.batch(chunk_size).make_one_shot_iterator().get_next()\n",
            "            while True:\n",
            "                try:\n",
            "                    part_path = cfg.DATASET.PATH % (i, part_num)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\split_tfrecords_bedroom.py",
        "line_number": 76,
        "API": ".write(",
        "context": [
            "                    with tf.python_io.TFRecordWriter(part_path) as writer:\n",
            "                        for k in tqdm.tqdm(range(part_size // chunk_size)):\n",
            "                            records = sess.run(batch)\n",
            "                            for record in records:\n",
            "                                writer.write(record)\n",
            "                    part_num += 1\n",
            "                except tf.errors.OutOfRangeError:\n",
            "                    break\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\split_tfrecords_bedroom.py",
        "line_number": 116,
        "API": ".join(",
        "context": [
            "    formatter = logging.Formatter(\"%(asctime)s %(name)s %(levelname)s: %(message)s\")\n",
            "    ch.setFormatter(formatter)\n",
            "    logger.addHandler(ch)\n",
            "\n",
            "    fh = logging.FileHandler(os.path.join(output_dir, 'log.txt'))\n",
            "    fh.setLevel(logging.DEBUG)\n",
            "    fh.setFormatter(formatter)\n",
            "    logger.addHandler(fh)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\split_tfrecords_bedroom.py",
        "line_number": 121,
        "API": ".info(",
        "context": [
            "    fh.setLevel(logging.DEBUG)\n",
            "    fh.setFormatter(formatter)\n",
            "    logger.addHandler(fh)\n",
            "\n",
            "    logger.info(args)\n",
            "\n",
            "    logger.info(\"Loaded configuration file {}\".format(args.config_file))\n",
            "    with open(args.config_file, \"r\") as cf:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\split_tfrecords_bedroom.py",
        "line_number": 126,
        "API": ".info(",
        "context": [
            "\n",
            "    logger.info(\"Loaded configuration file {}\".format(args.config_file))\n",
            "    with open(args.config_file, \"r\") as cf:\n",
            "        config_str = \"\\n\" + cf.read()\n",
            "        logger.info(config_str)\n",
            "    logger.info(\"Running with config:\\n{}\".format(cfg))\n",
            "\n",
            "    split_tfrecord(cfg, logger)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\split_tfrecords_ffhq.py",
        "line_number": 30,
        "API": ".info(",
        "context": [
            "    ffhq_train_size = 60000\n",
            "\n",
            "    part_size = ffhq_train_size // cfg.DATASET.PART_COUNT\n",
            "\n",
            "    logger.info(\"Splitting into % size parts\" % part_size)\n",
            "\n",
            "    for i in range(2, cfg.DATASET.MAX_RESOLUTION_LEVEL + 1):\n",
            "        with tf.Graph().as_default(), tf.Session() as sess:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\split_tfrecords_ffhq.py",
        "line_number": 35,
        "API": ".batch(",
        "context": [
            "\n",
            "    for i in range(2, cfg.DATASET.MAX_RESOLUTION_LEVEL + 1):\n",
            "        with tf.Graph().as_default(), tf.Session() as sess:\n",
            "            ds = tf.data.TFRecordDataset(tfrecord_path % i)\n",
            "            ds = ds.batch(part_size)\n",
            "            batch = ds.make_one_shot_iterator().get_next()\n",
            "            part_num = 0\n",
            "            while True:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\split_tfrecords_ffhq.py",
        "line_number": 43,
        "API": ".makedirs(",
        "context": [
            "                try:\n",
            "                    records = sess.run(batch)\n",
            "                    if part_num < cfg.DATASET.PART_COUNT:\n",
            "                        part_path = cfg.DATASET.PATH % (i, part_num)\n",
            "                        os.makedirs(os.path.dirname(part_path), exist_ok=True)\n",
            "                        with tf.python_io.TFRecordWriter(part_path) as writer:\n",
            "                            for record in records:\n",
            "                                writer.write(record)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\dataset_preparation\\split_tfrecords_ffhq.py",
        "line_number": 49,
        "API": ".makedirs(",
        "context": [
            "                            for record in records:\n",
            "                                writer.write(record)\n",
            "                    else:\n",
            "                        part_path = cfg.DATASET.PATH_TEST % (i, part_num - cfg.DATASET.PART_COUNT)\n",
            "                        os.makedirs(os.path.dirname(part_path), exist_ok=True)\n",
            "                        with tf.python_io.TFRecordWriter(part_path) as writer:\n",
            "                            for record in records:\n",
            "                                writer.write(record)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_generation_figure.py",
        "line_number": 27,
        "API": ".randn(",
        "context": [
            "    images = []\n",
            "\n",
            "    rnd = np.random.RandomState(5)\n",
            "    for i in range(N):\n",
            "        latents = rnd.randn(1, cfg.MODEL.LATENT_SPACE_SIZE)\n",
            "        samplez = torch.tensor(latents).float().cuda()\n",
            "        image = model.generate(cfg.DATASET.MAX_RESOLUTION_LEVEL-2, 1, samplez, 1, mixing=True)\n",
            "        images.append(image[0])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_generation_figure.py",
        "line_number": 37,
        "API": ".transpose(",
        "context": [
            "    image_iter = iter(list(images))\n",
            "    for col, lod in enumerate(lods):\n",
            "        for row in range(rows * 2**lod):\n",
            "            im = next(image_iter).cpu().numpy()\n",
            "            im = im.transpose(1, 2, 0)\n",
            "            im = im * 0.5 + 0.5\n",
            "            image = PIL.Image.fromarray(np.clip(im * 255, 0, 255).astype(np.uint8), 'RGB')\n",
            "            image = image.crop((cx, cy, cx + cw, cy + ch))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_generation_figure.py",
        "line_number": 43,
        "API": ".save(",
        "context": [
            "            image = PIL.Image.fromarray(np.clip(im * 255, 0, 255).astype(np.uint8), 'RGB')\n",
            "            image = image.crop((cx, cy, cx + cw, cy + ch))\n",
            "            image = image.resize((cw // 2**lod, ch // 2**lod), PIL.Image.ANTIALIAS)\n",
            "            canvas.paste(image, (sum(cw // 2**lod for lod in lods[:col]), row * ch // 2**lod))\n",
            "    canvas.save(png)\n",
            "\n",
            "\n",
            "def sample(cfg, logger):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_generation_figure.py",
        "line_number": 72,
        "API": ".info(",
        "context": [
            "    mapping_fl = model.mapping_f\n",
            "\n",
            "    dlatent_avg = model.dlatent_avg\n",
            "\n",
            "    logger.info(\"Trainable parameters generator:\")\n",
            "    count_parameters(decoder)\n",
            "\n",
            "    logger.info(\"Trainable parameters discriminator:\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_generation_figure.py",
        "line_number": 95,
        "API": ".load(",
        "context": [
            "                                {},\n",
            "                                logger=logger,\n",
            "                                save=False)\n",
            "\n",
            "    checkpointer.load()\n",
            "\n",
            "    model.eval()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_celeba_pioneer.py",
        "line_number": 41,
        "API": ".makedirs(",
        "context": [
            "    canvas[:, y: y + im_size, x: x + im_size] = image * 0.5 + 0.5\n",
            "\n",
            "\n",
            "def save_sample(model, sample, i):\n",
            "    os.makedirs('results', exist_ok=True)\n",
            "\n",
            "    with torch.no_grad():\n",
            "        model.eval()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_celeba_pioneer.py",
        "line_number": 57,
        "API": ".set_device(",
        "context": [
            "        save_pic(x_rec)\n",
            "\n",
            "\n",
            "def sample(cfg, logger):\n",
            "    torch.cuda.set_device(0)\n",
            "    model = Model(\n",
            "        startf=cfg.MODEL.START_CHANNEL_COUNT,\n",
            "        layer_count=cfg.MODEL.LAYER_COUNT,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_celeba_pioneer.py",
        "line_number": 115,
        "API": ".ones(",
        "context": [
            "        return Z\n",
            "\n",
            "    def decode(x):\n",
            "        layer_idx = torch.arange(2 * cfg.MODEL.LAYER_COUNT)[np.newaxis, :, np.newaxis]\n",
            "        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n",
            "        coefs = torch.where(layer_idx < model.truncation_cutoff, ones, ones)\n",
            "        # x = torch.lerp(model.dlatent_avg.buff.data, x, coefs)\n",
            "        return model.decoder(x, layer_count - 1, 1, noise=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_celeba_pioneer.py",
        "line_number": 130,
        "API": ".transpose(",
        "context": [
            "            for filename in paths:\n",
            "                img = np.asarray(Image.open(path + '/' + filename))\n",
            "                if img.shape[2] == 4:\n",
            "                    img = img[:, :, :3]\n",
            "                im = img.transpose((2, 0, 1))\n",
            "\n",
            "                x = torch.tensor(np.asarray(im, dtype=np.float32), device='cpu', requires_grad=True).cuda() / 127.5 - 1.\n",
            "                if x.shape[0] == 4:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_celeba_pioneer.py",
        "line_number": 141,
        "API": ".cat(",
        "context": [
            "                    x = F.avg_pool2d(x, 2, 2)\n",
            "\n",
            "                latents = encode(x[None, ...].cuda())\n",
            "                f = decode(latents)\n",
            "                r = torch.cat([x[None, ...].detach().cpu(), f.detach().cpu()], dim=3)\n",
            "                os.makedirs('make_figures/output/pioneer/', exist_ok=True)\n",
            "                save_image(f.detach().cpu() * 0.5 + 0.5, 'make_figures/output/pioneer/%s_alae.png' % filename[:-9], nrow=1, pad_value=1.0)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_ffhq_real.py",
        "line_number": 117,
        "API": ".randn(",
        "context": [
            "        # x = torch.lerp(model.dlatent_avg.buff.data, x, coefs)\n",
            "        return model.decoder(x, layer_count - 1, 1, noise=True)\n",
            "\n",
            "    rnd = np.random.RandomState(5)\n",
            "    latents = rnd.randn(1, cfg.MODEL.LATENT_SPACE_SIZE)\n",
            "\n",
            "    dataset = TFRecordsDataset(cfg, logger, rank=0, world_size=1, buffer_size_mb=10, channels=cfg.MODEL.CHANNELS, train=False)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_ffhq_real.py",
        "line_number": 133,
        "API": ".cat(",
        "context": [
            "                if x.shape[0] == 4:\n",
            "                    x = x[:3]\n",
            "                latents = encode(x[None, ...].cuda())\n",
            "                f = decode(latents)\n",
            "                r = torch.cat([x[None, ...].detach().cpu(), f.detach().cpu()], dim=3)\n",
            "                canvas.append(r)\n",
            "        return canvas\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_ffhq_real.py",
        "line_number": 139,
        "API": ".cat(",
        "context": [
            "        return canvas\n",
            "\n",
            "    sample = next(b)\n",
            "    canvas = make(sample)\n",
            "    canvas = torch.cat(canvas, dim=0)\n",
            "\n",
            "    save_image(canvas * 0.5 + 0.5, 'make_figures/reconstructions_ffhq_real_1.png', nrow=2, pad_value=1.0)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_ffhq_real.py",
        "line_number": 145,
        "API": ".cat(",
        "context": [
            "    save_image(canvas * 0.5 + 0.5, 'make_figures/reconstructions_ffhq_real_1.png', nrow=2, pad_value=1.0)\n",
            "\n",
            "    sample = next(b)\n",
            "    canvas = make(sample)\n",
            "    canvas = torch.cat(canvas, dim=0)\n",
            "\n",
            "    save_image(canvas * 0.5 + 0.5, 'make_figures/reconstructions_ffhq_real_2.png', nrow=2, pad_value=1.0)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_interpolation.py",
        "line_number": 118,
        "API": ".randn(",
        "context": [
            "        # x = torch.lerp(model.dlatent_avg.buff.data, x, coefs)\n",
            "        return model.decoder(x, layer_count - 1, 1, noise=True)\n",
            "\n",
            "    rnd = np.random.RandomState(4)\n",
            "    latents = rnd.randn(1, cfg.MODEL.LATENT_SPACE_SIZE)\n",
            "\n",
            "    path = cfg.DATASET.SAMPLES_PATH\n",
            "    im_size = 2 ** (cfg.MODEL.LAYER_COUNT + 1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_interpolation.py",
        "line_number": 132,
        "API": ".transpose(",
        "context": [
            "    def open_image(filename):\n",
            "        img = np.asarray(Image.open(path + '/' + filename))\n",
            "        if img.shape[2] == 4:\n",
            "            img = img[:, :, :3]\n",
            "        im = img.transpose((2, 0, 1))\n",
            "        x = torch.tensor(np.asarray(im, dtype=np.float32), device='cpu', requires_grad=True).cuda() / 127.5 - 1.\n",
            "        if x.shape[0] == 4:\n",
            "            x = x[:3]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_interpolation.py",
        "line_number": 146,
        "API": ".repeat(",
        "context": [
            "        return latents\n",
            "\n",
            "    def make(w):\n",
            "        with torch.no_grad():\n",
            "            w = w[None, None, ...].repeat(1, model.mapping_f.num_layers, 1)\n",
            "            x_rec = decode(w)\n",
            "            return x_rec\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_interpolation.py",
        "line_number": 175,
        "API": ".cat(",
        "context": [
            "\n",
            "            interpolated = make(w)\n",
            "            images.append(interpolated)\n",
            "\n",
            "    images = torch.cat(images)\n",
            "\n",
            "    save_image(images * 0.5 + 0.5, 'make_figures/output/%s/interpolations.png' % cfg.NAME, nrow=width)\n",
            "    save_image(images * 0.5 + 0.5, 'make_figures/output/%s/interpolations.jpg' % cfg.NAME, nrow=width)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_multires.py",
        "line_number": 113,
        "API": ".ones(",
        "context": [
            "        return Z\n",
            "\n",
            "    def decode(x):\n",
            "        layer_idx = torch.arange(2 * cfg.MODEL.LAYER_COUNT)[np.newaxis, :, np.newaxis]\n",
            "        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n",
            "        coefs = torch.where(layer_idx < model.truncation_cutoff, 1.0 * ones, ones)\n",
            "        # x = torch.lerp(model.dlatent_avg.buff.data, x, coefs)\n",
            "        return model.decoder(x, layer_count - 1, 1, noise=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_multires.py",
        "line_number": 126,
        "API": ".seed(",
        "context": [
            "\n",
            "    paths = list(os.listdir(path))\n",
            "\n",
            "    paths = sorted(paths)\n",
            "    random.seed(5)\n",
            "    random.shuffle(paths)\n",
            "\n",
            "    def move_to(list, item, new_index):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_multires.py",
        "line_number": 144,
        "API": ".transpose(",
        "context": [
            "        for filename in paths:\n",
            "            img = np.asarray(Image.open(path + '/' + filename))\n",
            "            if img.shape[2] == 4:\n",
            "                img = img[:, :, :3]\n",
            "            im = img.transpose((2, 0, 1))\n",
            "            x = torch.tensor(np.asarray(im, dtype=np.float32), requires_grad=True).cuda() / 127.5 - 1.\n",
            "            if x.shape[0] == 4:\n",
            "                x = x[:3]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_multires.py",
        "line_number": 167,
        "API": ".reverse(",
        "context": [
            "\n",
            "    final = chunker_list(paths, 4)\n",
            "    path0, path1, path2, path3 = final\n",
            "\n",
            "    path0.reverse()\n",
            "    path1.reverse()\n",
            "    path2.reverse()\n",
            "    path3.reverse()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_multires.py",
        "line_number": 203,
        "API": ".ones(",
        "context": [
            "    width = int(width)\n",
            "    height = int(height)\n",
            "\n",
            "    def make_part(current_padding, src, rec):\n",
            "        canvas = np.ones([3, height + 20, width + 10])\n",
            "\n",
            "        padd = 0\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_multires.py",
        "line_number": 249,
        "API": ".makedirs(",
        "context": [
            "    canvas = np.concatenate(canvas, axis=2)\n",
            "\n",
            "    print('Saving image')\n",
            "    save_path = 'make_figures/output/%s/reconstructions_multiresolution.png' % cfg.NAME\n",
            "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
            "    save_image(torch.Tensor(canvas), save_path)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_paged.py",
        "line_number": 125,
        "API": ".seed(",
        "context": [
            "\n",
            "    paths = list(os.listdir(path))\n",
            "\n",
            "    paths = sorted(paths)\n",
            "    random.seed(1)\n",
            "    random.shuffle(paths)\n",
            "\n",
            "    def make(paths):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_paged.py",
        "line_number": 135,
        "API": ".transpose(",
        "context": [
            "            for filename in paths:\n",
            "                img = np.asarray(Image.open(path + '/' + filename))\n",
            "                if img.shape[2] == 4:\n",
            "                    img = img[:, :, :3]\n",
            "                im = img.transpose((2, 0, 1))\n",
            "                x = torch.tensor(np.asarray(im, dtype=np.float32), device='cpu', requires_grad=True).cuda() / 127.5 - 1.\n",
            "                if x.shape[0] == 4:\n",
            "                    x = x[:3]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_paged.py",
        "line_number": 145,
        "API": ".cat(",
        "context": [
            "                    x = torch.nn.functional.avg_pool2d(x[None, ...], factor, factor)[0]\n",
            "                assert x.shape[2] == im_size\n",
            "                latents = encode(x[None, ...].cuda())\n",
            "                f = decode(latents)\n",
            "                r = torch.cat([x[None, ...].detach().cpu(), f.detach().cpu()], dim=3)\n",
            "                canvas.append(r)\n",
            "        return canvas\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_recon_figure_paged.py",
        "line_number": 156,
        "API": ".cat(",
        "context": [
            "    paths = chunker_list(paths, 8 * 3)\n",
            "\n",
            "    for i, chunk in enumerate(paths):\n",
            "        canvas = make(chunk)\n",
            "        canvas = torch.cat(canvas, dim=0)\n",
            "\n",
            "        save_path = 'make_figures/output/%s/reconstructions_%d.png' % (cfg.NAME, i)\n",
            "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_traversarls.py",
        "line_number": 32,
        "API": ".set_device(",
        "context": [
            "lreq.use_implicit_lreq.set(True)\n",
            "\n",
            "\n",
            "def sample(cfg, logger):\n",
            "    torch.cuda.set_device(0)\n",
            "    model = Model(\n",
            "        startf=cfg.MODEL.START_CHANNEL_COUNT,\n",
            "        layer_count=cfg.MODEL.LAYER_COUNT,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_traversarls.py",
        "line_number": 102,
        "API": ".transpose(",
        "context": [
            "    def do_attribute_traversal(path, attrib_idx, start, end):\n",
            "        img = np.asarray(Image.open(path))\n",
            "        if img.shape[2] == 4:\n",
            "            img = img[:, :, :3]\n",
            "        im = img.transpose((2, 0, 1))\n",
            "        x = torch.tensor(np.asarray(im, dtype=np.float32), device='cpu', requires_grad=True).cuda() / 127.5 - 1.\n",
            "        if x.shape[0] == 4:\n",
            "            x = x[:3]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_traversarls.py",
        "line_number": 115,
        "API": ".load(",
        "context": [
            "        latents = _latents[0, 0]\n",
            "\n",
            "        latents -= model.dlatent_avg.buff.data[0]\n",
            "\n",
            "        w0 = torch.tensor(np.load(\"principal_directions/direction_%d.npy\" % attrib_idx), dtype=torch.float32)\n",
            "\n",
            "        attr0 = (latents * w0).sum()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_traversarls.py",
        "line_number": 124,
        "API": ".repeat(",
        "context": [
            "\n",
            "        def update_image(w):\n",
            "            with torch.no_grad():\n",
            "                w = w + model.dlatent_avg.buff.data[0]\n",
            "                w = w[None, None, ...].repeat(1, model.mapping_f.num_layers, 1)\n",
            "\n",
            "                layer_idx = torch.arange(model.mapping_f.num_layers)[np.newaxis, :, np.newaxis]\n",
            "                cur_layers = (7 + 1) * 2\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_traversarls.py",
        "line_number": 129,
        "API": ".where(",
        "context": [
            "\n",
            "                layer_idx = torch.arange(model.mapping_f.num_layers)[np.newaxis, :, np.newaxis]\n",
            "                cur_layers = (7 + 1) * 2\n",
            "                mixing_cutoff = cur_layers\n",
            "                styles = torch.where(layer_idx < mixing_cutoff, w, _latents[0])\n",
            "\n",
            "                x_rec = decode(styles)\n",
            "                return x_rec\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\make_traversarls.py",
        "line_number": 145,
        "API": ".cat(",
        "context": [
            "            im = update_image(W)\n",
            "\n",
            "            traversal.append(im)\n",
            "            attr0 += inc\n",
            "        res = torch.cat(traversal)\n",
            "\n",
            "        indices = [0, 1, 2, 3, 4, 10, 11, 17, 19]\n",
            "        labels = [\"gender\",\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\old\\make_recon_figure_bed.py",
        "line_number": 122,
        "API": ".randn(",
        "context": [
            "        # x = torch.lerp(model.dlatent_avg.buff.data, x, coefs)\n",
            "        return model.decoder(x, layer_count - 1, 1, noise=True)\n",
            "\n",
            "    rnd = np.random.RandomState(5)\n",
            "    latents = rnd.randn(1, cfg.MODEL.LATENT_SPACE_SIZE)\n",
            "\n",
            "    path = cfg.DATASET.SAMPLES_PATH\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\old\\make_recon_figure_bed.py",
        "line_number": 129,
        "API": ".seed(",
        "context": [
            "\n",
            "    paths = list(os.listdir(path))\n",
            "\n",
            "    paths = sorted(paths)\n",
            "    random.seed(3456)\n",
            "    random.shuffle(paths)\n",
            "\n",
            "    def make(paths):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\old\\make_recon_figure_bed.py",
        "line_number": 150,
        "API": ".cat(",
        "context": [
            "                canvas.append(r)\n",
            "        return canvas\n",
            "\n",
            "    canvas = make(paths[:40])\n",
            "    canvas = torch.cat(canvas, dim=0)\n",
            "\n",
            "    save_image(canvas * 0.5 + 0.5, 'make_figures/output/reconstructions_bed_1.png', nrow=4, pad_value=1.0)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\old\\make_recon_figure_bed.py",
        "line_number": 155,
        "API": ".cat(",
        "context": [
            "\n",
            "    save_image(canvas * 0.5 + 0.5, 'make_figures/output/reconstructions_bed_1.png', nrow=4, pad_value=1.0)\n",
            "\n",
            "    canvas = make(paths[40:80])\n",
            "    canvas = torch.cat(canvas, dim=0)\n",
            "\n",
            "    save_image(canvas * 0.5 + 0.5, 'make_figures/output/reconstructions_bed_2.png', nrow=4, pad_value=1.0)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\old\\make_recon_figure_celeba.py",
        "line_number": 150,
        "API": ".cat(",
        "context": [
            "                    x = F.avg_pool2d(x, 2, 2)\n",
            "\n",
            "                latents = encode(x[None, ...].cuda())\n",
            "                f = decode(latents)\n",
            "                r = torch.cat([x[None, ...].detach().cpu(), f.detach().cpu()], dim=3)\n",
            "                canvas.append(r)\n",
            "        return canvas\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\old\\make_recon_figure_celeba.py",
        "line_number": 155,
        "API": ".cat(",
        "context": [
            "                canvas.append(r)\n",
            "        return canvas\n",
            "\n",
            "    canvas = make(paths[:10])\n",
            "    canvas = torch.cat(canvas, dim=0)\n",
            "\n",
            "    save_image(canvas * 0.5 + 0.5, 'make_figures/output/reconstructions_celeba_1.png', nrow=2, pad_value=1.0)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\make_figures\\old\\make_recon_figure_celeba.py",
        "line_number": 160,
        "API": ".cat(",
        "context": [
            "\n",
            "    save_image(canvas * 0.5 + 0.5, 'make_figures/output/reconstructions_celeba_1.png', nrow=2, pad_value=1.0)\n",
            "\n",
            "    canvas = make(paths[10:20])\n",
            "    canvas = torch.cat(canvas, dim=0)\n",
            "\n",
            "    save_image(canvas * 0.5 + 0.5, 'make_figures/output/reconstructions_celeba_2.png', nrow=2, pad_value=1.0)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\fid.py",
        "line_number": 42,
        "API": ".load(",
        "context": [
            "        self.cfg = cfg\n",
            "\n",
            "    def evaluate(self, logger, mapping, decoder, model, lod):\n",
            "        gpu_count = torch.cuda.device_count()\n",
            "        inception = pickle.load(open('metrics/inception_v3_features.pkl', 'rb'))\n",
            "\n",
            "        # Sampling loop.\n",
            "        @utils.cache\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\fid.py",
        "line_number": 68,
        "API": ".mean(",
        "context": [
            "            assert activations.shape[0] >= num_images\n",
            "            activations = activations[:num_images]\n",
            "            assert activations.shape[0] == num_images\n",
            "\n",
            "            mu_real = np.mean(activations, axis=0)\n",
            "            sigma_real = np.cov(activations, rowvar=False)\n",
            "            return mu_real, sigma_real\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\fid.py",
        "line_number": 76,
        "API": ".set_device(",
        "context": [
            "        mu_real, sigma_real = compute_for_reals(self.num_images, self.cfg.DATASET.PATH, lod)\n",
            "\n",
            "        activations = []\n",
            "        for _ in tqdm(range(0, self.num_images, self.minibatch_size)):\n",
            "            torch.cuda.set_device(0)\n",
            "            images = model.generate(lod, 1, count=self.minibatch_size, no_truncation=True)\n",
            "\n",
            "            images = np.clip((images.cpu().numpy() + 1.0) * 127, 0, 255).astype(np.uint8)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\fid.py",
        "line_number": 93,
        "API": ".mean(",
        "context": [
            "        assert activations.shape[0] >= self.num_images\n",
            "        activations = activations[:self.num_images]\n",
            "        assert activations.shape[0] == self.num_images\n",
            "\n",
            "        mu_fake = np.mean(activations, axis=0)\n",
            "        sigma_fake = np.cov(activations, rowvar=False)\n",
            "\n",
            "        # Calculate FID.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\fid.py",
        "line_number": 99,
        "API": ".trace(",
        "context": [
            "\n",
            "        # Calculate FID.\n",
            "        m = np.square(mu_fake - mu_real).sum()\n",
            "        s, _ = scipy.linalg.sqrtm(np.dot(sigma_fake, sigma_real), disp=False)\n",
            "        dist = m + np.trace(sigma_fake + sigma_real - 2*s)\n",
            "\n",
            "        logger.info(\"Result = %f\" % (np.real(dist)))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\fid.py",
        "line_number": 105,
        "API": ".set_device(",
        "context": [
            "        logger.info(\"Result = %f\" % (np.real(dist)))\n",
            "\n",
            "\n",
            "def sample(cfg, logger):\n",
            "    torch.cuda.set_device(0)\n",
            "    model = Model(\n",
            "        startf=cfg.MODEL.START_CHANNEL_COUNT,\n",
            "        layer_count=cfg.MODEL.LAYER_COUNT,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\fid.py",
        "line_number": 129,
        "API": ".info(",
        "context": [
            "\n",
            "    mapping_fl = model.mapping_f\n",
            "    dlatent_avg = model.dlatent_avg\n",
            "\n",
            "    logger.info(\"Trainable parameters generator:\")\n",
            "    count_parameters(decoder)\n",
            "\n",
            "    logger.info(\"Trainable parameters discriminator:\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\fid.py",
        "line_number": 151,
        "API": ".load(",
        "context": [
            "                                {},\n",
            "                                logger=logger,\n",
            "                                save=False)\n",
            "\n",
            "    extra_checkpoint_data = checkpointer.load()\n",
            "    last_epoch = list(extra_checkpoint_data['auxiliary']['scheduler'].values())[0]['last_epoch']\n",
            "    logger.info(\"Model trained for %d epochs\" % last_epoch)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\fid.py",
        "line_number": 159,
        "API": ".info(",
        "context": [
            "    model.eval()\n",
            "\n",
            "    layer_count = cfg.MODEL.LAYER_COUNT\n",
            "\n",
            "    logger.info(\"Evaluating FID metric\")\n",
            "\n",
            "    model.decoder = nn.DataParallel(decoder)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\fid_rec.py",
        "line_number": 42,
        "API": ".load(",
        "context": [
            "        self.cfg = cfg\n",
            "\n",
            "    def evaluate(self, logger, mapping, decoder, encoder, lod):\n",
            "        gpu_count = torch.cuda.device_count()\n",
            "        inception = pickle.load(open('metrics/inception_v3_features.pkl', 'rb'))\n",
            "\n",
            "        # Sampling loop.\n",
            "        @utils.cache\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\fid_rec.py",
        "line_number": 83,
        "API": ".set_device(",
        "context": [
            "\n",
            "        activations = []\n",
            "        num_images_processed = 0\n",
            "        for idx, x in tqdm(enumerate(batches)):\n",
            "            torch.cuda.set_device(0)\n",
            "            x = (x / 127.5 - 1.)\n",
            "\n",
            "            Z = encoder(x, lod, 1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\fid_rec.py",
        "line_number": 175,
        "API": ".info(",
        "context": [
            "    model.eval()\n",
            "\n",
            "    layer_count = cfg.MODEL.LAYER_COUNT\n",
            "\n",
            "    logger.info(\"Evaluating FID metric\")\n",
            "\n",
            "    encoder = nn.DataParallel(encoder)\n",
            "    decoder = nn.DataParallel(decoder)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\fid_sep.py",
        "line_number": 128,
        "API": ".info(",
        "context": [
            "\n",
            "    mapping_fl = model.mapping_fl\n",
            "    dlatent_avg = model.dlatent_avg\n",
            "\n",
            "    logger.info(\"Trainable parameters generator:\")\n",
            "    count_parameters(decoder)\n",
            "\n",
            "    logger.info(\"Trainable parameters discriminator:\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\lpips.py",
        "line_number": 42,
        "API": ".reshape(",
        "context": [
            "\n",
            "def downscale(images):\n",
            "    if images.shape[2] > 256:\n",
            "        factor = images.shape[2] // 256\n",
            "        images = torch.reshape(images,\n",
            "                               [-1, images.shape[1], images.shape[2] // factor, factor, images.shape[3] // factor,\n",
            "                                factor])\n",
            "        images = torch.mean(images, dim=(3, 5))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\lpips.py",
        "line_number": 58,
        "API": ".load(",
        "context": [
            "        self.cfg = cfg\n",
            "\n",
            "    def evaluate(self, logger, mapping, decoder, encoder, lod):\n",
            "        gpu_count = torch.cuda.device_count()\n",
            "        distance_measure = pickle.load(open('metrics/vgg16_zhang_perceptual.pkl', 'rb'))\n",
            "\n",
            "        dataset = TFRecordsDataset(self.cfg, logger, rank=0, world_size=1, buffer_size_mb=128,\n",
            "                                   channels=self.cfg.MODEL.CHANNELS, train=False)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\lpips.py",
        "line_number": 69,
        "API": ".set_device(",
        "context": [
            "\n",
            "        distance = []\n",
            "        num_images_processed = 0\n",
            "        for idx, x in tqdm(enumerate(batches)):\n",
            "            torch.cuda.set_device(0)\n",
            "            x = (x / 127.5 - 1.)\n",
            "\n",
            "            Z = encoder(x, lod, 1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\lpips.py",
        "line_number": 87,
        "API": ".info(",
        "context": [
            "            if num_images_processed > self.num_images:\n",
            "                break\n",
            "\n",
            "        print(len(distance))\n",
            "        logger.info(\"Result = %f\" % (np.asarray(distance).mean()))\n",
            "\n",
            "\n",
            "def sample(cfg, logger):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\lpips.py",
        "line_number": 145,
        "API": ".info(",
        "context": [
            "    model.eval()\n",
            "\n",
            "    layer_count = cfg.MODEL.LAYER_COUNT\n",
            "\n",
            "    logger.info(\"Evaluating LPIPS metric\")\n",
            "\n",
            "    decoder = nn.DataParallel(decoder)\n",
            "    encoder = nn.DataParallel(encoder)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\ppl.py",
        "line_number": 32,
        "API": ".sqrt(",
        "context": [
            "\n",
            "\n",
            "# Normalize batch of vectors.\n",
            "def normalize(v):\n",
            "    return v / torch.sqrt(torch.sum(v * v, dim=-1, keepdim=True))\n",
            "\n",
            "\n",
            "# Spherical interpolation of a batch of vectors.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\ppl.py",
        "line_number": 39,
        "API": ".sum(",
        "context": [
            "# Spherical interpolation of a batch of vectors.\n",
            "def slerp(a, b, t):\n",
            "    a = normalize(a)\n",
            "    b = normalize(b)\n",
            "    d = torch.sum(a * b, dim=-1, keepdim=True)\n",
            "    p = t * torch.acos(d)\n",
            "    c = normalize(b - d * a)\n",
            "    d = a * torch.cos(p) + c * torch.sin(p)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\ppl.py",
        "line_number": 58,
        "API": ".load(",
        "context": [
            "        self.minibatch_size = minibatch_size\n",
            "        self.cfg = cfg\n",
            "\n",
            "    def evaluate(self, logger, mapping, decoder, lod, celeba_style=False):\n",
            "        distance_measure = pickle.load(open('metrics/vgg16_zhang_perceptual.pkl', 'rb'))\n",
            "        gpu_count = torch.cuda.device_count()\n",
            "\n",
            "        # Sampling loop.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\ppl.py",
        "line_number": 64,
        "API": ".set_device(",
        "context": [
            "\n",
            "        # Sampling loop.\n",
            "        all_distances = []\n",
            "        for _ in tqdm.tqdm(range(0, self.num_samples, self.minibatch_size)):\n",
            "            torch.cuda.set_device(0)\n",
            "            # Generate random latents and interpolation t-values.\n",
            "            lat_t01 = torch.randn([self.minibatch_size * 2, self.cfg.MODEL.LATENT_SPACE_SIZE])\n",
            "            lerp_t = torch.rand(self.minibatch_size) * (1.0 if self.sampling == 'full' else 0.0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\ppl.py",
        "line_number": 75,
        "API": ".stack(",
        "context": [
            "                dlat_t01 = mapping(lat_t01)\n",
            "                dlat_t0, dlat_t1 = dlat_t01[0::2], dlat_t01[1::2]\n",
            "                dlat_e0 = torch.lerp(dlat_t0, dlat_t1, lerp_t[:, np.newaxis, np.newaxis])\n",
            "                dlat_e1 = torch.lerp(dlat_t0, dlat_t1, lerp_t[:, np.newaxis, np.newaxis] + self.epsilon)\n",
            "                dlat_e01 = torch.reshape(torch.stack([dlat_e0, dlat_e1], dim=1), dlat_t01.shape)\n",
            "            else:  # space == 'z'\n",
            "                lat_t0, lat_t1 = lat_t01[0::2], lat_t01[1::2]\n",
            "                lat_e0 = slerp(lat_t0, lat_t1, lerp_t[:, np.newaxis])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\ppl.py",
        "line_number": 80,
        "API": ".stack(",
        "context": [
            "            else:  # space == 'z'\n",
            "                lat_t0, lat_t1 = lat_t01[0::2], lat_t01[1::2]\n",
            "                lat_e0 = slerp(lat_t0, lat_t1, lerp_t[:, np.newaxis])\n",
            "                lat_e1 = slerp(lat_t0, lat_t1, lerp_t[:, np.newaxis] + self.epsilon)\n",
            "                lat_e01 = torch.reshape(torch.stack([lat_e0, lat_e1], dim=1), lat_t01.shape)\n",
            "                dlat_e01 = mapping(lat_e01)\n",
            "\n",
            "            # Synthesize images.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\ppl.py",
        "line_number": 106,
        "API": ".transpose(",
        "context": [
            "                vc = int(vc * c)\n",
            "                images = images[:, :, vc - h // 2: vc + h // 2, hc - w // 2: hc + w // 2]\n",
            "\n",
            "            # print(images.shape)\n",
            "            # plt.imshow(images[0].cpu().numpy().transpose(1, 2, 0), interpolation='nearest')\n",
            "            # plt.show()\n",
            "            # exit()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\ppl.py",
        "line_number": 113,
        "API": ".reshape(",
        "context": [
            "\n",
            "            # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
            "            if images.shape[2] > 256:\n",
            "                factor = images.shape[2] // 256\n",
            "                images = torch.reshape(images,\n",
            "                                       [-1, images.shape[1], images.shape[2] // factor, factor,\n",
            "                                        images.shape[3] // factor,\n",
            "                                        factor])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\ppl.py",
        "line_number": 135,
        "API": ".info(",
        "context": [
            "        # Reject outliers.\n",
            "        lo = np.percentile(all_distances, 1, interpolation='lower')\n",
            "        hi = np.percentile(all_distances, 99, interpolation='higher')\n",
            "        filtered_distances = np.extract(np.logical_and(lo <= all_distances, all_distances <= hi), all_distances)\n",
            "        logger.info(\"Result %s = %f\" % (self.sampling, np.mean(filtered_distances)))\n",
            "\n",
            "\n",
            "def sample(cfg, logger):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\ppl.py",
        "line_number": 198,
        "API": ".ones(",
        "context": [
            "        return Z\n",
            "\n",
            "    def decode(x):\n",
            "        layer_idx = torch.arange(2 * cfg.MODEL.LAYER_COUNT)[np.newaxis, :, np.newaxis]\n",
            "        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n",
            "        coefs = torch.where(layer_idx < model.truncation_cutoff, 1.2 * ones, ones)\n",
            "        x = torch.lerp(model.dlatent_avg.buff.data, x, coefs)\n",
            "        return model.decoder(x, layer_count - 1, 1, noise=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\metrics\\ppl.py",
        "line_number": 203,
        "API": ".info(",
        "context": [
            "        coefs = torch.where(layer_idx < model.truncation_cutoff, 1.2 * ones, ones)\n",
            "        x = torch.lerp(model.dlatent_avg.buff.data, x, coefs)\n",
            "        return model.decoder(x, layer_count - 1, 1, noise=True)\n",
            "\n",
            "    logger.info(\"Evaluating PPL metric\")\n",
            "\n",
            "    decoder = nn.DataParallel(decoder)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\principal_directions\\classifier.py",
        "line_number": 23,
        "API": ".load(",
        "context": [
            "    if dnnlib.util.is_url(file_or_url):\n",
            "        file = dnnlib.util.open_url(file_or_url, cache_dir=_cache_dir)\n",
            "    else:\n",
            "        file = open(file_or_url, 'rb')\n",
            "    return pickle.load(file, encoding='latin1')\n",
            "\n",
            "\n",
            "classifier_urls = [\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\principal_directions\\extract_attributes.py",
        "line_number": 34,
        "API": ".reshape(",
        "context": [
            "    shape = ex.features.feature['shape'].int64_list.value\n",
            "    data = ex.features.feature['data'].bytes_list.value[0]\n",
            "    dlat = ex.features.feature['dlat'].bytes_list.value[0]\n",
            "    lat = ex.features.feature['lat'].bytes_list.value[0]\n",
            "    return np.fromstring(data, np.uint8).reshape(shape), np.fromstring(dlat, np.float32), np.fromstring(lat, np.float32)\n",
            "\n",
            "\n",
            "class Predictions:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\principal_directions\\extract_attributes.py",
        "line_number": 49,
        "API": ".batch(",
        "context": [
            "        rnd = np.random.RandomState(5)\n",
            "\n",
            "        with tf.Graph().as_default(), tf.Session() as sess:\n",
            "            ds = tf.data.TFRecordDataset(\"principal_directions/generated_data.000\")\n",
            "            ds = ds.batch(self.minibatch_size)\n",
            "            batch = ds.make_one_shot_iterator().get_next()\n",
            "\n",
            "            classifier = principal_directions.classifier.make_classifier(attrib_idx)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\principal_directions\\extract_attributes.py",
        "line_number": 63,
        "API": ".transpose(",
        "context": [
            "                    lats = []\n",
            "                    for r in records:\n",
            "                        im, dlat, lat = parse_tfrecord_np(r)\n",
            "\n",
            "                        # plt.imshow(im.transpose(1, 2, 0), interpolation='nearest')\n",
            "                        # plt.show()\n",
            "\n",
            "                        images.append(im)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\principal_directions\\extract_attributes.py",
        "line_number": 69,
        "API": ".stack(",
        "context": [
            "\n",
            "                        images.append(im)\n",
            "                        dlats.append(dlat)\n",
            "                        lats.append(lat)\n",
            "                    images = np.stack(images)\n",
            "                    dlats = np.stack(dlats)\n",
            "                    lats = np.stack(lats)\n",
            "                    logits = classifier.run(images, None, num_gpus=1, assume_frozen=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\principal_directions\\extract_attributes.py",
        "line_number": 74,
        "API": ".softmax(",
        "context": [
            "                    dlats = np.stack(dlats)\n",
            "                    lats = np.stack(lats)\n",
            "                    logits = classifier.run(images, None, num_gpus=1, assume_frozen=True)\n",
            "                    logits = torch.tensor(logits)\n",
            "                    predictions = torch.softmax(torch.cat([logits, -logits], dim=1), dim=1)\n",
            "\n",
            "                    result_dict = dict(latents=lats, dlatents=dlats)\n",
            "                    result_dict[attrib_idx] = predictions.cpu().numpy()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\principal_directions\\extract_attributes.py",
        "line_number": 85,
        "API": ".save(",
        "context": [
            "                    break\n",
            "\n",
            "        results = {key: np.concatenate([value[key] for value in result_expr], axis=0) for key in result_expr[0].keys()}\n",
            "\n",
            "        np.save(\"principal_directions/wspace_att_%d\" % attrib_idx, results)\n",
            "\n",
            "\n",
            "def main(cfg, logger):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\principal_directions\\extract_attributes.py",
        "line_number": 141,
        "API": ".info(",
        "context": [
            "    model.eval()\n",
            "\n",
            "    layer_count = cfg.MODEL.LAYER_COUNT\n",
            "\n",
            "    logger.info(\"Extracting attributes\")\n",
            "\n",
            "    decoder = nn.DataParallel(decoder)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\principal_directions\\find_principal_directions.py",
        "line_number": 23,
        "API": ".load(",
        "context": [
            "indices = [0, 1, 2, 3, 4, 10, 11, 17, 19]\n",
            "\n",
            "\n",
            "def run(attrib_idx):\n",
            "    results = np.load(\"principal_directions/wspace_att_%d.npy\" % attrib_idx).item()\n",
            "\n",
            "    pruned_indices = list(range(results['latents'].shape[0]))\n",
            "    # pruned_indices = sorted(pruned_indices, key=lambda i: -np.max(results[attrib_idx][i]))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\principal_directions\\find_principal_directions.py",
        "line_number": 32,
        "API": ".argmax(",
        "context": [
            "    # print('Keeping: %d' % keep)\n",
            "    # pruned_indices = pruned_indices[:keep]\n",
            "\n",
            "    # Fit SVM to the remaining samples.\n",
            "    svm_targets = np.argmax(results[attrib_idx][pruned_indices], axis=1)\n",
            "    space = 'dlatents'\n",
            "\n",
            "    svm_inputs = results[space][pruned_indices]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\principal_directions\\find_principal_directions.py",
        "line_number": 44,
        "API": ".save(",
        "context": [
            "    svm_outputs = svm.predict(svm_inputs)\n",
            "\n",
            "    w = svm.coef_[0]\n",
            "\n",
            "    np.save(\"principal_directions/direction_%d\" % attrib_idx, w)\n",
            "\n",
            "\n",
            "p = mp.Pool(processes=4)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\principal_directions\\generate_images.py",
        "line_number": 40,
        "API": ".set_device(",
        "context": [
            "\n",
            "        rnd = np.random.RandomState(5)\n",
            "\n",
            "        for _ in tqdm(range(0, self.num_samples, self.minibatch_size)):\n",
            "            torch.cuda.set_device(0)\n",
            "            latents = rnd.randn(self.minibatch_size, self.cfg.MODEL.LATENT_SPACE_SIZE)\n",
            "            lat = torch.tensor(latents).float().cuda()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\principal_directions\\generate_images.py",
        "line_number": 59,
        "API": ".write(",
        "context": [
            "                    'shape': tf.train.Feature(int64_list=tf.train.Int64List(value=img.shape)),\n",
            "                    'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.tostring()])),\n",
            "                    'lat': tf.train.Feature(bytes_list=tf.train.BytesList(value=[lat[i].cpu().numpy().tostring()])),\n",
            "                    'dlat': tf.train.Feature(bytes_list=tf.train.BytesList(value=[dlat[i, 0].cpu().numpy().tostring()]))}))\n",
            "                tfr_writer.write(ex.SerializeToString())\n",
            "\n",
            "\n",
            "def sample(cfg, logger):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\principal_directions\\generate_images.py",
        "line_number": 115,
        "API": ".info(",
        "context": [
            "    model.eval()\n",
            "\n",
            "    layer_count = cfg.MODEL.LAYER_COUNT\n",
            "\n",
            "    logger.info(\"Generating...\")\n",
            "\n",
            "    decoder = nn.DataParallel(decoder)\n",
            "    mapping_fl = nn.DataParallel(mapping_fl)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\style_mixing\\stylemix.py",
        "line_number": 45,
        "API": ".set_device(",
        "context": [
            "        _main(cfg, logger)\n",
            "\n",
            "\n",
            "def _main(cfg, logger):\n",
            "    torch.cuda.set_device(0)\n",
            "    model = Model(\n",
            "        startf=cfg.MODEL.START_CHANNEL_COUNT,\n",
            "        layer_count=cfg.MODEL.LAYER_COUNT,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\style_mixing\\stylemix.py",
        "line_number": 105,
        "API": ".cat(",
        "context": [
            "        zlist = []\n",
            "        for i in range(x.shape[0]):\n",
            "            Z, _ = model.encode(x[i][None, ...], layer_count - 1, 1)\n",
            "            zlist.append(Z)\n",
            "        Z = torch.cat(zlist)\n",
            "        Z = Z.repeat(1, model.mapping_f.num_layers, 1)\n",
            "        return Z\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\style_mixing\\stylemix.py",
        "line_number": 114,
        "API": ".cat(",
        "context": [
            "        decoded = []\n",
            "        for i in range(x.shape[0]):\n",
            "            r = model.decoder(x[i][None, ...], layer_count - 1, 1, noise=True)\n",
            "            decoded.append(r)\n",
            "        return torch.cat(decoded)\n",
            "\n",
            "    path = cfg.DATASET.STYLE_MIX_PATH\n",
            "    im_size = 2 ** (cfg.MODEL.LAYER_COUNT + 1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\style_mixing\\stylemix.py",
        "line_number": 122,
        "API": ".join(",
        "context": [
            "\n",
            "    src_originals = []\n",
            "    for i in range(src_len):\n",
            "        try:\n",
            "            im = np.asarray(Image.open(os.path.join(path, 'src/%d.png' % i)))\n",
            "        except FileNotFoundError:\n",
            "            im = np.asarray(Image.open(os.path.join(path, 'src/%d.jpg' % i)))\n",
            "        im = im.transpose((2, 0, 1))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\style_mixing\\stylemix.py",
        "line_number": 134,
        "API": ".stack(",
        "context": [
            "        if factor != 1:\n",
            "            x = torch.nn.functional.avg_pool2d(x[None, ...], factor, factor)[0]\n",
            "        assert x.shape[2] == im_size\n",
            "        src_originals.append(x)\n",
            "    src_originals = torch.stack([x for x in src_originals])\n",
            "    dst_originals = []\n",
            "    for i in range(dst_len):\n",
            "        try:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\style_mixing\\stylemix.py",
        "line_number": 140,
        "API": ".join(",
        "context": [
            "    for i in range(dst_len):\n",
            "        try:\n",
            "            im = np.asarray(Image.open(os.path.join(path, 'dst/%d.png' % i)))\n",
            "        except FileNotFoundError:\n",
            "            im = np.asarray(Image.open(os.path.join(path, 'dst/%d.jpg' % i)))\n",
            "        im = im.transpose((2, 0, 1))\n",
            "        x = torch.tensor(np.asarray(im, dtype=np.float32), requires_grad=True).cuda() / 127.5 - 1.\n",
            "        if x.shape[0] == 4:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\style_mixing\\stylemix.py",
        "line_number": 150,
        "API": ".stack(",
        "context": [
            "        if factor != 1:\n",
            "            x = torch.nn.functional.avg_pool2d(x[None, ...], factor, factor)[0]\n",
            "        assert x.shape[2] == im_size\n",
            "        dst_originals.append(x)\n",
            "    dst_originals = torch.stack([x for x in dst_originals])\n",
            "\n",
            "    src_latents = encode(src_originals)\n",
            "    src_images = decode(src_latents)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\style_mixing\\stylemix.py",
        "line_number": 158,
        "API": ".zeros(",
        "context": [
            "\n",
            "    dst_latents = encode(dst_originals)\n",
            "    dst_images = decode(dst_latents)\n",
            "\n",
            "    canvas = np.zeros([3, im_size * (dst_len + 1), im_size * (src_len + 1)])\n",
            "\n",
            "    os.makedirs('style_mixing/output/%s/' % cfg.NAME, exist_ok=True)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\style_mixing\\stylemix.py",
        "line_number": 173,
        "API": ".clone(",
        "context": [
            "\n",
            "    style_ranges = [range(0, 4)] * 3 + [range(4, 8)] * 2 + [range(8, layer_count * 2)]\n",
            "\n",
            "    def mix_styles(style_src, style_dst, r):\n",
            "        style = style_dst.clone()\n",
            "        style[:, r] = style_src[:, r]\n",
            "        return style\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\ALAE\\style_mixing\\stylemix.py",
        "line_number": 178,
        "API": ".stack(",
        "context": [
            "        style[:, r] = style_src[:, r]\n",
            "        return style\n",
            "\n",
            "    for row in range(dst_len):\n",
            "        row_latents = torch.stack([dst_latents[row]] * src_len)\n",
            "        style = mix_styles(src_latents, row_latents, style_ranges[row])\n",
            "        rec = model.decoder(style, layer_count - 1, 1, noise=True)\n",
            "        for j in range(rec.shape[0]):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\args.py",
        "line_number": 9,
        "API": ".join(",
        "context": [
            "file_path = os.path.dirname(__file__)\n",
            "\n",
            "\n",
            "#\u6a21\u578b\u76ee\u5f55\n",
            "model_dir = os.path.join(file_path, 'albert_lcqmc_checkpoints/')\n",
            "\n",
            "#config\u6587\u4ef6\n",
            "config_name = os.path.join(file_path, 'albert_config/albert_config_tiny.json')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\args.py",
        "line_number": 14,
        "API": ".join(",
        "context": [
            "\n",
            "#config\u6587\u4ef6\n",
            "config_name = os.path.join(file_path, 'albert_config/albert_config_tiny.json')\n",
            "#ckpt\u6587\u4ef6\u540d\u79f0\n",
            "ckpt_name = os.path.join(model_dir, 'model.ckpt')\n",
            "#\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\n",
            "output_dir = os.path.join(file_path, 'albert_lcqmc_checkpoints/')\n",
            "#vocab\u6587\u4ef6\u76ee\u5f55\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\args.py",
        "line_number": 20,
        "API": ".join(",
        "context": [
            "output_dir = os.path.join(file_path, 'albert_lcqmc_checkpoints/')\n",
            "#vocab\u6587\u4ef6\u76ee\u5f55\n",
            "vocab_file = os.path.join(file_path, 'albert_config/vocab.txt')\n",
            "#\u6570\u636e\u76ee\u5f55\n",
            "data_dir = os.path.join(file_path, 'data/')\n",
            "\n",
            "num_train_epochs = 10\n",
            "batch_size = 128\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\args.py",
        "line_number": 36,
        "API": ".join(",
        "context": [
            "# \u5e8f\u5217\u7684\u6700\u5927\u7a0b\u5ea6\uff0c\u5355\u6587\u672c\u5efa\u8bae\u628a\u8be5\u503c\u8c03\u5c0f\n",
            "max_seq_len = 128\n",
            "\n",
            "# graph\u540d\u5b57\n",
            "graph_file = os.path.join(file_path, 'albert_lcqmc_checkpoints/graph')"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\bert_utils.py",
        "line_number": 43,
        "API": ".shape(",
        "context": [
            "\n",
            "\tif not non_static_indexes:\n",
            "\t\treturn shape\n",
            "\n",
            "\tdyn_shape = tf.shape(tensor)\n",
            "\tfor index in non_static_indexes:\n",
            "\t\tshape[index] = dyn_shape[index]\n",
            "\treturn shape\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\bert_utils.py",
        "line_number": 58,
        "API": ".reshape(",
        "context": [
            "\tif ndims == 2:\n",
            "\t\treturn input_tensor\n",
            "\n",
            "\twidth = input_tensor.shape[-1]\n",
            "\toutput_tensor = tf.reshape(input_tensor, [-1, width])\n",
            "\treturn output_tensor\n",
            "\n",
            "def reshape_from_matrix(output_tensor, orig_shape_list):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\bert_utils.py",
        "line_number": 71,
        "API": ".reshape(",
        "context": [
            "\n",
            "\torig_dims = orig_shape_list[0:-1]\n",
            "\twidth = output_shape[-1]\n",
            "\n",
            "\treturn tf.reshape(output_tensor, orig_dims + [width])\n",
            "\n",
            "def assert_rank(tensor, expected_rank, name=None):\n",
            "\t\"\"\"Raises an exception if the tensor rank is not of the expected rank.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\bert_utils.py",
        "line_number": 109,
        "API": ".reshape(",
        "context": [
            "\tbatch_size = sequence_shape[0]\n",
            "\tseq_length = sequence_shape[1]\n",
            "\twidth = sequence_shape[2]\n",
            "\n",
            "\tflat_offsets = tf.reshape(\n",
            "\t\t\ttf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n",
            "\tflat_positions = tf.reshape(positions + flat_offsets, [-1])\n",
            "\tflat_sequence_tensor = tf.reshape(sequence_tensor,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\bert_utils.py",
        "line_number": 114,
        "API": ".gather(",
        "context": [
            "\t\t\ttf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n",
            "\tflat_positions = tf.reshape(positions + flat_offsets, [-1])\n",
            "\tflat_sequence_tensor = tf.reshape(sequence_tensor,\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[batch_size * seq_length, width])\n",
            "\toutput_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n",
            "\treturn output_tensor\n",
            "\n",
            "# add sequence mask for:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\bert_utils.py",
        "line_number": 126,
        "API": ".ones(",
        "context": [
            "\tif seq_type == 'seq2seq':\n",
            "\t\tif mask_sequence is not None:\n",
            "\t\t\tseq_shape = get_shape_list(mask_sequence, expected_rank=2)\n",
            "\t\t\tseq_len = seq_shape[1]\n",
            "\t\t\tones = tf.ones((1, seq_len, seq_len))\n",
            "\t\t\ta_mask = tf.matrix_band_part(ones, -1, 0)\n",
            "\t\t\ts_ex12 = tf.expand_dims(tf.expand_dims(mask_sequence, 1), 2)\n",
            "\t\t\ts_ex13 = tf.expand_dims(tf.expand_dims(mask_sequence, 1), 3)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\bert_utils.py",
        "line_number": 132,
        "API": ".reshape(",
        "context": [
            "\t\t\ts_ex12 = tf.expand_dims(tf.expand_dims(mask_sequence, 1), 2)\n",
            "\t\t\ts_ex13 = tf.expand_dims(tf.expand_dims(mask_sequence, 1), 3)\n",
            "\t\t\ta_mask = (1 - s_ex13) * (1 - s_ex12) + s_ex13 * a_mask\n",
            "\t\t\t# generate mask of batch x seq_len x seq_len\n",
            "\t\t\ta_mask = tf.reshape(a_mask, (-1, seq_len, seq_len))\n",
            "\t\t\tout_mask = attention_mask * a_mask\n",
            "\t\telse:\n",
            "\t\t\tones = tf.ones_like(attention_mask[:1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 122,
        "API": ".strip(",
        "context": [
            "    with tf.gfile.Open(input_file, \"r\") as f:\n",
            "      reader = f.readlines()\n",
            "      lines = []\n",
            "      for line in reader:\n",
            "        lines.append(line.strip().split(\"_!_\"))\n",
            "      return lines\n",
            "\n",
            "  @classmethod\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 132,
        "API": ".strip(",
        "context": [
            "    with tf.gfile.Open(input_file, \"r\") as f:\n",
            "      reader = f.readlines()\n",
            "      lines = []\n",
            "      for line in reader:\n",
            "        lines.append(json.loads(line.strip()))\n",
            "      return lines\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 142,
        "API": ".join(",
        "context": [
            "\n",
            "  def get_train_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    return self._create_examples(\n",
            "        self._read_json(os.path.join(data_dir, \"train.json\")), \"train\")\n",
            "\n",
            "  def get_dev_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 147,
        "API": ".join(",
        "context": [
            "\n",
            "  def get_dev_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    return self._create_examples(\n",
            "        self._read_json(os.path.join(data_dir, \"dev.json\")), \"dev\")\n",
            "\n",
            "  def get_test_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 152,
        "API": ".join(",
        "context": [
            "\n",
            "  def get_test_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    return self._create_examples(\n",
            "        self._read_json(os.path.join(data_dir, \"test.json\")), \"test\")\n",
            "\n",
            "  def _create_examples(self, lines, set_type):\n",
            "    \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 177,
        "API": ".join(",
        "context": [
            "#\n",
            "#     def get_train_examples(self, data_dir):\n",
            "#         \"\"\"See base class.\"\"\"\n",
            "#         return self._create_examples(\n",
            "#             self._read_txt(os.path.join(data_dir, \"toutiao_category_train.txt\")), \"train\")\n",
            "#\n",
            "#     def get_dev_examples(self, data_dir):\n",
            "#         \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 182,
        "API": ".join(",
        "context": [
            "#\n",
            "#     def get_dev_examples(self, data_dir):\n",
            "#         \"\"\"See base class.\"\"\"\n",
            "#         return self._create_examples(\n",
            "#             self._read_txt(os.path.join(data_dir, \"toutiao_category_dev.txt\")), \"dev\")\n",
            "#\n",
            "#     def get_test_examples(self, data_dir):\n",
            "#         \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 187,
        "API": ".join(",
        "context": [
            "#\n",
            "#     def get_test_examples(self, data_dir):\n",
            "#         \"\"\"See base class.\"\"\"\n",
            "#         return self._create_examples(\n",
            "#             self._read_txt(os.path.join(data_dir, \"toutiao_category_test.txt\")), \"test\")\n",
            "#\n",
            "#     def get_labels(self):\n",
            "#         \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 229,
        "API": ".join(",
        "context": [
            "\n",
            "  def get_test_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    return self._create_examples(\n",
            "        self._read_json(os.path.join(data_dir, \"test.json\")), \"test\")\n",
            "\n",
            "  def get_labels(self):\n",
            "    \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 259,
        "API": ".join(",
        "context": [
            "#\n",
            "#     def get_train_examples(self, data_dir):\n",
            "#         \"\"\"See base class.\"\"\"\n",
            "#         return self._create_examples(\n",
            "#             self._read_txt(os.path.join(data_dir, \"train.txt\")), \"train\")\n",
            "#\n",
            "#     def get_dev_examples(self, data_dir):\n",
            "#         \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 264,
        "API": ".join(",
        "context": [
            "#\n",
            "#     def get_dev_examples(self, data_dir):\n",
            "#         \"\"\"See base class.\"\"\"\n",
            "#         return self._create_examples(\n",
            "#             self._read_txt(os.path.join(data_dir, \"dev.txt\")), \"dev\")\n",
            "#\n",
            "#     def get_test_examples(self, data_dir):\n",
            "#         \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 269,
        "API": ".join(",
        "context": [
            "#\n",
            "#     def get_test_examples(self, data_dir):\n",
            "#         \"\"\"See base class.\"\"\"\n",
            "#         return self._create_examples(\n",
            "#             self._read_txt(os.path.join(data_dir, \"test.txt\")), \"test\")\n",
            "#\n",
            "#     def get_labels(self):\n",
            "#         \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 371,
        "API": ".join(",
        "context": [
            "  \"\"\"Processor for the CMNLI data set.\"\"\"\n",
            "\n",
            "  def get_train_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    return self._create_examples_json(os.path.join(data_dir, \"train.json\"), \"train\")\n",
            "\n",
            "  def get_dev_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 379,
        "API": ".join(",
        "context": [
            "    return self._create_examples_json(os.path.join(data_dir, \"dev.json\"), \"dev\")\n",
            "\n",
            "  def get_test_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    return self._create_examples_json(os.path.join(data_dir, \"test.json\"), \"test\")\n",
            "\n",
            "  def get_labels(self):\n",
            "    \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 431,
        "API": ".join(",
        "context": [
            "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
            "    examples = []\n",
            "    for (i, line) in enumerate(lines):\n",
            "      guid = \"%s-%s\" % (set_type, i)\n",
            "      text_a = convert_to_unicode(\" \".join(line['keyword']))\n",
            "      text_b = convert_to_unicode(line['abst'])\n",
            "      label = convert_to_unicode(line['label']) if set_type != 'test' else '0'\n",
            "      examples.append(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 445,
        "API": ".join(",
        "context": [
            "#\n",
            "#   def get_train_examples(self, data_dir):\n",
            "#     \"\"\"See base class.\"\"\"\n",
            "#     return self._create_examples(\n",
            "#         self._read_txt(os.path.join(data_dir, \"train.txt\")), \"train\")\n",
            "#\n",
            "#   def get_dev_examples(self, data_dir):\n",
            "#     \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 450,
        "API": ".join(",
        "context": [
            "#\n",
            "#   def get_dev_examples(self, data_dir):\n",
            "#     \"\"\"See base class.\"\"\"\n",
            "#     return self._create_examples(\n",
            "#         self._read_txt(os.path.join(data_dir, \"dev.txt\")), \"dev\")\n",
            "#\n",
            "#   def get_test_examples(self, data_dir):\n",
            "#     \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 455,
        "API": ".join(",
        "context": [
            "#\n",
            "#   def get_test_examples(self, data_dir):\n",
            "#     \"\"\"See base class.\"\"\"\n",
            "#     return self._create_examples(\n",
            "#         self._read_txt(os.path.join(data_dir, \"test.txt\")), \"test\")\n",
            "#\n",
            "#   def get_labels(self):\n",
            "#     \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 525,
        "API": ".join(",
        "context": [
            "#\n",
            "#   def get_train_examples(self, data_dir):\n",
            "#     \"\"\"See base class.\"\"\"\n",
            "#     return self._create_examples(\n",
            "#         self._read_tsv(os.path.join(data_dir, \"train.txt\")), \"train\")\n",
            "#     # dev_0827.tsv\n",
            "#\n",
            "#   def get_dev_examples(self, data_dir):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 531,
        "API": ".join(",
        "context": [
            "#\n",
            "#   def get_dev_examples(self, data_dir):\n",
            "#     \"\"\"See base class.\"\"\"\n",
            "#     return self._create_examples(\n",
            "#         self._read_tsv(os.path.join(data_dir, \"dev.txt\")), \"dev\")\n",
            "#\n",
            "#   def get_test_examples(self, data_dir):\n",
            "#     \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 536,
        "API": ".join(",
        "context": [
            "#\n",
            "#   def get_test_examples(self, data_dir):\n",
            "#     \"\"\"See base class.\"\"\"\n",
            "#     return self._create_examples(\n",
            "#         self._read_tsv(os.path.join(data_dir, \"test.txt\")), \"test\")\n",
            "#\n",
            "#   def get_labels(self):\n",
            "#     \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 572,
        "API": ".join(",
        "context": [
            "#\n",
            "#   def get_train_examples(self, data_dir):\n",
            "#     \"\"\"See base class.\"\"\"\n",
            "#     return self._create_examples(\n",
            "#         self._read_tsv(os.path.join(data_dir, \"jd_train.csv\"), \",\", \"\\\"\"), \"train\")\n",
            "#     # dev_0827.tsv\n",
            "#\n",
            "#   def get_dev_examples(self, data_dir):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 578,
        "API": ".join(",
        "context": [
            "#\n",
            "#   def get_dev_examples(self, data_dir):\n",
            "#     \"\"\"See base class.\"\"\"\n",
            "#     return self._create_examples(\n",
            "#         self._read_tsv(os.path.join(data_dir, \"jd_dev.csv\"), \",\", \"\\\"\"), \"dev\")\n",
            "#\n",
            "#   def get_test_examples(self, data_dir):\n",
            "#     \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 583,
        "API": ".join(",
        "context": [
            "#\n",
            "#   def get_test_examples(self, data_dir):\n",
            "#     \"\"\"See base class.\"\"\"\n",
            "#     return self._create_examples(\n",
            "#         self._read_tsv(os.path.join(data_dir, \"jd_test.csv\"), \",\", \"\\\"\"), \"test\")\n",
            "#\n",
            "#   def get_labels(self):\n",
            "#     \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 663,
        "API": ".join(",
        "context": [
            "#\n",
            "#   def get_train_examples(self, data_dir):\n",
            "#     \"\"\"See base class.\"\"\"\n",
            "#     return self._create_examples(\n",
            "#         self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
            "#\n",
            "#   def get_dev_examples(self, data_dir):\n",
            "#     \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 668,
        "API": ".join(",
        "context": [
            "#\n",
            "#   def get_dev_examples(self, data_dir):\n",
            "#     \"\"\"See base class.\"\"\"\n",
            "#     return self._create_examples(\n",
            "#         self._read_tsv(os.path.join(data_dir, \"dev_matched.tsv\")),\n",
            "#         \"dev_matched\")\n",
            "#\n",
            "#   def get_test_examples(self, data_dir):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 674,
        "API": ".join(",
        "context": [
            "#\n",
            "#   def get_test_examples(self, data_dir):\n",
            "#     \"\"\"See base class.\"\"\"\n",
            "#     return self._create_examples(\n",
            "#         self._read_tsv(os.path.join(data_dir, \"test_matched.tsv\")), \"test\")\n",
            "#\n",
            "#   def get_labels(self):\n",
            "#     \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 709,
        "API": ".join(",
        "context": [
            "#\n",
            "#   def get_dev_examples(self, data_dir):\n",
            "#     \"\"\"See base class.\"\"\"\n",
            "#     return self._create_examples(\n",
            "#         self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
            "#\n",
            "#   def get_test_examples(self, data_dir):\n",
            "#     \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 714,
        "API": ".join(",
        "context": [
            "#\n",
            "#   def get_test_examples(self, data_dir):\n",
            "#     \"\"\"See base class.\"\"\"\n",
            "#     return self._create_examples(\n",
            "#         self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
            "#\n",
            "#   def get_labels(self):\n",
            "#     \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 814,
        "API": ".format(",
        "context": [
            "      pronoun = target['span2_text']\n",
            "      pronoun_idx = target['span2_index']\n",
            "\n",
            "      assert text_a[pronoun_idx: (pronoun_idx + len(pronoun))\n",
            "                    ] == pronoun, \"pronoun: {}\".format(pronoun)\n",
            "      assert text_a[query_idx: (query_idx + len(query))] == query, \"query: {}\".format(query)\n",
            "\n",
            "      if pronoun_idx > query_idx:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 828,
        "API": ".join(",
        "context": [
            "        text_a_list.insert(pronoun_idx + len(pronoun) + 1, \"]\")\n",
            "        text_a_list.insert(query_idx + 2, \"_\")\n",
            "        text_a_list.insert(query_idx + len(query) + 2 + 1, \"_\")\n",
            "\n",
            "      text_a = \"\".join(text_a_list)\n",
            "\n",
            "      if set_type == \"test\":\n",
            "        label = \"true\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\classifier_utils.py",
        "line_number": 849,
        "API": ".join(",
        "context": [
            "\n",
            "  def get_train_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    return self._create_examples(\n",
            "        self._read_json(os.path.join(data_dir, \"train.json\")), \"train\")\n",
            "    # dev_0827.tsv\n",
            "\n",
            "  def get_dev_examples(self, data_dir):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data.py",
        "line_number": 83,
        "API": ".join(",
        "context": [
            "    self.masked_lm_labels = masked_lm_labels\n",
            "\n",
            "  def __str__(self):\n",
            "    s = \"\"\n",
            "    s += \"tokens: %s\\n\" % (\" \".join(\n",
            "        [tokenization.printable_text(x) for x in self.tokens]))\n",
            "    s += \"segment_ids: %s\\n\" % (\" \".join([str(x) for x in self.segment_ids]))\n",
            "    s += \"is_random_next: %s\\n\" % self.is_random_next\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data.py",
        "line_number": 89,
        "API": ".join(",
        "context": [
            "    s += \"segment_ids: %s\\n\" % (\" \".join([str(x) for x in self.segment_ids]))\n",
            "    s += \"is_random_next: %s\\n\" % self.is_random_next\n",
            "    s += \"masked_lm_positions: %s\\n\" % (\" \".join(\n",
            "        [str(x) for x in self.masked_lm_positions]))\n",
            "    s += \"masked_lm_labels: %s\\n\" % (\" \".join(\n",
            "        [tokenization.printable_text(x) for x in self.masked_lm_labels]))\n",
            "    s += \"\\n\"\n",
            "    return s\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data.py",
        "line_number": 145,
        "API": ".write(",
        "context": [
            "    features[\"next_sentence_labels\"] = create_int_feature([next_sentence_label])\n",
            "\n",
            "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
            "\n",
            "    writers[writer_index].write(tf_example.SerializeToString())\n",
            "    writer_index = (writer_index + 1) % len(writers)\n",
            "\n",
            "    total_written += 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data.py",
        "line_number": 151,
        "API": ".info(",
        "context": [
            "\n",
            "    total_written += 1\n",
            "\n",
            "    if inst_index < 20:\n",
            "      tf.logging.info(\"*** Example ***\")\n",
            "      tf.logging.info(\"tokens: %s\" % \" \".join(\n",
            "          [tokenization.printable_text(x) for x in instance.tokens]))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data.py",
        "line_number": 162,
        "API": ".info(",
        "context": [
            "        if feature.int64_list.value:\n",
            "          values = feature.int64_list.value\n",
            "        elif feature.float_list.value:\n",
            "          values = feature.float_list.value\n",
            "        tf.logging.info(\n",
            "            \"%s: %s\" % (feature_name, \" \".join([str(x) for x in values])))\n",
            "\n",
            "  for writer in writers:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data.py",
        "line_number": 168,
        "API": ".info(",
        "context": [
            "\n",
            "  for writer in writers:\n",
            "    writer.close()\n",
            "\n",
            "  tf.logging.info(\"Wrote %d total instances\", total_written)\n",
            "\n",
            "\n",
            "def create_int_feature(values):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data.py",
        "line_number": 201,
        "API": ".strip(",
        "context": [
            "        strings=strings.replace(\"   \",\" \").replace(\"  \",\" \") # \u5982\u679c\u6709\u4e24\u4e2a\u6216\u4e09\u4e2a\u7a7a\u683c\uff0c\u66ff\u6362\u4e3a\u4e00\u4e2a\u7a7a\u683c\n",
            "        line = tokenization.convert_to_unicode(strings)\n",
            "        if not line:\n",
            "          break\n",
            "        line = line.strip()\n",
            "\n",
            "        # Empty lines are used as document delimiters\n",
            "        if not line:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data.py",
        "line_number": 232,
        "API": ".join(",
        "context": [
            "    \u8f93\u5165\u4e00\u53e5\u8bdd\uff0c\u8fd4\u56de\u4e00\u53e5\u7ecf\u8fc7\u5904\u7406\u7684\u8bdd: \u4e3a\u4e86\u652f\u6301\u4e2d\u6587\u5168\u79f0mask\uff0c\u5c06\u88ab\u5206\u5f00\u7684\u8bcd\uff0c\u5c06\u4e0a\u7279\u6b8a\u6807\u8bb0(\"#\")\uff0c\u4f7f\u5f97\u540e\u7eed\u5904\u7406\u6a21\u5757\uff0c\u80fd\u591f\u77e5\u9053\u54ea\u4e9b\u5b57\u662f\u5c5e\u4e8e\u540c\u4e00\u4e2a\u8bcd\u7684\u3002\n",
            "    :param segment: \u4e00\u53e5\u8bdd. e.g.  ['\u60ac', '\u7078', '\u6280', '\u672f', '\u57f9', '\u8bad', '\u4e13', '\u5bb6', '\u6559', '\u4f60', '\u827e', '\u7078', '\u964d', '\u8840', '\u7cd6', '\uff0c', '\u4e3a', '\u7238', '\u5988', '\u6536', '\u597d', '\u4e86', '\uff01']\n",
            "    :return: \u4e00\u53e5\u5904\u7406\u8fc7\u7684\u8bdd e.g.    ['\u60ac', '##\u7078', '\u6280', '\u672f', '\u57f9', '\u8bad', '\u4e13', '##\u5bb6', '\u6559', '\u4f60', '\u827e', '##\u7078', '\u964d', '##\u8840', '##\u7cd6', '\uff0c', '\u4e3a', '\u7238', '##\u5988', '\u6536', '##\u597d', '\u4e86', '\uff01']\n",
            "    \"\"\"\n",
            "    seq_cws = jieba.lcut(\"\".join(segment)) # \u5206\u8bcd\n",
            "    seq_cws_dict = {x: 1 for x in seq_cws} # \u5206\u8bcd\u540e\u7684\u8bcd\u52a0\u5165\u5230\u8bcd\u5178dict\n",
            "    new_segment = []\n",
            "    i = 0\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data.py",
        "line_number": 246,
        "API": ".join(",
        "context": [
            "      has_add = False\n",
            "      for length in range(3, 0, -1):\n",
            "        if i + length > len(segment):\n",
            "          continue\n",
            "        if ''.join(segment[i:i + length]) in seq_cws_dict:\n",
            "          new_segment.append(segment[i])\n",
            "          for l in range(1, length):\n",
            "            new_segment.append('##' + segment[i + l])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data.py",
        "line_number": 547,
        "API": ".add(",
        "context": [
            "        break\n",
            "    if is_any_index_covered:\n",
            "      continue\n",
            "    for index in index_set:\n",
            "      covered_indexes.add(index)\n",
            "\n",
            "      masked_token = None\n",
            "      # 80% of the time, replace with [MASK]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data.py",
        "line_number": 576,
        "API": ".info(",
        "context": [
            "  for p in masked_lms:\n",
            "    masked_lm_positions.append(p.index)\n",
            "    masked_lm_labels.append(p.label)\n",
            "\n",
            "  # tf.logging.info('%s' % (tokens))\n",
            "  # tf.logging.info('%s' % (output_tokens))\n",
            "  return (output_tokens, masked_lm_positions, masked_lm_labels)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data.py",
        "line_number": 675,
        "API": ".set_verbosity(",
        "context": [
            "      trunc_tokens.pop()\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  tf.logging.set_verbosity(tf.logging.INFO)\n",
            "\n",
            "  tokenizer = tokenization.FullTokenizer(\n",
            "      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data.py",
        "line_number": 681,
        "API": ".split(",
        "context": [
            "  tokenizer = tokenization.FullTokenizer(\n",
            "      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
            "\n",
            "  input_files = []\n",
            "  for input_pattern in FLAGS.input_file.split(\",\"):\n",
            "    input_files.extend(tf.gfile.Glob(input_pattern))\n",
            "\n",
            "  tf.logging.info(\"*** Reading from input files ***\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data.py",
        "line_number": 686,
        "API": ".info(",
        "context": [
            "    input_files.extend(tf.gfile.Glob(input_pattern))\n",
            "\n",
            "  tf.logging.info(\"*** Reading from input files ***\")\n",
            "  for input_file in input_files:\n",
            "    tf.logging.info(\"  %s\", input_file)\n",
            "\n",
            "  rng = random.Random(FLAGS.random_seed)\n",
            "  instances = create_training_instances(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data.py",
        "line_number": 694,
        "API": ".split(",
        "context": [
            "      input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,\n",
            "      FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,\n",
            "      rng)\n",
            "\n",
            "  output_files = FLAGS.output_file.split(\",\")\n",
            "  tf.logging.info(\"*** Writing to output files ***\")\n",
            "  for output_file in output_files:\n",
            "    tf.logging.info(\"  %s\", output_file)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data_google.py",
        "line_number": 110,
        "API": ".join(",
        "context": [
            "    self.masked_lm_labels = masked_lm_labels\n",
            "\n",
            "  def __str__(self):\n",
            "    s = \"\"\n",
            "    s += \"tokens: %s\\n\" % (\" \".join(\n",
            "        [tokenization.printable_text(x) for x in self.tokens]))\n",
            "    s += \"segment_ids: %s\\n\" % (\" \".join([str(x) for x in self.segment_ids]))\n",
            "    s += \"token_boundary: %s\\n\" % (\" \".join(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data_google.py",
        "line_number": 116,
        "API": ".join(",
        "context": [
            "    s += \"segment_ids: %s\\n\" % (\" \".join([str(x) for x in self.segment_ids]))\n",
            "    s += \"token_boundary: %s\\n\" % (\" \".join(\n",
            "        [str(x) for x in self.token_boundary]))\n",
            "    s += \"is_random_next: %s\\n\" % self.is_random_next\n",
            "    s += \"masked_lm_positions: %s\\n\" % (\" \".join(\n",
            "        [str(x) for x in self.masked_lm_positions]))\n",
            "    s += \"masked_lm_labels: %s\\n\" % (\" \".join(\n",
            "        [tokenization.printable_text(x) for x in self.masked_lm_labels]))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data_google.py",
        "line_number": 182,
        "API": ".write(",
        "context": [
            "        [sentence_order_label])\n",
            "\n",
            "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
            "\n",
            "    writers[writer_index].write(tf_example.SerializeToString())\n",
            "    writer_index = (writer_index + 1) % len(writers)\n",
            "\n",
            "    total_written += 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data_google.py",
        "line_number": 188,
        "API": ".info(",
        "context": [
            "\n",
            "    total_written += 1\n",
            "\n",
            "    if inst_index < 6:\n",
            "      tf.logging.info(\"*** Example ***\")\n",
            "      tf.logging.info(\"tokens: %s\" % \" \".join(\n",
            "          [tokenization.printable_text(x) for x in instance.tokens]))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data_google.py",
        "line_number": 241,
        "API": ".strip(",
        "context": [
            "          break\n",
            "        if FLAGS.spm_model_file:\n",
            "          line = tokenization.preprocess_text(line, lower=FLAGS.do_lower_case)\n",
            "        else:\n",
            "          line = line.strip()\n",
            "\n",
            "        # Empty lines are used as document delimiters\n",
            "        if not line:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data_google.py",
        "line_number": 397,
        "API": ".add(",
        "context": [
            "\n",
            "def _is_start_piece_sp(piece):\n",
            "  \"\"\"Check if the current word piece is the starting piece (sentence piece).\"\"\"\n",
            "  special_pieces = set(list('!\"#$%&\\\"()*+,-./:;?@[\\\\]^_`{|}~'))\n",
            "  special_pieces.add(u\"\u20ac\".encode(\"utf-8\"))\n",
            "  special_pieces.add(u\"\u00a3\".encode(\"utf-8\"))\n",
            "  # Note(mingdachen):\n",
            "  # For foreign characters, we always treat them as a whole piece.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data_google.py",
        "line_number": 404,
        "API": ".lower(",
        "context": [
            "  # For foreign characters, we always treat them as a whole piece.\n",
            "  english_chars = set(list(\"abcdefghijklmnopqrstuvwhyz\"))\n",
            "  if (six.ensure_str(piece).startswith(\"\u2581\") or\n",
            "      six.ensure_str(piece).startswith(\"<\") or piece in special_pieces or\n",
            "      not all([i.lower() in english_chars.union(special_pieces)\n",
            "               for i in piece])):\n",
            "    return True\n",
            "  else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data_google.py",
        "line_number": 469,
        "API": ".arange(",
        "context": [
            "                       max(1, int(round(len(tokens) * masked_lm_prob))))\n",
            "\n",
            "  # Note(mingdachen):\n",
            "  # By default, we set the probilities to favor longer ngram sequences.\n",
            "  ngrams = np.arange(1, FLAGS.ngram + 1, dtype=np.int64)\n",
            "  pvals = 1. / np.arange(1, FLAGS.ngram + 1)\n",
            "  pvals /= pvals.sum(keepdims=True)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data_google.py",
        "line_number": 501,
        "API": ".sum(",
        "context": [
            "          continue\n",
            "\n",
            "    n = np.random.choice(ngrams[:len(cand_index_set)],\n",
            "                         p=pvals[:len(cand_index_set)] /\n",
            "                         pvals[:len(cand_index_set)].sum(keepdims=True))\n",
            "    index_set = sum(cand_index_set[n - 1], [])\n",
            "    n -= 1\n",
            "    # Note(mingdachen):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data_google.py",
        "line_number": 561,
        "API": ".sum(",
        "context": [
            "            continue\n",
            "\n",
            "      n = np.random.choice(ngrams[:len(cand_index_set)],\n",
            "                           p=pvals[:len(cand_index_set)] /\n",
            "                           pvals[:len(cand_index_set)].sum(keepdims=True))\n",
            "      index_set = sum(cand_index_set[n - 1], [])\n",
            "      n -= 1\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data_google.py",
        "line_number": 582,
        "API": ".add(",
        "context": [
            "          break\n",
            "      if is_any_index_covered:\n",
            "        continue\n",
            "      for index in index_set:\n",
            "        select_indexes.add(index)\n",
            "    assert len(select_indexes) <= num_to_predict\n",
            "\n",
            "    select_indexes = sorted(select_indexes)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data_google.py",
        "line_number": 621,
        "API": ".set_verbosity(",
        "context": [
            "      trunc_tokens.pop()\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  tf.logging.set_verbosity(tf.logging.INFO)\n",
            "\n",
            "  tokenizer = tokenization.FullTokenizer(\n",
            "      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data_google.py",
        "line_number": 628,
        "API": ".split(",
        "context": [
            "      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case,\n",
            "      spm_model_file=FLAGS.spm_model_file)\n",
            "\n",
            "  input_files = []\n",
            "  for input_pattern in FLAGS.input_file.split(\",\"):\n",
            "    input_files.extend(tf.gfile.Glob(input_pattern))\n",
            "\n",
            "  tf.logging.info(\"*** Reading from input files ***\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data_google.py",
        "line_number": 641,
        "API": ".info(",
        "context": [
            "      input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,\n",
            "      FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,\n",
            "      rng)\n",
            "\n",
            "  tf.logging.info(\"number of instances: %i\", len(instances))\n",
            "\n",
            "  output_files = FLAGS.output_file.split(\",\")\n",
            "  tf.logging.info(\"*** Writing to output files ***\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\create_pretraining_data_google.py",
        "line_number": 646,
        "API": ".info(",
        "context": [
            "\n",
            "  output_files = FLAGS.output_file.split(\",\")\n",
            "  tf.logging.info(\"*** Writing to output files ***\")\n",
            "  for output_file in output_files:\n",
            "    tf.logging.info(\"  %s\", output_file)\n",
            "\n",
            "  write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,\n",
            "                                  FLAGS.max_predictions_per_seq, output_files)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\lamb_optimizer_google.py",
        "line_number": 91,
        "API": ".multiply(",
        "context": [
            "          initializer=tf.zeros_initializer())\n",
            "\n",
            "      # Standard Adam update.\n",
            "      next_m = (\n",
            "          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n",
            "      next_v = (\n",
            "          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n",
            "                                                    tf.square(grad)))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\lamb_optimizer_google.py",
        "line_number": 96,
        "API": ".sqrt(",
        "context": [
            "      next_v = (\n",
            "          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n",
            "                                                    tf.square(grad)))\n",
            "\n",
            "      update = next_m / (tf.sqrt(next_v) + self.epsilon)\n",
            "\n",
            "      # Just adding the square of the weights to the loss function is *not*\n",
            "      # the correct way of using L2 regularization/weight decay with Adam,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\lamb_optimizer_google.py",
        "line_number": 110,
        "API": ".norm(",
        "context": [
            "        update += self.weight_decay_rate * param\n",
            "\n",
            "      ratio = 1.0\n",
            "      if self._do_layer_adaptation(param_name):\n",
            "        w_norm = linalg_ops.norm(param, ord=2)\n",
            "        g_norm = linalg_ops.norm(update, ord=2)\n",
            "        ratio = array_ops.where(math_ops.greater(w_norm, 0), array_ops.where(\n",
            "            math_ops.greater(g_norm, 0), (w_norm / g_norm), 1.0), 1.0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\lamb_optimizer_google.py",
        "line_number": 120,
        "API": ".assign(",
        "context": [
            "\n",
            "      next_param = param - update_with_lr\n",
            "\n",
            "      assignments.extend(\n",
            "          [param.assign(next_param),\n",
            "           m.assign(next_m),\n",
            "           v.assign(next_v)])\n",
            "    return tf.group(*assignments, name=name)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\lamb_optimizer_google.py",
        "line_number": 147,
        "API": ".group(",
        "context": [
            "  def _get_variable_name(self, param_name):\n",
            "    \"\"\"Get the variable name from the tensor name.\"\"\"\n",
            "    m = re.match(\"^(.*):\\\\d+$\", six.ensure_str(param_name))\n",
            "    if m is not None:\n",
            "      param_name = m.group(1)\n",
            "    return param_name\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 113,
        "API": ".constant(",
        "context": [
            "  Example usage:\n",
            "\n",
            "  ```python\n",
            "  # Already been converted into WordPiece token ids\n",
            "  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n",
            "  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n",
            "  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 125,
        "API": ".matmul(",
        "context": [
            "    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n",
            "\n",
            "  label_embeddings = tf.get_variable(...)\n",
            "  pooled_output = model.get_pooled_output()\n",
            "  logits = tf.matmul(pooled_output, label_embeddings)\n",
            "  ...\n",
            "  ```\n",
            "  \"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 148,
        "API": ".embedding_lookup(",
        "context": [
            "      input_ids: int32 Tensor of shape [batch_size, seq_length].\n",
            "      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
            "      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
            "      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n",
            "        embeddings or tf.embedding_lookup() for the word embeddings.\n",
            "      scope: (optional) variable scope. Defaults to \"bert\".\n",
            "\n",
            "    Raises:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 165,
        "API": ".ones(",
        "context": [
            "    batch_size = input_shape[0]\n",
            "    seq_length = input_shape[1]\n",
            "\n",
            "    if input_mask is None:\n",
            "      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n",
            "\n",
            "    if token_type_ids is None:\n",
            "      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 246,
        "API": ".squeeze(",
        "context": [
            "      # dimensional representation of the segment.\n",
            "      with tf.variable_scope(\"pooler\"):\n",
            "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
            "        # to the first token. We assume that this has been pre-trained\n",
            "        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n",
            "        self.pooled_output = tf.layers.dense(\n",
            "            first_token_tensor,\n",
            "            config.hidden_size,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 296,
        "API": ".tanh(",
        "context": [
            "\n",
            "  Returns:\n",
            "    `x` with the GELU activation applied.\n",
            "  \"\"\"\n",
            "  cdf = 0.5 * (1.0 + tf.tanh(\n",
            "      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
            "  return x * cdf\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 325,
        "API": ".lower(",
        "context": [
            "\n",
            "  if not activation_string:\n",
            "    return None\n",
            "\n",
            "  act = activation_string.lower()\n",
            "  if act == \"linear\":\n",
            "    return None\n",
            "  elif act == \"relu\":\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 348,
        "API": ".group(",
        "context": [
            "  for var in tvars:\n",
            "    name = var.name\n",
            "    m = re.match(\"^(.*):\\\\d+$\", name)\n",
            "    if m is not None:\n",
            "      name = m.group(1)\n",
            "    name_to_variable[name] = var\n",
            "\n",
            "  init_vars = tf.train.list_variables(init_checkpoint)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 379,
        "API": ".dropout(",
        "context": [
            "  \"\"\"\n",
            "  if dropout_prob is None or dropout_prob == 0.0:\n",
            "    return input_tensor\n",
            "\n",
            "  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n",
            "  return output\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 417,
        "API": ".gather(",
        "context": [
            "    embedding_size: int. Width of the word embeddings.\n",
            "    initializer_range: float. Embedding initialization range.\n",
            "    word_embedding_name: string. Name of the embedding table.\n",
            "    use_one_hot_embeddings: bool. If True, use one-hot method for word\n",
            "      embeddings. If False, use `tf.gather()`.\n",
            "\n",
            "  Returns:\n",
            "    float Tensor of shape [batch_size, seq_length, embedding_size].\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 428,
        "API": ".expand_dims(",
        "context": [
            "  #\n",
            "  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n",
            "  # reshape to [batch_size, seq_length, 1].\n",
            "  if input_ids.shape.ndims == 2:\n",
            "    input_ids = tf.expand_dims(input_ids, axis=[-1]) # shape of input_ids is:[ batch_size, seq_length, 1]\n",
            "\n",
            "  embedding_table = tf.get_variable( # [vocab_size, embedding_size]\n",
            "      name=word_embedding_name,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 435,
        "API": ".reshape(",
        "context": [
            "      name=word_embedding_name,\n",
            "      shape=[vocab_size, embedding_size],\n",
            "      initializer=create_initializer(initializer_range))\n",
            "\n",
            "  flat_input_ids = tf.reshape(input_ids, [-1]) # one rank. shape as (batch_size * sequence_length,)\n",
            "  if use_one_hot_embeddings:\n",
            "    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size) # one_hot_input_ids=[batch_size * sequence_length,vocab_size]\n",
            "    output = tf.matmul(one_hot_input_ids, embedding_table) # output=[batch_size * sequence_length,embedding_size]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 440,
        "API": ".gather(",
        "context": [
            "  if use_one_hot_embeddings:\n",
            "    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size) # one_hot_input_ids=[batch_size * sequence_length,vocab_size]\n",
            "    output = tf.matmul(one_hot_input_ids, embedding_table) # output=[batch_size * sequence_length,embedding_size]\n",
            "  else:\n",
            "    output = tf.gather(embedding_table, flat_input_ids) # [vocab_size, embedding_size]*[batch_size * sequence_length,]--->[batch_size * sequence_length,embedding_size]\n",
            "\n",
            "  input_shape = get_shape_list(input_ids) # input_shape=[ batch_size, seq_length, 1]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 465,
        "API": ".gather(",
        "context": [
            "       embedding_size: int. Width of the word embeddings.\n",
            "       initializer_range: float. Embedding initialization range.\n",
            "       word_embedding_name: string. Name of the embedding table.\n",
            "       use_one_hot_embeddings: bool. If True, use one-hot method for word\n",
            "         embeddings. If False, use `tf.gather()`.\n",
            "\n",
            "     Returns:\n",
            "       float Tensor of shape [batch_size, seq_length, embedding_size].\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 479,
        "API": ".expand_dims(",
        "context": [
            "\n",
            "    # 1.first project one-hot vectors into a lower dimensional embedding space of size E\n",
            "    print(\"embedding_lookup_factorized. factorized embedding parameterization is used.\")\n",
            "    if input_ids.shape.ndims == 2:\n",
            "        input_ids = tf.expand_dims(input_ids, axis=[-1])  # shape of input_ids is:[ batch_size, seq_length, 1]\n",
            "\n",
            "    embedding_table = tf.get_variable(  # [vocab_size, embedding_size]\n",
            "        name=word_embedding_name,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 486,
        "API": ".reshape(",
        "context": [
            "        name=word_embedding_name,\n",
            "        shape=[vocab_size, embedding_size],\n",
            "        initializer=create_initializer(initializer_range))\n",
            "\n",
            "    flat_input_ids = tf.reshape(input_ids, [-1])  # one rank. shape as (batch_size * sequence_length,)\n",
            "    if use_one_hot_embeddings:\n",
            "        one_hot_input_ids = tf.one_hot(flat_input_ids,depth=vocab_size)  # one_hot_input_ids=[batch_size * sequence_length,vocab_size]\n",
            "        output_middle = tf.matmul(one_hot_input_ids, embedding_table)  # output=[batch_size * sequence_length,embedding_size]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 491,
        "API": ".gather(",
        "context": [
            "    if use_one_hot_embeddings:\n",
            "        one_hot_input_ids = tf.one_hot(flat_input_ids,depth=vocab_size)  # one_hot_input_ids=[batch_size * sequence_length,vocab_size]\n",
            "        output_middle = tf.matmul(one_hot_input_ids, embedding_table)  # output=[batch_size * sequence_length,embedding_size]\n",
            "    else:\n",
            "        output_middle = tf.gather(embedding_table,flat_input_ids)  # [vocab_size, embedding_size]*[batch_size * sequence_length,]--->[batch_size * sequence_length,embedding_size]\n",
            "\n",
            "    # 2. project vector(output_middle) to the hidden space\n",
            "    project_variable = tf.get_variable(  # [embedding_size, hidden_size]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 498,
        "API": ".matmul(",
        "context": [
            "    project_variable = tf.get_variable(  # [embedding_size, hidden_size]\n",
            "        name=word_embedding_name+\"_2\",\n",
            "        shape=[embedding_size, hidden_size],\n",
            "        initializer=create_initializer(initializer_range))\n",
            "    output = tf.matmul(output_middle, project_variable) # ([batch_size * sequence_length, embedding_size] * [embedding_size, hidden_size])--->[batch_size * sequence_length, hidden_size]\n",
            "    # reshape back to 3 rank\n",
            "    input_shape = get_shape_list(input_ids)  # input_shape=[ batch_size, seq_length, 1]\n",
            "    batch_size, sequene_length, _=input_shape\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 560,
        "API": ".reshape(",
        "context": [
            "        shape=[token_type_vocab_size, width],\n",
            "        initializer=create_initializer(initializer_range))\n",
            "    # This vocab will be small so we always do one-hot here, since it is always\n",
            "    # faster for a small vocabulary.\n",
            "    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n",
            "    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)\n",
            "    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n",
            "    token_type_embeddings = tf.reshape(token_type_embeddings,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 569,
        "API": ".control_dependencies(",
        "context": [
            "    output += token_type_embeddings\n",
            "\n",
            "  if use_position_embeddings:\n",
            "    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n",
            "    with tf.control_dependencies([assert_op]):\n",
            "      full_position_embeddings = tf.get_variable(\n",
            "          name=position_embedding_name,\n",
            "          shape=[max_position_embeddings, width],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 583,
        "API": ".slice(",
        "context": [
            "      # So `full_position_embeddings` is effectively an embedding table\n",
            "      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n",
            "      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n",
            "      # perform a slice.\n",
            "      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n",
            "                                     [seq_length, -1])\n",
            "      num_dims = len(output.shape.as_list())\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 594,
        "API": ".reshape(",
        "context": [
            "      position_broadcast_shape = []\n",
            "      for _ in range(num_dims - 2):\n",
            "        position_broadcast_shape.append(1)\n",
            "      position_broadcast_shape.extend([seq_length, width])\n",
            "      position_embeddings = tf.reshape(position_embeddings,\n",
            "                                       position_broadcast_shape)\n",
            "      output += position_embeddings\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 619,
        "API": ".cast(",
        "context": [
            "\n",
            "  to_shape = get_shape_list(to_mask, expected_rank=2)\n",
            "  to_seq_length = to_shape[1]\n",
            "\n",
            "  to_mask = tf.cast(\n",
            "      tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n",
            "\n",
            "  # We don't assume that `from_tensor` is a mask (although it could be). We\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 627,
        "API": ".ones(",
        "context": [
            "  # don't actually care if we attend *from* padding tokens (only *to* padding)\n",
            "  # tokens so we create a tensor of all ones.\n",
            "  #\n",
            "  # `broadcast_ones` = [batch_size, from_seq_length, 1]\n",
            "  broadcast_ones = tf.ones(\n",
            "      shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n",
            "\n",
            "  # Here we broadcast along two dimensions to create the mask.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 709,
        "API": ".reshape(",
        "context": [
            "  \"\"\"\n",
            "\n",
            "  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n",
            "                           seq_length, width):\n",
            "    output_tensor = tf.reshape(\n",
            "        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n",
            "\n",
            "    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 779,
        "API": ".matmul(",
        "context": [
            "\n",
            "  # Take the dot product between \"query\" and \"key\" to get the raw\n",
            "  # attention scores.\n",
            "  # `attention_scores` = [B, N, F, T]\n",
            "  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n",
            "  attention_scores = tf.multiply(attention_scores,\n",
            "                                 1.0 / math.sqrt(float(size_per_head)))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 785,
        "API": ".expand_dims(",
        "context": [
            "                                 1.0 / math.sqrt(float(size_per_head)))\n",
            "\n",
            "  if attention_mask is not None:\n",
            "    # `attention_mask` = [B, 1, F, T]\n",
            "    attention_mask = tf.expand_dims(attention_mask, axis=[1])\n",
            "\n",
            "    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
            "    # masked positions, this operation will create a tensor which is 0.0 for\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 790,
        "API": ".cast(",
        "context": [
            "\n",
            "    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
            "    # masked positions, this operation will create a tensor which is 0.0 for\n",
            "    # positions we want to attend and -10000.0 for masked positions.\n",
            "    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n",
            "\n",
            "    # Since we are adding it to the raw scores before the softmax, this is\n",
            "    # effectively the same as removing these entirely.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 798,
        "API": ".softmax(",
        "context": [
            "    attention_scores += adder\n",
            "\n",
            "  # Normalize the attention scores to probabilities.\n",
            "  # `attention_probs` = [B, N, F, T]\n",
            "  attention_probs = tf.nn.softmax(attention_scores)\n",
            "\n",
            "  # This is actually dropping out entire tokens to attend to, which might\n",
            "  # seem a bit unusual, but is taken from the original Transformer paper.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 805,
        "API": ".reshape(",
        "context": [
            "  # seem a bit unusual, but is taken from the original Transformer paper.\n",
            "  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n",
            "\n",
            "  # `value_layer` = [B, T, N, H]\n",
            "  value_layer = tf.reshape(\n",
            "      value_layer,\n",
            "      [batch_size, to_seq_length, num_attention_heads, size_per_head])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 810,
        "API": ".transpose(",
        "context": [
            "      value_layer,\n",
            "      [batch_size, to_seq_length, num_attention_heads, size_per_head])\n",
            "\n",
            "  # `value_layer` = [B, N, T, H]\n",
            "  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n",
            "\n",
            "  # `context_layer` = [B, N, F, H]\n",
            "  context_layer = tf.matmul(attention_probs, value_layer)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 816,
        "API": ".transpose(",
        "context": [
            "  # `context_layer` = [B, N, F, H]\n",
            "  context_layer = tf.matmul(attention_probs, value_layer)\n",
            "\n",
            "  # `context_layer` = [B, F, N, H]\n",
            "  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n",
            "\n",
            "  if do_return_2d_tensor:\n",
            "    # `context_layer` = [B*F, N*H]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 825,
        "API": ".reshape(",
        "context": [
            "        context_layer,\n",
            "        [batch_size * from_seq_length, num_attention_heads * size_per_head])\n",
            "  else:\n",
            "    # `context_layer` = [B, F, N*H]\n",
            "    context_layer = tf.reshape(\n",
            "        context_layer,\n",
            "        [batch_size, from_seq_length, num_attention_heads * size_per_head])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 938,
        "API": ".concat(",
        "context": [
            "          attention_output = attention_heads[0]\n",
            "        else:\n",
            "          # In the case where we have other sequences, we just concatenate\n",
            "          # them to the self-attention head before the projection.\n",
            "          attention_output = tf.concat(attention_heads, axis=-1)\n",
            "\n",
            "        # Run a linear projection of `hidden_size` then add a residual\n",
            "        # with `layer_input`.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 1011,
        "API": ".shape(",
        "context": [
            "\n",
            "  if not non_static_indexes:\n",
            "    return shape\n",
            "\n",
            "  dyn_shape = tf.shape(tensor)\n",
            "  for index in non_static_indexes:\n",
            "    shape[index] = dyn_shape[index]\n",
            "  return shape\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 1027,
        "API": ".reshape(",
        "context": [
            "  if ndims == 2:\n",
            "    return input_tensor\n",
            "\n",
            "  width = input_tensor.shape[-1]\n",
            "  output_tensor = tf.reshape(input_tensor, [-1, width])\n",
            "  return output_tensor\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 1041,
        "API": ".reshape(",
        "context": [
            "\n",
            "  orig_dims = orig_shape_list[0:-1]\n",
            "  width = output_shape[-1]\n",
            "\n",
            "  return tf.reshape(output_tensor, orig_dims + [width])\n",
            "\n",
            "\n",
            "def assert_rank(tensor, expected_rank, name=None):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 1161,
        "API": ".format(",
        "context": [
            "\t\telif shared_type == 'attention':\n",
            "\t\t\ttmp = {\n",
            "\t\t\t\t\"layer\":\"layer_shared\",\n",
            "\t\t\t\t'attention':'attention',\n",
            "\t\t\t\t'intermediate':'intermediate_{}'.format(idx),\n",
            "\t\t\t\t'output':'output_{}'.format(idx)\n",
            "\t\t\t}\n",
            "\t\telif shared_type == 'ffn':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 1167,
        "API": ".format(",
        "context": [
            "\t\t\t}\n",
            "\t\telif shared_type == 'ffn':\n",
            "\t\t\ttmp = {\n",
            "\t\t\t\t\"layer\":\"layer_shared\",\n",
            "\t\t\t\t'attention':'attention_{}'.format(idx),\n",
            "\t\t\t\t'intermediate':'intermediate',\n",
            "\t\t\t\t'output':'output'\n",
            "\t\t\t}\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 1173,
        "API": ".format(",
        "context": [
            "\t\t\t\t'output':'output'\n",
            "\t\t\t}\n",
            "\t\telse:\n",
            "\t\t\ttmp = {\n",
            "\t\t\t\t\"layer\":\"layer_{}\".format(idx),\n",
            "\t\t\t\t'attention':'attention',\n",
            "\t\t\t\t'intermediate':'intermediate',\n",
            "\t\t\t\t'output':'output'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling.py",
        "line_number": 1217,
        "API": ".concat(",
        "context": [
            "\t\t\t\t\tattention_output = attention_heads[0]\n",
            "\t\t\t\telse:\n",
            "\t\t\t\t\t# In the case where we have other sequences, we just concatenate\n",
            "\t\t\t\t\t# them to the self-attention head before the projection.\n",
            "\t\t\t\t\tattention_output = tf.concat(attention_heads, axis=-1)\n",
            "\n",
            "\t\t\t\t# Run a linear projection of `hidden_size` then add a residual\n",
            "\t\t\t\t# with `layer_input`.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 130,
        "API": ".constant(",
        "context": [
            "  \"\"\"BERT model (\"Bidirectional Encoder Representations from Transformers\").\n",
            "  Example usage:\n",
            "  ```python\n",
            "  # Already been converted from strings into ids\n",
            "  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n",
            "  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n",
            "  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n",
            "  config = modeling.AlbertConfig(vocab_size=32000, hidden_size=512,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 139,
        "API": ".matmul(",
        "context": [
            "  model = modeling.AlbertModel(config=config, is_training=True,\n",
            "    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n",
            "  label_embeddings = tf.get_variable(...)\n",
            "  pooled_output = model.get_pooled_output()\n",
            "  logits = tf.matmul(pooled_output, label_embeddings)\n",
            "  ...\n",
            "  ```\n",
            "  \"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 161,
        "API": ".embedding_lookup(",
        "context": [
            "      input_ids: int32 Tensor of shape [batch_size, seq_length].\n",
            "      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
            "      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
            "      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n",
            "        embeddings or tf.embedding_lookup() for the word embeddings.\n",
            "      scope: (optional) variable scope. Defaults to \"bert\".\n",
            "    Raises:\n",
            "      ValueError: The config is invalid or one of the input tensor shapes\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 290,
        "API": ".tanh(",
        "context": [
            "    x: float Tensor to perform activation.\n",
            "  Returns:\n",
            "    `x` with the GELU activation applied.\n",
            "  \"\"\"\n",
            "  cdf = 0.5 * (1.0 + tf.tanh(\n",
            "      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
            "  return x * cdf\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 339,
        "API": ".group(",
        "context": [
            "  for var in tvars:\n",
            "    name = var.name\n",
            "    m = re.match(\"^(.*):\\\\d+$\", name)\n",
            "    if m is not None:\n",
            "      name = m.group(1)\n",
            "    name_to_variable[name] = var\n",
            "  init_vars = tf.train.list_variables(init_checkpoint)\n",
            "  init_vars_name = [name for (name, _) in init_vars]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 354,
        "API": ".sub(",
        "context": [
            "\n",
            "  for name in name_to_variable:\n",
            "    if name in init_vars_name:\n",
            "      tvar_name = name\n",
            "    elif (re.sub(r\"/group_\\d+/\", \"/group_0/\",\n",
            "                 six.ensure_str(name)) in init_vars_name and\n",
            "          num_of_group > 1):\n",
            "      tvar_name = re.sub(r\"/group_\\d+/\", \"/group_0/\", six.ensure_str(name))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 360,
        "API": ".sub(",
        "context": [
            "          num_of_group > 1):\n",
            "      tvar_name = re.sub(r\"/group_\\d+/\", \"/group_0/\", six.ensure_str(name))\n",
            "    elif (re.sub(r\"/ffn_\\d+/\", \"/ffn_1/\", six.ensure_str(name))\n",
            "          in init_vars_name and num_of_group > 1):\n",
            "      tvar_name = re.sub(r\"/ffn_\\d+/\", \"/ffn_1/\", six.ensure_str(name))\n",
            "    elif (re.sub(r\"/attention_\\d+/\", \"/attention_1/\", six.ensure_str(name))\n",
            "          in init_vars_name and num_of_group > 1):\n",
            "      tvar_name = re.sub(r\"/attention_\\d+/\", \"/attention_1/\",\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 366,
        "API": ".info(",
        "context": [
            "          in init_vars_name and num_of_group > 1):\n",
            "      tvar_name = re.sub(r\"/attention_\\d+/\", \"/attention_1/\",\n",
            "                         six.ensure_str(name))\n",
            "    else:\n",
            "      tf.logging.info(\"name %s does not get matched\", name)\n",
            "      continue\n",
            "    tf.logging.info(\"name %s match to %s\", name, tvar_name)\n",
            "    if num_of_group > 0:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 376,
        "API": ".info(",
        "context": [
            "        if ((\"/group_\" + str(gid) + \"/\" in name) or\n",
            "            (\"/ffn_\" + str(gid) + \"/\" in name) or\n",
            "            (\"/attention_\" + str(gid) + \"/\" in name)):\n",
            "          group_matched = True\n",
            "          tf.logging.info(\"%s belongs to %dth\", name, gid)\n",
            "          assignment_map[gid][tvar_name] = name\n",
            "      if not group_matched:\n",
            "        assignment_map[0][tvar_name] = name\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 400,
        "API": ".dropout(",
        "context": [
            "  \"\"\"\n",
            "  if dropout_prob is None or dropout_prob == 0.0:\n",
            "    return input_tensor\n",
            "\n",
            "  output = tf.nn.dropout(input_tensor, rate=dropout_prob)\n",
            "  return output\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 440,
        "API": ".log(",
        "context": [
            "    a Tensor of timing signals [batch, seq_len, channels]\n",
            "  \"\"\"\n",
            "  num_timescales = channels // 2\n",
            "  log_timescale_increment = (\n",
            "      math.log(float(max_timescale) / float(min_timescale)) /\n",
            "      (tf.to_float(num_timescales) - 1))\n",
            "  inv_timescales = min_timescale * tf.exp(\n",
            "      tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 445,
        "API": ".expand_dims(",
        "context": [
            "      (tf.to_float(num_timescales) - 1))\n",
            "  inv_timescales = min_timescale * tf.exp(\n",
            "      tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n",
            "  scaled_time = (\n",
            "      tf.expand_dims(tf.to_float(position), 2) * tf.expand_dims(\n",
            "          tf.expand_dims(inv_timescales, 0), 0))\n",
            "  signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=2)\n",
            "  signal = tf.pad(signal, [[0, 0], [0, 0], [0, tf.mod(channels, 2)]])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 467,
        "API": ".embedding_lookup(",
        "context": [
            "    embedding_size: int. Width of the word embeddings.\n",
            "    initializer_range: float. Embedding initialization range.\n",
            "    word_embedding_name: string. Name of the embedding table.\n",
            "    use_one_hot_embeddings: bool. If True, use one-hot method for word\n",
            "      embeddings. If False, use `tf.nn.embedding_lookup()`.\n",
            "  Returns:\n",
            "    float Tensor of shape [batch_size, seq_length, embedding_size].\n",
            "  \"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 477,
        "API": ".expand_dims(",
        "context": [
            "  #\n",
            "  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n",
            "  # reshape to [batch_size, seq_length, 1].\n",
            "  if input_ids.shape.ndims == 2:\n",
            "    input_ids = tf.expand_dims(input_ids, axis=[-1])\n",
            "\n",
            "  embedding_table = tf.get_variable(\n",
            "      name=word_embedding_name,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 485,
        "API": ".reshape(",
        "context": [
            "      shape=[vocab_size, embedding_size],\n",
            "      initializer=create_initializer(initializer_range))\n",
            "\n",
            "  if use_one_hot_embeddings:\n",
            "    flat_input_ids = tf.reshape(input_ids, [-1])\n",
            "    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n",
            "    output = tf.matmul(one_hot_input_ids, embedding_table)\n",
            "  else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 493,
        "API": ".reshape(",
        "context": [
            "    output = tf.nn.embedding_lookup(embedding_table, input_ids)\n",
            "\n",
            "  input_shape = get_shape_list(input_ids)\n",
            "\n",
            "  output = tf.reshape(output,\n",
            "                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n",
            "  return (output, embedding_table)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 617,
        "API": ".reshape(",
        "context": [
            "    w = tf.get_variable(\n",
            "        name=\"kernel\",\n",
            "        shape=[hidden_size, num_attention_heads * head_size],\n",
            "        initializer=initializer)\n",
            "    w = tf.reshape(w, [hidden_size, num_attention_heads, head_size])\n",
            "    b = tf.get_variable(\n",
            "        name=\"bias\",\n",
            "        shape=[num_attention_heads * head_size],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 622,
        "API": ".reshape(",
        "context": [
            "    b = tf.get_variable(\n",
            "        name=\"bias\",\n",
            "        shape=[num_attention_heads * head_size],\n",
            "        initializer=tf.zeros_initializer)\n",
            "    b = tf.reshape(b, [num_attention_heads, head_size])\n",
            "    ret = tf.einsum(\"BFH,HND->BFND\", input_tensor, w)\n",
            "    ret += b\n",
            "  if activation is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 657,
        "API": ".reshape(",
        "context": [
            "    w = tf.get_variable(\n",
            "        name=\"kernel\",\n",
            "        shape=[num_attention_heads * head_size, hidden_size],\n",
            "        initializer=initializer)\n",
            "    w = tf.reshape(w, [num_attention_heads, head_size, hidden_size])\n",
            "    b = tf.get_variable(\n",
            "        name=\"bias\", shape=[hidden_size], initializer=tf.zeros_initializer)\n",
            "    ret = tf.einsum(\"BFND,NDH->BFH\", input_tensor, w)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 695,
        "API": ".einsum(",
        "context": [
            "        shape=[hidden_size, output_size],\n",
            "        initializer=initializer)\n",
            "    b = tf.get_variable(\n",
            "        name=\"bias\", shape=[output_size], initializer=tf.zeros_initializer)\n",
            "    ret = tf.einsum(\"BFH,HO->BFO\", input_tensor, w)\n",
            "    ret += b\n",
            "  if activation is not None:\n",
            "    return activation(ret)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 716,
        "API": ".matmul(",
        "context": [
            "    dropout_rate: a float.\n",
            "  Returns:\n",
            "    Tensor with shape [..., length_q, depth_v].\n",
            "  \"\"\"\n",
            "  logits = tf.matmul(q, k, transpose_b=True)  # [..., length_q, length_kv]\n",
            "  logits = tf.multiply(logits, 1.0 / math.sqrt(float(get_shape_list(q)[-1])))\n",
            "  if bias is not None:\n",
            "    # `attention_mask` = [B, T]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 722,
        "API": ".ones(",
        "context": [
            "  if bias is not None:\n",
            "    # `attention_mask` = [B, T]\n",
            "    from_shape = get_shape_list(q)\n",
            "    if len(from_shape) == 4:\n",
            "      broadcast_ones = tf.ones([from_shape[0], 1, from_shape[2], 1], tf.float32)\n",
            "    elif len(from_shape) == 5:\n",
            "      # from_shape = [B, N, Block_num, block_size, depth]#\n",
            "      broadcast_ones = tf.ones([from_shape[0], 1, from_shape[2], from_shape[3],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 728,
        "API": ".matmul(",
        "context": [
            "      # from_shape = [B, N, Block_num, block_size, depth]#\n",
            "      broadcast_ones = tf.ones([from_shape[0], 1, from_shape[2], from_shape[3],\n",
            "                                1], tf.float32)\n",
            "\n",
            "    bias = tf.matmul(broadcast_ones,\n",
            "                     tf.cast(bias, tf.float32), transpose_b=True)\n",
            "\n",
            "    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 742,
        "API": ".softmax(",
        "context": [
            "    logits += adder\n",
            "  else:\n",
            "    adder = 0.0\n",
            "\n",
            "  attention_probs = tf.nn.softmax(logits, name=\"attention_probs\")\n",
            "  attention_probs = dropout(attention_probs, dropout_rate)\n",
            "  return tf.matmul(attention_probs, v)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 823,
        "API": ".transpose(",
        "context": [
            "                     create_initializer(initializer_range), key_act, \"key\")\n",
            "  # `value_layer` = [B, T, N, H]\n",
            "  v = dense_layer_3d(to_tensor, num_attention_heads, size_per_head,\n",
            "                     create_initializer(initializer_range), value_act, \"value\")\n",
            "  q = tf.transpose(q, [0, 2, 1, 3])\n",
            "  k = tf.transpose(k, [0, 2, 1, 3])\n",
            "  v = tf.transpose(v, [0, 2, 1, 3])\n",
            "  if attention_mask is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google.py",
        "line_number": 833,
        "API": ".transpose(",
        "context": [
            "    # 'new_embeddings = [B, N, F, H]'\n",
            "  new_embeddings = dot_product_attention(q, k, v, attention_mask,\n",
            "                                         attention_probs_dropout_prob)\n",
            "\n",
            "  return tf.transpose(new_embeddings, [0, 2, 1, 3])\n",
            "\n",
            "\n",
            "def attention_ffn_block(layer_input,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google_fast.py",
        "line_number": 326,
        "API": ".sigmoid(",
        "context": [
            "    return gelu\n",
            "  elif act == \"tanh\":\n",
            "    return tf.tanh\n",
            "  elif act == \"swish\":\n",
            "    return lambda x: x * tf.sigmoid(x)\n",
            "  else:\n",
            "    raise ValueError(\"Unsupported activation: %s\" % act)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google_fast.py",
        "line_number": 699,
        "API": ".einsum(",
        "context": [
            "          shape=[hidden_size, output_size],\n",
            "          initializer=initializer)\n",
            "      b = tf.get_variable(\n",
            "          name=\"bias\", shape=[output_size], initializer=tf.zeros_initializer)\n",
            "      ret = tf.einsum(\"BFH,HO->BFO\", input_tensor, w)\n",
            "      ret += b\n",
            "  else:\n",
            "    assert hidden_size % num_groups == 0\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google_fast.py",
        "line_number": 711,
        "API": ".reshape(",
        "context": [
            "          shape=[hidden_size//num_groups, output_size//num_groups, num_groups],\n",
            "          initializer=initializer)\n",
            "      b = tf.get_variable(\n",
            "          name=\"bias\", shape=[output_size], initializer=tf.zeros_initializer)\n",
            "      input_tensor = tf.reshape(input_tensor, input_shape[:2] + [hidden_size//num_groups, num_groups])\n",
            "      ret = tf.einsum(\"BFHG,HOG->BFGO\", input_tensor, w)\n",
            "      ret = tf.reshape(ret, input_shape[:2] + [output_size])\n",
            "      ret += b\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google_fast.py",
        "line_number": 751,
        "API": ".einsum(",
        "context": [
            "          shape=[hidden_size, output_size],\n",
            "          initializer=initializer)\n",
            "      b = tf.get_variable(\n",
            "          name=\"bias\", shape=[output_size], initializer=tf.zeros_initializer)\n",
            "      ret = tf.einsum(\"BFH,HO->BFO\", input_tensor, w)\n",
            "      ret += b\n",
            "  else: # e.g. input_shape = [2, 512, 768] = [ batch_size,sequence_length, hidden_size]\n",
            "    assert hidden_size % num_groups == 0\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google_fast.py",
        "line_number": 767,
        "API": ".reshape(",
        "context": [
            "      b = tf.get_variable(\n",
            "          name=\"bias\", shape=[num_groups, output_size//num_groups], initializer=tf.zeros_initializer)\n",
            "      # input_tensor = [ batch_size,sequence_length, hidden_size].\n",
            "      # input_shape[:2] + [hidden_size//num_groups, num_groups] = [batch_size, sequence_length, hidden_size/num_groups, num_groups]\n",
            "      input_tensor = tf.reshape(input_tensor, input_shape[:2] + [hidden_size//num_groups, num_groups])\n",
            "      # print(\"#dense_layer_2d.2.input_shape of input_tensor:\", input_tensor.shape)\n",
            "      input_tensor = tf.transpose(input_tensor, [3, 0, 1, 2]) # [num_groups, batch_size, sequence_length, hidden_size/num_groups]\n",
            "      # print(\"#dense_layer_2d.3.input_shape of input_tensor:\", input_tensor.shape) #  input_tensor=(16, 2, 512, 192)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google_fast.py",
        "line_number": 773,
        "API": ".einsum(",
        "context": [
            "      input_tensor = tf.transpose(input_tensor, [3, 0, 1, 2]) # [num_groups, batch_size, sequence_length, hidden_size/num_groups]\n",
            "      # print(\"#dense_layer_2d.3.input_shape of input_tensor:\", input_tensor.shape) #  input_tensor=(16, 2, 512, 192)\n",
            "      # input_tensor=[num_groups, batch_size, sequence_length, hidden_size/num_groups], w=[num_groups, hidden_size/num_groups, output_size/num_groups]\n",
            "\n",
            "      ret = tf.einsum(\"GBFH,GHO->GBFO\", input_tensor, w)\n",
            "      # print(\"#dense_layer_2d.4. shape of ret:\", ret.shape) #  (16, 2, 512, 48) = [num_groups, batch_size, sequence_length ,output_size]\n",
            "      b = tf.expand_dims(b, 1)\n",
            "      b = tf.expand_dims(b, 1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\modeling_google_fast.py",
        "line_number": 779,
        "API": ".transpose(",
        "context": [
            "      b = tf.expand_dims(b, 1)\n",
            "      b = tf.expand_dims(b, 1)\n",
            "      # print(\"#dense_layer_2d.4.2.b:\",b.shape) #  (16, 1, 1, 48)\n",
            "      ret += b\n",
            "      ret = tf.transpose(ret, [1, 2, 0, 3]) #  (2, 512, 16, 48)\n",
            "      # print(\"#dense_layer_2d.5. shape of ret:\", ret.shape)\n",
            "      ret = tf.reshape(ret, input_shape[:2] + [output_size]) # [2, 512, 768]\n",
            "  if activation is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization.py",
        "line_number": 28,
        "API": ".constant(",
        "context": [
            "def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n",
            "    \"\"\"Creates an optimizer training op.\"\"\"\n",
            "    global_step = tf.train.get_or_create_global_step()\n",
            "\n",
            "    learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
            "\n",
            "    # Implements linear decay of the learning rate.\n",
            "    learning_rate = tf.train.polynomial_decay(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization.py",
        "line_number": 42,
        "API": ".cast(",
        "context": [
            "\n",
            "    # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n",
            "    # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
            "    if num_warmup_steps:\n",
            "        global_steps_int = tf.cast(global_step, tf.int32)\n",
            "        warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
            "\n",
            "        global_steps_float = tf.cast(global_steps_int, tf.float32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization.py",
        "line_number": 51,
        "API": ".cast(",
        "context": [
            "\n",
            "        warmup_percent_done = global_steps_float / warmup_steps_float\n",
            "        warmup_learning_rate = init_lr * warmup_percent_done\n",
            "\n",
            "        is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
            "        learning_rate = (\n",
            "                (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization.py",
        "line_number": 70,
        "API": ".gradients(",
        "context": [
            "    if use_tpu:\n",
            "        optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
            "\n",
            "    tvars = tf.trainable_variables()\n",
            "    grads = tf.gradients(loss, tvars)\n",
            "\n",
            "    # This is how the model was pre-trained.\n",
            "    (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization.py",
        "line_number": 82,
        "API": ".group(",
        "context": [
            "    # Normally the global step update is done inside of `apply_gradients`.\n",
            "    # However, `AdamWeightDecayOptimizer` doesn't do this. But if you use\n",
            "    # a different optimizer, you should probably take this line out.\n",
            "    new_global_step = global_step + 1\n",
            "    train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n",
            "    return train_op\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization.py",
        "line_number": 131,
        "API": ".multiply(",
        "context": [
            "                initializer=tf.zeros_initializer())\n",
            "\n",
            "            # Standard Adam update.\n",
            "            next_m = (\n",
            "                    tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n",
            "            next_v = (\n",
            "                    tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n",
            "                                                              tf.square(grad)))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization.py",
        "line_number": 136,
        "API": ".sqrt(",
        "context": [
            "            next_v = (\n",
            "                    tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n",
            "                                                              tf.square(grad)))\n",
            "\n",
            "            update = next_m / (tf.sqrt(next_v) + self.epsilon)\n",
            "\n",
            "            # Just adding the square of the weights to the loss function is *not*\n",
            "            # the correct way of using L2 regularization/weight decay with Adam,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization.py",
        "line_number": 153,
        "API": ".assign(",
        "context": [
            "\n",
            "            next_param = param - update_with_lr\n",
            "\n",
            "            assignments.extend(\n",
            "                [param.assign(next_param),\n",
            "                 m.assign(next_m),\n",
            "                 v.assign(next_v)])\n",
            "        return tf.group(*assignments, name=name)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization.py",
        "line_number": 172,
        "API": ".group(",
        "context": [
            "    def _get_variable_name(self, param_name):\n",
            "        \"\"\"Get the variable name from the tensor name.\"\"\"\n",
            "        m = re.match(\"^(.*):\\\\d+$\", param_name)\n",
            "        if m is not None:\n",
            "            param_name = m.group(1)\n",
            "        return param_name\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization.py",
        "line_number": 260,
        "API": ".where(",
        "context": [
            "            # minmax:   \\phi(z) = min(max(z, \\gamma_l), \\gamma_u)\n",
            "            # identity: \\phi(z) = z\n",
            "            # The authors does not mention what is \\gamma_l and \\gamma_u\n",
            "            # UPDATE: after asking authors, they provide me the code below.\n",
            "            # ratio = array_ops.where(math_ops.greater(w_norm, 0), array_ops.where(\n",
            "            #      math_ops.greater(g_norm, 0), (w_norm / g_norm), 1.0), 1.0)\n",
            "\n",
            "            r1 = tf.sqrt(tf.reduce_sum(tf.square(param)))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization.py",
        "line_number": 266,
        "API": ".where(",
        "context": [
            "\n",
            "            r1 = tf.sqrt(tf.reduce_sum(tf.square(param)))\n",
            "            r2 = tf.sqrt(tf.reduce_sum(tf.square(update)))\n",
            "\n",
            "            r = tf.where(tf.greater(r1, 0.0),\n",
            "                         tf.where(tf.greater(r2, 0.0),\n",
            "                                  r1 / r2,\n",
            "                                  1.0),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization.py",
        "line_number": 298,
        "API": ".group(",
        "context": [
            "    def _get_variable_name(self, param_name):\n",
            "        \"\"\"Get the variable name from the tensor name.\"\"\"\n",
            "        m = re.match(\"^(.*):\\\\d+$\", param_name)\n",
            "        if m is not None:\n",
            "            param_name = m.group(1)\n",
            "        return param_name"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization_finetuning.py",
        "line_number": 28,
        "API": ".constant(",
        "context": [
            "def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n",
            "  \"\"\"Creates an optimizer training op.\"\"\"\n",
            "  global_step = tf.train.get_or_create_global_step()\n",
            "\n",
            "  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
            "\n",
            "  # Implements linear decay of the learning rate.\n",
            "  learning_rate = tf.train.polynomial_decay(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization_finetuning.py",
        "line_number": 42,
        "API": ".cast(",
        "context": [
            "\n",
            "  # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n",
            "  # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
            "  if num_warmup_steps:\n",
            "    global_steps_int = tf.cast(global_step, tf.int32)\n",
            "    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
            "\n",
            "    global_steps_float = tf.cast(global_steps_int, tf.float32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization_finetuning.py",
        "line_number": 51,
        "API": ".cast(",
        "context": [
            "\n",
            "    warmup_percent_done = global_steps_float / warmup_steps_float\n",
            "    warmup_learning_rate = init_lr * warmup_percent_done\n",
            "\n",
            "    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
            "    learning_rate = (\n",
            "        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization_finetuning.py",
        "line_number": 70,
        "API": ".gradients(",
        "context": [
            "  if use_tpu:\n",
            "    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
            "\n",
            "  tvars = tf.trainable_variables()\n",
            "  grads = tf.gradients(loss, tvars)\n",
            "\n",
            "  # This is how the model was pre-trained.\n",
            "  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization_finetuning.py",
        "line_number": 82,
        "API": ".group(",
        "context": [
            "  # Normally the global step update is done inside of `apply_gradients`.\n",
            "  # However, `AdamWeightDecayOptimizer` doesn't do this. But if you use\n",
            "  # a different optimizer, you should probably take this line out.\n",
            "  new_global_step = global_step + 1\n",
            "  train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n",
            "  return train_op\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization_finetuning.py",
        "line_number": 172,
        "API": ".group(",
        "context": [
            "  def _get_variable_name(self, param_name):\n",
            "    \"\"\"Get the variable name from the tensor name.\"\"\"\n",
            "    m = re.match(\"^(.*):\\\\d+$\", param_name)\n",
            "    if m is not None:\n",
            "      param_name = m.group(1)\n",
            "    return param_name\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization_google.py",
        "line_number": 36,
        "API": ".constant(",
        "context": [
            "                     optimizer=\"adamw\", poly_power=1.0, start_warmup_step=0):\n",
            "  \"\"\"Creates an optimizer training op.\"\"\"\n",
            "  global_step = tf.train.get_or_create_global_step()\n",
            "\n",
            "  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
            "\n",
            "  # Implements linear decay of the learning rate.\n",
            "  learning_rate = tf.train.polynomial_decay(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization_google.py",
        "line_number": 51,
        "API": ".info(",
        "context": [
            "  # Implements linear warmup. I.e., if global_step - start_warmup_step <\n",
            "  # num_warmup_steps, the learning rate will be\n",
            "  # `(global_step - start_warmup_step)/num_warmup_steps * init_lr`.\n",
            "  if num_warmup_steps:\n",
            "    tf.logging.info(\"++++++ warmup starts at step \" + str(start_warmup_step)\n",
            "                    + \", for \" + str(num_warmup_steps) + \" steps ++++++\")\n",
            "    global_steps_int = tf.cast(global_step, tf.int32)\n",
            "    start_warm_int = tf.constant(start_warmup_step, dtype=tf.int32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization_google.py",
        "line_number": 56,
        "API": ".constant(",
        "context": [
            "                    + \", for \" + str(num_warmup_steps) + \" steps ++++++\")\n",
            "    global_steps_int = tf.cast(global_step, tf.int32)\n",
            "    start_warm_int = tf.constant(start_warmup_step, dtype=tf.int32)\n",
            "    global_steps_int = global_steps_int - start_warm_int\n",
            "    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
            "\n",
            "    global_steps_float = tf.cast(global_steps_int, tf.float32)\n",
            "    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization_google.py",
        "line_number": 76,
        "API": ".info(",
        "context": [
            "  # As report in the Bert pulic github, the learning rate for SQuAD 1.1 finetune\n",
            "  # is 3e-5, 4e-5 or 5e-5. For LAMB, the users can use 3e-4, 4e-4,or 5e-4 for a\n",
            "  # batch size of 64 in the finetune.\n",
            "  if optimizer == \"adamw\":\n",
            "    tf.logging.info(\"using adamw\")\n",
            "    optimizer = AdamWeightDecayOptimizer(\n",
            "        learning_rate=learning_rate,\n",
            "        weight_decay_rate=0.01,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization_google.py",
        "line_number": 85,
        "API": ".info(",
        "context": [
            "        beta_2=0.999,\n",
            "        epsilon=1e-6,\n",
            "        exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n",
            "  elif optimizer == \"lamb\":\n",
            "    tf.logging.info(\"using lamb\")\n",
            "    optimizer = lamb_optimizer.LAMBOptimizer(\n",
            "        learning_rate=learning_rate,\n",
            "        weight_decay_rate=0.01,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\optimization_google.py",
        "line_number": 113,
        "API": ".group(",
        "context": [
            "  # However, neither `AdamWeightDecayOptimizer` nor `LAMBOptimizer` do this.\n",
            "  # But if you use a different optimizer, you should probably take this line\n",
            "  # out.\n",
            "  new_global_step = global_step + 1\n",
            "  train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n",
            "  return train_op\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 287,
        "API": ".info(",
        "context": [
            "  assert len(segment_ids) == max_seq_length\n",
            "\n",
            "  label_id = label_map[example.label]\n",
            "  if ex_index < 5:\n",
            "    tf.logging.info(\"*** Example ***\")\n",
            "    tf.logging.info(\"guid: %s\" % (example.guid))\n",
            "    tf.logging.info(\"tokens: %s\" % \" \".join(\n",
            "        [tokenization.printable_text(x) for x in tokens]))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 292,
        "API": ".join(",
        "context": [
            "    tf.logging.info(\"guid: %s\" % (example.guid))\n",
            "    tf.logging.info(\"tokens: %s\" % \" \".join(\n",
            "        [tokenization.printable_text(x) for x in tokens]))\n",
            "    tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
            "    tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
            "    tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
            "    tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 313,
        "API": ".info(",
        "context": [
            "  writer = tf.python_io.TFRecordWriter(output_file)\n",
            "\n",
            "  for (ex_index, example) in enumerate(examples):\n",
            "    if ex_index % 10000 == 0:\n",
            "      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
            "\n",
            "    feature = convert_single_example(ex_index, example, label_list,\n",
            "                                     max_seq_length, tokenizer)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 331,
        "API": ".write(",
        "context": [
            "    features[\"is_real_example\"] = create_int_feature(\n",
            "        [int(feature.is_real_example)])\n",
            "\n",
            "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
            "    writer.write(tf_example.SerializeToString())\n",
            "  writer.close()\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 369,
        "API": ".repeat(",
        "context": [
            "    # For training, we want a lot of parallel reading and shuffling.\n",
            "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
            "    d = tf.data.TFRecordDataset(input_file)\n",
            "    if is_training:\n",
            "      d = d.repeat()\n",
            "      d = d.shuffle(buffer_size=100)\n",
            "\n",
            "    d = d.apply(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 437,
        "API": ".dropout(",
        "context": [
            "        print(\"ln_type is postln or other,do nothing.\")\n",
            "\n",
            "    if is_training:\n",
            "      # I.e., 0.1 dropout\n",
            "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
            "\n",
            "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
            "    logits = tf.nn.bias_add(logits, output_bias)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 442,
        "API": ".log_softmax(",
        "context": [
            "\n",
            "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
            "    logits = tf.nn.bias_add(logits, output_bias)\n",
            "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
            "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
            "\n",
            "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 448,
        "API": ".info(",
        "context": [
            "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
            "\n",
            "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1) # todo 08-29 try temp-loss\n",
            "    ###############bi_tempered_logistic_loss############################################################################\n",
            "    # print(\"##cross entropy loss is used....\"); tf.logging.info(\"##cross entropy loss is used....\")\n",
            "    # t1=0.9 #t1=0.90\n",
            "    # t2=1.05 #t2=1.05\n",
            "    # per_example_loss=bi_tempered_logistic_loss(log_probs,one_hot_labels,t1,t2,label_smoothing=0.1,num_iters=5) # TODO label_smoothing=0.0\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 455,
        "API": ".reduce_mean(",
        "context": [
            "    # per_example_loss=bi_tempered_logistic_loss(log_probs,one_hot_labels,t1,t2,label_smoothing=0.1,num_iters=5) # TODO label_smoothing=0.0\n",
            "    #tf.logging.info(\"per_example_loss:\"+str(per_example_loss.shape))\n",
            "    ##############bi_tempered_logistic_loss#############################################################################\n",
            "\n",
            "    loss = tf.reduce_mean(per_example_loss)\n",
            "\n",
            "    return (loss, per_example_loss, logits, probabilities)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 472,
        "API": ".info(",
        "context": [
            "\n",
            "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
            "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
            "\n",
            "    tf.logging.info(\"*** Features ***\")\n",
            "    for name in sorted(features.keys()):\n",
            "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 482,
        "API": ".cast(",
        "context": [
            "    segment_ids = features[\"segment_ids\"]\n",
            "    label_ids = features[\"label_ids\"]\n",
            "    is_real_example = None\n",
            "    if \"is_real_example\" in features:\n",
            "      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
            "    else:\n",
            "      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 508,
        "API": ".info(",
        "context": [
            "        scaffold_fn = tpu_scaffold\n",
            "      else:\n",
            "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
            "\n",
            "    tf.logging.info(\"**** Trainable Variables ****\")\n",
            "    for var in tvars:\n",
            "      init_string = \"\"\n",
            "      if var.name in initialized_variable_names:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 513,
        "API": ".info(",
        "context": [
            "    for var in tvars:\n",
            "      init_string = \"\"\n",
            "      if var.name in initialized_variable_names:\n",
            "        init_string = \", *INIT_FROM_CKPT*\"\n",
            "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
            "                      init_string)\n",
            "\n",
            "    output_spec = None\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 530,
        "API": ".argmax(",
        "context": [
            "          scaffold_fn=scaffold_fn)\n",
            "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
            "\n",
            "      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
            "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
            "        accuracy = tf.metrics.accuracy(\n",
            "            labels=label_ids, predictions=predictions, weights=is_real_example)\n",
            "        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 583,
        "API": ".constant(",
        "context": [
            "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
            "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
            "    d = tf.data.Dataset.from_tensor_slices({\n",
            "        \"input_ids\":\n",
            "            tf.constant(\n",
            "                all_input_ids, shape=[num_examples, seq_length],\n",
            "                dtype=tf.int32),\n",
            "        \"input_mask\":\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 592,
        "API": ".constant(",
        "context": [
            "                all_input_mask,\n",
            "                shape=[num_examples, seq_length],\n",
            "                dtype=tf.int32),\n",
            "        \"segment_ids\":\n",
            "            tf.constant(\n",
            "                all_segment_ids,\n",
            "                shape=[num_examples, seq_length],\n",
            "                dtype=tf.int32),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 597,
        "API": ".constant(",
        "context": [
            "                all_segment_ids,\n",
            "                shape=[num_examples, seq_length],\n",
            "                dtype=tf.int32),\n",
            "        \"label_ids\":\n",
            "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
            "    })\n",
            "\n",
            "    if is_training:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 604,
        "API": ".batch(",
        "context": [
            "    if is_training:\n",
            "      d = d.repeat()\n",
            "      d = d.shuffle(buffer_size=100)\n",
            "\n",
            "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
            "    return d\n",
            "\n",
            "  return input_fn\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 617,
        "API": ".join(",
        "context": [
            "\n",
            "  def get_train_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    return self._create_examples(\n",
            "        self._read_tsv(os.path.join(data_dir, \"train.txt\")), \"train\")\n",
            "    # dev_0827.tsv\n",
            "\n",
            "  def get_dev_examples(self, data_dir):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 623,
        "API": ".join(",
        "context": [
            "\n",
            "  def get_dev_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    return self._create_examples(\n",
            "        self._read_tsv(os.path.join(data_dir, \"dev.txt\")), \"dev\")\n",
            "\n",
            "  def get_test_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 628,
        "API": ".join(",
        "context": [
            "\n",
            "  def get_test_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    return self._create_examples(\n",
            "        self._read_tsv(os.path.join(data_dir, \"test.txt\")), \"test\")\n",
            "\n",
            "  def get_labels(self):\n",
            "    \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 662,
        "API": ".join(",
        "context": [
            "\n",
            "  def get_train_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    return self._create_examples(\n",
            "        self._read_tsv(os.path.join(data_dir, \"train_0827.tsv\")), \"train\")\n",
            "    # dev_0827.tsv\n",
            "\n",
            "  def get_dev_examples(self, data_dir):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 668,
        "API": ".join(",
        "context": [
            "\n",
            "  def get_dev_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    return self._create_examples(\n",
            "        self._read_tsv(os.path.join(data_dir, \"dev_0827.tsv\")), \"dev\")\n",
            "\n",
            "  def get_test_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 673,
        "API": ".join(",
        "context": [
            "\n",
            "  def get_test_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    return self._create_examples(\n",
            "        self._read_tsv(os.path.join(data_dir, \"test_0827.tsv\")), \"test\")\n",
            "\n",
            "  def get_labels(self):\n",
            "    \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 708,
        "API": ".info(",
        "context": [
            "\n",
            "  features = []\n",
            "  for (ex_index, example) in enumerate(examples):\n",
            "    if ex_index % 10000 == 0:\n",
            "      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
            "\n",
            "    feature = convert_single_example(ex_index, example, label_list,\n",
            "                                     max_seq_length, tokenizer)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 718,
        "API": ".set_verbosity(",
        "context": [
            "  return features\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  tf.logging.set_verbosity(tf.logging.INFO)\n",
            "\n",
            "  processors = {\n",
            "      \"sentence_pair\": SentencePairClassificationProcessor,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 744,
        "API": ".lower(",
        "context": [
            "        (FLAGS.max_seq_length, bert_config.max_position_embeddings))\n",
            "\n",
            "  tf.gfile.MakeDirs(FLAGS.output_dir)\n",
            "\n",
            "  task_name = FLAGS.task_name.lower()\n",
            "\n",
            "  if task_name not in processors:\n",
            "    raise ValueError(\"Task not found: %s\" % (task_name))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 804,
        "API": ".join(",
        "context": [
            "      eval_batch_size=FLAGS.eval_batch_size,\n",
            "      predict_batch_size=FLAGS.predict_batch_size)\n",
            "\n",
            "  if FLAGS.do_train:\n",
            "    train_file = os.path.join(FLAGS.output_dir, \"train.tf_record\")\n",
            "    train_file_exists=os.path.exists(train_file)\n",
            "    print(\"###train_file_exists:\", train_file_exists,\" ;train_file:\",train_file)\n",
            "    if not train_file_exists: # if tf_record file not exist, convert from raw text file. # TODO\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 809,
        "API": ".info(",
        "context": [
            "    train_file_exists=os.path.exists(train_file)\n",
            "    print(\"###train_file_exists:\", train_file_exists,\" ;train_file:\",train_file)\n",
            "    if not train_file_exists: # if tf_record file not exist, convert from raw text file. # TODO\n",
            "        file_based_convert_examples_to_features(train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)\n",
            "    tf.logging.info(\"***** Running training *****\")\n",
            "    tf.logging.info(\"  Num examples = %d\", len(train_examples))\n",
            "    tf.logging.info(\"  Batch size = %d\", FLAGS.train_batch_size)\n",
            "    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 832,
        "API": ".join(",
        "context": [
            "      # support a per-instance weight, and these get a weight of 0.0).\n",
            "      while len(eval_examples) % FLAGS.eval_batch_size != 0:\n",
            "        eval_examples.append(PaddingInputExample())\n",
            "\n",
            "    eval_file = os.path.join(FLAGS.output_dir, \"eval.tf_record\")\n",
            "    file_based_convert_examples_to_features(\n",
            "        eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 837,
        "API": ".info(",
        "context": [
            "    file_based_convert_examples_to_features(\n",
            "        eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file)\n",
            "\n",
            "    tf.logging.info(\"***** Running evaluation *****\")\n",
            "    tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
            "                    len(eval_examples), num_actual_eval_examples,\n",
            "                    len(eval_examples) - num_actual_eval_examples)\n",
            "    tf.logging.info(\"  Batch size = %d\", FLAGS.eval_batch_size)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 864,
        "API": ".join(",
        "context": [
            "    filenames = tf.gfile.ListDirectory(FLAGS.output_dir)\n",
            "    for filename in filenames:\n",
            "        if filename.endswith(\".index\"):\n",
            "            ckpt_name = filename[:-6]\n",
            "            cur_filename = os.path.join(FLAGS.output_dir, ckpt_name)\n",
            "            global_step = int(cur_filename.split(\"-\")[-1])\n",
            "            tf.logging.info(\"Add {} to eval list.\".format(cur_filename))\n",
            "            steps_and_files.append([global_step, cur_filename])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 870,
        "API": ".join(",
        "context": [
            "            tf.logging.info(\"Add {} to eval list.\".format(cur_filename))\n",
            "            steps_and_files.append([global_step, cur_filename])\n",
            "    steps_and_files = sorted(steps_and_files, key=lambda x: x[0])\n",
            "\n",
            "    output_eval_file = os.path.join(FLAGS.data_dir, \"eval_results_albert_zh.txt\")\n",
            "    print(\"output_eval_file:\",output_eval_file)\n",
            "    tf.logging.info(\"output_eval_file:\"+output_eval_file)\n",
            "    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 877,
        "API": ".info(",
        "context": [
            "    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
            "        for global_step, filename in sorted(steps_and_files, key=lambda x: x[0]):\n",
            "            result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps, checkpoint_path=filename)\n",
            "\n",
            "            tf.logging.info(\"***** Eval results %s *****\" % (filename))\n",
            "            writer.write(\"***** Eval results %s *****\\n\" % (filename))\n",
            "            for key in sorted(result.keys()):\n",
            "                tf.logging.info(\"  %s = %s\", key, str(result[key]))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 886,
        "API": ".join(",
        "context": [
            "    #######################################################################################################################\n",
            "\n",
            "    #result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
            "    #\n",
            "    #output_eval_file = os.path.join(FLAGS.output_dir, \"eval_results.txt\")\n",
            "    #with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
            "    #  tf.logging.info(\"***** Eval results *****\")\n",
            "    #  for key in sorted(result.keys()):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 891,
        "API": ".write(",
        "context": [
            "    #with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
            "    #  tf.logging.info(\"***** Eval results *****\")\n",
            "    #  for key in sorted(result.keys()):\n",
            "    #    tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
            "    #    writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
            "\n",
            "  if FLAGS.do_predict:\n",
            "    predict_examples = processor.get_test_examples(FLAGS.data_dir)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 904,
        "API": ".join(",
        "context": [
            "      # later on.\n",
            "      while len(predict_examples) % FLAGS.predict_batch_size != 0:\n",
            "        predict_examples.append(PaddingInputExample())\n",
            "\n",
            "    predict_file = os.path.join(FLAGS.output_dir, \"predict.tf_record\")\n",
            "    file_based_convert_examples_to_features(predict_examples, label_list,\n",
            "                                            FLAGS.max_seq_length, tokenizer,\n",
            "                                            predict_file)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 909,
        "API": ".info(",
        "context": [
            "    file_based_convert_examples_to_features(predict_examples, label_list,\n",
            "                                            FLAGS.max_seq_length, tokenizer,\n",
            "                                            predict_file)\n",
            "\n",
            "    tf.logging.info(\"***** Running prediction*****\")\n",
            "    tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
            "                    len(predict_examples), num_actual_predict_examples,\n",
            "                    len(predict_examples) - num_actual_predict_examples)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 924,
        "API": ".join(",
        "context": [
            "        drop_remainder=predict_drop_remainder)\n",
            "\n",
            "    result = estimator.predict(input_fn=predict_input_fn)\n",
            "\n",
            "    output_predict_file = os.path.join(FLAGS.output_dir, \"test_results.tsv\")\n",
            "    with tf.gfile.GFile(output_predict_file, \"w\") as writer:\n",
            "      num_written_lines = 0\n",
            "      tf.logging.info(\"***** Predict results *****\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier.py",
        "line_number": 932,
        "API": ".join(",
        "context": [
            "      for (i, prediction) in enumerate(result):\n",
            "        probabilities = prediction[\"probabilities\"]\n",
            "        if i >= num_actual_predict_examples:\n",
            "          break\n",
            "        output_line = \"\\t\".join(\n",
            "            str(class_probability)\n",
            "            for class_probability in probabilities) + \"\\n\"\n",
            "        writer.write(output_line)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 279,
        "API": ".info(",
        "context": [
            "  writer = tf.python_io.TFRecordWriter(output_file)\n",
            "  num_example = 0\n",
            "  for (ex_index, example) in enumerate(examples):\n",
            "    if ex_index % 1000 == 0:\n",
            "      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
            "\n",
            "    feature_list = convert_example_list_for_inews(ex_index, example, label_list,\n",
            "                                                  max_seq_length, tokenizer)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 299,
        "API": ".write(",
        "context": [
            "      features[\"is_real_example\"] = create_int_feature(\n",
            "          [int(feature.is_real_example)])\n",
            "\n",
            "      tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
            "      writer.write(tf_example.SerializeToString())\n",
            "  tf.logging.info(\"feature num: %s\", num_example)\n",
            "  writer.close()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 538,
        "API": ".dropout(",
        "context": [
            "      print(\"ln_type is postln or other,do nothing.\")\n",
            "\n",
            "    if is_training:\n",
            "      # I.e., 0.1 dropout\n",
            "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
            "\n",
            "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
            "    logits = tf.nn.bias_add(logits, output_bias)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 550,
        "API": ".info(",
        "context": [
            "\n",
            "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs,\n",
            "                                      axis=-1)  # todo 08-29 try temp-loss\n",
            "    ###############bi_tempered_logistic_loss############################################################################\n",
            "    # print(\"##cross entropy loss is used....\"); tf.logging.info(\"##cross entropy loss is used....\")\n",
            "    # t1=0.9 #t1=0.90\n",
            "    # t2=1.05 #t2=1.05\n",
            "    # per_example_loss=bi_tempered_logistic_loss(log_probs,one_hot_labels,t1,t2,label_smoothing=0.1,num_iters=5) # TODO label_smoothing=0.0\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 557,
        "API": ".reduce_mean(",
        "context": [
            "    # per_example_loss=bi_tempered_logistic_loss(log_probs,one_hot_labels,t1,t2,label_smoothing=0.1,num_iters=5) # TODO label_smoothing=0.0\n",
            "    # tf.logging.info(\"per_example_loss:\"+str(per_example_loss.shape))\n",
            "    ##############bi_tempered_logistic_loss#############################################################################\n",
            "\n",
            "    loss = tf.reduce_mean(per_example_loss)\n",
            "\n",
            "    return (loss, per_example_loss, logits, probabilities)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 733,
        "API": ".set_verbosity(",
        "context": [
            "  return features\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  tf.logging.set_verbosity(tf.logging.INFO)\n",
            "\n",
            "  processors = {\n",
            "      \"xnli\": XnliProcessor,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 824,
        "API": ".join(",
        "context": [
            "      eval_batch_size=FLAGS.eval_batch_size,\n",
            "      predict_batch_size=FLAGS.predict_batch_size)\n",
            "\n",
            "  if FLAGS.do_train:\n",
            "    train_file = os.path.join(FLAGS.output_dir, \"train.tf_record\")\n",
            "    train_file_exists = os.path.exists(train_file)\n",
            "    print(\"###train_file_exists:\", train_file_exists, \" ;train_file:\", train_file)\n",
            "    if not train_file_exists:  # if tf_record file not exist, convert from raw text file. # TODO\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 834,
        "API": ".info(",
        "context": [
            "            train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)\n",
            "      else:\n",
            "        file_based_convert_examples_to_features(\n",
            "            train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)\n",
            "    tf.logging.info(\"***** Running training *****\")\n",
            "    tf.logging.info(\"  Num examples = %d\", len(train_examples))\n",
            "    tf.logging.info(\"  Batch size = %d\", FLAGS.train_batch_size)\n",
            "    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 858,
        "API": ".join(",
        "context": [
            "      # support a per-instance weight, and these get a weight of 0.0).\n",
            "      while len(eval_examples) % FLAGS.eval_batch_size != 0:\n",
            "        eval_examples.append(PaddingInputExample())\n",
            "\n",
            "    eval_file = os.path.join(FLAGS.output_dir, \"dev.tf_record\")\n",
            "    if task_name == \"inews\":\n",
            "      file_based_convert_examples_to_features_for_inews(\n",
            "          eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 866,
        "API": ".info(",
        "context": [
            "    else:\n",
            "      file_based_convert_examples_to_features(\n",
            "          eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file)\n",
            "\n",
            "    tf.logging.info(\"***** Running evaluation *****\")\n",
            "    tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
            "                    len(eval_examples), num_actual_eval_examples,\n",
            "                    len(eval_examples) - num_actual_eval_examples)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 894,
        "API": ".join(",
        "context": [
            "    filenames = tf.gfile.ListDirectory(FLAGS.output_dir)\n",
            "    for filename in filenames:\n",
            "      if filename.endswith(\".index\"):\n",
            "        ckpt_name = filename[:-6]\n",
            "        cur_filename = os.path.join(FLAGS.output_dir, ckpt_name)\n",
            "        global_step = int(cur_filename.split(\"-\")[-1])\n",
            "        tf.logging.info(\"Add {} to eval list.\".format(cur_filename))\n",
            "        steps_and_files.append([global_step, cur_filename])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 900,
        "API": ".join(",
        "context": [
            "        tf.logging.info(\"Add {} to eval list.\".format(cur_filename))\n",
            "        steps_and_files.append([global_step, cur_filename])\n",
            "    steps_and_files = sorted(steps_and_files, key=lambda x: x[0])\n",
            "\n",
            "    output_eval_file = os.path.join(FLAGS.data_dir, \"dev_results_albert_zh.txt\")\n",
            "    print(\"output_eval_file:\", output_eval_file)\n",
            "    tf.logging.info(\"output_eval_file:\" + output_eval_file)\n",
            "    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 908,
        "API": ".info(",
        "context": [
            "      for global_step, filename in sorted(steps_and_files, key=lambda x: x[0]):\n",
            "        result = estimator.evaluate(input_fn=eval_input_fn,\n",
            "                                    steps=eval_steps, checkpoint_path=filename)\n",
            "\n",
            "        tf.logging.info(\"***** Eval results %s *****\" % (filename))\n",
            "        writer.write(\"***** Eval results %s *****\\n\" % (filename))\n",
            "        for key in sorted(result.keys()):\n",
            "          tf.logging.info(\"  %s = %s\", key, str(result[key]))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 917,
        "API": ".join(",
        "context": [
            "    #######################################################################################################################\n",
            "\n",
            "    # result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
            "    #\n",
            "    # output_eval_file = os.path.join(FLAGS.output_dir, \"dev_results_albert_zh.txt\")\n",
            "    # with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
            "    #  tf.logging.info(\"***** Eval results *****\")\n",
            "    #  for key in sorted(result.keys()):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 922,
        "API": ".write(",
        "context": [
            "    # with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
            "    #  tf.logging.info(\"***** Eval results *****\")\n",
            "    #  for key in sorted(result.keys()):\n",
            "    #    tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
            "    #    writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
            "\n",
            "  if FLAGS.do_predict:\n",
            "    predict_examples = processor.get_test_examples(FLAGS.data_dir)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 935,
        "API": ".join(",
        "context": [
            "      # later on.\n",
            "      while len(predict_examples) % FLAGS.predict_batch_size != 0:\n",
            "        predict_examples.append(PaddingInputExample())\n",
            "\n",
            "    predict_file = os.path.join(FLAGS.output_dir, \"predict.tf_record\")\n",
            "    if task_name == \"inews\":\n",
            "      file_based_convert_examples_to_features_for_inews(predict_examples, label_list,\n",
            "                                                        FLAGS.max_seq_length, tokenizer,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 945,
        "API": ".info(",
        "context": [
            "      file_based_convert_examples_to_features(predict_examples, label_list,\n",
            "                                              FLAGS.max_seq_length, tokenizer,\n",
            "                                              predict_file)\n",
            "\n",
            "    tf.logging.info(\"***** Running prediction*****\")\n",
            "    tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
            "                    len(predict_examples), num_actual_predict_examples,\n",
            "                    len(predict_examples) - num_actual_predict_examples)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 963,
        "API": ".join(",
        "context": [
            "    index2label_map = {}\n",
            "    for (i, label) in enumerate(label_list):\n",
            "      index2label_map[i] = label\n",
            "    output_predict_file_label_name = task_name + \"_predict.json\"\n",
            "    output_predict_file_label = os.path.join(FLAGS.output_dir, output_predict_file_label_name)\n",
            "    output_predict_file = os.path.join(FLAGS.output_dir, \"test_results.tsv\")\n",
            "    with tf.gfile.GFile(output_predict_file_label, \"w\") as writer_label:\n",
            "      with tf.gfile.GFile(output_predict_file, \"w\") as writer:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 968,
        "API": ".info(",
        "context": [
            "    output_predict_file = os.path.join(FLAGS.output_dir, \"test_results.tsv\")\n",
            "    with tf.gfile.GFile(output_predict_file_label, \"w\") as writer_label:\n",
            "      with tf.gfile.GFile(output_predict_file, \"w\") as writer:\n",
            "        num_written_lines = 0\n",
            "        tf.logging.info(\"***** Predict results *****\")\n",
            "        for (i, prediction) in enumerate(result):\n",
            "          probabilities = prediction[\"probabilities\"]\n",
            "          label_index = probabilities.argmax(0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 974,
        "API": ".join(",
        "context": [
            "          probabilities = prediction[\"probabilities\"]\n",
            "          label_index = probabilities.argmax(0)\n",
            "          if i >= num_actual_predict_examples:\n",
            "            break\n",
            "          output_line = \"\\t\".join(\n",
            "              str(class_probability)\n",
            "              for class_probability in probabilities) + \"\\n\"\n",
            "          test_label_dict = {}\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_clue.py",
        "line_number": 982,
        "API": ".write(",
        "context": [
            "          test_label_dict[\"id\"] = i\n",
            "          test_label_dict[\"label\"] = str(index2label_map[label_index])\n",
            "          if task_name == \"tnews\":\n",
            "            test_label_dict[\"label_desc\"] = \"\"\n",
            "          writer.write(output_line)\n",
            "          json.dump(test_label_dict, writer_label)\n",
            "          writer_label.write(\"\\n\")\n",
            "          num_written_lines += 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 224,
        "API": ".join(",
        "context": [
            "\n",
            "  def get_train_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    lines = self._read_tsv(\n",
            "        os.path.join(data_dir, \"multinli\",\n",
            "                     \"multinli.train.%s.tsv\" % self.language))\n",
            "    examples = []\n",
            "    for (i, line) in enumerate(lines):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 242,
        "API": ".join(",
        "context": [
            "    return examples\n",
            "\n",
            "  def get_dev_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    lines = self._read_tsv(os.path.join(data_dir, \"xnli.dev.tsv\"))\n",
            "    examples = []\n",
            "    for (i, line) in enumerate(lines):\n",
            "      if i == 0:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 269,
        "API": ".join(",
        "context": [
            "\n",
            "  def get_train_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    return self._create_examples(\n",
            "        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
            "\n",
            "  def get_dev_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 274,
        "API": ".join(",
        "context": [
            "\n",
            "  def get_dev_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    return self._create_examples(\n",
            "        self._read_tsv(os.path.join(data_dir, \"dev_matched.tsv\")),\n",
            "        \"dev_matched\")\n",
            "\n",
            "  def get_test_examples(self, data_dir):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 280,
        "API": ".join(",
        "context": [
            "\n",
            "  def get_test_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    return self._create_examples(\n",
            "        self._read_tsv(os.path.join(data_dir, \"test_matched.tsv\")), \"test\")\n",
            "\n",
            "  def get_labels(self):\n",
            "    \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 319,
        "API": ".join(",
        "context": [
            "\n",
            "  def get_dev_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    return self._create_examples(\n",
            "        self._read_tsv(os.path.join(data_dir, \"test.txt\")), \"dev\")\n",
            "\n",
            "  def get_test_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 359,
        "API": ".join(",
        "context": [
            "\n",
            "  def get_dev_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    return self._create_examples(\n",
            "        self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
            "\n",
            "  def get_test_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 364,
        "API": ".join(",
        "context": [
            "\n",
            "  def get_test_examples(self, data_dir):\n",
            "    \"\"\"See base class.\"\"\"\n",
            "    return self._create_examples(\n",
            "        self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
            "\n",
            "  def get_labels(self):\n",
            "    \"\"\"See base class.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 647,
        "API": ".info(",
        "context": [
            "  #\n",
            "  # If you want to use the token-level output, use model.get_sequence_output()\n",
            "  # instead.\n",
            "  if FLAGS.use_pooled_output:\n",
            "    tf.logging.info(\"using pooled output\")\n",
            "    output_layer = model.get_pooled_output()\n",
            "  else:\n",
            "    tf.logging.info(\"using meaned output\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 665,
        "API": ".dropout(",
        "context": [
            "\n",
            "  with tf.variable_scope(\"loss\"):\n",
            "    if is_training:\n",
            "      # I.e., 0.1 dropout\n",
            "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
            "\n",
            "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
            "    logits = tf.nn.bias_add(logits, output_bias)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 670,
        "API": ".softmax(",
        "context": [
            "\n",
            "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
            "    logits = tf.nn.bias_add(logits, output_bias)\n",
            "    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
            "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
            "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
            "\n",
            "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 675,
        "API": ".reduce_sum(",
        "context": [
            "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
            "\n",
            "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
            "\n",
            "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
            "    loss = tf.reduce_mean(per_example_loss)\n",
            "\n",
            "    return (loss, per_example_loss, probabilities, predictions)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 749,
        "API": ".mean(",
        "context": [
            "\n",
            "      def metric_fn(per_example_loss, label_ids, predictions, is_real_example):\n",
            "        accuracy = tf.metrics.accuracy(\n",
            "            labels=label_ids, predictions=predictions, weights=is_real_example)\n",
            "        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
            "        return {\n",
            "            \"eval_accuracy\": accuracy,\n",
            "            \"eval_loss\": loss,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 847,
        "API": ".set_verbosity(",
        "context": [
            "  return features\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  tf.logging.set_verbosity(tf.logging.INFO)\n",
            "\n",
            "  processors = {\n",
            "      \"cola\": ColaProcessor,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 875,
        "API": ".lower(",
        "context": [
            "        (FLAGS.max_seq_length, albert_config.max_position_embeddings))\n",
            "\n",
            "  tf.gfile.MakeDirs(FLAGS.output_dir)\n",
            "\n",
            "  task_name = FLAGS.task_name.lower()\n",
            "\n",
            "  if task_name not in processors:\n",
            "    raise ValueError(\"Task not found: %s\" % (task_name))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 934,
        "API": ".join(",
        "context": [
            "      eval_batch_size=FLAGS.eval_batch_size,\n",
            "      predict_batch_size=FLAGS.predict_batch_size)\n",
            "\n",
            "  if FLAGS.do_train:\n",
            "    train_file = os.path.join(FLAGS.output_dir, \"train.tf_record\")\n",
            "    file_based_convert_examples_to_features(\n",
            "        train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)\n",
            "    tf.logging.info(\"***** Running training *****\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 939,
        "API": ".info(",
        "context": [
            "    file_based_convert_examples_to_features(\n",
            "        train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)\n",
            "    tf.logging.info(\"***** Running training *****\")\n",
            "    tf.logging.info(\"  Num examples = %d\", len(train_examples))\n",
            "    tf.logging.info(\"  Batch size = %d\", FLAGS.train_batch_size)\n",
            "    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
            "    train_input_fn = file_based_input_fn_builder(\n",
            "        input_file=train_file,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 1012,
        "API": ".join(",
        "context": [
            "                tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
            "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
            "    #######################################################################################################################\n",
            "    # result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
            "    # output_eval_file = os.path.join(FLAGS.output_dir, \"eval_results.txt\")\n",
            "    # with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
            "    #  tf.logging.info(\"***** Eval results *****\")\n",
            "    #  for key in sorted(result.keys()):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 1050,
        "API": ".join(",
        "context": [
            "        drop_remainder=predict_drop_remainder)\n",
            "\n",
            "    result = estimator.predict(input_fn=predict_input_fn)\n",
            "\n",
            "    output_predict_file = os.path.join(FLAGS.output_dir, \"test_results.tsv\")\n",
            "    output_submit_file = os.path.join(FLAGS.output_dir, \"submit_results.tsv\")\n",
            "    with tf.gfile.GFile(output_predict_file, \"w\") as pred_writer,\\\n",
            "        tf.gfile.GFile(output_submit_file, \"w\") as sub_writer:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 1055,
        "API": ".info(",
        "context": [
            "    output_submit_file = os.path.join(FLAGS.output_dir, \"submit_results.tsv\")\n",
            "    with tf.gfile.GFile(output_predict_file, \"w\") as pred_writer,\\\n",
            "        tf.gfile.GFile(output_submit_file, \"w\") as sub_writer:\n",
            "      num_written_lines = 0\n",
            "      tf.logging.info(\"***** Predict results *****\")\n",
            "      for (i, (example, prediction)) in\\\n",
            "          enumerate(zip(predict_examples, result)):\n",
            "        probabilities = prediction[\"probabilities\"]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 1061,
        "API": ".join(",
        "context": [
            "          enumerate(zip(predict_examples, result)):\n",
            "        probabilities = prediction[\"probabilities\"]\n",
            "        if i >= num_actual_predict_examples:\n",
            "          break\n",
            "        output_line = \"\\t\".join(\n",
            "            str(class_probability)\n",
            "            for class_probability in probabilities) + \"\\n\"\n",
            "        pred_writer.write(output_line)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_classifier_sp_google.py",
        "line_number": 1067,
        "API": ".write(",
        "context": [
            "            for class_probability in probabilities) + \"\\n\"\n",
            "        pred_writer.write(output_line)\n",
            "\n",
            "        actual_label = label_list[int(prediction[\"predictions\"])]\n",
            "        sub_writer.write(\n",
            "            six.ensure_str(example.guid) + \"\\t\" + actual_label + \"\\n\")\n",
            "        num_written_lines += 1\n",
            "    assert num_written_lines == num_actual_predict_examples\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 191,
        "API": ".reshape(",
        "context": [
            "      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n",
            "                    masked_lm_weights, next_sentence_example_loss,\n",
            "                    next_sentence_log_probs, next_sentence_labels):\n",
            "        \"\"\"Computes the loss and accuracy of the model.\"\"\"\n",
            "        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,[-1, masked_lm_log_probs.shape[-1]])\n",
            "        masked_lm_predictions = tf.argmax(masked_lm_log_probs, axis=-1, output_type=tf.int32)\n",
            "        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n",
            "        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 200,
        "API": ".mean(",
        "context": [
            "        masked_lm_accuracy = tf.metrics.accuracy(\n",
            "            labels=masked_lm_ids,\n",
            "            predictions=masked_lm_predictions,\n",
            "            weights=masked_lm_weights)\n",
            "        masked_lm_mean_loss = tf.metrics.mean(\n",
            "            values=masked_lm_example_loss, weights=masked_lm_weights)\n",
            "\n",
            "        next_sentence_log_probs = tf.reshape(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 205,
        "API": ".argmax(",
        "context": [
            "            values=masked_lm_example_loss, weights=masked_lm_weights)\n",
            "\n",
            "        next_sentence_log_probs = tf.reshape(\n",
            "            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n",
            "        next_sentence_predictions = tf.argmax(\n",
            "            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n",
            "        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n",
            "        next_sentence_accuracy = tf.metrics.accuracy(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 210,
        "API": ".mean(",
        "context": [
            "            next_sentence_log_probs, axis=-1, output_type=tf.int32)\n",
            "        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n",
            "        next_sentence_accuracy = tf.metrics.accuracy(\n",
            "            labels=next_sentence_labels, predictions=next_sentence_predictions)\n",
            "        next_sentence_mean_loss = tf.metrics.mean(\n",
            "            values=next_sentence_example_loss)\n",
            "\n",
            "        return {\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 263,
        "API": ".matmul(",
        "context": [
            "    output_bias = tf.get_variable(\n",
            "        \"output_bias\",\n",
            "        shape=[bert_config.vocab_size],\n",
            "        initializer=tf.zeros_initializer())\n",
            "    # logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n",
            "    # input_tensor=[-1,hidden_size], project_weights=[embedding_size, hidden_size], project_weights_transpose=[hidden_size, embedding_size]--->[-1, embedding_size]\n",
            "    input_project = tf.matmul(input_tensor, project_weights, transpose_b=True)\n",
            "    logits = tf.matmul(input_project, output_weights, transpose_b=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 270,
        "API": ".log_softmax(",
        "context": [
            "    logits = tf.matmul(input_project, output_weights, transpose_b=True)\n",
            "    #  # input_project=[-1, embedding_size], output_weights=[vocab_size, embedding_size], output_weights_transpose=[embedding_size, vocab_size] ---> [-1, vocab_size]\n",
            "\n",
            "    logits = tf.nn.bias_add(logits, output_bias)\n",
            "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
            "\n",
            "    label_ids = tf.reshape(label_ids, [-1])\n",
            "    label_weights = tf.reshape(label_weights, [-1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 275,
        "API": ".one_hot(",
        "context": [
            "\n",
            "    label_ids = tf.reshape(label_ids, [-1])\n",
            "    label_weights = tf.reshape(label_weights, [-1])\n",
            "\n",
            "    one_hot_labels = tf.one_hot(label_ids, depth=bert_config.vocab_size, dtype=tf.float32)\n",
            "\n",
            "    # The `positions` tensor might be zero-padded (if the sequence is too\n",
            "    # short to have the maximum number of predictions). The `label_weights`\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 281,
        "API": ".reduce_sum(",
        "context": [
            "    # The `positions` tensor might be zero-padded (if the sequence is too\n",
            "    # short to have the maximum number of predictions). The `label_weights`\n",
            "    # tensor has a value of 1.0 for every real prediction and 0.0 for the\n",
            "    # padding predictions.\n",
            "    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\n",
            "    numerator = tf.reduce_sum(label_weights * per_example_loss)\n",
            "    denominator = tf.reduce_sum(label_weights) + 1e-5\n",
            "    loss = numerator / denominator\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 302,
        "API": ".matmul(",
        "context": [
            "        initializer=modeling.create_initializer(bert_config.initializer_range))\n",
            "    output_bias = tf.get_variable(\n",
            "        \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n",
            "\n",
            "    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n",
            "    logits = tf.nn.bias_add(logits, output_bias)\n",
            "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
            "    labels = tf.reshape(labels, [-1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 307,
        "API": ".reduce_sum(",
        "context": [
            "    logits = tf.nn.bias_add(logits, output_bias)\n",
            "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
            "    labels = tf.reshape(labels, [-1])\n",
            "    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n",
            "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
            "    loss = tf.reduce_mean(per_example_loss)\n",
            "    return (loss, per_example_loss, log_probs)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 319,
        "API": ".reshape(",
        "context": [
            "  batch_size = sequence_shape[0]\n",
            "  seq_length = sequence_shape[1]\n",
            "  width = sequence_shape[2]\n",
            "\n",
            "  flat_offsets = tf.reshape(\n",
            "      tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n",
            "  flat_positions = tf.reshape(positions + flat_offsets, [-1])\n",
            "  flat_sequence_tensor = tf.reshape(sequence_tensor,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 324,
        "API": ".gather(",
        "context": [
            "      tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n",
            "  flat_positions = tf.reshape(positions + flat_offsets, [-1])\n",
            "  flat_sequence_tensor = tf.reshape(sequence_tensor,\n",
            "                                    [batch_size * seq_length, width])\n",
            "  output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n",
            "  return output_tensor\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 359,
        "API": ".constant(",
        "context": [
            "\n",
            "    # For training, we want a lot of parallel reading and shuffling.\n",
            "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
            "    if is_training:\n",
            "      d = tf.data.Dataset.from_tensor_slices(tf.constant(input_files))\n",
            "      d = d.repeat()\n",
            "      d = d.shuffle(buffer_size=len(input_files))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 378,
        "API": ".repeat(",
        "context": [
            "    else:\n",
            "      d = tf.data.TFRecordDataset(input_files)\n",
            "      # Since we evaluate for a fixed number of steps we don't want to encounter\n",
            "      # out-of-range exceptions.\n",
            "      d = d.repeat()\n",
            "\n",
            "    # We must `drop_remainder` on training because the TPU requires fixed\n",
            "    # size dimensions. For eval, we assume we are evaluating on the CPU or GPU\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 411,
        "API": ".set_verbosity(",
        "context": [
            "  return example\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  tf.logging.set_verbosity(tf.logging.INFO)\n",
            "\n",
            "  if not FLAGS.do_train and not FLAGS.do_eval: # \u5fc5\u987b\u662f\u8bad\u7ec3\u6216\u9a8c\u8bc1\u7684\u7c7b\u578b\n",
            "    raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 421,
        "API": ".split(",
        "context": [
            "\n",
            "  tf.gfile.MakeDirs(FLAGS.output_dir)\n",
            "\n",
            "  input_files = [] # \u8f93\u5165\u53ef\u4ee5\u662f\u591a\u4e2a\u6587\u4ef6\uff0c\u4ee5\u201c\u9017\u53f7\u9694\u5f00\u201d\uff1b\u53ef\u4ee5\u662f\u4e00\u4e2a\u5339\u914d\u5f62\u5f0f\u7684\uff0c\u5982\u201cinput_x*\u201d\n",
            "  for input_pattern in FLAGS.input_file.split(\",\"):\n",
            "    input_files.extend(tf.gfile.Glob(input_pattern))\n",
            "\n",
            "  tf.logging.info(\"*** Input Files ***\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 426,
        "API": ".info(",
        "context": [
            "    input_files.extend(tf.gfile.Glob(input_pattern))\n",
            "\n",
            "  tf.logging.info(\"*** Input Files ***\")\n",
            "  for input_file in input_files:\n",
            "    tf.logging.info(\"  %s\" % input_file)\n",
            "\n",
            "  tpu_cluster_resolver = None\n",
            "  if FLAGS.use_tpu and FLAGS.tpu_name:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 467,
        "API": ".info(",
        "context": [
            "      train_batch_size=FLAGS.train_batch_size,\n",
            "      eval_batch_size=FLAGS.eval_batch_size)\n",
            "\n",
            "  if FLAGS.do_train:\n",
            "    tf.logging.info(\"***** Running training *****\")\n",
            "    tf.logging.info(\"  Batch size = %d\", FLAGS.train_batch_size)\n",
            "    train_input_fn = input_fn_builder(\n",
            "        input_files=input_files,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 477,
        "API": ".info(",
        "context": [
            "        is_training=True)\n",
            "    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps)\n",
            "\n",
            "  if FLAGS.do_eval:\n",
            "    tf.logging.info(\"***** Running evaluation *****\")\n",
            "    tf.logging.info(\"  Batch size = %d\", FLAGS.eval_batch_size)\n",
            "\n",
            "    eval_input_fn = input_fn_builder(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 488,
        "API": ".join(",
        "context": [
            "        is_training=False)\n",
            "\n",
            "    result = estimator.evaluate(input_fn=eval_input_fn, steps=FLAGS.max_eval_steps)\n",
            "\n",
            "    output_eval_file = os.path.join(FLAGS.output_dir, \"eval_results.txt\")\n",
            "    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
            "      tf.logging.info(\"***** Eval results *****\")\n",
            "      for key in sorted(result.keys()):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining.py",
        "line_number": 493,
        "API": ".write(",
        "context": [
            "    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
            "      tf.logging.info(\"***** Eval results *****\")\n",
            "      for key in sorted(result.keys()):\n",
            "        tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
            "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining_google.py",
        "line_number": 185,
        "API": ".info(",
        "context": [
            "\n",
            "    initialized_variable_names = {}\n",
            "    scaffold_fn = None\n",
            "    if init_checkpoint:\n",
            "      tf.logging.info(\"number of hidden group %d to initialize\",\n",
            "                      albert_config.num_hidden_groups)\n",
            "      num_of_initialize_group = 1\n",
            "      if FLAGS.init_from_group0:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining_google.py",
        "line_number": 199,
        "API": ".info(",
        "context": [
            "      if use_tpu:\n",
            "\n",
            "        def tpu_scaffold():\n",
            "          for gid in range(num_of_initialize_group):\n",
            "            tf.logging.info(\"initialize the %dth layer\", gid)\n",
            "            tf.logging.info(assignment_map[gid])\n",
            "            tf.train.init_from_checkpoint(init_checkpoint, assignment_map[gid])\n",
            "          return tf.train.Scaffold()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining_google.py",
        "line_number": 207,
        "API": ".info(",
        "context": [
            "\n",
            "        scaffold_fn = tpu_scaffold\n",
            "      else:\n",
            "        for gid in range(num_of_initialize_group):\n",
            "          tf.logging.info(\"initialize the %dth layer\", gid)\n",
            "          tf.logging.info(assignment_map[gid])\n",
            "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map[gid])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining_google.py",
        "line_number": 239,
        "API": ".reshape(",
        "context": [
            "         masked_lm_weights, sentence_order_example_loss,\n",
            "         sentence_order_log_probs, sentence_order_labels) = args[:7]\n",
            "\n",
            "\n",
            "        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n",
            "                                         [-1, masked_lm_log_probs.shape[-1]])\n",
            "        masked_lm_predictions = tf.argmax(\n",
            "            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining_google.py",
        "line_number": 244,
        "API": ".reshape(",
        "context": [
            "                                         [-1, masked_lm_log_probs.shape[-1]])\n",
            "        masked_lm_predictions = tf.argmax(\n",
            "            masked_lm_log_probs, axis=-1, output_type=tf.int32)\n",
            "        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n",
            "        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n",
            "        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n",
            "        masked_lm_accuracy = tf.metrics.accuracy(\n",
            "            labels=masked_lm_ids,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining_google.py",
        "line_number": 250,
        "API": ".mean(",
        "context": [
            "        masked_lm_accuracy = tf.metrics.accuracy(\n",
            "            labels=masked_lm_ids,\n",
            "            predictions=masked_lm_predictions,\n",
            "            weights=masked_lm_weights)\n",
            "        masked_lm_mean_loss = tf.metrics.mean(\n",
            "            values=masked_lm_example_loss, weights=masked_lm_weights)\n",
            "\n",
            "        metrics = {\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining_google.py",
        "line_number": 258,
        "API": ".reshape(",
        "context": [
            "            \"masked_lm_accuracy\": masked_lm_accuracy,\n",
            "            \"masked_lm_loss\": masked_lm_mean_loss,\n",
            "        }\n",
            "\n",
            "        sentence_order_log_probs = tf.reshape(\n",
            "            sentence_order_log_probs, [-1, sentence_order_log_probs.shape[-1]])\n",
            "        sentence_order_predictions = tf.argmax(\n",
            "            sentence_order_log_probs, axis=-1, output_type=tf.int32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining_google.py",
        "line_number": 266,
        "API": ".mean(",
        "context": [
            "        sentence_order_labels = tf.reshape(sentence_order_labels, [-1])\n",
            "        sentence_order_accuracy = tf.metrics.accuracy(\n",
            "            labels=sentence_order_labels,\n",
            "            predictions=sentence_order_predictions)\n",
            "        sentence_order_mean_loss = tf.metrics.mean(\n",
            "            values=sentence_order_example_loss)\n",
            "        metrics.update({\n",
            "            \"sentence_order_accuracy\": sentence_order_accuracy,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining_google.py",
        "line_number": 319,
        "API": ".matmul(",
        "context": [
            "    output_bias = tf.get_variable(\n",
            "        \"output_bias\",\n",
            "        shape=[albert_config.vocab_size],\n",
            "        initializer=tf.zeros_initializer())\n",
            "    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n",
            "    logits = tf.nn.bias_add(logits, output_bias)\n",
            "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining_google.py",
        "line_number": 324,
        "API": ".reshape(",
        "context": [
            "    logits = tf.nn.bias_add(logits, output_bias)\n",
            "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
            "\n",
            "    label_ids = tf.reshape(label_ids, [-1])\n",
            "    label_weights = tf.reshape(label_weights, [-1])\n",
            "\n",
            "    one_hot_labels = tf.one_hot(\n",
            "        label_ids, depth=albert_config.vocab_size, dtype=tf.float32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining_google.py",
        "line_number": 355,
        "API": ".matmul(",
        "context": [
            "            albert_config.initializer_range))\n",
            "    output_bias = tf.get_variable(\n",
            "        \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n",
            "\n",
            "    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n",
            "    logits = tf.nn.bias_add(logits, output_bias)\n",
            "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
            "    labels = tf.reshape(labels, [-1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining_google.py",
        "line_number": 449,
        "API": ".info(",
        "context": [
            "            lambda record: _decode_record(record, name_to_features),\n",
            "            batch_size=batch_size,\n",
            "            num_parallel_batches=num_cpu_threads,\n",
            "            drop_remainder=True))\n",
            "    tf.logging.info(d)\n",
            "    return d\n",
            "\n",
            "  return input_fn\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining_google.py",
        "line_number": 471,
        "API": ".set_verbosity(",
        "context": [
            "  return example\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  tf.logging.set_verbosity(tf.logging.INFO)\n",
            "\n",
            "  if not FLAGS.do_train and not FLAGS.do_eval:\n",
            "    raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining_google.py",
        "line_number": 481,
        "API": ".split(",
        "context": [
            "\n",
            "  tf.gfile.MakeDirs(FLAGS.output_dir)\n",
            "\n",
            "  input_files = []\n",
            "  for input_pattern in FLAGS.input_file.split(\",\"):\n",
            "    input_files.extend(tf.gfile.Glob(input_pattern))\n",
            "\n",
            "  tf.logging.info(\"*** Input Files ***\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining_google.py",
        "line_number": 536,
        "API": ".info(",
        "context": [
            "        is_training=True)\n",
            "    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps)\n",
            "\n",
            "  if FLAGS.do_eval:\n",
            "    tf.logging.info(\"***** Running evaluation *****\")\n",
            "    tf.logging.info(\"  Batch size = %d\", FLAGS.eval_batch_size)\n",
            "    global_step = -1\n",
            "    output_eval_file = os.path.join(FLAGS.output_dir, \"eval_results.txt\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining_google.py",
        "line_number": 548,
        "API": ".latest_checkpoint(",
        "context": [
            "        max_seq_length=FLAGS.max_seq_length,\n",
            "        max_predictions_per_seq=FLAGS.max_predictions_per_seq,\n",
            "        is_training=False)\n",
            "    while global_step < FLAGS.num_train_steps:\n",
            "      if estimator.latest_checkpoint() is None:\n",
            "        tf.logging.info(\"No checkpoint found yet. Sleeping.\")\n",
            "        time.sleep(1)\n",
            "      else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\run_pretraining_google.py",
        "line_number": 555,
        "API": ".info(",
        "context": [
            "      else:\n",
            "        result = estimator.evaluate(\n",
            "            input_fn=eval_input_fn, steps=FLAGS.max_eval_steps)\n",
            "        global_step = result[\"global_step\"]\n",
            "        tf.logging.info(\"***** Eval results *****\")\n",
            "        for key in sorted(result.keys()):\n",
            "          tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
            "          writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\similarity.py",
        "line_number": 43,
        "API": ".set_verbosity(",
        "context": [
            "        self.tokenizer = tokenization.FullTokenizer(vocab_file=args.vocab_file, do_lower_case=True)\n",
            "        self.batch_size = batch_size\n",
            "        self.estimator = None\n",
            "        self.processor = SimProcessor()\n",
            "        tf.logging.set_verbosity(tf.logging.INFO)\n",
            "\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\similarity.py",
        "line_number": 60,
        "API": ".info(",
        "context": [
            "\n",
            "        def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
            "            from tensorflow.python.estimator.model_fn import EstimatorSpec\n",
            "\n",
            "            tf.logging.info(\"*** Features ***\")\n",
            "            for name in sorted(features.keys()):\n",
            "                tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\similarity.py",
        "line_number": 83,
        "API": ".info(",
        "context": [
            "                (assignment_map, initialized_variable_names) \\\n",
            "                    = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
            "                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
            "\n",
            "            tf.logging.info(\"**** Trainable Variables ****\")\n",
            "            for var in tvars:\n",
            "                init_string = \"\"\n",
            "                if var.name in initialized_variable_names:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\similarity.py",
        "line_number": 88,
        "API": ".info(",
        "context": [
            "            for var in tvars:\n",
            "                init_string = \"\"\n",
            "                if var.name in initialized_variable_names:\n",
            "                    init_string = \", *INIT_FROM_CKPT*\"\n",
            "                tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
            "                                init_string)\n",
            "            output_spec = EstimatorSpec(mode=mode, predictions=probabilities)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\similarity.py",
        "line_number": 221,
        "API": ".info(",
        "context": [
            "        assert len(segment_ids) == max_seq_length\n",
            "\n",
            "        label_id = label_map[example.label]\n",
            "        if ex_index < 5:\n",
            "            tf.logging.info(\"*** Example ***\")\n",
            "            tf.logging.info(\"guid: %s\" % (example.guid))\n",
            "            tf.logging.info(\"tokens: %s\" % \" \".join(\n",
            "                [tokenization.printable_text(x) for x in tokens]))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\similarity.py",
        "line_number": 226,
        "API": ".join(",
        "context": [
            "            tf.logging.info(\"guid: %s\" % (example.guid))\n",
            "            tf.logging.info(\"tokens: %s\" % \" \".join(\n",
            "                [tokenization.printable_text(x) for x in tokens]))\n",
            "            tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
            "            tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
            "            tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
            "            tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\tokenization.py",
        "line_number": 42,
        "API": ".group(",
        "context": [
            "  m = re.match(\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\", init_checkpoint)\n",
            "  if m is None:\n",
            "    return\n",
            "\n",
            "  model_name = m.group(1)\n",
            "\n",
            "  lower_models = [\n",
            "      \"uncased_L-24_H-1024_A-16\", \"uncased_L-12_H-768_A-12\",\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\tokenization.py",
        "line_number": 129,
        "API": ".strip(",
        "context": [
            "    while True:\n",
            "      token = convert_to_unicode(reader.readline())\n",
            "      if not token:\n",
            "        break\n",
            "      token = token.strip()\n",
            "      vocab[token] = index\n",
            "      index += 1\n",
            "  return vocab\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\tokenization.py",
        "line_number": 155,
        "API": ".strip(",
        "context": [
            "\n",
            "\n",
            "def whitespace_tokenize(text):\n",
            "  \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
            "  text = text.strip()\n",
            "  if not text:\n",
            "    return []\n",
            "  tokens = text.split()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\tokenization.py",
        "line_number": 214,
        "API": ".lower(",
        "context": [
            "    orig_tokens = whitespace_tokenize(text)\n",
            "    split_tokens = []\n",
            "    for token in orig_tokens:\n",
            "      if self.do_lower_case:\n",
            "        token = token.lower()\n",
            "        token = self._run_strip_accents(token)\n",
            "      split_tokens.extend(self._run_split_on_punc(token))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\tokenization.py",
        "line_number": 223,
        "API": ".normalize(",
        "context": [
            "    return output_tokens\n",
            "\n",
            "  def _run_strip_accents(self, text):\n",
            "    \"\"\"Strips accents from a piece of text.\"\"\"\n",
            "    text = unicodedata.normalize(\"NFD\", text)\n",
            "    output = []\n",
            "    for char in text:\n",
            "      cat = unicodedata.category(char)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\tokenization.py",
        "line_number": 230,
        "API": ".join(",
        "context": [
            "      cat = unicodedata.category(char)\n",
            "      if cat == \"Mn\":\n",
            "        continue\n",
            "      output.append(char)\n",
            "    return \"\".join(output)\n",
            "\n",
            "  def _run_split_on_punc(self, text):\n",
            "    \"\"\"Splits punctuation on a piece of text.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\tokenization.py",
        "line_number": 250,
        "API": ".join(",
        "context": [
            "        start_new_word = False\n",
            "        output[-1].append(char)\n",
            "      i += 1\n",
            "\n",
            "    return [\"\".join(x) for x in output]\n",
            "\n",
            "  def _tokenize_chinese_chars(self, text):\n",
            "    \"\"\"Adds whitespace around any CJK character.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\tokenization.py",
        "line_number": 263,
        "API": ".join(",
        "context": [
            "        output.append(char)\n",
            "        output.append(\" \")\n",
            "      else:\n",
            "        output.append(char)\n",
            "    return \"\".join(output)\n",
            "\n",
            "  def _is_chinese_char(self, cp):\n",
            "    \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\tokenization.py",
        "line_number": 298,
        "API": ".join(",
        "context": [
            "      if _is_whitespace(char):\n",
            "        output.append(\" \")\n",
            "      else:\n",
            "        output.append(char)\n",
            "    return \"\".join(output)\n",
            "\n",
            "\n",
            "class WordpieceTokenizer(object):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\tokenization.py",
        "line_number": 343,
        "API": ".join(",
        "context": [
            "      while start < len(chars):\n",
            "        end = len(chars)\n",
            "        cur_substr = None\n",
            "        while start < end:\n",
            "          substr = \"\".join(chars[start:end])\n",
            "          if start > 0:\n",
            "            substr = \"##\" + substr\n",
            "          if substr in self.vocab:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\tokenization_google.py",
        "line_number": 50,
        "API": ".group(",
        "context": [
            "               six.ensure_str(init_checkpoint))\n",
            "  if m is None:\n",
            "    return\n",
            "\n",
            "  model_name = m.group(1)\n",
            "\n",
            "  lower_models = [\n",
            "      \"uncased_L-24_H-1024_A-16\", \"uncased_L-12_H-768_A-12\",\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\tokenization_google.py",
        "line_number": 89,
        "API": ".join(",
        "context": [
            "def preprocess_text(inputs, remove_space=True, lower=False):\n",
            "  \"\"\"preprocess data by removing extra space and normalize data.\"\"\"\n",
            "  outputs = inputs\n",
            "  if remove_space:\n",
            "    outputs = \" \".join(inputs.strip().split())\n",
            "\n",
            "  if six.PY2 and isinstance(outputs, str):\n",
            "    try:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\tokenization_google.py",
        "line_number": 97,
        "API": ".normalize(",
        "context": [
            "      outputs = six.ensure_text(outputs, \"utf-8\")\n",
            "    except UnicodeDecodeError:\n",
            "      outputs = six.ensure_text(outputs, \"latin-1\")\n",
            "\n",
            "  outputs = unicodedata.normalize(\"NFKD\", outputs)\n",
            "  outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
            "  if lower:\n",
            "    outputs = outputs.lower()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\tokenization_google.py",
        "line_number": 200,
        "API": ".strip(",
        "context": [
            "    while True:\n",
            "      token = convert_to_unicode(reader.readline())\n",
            "      if not token:\n",
            "        break\n",
            "      token = token.strip() # previous: token.strip().split()[0]\n",
            "      if token not in vocab:\n",
            "        vocab[token] = len(vocab)\n",
            "  return vocab\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\tokenization_google.py",
        "line_number": 241,
        "API": ".info(",
        "context": [
            "    print(\"spm_model_file:\",spm_model_file,\";vocab_file:\",vocab_file)\n",
            "    if spm_model_file:\n",
            "      print(\"#Use spm_model_file\")\n",
            "      self.sp_model = spm.SentencePieceProcessor()\n",
            "      tf.logging.info(\"loading sentence piece model\")\n",
            "      self.sp_model.Load(spm_model_file)\n",
            "      # Note(mingdachen): For the purpose of consisent API, we are\n",
            "      # generating a vocabulary for the sentence piece tokenizer.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\tokenization_google.py",
        "line_number": 267,
        "API": ".info(",
        "context": [
            "    return split_tokens\n",
            "\n",
            "  def convert_tokens_to_ids(self, tokens):\n",
            "    if self.sp_model:\n",
            "      tf.logging.info(\"using sentence piece tokenzier.\")\n",
            "      return [self.sp_model.PieceToId(\n",
            "          printable_text(token)) for token in tokens]\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\tokenization_google.py",
        "line_number": 275,
        "API": ".info(",
        "context": [
            "      return convert_by_vocab(self.vocab, tokens)\n",
            "\n",
            "  def convert_ids_to_tokens(self, ids):\n",
            "    if self.sp_model:\n",
            "      tf.logging.info(\"using sentence piece tokenzier.\")\n",
            "      return [self.sp_model.IdToPiece(id_) for id_ in ids]\n",
            "    else:\n",
            "      return convert_by_vocab(self.inv_vocab, ids)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\tokenization_google.py",
        "line_number": 438,
        "API": ".join(",
        "context": [
            "      while start < len(chars):\n",
            "        end = len(chars)\n",
            "        cur_substr = None\n",
            "        while start < end:\n",
            "          substr = \"\".join(chars[start:end])\n",
            "          if start > 0:\n",
            "            substr = \"##\" + six.ensure_str(substr)\n",
            "          if substr in self.vocab:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\resources\\create_pretraining_data_roberta.py",
        "line_number": 82,
        "API": ".join(",
        "context": [
            "        self.masked_lm_labels = masked_lm_labels\n",
            "\n",
            "    def __str__(self):\n",
            "        s = \"\"\n",
            "        s += \"tokens: %s\\n\" % (\" \".join(\n",
            "            [tokenization.printable_text(x) for x in self.tokens]))\n",
            "        s += \"segment_ids: %s\\n\" % (\" \".join([str(x) for x in self.segment_ids]))\n",
            "        s += \"is_random_next: %s\\n\" % self.is_random_next\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\resources\\create_pretraining_data_roberta.py",
        "line_number": 88,
        "API": ".join(",
        "context": [
            "        s += \"segment_ids: %s\\n\" % (\" \".join([str(x) for x in self.segment_ids]))\n",
            "        s += \"is_random_next: %s\\n\" % self.is_random_next\n",
            "        s += \"masked_lm_positions: %s\\n\" % (\" \".join(\n",
            "            [str(x) for x in self.masked_lm_positions]))\n",
            "        s += \"masked_lm_labels: %s\\n\" % (\" \".join(\n",
            "            [tokenization.printable_text(x) for x in self.masked_lm_labels]))\n",
            "        s += \"\\n\"\n",
            "        return s\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\resources\\create_pretraining_data_roberta.py",
        "line_number": 145,
        "API": ".write(",
        "context": [
            "        features[\"next_sentence_labels\"] = create_int_feature([next_sentence_label])\n",
            "\n",
            "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
            "\n",
            "        writers[writer_index].write(tf_example.SerializeToString())\n",
            "        writer_index = (writer_index + 1) % len(writers)\n",
            "\n",
            "        total_written += 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\resources\\create_pretraining_data_roberta.py",
        "line_number": 151,
        "API": ".info(",
        "context": [
            "\n",
            "        total_written += 1\n",
            "\n",
            "        if inst_index < 20:\n",
            "            tf.logging.info(\"*** Example ***\")\n",
            "            tf.logging.info(\"tokens: %s\" % \" \".join(\n",
            "                [tokenization.printable_text(x) for x in instance.tokens]))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\resources\\create_pretraining_data_roberta.py",
        "line_number": 162,
        "API": ".info(",
        "context": [
            "                if feature.int64_list.value:\n",
            "                    values = feature.int64_list.value\n",
            "                elif feature.float_list.value:\n",
            "                    values = feature.float_list.value\n",
            "                tf.logging.info(\n",
            "                    \"%s: %s\" % (feature_name, \" \".join([str(x) for x in values])))\n",
            "\n",
            "    for writer in writers:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\resources\\create_pretraining_data_roberta.py",
        "line_number": 168,
        "API": ".info(",
        "context": [
            "\n",
            "    for writer in writers:\n",
            "        writer.close()\n",
            "\n",
            "    tf.logging.info(\"Wrote %d total instances\", total_written)\n",
            "\n",
            "\n",
            "def create_int_feature(values):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\resources\\create_pretraining_data_roberta.py",
        "line_number": 200,
        "API": ".strip(",
        "context": [
            "            while True:\n",
            "                line = tokenization.convert_to_unicode(reader.readline().replace(\"<eop>\",\"\"))# .replace(\"\u201d\",\"\")) # \u5c06<eop>\u3001\u201d\u66ff\u6362\u6389\u3002\n",
            "                if not line:\n",
            "                    break\n",
            "                line = line.strip()\n",
            "\n",
            "                # Empty lines are used as document delimiters\n",
            "                if not line:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\resources\\create_pretraining_data_roberta.py",
        "line_number": 255,
        "API": ".join(",
        "context": [
            "    \u8f93\u5165\u4e00\u53e5\u8bdd\uff0c\u8fd4\u56de\u4e00\u53e5\u7ecf\u8fc7\u5904\u7406\u7684\u8bdd: \u4e3a\u4e86\u652f\u6301\u4e2d\u6587\u5168\u79f0mask\uff0c\u5c06\u88ab\u5206\u5f00\u7684\u8bcd\uff0c\u5c06\u4e0a\u7279\u6b8a\u6807\u8bb0(\"#\")\uff0c\u4f7f\u5f97\u540e\u7eed\u5904\u7406\u6a21\u5757\uff0c\u80fd\u591f\u77e5\u9053\u54ea\u4e9b\u5b57\u662f\u5c5e\u4e8e\u540c\u4e00\u4e2a\u8bcd\u7684\u3002\n",
            "    :param segment: \u4e00\u53e5\u8bdd\n",
            "    :return: \u4e00\u53e5\u5904\u7406\u8fc7\u7684\u8bdd\n",
            "    \"\"\"\n",
            "    seq_cws = jieba.lcut(\"\".join(segment))\n",
            "    seq_cws_dict = {x: 1 for x in seq_cws}\n",
            "    new_segment = []\n",
            "    i = 0\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\resources\\create_pretraining_data_roberta.py",
        "line_number": 269,
        "API": ".join(",
        "context": [
            "        has_add = False\n",
            "        for length in range(3,0,-1):\n",
            "            if i+length>len(segment):\n",
            "                continue\n",
            "            if ''.join(segment[i:i+length]) in seq_cws_dict:\n",
            "                new_segment.append(segment[i])\n",
            "                for l in range(1, length):\n",
            "                    new_segment.append('##' + segment[i+l])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\resources\\create_pretraining_data_roberta.py",
        "line_number": 547,
        "API": ".add(",
        "context": [
            "                break\n",
            "        if is_any_index_covered:\n",
            "            continue\n",
            "        for index in index_set:\n",
            "            covered_indexes.add(index)\n",
            "\n",
            "            masked_token = None\n",
            "            # 80% of the time, replace with [MASK]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\resources\\create_pretraining_data_roberta.py",
        "line_number": 573,
        "API": ".info(",
        "context": [
            "    for p in masked_lms:\n",
            "        masked_lm_positions.append(p.index)\n",
            "        masked_lm_labels.append(p.label)\n",
            "\n",
            "    # tf.logging.info('%s' % (tokens))\n",
            "    # tf.logging.info('%s' % (output_tokens))\n",
            "    return (output_tokens, masked_lm_positions, masked_lm_labels)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\resources\\create_pretraining_data_roberta.py",
        "line_number": 597,
        "API": ".set_verbosity(",
        "context": [
            "            trunc_tokens.pop()\n",
            "\n",
            "\n",
            "def main(_):\n",
            "    tf.logging.set_verbosity(tf.logging.INFO)\n",
            "\n",
            "    tokenizer = tokenization.FullTokenizer(\n",
            "        vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\resources\\create_pretraining_data_roberta.py",
        "line_number": 603,
        "API": ".split(",
        "context": [
            "    tokenizer = tokenization.FullTokenizer(\n",
            "        vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
            "\n",
            "    input_files = []\n",
            "    for input_pattern in FLAGS.input_file.split(\",\"):\n",
            "        input_files.extend(tf.gfile.Glob(input_pattern))\n",
            "\n",
            "    tf.logging.info(\"*** Reading from input files ***\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\resources\\create_pretraining_data_roberta.py",
        "line_number": 608,
        "API": ".info(",
        "context": [
            "        input_files.extend(tf.gfile.Glob(input_pattern))\n",
            "\n",
            "    tf.logging.info(\"*** Reading from input files ***\")\n",
            "    for input_file in input_files:\n",
            "        tf.logging.info(\"  %s\", input_file)\n",
            "\n",
            "    rng = random.Random(FLAGS.random_seed)\n",
            "    instances = create_training_instances(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_pytorch_test\\albert_zh\\resources\\create_pretraining_data_roberta.py",
        "line_number": 616,
        "API": ".split(",
        "context": [
            "        input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,\n",
            "        FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,\n",
            "        rng)\n",
            "\n",
            "    output_files = FLAGS.output_file.split(\",\")\n",
            "    tf.logging.info(\"*** Writing to output files ***\")\n",
            "    for output_file in output_files:\n",
            "        tf.logging.info(\"  %s\", output_file)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\setup.py",
        "line_number": 36,
        "API": ".join(",
        "context": [
            "from setuptools.command.test import test as TestCommandBase\n",
            "from setuptools.dist import Distribution\n",
            "\n",
            "# To enable importing version.py directly, we add its path to sys.path.\n",
            "version_path = os.path.join(os.path.dirname(__file__), 'tf_agents')\n",
            "sys.path.append(version_path)\n",
            "import version as tf_agents_version  # pylint: disable=g-import-not-at-top\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\setup.py",
        "line_number": 56,
        "API": ".write(",
        "context": [
            "\n",
            "class StderrWrapper(io.IOBase):\n",
            "\n",
            "  def write(self, *args, **kwargs):\n",
            "    return sys.stderr.write(*args, **kwargs)\n",
            "\n",
            "  def writeln(self, *args, **kwargs):\n",
            "    if args or kwargs:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\setup.py",
        "line_number": 61,
        "API": ".write(",
        "context": [
            "\n",
            "  def writeln(self, *args, **kwargs):\n",
            "    if args or kwargs:\n",
            "      sys.stderr.write(*args, **kwargs)\n",
            "    sys.stderr.write('\\n')\n",
            "\n",
            "\n",
            "class TestLoader(unittest.TestLoader):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\setup.py",
        "line_number": 82,
        "API": ".strip(",
        "context": [
            "\n",
            "def load_test_list(filename):\n",
            "  testcases = [x.rstrip() for x in open(filename, 'r').readlines() if x]\n",
            "  # Remove comments and blanks after comments are removed.\n",
            "  testcases = [x.partition('#')[0].strip() for x in testcases]\n",
            "  return [x for x in testcases if x]\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\setup.py",
        "line_number": 112,
        "API": ".list_physical_devices(",
        "context": [
            "      # tests allocates all GPU memory because by default TensorFlow allocates\n",
            "      # all GPU memory during initialization. This causes tests in\n",
            "      # run_seperately to fail with out of memory errors because they are run as\n",
            "      # a subprocess of the process holding the GPU memory.\n",
            "      gpus = tf.config.experimental.list_physical_devices('GPU')\n",
            "      for gpu in gpus:\n",
            "        tf.config.set_logical_device_configuration(\n",
            "            gpu, [tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\setup.py",
        "line_number": 141,
        "API": ".format(",
        "context": [
            "      for failure in external_test_failures:\n",
            "        stderr.writeln(str(failure))\n",
            "\n",
            "      final_output = (\n",
            "          'Tests run: {} grouped and {} external.  '.format(\n",
            "              result.testsRun, len(run_separately)) +\n",
            "          'Errors: {}  Failures: {}  External failures: {}.'.format(\n",
            "              len(result.errors),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\setup.py",
        "line_number": 263,
        "API": ".join(",
        "context": [
            "  \"\"\"Triggers build, install, and other features of `setuptools.setup`.\"\"\"\n",
            "\n",
            "  # Builds the long description from the README.\n",
            "  root_path = os.path.abspath(os.path.dirname(__file__))\n",
            "  with codecs.open(os.path.join(root_path, 'README.md'), encoding='utf-8') as f:\n",
            "    long_description = f.read()\n",
            "\n",
            "  version, project_name = get_version()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\docs\\tutorials\\colab_kernel_init.py",
        "line_number": 33,
        "API": ".glob(",
        "context": [
            "  result = []\n",
            "  while not result:\n",
            "    if time.time() - start > timeout_sec:\n",
            "      return result\n",
            "    result = glob.glob(path_pattern)\n",
            "    time.sleep(0.1)\n",
            "  return result\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\docs\\tutorials\\colab_kernel_init.py",
        "line_number": 56,
        "API": ".info(",
        "context": [
            "\n",
            "  # If we find \"/tmp/.X11-unix/X1\", then we will set DISPLAY to be \":1\".\n",
            "  display = \":\" + res[0][len(pattern)-1:]\n",
            "  os.environ[\"DISPLAY\"] = display\n",
            "  logging.info(\"Set DISPLAY=%s\", display)\n",
            "\n",
            "\n",
            "SetDisplayFromWebTest()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\version.py",
        "line_number": 31,
        "API": ".join(",
        "context": [
            "_DEV_SUFFIX = 'dev'\n",
            "_REL_SUFFIX = 'rc0'\n",
            "\n",
            "# Example, '0.10.0rc0'\n",
            "__version__ = '.'.join([\n",
            "    _MAJOR_VERSION,\n",
            "    _MINOR_VERSION,\n",
            "    _PATCH_VERSION,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\version.py",
        "line_number": 36,
        "API": ".format(",
        "context": [
            "    _MAJOR_VERSION,\n",
            "    _MINOR_VERSION,\n",
            "    _PATCH_VERSION,\n",
            "])\n",
            "__dev_version__ = '{}.{}'.format(__version__, _DEV_SUFFIX)\n",
            "__rel_version__ = '{}{}'.format(__version__, _REL_SUFFIX)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\data_converter.py",
        "line_number": 105,
        "API": ".flatten(",
        "context": [
            "        objects.\n",
            "    \"\"\"\n",
            "    def _each_isinstance(spec, spec_types):\n",
            "      \"\"\"Checks if each element of `spec` is instance of `spec_types`.\"\"\"\n",
            "      return all([isinstance(s, spec_types) for s in tf.nest.flatten(spec)])\n",
            "\n",
            "    for (spec, label) in ((time_step_spec, 'time_step_spec'),\n",
            "                          (action_spec, 'action_spec'),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\data_converter.py",
        "line_number": 114,
        "API": ".format(",
        "context": [
            "      if not _each_isinstance(spec, tf.TypeSpec):\n",
            "        raise TypeError(\n",
            "            '{} has to contain TypeSpec (TensorSpec, '\n",
            "            'SparseTensorSpec, etc) objects, but received: {}'\n",
            "            .format(label, spec))\n",
            "\n",
            "    self._time_step_spec = time_step_spec\n",
            "    self._action_spec = action_spec\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\data_converter.py",
        "line_number": 176,
        "API": ".map_structure(",
        "context": [
            "  \"\"\"Validate a Trajectory given its spec and a sequence length.\"\"\"\n",
            "  if not nest_utils.is_batched_nested_tensors(\n",
            "      value, trajectory_spec, num_outer_dims=num_outer_dims,\n",
            "      allow_extra_fields=True):\n",
            "    debug_str_1 = tf.nest.map_structure(lambda tp: tp.shape, value)\n",
            "    debug_str_2 = tf.nest.map_structure(\n",
            "        lambda spec: spec.shape, trajectory_spec)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\data_converter.py",
        "line_number": 189,
        "API": ".format(",
        "context": [
            "        'All of the Tensors in `value` must have {shape_str}. Specifically, '\n",
            "        'tensors must have shape `{shape_prefix_str} + spec.shape`.\\n'\n",
            "        'Full shapes of value tensors:\\n  {debug_str_1}.\\n'\n",
            "        'Expected shapes (excluding the {shape_str}):\\n  {debug_str_2}.'\n",
            "        .format(\n",
            "            shape_str=shape_str,\n",
            "            debug_str_1=debug_str_1,\n",
            "            debug_str_2=debug_str_2,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\data_converter.py",
        "line_number": 200,
        "API": ".map_structure(",
        "context": [
            "  # match.\n",
            "  if sequence_length is not None:\n",
            "    def check_shape(path, t):  # pylint: disable=invalid-name\n",
            "      if t.shape[1] != sequence_length:\n",
            "        debug_str = tf.nest.map_structure(lambda tp: tp.shape, value)\n",
            "        raise ValueError(\n",
            "            'The agent was configured to expect a `sequence_length` '\n",
            "            'of \\'{seq_len}\\'. Value is expected to be shaped `[B, T] + '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\data_converter.py",
        "line_number": 208,
        "API": ".format(",
        "context": [
            "            'spec.shape` but at least one of the Tensors in `value` has a '\n",
            "            'time axis dim value \\'{t_dim}\\' vs '\n",
            "            'the expected \\'{seq_len}\\'.\\nFirst such tensor is:\\n\\t'\n",
            "            'value.{path}. \\nFull shape structure of '\n",
            "            'value:\\n\\t{debug_str}'.format(\n",
            "                seq_len=sequence_length,\n",
            "                t_dim=t.shape[1],\n",
            "                path=path,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\data_converter.py",
        "line_number": 241,
        "API": ".map_structure(",
        "context": [
            "      value_to_validate,\n",
            "      spec_to_validate,\n",
            "      num_outer_dims=num_outer_dims,\n",
            "      allow_extra_fields=True):\n",
            "    debug_str_1 = tf.nest.map_structure(\n",
            "        lambda tp: tp.shape, value_to_validate)\n",
            "    debug_str_2 = tf.nest.map_structure(\n",
            "        lambda spec: spec.shape, spec_to_validate)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\data_converter.py",
        "line_number": 250,
        "API": ".format(",
        "context": [
            "        'All of the Tensors in `value` must have a single outer (batch size) '\n",
            "        'dimension. Specifically, tensors must have {} outer dimensions.'\n",
            "        '\\nFull shapes of value tensors:\\n  {}.\\n'\n",
            "        'Expected shapes (excluding the outer dimensions):\\n  {}.'\n",
            "        .format(num_outer_dims, debug_str_1, debug_str_2))\n",
            "\n",
            "\n",
            "def _validate_state(state: types.NestedTensor, spec: types.NestedTensorSpec):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\data_converter.py",
        "line_number": 263,
        "API": ".format(",
        "context": [
            "    raise ValueError('action_step.state does not match spec. '\n",
            "                     'action_step.state.shape: {state_shape}, '\n",
            "                     'spec.shape: {spec_shape}'\n",
            "                     'action_step.state: {state_value}, spec: '\n",
            "                     '{spec_value}'.format(\n",
            "                         state_shape=state.shape,\n",
            "                         spec_shape=spec.shape,\n",
            "                         state_value=state,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\data_converter.py",
        "line_number": 338,
        "API": ".format(",
        "context": [
            "          next_step_type=value.next_time_step.step_type,\n",
            "          reward=value.next_time_step.reward,\n",
            "          discount=value.next_time_step.discount)\n",
            "    else:\n",
            "      raise TypeError('Input type not supported: {}'.format(value))\n",
            "    _validate_trajectory(\n",
            "        value, self._data_context.trajectory_spec,\n",
            "        sequence_length=self._sequence_length,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\data_converter.py",
        "line_number": 420,
        "API": ".map_structure(",
        "context": [
            "          sequence_length=required_sequence_length)\n",
            "      value = trajectory.to_transition(value)\n",
            "      # Remove the now-singleton time dim.\n",
            "      if self._squeeze_time_dim:\n",
            "        value = tf.nest.map_structure(\n",
            "            lambda x: composite.squeeze(x, axis=1), value)\n",
            "    else:\n",
            "      raise TypeError('Input type not supported: {}'.format(value))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\data_converter.py",
        "line_number": 436,
        "API": ".map_structure(",
        "context": [
            "    if self._prepend_t0_to_next_time_step:\n",
            "      # This is useful when using sequential model. It allows target_q network\n",
            "      # to take all the information.\n",
            "      next_time_step_with_t0 = value.next_time_step._replace(\n",
            "          observation=tf.nest.map_structure(\n",
            "              lambda x, y: tf.concat([x[:, :1, ...], y], axis=1),\n",
            "              value.time_step.observation, value.next_time_step.observation))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\data_converter.py",
        "line_number": 506,
        "API": ".map_structure(",
        "context": [
            "          value,\n",
            "          self._data_context.trajectory_spec,\n",
            "          sequence_length=required_sequence_length)\n",
            "      if self._squeeze_time_dim:\n",
            "        value = tf.nest.map_structure(lambda e: tf.squeeze(e, axis=1), value)\n",
            "      policy_steps = policy_step.PolicyStep(\n",
            "          action=value.action, state=(), info=value.policy_info)\n",
            "      # TODO(b/130244652): Consider replacing 0 rewards & discounts with ().\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\data_converter.py",
        "line_number": 512,
        "API": ".map_structure(",
        "context": [
            "          action=value.action, state=(), info=value.policy_info)\n",
            "      # TODO(b/130244652): Consider replacing 0 rewards & discounts with ().\n",
            "      time_steps = ts.TimeStep(\n",
            "          value.step_type,\n",
            "          reward=tf.nest.map_structure(tf.zeros_like, value.reward),  # unknown\n",
            "          discount=tf.zeros_like(value.discount),  # unknown\n",
            "          observation=value.observation)\n",
            "      next_time_steps = ts.TimeStep(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\data_converter.py",
        "line_number": 519,
        "API": ".zeros_like(",
        "context": [
            "      next_time_steps = ts.TimeStep(\n",
            "          step_type=value.next_step_type,\n",
            "          reward=value.reward,\n",
            "          discount=value.discount,\n",
            "          observation=tf.zeros_like(value.discount))\n",
            "      value = trajectory.Transition(time_steps, policy_steps, next_time_steps)\n",
            "    else:\n",
            "      raise TypeError('Input type not supported: {}'.format(value))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\data_converter.py",
        "line_number": 600,
        "API": ".format(",
        "context": [
            "          self._data_context.trajectory_spec,\n",
            "          sequence_length=None if self._n is None else self._n + 1)\n",
            "      value = trajectory.to_n_step_transition(value, gamma=self._gamma)\n",
            "    else:\n",
            "      raise TypeError('Input type not supported: {}'.format(value))\n",
            "\n",
            "    _validate_transition(\n",
            "        value, self._data_context.transition_spec, num_outer_dims=1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\tf_agent.py",
        "line_number": 190,
        "API": ".format(",
        "context": [
            "    common.tf_agents_gauge.get_cell(str(type(self))).set(True)\n",
            "    if not isinstance(time_step_spec, ts.TimeStep):\n",
            "      raise TypeError(\n",
            "          \"The `time_step_spec` must be an instance of `TimeStep`, but is `{}`.\"\n",
            "          .format(type(time_step_spec)))\n",
            "\n",
            "    if num_outer_dims not in [1, 2]:\n",
            "      raise ValueError(\"num_outer_dims must be in [1, 2].\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\tf_agent.py",
        "line_number": 268,
        "API": ".executing_eagerly(",
        "context": [
            "          \"Cannot find _initialize_fn.  Did %s.__init__ call super?\"\n",
            "          % type(self).__name__)\n",
            "    if self._enable_functions:\n",
            "      self._initialize_fn()\n",
            "      if not tf.executing_eagerly():\n",
            "        # Latest op in graph is the call op for above fn so return it.\n",
            "        return tf.compat.v1.get_default_graph().get_operations()[-1]\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\tf_agent.py",
        "line_number": 339,
        "API": ".format(",
        "context": [
            "      loss_info = self._train(experience=experience, weights=weights, **kwargs)\n",
            "\n",
            "    if not isinstance(loss_info, LossInfo):\n",
            "      raise TypeError(\n",
            "          \"loss_info is not a subclass of LossInfo: {}\".format(loss_info))\n",
            "    return loss_info\n",
            "\n",
            "  def loss(self,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\tf_agent.py",
        "line_number": 390,
        "API": ".format(",
        "context": [
            "          experience=experience, weights=weights, training=training, **kwargs)\n",
            "\n",
            "    if not isinstance(loss_info, LossInfo):\n",
            "      raise TypeError(\n",
            "          \"loss_info is not a subclass of LossInfo: {}\".format(loss_info))\n",
            "    return loss_info\n",
            "\n",
            "  def _apply_loss(self, aggregated_losses, variables_to_train, tape, optimizer):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\behavioral_cloning\\behavioral_cloning_agent.py",
        "line_number": 77,
        "API": ".sparse_softmax_cross_entropy_with_logits(",
        "context": [
            "  ```python\n",
            "  def discrete_loss(agent, experience, training=False):\n",
            "    bc_logits = self._cloning_network(experience.observation, training=training)\n",
            "\n",
            "    return tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
            "      labels=experience.action - action_spec.minimum, logits=bc_logits)\n",
            "  ```\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\behavioral_cloning\\behavioral_cloning_agent.py",
        "line_number": 160,
        "API": ".flatten(",
        "context": [
            "    self._optimizer = optimizer\n",
            "    self._gradient_clipping = gradient_clipping\n",
            "\n",
            "    action_spec = tensor_spec.from_spec(action_spec)\n",
            "    flat_action_spec = tf.nest.flatten(action_spec)\n",
            "    continuous_specs = [tensor_spec.is_continuous(s) for s in flat_action_spec]\n",
            "\n",
            "    if not flat_action_spec:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\behavioral_cloning\\behavioral_cloning_agent.py",
        "line_number": 216,
        "API": ".shape(",
        "context": [
            "\n",
            "  def _discrete_loss(self, experience, training=False):\n",
            "    batch_size = (\n",
            "        tf.compat.dimension_value(experience.step_type.shape[0]) or\n",
            "        tf.shape(experience.step_type)[0])\n",
            "\n",
            "    network_state = self._cloning_network.get_initial_state(batch_size)\n",
            "    action, _ = self._cloning_network(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\behavioral_cloning\\behavioral_cloning_agent.py",
        "line_number": 234,
        "API": ".sparse_softmax_cross_entropy_with_logits(",
        "context": [
            "\n",
            "    def loss(action, bc_logits, spec):\n",
            "      # Subtract the minimum so that we get a proper cross entropy loss on\n",
            "      # [0, maximum - minimum).\n",
            "      return tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
            "          labels=action - spec.minimum, logits=bc_logits)\n",
            "\n",
            "    losses = tf.nest.map_structure(loss, experience.action, bc_logits,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\behavioral_cloning\\behavioral_cloning_agent.py",
        "line_number": 239,
        "API": ".flatten(",
        "context": [
            "          labels=action - spec.minimum, logits=bc_logits)\n",
            "\n",
            "    losses = tf.nest.map_structure(loss, experience.action, bc_logits,\n",
            "                                   self.action_spec)\n",
            "    losses = tf.nest.flatten(losses)\n",
            "    return tf.add_n(losses)\n",
            "\n",
            "  def _continuous_loss_fn(self, experience, training: bool = False):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\behavioral_cloning\\behavioral_cloning_agent.py",
        "line_number": 245,
        "API": ".shape(",
        "context": [
            "\n",
            "  def _continuous_loss_fn(self, experience, training: bool = False):\n",
            "    batch_size = (\n",
            "        tf.compat.dimension_value(experience.step_type.shape[0]) or\n",
            "        tf.shape(experience.step_type)[0])\n",
            "    network_state = self._cloning_network.get_initial_state(batch_size)\n",
            "    bc_output, _ = self._cloning_network(\n",
            "        experience.observation,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\behavioral_cloning\\behavioral_cloning_agent.py",
        "line_number": 258,
        "API": ".map_structure(",
        "context": [
            "      bc_action = bc_output.sample()\n",
            "    else:\n",
            "      bc_action = bc_output\n",
            "\n",
            "    losses = tf.nest.map_structure(tf.losses.mse, experience.action, bc_action)\n",
            "    losses = tf.nest.flatten(losses)\n",
            "    return tf.add_n(losses)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\behavioral_cloning\\behavioral_cloning_agent.py",
        "line_number": 267,
        "API": ".flatten(",
        "context": [
            "                         epsilon_greedy):\n",
            "    self._bc_loss_fn = loss_fn or self._discrete_loss\n",
            "\n",
            "    if any(isinstance(d, distribution_utils.DistributionSpecV2) for\n",
            "           d in tf.nest.flatten([self._network_output_spec])):\n",
            "      # If the output of the cloning network contains a distribution.\n",
            "      base_policy = actor_policy.ActorPolicy(time_step_spec, action_spec,\n",
            "                                             self._cloning_network)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 180,
        "API": ".format(",
        "context": [
            "        num_atoms = net.num_atoms\n",
            "      except AttributeError:\n",
            "        raise TypeError('Expected {} to have property `num_atoms`, but it '\n",
            "                        'doesn\\'t. (Note: you likely want to use a '\n",
            "                        'CategoricalQNetwork.) Network is: {}'.format(\n",
            "                            label, net))\n",
            "      return num_atoms\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 193,
        "API": ".format(",
        "context": [
            "          target_categorical_q_network, 'target_categorical_q_network')\n",
            "      if self._num_atoms != target_num_atoms:\n",
            "        raise ValueError(\n",
            "            'categorical_q_network and target_categorical_q_network have '\n",
            "            'different numbers of atoms: {} vs. {}'.format(\n",
            "                self._num_atoms, target_num_atoms))\n",
            "\n",
            "    self._min_q_value = min_q_value\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 198,
        "API": ".convert_to_tensor(",
        "context": [
            "                self._num_atoms, target_num_atoms))\n",
            "\n",
            "    self._min_q_value = min_q_value\n",
            "    self._max_q_value = max_q_value\n",
            "    min_q_value = tf.convert_to_tensor(min_q_value, dtype_hint=tf.float32)\n",
            "    max_q_value = tf.convert_to_tensor(max_q_value, dtype_hint=tf.float32)\n",
            "    self._support = tf.linspace(min_q_value, max_q_value, self._num_atoms)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 301,
        "API": ".map_structure(",
        "context": [
            "    else:\n",
            "      # To compute n-step returns, we need the first time steps, the first\n",
            "      # actions, and the last time steps. Therefore we extract the first and\n",
            "      # last transitions from our Trajectory.\n",
            "      first_two_steps = tf.nest.map_structure(lambda x: x[:, :2], experience)\n",
            "      last_two_steps = tf.nest.map_structure(lambda x: x[:, -2:], experience)\n",
            "      time_steps, policy_steps, _ = (\n",
            "          trajectory.experience_to_transitions(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 341,
        "API": ".flatten(",
        "context": [
            "      if batch_squash is not None:\n",
            "        # Squash outer dimensions to a single dimensions for facilitation\n",
            "        # computing the loss the following. Required for supporting temporal\n",
            "        # inputs, for example.\n",
            "        q_logits = batch_squash.flatten(q_logits)\n",
            "        actions = batch_squash.flatten(actions)\n",
            "        next_time_steps = tf.nest.map_structure(batch_squash.flatten,\n",
            "                                                next_time_steps)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 349,
        "API": ".squeeze(",
        "context": [
            "\n",
            "      next_q_distribution = self._next_q_distribution(next_time_steps)\n",
            "\n",
            "      if actions.shape.rank > 1:\n",
            "        actions = tf.squeeze(actions, list(range(1, actions.shape.rank)))\n",
            "\n",
            "      # Project the sample Bellman update \\hat{T}Z_{\\theta} onto the original\n",
            "      # support of Z_{\\theta} (see Figure 1 in paper).\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 354,
        "API": ".tile(",
        "context": [
            "\n",
            "      # Project the sample Bellman update \\hat{T}Z_{\\theta} onto the original\n",
            "      # support of Z_{\\theta} (see Figure 1 in paper).\n",
            "      batch_size = q_logits.shape[0] or tf.shape(q_logits)[0]\n",
            "      tiled_support = tf.tile(self._support, [batch_size])\n",
            "      tiled_support = tf.reshape(tiled_support, [batch_size, self._num_atoms])\n",
            "\n",
            "      if self._n_step_update == 1:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 363,
        "API": ".expand_dims(",
        "context": [
            "        if discount.shape.rank == 1:\n",
            "          # We expect discount to have a shape of [batch_size], while\n",
            "          # tiled_support will have a shape of [batch_size, num_atoms]. To\n",
            "          # multiply these, we add a second dimension of 1 to the discount.\n",
            "          discount = tf.expand_dims(discount, -1)\n",
            "        next_value_term = tf.multiply(discount,\n",
            "                                      tiled_support,\n",
            "                                      name='next_value_term')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 371,
        "API": ".expand_dims(",
        "context": [
            "\n",
            "        reward = next_time_steps.reward\n",
            "        if reward.shape.rank == 1:\n",
            "          # See the explanation above.\n",
            "          reward = tf.expand_dims(reward, -1)\n",
            "        reward_term = tf.multiply(reward_scale_factor,\n",
            "                                  reward,\n",
            "                                  name='reward_term')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 376,
        "API": ".add(",
        "context": [
            "        reward_term = tf.multiply(reward_scale_factor,\n",
            "                                  reward,\n",
            "                                  name='reward_term')\n",
            "\n",
            "        target_support = tf.add(reward_term, gamma * next_value_term,\n",
            "                                name='target_support')\n",
            "      else:\n",
            "        # When computing discounted return, we need to throw out the last time\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 391,
        "API": ".zeros(",
        "context": [
            "\n",
            "        discounted_returns = value_ops.discounted_return(\n",
            "            rewards=rewards,\n",
            "            discounts=discounts,\n",
            "            final_value=tf.zeros([batch_size], dtype=discounts.dtype),\n",
            "            time_major=False,\n",
            "            provide_all_returns=False)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 396,
        "API": ".expand_dims(",
        "context": [
            "            time_major=False,\n",
            "            provide_all_returns=False)\n",
            "\n",
            "        # Convert discounted_returns from [batch_size] to [batch_size, 1]\n",
            "        discounted_returns = tf.expand_dims(discounted_returns, -1)\n",
            "\n",
            "        final_value_discount = tf.reduce_prod(discounts, axis=1)\n",
            "        final_value_discount = tf.expand_dims(final_value_discount, -1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 406,
        "API": ".add(",
        "context": [
            "        # order to check them in unit tests.\n",
            "        self._discounted_returns = discounted_returns\n",
            "        self._final_value_discount = final_value_discount\n",
            "\n",
            "        target_support = tf.add(discounted_returns,\n",
            "                                final_value_discount * tiled_support,\n",
            "                                name='target_support')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 414,
        "API": ".range(",
        "context": [
            "      target_distribution = tf.stop_gradient(project_distribution(\n",
            "          target_support, next_q_distribution, self._support))\n",
            "\n",
            "      # Obtain the current Q-value logits for the selected actions.\n",
            "      indices = tf.range(batch_size)\n",
            "      indices = tf.cast(indices, actions.dtype)\n",
            "      reshaped_actions = tf.stack([indices, actions], axis=-1)\n",
            "      chosen_action_logits = tf.gather_nd(q_logits, reshaped_actions)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 425,
        "API": ".reduce_sum(",
        "context": [
            "      # computing the mean over the batch dimension.\n",
            "      if batch_squash is not None:\n",
            "        target_distribution = batch_squash.unflatten(target_distribution)\n",
            "        chosen_action_logits = batch_squash.unflatten(chosen_action_logits)\n",
            "        critic_loss = tf.reduce_sum(\n",
            "            tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(\n",
            "                labels=target_distribution,\n",
            "                logits=chosen_action_logits),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 454,
        "API": ".scalar(",
        "context": [
            "        with tf.name_scope('distribution_errors'):\n",
            "          common.generate_tensor_summaries(\n",
            "              'distribution_errors', distribution_errors,\n",
            "              step=self.train_step_counter)\n",
            "          tf.compat.v2.summary.scalar(\n",
            "              'mean', tf.reduce_mean(distribution_errors),\n",
            "              step=self.train_step_counter)\n",
            "          tf.compat.v2.summary.scalar(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 460,
        "API": ".scalar(",
        "context": [
            "              step=self.train_step_counter)\n",
            "          tf.compat.v2.summary.scalar(\n",
            "              'mean_abs', tf.reduce_mean(tf.abs(distribution_errors)),\n",
            "              step=self.train_step_counter)\n",
            "          tf.compat.v2.summary.scalar(\n",
            "              'max', tf.reduce_max(distribution_errors),\n",
            "              step=self.train_step_counter)\n",
            "          tf.compat.v2.summary.scalar(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 496,
        "API": ".shape(",
        "context": [
            "    next_target_logits, _ = self._target_q_network(\n",
            "        network_observation,\n",
            "        step_type=next_time_steps.step_type,\n",
            "        training=False)\n",
            "    batch_size = next_target_logits.shape[0] or tf.shape(next_target_logits)[0]\n",
            "    next_target_probabilities = tf.nn.softmax(next_target_logits)\n",
            "    next_target_q_values = tf.reduce_sum(\n",
            "        self._support * next_target_probabilities, axis=-1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 505,
        "API": ".cast(",
        "context": [
            "    # Find the greedy actions using our target greedy policy. This ensures that\n",
            "    # action constraints are respected and helps centralize the greedy logic.\n",
            "    greedy_actions = self._target_greedy_policy.action(\n",
            "        next_time_steps, dummy_state).action\n",
            "    next_qt_argmax = tf.cast(greedy_actions, tf.int32)[:, None]\n",
            "    batch_indices = tf.range(\n",
            "        tf.cast(tf.shape(next_target_q_values)[0], tf.int32))[:, None]\n",
            "    next_qt_argmax = tf.concat([batch_indices, next_qt_argmax], axis=-1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 569,
        "API": ".shape(",
        "context": [
            "  if validate_args:\n",
            "    # Assert that supports and weights have the same shapes.\n",
            "    validate_deps.append(\n",
            "        tf.Assert(\n",
            "            tf.reduce_all(tf.equal(tf.shape(supports), tf.shape(weights))),\n",
            "            [supports, weights]))\n",
            "    # Assert that elements of supports and target_support have the same shape.\n",
            "    validate_deps.append(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 574,
        "API": ".reduce_all(",
        "context": [
            "            [supports, weights]))\n",
            "    # Assert that elements of supports and target_support have the same shape.\n",
            "    validate_deps.append(\n",
            "        tf.Assert(\n",
            "            tf.reduce_all(\n",
            "                tf.equal(tf.shape(supports)[1], tf.shape(target_support))),\n",
            "            [supports, target_support]))\n",
            "    # Assert that target_support has a single dimension.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 580,
        "API": ".shape(",
        "context": [
            "            [supports, target_support]))\n",
            "    # Assert that target_support has a single dimension.\n",
            "    validate_deps.append(\n",
            "        tf.Assert(\n",
            "            tf.equal(tf.size(tf.shape(target_support)), 1), [target_support]))\n",
            "    # Assert that the target_support is monotonically increasing.\n",
            "    validate_deps.append(\n",
            "        tf.Assert(tf.reduce_all(target_support_deltas > 0), [target_support]))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 587,
        "API": ".reduce_all(",
        "context": [
            "        tf.Assert(tf.reduce_all(target_support_deltas > 0), [target_support]))\n",
            "    # Assert that the values in target_support are equally spaced.\n",
            "    validate_deps.append(\n",
            "        tf.Assert(\n",
            "            tf.reduce_all(tf.equal(target_support_deltas, delta_z)),\n",
            "            [target_support]))\n",
            "\n",
            "  with tf.control_dependencies(validate_deps):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 594,
        "API": ".shape(",
        "context": [
            "  with tf.control_dependencies(validate_deps):\n",
            "    # Ex: `v_min, v_max = 4, 8`.\n",
            "    v_min, v_max = target_support[0], target_support[-1]\n",
            "    # Ex: `batch_size = 2`.\n",
            "    batch_size = tf.shape(supports)[0]\n",
            "    # `N` in Eq7.\n",
            "    # Ex: `num_dims = 5`.\n",
            "    num_dims = tf.shape(target_support)[0]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 601,
        "API": ".clip_by_value(",
        "context": [
            "    num_dims = tf.shape(target_support)[0]\n",
            "    # clipped_support = `[\\hat{T}_{z_j}]^{V_max}_{V_min}` in Eq7.\n",
            "    # Ex: `clipped_support = [[[ 4.  4.  4.  6.  8.]]\n",
            "    #                         [[ 4.  4.  4.  5.  6.]]]`.\n",
            "    clipped_support = tf.clip_by_value(supports, v_min, v_max)[:, None, :]\n",
            "    # Ex: `tiled_support = [[[[ 4.  4.  4.  6.  8.]\n",
            "    #                         [ 4.  4.  4.  6.  8.]\n",
            "    #                         [ 4.  4.  4.  6.  8.]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 612,
        "API": ".tile(",
        "context": [
            "    #                         [ 4.  4.  4.  5.  6.]\n",
            "    #                         [ 4.  4.  4.  5.  6.]\n",
            "    #                         [ 4.  4.  4.  5.  6.]\n",
            "    #                         [ 4.  4.  4.  5.  6.]]]]`.\n",
            "    tiled_support = tf.tile([clipped_support], [1, 1, num_dims, 1])\n",
            "    # Ex: `reshaped_target_support = [[[ 4.]\n",
            "    #                                  [ 5.]\n",
            "    #                                  [ 6.]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 623,
        "API": ".tile(",
        "context": [
            "    #                                  [ 5.]\n",
            "    #                                  [ 6.]\n",
            "    #                                  [ 7.]\n",
            "    #                                  [ 8.]]]`.\n",
            "    reshaped_target_support = tf.tile(target_support[:, None], [batch_size, 1])\n",
            "    reshaped_target_support = tf.reshape(reshaped_target_support,\n",
            "                                         [batch_size, num_dims, 1])\n",
            "    # numerator = `|clipped_support - z_i|` in Eq7.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 637,
        "API": ".abs(",
        "context": [
            "    #                     [ 1.  1.  1.  0.  1.]\n",
            "    #                     [ 2.  2.  2.  1.  0.]\n",
            "    #                     [ 3.  3.  3.  2.  1.]\n",
            "    #                     [ 4.  4.  4.  3.  2.]]]]`.\n",
            "    numerator = tf.abs(tiled_support - reshaped_target_support)\n",
            "    quotient = 1 - (numerator / delta_z)\n",
            "    # clipped_quotient = `[1 - numerator / (\\Delta z)]_0^1` in Eq7.\n",
            "    # Ex: `clipped_quotient = [[[[ 1.  1.  1.  0.  0.]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 650,
        "API": ".clip_by_value(",
        "context": [
            "    #                            [ 0.  0.  0.  1.  0.]\n",
            "    #                            [ 0.  0.  0.  0.  1.]\n",
            "    #                            [ 0.  0.  0.  0.  0.]\n",
            "    #                            [ 0.  0.  0.  0.  0.]]]]`.\n",
            "    clipped_quotient = tf.clip_by_value(quotient, 0, 1)\n",
            "    # Ex: `weights = [[ 0.1  0.6  0.1  0.1  0.1]\n",
            "    #                 [ 0.1  0.2  0.5  0.1  0.1]]`.\n",
            "    weights = weights[:, None, :]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\categorical_dqn_agent.py",
        "line_number": 669,
        "API": ".reduce_sum(",
        "context": [
            "    #                      [ 0.   0.   0.   0.  0. ]]]]`.\n",
            "    inner_prod = clipped_quotient * weights\n",
            "    # Ex: `projection = [[ 0.8 0.0 0.1 0.0 0.1]\n",
            "    #                    [ 0.8 0.1 0.1 0.0 0.0]]`.\n",
            "    projection = tf.reduce_sum(inner_prod, 3)\n",
            "    projection = tf.reshape(projection, [batch_size, num_dims])\n",
            "    return projection\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\examples\\train_eval_atari.py",
        "line_number": 80,
        "API": ".getenv(",
        "context": [
            "from tf_agents.trajectories import trajectory\n",
            "from tf_agents.utils import common\n",
            "from tf_agents.utils import timer\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_string('game_name', 'Pong', 'Name of Atari game to run.')\n",
            "flags.DEFINE_integer('num_iterations', None,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\examples\\train_eval_atari.py",
        "line_number": 124,
        "API": ".cast(",
        "context": [
            "  def num_atoms(self):\n",
            "    return self._categorical_q_network.num_atoms\n",
            "\n",
            "  def call(self, observation, step_type=None, network_state=()):\n",
            "    state = tf.cast(observation, tf.float32)\n",
            "    # We divide the grayscale pixel values by 255 here rather than storing\n",
            "    # normalized values beause uint8s are 4x cheaper to store than float32s.\n",
            "    # TODO(b/129805821): handle the division by 255 for train_eval_atari.py in\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\examples\\train_eval_atari.py",
        "line_number": 136,
        "API": ".info(",
        "context": [
            "\n",
            "\n",
            "def log_metric(metric, prefix):\n",
            "  tag = common.join_scope(prefix, metric.name)\n",
            "  logging.info('%s', '{0} = {1}'.format(tag, metric.result()))\n",
            "\n",
            "\n",
            "@gin.configurable\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\examples\\train_eval_atari.py",
        "line_number": 248,
        "API": ".join(",
        "context": [
            "                          'AtariPreprocessing.terminal_on_life_loss'),\n",
            "                         terminal_on_life_loss)\n",
            "\n",
            "    root_dir = os.path.expanduser(root_dir)\n",
            "    train_dir = os.path.join(root_dir, 'train')\n",
            "    eval_dir = os.path.join(root_dir, 'eval')\n",
            "\n",
            "    train_summary_writer = tf.compat.v2.summary.create_file_writer(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\examples\\train_eval_atari.py",
        "line_number": 269,
        "API": ".equal(",
        "context": [
            "      ]\n",
            "\n",
            "    self._global_step = tf.compat.v1.train.get_or_create_global_step()\n",
            "    with tf.compat.v2.summary.record_if(\n",
            "        lambda: tf.math.equal(self._global_step % self._summary_interval, 0)):\n",
            "      self._env = suite_atari.load(\n",
            "          env_name,\n",
            "          max_episode_steps=max_episode_frames / ATARI_FRAME_SKIP,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\examples\\train_eval_atari.py",
        "line_number": 280,
        "API": ".device(",
        "context": [
            "      observation_spec = tensor_spec.from_spec(self._env.observation_spec())\n",
            "      time_step_spec = ts.time_step_spec(observation_spec)\n",
            "      action_spec = tensor_spec.from_spec(self._env.action_spec())\n",
            "\n",
            "      with tf.device('/cpu:0'):\n",
            "        epsilon = tf.compat.v1.train.polynomial_decay(\n",
            "            1.0,\n",
            "            self._global_step,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\examples\\train_eval_atari.py",
        "line_number": 287,
        "API": ".device(",
        "context": [
            "            self._global_step,\n",
            "            epsilon_decay_period / ATARI_FRAME_SKIP / self._update_period,\n",
            "            end_learning_rate=epsilon_greedy)\n",
            "\n",
            "      with tf.device('/gpu:0'):\n",
            "        optimizer = tf.compat.v1.train.RMSPropOptimizer(\n",
            "            learning_rate=learning_rate,\n",
            "            decay=0.95,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\examples\\train_eval_atari.py",
        "line_number": 332,
        "API": ".device(",
        "context": [
            "            py_time_step_spec, py_action_spec, py_time_step_spec)\n",
            "        self._replay_buffer = py_hashed_replay_buffer.PyHashedReplayBuffer(\n",
            "            data_spec=data_spec, capacity=replay_buffer_capacity)\n",
            "\n",
            "      with tf.device('/cpu:0'):\n",
            "        ds = self._replay_buffer.as_dataset(\n",
            "            sample_batch_size=batch_size, num_steps=n_step_update + 1)\n",
            "        ds = ds.prefetch(4)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\examples\\train_eval_atari.py",
        "line_number": 338,
        "API": ".device(",
        "context": [
            "            sample_batch_size=batch_size, num_steps=n_step_update + 1)\n",
            "        ds = ds.prefetch(4)\n",
            "        ds = ds.apply(tf.data.experimental.prefetch_to_device('/gpu:0'))\n",
            "\n",
            "      with tf.device('/gpu:0'):\n",
            "        self._ds_itr = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
            "        experience = self._ds_itr.get_next()\n",
            "        self._train_op = agent.train(experience)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\examples\\train_eval_atari.py",
        "line_number": 365,
        "API": ".placeholder(",
        "context": [
            "\n",
            "        # Summaries written from python should run every time they are\n",
            "        # generated.\n",
            "        with tf.compat.v2.summary.record_if(True):\n",
            "          self._steps_per_second_ph = tf.compat.v1.placeholder(\n",
            "              tf.float32, shape=(), name='steps_per_sec_ph')\n",
            "          self._steps_per_second_summary = tf.compat.v2.summary.scalar(\n",
            "              name='global_steps_per_sec', data=self._steps_per_second_ph,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\examples\\train_eval_atari.py",
        "line_number": 397,
        "API": ".join(",
        "context": [
            "            metrics=metric_utils.MetricsGroup(\n",
            "                self._train_metrics + self._train_phase_metrics +\n",
            "                [self._iteration_metric], 'train_metrics'))\n",
            "        self._policy_checkpointer = common.Checkpointer(\n",
            "            ckpt_dir=os.path.join(train_dir, 'policy'),\n",
            "            policy=agent.policy,\n",
            "            global_step=self._global_step)\n",
            "        self._rb_checkpointer = common.Checkpointer(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\examples\\train_eval_atari.py",
        "line_number": 454,
        "API": ".save(",
        "context": [
            "            log_metric(metric, prefix='Eval/Metrics')\n",
            "\n",
            "        self._iteration_metric()\n",
            "\n",
            "        self._train_checkpointer.save(global_step=global_step_val)\n",
            "        self._policy_checkpointer.save(global_step=global_step_val)\n",
            "        self._rb_checkpointer.save(global_step=global_step_val)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\examples\\train_eval_atari.py",
        "line_number": 479,
        "API": ".save(",
        "context": [
            "    self._timed_at_step = global_step_val\n",
            "\n",
            "    # Call save to initialize the save_counter (need to do this before\n",
            "    # finalizing the graph).\n",
            "    self._train_checkpointer.save(global_step=global_step_val)\n",
            "    self._policy_checkpointer.save(global_step=global_step_val)\n",
            "    self._rb_checkpointer.save(global_step=global_step_val)\n",
            "    sess.run(self._train_summary_writer.init())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\examples\\train_eval_atari.py",
        "line_number": 485,
        "API": ".init(",
        "context": [
            "    self._rb_checkpointer.save(global_step=global_step_val)\n",
            "    sess.run(self._train_summary_writer.init())\n",
            "\n",
            "    if self._do_eval:\n",
            "      sess.run(self._eval_summary_writer.init())\n",
            "\n",
            "  def _initial_collect(self):\n",
            "    \"\"\"Collect initial experience before training begins.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\examples\\train_eval_atari.py",
        "line_number": 502,
        "API": ".info(",
        "context": [
            "      next_time_step = self._env.step(action_step.action)\n",
            "      self._replay_buffer.add_batch(trajectory.from_transition(\n",
            "          time_step, action_step, next_time_step))\n",
            "      time_step = next_time_step\n",
            "    logging.info('Done.')\n",
            "\n",
            "  def _run_episode(self, sess, metric_observers, train=False):\n",
            "    \"\"\"Run a single episode.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\examples\\train_eval_atari.py",
        "line_number": 552,
        "API": ".array(",
        "context": [
            "      next_time_step = self._env.step(action_step.action)\n",
            "      traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
            "\n",
            "    if next_time_step.is_last() and not self.game_over():\n",
            "      traj = traj._replace(discount=np.array([1.0], dtype=np.float32))\n",
            "\n",
            "    if train:\n",
            "      self._store_to_rb(traj)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\examples\\train_eval_atari.py",
        "line_number": 600,
        "API": ".info(",
        "context": [
            "\n",
            "  def _maybe_log(self, sess, global_step_val, total_loss):\n",
            "    \"\"\"Log some stats if global_step_val is a multiple of log_interval.\"\"\"\n",
            "    if global_step_val % self._log_interval == 0:\n",
            "      logging.info('step = %d, loss = %f', global_step_val, total_loss.loss)\n",
            "      logging.info('%s', 'action_time = {}'.format(self._action_timer.value()))\n",
            "      logging.info('%s', 'step_time = {}'.format(self._step_timer.value()))\n",
            "      logging.info('%s', 'observer_time = {}'.format(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\examples\\train_eval_atari.py",
        "line_number": 606,
        "API": ".value(",
        "context": [
            "      logging.info('%s', 'step_time = {}'.format(self._step_timer.value()))\n",
            "      logging.info('%s', 'observer_time = {}'.format(\n",
            "          self._observer_timer.value()))\n",
            "      steps_per_sec = ((global_step_val - self._timed_at_step) /\n",
            "                       (self._collect_timer.value()\n",
            "                        + self._train_timer.value()))\n",
            "      sess.run(self._steps_per_second_summary,\n",
            "               feed_dict={self._steps_per_second_ph: steps_per_sec})\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\examples\\train_eval_atari.py",
        "line_number": 611,
        "API": ".info(",
        "context": [
            "                        + self._train_timer.value()))\n",
            "      sess.run(self._steps_per_second_summary,\n",
            "               feed_dict={self._steps_per_second_ph: steps_per_sec})\n",
            "      logging.info('%.3f steps/sec', steps_per_sec)\n",
            "      logging.info('%s', 'collect_time = {}, train_time = {}'.format(\n",
            "          self._collect_timer.value(), self._train_timer.value()))\n",
            "      for metric in self._train_metrics:\n",
            "        log_metric(metric, prefix='Train/Metrics')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\categorical_dqn\\examples\\train_eval_atari.py",
        "line_number": 642,
        "API": ".set_verbosity(",
        "context": [
            "  return run_args\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  logging.set_verbosity(logging.INFO)\n",
            "  tf.compat.v1.enable_resource_variables()\n",
            "  TrainEval(FLAGS.root_dir, suite_atari.game(name=FLAGS.game_name),\n",
            "            **get_run_args()).run()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 202,
        "API": ".log(",
        "context": [
            "        train_step_counter=train_step_counter,\n",
            "        name=name)\n",
            "    self._use_lagrange_cql_alpha = use_lagrange_cql_alpha\n",
            "    if self._use_lagrange_cql_alpha:\n",
            "      self._log_cql_alpha = tf.Variable(tf.math.log(cql_alpha), trainable=True)\n",
            "      self._cql_tau = cql_tau\n",
            "      self._cql_alpha_optimizer = tf.keras.optimizers.Adam(\n",
            "          learning_rate=cql_alpha_learning_rate)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 224,
        "API": ".flatten(",
        "context": [
            "\n",
            "  def _check_action_spec(self, action_spec):\n",
            "    super(CqlSacAgent, self)._check_action_spec(action_spec)\n",
            "\n",
            "    flat_action_spec = tf.nest.flatten(action_spec)\n",
            "    if len(flat_action_spec) > 1:\n",
            "      raise ValueError(\n",
            "          'Only single action specs are supported now, but action spec is: {}'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 315,
        "API": ".constant(",
        "context": [
            "    # If the expected difference in Q-values is less than tau, alpha\n",
            "    # will adjust to be closer to 0. If the difference is higher than tau,\n",
            "    # alpha is likely to take on high values and more aggressively penalize\n",
            "    # Q-values.\n",
            "    cql_alpha_loss = tf.constant(0.)\n",
            "    if self._use_lagrange_cql_alpha:\n",
            "      cql_alpha_variable = [self._log_cql_alpha]\n",
            "      with tf.GradientTape(watch_accessed_variables=False) as tape:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 328,
        "API": ".scalar(",
        "context": [
            "      self._apply_gradients(cql_alpha_gradients, cql_alpha_variable,\n",
            "                            self._cql_alpha_optimizer)\n",
            "\n",
            "    with tf.name_scope('Losses'):\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='critic_loss', data=critic_loss, step=self.train_step_counter)\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='actor_loss', data=actor_loss, step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 334,
        "API": ".scalar(",
        "context": [
            "      tf.compat.v2.summary.scalar(\n",
            "          name='actor_loss', data=actor_loss, step=self.train_step_counter)\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='alpha_loss', data=alpha_loss, step=self.train_step_counter)\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='cql_loss', data=cql_loss, step=self.train_step_counter)\n",
            "      if self._use_lagrange_cql_alpha:\n",
            "        tf.compat.v2.summary.scalar(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 341,
        "API": ".scalar(",
        "context": [
            "        tf.compat.v2.summary.scalar(\n",
            "            name='cql_alpha_loss',\n",
            "            data=cql_alpha_loss,\n",
            "            step=self.train_step_counter)\n",
            "    tf.compat.v2.summary.scalar(\n",
            "        name='cql_alpha', data=cql_alpha, step=self.train_step_counter)\n",
            "    tf.compat.v2.summary.scalar(\n",
            "        name='sac_alpha', data=tf.exp(self._log_alpha),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 365,
        "API": ".transpose(",
        "context": [
            "\n",
            "  def _transpose_tile_and_batch_dims(\n",
            "      self, original_tensor: types.Tensor) -> types.Tensor:\n",
            "    \"\"\"Transposes [tile, batch, ...] to [batch, tile, ...].\"\"\"\n",
            "    return tf.transpose(\n",
            "        original_tensor, [1, 0] + list(range(2, len(original_tensor.shape))))\n",
            "\n",
            "  def _actions_and_log_probs(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 384,
        "API": ".map_structure(",
        "context": [
            "      action_distribution = self._policy.distribution(\n",
            "          time_steps, policy_state=policy_state).action\n",
            "\n",
            "    # Sample actions and log_pis from transformed distribution.\n",
            "    actions = tf.nest.map_structure(\n",
            "        lambda d: d.sample((), seed=self._action_seed_stream()),\n",
            "        action_distribution)\n",
            "    log_pi = common.log_probability(action_distribution, actions,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 408,
        "API": ".map_structure(",
        "context": [
            "    else:\n",
            "      action_distribution = self._policy.distribution(\n",
            "          time_steps, policy_state=policy_state).action\n",
            "\n",
            "    actions = tf.nest.map_structure(\n",
            "        lambda d: d.sample(num_action_samples, seed=self._action_seed_stream()),\n",
            "        action_distribution)\n",
            "    log_pi = common.log_probability(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 464,
        "API": ".reshape(",
        "context": [
            "    # Optionally reshape to [batch_size, num_cql_samples, q_value_dim].\n",
            "    if reshape_batch_size is not None:\n",
            "      reshaped_dims = [reshape_batch_size, self._num_cql_samples] + (\n",
            "          q_values1.shape.as_list()[1:])\n",
            "      q_values1 = tf.reshape(q_values1, reshaped_dims)\n",
            "      q_values2 = tf.reshape(q_values2, reshaped_dims)\n",
            "\n",
            "    q_values1 = tf.expand_dims(q_values1, axis=-1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 477,
        "API": ".scalar(",
        "context": [
            "    \"\"\"Generates summaries for _cql_loss.\"\"\"\n",
            "    if self._debug_summaries:\n",
            "      with tf.name_scope('cql_loss'):\n",
            "        for key in debug_summaries_dict:\n",
            "          tf.compat.v2.summary.scalar(\n",
            "              name=key,\n",
            "              data=debug_summaries_dict[key],\n",
            "              step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 527,
        "API": ".reduce_mean(",
        "context": [
            "        step_type,\n",
            "        reshape_batch_size=batch_size,\n",
            "        training=False)\n",
            "    debug_summaries_dict = {}\n",
            "    debug_summaries_dict['q_estimates1'] = tf.reduce_mean(q_estimates1)\n",
            "    debug_summaries_dict['q_estimates2'] = tf.reduce_mean(q_estimates2)\n",
            "\n",
            "    # We're supposed to be taking an unweighted sum of Q-values of actions\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 546,
        "API": ".flatten(",
        "context": [
            "    policy_log_probs2 = (q_estimates2 * self._softmax_temperature\n",
            "                        ) - sampled_actions_log_probs[..., None]\n",
            "\n",
            "    # Sample self._num_cql_samples from the uniform-at-random distribution.\n",
            "    flattened_action_spec = tf.nest.flatten(self.action_spec)[0]\n",
            "    uniform_actions = tf.random.uniform(\n",
            "        tf.shape(sampled_actions),\n",
            "        minval=flattened_action_spec.minimum,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 555,
        "API": ".reduce_mean(",
        "context": [
            "        seed=self._action_seed_stream())\n",
            "    target_input = (observation, uniform_actions)\n",
            "    q_uniform1, q_uniform2 = self._get_q_values(target_input, step_type,\n",
            "                                                batch_size, training=False)\n",
            "    debug_summaries_dict['q_uniform1'] = tf.reduce_mean(q_uniform1)\n",
            "    debug_summaries_dict['q_uniform2'] = tf.reduce_mean(q_uniform2)\n",
            "\n",
            "    # Uniform density is `(1/range)^dimension`, so the log probability of the\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 562,
        "API": ".reduce_sum(",
        "context": [
            "    # Uniform density is `(1/range)^dimension`, so the log probability of the\n",
            "    # uniform distribution is `-log(range)*dimension`.\n",
            "    # Once again, we subtract this from the Q-value to correct for drawing\n",
            "    # from this distribution and contribute to an unweighted sum of Q-values.\n",
            "    uniform_actions_log_probs = tf.reduce_sum(-tf.math.log(\n",
            "        flattened_action_spec.maximum -\n",
            "        flattened_action_spec.minimum)) * flattened_action_spec.shape[0]\n",
            "    uniform_log_probs1 = (q_uniform1 *\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 575,
        "API": ".concat(",
        "context": [
            "    # can't tractably compute the exact Q-value in a continuous action space.\n",
            "    # Based on the first part of equation (4):\n",
            "    # ```log_sum_exp(Q(s, a'))```\n",
            "    # Concatenates and collapses along the self._num_cql_samples dimension.\n",
            "    combined_log_probs1 = tf.concat([policy_log_probs1, uniform_log_probs1],\n",
            "                                    axis=1)\n",
            "    combined_log_probs2 = tf.concat([policy_log_probs2, uniform_log_probs2],\n",
            "                                    axis=1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 591,
        "API": ".reduce_mean(",
        "context": [
            "        target_input,\n",
            "        time_steps.step_type,\n",
            "        reshape_batch_size=None,\n",
            "        training=training)\n",
            "    debug_summaries_dict['q_original1'] = tf.reduce_mean(q_original1)\n",
            "    debug_summaries_dict['q_original2'] = tf.reduce_mean(q_original2)\n",
            "\n",
            "    cql_loss = tf.reduce_mean((logsumexp1 - q_original1) +\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 601,
        "API": ".function(",
        "context": [
            "    self._cql_loss_debug_summaries(debug_summaries_dict)\n",
            "\n",
            "    return cql_loss\n",
            "\n",
            "  @common.function(autograph=True)\n",
            "  def actor_loss(self,\n",
            "                 time_steps: ts.TimeStep,\n",
            "                 actions: types.Tensor,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 633,
        "API": ".exp(",
        "context": [
            "      if self.train_step_counter < self._num_bc_steps:\n",
            "        distribution, _ = self._actor_network(time_steps.observation,\n",
            "                                              time_steps.step_type, ())\n",
            "        actor_log_prob = distribution.log_prob(actions)\n",
            "        actor_loss = tf.exp(self._log_alpha) * sampled_log_pi - actor_log_prob\n",
            "        target_q_values = tf.zeros(tf.shape(sampled_log_pi))\n",
            "      else:\n",
            "        target_input = (time_steps.observation, sampled_actions)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 641,
        "API": ".minimum(",
        "context": [
            "        target_q_values1, _ = self._critic_network_1(\n",
            "            target_input, time_steps.step_type, training=False)\n",
            "        target_q_values2, _ = self._critic_network_2(\n",
            "            target_input, time_steps.step_type, training=False)\n",
            "        target_q_values = tf.minimum(target_q_values1, target_q_values2)\n",
            "        actor_loss = tf.exp(self._log_alpha) * sampled_log_pi - target_q_values\n",
            "\n",
            "      if actor_loss.shape.rank > 1:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 646,
        "API": ".reduce_sum(",
        "context": [
            "        actor_loss = tf.exp(self._log_alpha) * sampled_log_pi - target_q_values\n",
            "\n",
            "      if actor_loss.shape.rank > 1:\n",
            "        # Sum over the time dimension.\n",
            "        actor_loss = tf.reduce_sum(\n",
            "            actor_loss, axis=range(1, actor_loss.shape.rank))\n",
            "      reg_loss = self._actor_network.losses if self._actor_network else None\n",
            "      agg_loss = common.aggregate_losses(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 665,
        "API": ".clip_by_value(",
        "context": [
            "    \"\"\"Returns CQL alpha.\"\"\"\n",
            "    if self._use_lagrange_cql_alpha:\n",
            "      log_cql_alpha = self._log_cql_alpha\n",
            "      if self._log_cql_alpha_clipping is not None:\n",
            "        log_cql_alpha = tf.clip_by_value(\n",
            "            log_cql_alpha,\n",
            "            clip_value_min=self._log_cql_alpha_clipping[0],\n",
            "            clip_value_max=self._log_cql_alpha_clipping[1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 672,
        "API": ".convert_to_tensor(",
        "context": [
            "            clip_value_max=self._log_cql_alpha_clipping[1])\n",
            "      cql_alpha = tf.math.exp(log_cql_alpha)\n",
            "      return cql_alpha\n",
            "    else:\n",
            "      return tf.convert_to_tensor(self._cql_alpha)\n",
            "\n",
            "  def _critic_loss_with_optional_entropy_term(\n",
            "      self,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 729,
        "API": ".minimum(",
        "context": [
            "      target_q_values1, unused_network_state1 = self._target_critic_network_1(\n",
            "          target_input, next_time_steps.step_type, training=False)\n",
            "      target_q_values2, unused_network_state2 = self._target_critic_network_2(\n",
            "          target_input, next_time_steps.step_type, training=False)\n",
            "      target_q_values = tf.minimum(target_q_values1, target_q_values2)\n",
            "\n",
            "      if self._include_critic_entropy_term:\n",
            "        target_q_values -= (tf.exp(self._log_alpha) * next_log_pis)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 736,
        "API": ".normal(",
        "context": [
            "        target_q_values -= (tf.exp(self._log_alpha) * next_log_pis)\n",
            "\n",
            "      reward = next_time_steps.reward\n",
            "      if self._reward_noise_variance > 0:\n",
            "        reward_noise = tf.random.normal(\n",
            "            tf.shape(reward),\n",
            "            0.0,\n",
            "            self._reward_noise_variance,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 743,
        "API": ".stop_gradient(",
        "context": [
            "            self._reward_noise_variance,\n",
            "            seed=self._reward_seed_stream())\n",
            "        reward += reward_noise\n",
            "\n",
            "      td_targets = tf.stop_gradient(reward_scale_factor * reward + gamma *\n",
            "                                    next_time_steps.discount * target_q_values)\n",
            "\n",
            "      pred_input = (time_steps.observation, actions)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\cql\\cql_sac_agent.py",
        "line_number": 757,
        "API": ".reduce_sum(",
        "context": [
            "      critic_loss = critic_loss1 + critic_loss2\n",
            "\n",
            "      if critic_loss.shape.rank > 1:\n",
            "        # Sum over the time dimension.\n",
            "        critic_loss = tf.reduce_sum(\n",
            "            critic_loss, axis=range(1, critic_loss.shape.rank))\n",
            "\n",
            "      agg_loss = common.aggregate_losses(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\actor_network.py",
        "line_number": 84,
        "API": ".flatten(",
        "context": [
            "        name=name)\n",
            "\n",
            "    output_tensor_spec = tensor_spec.from_spec(output_tensor_spec)\n",
            "\n",
            "    if len(tf.nest.flatten(input_tensor_spec)) > 1:\n",
            "      raise ValueError('Only a single observation is supported by this network')\n",
            "\n",
            "    flat_action_spec = tf.nest.flatten(output_tensor_spec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\actor_network.py",
        "line_number": 122,
        "API": ".flatten(",
        "context": [
            "    self._output_tensor_spec = output_tensor_spec\n",
            "\n",
            "  def call(self, observations, step_type=(), network_state=(), training=False):\n",
            "    del step_type  # unused.\n",
            "    observations = tf.nest.flatten(observations)\n",
            "    output = tf.cast(observations[0], tf.float32)\n",
            "    for layer in self._mlp_layers:\n",
            "      output = layer(output, training=training)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\actor_rnn_network.py",
        "line_number": 74,
        "API": ".flatten(",
        "context": [
            "\n",
            "    Raises:\n",
            "      ValueError: If `input_tensor_spec` contains more than one observation.\n",
            "    \"\"\"\n",
            "    if len(tf.nest.flatten(input_tensor_spec)) > 1:\n",
            "      raise ValueError('Only a single observation is supported by this network')\n",
            "\n",
            "    input_layers = utils.mlp_layers(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\actor_rnn_network.py",
        "line_number": 91,
        "API": ".map_structure(",
        "context": [
            "    else:\n",
            "      cell = tf.keras.layers.StackedRNNCells(\n",
            "          [tf.keras.layers.LSTMCell(size) for size in lstm_size])\n",
            "\n",
            "    state_spec = tf.nest.map_structure(\n",
            "        functools.partial(\n",
            "            tensor_spec.TensorSpec, dtype=tf.float32,\n",
            "            name='network_state_spec'), list(cell.state_size))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\actor_rnn_network.py",
        "line_number": 99,
        "API": ".flatten(",
        "context": [
            "\n",
            "    output_layers = utils.mlp_layers(fc_layer_params=output_fc_layer_params,\n",
            "                                     name='output')\n",
            "\n",
            "    flat_action_spec = tf.nest.flatten(output_tensor_spec)\n",
            "    action_layers = [\n",
            "        tf.keras.layers.Dense(\n",
            "            single_action_spec.shape.num_elements(),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\actor_rnn_network.py",
        "line_number": 133,
        "API": ".expand_dims(",
        "context": [
            "\n",
            "    has_time_dim = num_outer_dims == 2\n",
            "    if not has_time_dim:\n",
            "      # Add a time dimension to the inputs.\n",
            "      observation = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1),\n",
            "                                          observation)\n",
            "      step_type = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1),\n",
            "                                        step_type)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\actor_rnn_network.py",
        "line_number": 138,
        "API": ".cast(",
        "context": [
            "                                          observation)\n",
            "      step_type = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1),\n",
            "                                        step_type)\n",
            "\n",
            "    states = tf.cast(tf.nest.flatten(observation)[0], tf.float32)\n",
            "    batch_squash = utils.BatchSquash(2)  # Squash B, and T dims.\n",
            "    states = batch_squash.flatten(states)  # [B, T, ...] -> [B x T, ...]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\actor_rnn_network.py",
        "line_number": 148,
        "API": ".equal(",
        "context": [
            "\n",
            "    states = batch_squash.unflatten(states)  # [B x T, ...] -> [B, T, ...]\n",
            "\n",
            "    with tf.name_scope('reset_mask'):\n",
            "      reset_mask = tf.equal(step_type, time_step.StepType.FIRST)\n",
            "    # Unroll over the time sequence.\n",
            "    states, network_state = self._dynamic_unroll(\n",
            "        states,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\actor_rnn_network.py",
        "line_number": 156,
        "API": ".flatten(",
        "context": [
            "        reset_mask=reset_mask,\n",
            "        initial_state=network_state,\n",
            "        training=training)\n",
            "\n",
            "    states = batch_squash.flatten(states)  # [B, T, ...] -> [B x T, ...]\n",
            "\n",
            "    for layer in self._output_layers:\n",
            "      states = layer(states, training=training)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\actor_rnn_network.py",
        "line_number": 167,
        "API": ".squeeze(",
        "context": [
            "      action = layer(states, training=training)\n",
            "      action = common.scale_to_spec(action, spec)\n",
            "      action = batch_squash.unflatten(action)  # [B x T, ...] -> [B, T, ...]\n",
            "      if not has_time_dim:\n",
            "        action = tf.squeeze(action, axis=1)\n",
            "      actions.append(action)\n",
            "\n",
            "    output_actions = tf.nest.pack_sequence_as(self._output_tensor_spec, actions)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\critic_network.py",
        "line_number": 108,
        "API": ".flatten(",
        "context": [
            "        name=name)\n",
            "\n",
            "    observation_spec, action_spec = input_tensor_spec\n",
            "\n",
            "    if len(tf.nest.flatten(observation_spec)) > 1:\n",
            "      raise ValueError('Only a single observation is supported by this network')\n",
            "\n",
            "    flat_action_spec = tf.nest.flatten(action_spec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\critic_network.py",
        "line_number": 159,
        "API": ".cast(",
        "context": [
            "\n",
            "  def call(self, inputs, step_type=(), network_state=(), training=False):\n",
            "    observations, actions = inputs\n",
            "    del step_type  # unused.\n",
            "    observations = tf.cast(tf.nest.flatten(observations)[0], tf.float32)\n",
            "    for layer in self._observation_layers:\n",
            "      observations = layer(observations, training=training)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\critic_network.py",
        "line_number": 167,
        "API": ".concat(",
        "context": [
            "    actions = tf.cast(tf.nest.flatten(actions)[0], tf.float32)\n",
            "    for layer in self._action_layers:\n",
            "      actions = layer(actions, training=training)\n",
            "\n",
            "    joint = tf.concat([observations, actions], 1)\n",
            "    for layer in self._joint_layers:\n",
            "      joint = layer(joint, training=training)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\critic_rnn_network.py",
        "line_number": 101,
        "API": ".flatten(",
        "context": [
            "                       'lstm_size.')\n",
            "\n",
            "    observation_spec, action_spec = input_tensor_spec\n",
            "\n",
            "    if len(tf.nest.flatten(observation_spec)) > 1:\n",
            "      raise ValueError(\n",
            "          'Only a single observation is supported by this network.')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\critic_rnn_network.py",
        "line_number": 155,
        "API": ".map_structure(",
        "context": [
            "      counter[0] += 1\n",
            "      return tensor_spec.TensorSpec(\n",
            "          size, dtype=tf.float32, name='network_state_%d' % counter[0])\n",
            "\n",
            "    state_spec = tf.nest.map_structure(create_spec,\n",
            "                                       lstm_network.cell.state_size)\n",
            "\n",
            "    output_layers = utils.mlp_layers(fc_layer_params=output_fc_layer_params,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\critic_rnn_network.py",
        "line_number": 192,
        "API": ".expand_dims(",
        "context": [
            "\n",
            "    has_time_dim = num_outer_dims == 2\n",
            "    if not has_time_dim:\n",
            "      # Add a time dimension to the inputs.\n",
            "      observation = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1),\n",
            "                                          observation)\n",
            "      action = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1), action)\n",
            "      step_type = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\critic_rnn_network.py",
        "line_number": 198,
        "API": ".cast(",
        "context": [
            "      action = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1), action)\n",
            "      step_type = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1),\n",
            "                                        step_type)\n",
            "\n",
            "    observation = tf.cast(tf.nest.flatten(observation)[0], tf.float32)\n",
            "    action = tf.cast(tf.nest.flatten(action)[0], tf.float32)\n",
            "\n",
            "    batch_squash = utils.BatchSquash(2)  # Squash B, and T dims.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\critic_rnn_network.py",
        "line_number": 203,
        "API": ".flatten(",
        "context": [
            "    action = tf.cast(tf.nest.flatten(action)[0], tf.float32)\n",
            "\n",
            "    batch_squash = utils.BatchSquash(2)  # Squash B, and T dims.\n",
            "    observation = batch_squash.flatten(observation)  # [B, T, ...] -> [BxT, ...]\n",
            "    action = batch_squash.flatten(action)\n",
            "\n",
            "    for layer in self._observation_layers:\n",
            "      observation = layer(observation, training=training)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\critic_rnn_network.py",
        "line_number": 211,
        "API": ".concat(",
        "context": [
            "\n",
            "    for layer in self._action_layers:\n",
            "      action = layer(action, training=training)\n",
            "\n",
            "    joint = tf.concat([observation, action], -1)\n",
            "    for layer in self._joint_layers:\n",
            "      joint = layer(joint, training=training)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\critic_rnn_network.py",
        "line_number": 219,
        "API": ".equal(",
        "context": [
            "    joint = batch_squash.unflatten(joint)  # [B x T, ...] -> [B, T, ...]\n",
            "\n",
            "    network_kwargs = {}\n",
            "    if isinstance(self._lstm_network, dynamic_unroll_layer.DynamicUnroll):\n",
            "      network_kwargs['reset_mask'] = tf.equal(step_type,\n",
            "                                              time_step.StepType.FIRST,\n",
            "                                              name='mask')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\critic_rnn_network.py",
        "line_number": 234,
        "API": ".flatten(",
        "context": [
            "      joint, network_state = output\n",
            "    else:\n",
            "      joint = output[0]\n",
            "      network_state = tf.nest.pack_sequence_as(\n",
            "          self._lstm_network.cell.state_size, tf.nest.flatten(output[1:]))\n",
            "\n",
            "    output = batch_squash.flatten(joint)  # [B, T, ...] -> [B x T, ...]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\critic_rnn_network.py",
        "line_number": 241,
        "API": ".reshape(",
        "context": [
            "\n",
            "    for layer in self._output_layers:\n",
            "      output = layer(output, training=training)\n",
            "\n",
            "    q_value = tf.reshape(output, [-1])\n",
            "    q_value = batch_squash.unflatten(q_value)  # [B x T, ...] -> [B, T, ...]\n",
            "    if not has_time_dim:\n",
            "      q_value = tf.squeeze(q_value, axis=1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\ddpg_agent.py",
        "line_number": 231,
        "API": ".group(",
        "context": [
            "            self._actor_network.variables,\n",
            "            self._target_actor_network.variables,\n",
            "            tau,\n",
            "            tau_non_trainable=1.0)\n",
            "        return tf.group(critic_update, actor_update)\n",
            "\n",
            "      return common.Periodically(update, period, 'periodic_update_targets')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\ddpg_agent.py",
        "line_number": 330,
        "API": ".stop_gradient(",
        "context": [
            "      target_q_values, _ = self._target_critic_network(\n",
            "          target_critic_net_input, step_type=next_time_steps.step_type,\n",
            "          training=False)\n",
            "\n",
            "      td_targets = tf.stop_gradient(\n",
            "          self._reward_scale_factor * next_time_steps.reward +\n",
            "          self._gamma * next_time_steps.discount * target_q_values)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\ddpg_agent.py",
        "line_number": 343,
        "API": ".reduce_sum(",
        "context": [
            "      critic_loss = self._td_errors_loss_fn(td_targets, q_values)\n",
            "      if nest_utils.is_batched_nested_tensors(\n",
            "          time_steps, self.time_step_spec, num_outer_dims=2):\n",
            "        # Do a sum over the time dimension.\n",
            "        critic_loss = tf.reduce_sum(critic_loss, axis=1)\n",
            "      if weights is not None:\n",
            "        critic_loss *= weights\n",
            "      critic_loss = tf.reduce_mean(critic_loss)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\ddpg_agent.py",
        "line_number": 349,
        "API": ".scalar(",
        "context": [
            "        critic_loss *= weights\n",
            "      critic_loss = tf.reduce_mean(critic_loss)\n",
            "\n",
            "      with tf.name_scope('Losses/'):\n",
            "        tf.compat.v2.summary.scalar(\n",
            "            name='critic_loss', data=critic_loss, step=self.train_step_counter)\n",
            "\n",
            "      if self._debug_summaries:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\ddpg_agent.py",
        "line_number": 387,
        "API": ".flatten(",
        "context": [
            "        tape.watch(actions)\n",
            "        q_values, _ = self._critic_network((time_steps.observation, actions),\n",
            "                                           step_type=time_steps.step_type,\n",
            "                                           training=False)\n",
            "        actions = tf.nest.flatten(actions)\n",
            "\n",
            "      dqdas = tape.gradient([q_values], actions)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\ddpg_agent.py",
        "line_number": 394,
        "API": ".clip_by_value(",
        "context": [
            "\n",
            "      actor_losses = []\n",
            "      for dqda, action in zip(dqdas, actions):\n",
            "        if self._dqda_clipping is not None:\n",
            "          dqda = tf.clip_by_value(dqda, -1 * self._dqda_clipping,\n",
            "                                  self._dqda_clipping)\n",
            "        loss = common.element_wise_squared_loss(\n",
            "            tf.stop_gradient(dqda + action), action)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\ddpg_agent.py",
        "line_number": 401,
        "API": ".reduce_sum(",
        "context": [
            "            tf.stop_gradient(dqda + action), action)\n",
            "        if nest_utils.is_batched_nested_tensors(\n",
            "            time_steps, self.time_step_spec, num_outer_dims=2):\n",
            "          # Sum over the time dimension.\n",
            "          loss = tf.reduce_sum(loss, axis=1)\n",
            "        if weights is not None:\n",
            "          loss *= weights\n",
            "        loss = tf.reduce_mean(loss)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\ddpg_agent.py",
        "line_number": 407,
        "API": ".add_n(",
        "context": [
            "          loss *= weights\n",
            "        loss = tf.reduce_mean(loss)\n",
            "        actor_losses.append(loss)\n",
            "\n",
            "      actor_loss = tf.add_n(actor_losses)\n",
            "\n",
            "      with tf.name_scope('Losses/'):\n",
            "        tf.compat.v2.summary.scalar(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\examples\\v2\\train_eval.py",
        "line_number": 59,
        "API": ".getenv(",
        "context": [
            "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
            "from tf_agents.utils import common\n",
            "\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_integer('num_iterations', 100000,\n",
            "                     'Total number train/eval iterations to perform.')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\examples\\v2\\train_eval.py",
        "line_number": 115,
        "API": ".join(",
        "context": [
            "    eval_metrics_callback=None):\n",
            "\n",
            "  \"\"\"A simple train and eval for DDPG.\"\"\"\n",
            "  root_dir = os.path.expanduser(root_dir)\n",
            "  train_dir = os.path.join(root_dir, 'train')\n",
            "  eval_dir = os.path.join(root_dir, 'eval')\n",
            "\n",
            "  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\examples\\v2\\train_eval.py",
        "line_number": 131,
        "API": ".equal(",
        "context": [
            "  ]\n",
            "\n",
            "  global_step = tf.compat.v1.train.get_or_create_global_step()\n",
            "  with tf.compat.v2.summary.record_if(\n",
            "      lambda: tf.math.equal(global_step % summary_interval, 0)):\n",
            "    if num_parallel_environments > 1:\n",
            "      tf_env = tf_py_environment.TFPyEnvironment(\n",
            "          parallel_py_environment.ParallelPyEnvironment(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\examples\\v2\\train_eval.py",
        "line_number": 197,
        "API": ".function(",
        "context": [
            "        observers=[replay_buffer.add_batch] + train_metrics,\n",
            "        num_steps=collect_steps_per_iteration)\n",
            "\n",
            "    if use_tf_functions:\n",
            "      initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
            "      collect_driver.run = common.function(collect_driver.run)\n",
            "      tf_agent.train = common.function(tf_agent.train)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\examples\\v2\\train_eval.py",
        "line_number": 202,
        "API": ".info(",
        "context": [
            "      collect_driver.run = common.function(collect_driver.run)\n",
            "      tf_agent.train = common.function(tf_agent.train)\n",
            "\n",
            "    # Collect initial replay data.\n",
            "    logging.info(\n",
            "        'Initializing replay buffer by collecting experience for %d steps with '\n",
            "        'a random policy.', initial_collect_steps)\n",
            "    initial_collect_driver.run()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\examples\\v2\\train_eval.py",
        "line_number": 238,
        "API": ".function(",
        "context": [
            "      experience, _ = next(iterator)\n",
            "      return tf_agent.train(experience)\n",
            "\n",
            "    if use_tf_functions:\n",
            "      train_step = common.function(train_step)\n",
            "\n",
            "    for _ in range(num_iterations):\n",
            "      start_time = time.time()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\examples\\v2\\train_eval.py",
        "line_number": 251,
        "API": ".info(",
        "context": [
            "        train_loss = train_step()\n",
            "      time_acc += time.time() - start_time\n",
            "\n",
            "      if global_step.numpy() % log_interval == 0:\n",
            "        logging.info('step = %d, loss = %f', global_step.numpy(),\n",
            "                     train_loss.loss)\n",
            "        steps_per_sec = (global_step.numpy() - timed_at_step) / time_acc\n",
            "        logging.info('%.3f steps/sec', steps_per_sec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\examples\\v2\\train_eval.py",
        "line_number": 298,
        "API": ".flatten(",
        "context": [
            "\n",
            "\n",
            "def create_actor_network(fc_layer_units, action_spec):\n",
            "  \"\"\"Create an actor network for DDPG.\"\"\"\n",
            "  flat_action_spec = tf.nest.flatten(action_spec)\n",
            "  if len(flat_action_spec) > 1:\n",
            "    raise ValueError('Only a single action tensor is supported by this network')\n",
            "  flat_action_spec = flat_action_spec[0]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\examples\\v2\\train_eval.py",
        "line_number": 355,
        "API": ".set_verbosity(",
        "context": [
            "\n",
            "\n",
            "def main(_):\n",
            "  tf.compat.v1.enable_v2_behavior()\n",
            "  logging.set_verbosity(logging.INFO)\n",
            "  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_param)\n",
            "  train_eval(FLAGS.root_dir, num_iterations=FLAGS.num_iterations)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\examples\\v2\\train_eval_rnn.py",
        "line_number": 135,
        "API": ".equal(",
        "context": [
            "  ]\n",
            "\n",
            "  global_step = tf.compat.v1.train.get_or_create_global_step()\n",
            "  with tf.compat.v2.summary.record_if(\n",
            "      lambda: tf.math.equal(global_step % summary_interval, 0)):\n",
            "    if observations_allowlist is not None:\n",
            "      env_wrappers = [\n",
            "          functools.partial(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\examples\\v2\\train_eval_rnn.py",
        "line_number": 146,
        "API": ".load(",
        "context": [
            "    else:\n",
            "      env_wrappers = []\n",
            "\n",
            "    tf_env = tf_py_environment.TFPyEnvironment(\n",
            "        suite_dm_control.load(env_name, task_name, env_wrappers=env_wrappers))\n",
            "    eval_tf_env = tf_py_environment.TFPyEnvironment(\n",
            "        suite_dm_control.load(env_name, task_name, env_wrappers=env_wrappers))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\examples\\v2\\train_eval_rnn.py",
        "line_number": 220,
        "API": ".function(",
        "context": [
            "        observers=[replay_buffer.add_batch] + train_metrics,\n",
            "        num_episodes=collect_episodes_per_iteration)\n",
            "\n",
            "    if use_tf_functions:\n",
            "      initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
            "      collect_driver.run = common.function(collect_driver.run)\n",
            "      tf_agent.train = common.function(tf_agent.train)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ddpg\\examples\\v2\\train_eval_rnn.py",
        "line_number": 225,
        "API": ".info(",
        "context": [
            "      collect_driver.run = common.function(collect_driver.run)\n",
            "      tf_agent.train = common.function(tf_agent.train)\n",
            "\n",
            "    # Collect initial replay data.\n",
            "    logging.info(\n",
            "        'Initializing replay buffer by collecting experience for %d episodes '\n",
            "        'with a random policy.', initial_collect_episodes)\n",
            "    initial_collect_driver.run()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\dqn_agent.py",
        "line_number": 76,
        "API": ".stop_gradient(",
        "context": [
            "\n",
            "def compute_td_targets(next_q_values: types.Tensor,\n",
            "                       rewards: types.Tensor,\n",
            "                       discounts: types.Tensor) -> types.Tensor:\n",
            "  return tf.stop_gradient(rewards + discounts * next_q_values)\n",
            "\n",
            "\n",
            "@gin.configurable\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\dqn_agent.py",
        "line_number": 228,
        "API": ".format(",
        "context": [
            "\n",
            "    if epsilon_greedy is not None and boltzmann_temperature is not None:\n",
            "      raise ValueError(\n",
            "          'Configured both epsilon_greedy value {} and temperature {}, '\n",
            "          'however only one of them can be used for exploration.'.format(\n",
            "              epsilon_greedy, boltzmann_temperature))\n",
            "\n",
            "    self._observation_and_action_constraint_splitter = (\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\dqn_agent.py",
        "line_number": 267,
        "API": ".format(",
        "context": [
            "\n",
            "    if q_network.state_spec and n_step_update != 1:\n",
            "      raise NotImplementedError(\n",
            "          'DqnAgent does not currently support n-step updates with stateful '\n",
            "          'networks (i.e., RNNs), but n_step_update = {}'.format(n_step_update))\n",
            "\n",
            "    train_sequence_length = (\n",
            "        n_step_update + 1 if not q_network.state_spec else None)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\dqn_agent.py",
        "line_number": 297,
        "API": ".flatten(",
        "context": [
            "      self._as_transition = data_converter.AsNStepTransition(\n",
            "          self.data_context, gamma=gamma, n=n_step_update)\n",
            "\n",
            "  def _check_action_spec(self, action_spec):\n",
            "    flat_action_spec = tf.nest.flatten(action_spec)\n",
            "\n",
            "    # TODO(oars): Get DQN working with more than one dim in the actions.\n",
            "    if len(flat_action_spec) > 1 or flat_action_spec[0].shape.rank > 0:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\dqn_agent.py",
        "line_number": 303,
        "API": ".format(",
        "context": [
            "    # TODO(oars): Get DQN working with more than one dim in the actions.\n",
            "    if len(flat_action_spec) > 1 or flat_action_spec[0].shape.rank > 0:\n",
            "      raise ValueError(\n",
            "          'Only scalar actions are supported now, but action spec is: {}'\n",
            "          .format(action_spec))\n",
            "\n",
            "    spec = flat_action_spec[0]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\dqn_agent.py",
        "line_number": 311,
        "API": ".format(",
        "context": [
            "    # TODO(b/119321125): Disable this once index_with_actions supports\n",
            "    # negative-valued actions.\n",
            "    if spec.minimum != 0:\n",
            "      raise ValueError(\n",
            "          'Action specs should have minimum of 0, but saw: {0}'.format(spec))\n",
            "\n",
            "    self._num_actions = spec.maximum - spec.minimum + 1\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\dqn_agent.py",
        "line_number": 485,
        "API": ".cast(",
        "context": [
            "\n",
            "      td_loss, td_error = self._td_loss(q_values, next_q_values, rewards,\n",
            "                                        discounts)\n",
            "\n",
            "      valid_mask = tf.cast(~time_steps.is_last(), tf.float32)\n",
            "      td_error = valid_mask * td_error\n",
            "\n",
            "      td_loss = valid_mask * td_loss\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\dqn_agent.py",
        "line_number": 493,
        "API": ".reduce_sum(",
        "context": [
            "\n",
            "      if nest_utils.is_batched_nested_tensors(\n",
            "          time_steps, self.time_step_spec, num_outer_dims=2):\n",
            "        # Do a sum over the time dimension.\n",
            "        td_loss = tf.reduce_sum(input_tensor=td_loss, axis=1)\n",
            "\n",
            "      # Aggregate across the elements of the batch and add regularization loss.\n",
            "      # Note: We use an element wise loss above to ensure each element is always\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\dqn_agent.py",
        "line_number": 520,
        "API": ".histogram(",
        "context": [
            "\n",
            "      if self._summarize_grads_and_vars:\n",
            "        with tf.name_scope('Variables/'):\n",
            "          for var in self._q_network.trainable_weights:\n",
            "            tf.compat.v2.summary.histogram(\n",
            "                name=var.name.replace(':', '_'),\n",
            "                data=var,\n",
            "                step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\dqn_agent.py",
        "line_number": 552,
        "API": ".flatten(",
        "context": [
            "    q_values, _ = self._q_network(network_observation,\n",
            "                                  step_type=time_steps.step_type,\n",
            "                                  training=training)\n",
            "    # Handle action_spec.shape=(), and shape=(1,) by using the multi_dim_actions\n",
            "    # param. Note: assumes len(tf.nest.flatten(action_spec)) == 1.\n",
            "    action_spec = cast(tensor_spec.BoundedTensorSpec, self._action_spec)\n",
            "    multi_dim_actions = action_spec.shape.rank > 0\n",
            "    return common.index_with_actions(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\dqn_agent.py",
        "line_number": 557,
        "API": ".cast(",
        "context": [
            "    action_spec = cast(tensor_spec.BoundedTensorSpec, self._action_spec)\n",
            "    multi_dim_actions = action_spec.shape.rank > 0\n",
            "    return common.index_with_actions(\n",
            "        q_values,\n",
            "        tf.cast(actions, dtype=tf.int32),\n",
            "        multi_dim_actions=multi_dim_actions)\n",
            "\n",
            "  def _compute_next_q_values(self, next_time_steps, info):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\dqn_agent.py",
        "line_number": 580,
        "API": ".shape(",
        "context": [
            "\n",
            "    next_target_q_values, _ = self._target_q_network(\n",
            "        network_observation, step_type=next_time_steps.step_type)\n",
            "    batch_size = (\n",
            "        next_target_q_values.shape[0] or tf.shape(next_target_q_values)[0])\n",
            "    dummy_state = self._target_greedy_policy.get_initial_state(batch_size)\n",
            "    # Find the greedy actions using our target greedy policy. This ensures that\n",
            "    # action constraints are respected and helps centralize the greedy logic.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\dqn_agent.py",
        "line_number": 588,
        "API": ".flatten(",
        "context": [
            "    greedy_actions = self._target_greedy_policy.action(\n",
            "        next_time_steps, dummy_state).action\n",
            "\n",
            "    # Handle action_spec.shape=(), and shape=(1,) by using the multi_dim_actions\n",
            "    # param. Note: assumes len(tf.nest.flatten(action_spec)) == 1.\n",
            "    multi_dim_actions = tf.nest.flatten(self._action_spec)[0].shape.rank > 0\n",
            "    return common.index_with_actions(\n",
            "        next_target_q_values,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\dqn_agent.py",
        "line_number": 630,
        "API": ".shape(",
        "context": [
            "\n",
            "    next_target_q_values, _ = self._target_q_network(\n",
            "        network_observation, step_type=next_time_steps.step_type)\n",
            "    batch_size = (\n",
            "        next_target_q_values.shape[0] or tf.shape(next_target_q_values)[0])\n",
            "    dummy_state = self._policy.get_initial_state(batch_size)\n",
            "    # Find the greedy actions using our greedy policy. This ensures that action\n",
            "    # constraints are respected and helps centralize the greedy logic.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\dqn_agent.py",
        "line_number": 637,
        "API": ".flatten(",
        "context": [
            "    # constraints are respected and helps centralize the greedy logic.\n",
            "    best_next_actions = self._policy.action(next_time_steps, dummy_state).action\n",
            "\n",
            "    # Handle action_spec.shape=(), and shape=(1,) by using the multi_dim_actions\n",
            "    # param. Note: assumes len(tf.nest.flatten(action_spec)) == 1.\n",
            "    multi_dim_actions = tf.nest.flatten(self._action_spec)[0].shape.rank > 0\n",
            "    return common.index_with_actions(\n",
            "        next_target_q_values,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\examples\\v2\\train_eval.py",
        "line_number": 68,
        "API": ".getenv(",
        "context": [
            "from tf_agents.policies import random_tf_policy\n",
            "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
            "from tf_agents.utils import common\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_integer('num_iterations', 100000,\n",
            "                     'Total number train/eval iterations to perform.')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\examples\\v2\\train_eval.py",
        "line_number": 126,
        "API": ".join(",
        "context": [
            "    summarize_grads_and_vars=False,\n",
            "    eval_metrics_callback=None):\n",
            "  \"\"\"A simple train and eval for DQN.\"\"\"\n",
            "  root_dir = os.path.expanduser(root_dir)\n",
            "  train_dir = os.path.join(root_dir, 'train')\n",
            "  eval_dir = os.path.join(root_dir, 'eval')\n",
            "\n",
            "  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\examples\\v2\\train_eval.py",
        "line_number": 142,
        "API": ".equal(",
        "context": [
            "  ]\n",
            "\n",
            "  global_step = tf.compat.v1.train.get_or_create_global_step()\n",
            "  with tf.compat.v2.summary.record_if(\n",
            "      lambda: tf.math.equal(global_step % summary_interval, 0)):\n",
            "    tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n",
            "    eval_tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\examples\\v2\\train_eval.py",
        "line_number": 210,
        "API": ".join(",
        "context": [
            "        agent=tf_agent,\n",
            "        global_step=global_step,\n",
            "        metrics=metric_utils.MetricsGroup(train_metrics, 'train_metrics'))\n",
            "    policy_checkpointer = common.Checkpointer(\n",
            "        ckpt_dir=os.path.join(train_dir, 'policy'),\n",
            "        policy=eval_policy,\n",
            "        global_step=global_step)\n",
            "    rb_checkpointer = common.Checkpointer(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\examples\\v2\\train_eval.py",
        "line_number": 223,
        "API": ".function(",
        "context": [
            "    rb_checkpointer.initialize_or_restore()\n",
            "\n",
            "    if use_tf_functions:\n",
            "      # To speed up collect use common.function.\n",
            "      collect_driver.run = common.function(collect_driver.run)\n",
            "      tf_agent.train = common.function(tf_agent.train)\n",
            "\n",
            "    initial_collect_policy = random_tf_policy.RandomTFPolicy(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\examples\\v2\\train_eval.py",
        "line_number": 230,
        "API": ".info(",
        "context": [
            "    initial_collect_policy = random_tf_policy.RandomTFPolicy(\n",
            "        tf_env.time_step_spec(), tf_env.action_spec())\n",
            "\n",
            "    # Collect initial replay data.\n",
            "    logging.info(\n",
            "        'Initializing replay buffer by collecting experience for %d steps with '\n",
            "        'a random policy.', initial_collect_steps)\n",
            "    dynamic_step_driver.DynamicStepDriver(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\examples\\v2\\train_eval.py",
        "line_number": 297,
        "API": ".save(",
        "context": [
            "        train_metric.tf_summaries(\n",
            "            train_step=global_step, step_metrics=train_metrics[:2])\n",
            "\n",
            "      if global_step.numpy() % train_checkpoint_interval == 0:\n",
            "        train_checkpointer.save(global_step=global_step.numpy())\n",
            "\n",
            "      if global_step.numpy() % policy_checkpoint_interval == 0:\n",
            "        policy_checkpointer.save(global_step=global_step.numpy())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\examples\\v2\\train_eval.py",
        "line_number": 303,
        "API": ".save(",
        "context": [
            "      if global_step.numpy() % policy_checkpoint_interval == 0:\n",
            "        policy_checkpointer.save(global_step=global_step.numpy())\n",
            "\n",
            "      if global_step.numpy() % rb_checkpoint_interval == 0:\n",
            "        rb_checkpointer.save(global_step=global_step.numpy())\n",
            "\n",
            "      if global_step.numpy() % eval_interval == 0:\n",
            "        results = metric_utils.eager_compute(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\dqn\\examples\\v2\\train_eval.py",
        "line_number": 360,
        "API": ".set_verbosity(",
        "context": [
            "      + [logits(num_actions)])\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  logging.set_verbosity(logging.INFO)\n",
            "  tf.compat.v1.enable_v2_behavior()\n",
            "  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_param)\n",
            "  train_eval(FLAGS.root_dir, num_iterations=FLAGS.num_iterations)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_actor_network.py",
        "line_number": 34,
        "API": ".tanh(",
        "context": [
            "  \"\"\"Maps inputs with arbitrary range to range defined by spec using `tanh`.\"\"\"\n",
            "  means = (spec.maximum + spec.minimum) / 2.0\n",
            "  magnitudes = (spec.maximum - spec.minimum) / 2.0\n",
            "\n",
            "  return means + magnitudes * tf.tanh(inputs)\n",
            "\n",
            "\n",
            "class PPOActorNetwork():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_actor_network.py",
        "line_number": 78,
        "API": ".log(",
        "context": [
            "          name='means_projection_layer')\n",
            "\n",
            "    def std_layers():\n",
            "      # TODO(b/179510447): align these parameters with Schulman 17.\n",
            "      std_bias_initializer_value = np.log(np.exp(0.35) - 1)\n",
            "      return bias_layer.BiasLayer(\n",
            "          bias_initializer=tf.constant_initializer(\n",
            "              value=std_bias_initializer_value))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_actor_network.py",
        "line_number": 96,
        "API": ".zeros_like(",
        "context": [
            "    return sequential.Sequential(\n",
            "        [dense(num_units) for num_units in fc_layer_units] +\n",
            "        [means_layers()] +\n",
            "        [tf.keras.layers.Lambda(\n",
            "            lambda x: {'loc': x, 'scale': tf.zeros_like(x)})] +\n",
            "        [nest_map.NestMap({\n",
            "            'loc': no_op_layers(),\n",
            "            'scale': std_layers(),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 99,
        "API": ".moments(",
        "context": [
            "))\n",
            "\n",
            "\n",
            "def _normalize_advantages(advantages, axes=(0, 1), variance_epsilon=1e-8):\n",
            "  adv_mean, adv_var = tf.nn.moments(advantages, axes=axes, keepdims=True)\n",
            "  normalized_advantages = tf.nn.batch_normalization(\n",
            "      advantages,\n",
            "      adv_mean,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 226,
        "API": ".map_structure(",
        "context": [
            "        Example usage:\n",
            "          ```python\n",
            "        observation_tensor_spec, action_spec, time_step_tensor_spec = (\n",
            "            spec_utils.get_tensor_specs(env))\n",
            "          normalized_observation_tensor_spec = tf.nest.map_structure(\n",
            "            lambda s: tf.TensorSpec(\n",
            "              dtype=tf.float32, shape=s.shape, name=s.name\n",
            "            ),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 339,
        "API": ".warning(",
        "context": [
            "        compute_value_and_advantage_in_train)\n",
            "    self._aggregate_losses_across_replicas = aggregate_losses_across_replicas\n",
            "    self.update_normalizers_in_train = update_normalizers_in_train\n",
            "    if not isinstance(self._optimizer, tf.keras.optimizers.Optimizer):\n",
            "      logging.warning(\n",
            "          'Only tf.keras.optimizers.Optimiers are well supported, got a '\n",
            "          'non-TF2 optimizer: %s', self._optimizer)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 397,
        "API": ".copy(",
        "context": [
            "    # iff return and normalized advantage are saved in preprocess_sequence.\n",
            "    if self._compute_value_and_advantage_in_train:\n",
            "      training_data_spec = None\n",
            "    else:\n",
            "      training_policy_info = collect_policy.trajectory_spec.policy_info.copy()\n",
            "      training_policy_info.update({\n",
            "          'value_prediction':\n",
            "              collect_policy.trajectory_spec.policy_info['value_prediction'],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 534,
        "API": ".stop_gradient(",
        "context": [
            "        training=training)\n",
            "    policy_gradient_loss = self.policy_gradient_loss(\n",
            "        time_steps,\n",
            "        actions,\n",
            "        tf.stop_gradient(act_log_probs),\n",
            "        tf.stop_gradient(normalized_advantages),\n",
            "        current_policy_distribution,\n",
            "        weights,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 544,
        "API": ".zeros_like(",
        "context": [
            "    if (self._policy_l2_reg > 0.0 or self._value_function_l2_reg > 0.0 or\n",
            "        self._shared_vars_l2_reg > 0.0):\n",
            "      l2_regularization_loss = self.l2_regularization_loss(debug_summaries)\n",
            "    else:\n",
            "      l2_regularization_loss = tf.zeros_like(policy_gradient_loss)\n",
            "\n",
            "    with tf.name_scope('entropy_regularization'):\n",
            "      outer_rank = nest_utils.get_outer_rank(time_steps, self.time_step_spec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 551,
        "API": ".zeros_like(",
        "context": [
            "      outer_rank = nest_utils.get_outer_rank(time_steps, self.time_step_spec)\n",
            "      entropy = common.entropy(current_policy_distribution, self.action_spec,\n",
            "                               outer_rank)\n",
            "      if entropy is None:\n",
            "        entropy = tf.zeros_like(weights, tf.float32)\n",
            "      else:\n",
            "        entropy = tf.cast(entropy, tf.float32)\n",
            "    if self._entropy_regularization > 0.0:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 558,
        "API": ".zeros_like(",
        "context": [
            "    if self._entropy_regularization > 0.0:\n",
            "      entropy_regularization_loss = self.entropy_regularization_loss(\n",
            "          time_steps, entropy, weights, debug_summaries)\n",
            "    else:\n",
            "      entropy_regularization_loss = tf.zeros_like(policy_gradient_loss)\n",
            "\n",
            "    with tf.name_scope('Losses/'):\n",
            "      tf.compat.v2.summary.scalar(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 563,
        "API": ".reduce_mean(",
        "context": [
            "\n",
            "    with tf.name_scope('Losses/'):\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='entropy',\n",
            "          data=tf.reduce_mean(entropy * weights),\n",
            "          step=self.train_step_counter)\n",
            "\n",
            "    # TODO(b/161365079): Move this logic to PPOKLPenaltyAgent.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 568,
        "API": ".zeros_like(",
        "context": [
            "          step=self.train_step_counter)\n",
            "\n",
            "    # TODO(b/161365079): Move this logic to PPOKLPenaltyAgent.\n",
            "    if self._initial_adaptive_kl_beta == 0 and self._kl_cutoff_factor == 0:\n",
            "      kl_penalty_loss = tf.zeros_like(policy_gradient_loss)\n",
            "    else:\n",
            "      kl_penalty_loss = self.kl_penalty_loss(time_steps,\n",
            "                                             action_distribution_parameters,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 604,
        "API": ".constant(",
        "context": [
            "\n",
            "    Returns:\n",
            "      tuple of (return, advantage), both are batched tensors.\n",
            "    \"\"\"\n",
            "    discounts = next_time_steps.discount * tf.constant(\n",
            "        self._discount_factor, dtype=tf.float32)\n",
            "\n",
            "    rewards = next_time_steps.reward\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 613,
        "API": ".histogram(",
        "context": [
            "      # Summarize rewards before they get normalized below.\n",
            "      # TODO(b/171573175): remove the condition once histograms are\n",
            "      # supported on TPUs.\n",
            "      if not tf.config.list_logical_devices('TPU'):\n",
            "        tf.compat.v2.summary.histogram(\n",
            "            name='rewards', data=rewards, step=self.train_step_counter)\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='rewards_mean',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 622,
        "API": ".normalize(",
        "context": [
            "          step=self.train_step_counter)\n",
            "\n",
            "    # Normalize rewards if self._reward_normalizer is defined.\n",
            "    if self._reward_normalizer:\n",
            "      rewards = self._reward_normalizer.normalize(\n",
            "          rewards, center_mean=False, clip_value=self._reward_norm_clipping)\n",
            "      if self._debug_summaries:\n",
            "        # TODO(b/171573175): remove the condition once histograms are\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 628,
        "API": ".histogram(",
        "context": [
            "      if self._debug_summaries:\n",
            "        # TODO(b/171573175): remove the condition once histograms are\n",
            "        # supported on TPUs.\n",
            "        if not tf.config.list_logical_devices('TPU'):\n",
            "          tf.compat.v2.summary.histogram(\n",
            "              name='rewards_normalized',\n",
            "              data=rewards,\n",
            "              step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 634,
        "API": ".reduce_mean(",
        "context": [
            "              data=rewards,\n",
            "              step=self.train_step_counter)\n",
            "        tf.compat.v2.summary.scalar(\n",
            "            name='rewards_normalized_mean',\n",
            "            data=tf.reduce_mean(rewards),\n",
            "            step=self.train_step_counter)\n",
            "\n",
            "    # Make discount 0.0 at end of each episode to restart cumulative sum\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 657,
        "API": ".histogram(",
        "context": [
            "        final_value=final_value_bootstrapped)\n",
            "    # TODO(b/171573175): remove the condition once histograms are\n",
            "    # supported on TPUs.\n",
            "    if self._debug_summaries and not tf.config.list_logical_devices('TPU'):\n",
            "      tf.compat.v2.summary.histogram(\n",
            "          name='returns', data=returns, step=self.train_step_counter)\n",
            "\n",
            "    # Compute advantages.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 667,
        "API": ".histogram(",
        "context": [
            "\n",
            "    # TODO(b/171573175): remove the condition once historgrams are\n",
            "    # supported on TPUs.\n",
            "    if self._debug_summaries and not tf.config.list_logical_devices('TPU'):\n",
            "      tf.compat.v2.summary.histogram(\n",
            "          name='advantages', data=advantages, step=self.train_step_counter)\n",
            "\n",
            "    # Return TD-Lambda returns if both use_td_lambda_return and use_gae.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 673,
        "API": ".warning(",
        "context": [
            "\n",
            "    # Return TD-Lambda returns if both use_td_lambda_return and use_gae.\n",
            "    if self._use_td_lambda_return:\n",
            "      if not self._use_gae:\n",
            "        logging.warning('use_td_lambda_return was True, but use_gae was '\n",
            "                        'False. Using Monte Carlo return.')\n",
            "      else:\n",
            "        returns = tf.add(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 733,
        "API": ".stop_gradient(",
        "context": [
            "          batched_experience.observation,\n",
            "          batched_experience.step_type,\n",
            "          value_state=value_state,\n",
            "          training=False)\n",
            "      value_preds = tf.stop_gradient(value_preds)\n",
            "    else:\n",
            "      value_preds = batched_experience.policy_info['value_prediction']\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 750,
        "API": ".zeros(",
        "context": [
            "    # Pad returns and normalized_advantages in the time dimension so that the\n",
            "    # time dimensions are aligned with the input experience's time dimension.\n",
            "    # When the output trajectory gets sliced by trajectory.to_transition during\n",
            "    # training, the padded last timesteps will be automatically dropped.\n",
            "    last_transition_padding = tf.zeros((batch_size, 1), dtype=tf.float32)\n",
            "    new_policy_info['return'] = tf.concat([returns, last_transition_padding],\n",
            "                                          axis=1)\n",
            "    new_policy_info['advantage'] = tf.concat(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 824,
        "API": ".flatten(",
        "context": [
            "\n",
            "    # TODO(b/171573175): remove the condition once histograms are\n",
            "    # supported on TPUs.\n",
            "    if self._debug_summaries and not tf.config.list_logical_devices('TPU'):\n",
            "      actions_list = tf.nest.flatten(processed_experience.action)\n",
            "      show_action_index = len(actions_list) != 1\n",
            "      for i, single_action in enumerate(actions_list):\n",
            "        action_name = ('actions_{}'.format(i)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 829,
        "API": ".histogram(",
        "context": [
            "      show_action_index = len(actions_list) != 1\n",
            "      for i, single_action in enumerate(actions_list):\n",
            "        action_name = ('actions_{}'.format(i)\n",
            "                       if show_action_index else 'actions')\n",
            "        tf.compat.v2.summary.histogram(\n",
            "            name=action_name, data=single_action, step=self.train_step_counter)\n",
            "\n",
            "    time_steps = ts.TimeStep(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 847,
        "API": ".histogram(",
        "context": [
            "\n",
            "    # TODO(b/171573175): remove the condition once histograms are\n",
            "    # supported on TPUs.\n",
            "    if self._debug_summaries and not tf.config.list_logical_devices('TPU'):\n",
            "      tf.compat.v2.summary.histogram(\n",
            "          name='advantages_normalized',\n",
            "          data=normalized_advantages,\n",
            "          step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 893,
        "API": ".global_norm(",
        "context": [
            "        grads = tape.gradient(loss_info.loss, variables_to_train)\n",
            "        if self._gradient_clipping > 0:\n",
            "          grads, _ = tf.clip_by_global_norm(grads, self._gradient_clipping)\n",
            "\n",
            "        self._grad_norm = tf.linalg.global_norm(grads)\n",
            "\n",
            "        # Tuple is used for py3, where zip is a generator producing values once.\n",
            "        grads_and_vars = tuple(zip(grads, variables_to_train))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 932,
        "API": ".map_structure(",
        "context": [
            "    if self.update_normalizers_in_train:\n",
            "      self.update_observation_normalizer(time_steps.observation)\n",
            "      self.update_reward_normalizer(processed_experience.reward)\n",
            "\n",
            "    loss_info = tf.nest.map_structure(tf.identity, loss_info)\n",
            "\n",
            "    # Make summaries for total loss averaged across all epochs.\n",
            "    # The *_losses lists will have been populated by\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 939,
        "API": ".add_n(",
        "context": [
            "    # The *_losses lists will have been populated by\n",
            "    #   calls to self.get_loss. Assumes all the losses have same length.\n",
            "    with tf.name_scope('Losses/'):\n",
            "      num_epochs = len(policy_gradient_losses)\n",
            "      total_policy_gradient_loss = tf.add_n(policy_gradient_losses) / num_epochs\n",
            "      total_value_estimation_loss = tf.add_n(\n",
            "          value_estimation_losses) / num_epochs\n",
            "      total_l2_regularization_loss = tf.add_n(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 944,
        "API": ".add_n(",
        "context": [
            "      total_value_estimation_loss = tf.add_n(\n",
            "          value_estimation_losses) / num_epochs\n",
            "      total_l2_regularization_loss = tf.add_n(\n",
            "          l2_regularization_losses) / num_epochs\n",
            "      total_entropy_regularization_loss = tf.add_n(\n",
            "          entropy_regularization_losses) / num_epochs\n",
            "      total_kl_penalty_loss = tf.add_n(kl_penalty_losses) / num_epochs\n",
            "      tf.compat.v2.summary.scalar(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 951,
        "API": ".scalar(",
        "context": [
            "      tf.compat.v2.summary.scalar(\n",
            "          name='policy_gradient_loss',\n",
            "          data=total_policy_gradient_loss,\n",
            "          step=self.train_step_counter)\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='value_estimation_loss',\n",
            "          data=total_value_estimation_loss,\n",
            "          step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 959,
        "API": ".scalar(",
        "context": [
            "      tf.compat.v2.summary.scalar(\n",
            "          name='l2_regularization_loss',\n",
            "          data=total_l2_regularization_loss,\n",
            "          step=self.train_step_counter)\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='entropy_regularization_loss',\n",
            "          data=total_entropy_regularization_loss,\n",
            "          step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 969,
        "API": ".abs(",
        "context": [
            "          data=total_kl_penalty_loss,\n",
            "          step=self.train_step_counter)\n",
            "\n",
            "      total_abs_loss = (\n",
            "          tf.abs(total_policy_gradient_loss) +\n",
            "          tf.abs(total_value_estimation_loss) +\n",
            "          tf.abs(total_entropy_regularization_loss) +\n",
            "          tf.abs(total_l2_regularization_loss) + tf.abs(total_kl_penalty_loss))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 974,
        "API": ".scalar(",
        "context": [
            "          tf.abs(total_value_estimation_loss) +\n",
            "          tf.abs(total_entropy_regularization_loss) +\n",
            "          tf.abs(total_l2_regularization_loss) + tf.abs(total_kl_penalty_loss))\n",
            "\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='total_abs_loss',\n",
            "          data=total_abs_loss,\n",
            "          step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 981,
        "API": ".scalar(",
        "context": [
            "          step=self.train_step_counter)\n",
            "\n",
            "    with tf.name_scope('LearningRate/'):\n",
            "      learning_rate = ppo_utils.get_learning_rate(self._optimizer)\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='learning_rate',\n",
            "          data=learning_rate,\n",
            "          step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 995,
        "API": ".histogram(",
        "context": [
            "        all_vars = (\n",
            "            self._actor_net.trainable_weights +\n",
            "            self._value_net.trainable_weights)\n",
            "        for var in all_vars:\n",
            "          tf.compat.v2.summary.histogram(\n",
            "              name=var.name.replace(':', '_'),\n",
            "              data=var,\n",
            "              step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1029,
        "API": ".square(",
        "context": [
            "\n",
            "        # Regularize policy weights.\n",
            "        policy_l2_losses = [\n",
            "            common.aggregate_losses(\n",
            "                regularization_loss=tf.square(v)).regularization *\n",
            "            self._policy_l2_reg for v in unshared_policy_vars_to_regularize\n",
            "        ]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1036,
        "API": ".square(",
        "context": [
            "\n",
            "        # Regularize value function weights.\n",
            "        vf_l2_losses = [\n",
            "            common.aggregate_losses(\n",
            "                regularization_loss=tf.square(v)).regularization *\n",
            "            self._value_function_l2_reg for v in unshared_vf_vars_to_regularize\n",
            "        ]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1043,
        "API": ".square(",
        "context": [
            "\n",
            "        # Regularize shared weights\n",
            "        shared_l2_losses = [\n",
            "            common.aggregate_losses(\n",
            "                regularization_loss=tf.square(v)).regularization *\n",
            "            self._shared_vars_l2_reg for v in shared_vars_to_regularize\n",
            "        ]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1048,
        "API": ".add_n(",
        "context": [
            "            self._shared_vars_l2_reg for v in shared_vars_to_regularize\n",
            "        ]\n",
            "\n",
            "        l2_losses = policy_l2_losses + vf_l2_losses + shared_l2_losses\n",
            "        total_l2_loss = tf.add_n(l2_losses, name='l2_loss')\n",
            "\n",
            "        if self._check_numerics:\n",
            "          total_l2_loss = tf.debugging.check_numerics(total_l2_loss,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1057,
        "API": ".histogram(",
        "context": [
            "\n",
            "        # TODO(b/171573175): remove the condition once histograms are\n",
            "        # supported on TPUs.\n",
            "        if debug_summaries and not tf.config.list_logical_devices('TPU'):\n",
            "          tf.compat.v2.summary.histogram(\n",
            "              name='l2_loss', data=total_l2_loss, step=self.train_step_counter)\n",
            "    else:\n",
            "      total_l2_loss = tf.constant(0.0, dtype=tf.float32, name='zero_l2_loss')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1080,
        "API": ".reduce_mean(",
        "context": [
            "              per_example_loss=-entropy,\n",
            "              sample_weight=weights).total_loss * self._entropy_regularization\n",
            "        else:\n",
            "          entropy_reg_loss = (\n",
            "              tf.math.reduce_mean(-entropy * weights) *\n",
            "              self._entropy_regularization)\n",
            "\n",
            "        if self._check_numerics:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1090,
        "API": ".histogram(",
        "context": [
            "\n",
            "        # TODO(b/171573175): remove the condition once histograms are supported\n",
            "        # on TPUs.\n",
            "        if debug_summaries and not tf.config.list_logical_devices('TPU'):\n",
            "          tf.compat.v2.summary.histogram(\n",
            "              name='entropy_reg_loss',\n",
            "              data=entropy_reg_loss,\n",
            "              step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1095,
        "API": ".constant(",
        "context": [
            "              name='entropy_reg_loss',\n",
            "              data=entropy_reg_loss,\n",
            "              step=self.train_step_counter)\n",
            "    else:\n",
            "      entropy_reg_loss = tf.constant(\n",
            "          0.0, dtype=tf.float32, name='zero_entropy_reg_loss')\n",
            "\n",
            "    return entropy_reg_loss\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1135,
        "API": ".flatten(",
        "context": [
            "    observation = time_steps.observation\n",
            "    # TODO(b/171573175): remove the condition once histograms are\n",
            "    # supported on TPUs.\n",
            "    if debug_summaries and not tf.config.list_logical_devices('TPU'):\n",
            "      observation_list = tf.nest.flatten(observation)\n",
            "      show_observation_index = len(observation_list) != 1\n",
            "      for i, single_observation in enumerate(observation_list):\n",
            "        observation_name = ('observations_{}'.format(i)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1140,
        "API": ".histogram(",
        "context": [
            "      show_observation_index = len(observation_list) != 1\n",
            "      for i, single_observation in enumerate(observation_list):\n",
            "        observation_name = ('observations_{}'.format(i)\n",
            "                            if show_observation_index else 'observations')\n",
            "        tf.compat.v2.summary.histogram(\n",
            "            name=observation_name,\n",
            "            data=single_observation,\n",
            "            step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1153,
        "API": ".squared_difference(",
        "context": [
            "        time_steps.observation,\n",
            "        time_steps.step_type,\n",
            "        value_state=value_state,\n",
            "        training=training)\n",
            "    value_estimation_error = tf.math.squared_difference(returns, value_preds)\n",
            "\n",
            "    if self._value_clipping > 0:\n",
            "      if old_value_predictions is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1159,
        "API": ".clip_by_value(",
        "context": [
            "    if self._value_clipping > 0:\n",
            "      if old_value_predictions is None:\n",
            "        raise ValueError(\n",
            "            'old_value_predictions is None but needed for value clipping.')\n",
            "      clipped_value_preds = old_value_predictions + tf.clip_by_value(\n",
            "          value_preds - old_value_predictions, -self._value_clipping,\n",
            "          self._value_clipping)\n",
            "      clipped_value_estimation_error = tf.math.squared_difference(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1164,
        "API": ".maximum(",
        "context": [
            "          value_preds - old_value_predictions, -self._value_clipping,\n",
            "          self._value_clipping)\n",
            "      clipped_value_estimation_error = tf.math.squared_difference(\n",
            "          returns, clipped_value_preds)\n",
            "      value_estimation_error = tf.maximum(value_estimation_error,\n",
            "                                          clipped_value_estimation_error)\n",
            "\n",
            "    if self._aggregate_losses_across_replicas:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1173,
        "API": ".reduce_mean(",
        "context": [
            "          common.aggregate_losses(\n",
            "              per_example_loss=value_estimation_error,\n",
            "              sample_weight=weights).total_loss * self._value_pred_loss_coef)\n",
            "    else:\n",
            "      value_estimation_loss = tf.math.reduce_mean(\n",
            "          value_estimation_error * weights) * self._value_pred_loss_coef\n",
            "\n",
            "    if debug_summaries:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1179,
        "API": ".reduce_mean(",
        "context": [
            "\n",
            "    if debug_summaries:\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='value_pred_avg',\n",
            "          data=tf.reduce_mean(input_tensor=value_preds),\n",
            "          step=self.train_step_counter)\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='value_actual_avg',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1185,
        "API": ".scalar(",
        "context": [
            "      tf.compat.v2.summary.scalar(\n",
            "          name='value_actual_avg',\n",
            "          data=tf.reduce_mean(input_tensor=returns),\n",
            "          step=self.train_step_counter)\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='value_estimation_loss',\n",
            "          data=value_estimation_loss,\n",
            "          step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1192,
        "API": ".histogram(",
        "context": [
            "          step=self.train_step_counter)\n",
            "      # TODO(b/171573175): remove the condition once histograms are supported\n",
            "      # on TPUs.\n",
            "      if not tf.config.list_logical_devices('TPU'):\n",
            "        tf.compat.v2.summary.histogram(\n",
            "            name='value_preds', data=value_preds, step=self.train_step_counter)\n",
            "        tf.compat.v2.summary.histogram(\n",
            "            name='value_estimation_error',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1237,
        "API": ".cast(",
        "context": [
            "    \"\"\"\n",
            "    nest_utils.assert_same_structure(time_steps, self.time_step_spec)\n",
            "    action_log_prob = common.log_probability(current_policy_distribution,\n",
            "                                             actions, self._action_spec)\n",
            "    action_log_prob = tf.cast(action_log_prob, tf.float32)\n",
            "    if self._log_prob_clipping > 0.0:\n",
            "      action_log_prob = tf.clip_by_value(action_log_prob,\n",
            "                                         -self._log_prob_clipping,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1247,
        "API": ".exp(",
        "context": [
            "      action_log_prob = tf.debugging.check_numerics(action_log_prob,\n",
            "                                                    'action_log_prob')\n",
            "\n",
            "    # Prepare both clipped and unclipped importance ratios.\n",
            "    importance_ratio = tf.exp(action_log_prob - sample_action_log_probs)\n",
            "    importance_ratio_clipped = tf.clip_by_value(\n",
            "        importance_ratio, 1 - self._importance_ratio_clipping,\n",
            "        1 + self._importance_ratio_clipping)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1263,
        "API": ".minimum(",
        "context": [
            "    # Pessimistically choose the minimum objective value for clipped and\n",
            "    #   unclipped importance ratios.\n",
            "    per_timestep_objective = importance_ratio * advantages\n",
            "    per_timestep_objective_clipped = importance_ratio_clipped * advantages\n",
            "    per_timestep_objective_min = tf.minimum(per_timestep_objective,\n",
            "                                            per_timestep_objective_clipped)\n",
            "\n",
            "    if self._importance_ratio_clipping > 0.0:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1276,
        "API": ".reduce_mean(",
        "context": [
            "      policy_gradient_loss = common.aggregate_losses(\n",
            "          per_example_loss=policy_gradient_loss,\n",
            "          sample_weight=weights).total_loss\n",
            "    else:\n",
            "      policy_gradient_loss = tf.math.reduce_mean(policy_gradient_loss * weights)\n",
            "\n",
            "    if self._importance_ratio_clipping > 0.0:\n",
            "      self._clip_fraction = tf.reduce_mean(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1281,
        "API": ".greater(",
        "context": [
            "\n",
            "    if self._importance_ratio_clipping > 0.0:\n",
            "      self._clip_fraction = tf.reduce_mean(\n",
            "          input_tensor=tf.cast(\n",
            "              tf.greater(\n",
            "                  tf.abs(importance_ratio -\n",
            "                         1.0), self._importance_ratio_clipping), tf.float32))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1287,
        "API": ".scalar(",
        "context": [
            "                         1.0), self._importance_ratio_clipping), tf.float32))\n",
            "\n",
            "    if debug_summaries:\n",
            "      if self._importance_ratio_clipping > 0.0:\n",
            "        tf.compat.v2.summary.scalar(\n",
            "            name='clip_fraction',\n",
            "            data=self._clip_fraction,\n",
            "            step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1293,
        "API": ".reduce_mean(",
        "context": [
            "            data=self._clip_fraction,\n",
            "            step=self.train_step_counter)\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='importance_ratio_mean',\n",
            "          data=tf.reduce_mean(input_tensor=importance_ratio),\n",
            "          step=self.train_step_counter)\n",
            "      entropy = common.entropy(current_policy_distribution, self.action_spec)\n",
            "      tf.compat.v2.summary.scalar(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1298,
        "API": ".reduce_mean(",
        "context": [
            "          step=self.train_step_counter)\n",
            "      entropy = common.entropy(current_policy_distribution, self.action_spec)\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='policy_entropy_mean',\n",
            "          data=tf.reduce_mean(input_tensor=entropy),\n",
            "          step=self.train_step_counter)\n",
            "      # TODO(b/171573175): remove the condition once histograms are supported\n",
            "      # on TPUs.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1303,
        "API": ".histogram(",
        "context": [
            "          step=self.train_step_counter)\n",
            "      # TODO(b/171573175): remove the condition once histograms are supported\n",
            "      # on TPUs.\n",
            "      if not tf.config.list_logical_devices('TPU'):\n",
            "        tf.compat.v2.summary.histogram(\n",
            "            name='action_log_prob',\n",
            "            data=action_log_prob,\n",
            "            step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1311,
        "API": ".histogram(",
        "context": [
            "        tf.compat.v2.summary.histogram(\n",
            "            name='action_log_prob_sample',\n",
            "            data=sample_action_log_probs,\n",
            "            step=self.train_step_counter)\n",
            "        tf.compat.v2.summary.histogram(\n",
            "            name='importance_ratio',\n",
            "            data=importance_ratio,\n",
            "            step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1319,
        "API": ".histogram(",
        "context": [
            "        tf.compat.v2.summary.histogram(\n",
            "            name='importance_ratio_clipped',\n",
            "            data=importance_ratio_clipped,\n",
            "            step=self.train_step_counter)\n",
            "        tf.compat.v2.summary.histogram(\n",
            "            name='per_timestep_objective',\n",
            "            data=per_timestep_objective,\n",
            "            step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1327,
        "API": ".histogram(",
        "context": [
            "        tf.compat.v2.summary.histogram(\n",
            "            name='per_timestep_objective_clipped',\n",
            "            data=per_timestep_objective_clipped,\n",
            "            step=self.train_step_counter)\n",
            "        tf.compat.v2.summary.histogram(\n",
            "            name='per_timestep_objective_min',\n",
            "            data=per_timestep_objective_min,\n",
            "            step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1332,
        "API": ".histogram(",
        "context": [
            "            name='per_timestep_objective_min',\n",
            "            data=per_timestep_objective_min,\n",
            "            step=self.train_step_counter)\n",
            "\n",
            "        tf.compat.v2.summary.histogram(\n",
            "            name='policy_entropy', data=entropy, step=self.train_step_counter)\n",
            "        for i, (single_action, single_distribution) in enumerate(\n",
            "            zip(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1337,
        "API": ".flatten(",
        "context": [
            "            name='policy_entropy', data=entropy, step=self.train_step_counter)\n",
            "        for i, (single_action, single_distribution) in enumerate(\n",
            "            zip(\n",
            "                tf.nest.flatten(self.action_spec),\n",
            "                tf.nest.flatten(current_policy_distribution))):\n",
            "          # Categorical distribution (used for discrete actions) doesn't have a\n",
            "          # mean.\n",
            "          distribution_index = '_{}'.format(i) if i > 0 else ''\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1342,
        "API": ".histogram(",
        "context": [
            "          # Categorical distribution (used for discrete actions) doesn't have a\n",
            "          # mean.\n",
            "          distribution_index = '_{}'.format(i) if i > 0 else ''\n",
            "          if not tensor_spec.is_discrete(single_action):\n",
            "            tf.compat.v2.summary.histogram(\n",
            "                name='actions_distribution_mean' + distribution_index,\n",
            "                data=single_distribution.mean(),\n",
            "                step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1350,
        "API": ".histogram(",
        "context": [
            "            tf.compat.v2.summary.histogram(\n",
            "                name='actions_distribution_stddev' + distribution_index,\n",
            "                data=single_distribution.stddev(),\n",
            "                step=self.train_step_counter)\n",
            "        tf.compat.v2.summary.histogram(\n",
            "            name='policy_gradient_loss',\n",
            "            data=policy_gradient_loss,\n",
            "            step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1366,
        "API": ".constant(",
        "context": [
            "                     kl_divergence: types.Tensor,\n",
            "                     debug_summaries: bool = False) -> types.Tensor:\n",
            "    # Squared penalization for mean KL divergence above some threshold.\n",
            "    if self._kl_cutoff_factor <= 0.0:\n",
            "      return tf.constant(0.0, dtype=tf.float32, name='zero_kl_cutoff_loss')\n",
            "    kl_cutoff = self._kl_cutoff_factor * self._adaptive_kl_target\n",
            "    mean_kl = tf.reduce_mean(input_tensor=kl_divergence)\n",
            "    kl_over_cutoff = tf.maximum(mean_kl - kl_cutoff, 0.0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1373,
        "API": ".scalar(",
        "context": [
            "    kl_over_cutoff = tf.maximum(mean_kl - kl_cutoff, 0.0)\n",
            "    kl_cutoff_loss = self._kl_cutoff_coef * tf.square(kl_over_cutoff)\n",
            "\n",
            "    if debug_summaries:\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='kl_cutoff_count',\n",
            "          data=tf.reduce_sum(\n",
            "              input_tensor=tf.cast(kl_divergence > kl_cutoff, dtype=tf.int64)),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1378,
        "API": ".scalar(",
        "context": [
            "          name='kl_cutoff_count',\n",
            "          data=tf.reduce_sum(\n",
            "              input_tensor=tf.cast(kl_divergence > kl_cutoff, dtype=tf.int64)),\n",
            "          step=self.train_step_counter)\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='kl_cutoff_loss',\n",
            "          data=kl_cutoff_loss,\n",
            "          step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1383,
        "API": ".identity(",
        "context": [
            "          name='kl_cutoff_loss',\n",
            "          data=kl_cutoff_loss,\n",
            "          step=self.train_step_counter)\n",
            "\n",
            "    return tf.identity(kl_cutoff_loss, name='kl_cutoff_loss')\n",
            "\n",
            "  def adaptive_kl_loss(self,\n",
            "                       kl_divergence: types.Tensor,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1389,
        "API": ".constant(",
        "context": [
            "  def adaptive_kl_loss(self,\n",
            "                       kl_divergence: types.Tensor,\n",
            "                       debug_summaries: bool = False) -> types.Tensor:\n",
            "    if self._adaptive_kl_beta is None:\n",
            "      return tf.constant(0.0, dtype=tf.float32, name='zero_adaptive_kl_loss')\n",
            "\n",
            "    # Define the loss computation, which depends on the update computation.\n",
            "    mean_kl = tf.reduce_mean(input_tensor=kl_divergence)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1396,
        "API": ".scalar(",
        "context": [
            "    mean_kl = tf.reduce_mean(input_tensor=kl_divergence)\n",
            "    adaptive_kl_loss = self._adaptive_kl_beta * mean_kl\n",
            "\n",
            "    if debug_summaries:\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='adaptive_kl_loss',\n",
            "          data=adaptive_kl_loss,\n",
            "          step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1458,
        "API": ".histogram(",
        "context": [
            "\n",
            "    # TODO(b/171573175): remove the condition once histograms are supported\n",
            "    # on TPUs.\n",
            "    if debug_summaries and not tf.config.list_logical_devices('TPU'):\n",
            "      tf.compat.v2.summary.histogram(\n",
            "          name='kl_divergence',\n",
            "          data=kl_divergence,\n",
            "          step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1465,
        "API": ".add(",
        "context": [
            "          step=self.train_step_counter)\n",
            "\n",
            "    kl_cutoff_loss = self.kl_cutoff_loss(kl_divergence, debug_summaries)\n",
            "    adaptive_kl_loss = self.adaptive_kl_loss(kl_divergence, debug_summaries)\n",
            "    return tf.add(kl_cutoff_loss, adaptive_kl_loss, name='kl_penalty_loss')\n",
            "\n",
            "  def update_adaptive_kl_beta(\n",
            "      self, kl_divergence: types.Tensor) -> Optional[tf.Operation]:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1481,
        "API": ".reduce_mean(",
        "context": [
            "    \"\"\"\n",
            "    if self._adaptive_kl_beta is None:\n",
            "      return tf.no_op()\n",
            "\n",
            "    mean_kl = tf.reduce_mean(input_tensor=kl_divergence)\n",
            "\n",
            "    # Update the adaptive kl beta after each time it is computed.\n",
            "    mean_kl_below_bound = (\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1493,
        "API": ".constant(",
        "context": [
            "        (1.0 + self._adaptive_kl_tolerance))\n",
            "    adaptive_kl_update_factor = tf.case(\n",
            "        [\n",
            "            (mean_kl_below_bound,\n",
            "             lambda: tf.constant(1.0 / 1.5, dtype=tf.float32)),\n",
            "            (mean_kl_above_bound, lambda: tf.constant(1.5, dtype=tf.float32)),\n",
            "        ],\n",
            "        default=lambda: tf.constant(1.0, dtype=tf.float32),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1499,
        "API": ".clip_by_value(",
        "context": [
            "        ],\n",
            "        default=lambda: tf.constant(1.0, dtype=tf.float32),\n",
            "        exclusive=True)\n",
            "\n",
            "    new_adaptive_kl_beta = tf.clip_by_value(\n",
            "        self._adaptive_kl_beta * adaptive_kl_update_factor,\n",
            "        clip_value_min=10e-16,\n",
            "        clip_value_max=10e+16)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1506,
        "API": ".scalar(",
        "context": [
            "        clip_value_max=10e+16)\n",
            "    tf.compat.v1.assign(self._adaptive_kl_beta, new_adaptive_kl_beta)\n",
            "\n",
            "    if self._debug_summaries:\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='adaptive_kl_update_factor',\n",
            "          data=adaptive_kl_update_factor,\n",
            "          step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_agent.py",
        "line_number": 1512,
        "API": ".scalar(",
        "context": [
            "          data=adaptive_kl_update_factor,\n",
            "          step=self.train_step_counter)\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='mean_kl_divergence', data=mean_kl, step=self.train_step_counter)\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='adaptive_kl_beta',\n",
            "          data=self._adaptive_kl_beta,\n",
            "          step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_clip_agent.py",
        "line_number": 155,
        "API": ".map_structure(",
        "context": [
            "        Example usage:\n",
            "          ```python\n",
            "          observation_tensor_spec, action_spec, time_step_tensor_spec = (\n",
            "            spec_utils.get_tensor_specs(env))\n",
            "          normalized_observation_tensor_spec = tf.nest.map_structure(\n",
            "            lambda s: tf.TensorSpec(\n",
            "              dtype=tf.float32, shape=s.shape, name=s.name\n",
            "            ),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_policy.py",
        "line_number": 126,
        "API": ".map_structure(",
        "context": [
            "        # code path using DistributionSpecV2 for that.\n",
            "        network_output_spec = actor_network.output_spec\n",
            "        info_spec = {\n",
            "            'dist_params':\n",
            "                tf.nest.map_structure(lambda spec: spec.input_params_spec,\n",
            "                                      network_output_spec)\n",
            "        }\n",
            "      else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_policy.py",
        "line_number": 136,
        "API": ".format(",
        "context": [
            "          if not isinstance(spec, distribution_utils.DistributionSpecV2):\n",
            "            raise ValueError(\n",
            "                'Unexpected output from `actor_network`.  Expected '\n",
            "                '`Distribution` objects, but saw output spec: {}'\n",
            "                .format(actor_output_spec))\n",
            "          return distribution_utils.parameters_to_dict(\n",
            "              spec.parameters, tensors_only=True)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_policy.py",
        "line_number": 142,
        "API": ".map_structure(",
        "context": [
            "              spec.parameters, tensors_only=True)\n",
            "\n",
            "        info_spec = {\n",
            "            'dist_params':\n",
            "                tf.nest.map_structure(nested_dist_params,\n",
            "                                      actor_output_spec)\n",
            "        }\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_policy.py",
        "line_number": 213,
        "API": ".normalize(",
        "context": [
            "        - value_preds with same outer_dims as time_step\n",
            "        - value_state at the end of the time series\n",
            "    \"\"\"\n",
            "    if self._observation_normalizer:\n",
            "      observations = self._observation_normalizer.normalize(observations)\n",
            "    return self._value_network(observations, step_types, value_state,\n",
            "                               training=training)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_policy.py",
        "line_number": 220,
        "API": ".normalize(",
        "context": [
            "\n",
            "  def _apply_actor_network(self, time_step, policy_state, training=False):\n",
            "    observation = time_step.observation\n",
            "    if self._observation_normalizer:\n",
            "      observation = self._observation_normalizer.normalize(observation)\n",
            "\n",
            "    return self._actor_network(\n",
            "        observation,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_policy.py",
        "line_number": 239,
        "API": ".copy(",
        "context": [
            "  def _distribution(self, time_step, policy_state, training=False):\n",
            "    if not policy_state:\n",
            "      policy_state = {'actor_network_state': (), 'value_network_state': ()}\n",
            "    else:\n",
            "      policy_state = policy_state.copy()\n",
            "\n",
            "    if 'actor_network_state' not in policy_state:\n",
            "      policy_state['actor_network_state'] = ()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_utils.py",
        "line_number": 54,
        "API": ".equal(",
        "context": [
            "  # advantage are both 0. This happens to the last item when the experience gets\n",
            "  # preprocessed, as insufficient information was available for calculating\n",
            "  # advantages.\n",
            "  valid_return_value = ~(\n",
            "      tf.equal(batched_traj.policy_info['return'], 0)\n",
            "      & tf.equal(batched_traj.policy_info['advantage'], 0))\n",
            "\n",
            "  return tf.cast(not_between_episodes & valid_return_value, tf.float32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_utils.py",
        "line_number": 80,
        "API": ".cumsum(",
        "context": [
            "    episode_is_complete = None\n",
            "  else:\n",
            "    # 1.0 for timesteps of all complete episodes. 0.0 for incomplete episode at\n",
            "    #   the end of the sequence.\n",
            "    episode_is_complete = tf.cumsum(\n",
            "        tf.cast(batched_next_time_step.is_last(), tf.float32),\n",
            "        axis=1,\n",
            "        reverse=True) > 0\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_utils.py",
        "line_number": 89,
        "API": ".cast(",
        "context": [
            "  # 1.0 for all valid timesteps. 0.0 where between episodes.\n",
            "  not_between_episodes = ~batched_next_time_step.is_first()\n",
            "\n",
            "  if allow_partial_episodes:\n",
            "    return tf.cast(not_between_episodes, tf.float32)\n",
            "  else:\n",
            "    return tf.cast(episode_is_complete & not_between_episodes, tf.float32)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_utils.py",
        "line_number": 150,
        "API": ".map_structure(",
        "context": [
            "  if not legacy_distribution_network:\n",
            "    def dist_params_dict(d):\n",
            "      return distribution_utils.parameters_to_dict(\n",
            "          distribution_utils.get_parameters(d), tensors_only=True)\n",
            "    return tf.nest.map_structure(dist_params_dict, nested_distribution)\n",
            "\n",
            "  ## Legacy behavior below this line.\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_utils.py",
        "line_number": 165,
        "API": ".is_tensor(",
        "context": [
            "      return k\n",
            "\n",
            "  def _tensor_parameters_only(d, params):\n",
            "    return {_maybe_scale(d, k): params[k]\n",
            "            for k in params if tf.is_tensor(params[k])}\n",
            "\n",
            "  return tf.nest.map_structure(\n",
            "      lambda d: _tensor_parameters_only(d, d.parameters),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_utils.py",
        "line_number": 180,
        "API": ".flatten(",
        "context": [
            "  nest_utils.assert_same_structure(nested_from_distribution,\n",
            "                                   nested_to_distribution)\n",
            "\n",
            "  # Make list pairs of leaf distributions.\n",
            "  flat_from_distribution = tf.nest.flatten(nested_from_distribution)\n",
            "  flat_to_distribution = tf.nest.flatten(nested_to_distribution)\n",
            "  all_kl_divergences = [from_dist.kl_divergence(to_dist)\n",
            "                        for from_dist, to_dist\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\ppo_utils.py",
        "line_number": 193,
        "API": ".reduce_sum(",
        "context": [
            "    reduce_dims = list(range(len(kl_divergence.shape)))\n",
            "    for dim in outer_dims:\n",
            "      reduce_dims.remove(dim)\n",
            "    all_kl_divergences_reduced.append(\n",
            "        tf.reduce_sum(input_tensor=kl_divergence, axis=reduce_dims))\n",
            "\n",
            "  # Sum the kl of the leaves.\n",
            "  total_kl = tf.add_n(all_kl_divergences_reduced)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\examples\\v2\\train_eval_clip_agent.py",
        "line_number": 60,
        "API": ".getenv(",
        "context": [
            "from tf_agents.system import system_multiprocessing as multiprocessing\n",
            "from tf_agents.utils import common\n",
            "\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_string('env_name', 'HalfCheetah-v2', 'Name of an environment')\n",
            "flags.DEFINE_integer('replay_buffer_capacity', 1001,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\examples\\v2\\train_eval_clip_agent.py",
        "line_number": 119,
        "API": ".join(",
        "context": [
            "  if root_dir is None:\n",
            "    raise AttributeError('train_eval requires a root_dir.')\n",
            "\n",
            "  root_dir = os.path.expanduser(root_dir)\n",
            "  train_dir = os.path.join(root_dir, 'train')\n",
            "  eval_dir = os.path.join(root_dir, 'eval')\n",
            "  saved_model_dir = os.path.join(root_dir, 'policy_saved_model')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\examples\\v2\\train_eval_clip_agent.py",
        "line_number": 136,
        "API": ".equal(",
        "context": [
            "  ]\n",
            "\n",
            "  global_step = tf.compat.v1.train.get_or_create_global_step()\n",
            "  with tf.compat.v2.summary.record_if(\n",
            "      lambda: tf.math.equal(global_step % summary_interval, 0)):\n",
            "    if random_seed is not None:\n",
            "      tf.compat.v1.set_random_seed(random_seed)\n",
            "    eval_tf_env = tf_py_environment.TFPyEnvironment(env_load_fn(env_name))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\examples\\v2\\train_eval_clip_agent.py",
        "line_number": 211,
        "API": ".join(",
        "context": [
            "        agent=tf_agent,\n",
            "        global_step=global_step,\n",
            "        metrics=metric_utils.MetricsGroup(train_metrics, 'train_metrics'))\n",
            "    policy_checkpointer = common.Checkpointer(\n",
            "        ckpt_dir=os.path.join(train_dir, 'policy'),\n",
            "        policy=eval_policy,\n",
            "        global_step=global_step)\n",
            "    saved_model = policy_saver.PolicySaver(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\examples\\v2\\train_eval_clip_agent.py",
        "line_number": 231,
        "API": ".function(",
        "context": [
            "      return tf_agent.train(experience=trajectories)\n",
            "\n",
            "    if use_tf_functions:\n",
            "      # TODO(b/123828980): Enable once the cause for slowdown was identified.\n",
            "      collect_driver.run = common.function(collect_driver.run, autograph=False)\n",
            "      tf_agent.train = common.function(tf_agent.train, autograph=False)\n",
            "      train_step = common.function(train_step)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\examples\\v2\\train_eval_clip_agent.py",
        "line_number": 266,
        "API": ".info(",
        "context": [
            "        train_metric.tf_summaries(\n",
            "            train_step=global_step, step_metrics=step_metrics)\n",
            "\n",
            "      if global_step_val % log_interval == 0:\n",
            "        logging.info('step = %d, loss = %f', global_step_val, total_loss)\n",
            "        steps_per_sec = (\n",
            "            (global_step_val - timed_at_step) / (collect_time + train_time))\n",
            "        logging.info('%.3f steps/sec', steps_per_sec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\examples\\v2\\train_eval_clip_agent.py",
        "line_number": 273,
        "API": ".scalar(",
        "context": [
            "        logging.info('%.3f steps/sec', steps_per_sec)\n",
            "        logging.info('collect_time = %.3f, train_time = %.3f', collect_time,\n",
            "                     train_time)\n",
            "        with tf.compat.v2.summary.record_if(True):\n",
            "          tf.compat.v2.summary.scalar(\n",
            "              name='global_steps_per_sec', data=steps_per_sec, step=global_step)\n",
            "\n",
            "        if global_step_val % train_checkpoint_interval == 0:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\examples\\v2\\train_eval_clip_agent.py",
        "line_number": 280,
        "API": ".save(",
        "context": [
            "        if global_step_val % train_checkpoint_interval == 0:\n",
            "          train_checkpointer.save(global_step=global_step_val)\n",
            "\n",
            "        if global_step_val % policy_checkpoint_interval == 0:\n",
            "          policy_checkpointer.save(global_step=global_step_val)\n",
            "          saved_model_path = os.path.join(\n",
            "              saved_model_dir, 'policy_' + ('%d' % global_step_val).zfill(9))\n",
            "          saved_model.save(saved_model_path)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\ppo\\examples\\v2\\train_eval_clip_agent.py",
        "line_number": 302,
        "API": ".set_verbosity(",
        "context": [
            "    )\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  logging.set_verbosity(logging.INFO)\n",
            "  tf.compat.v1.enable_v2_behavior()\n",
            "  train_eval(\n",
            "      FLAGS.root_dir,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\qtopt\\qtopt_agent.py",
        "line_number": 50,
        "API": ".stop_gradient(",
        "context": [
            "\n",
            "def compute_td_targets(next_q_values: types.Tensor,\n",
            "                       rewards: types.Tensor,\n",
            "                       discounts: types.Tensor) -> types.Tensor:\n",
            "  return tf.stop_gradient(rewards + discounts * next_q_values)\n",
            "\n",
            "\n",
            "class QtOptLossInfo(typing.NamedTuple):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\qtopt\\qtopt_agent.py",
        "line_number": 304,
        "API": ".format(",
        "context": [
            "\n",
            "    if q_network.state_spec and n_step_update != 1:\n",
            "      raise NotImplementedError(\n",
            "          'QtOptAgent does not currently support n-step updates with stateful '\n",
            "          'networks (i.e., RNNs), but n_step_update = {}'.format(n_step_update))\n",
            "\n",
            "    # Bypass the train_sequence_length check when RNN is used.\n",
            "    train_sequence_length = (\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\qtopt\\qtopt_agent.py",
        "line_number": 540,
        "API": ".format(",
        "context": [
            "            sample_weight=weights,\n",
            "            regularization_loss=auxiliary_reg_loss)\n",
            "        total_auxiliary_loss += agg_auxiliary_loss.total_loss\n",
            "        losses_dict.update(\n",
            "            {'auxiliary_loss_{}'.format(\n",
            "                auxiliary_loss_fn.__name__\n",
            "                ): agg_auxiliary_loss.weighted,\n",
            "             'auxiliary_reg_loss_{}'.format(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\qtopt\\qtopt_agent.py",
        "line_number": 592,
        "API": ".cast(",
        "context": [
            "          next_q_values,\n",
            "          rewards=self._reward_scale_factor * next_time_steps.reward,\n",
            "          discounts=self._gamma * next_time_steps.discount)\n",
            "\n",
            "      valid_mask = tf.cast(~time_steps.is_last(), tf.float32)\n",
            "      td_error = valid_mask * (td_targets - q_values)\n",
            "\n",
            "      td_loss = valid_mask * self._td_errors_loss_fn(td_targets, q_values)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\qtopt\\qtopt_agent.py",
        "line_number": 601,
        "API": ".reduce_sum(",
        "context": [
            "      if nest_utils.is_batched_nested_tensors(\n",
            "          time_steps, self.time_step_spec, num_outer_dims=2):\n",
            "\n",
            "        # Do a sum over the time dimension.\n",
            "        td_loss = tf.reduce_sum(input_tensor=td_loss, axis=1)\n",
            "\n",
            "      # Aggregate across the elements of the batch and add regularization loss.\n",
            "      # Note: We use an element wise loss above to ensure each element is always\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\qtopt\\qtopt_agent.py",
        "line_number": 681,
        "API": ".minimum(",
        "context": [
            "          step_type=next_time_steps.step_type,\n",
            "          network_state=network_state,\n",
            "          training=False)\n",
            "\n",
            "      q_next_state = tf.minimum(q_values_target_delayed_2,\n",
            "                                q_values_target_delayed)\n",
            "    else:\n",
            "      q_next_state, _ = self._target_q_network(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\reinforce_agent.py",
        "line_number": 58,
        "API": ".moments(",
        "context": [
            "\n",
            "  Returns:\n",
            "    Standardized values (values - mean(values[axes])) / std(values[axes]).\n",
            "  \"\"\"\n",
            "  values_mean, values_var = tf.nn.moments(x=values, axes=axes, keepdims=True)\n",
            "  epsilon = np.finfo(values.dtype.as_numpy_dtype).eps\n",
            "  normalized_values = ((values - values_mean) / (tf.sqrt(values_var) + epsilon))\n",
            "  return normalized_values\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\reinforce_agent.py",
        "line_number": 77,
        "API": ".cast(",
        "context": [
            "  Returns:\n",
            "    A Tensor representing the entropy loss.\n",
            "  \"\"\"\n",
            "  with tf.name_scope('entropy_regularization'):\n",
            "    entropy = -tf.cast(common.entropy(distributions, spec), tf.float32)\n",
            "    if weights is not None:\n",
            "      entropy *= weights\n",
            "    return tf.reduce_mean(input_tensor=entropy)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\reinforce_agent.py",
        "line_number": 87,
        "API": ".shape(",
        "context": [
            "def _get_initial_policy_state(policy, time_steps):\n",
            "  \"\"\"Gets the initial state of a policy.\"\"\"\n",
            "  batch_size = (\n",
            "      tf.compat.dimension_at_index(time_steps.discount.shape, 0) or\n",
            "      tf.shape(time_steps.discount)[0])\n",
            "  return policy.get_initial_state(batch_size=batch_size)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\reinforce_agent.py",
        "line_number": 252,
        "API": ".cast(",
        "context": [
            "    # Add a mask to ensure we reset the return calculation at episode\n",
            "    # boundaries. This is needed in cases where episodes are truncated before\n",
            "    # reaching a terminal state. Note experience is a batch of trajectories\n",
            "    # where reward=next_step.reward so the mask may look shifted at first.\n",
            "    non_last_mask = tf.cast(\n",
            "        tf.math.not_equal(experience.next_step_type, ts.StepType.LAST),\n",
            "        tf.float32)\n",
            "    discounts = non_last_mask * experience.discount * self._gamma\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\reinforce_agent.py",
        "line_number": 260,
        "API": ".histogram(",
        "context": [
            "    returns = value_ops.discounted_return(\n",
            "        experience.reward, discounts, time_major=False)\n",
            "\n",
            "    if self._debug_summaries:\n",
            "      tf.compat.v2.summary.histogram(\n",
            "          name='rewards', data=experience.reward, step=self.train_step_counter)\n",
            "      tf.compat.v2.summary.histogram(\n",
            "          name='discounts',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\reinforce_agent.py",
        "line_number": 266,
        "API": ".histogram(",
        "context": [
            "      tf.compat.v2.summary.histogram(\n",
            "          name='discounts',\n",
            "          data=experience.discount,\n",
            "          step=self.train_step_counter)\n",
            "      tf.compat.v2.summary.histogram(\n",
            "          name='returns', data=returns, step=self.train_step_counter)\n",
            "\n",
            "    with tf.GradientTape() as tape:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\reinforce_agent.py",
        "line_number": 271,
        "API": ".stop_gradient(",
        "context": [
            "          name='returns', data=returns, step=self.train_step_counter)\n",
            "\n",
            "    with tf.GradientTape() as tape:\n",
            "      loss_info = self.total_loss(\n",
            "          experience, tf.stop_gradient(returns), weights=weights,\n",
            "          training=True)\n",
            "      tf.debugging.check_numerics(loss_info.loss, 'Loss is inf or nan')\n",
            "    variables_to_train = self._actor_network.trainable_weights\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\reinforce_agent.py",
        "line_number": 293,
        "API": ".map_structure(",
        "context": [
            "\n",
            "    self._optimizer.apply_gradients(grads_and_vars)\n",
            "    self.train_step_counter.assign_add(1)\n",
            "\n",
            "    return tf.nest.map_structure(tf.identity, loss_info)\n",
            "\n",
            "  def total_loss(self,\n",
            "                 experience: traj.Trajectory,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\reinforce_agent.py",
        "line_number": 302,
        "API": ".zeros_like(",
        "context": [
            "                 weights: types.Tensor,\n",
            "                 training: bool = False) -> tf_agent.LossInfo:\n",
            "    # Ensure we see at least one full episode.\n",
            "    time_steps = ts.TimeStep(experience.step_type,\n",
            "                             tf.zeros_like(experience.reward),\n",
            "                             tf.zeros_like(experience.discount),\n",
            "                             experience.observation)\n",
            "    is_last = experience.is_last()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\reinforce_agent.py",
        "line_number": 319,
        "API": ".cast(",
        "context": [
            "    # is the transition with the last valid reward.  In other words, the\n",
            "    # reward on the boundary transitions do not have valid rewards.  Since\n",
            "    # REINFORCE is calculating a loss w.r.t. the returns (and not bootstrapping)\n",
            "    # keeping the boundary transitions is irrelevant.\n",
            "    valid_mask = tf.cast(experience.is_last(), dtype=tf.float32)\n",
            "    valid_mask = tf.math.cumsum(valid_mask, axis=1, reverse=True)\n",
            "    valid_mask = tf.cast(valid_mask > 0, dtype=tf.float32)\n",
            "    if weights is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\reinforce_agent.py",
        "line_number": 335,
        "API": ".histogram(",
        "context": [
            "      value_preds, _ = self._value_network(time_steps.observation,\n",
            "                                           time_steps.step_type,\n",
            "                                           training=True)\n",
            "      if self._debug_summaries:\n",
            "        tf.compat.v2.summary.histogram(\n",
            "            name='value_preds', data=value_preds, step=self.train_step_counter)\n",
            "\n",
            "    advantages = self._advantage_fn(returns, value_preds)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\reinforce_agent.py",
        "line_number": 340,
        "API": ".histogram(",
        "context": [
            "            name='value_preds', data=value_preds, step=self.train_step_counter)\n",
            "\n",
            "    advantages = self._advantage_fn(returns, value_preds)\n",
            "    if self._debug_summaries:\n",
            "      tf.compat.v2.summary.histogram(\n",
            "          name='advantages', data=advantages, step=self.train_step_counter)\n",
            "\n",
            "    # TODO(b/126592060): replace with tensor normalizer.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\reinforce_agent.py",
        "line_number": 347,
        "API": ".histogram(",
        "context": [
            "    # TODO(b/126592060): replace with tensor normalizer.\n",
            "    if self._normalize_returns:\n",
            "      advantages = _standard_normalize(advantages, axes=(0, 1))\n",
            "      if self._debug_summaries:\n",
            "        tf.compat.v2.summary.histogram(\n",
            "            name='normalized_%s' %\n",
            "            ('advantages' if self._baseline else 'returns'),\n",
            "            data=advantages,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\reinforce_agent.py",
        "line_number": 438,
        "API": ".cast(",
        "context": [
            "                                             self.action_spec)\n",
            "\n",
            "    # Filter out transitions between end state of previous episode and start\n",
            "    # state of next episode.\n",
            "    valid_mask = tf.cast(~is_boundary, tf.float32)\n",
            "    action_log_prob *= valid_mask\n",
            "\n",
            "    action_log_prob_times_return = action_log_prob * returns\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\reinforce_agent.py",
        "line_number": 447,
        "API": ".histogram(",
        "context": [
            "    if weights is not None:\n",
            "      action_log_prob_times_return *= weights\n",
            "\n",
            "    if self._debug_summaries:\n",
            "      tf.compat.v2.summary.histogram(\n",
            "          name='action_log_prob',\n",
            "          data=action_log_prob,\n",
            "          step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\reinforce_agent.py",
        "line_number": 459,
        "API": ".reduce_sum(",
        "context": [
            "\n",
            "    # Policy gradient loss is defined as the sum, over timesteps, of action\n",
            "    #   log-probability times the cumulative return from that timestep onward.\n",
            "    #   For more information, see (Williams, 1992).\n",
            "    policy_gradient_loss = -tf.reduce_sum(\n",
            "        input_tensor=action_log_prob_times_return)\n",
            "\n",
            "    # We take the mean over episodes by dividing by num_episodes.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\reinforce_agent.py",
        "line_number": 490,
        "API": ".constant(",
        "context": [
            "    if self._entropy_regularization:\n",
            "      loss = _entropy_loss(actions_distribution, self.action_spec, weights)\n",
            "      loss *= self._entropy_regularization\n",
            "    else:\n",
            "      loss = tf.constant(0.0, dtype=tf.float32)\n",
            "\n",
            "    return loss\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\reinforce_agent.py",
        "line_number": 512,
        "API": ".squared_difference(",
        "context": [
            "\n",
            "    Returns:\n",
            "      value_estimation_loss: A scalar value_estimation_loss loss.\n",
            "    \"\"\"\n",
            "    value_estimation_error = tf.math.squared_difference(returns, value_preds)\n",
            "    if weights is not None:\n",
            "      value_estimation_error *= weights\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\reinforce_agent.py",
        "line_number": 517,
        "API": ".reduce_sum(",
        "context": [
            "    if weights is not None:\n",
            "      value_estimation_error *= weights\n",
            "\n",
            "    value_estimation_loss = (\n",
            "        tf.reduce_sum(input_tensor=value_estimation_error) *\n",
            "        self._value_estimation_loss_coef)\n",
            "\n",
            "    # We take the mean over episodes by dividing by num_episodes.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\examples\\v2\\train_eval.py",
        "line_number": 53,
        "API": ".getenv(",
        "context": [
            "from tf_agents.networks import value_network\n",
            "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
            "from tf_agents.utils import common\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_integer('num_iterations', 500,\n",
            "                     'Total number train/eval iterations to perform.')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\examples\\v2\\train_eval.py",
        "line_number": 89,
        "API": ".join(",
        "context": [
            "    summarize_grads_and_vars=False,\n",
            "    eval_metrics_callback=None):\n",
            "  \"\"\"A simple train and eval for Reinforce.\"\"\"\n",
            "  root_dir = os.path.expanduser(root_dir)\n",
            "  train_dir = os.path.join(root_dir, 'train')\n",
            "  eval_dir = os.path.join(root_dir, 'eval')\n",
            "\n",
            "  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\examples\\v2\\train_eval.py",
        "line_number": 104,
        "API": ".equal(",
        "context": [
            "      tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes),\n",
            "  ]\n",
            "\n",
            "  with tf.compat.v2.summary.record_if(\n",
            "      lambda: tf.math.equal(global_step % summary_interval, 0)):\n",
            "    tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n",
            "    eval_tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\examples\\v2\\train_eval.py",
        "line_number": 162,
        "API": ".function(",
        "context": [
            "      return tf_agent.train(experience)\n",
            "\n",
            "    if use_tf_functions:\n",
            "      # To speed up collect use TF function.\n",
            "      collect_driver.run = common.function(collect_driver.run)\n",
            "      # To speed up train use TF function.\n",
            "      tf_agent.train = common.function(tf_agent.train)\n",
            "      train_step = common.function(train_step)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\examples\\v2\\train_eval.py",
        "line_number": 199,
        "API": ".info(",
        "context": [
            "      time_acc += time.time() - start_time\n",
            "\n",
            "      global_step_val = global_step.numpy()\n",
            "      if global_step_val % log_interval == 0:\n",
            "        logging.info('step = %d, loss = %f', global_step_val, total_loss.loss)\n",
            "        steps_per_sec = (global_step_val - timed_at_step) / time_acc\n",
            "        logging.info('%.3f steps/sec', steps_per_sec)\n",
            "        tf.compat.v2.summary.scalar(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\reinforce\\examples\\v2\\train_eval.py",
        "line_number": 231,
        "API": ".set_verbosity(",
        "context": [
            "def main(_):\n",
            "  tf.compat.v1.enable_eager_execution(\n",
            "      config=tf.compat.v1.ConfigProto(allow_soft_placement=True))\n",
            "  tf.compat.v1.enable_v2_behavior()\n",
            "  logging.set_verbosity(logging.INFO)\n",
            "  train_eval(FLAGS.root_dir, num_iterations=FLAGS.num_iterations)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\sac_agent.py",
        "line_number": 53,
        "API": ".clip_by_value(",
        "context": [
            "\n",
            "# TODO(b/148889463): deprecate std_clip_transform\n",
            "@gin.configurable\n",
            "def std_clip_transform(stddevs: types.NestedTensor) -> types.NestedTensor:\n",
            "  stddevs = tf.nest.map_structure(lambda t: tf.clip_by_value(t, -20, 2),\n",
            "                                  stddevs)\n",
            "  return tf.exp(stddevs)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\sac_agent.py",
        "line_number": 161,
        "API": ".copy(",
        "context": [
            "\n",
            "    if critic_network_2 is not None:\n",
            "      self._critic_network_2 = critic_network_2\n",
            "    else:\n",
            "      self._critic_network_2 = critic_network.copy(name='CriticNetwork2')\n",
            "      # Do not use target_critic_network_2 if critic_network_2 is None.\n",
            "      target_critic_network_2 = None\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\sac_agent.py",
        "line_number": 257,
        "API": ".flatten(",
        "context": [
            "          optimizer)\n",
            "    return optimizer\n",
            "\n",
            "  def _check_action_spec(self, action_spec):\n",
            "    flat_action_spec = tf.nest.flatten(action_spec)\n",
            "    for spec in flat_action_spec:\n",
            "      if spec.dtype.is_integer:\n",
            "        raise NotImplementedError(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\sac_agent.py",
        "line_number": 262,
        "API": ".format(",
        "context": [
            "    for spec in flat_action_spec:\n",
            "      if spec.dtype.is_integer:\n",
            "        raise NotImplementedError(\n",
            "            'SacAgent does not currently support discrete actions. '\n",
            "            'Action spec: {}'.format(action_spec))\n",
            "\n",
            "  def _get_default_target_entropy(self, action_spec):\n",
            "    # If target_entropy was not passed, set it to -dim(A)/2.0\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\sac_agent.py",
        "line_number": 269,
        "API": ".flatten(",
        "context": [
            "    # If target_entropy was not passed, set it to -dim(A)/2.0\n",
            "    # Note that the original default entropy target is -dim(A) in the SAC paper.\n",
            "    # However this formulation has also been used in practice by the original\n",
            "    # authors and has in our experience been more stable for gym/mujoco.\n",
            "    flat_action_spec = tf.nest.flatten(action_spec)\n",
            "    target_entropy = -np.sum([\n",
            "        np.prod(single_spec.shape.as_list())\n",
            "        for single_spec in flat_action_spec\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\sac_agent.py",
        "line_number": 357,
        "API": ".scalar(",
        "context": [
            "    alpha_grads = tape.gradient(alpha_loss, alpha_variable)\n",
            "    self._apply_gradients(alpha_grads, alpha_variable, self._alpha_optimizer)\n",
            "\n",
            "    with tf.name_scope('Losses'):\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='critic_loss', data=critic_loss, step=self.train_step_counter)\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='actor_loss', data=actor_loss, step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\sac_agent.py",
        "line_number": 414,
        "API": ".scalar(",
        "context": [
            "        time_steps, weights=weights, training=training)\n",
            "    tf.debugging.check_numerics(alpha_loss, 'Alpha loss is inf or nan.')\n",
            "\n",
            "    with tf.name_scope('Losses'):\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='critic_loss', data=critic_loss, step=self.train_step_counter)\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='actor_loss', data=actor_loss, step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\sac_agent.py",
        "line_number": 479,
        "API": ".group(",
        "context": [
            "            target_critic_2_update_vars,\n",
            "            tau,\n",
            "            tau_non_trainable=1.0)\n",
            "\n",
            "        return tf.group(critic_update_1, critic_update_2)\n",
            "\n",
            "      return common.Periodically(update, period, 'update_targets')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\sac_agent.py",
        "line_number": 496,
        "API": ".map_structure(",
        "context": [
            "      action_distribution = self._policy.distribution(\n",
            "          time_steps, policy_state=policy_state).action\n",
            "\n",
            "    # Sample actions and log_pis from transformed distribution.\n",
            "    actions = tf.nest.map_structure(lambda d: d.sample(), action_distribution)\n",
            "    log_pi = common.log_probability(action_distribution, actions,\n",
            "                                    self.action_spec)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\sac_agent.py",
        "line_number": 542,
        "API": ".minimum(",
        "context": [
            "          target_input, step_type=next_time_steps.step_type, training=False)\n",
            "      target_q_values2, unused_network_state2 = self._target_critic_network_2(\n",
            "          target_input, step_type=next_time_steps.step_type, training=False)\n",
            "      target_q_values = (\n",
            "          tf.minimum(target_q_values1, target_q_values2) -\n",
            "          tf.exp(self._log_alpha) * next_log_pis)\n",
            "\n",
            "      td_targets = tf.stop_gradient(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\sac_agent.py",
        "line_number": 601,
        "API": ".minimum(",
        "context": [
            "      target_q_values1, _ = self._critic_network_1(\n",
            "          target_input, step_type=time_steps.step_type, training=False)\n",
            "      target_q_values2, _ = self._critic_network_2(\n",
            "          target_input, step_type=time_steps.step_type, training=False)\n",
            "      target_q_values = tf.minimum(target_q_values1, target_q_values2)\n",
            "      actor_loss = tf.exp(self._log_alpha) * log_pi - target_q_values\n",
            "      if actor_loss.shape.rank > 1:\n",
            "        # Sum over the time dimension.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\sac_agent.py",
        "line_number": 639,
        "API": ".stop_gradient(",
        "context": [
            "\n",
            "      # We do not update actor during alpha loss.\n",
            "      unused_actions, log_pi = self._actions_and_log_probs(\n",
            "          time_steps, training=False)\n",
            "      entropy_diff = tf.stop_gradient(-log_pi - self._target_entropy)\n",
            "      if self._use_log_alpha_in_alpha_loss:\n",
            "        alpha_loss = (self._log_alpha * entropy_diff)\n",
            "      else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\sac_agent.py",
        "line_number": 647,
        "API": ".reduce_sum(",
        "context": [
            "        alpha_loss = (tf.exp(self._log_alpha) * entropy_diff)\n",
            "\n",
            "      if alpha_loss.shape.rank > 1:\n",
            "        # Sum over the time dimension.\n",
            "        alpha_loss = tf.reduce_sum(\n",
            "            alpha_loss, axis=range(1, alpha_loss.shape.rank))\n",
            "\n",
            "      agg_loss = common.aggregate_losses(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\sac_agent.py",
        "line_number": 663,
        "API": ".concat(",
        "context": [
            "                                   pred_td_targets2):\n",
            "    if self._debug_summaries:\n",
            "      td_errors1 = td_targets - pred_td_targets1\n",
            "      td_errors2 = td_targets - pred_td_targets2\n",
            "      td_errors = tf.concat([td_errors1, td_errors2], axis=0)\n",
            "      common.generate_tensor_summaries('td_errors', td_errors,\n",
            "                                       self.train_step_counter)\n",
            "      common.generate_tensor_summaries('td_targets', td_targets,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\sac_agent.py",
        "line_number": 688,
        "API": ".scalar(",
        "context": [
            "        # generate actions.\n",
            "\n",
            "      common.generate_tensor_summaries('log_pi', log_pi,\n",
            "                                       self.train_step_counter)\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='entropy_avg',\n",
            "          data=-tf.reduce_mean(input_tensor=log_pi),\n",
            "          step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\sac_agent.py",
        "line_number": 724,
        "API": ".scalar(",
        "context": [
            "                                       self.train_step_counter)\n",
            "      common.generate_tensor_summaries('entropy_diff', entropy_diff,\n",
            "                                       self.train_step_counter)\n",
            "\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='log_alpha', data=self._log_alpha, step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\tanh_normal_projection_network.py",
        "line_number": 63,
        "API": ".flatten(",
        "context": [
            "      activation_fn: Activation function to use in dense layer.\n",
            "      std_transform: Transformation function to apply to the stddevs.\n",
            "      name: A string representing name of the network.\n",
            "    \"\"\"\n",
            "    if len(tf.nest.flatten(sample_spec)) != 1:\n",
            "      raise ValueError('Tanh Normal Projection network only supports single'\n",
            "                       ' spec samples.')\n",
            "    output_spec = self._output_distribution_spec(sample_spec, name)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\tanh_normal_projection_network.py",
        "line_number": 115,
        "API": ".format(",
        "context": [
            "\n",
            "    if mask is not None:\n",
            "      raise NotImplementedError(\n",
            "          'TanhNormalProjectionNetwork does not yet implement action masking; '\n",
            "          'got mask={}'.format(mask))\n",
            "\n",
            "    # outer_rank is needed because the projection is not done on the raw\n",
            "    # observations so getting the outer rank is hard as there is no spec to\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\tanh_normal_projection_network.py",
        "line_number": 121,
        "API": ".flatten(",
        "context": [
            "    # outer_rank is needed because the projection is not done on the raw\n",
            "    # observations so getting the outer rank is hard as there is no spec to\n",
            "    # compare to.\n",
            "    batch_squash = network_utils.BatchSquash(outer_rank)\n",
            "    inputs = batch_squash.flatten(inputs)\n",
            "\n",
            "    means_and_stds = self._projection_layer(inputs, training=training)\n",
            "    means, stds = tf.split(means_and_stds, num_or_size_splits=2, axis=-1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\tanh_normal_projection_network.py",
        "line_number": 126,
        "API": ".cast(",
        "context": [
            "\n",
            "    means_and_stds = self._projection_layer(inputs, training=training)\n",
            "    means, stds = tf.split(means_and_stds, num_or_size_splits=2, axis=-1)\n",
            "    means = tf.reshape(means, [-1] + self._sample_spec.shape.as_list())\n",
            "    means = tf.cast(means, self._sample_spec.dtype)\n",
            "\n",
            "    if self._std_transform is not None:\n",
            "      stds = self._std_transform(stds)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval.py",
        "line_number": 61,
        "API": ".getenv(",
        "context": [
            "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
            "from tf_agents.utils import common\n",
            "\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_multi_string('gin_file', None, 'Path to the trainer config files.')\n",
            "flags.DEFINE_multi_string('gin_param', None, 'Gin binding to pass through.')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval.py",
        "line_number": 121,
        "API": ".join(",
        "context": [
            "    summarize_grads_and_vars=False,\n",
            "    eval_metrics_callback=None):\n",
            "  \"\"\"A simple train and eval for SAC.\"\"\"\n",
            "  root_dir = os.path.expanduser(root_dir)\n",
            "  train_dir = os.path.join(root_dir, 'train')\n",
            "  eval_dir = os.path.join(root_dir, 'eval')\n",
            "\n",
            "  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval.py",
        "line_number": 137,
        "API": ".equal(",
        "context": [
            "  ]\n",
            "\n",
            "  global_step = tf.compat.v1.train.get_or_create_global_step()\n",
            "  with tf.compat.v2.summary.record_if(\n",
            "      lambda: tf.math.equal(global_step % summary_interval, 0)):\n",
            "    tf_env = tf_py_environment.TFPyEnvironment(env_load_fn(env_name))\n",
            "    eval_env_name = eval_env_name or env_name\n",
            "    eval_tf_env = tf_py_environment.TFPyEnvironment(env_load_fn(eval_env_name))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval.py",
        "line_number": 233,
        "API": ".function(",
        "context": [
            "        observers=replay_observer + train_metrics,\n",
            "        num_steps=collect_steps_per_iteration)\n",
            "\n",
            "    if use_tf_functions:\n",
            "      initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
            "      collect_driver.run = common.function(collect_driver.run)\n",
            "      tf_agent.train = common.function(tf_agent.train)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval.py",
        "line_number": 239,
        "API": ".info(",
        "context": [
            "      tf_agent.train = common.function(tf_agent.train)\n",
            "\n",
            "    if replay_buffer.num_frames() == 0:\n",
            "      # Collect initial replay data.\n",
            "      logging.info(\n",
            "          'Initializing replay buffer by collecting experience for %d steps '\n",
            "          'with a random policy.', initial_collect_steps)\n",
            "      initial_collect_driver.run()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval.py",
        "line_number": 269,
        "API": ".batch(",
        "context": [
            "      return ~trajectories.is_boundary()[0]\n",
            "    dataset = replay_buffer.as_dataset(\n",
            "        sample_batch_size=batch_size,\n",
            "        num_steps=2).unbatch().filter(\n",
            "            _filter_invalid_transition).batch(batch_size).prefetch(5)\n",
            "    # Dataset generates trajectories with shape [Bx2x...]\n",
            "    iterator = iter(dataset)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval.py",
        "line_number": 278,
        "API": ".function(",
        "context": [
            "      experience, _ = next(iterator)\n",
            "      return tf_agent.train(experience)\n",
            "\n",
            "    if use_tf_functions:\n",
            "      train_step = common.function(train_step)\n",
            "\n",
            "    global_step_val = global_step.numpy()\n",
            "    while global_step_val < num_iterations:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval.py",
        "line_number": 294,
        "API": ".info(",
        "context": [
            "\n",
            "      global_step_val = global_step.numpy()\n",
            "\n",
            "      if global_step_val % log_interval == 0:\n",
            "        logging.info('step = %d, loss = %f', global_step_val,\n",
            "                     train_loss.loss)\n",
            "        steps_per_sec = (global_step_val - timed_at_step) / time_acc\n",
            "        logging.info('%.3f steps/sec', steps_per_sec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval.py",
        "line_number": 322,
        "API": ".save(",
        "context": [
            "          eval_metrics_callback(results, global_step_val)\n",
            "        metric_utils.log_metrics(eval_metrics)\n",
            "\n",
            "      if global_step_val % train_checkpoint_interval == 0:\n",
            "        train_checkpointer.save(global_step=global_step_val)\n",
            "\n",
            "      if global_step_val % policy_checkpoint_interval == 0:\n",
            "        policy_checkpointer.save(global_step=global_step_val)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval.py",
        "line_number": 328,
        "API": ".save(",
        "context": [
            "      if global_step_val % policy_checkpoint_interval == 0:\n",
            "        policy_checkpointer.save(global_step=global_step_val)\n",
            "\n",
            "      if global_step_val % rb_checkpoint_interval == 0:\n",
            "        rb_checkpointer.save(global_step=global_step_val)\n",
            "    return train_loss\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval.py",
        "line_number": 334,
        "API": ".set_verbosity(",
        "context": [
            "\n",
            "\n",
            "def main(_):\n",
            "  tf.compat.v1.enable_v2_behavior()\n",
            "  logging.set_verbosity(logging.INFO)\n",
            "  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_param)\n",
            "  train_eval(FLAGS.root_dir)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval_rnn.py",
        "line_number": 60,
        "API": ".getenv(",
        "context": [
            "from tf_agents.policies import random_tf_policy\n",
            "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
            "from tf_agents.utils import common\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_multi_string('gin_file', None, 'Path to the trainer config files.')\n",
            "flags.DEFINE_multi_string('gin_param', None, 'Gin binding to pass through.')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval_rnn.py",
        "line_number": 228,
        "API": ".join(",
        "context": [
            "        tf_env.time_step_spec(), tf_env.action_spec())\n",
            "    collect_policy = tf_agent.collect_policy\n",
            "\n",
            "    train_checkpointer = common.Checkpointer(\n",
            "        ckpt_dir=os.path.join(root_dir, 'train'),\n",
            "        agent=tf_agent,\n",
            "        global_step=global_step,\n",
            "        metrics=metric_utils.MetricsGroup(train_metrics, 'train_metrics'))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval_rnn.py",
        "line_number": 233,
        "API": ".join(",
        "context": [
            "        agent=tf_agent,\n",
            "        global_step=global_step,\n",
            "        metrics=metric_utils.MetricsGroup(train_metrics, 'train_metrics'))\n",
            "    policy_checkpointer = common.Checkpointer(\n",
            "        ckpt_dir=os.path.join(root_dir, 'policy'),\n",
            "        policy=eval_policy,\n",
            "        global_step=global_step)\n",
            "    rb_checkpointer = common.Checkpointer(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval_rnn.py",
        "line_number": 257,
        "API": ".function(",
        "context": [
            "        observers=replay_observer + train_metrics,\n",
            "        num_episodes=collect_episodes_per_iteration)\n",
            "\n",
            "    if use_tf_functions:\n",
            "      initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
            "      collect_driver.run = common.function(collect_driver.run)\n",
            "      tf_agent.train = common.function(tf_agent.train)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval_rnn.py",
        "line_number": 263,
        "API": ".info(",
        "context": [
            "      tf_agent.train = common.function(tf_agent.train)\n",
            "\n",
            "    # Collect initial replay data.\n",
            "    if env_steps.result() == 0 or replay_buffer.num_frames() == 0:\n",
            "      logging.info(\n",
            "          'Initializing replay buffer by collecting experience for %d episodes '\n",
            "          'with a random policy.', initial_collect_episodes)\n",
            "      initial_collect_driver.run()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval_rnn.py",
        "line_number": 292,
        "API": ".reduce_all(",
        "context": [
            "    def _filter_invalid_transition(trajectories, unused_arg1):\n",
            "      # Reduce filter_fn over full trajectory sampled. The sequence is kept only\n",
            "      # if all elements except for the last one pass the filter. This is to\n",
            "      # allow training on terminal steps.\n",
            "      return tf.reduce_all(~trajectories.is_boundary()[:-1])\n",
            "    dataset = replay_buffer.as_dataset(\n",
            "        sample_batch_size=batch_size,\n",
            "        num_steps=train_sequence_length+1).unbatch().filter(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval_rnn.py",
        "line_number": 322,
        "API": ".info(",
        "context": [
            "          train_step()\n",
            "        time_acc += time.time() - start_time\n",
            "\n",
            "        if global_step.numpy() % log_interval == 0:\n",
            "          logging.info('env steps = %d, average return = %f',\n",
            "                       env_steps.result(), average_return.result())\n",
            "          env_steps_per_sec = (env_steps.result().numpy() -\n",
            "                               env_steps_before) / time_acc\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval_rnn.py",
        "line_number": 327,
        "API": ".scalar(",
        "context": [
            "                       env_steps.result(), average_return.result())\n",
            "          env_steps_per_sec = (env_steps.result().numpy() -\n",
            "                               env_steps_before) / time_acc\n",
            "          logging.info('%.3f env steps/sec', env_steps_per_sec)\n",
            "          tf.compat.v2.summary.scalar(\n",
            "              name='env_steps_per_sec',\n",
            "              data=env_steps_per_sec,\n",
            "              step=env_steps.result())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval_rnn.py",
        "line_number": 353,
        "API": ".save(",
        "context": [
            "          metric_utils.log_metrics(eval_metrics)\n",
            "\n",
            "        global_step_val = global_step.numpy()\n",
            "        if global_step_val % train_checkpoint_interval == 0:\n",
            "          train_checkpointer.save(global_step=global_step_val)\n",
            "\n",
            "        if global_step_val % policy_checkpoint_interval == 0:\n",
            "          policy_checkpointer.save(global_step=global_step_val)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\sac\\examples\\v2\\train_eval_rnn.py",
        "line_number": 359,
        "API": ".save(",
        "context": [
            "        if global_step_val % policy_checkpoint_interval == 0:\n",
            "          policy_checkpointer.save(global_step=global_step_val)\n",
            "\n",
            "        if global_step_val % rb_checkpoint_interval == 0:\n",
            "          rb_checkpointer.save(global_step=global_step_val)\n",
            "\n",
            "\n",
            "def main(_):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\td3_agent.py",
        "line_number": 157,
        "API": ".copy(",
        "context": [
            "\n",
            "    if critic_network_2 is not None:\n",
            "      self._critic_network_2 = critic_network_2\n",
            "    else:\n",
            "      self._critic_network_2 = critic_network.copy(name='CriticNetwork2')\n",
            "      # Do not use target_critic_network_2 if critic_network_2 is None.\n",
            "      target_critic_network_2 = None\n",
            "    self._critic_network_2.create_variables()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\td3_agent.py",
        "line_number": 276,
        "API": ".group(",
        "context": [
            "            actor_update_vars,\n",
            "            target_actor_update_vars,\n",
            "            tau,\n",
            "            tau_non_trainable=1.0)\n",
            "        return tf.group(critic_update_1, critic_update_2, actor_update)\n",
            "\n",
            "      return common.Periodically(update, period, 'update_targets')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\td3_agent.py",
        "line_number": 315,
        "API": ".cond(",
        "context": [
            "                            self._actor_optimizer)\n",
            "      return tf.no_op()\n",
            "\n",
            "    remainder = tf.math.mod(self.train_step_counter, self._actor_update_period)\n",
            "    tf.cond(\n",
            "        pred=tf.equal(remainder, 0), true_fn=optimize_actor, false_fn=tf.no_op)\n",
            "\n",
            "    self.train_step_counter.assign_add(1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\td3_agent.py",
        "line_number": 368,
        "API": ".zeros_like(",
        "context": [
            "          training=training)\n",
            "\n",
            "      # Add gaussian noise to each action before computing target q values\n",
            "      def add_noise_to_action(action):  # pylint: disable=missing-docstring\n",
            "        dist = tfp.distributions.Normal(loc=tf.zeros_like(action),\n",
            "                                        scale=self._target_policy_noise * \\\n",
            "                                        tf.ones_like(action))\n",
            "        noise = dist.sample()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\td3_agent.py",
        "line_number": 376,
        "API": ".map_structure(",
        "context": [
            "        noise = tf.clip_by_value(noise, -self._target_policy_noise_clip,\n",
            "                                 self._target_policy_noise_clip)\n",
            "        return action + noise\n",
            "\n",
            "      noisy_target_actions = tf.nest.map_structure(add_noise_to_action,\n",
            "                                                   target_actions)\n",
            "\n",
            "      # Target q-values are the min of the two networks\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\td3_agent.py",
        "line_number": 390,
        "API": ".minimum(",
        "context": [
            "      target_q_values_2, _ = self._target_critic_network_2(\n",
            "          target_q_input_2,\n",
            "          next_time_steps.step_type,\n",
            "          training=False)\n",
            "      target_q_values = tf.minimum(target_q_values_1, target_q_values_2)\n",
            "\n",
            "      td_targets = tf.stop_gradient(\n",
            "          self._reward_scale_factor * next_time_steps.reward +\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\td3_agent.py",
        "line_number": 405,
        "API": ".histogram(",
        "context": [
            "          pred_input_2, time_steps.step_type, training=training)\n",
            "      pred_td_targets_all = [pred_td_targets_1, pred_td_targets_2]\n",
            "\n",
            "      if self._debug_summaries:\n",
            "        tf.compat.v2.summary.histogram(\n",
            "            name='td_targets', data=td_targets, step=self.train_step_counter)\n",
            "        with tf.name_scope('td_targets'):\n",
            "          tf.compat.v2.summary.scalar(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\td3_agent.py",
        "line_number": 410,
        "API": ".reduce_mean(",
        "context": [
            "            name='td_targets', data=td_targets, step=self.train_step_counter)\n",
            "        with tf.name_scope('td_targets'):\n",
            "          tf.compat.v2.summary.scalar(\n",
            "              name='mean',\n",
            "              data=tf.reduce_mean(input_tensor=td_targets),\n",
            "              step=self.train_step_counter)\n",
            "          tf.compat.v2.summary.scalar(\n",
            "              name='max',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\td3_agent.py",
        "line_number": 416,
        "API": ".scalar(",
        "context": [
            "          tf.compat.v2.summary.scalar(\n",
            "              name='max',\n",
            "              data=tf.reduce_max(input_tensor=td_targets),\n",
            "              step=self.train_step_counter)\n",
            "          tf.compat.v2.summary.scalar(\n",
            "              name='min',\n",
            "              data=tf.reduce_min(input_tensor=td_targets),\n",
            "              step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\td3_agent.py",
        "line_number": 425,
        "API": ".histogram(",
        "context": [
            "        for td_target_idx in range(2):\n",
            "          pred_td_targets = pred_td_targets_all[td_target_idx]\n",
            "          td_errors = td_targets - pred_td_targets\n",
            "          with tf.name_scope('critic_net_%d' % (td_target_idx + 1)):\n",
            "            tf.compat.v2.summary.histogram(\n",
            "                name='td_errors', data=td_errors, step=self.train_step_counter)\n",
            "            tf.compat.v2.summary.histogram(\n",
            "                name='pred_td_targets',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\td3_agent.py",
        "line_number": 432,
        "API": ".scalar(",
        "context": [
            "                name='pred_td_targets',\n",
            "                data=pred_td_targets,\n",
            "                step=self.train_step_counter)\n",
            "            with tf.name_scope('td_errors'):\n",
            "              tf.compat.v2.summary.scalar(\n",
            "                  name='mean',\n",
            "                  data=tf.reduce_mean(input_tensor=td_errors),\n",
            "                  step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\td3_agent.py",
        "line_number": 438,
        "API": ".abs(",
        "context": [
            "                  data=tf.reduce_mean(input_tensor=td_errors),\n",
            "                  step=self.train_step_counter)\n",
            "              tf.compat.v2.summary.scalar(\n",
            "                  name='mean_abs',\n",
            "                  data=tf.reduce_mean(input_tensor=tf.abs(td_errors)),\n",
            "                  step=self.train_step_counter)\n",
            "              tf.compat.v2.summary.scalar(\n",
            "                  name='max',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\td3_agent.py",
        "line_number": 444,
        "API": ".scalar(",
        "context": [
            "              tf.compat.v2.summary.scalar(\n",
            "                  name='max',\n",
            "                  data=tf.reduce_max(input_tensor=td_errors),\n",
            "                  step=self.train_step_counter)\n",
            "              tf.compat.v2.summary.scalar(\n",
            "                  name='min',\n",
            "                  data=tf.reduce_min(input_tensor=td_errors),\n",
            "                  step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\td3_agent.py",
        "line_number": 449,
        "API": ".scalar(",
        "context": [
            "                  name='min',\n",
            "                  data=tf.reduce_min(input_tensor=td_errors),\n",
            "                  step=self.train_step_counter)\n",
            "            with tf.name_scope('pred_td_targets'):\n",
            "              tf.compat.v2.summary.scalar(\n",
            "                  name='mean',\n",
            "                  data=tf.reduce_mean(input_tensor=pred_td_targets),\n",
            "                  step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\td3_agent.py",
        "line_number": 455,
        "API": ".reduce_max(",
        "context": [
            "                  data=tf.reduce_mean(input_tensor=pred_td_targets),\n",
            "                  step=self.train_step_counter)\n",
            "              tf.compat.v2.summary.scalar(\n",
            "                  name='max',\n",
            "                  data=tf.reduce_max(input_tensor=pred_td_targets),\n",
            "                  step=self.train_step_counter)\n",
            "              tf.compat.v2.summary.scalar(\n",
            "                  name='min',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\td3_agent.py",
        "line_number": 467,
        "API": ".reduce_sum(",
        "context": [
            "                     + self._td_errors_loss_fn(td_targets, pred_td_targets_2))\n",
            "      if nest_utils.is_batched_nested_tensors(\n",
            "          time_steps, self.time_step_spec, num_outer_dims=2):\n",
            "        # Sum over the time dimension.\n",
            "        critic_loss = tf.reduce_sum(\n",
            "            input_tensor=critic_loss, axis=range(1, critic_loss.shape.rank))\n",
            "\n",
            "      if weights is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\td3_agent.py",
        "line_number": 473,
        "API": ".reduce_mean(",
        "context": [
            "\n",
            "      if weights is not None:\n",
            "        critic_loss *= weights\n",
            "\n",
            "      return tf.reduce_mean(input_tensor=critic_loss)\n",
            "\n",
            "  def actor_loss(self,\n",
            "                 time_steps: ts.TimeStep,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\td3_agent.py",
        "line_number": 501,
        "API": ".reduce_sum(",
        "context": [
            "                                           training=False)\n",
            "      actor_loss = -q_values\n",
            "      # Sum over the time dimension.\n",
            "      if actor_loss.shape.rank > 1:\n",
            "        actor_loss = tf.reduce_sum(\n",
            "            actor_loss, axis=range(1, actor_loss.shape.rank))\n",
            "      actor_loss = common.aggregate_losses(\n",
            "          per_example_loss=actor_loss, sample_weight=weights).total_loss\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\td3_agent.py",
        "line_number": 507,
        "API": ".scalar(",
        "context": [
            "      actor_loss = common.aggregate_losses(\n",
            "          per_example_loss=actor_loss, sample_weight=weights).total_loss\n",
            "\n",
            "      with tf.name_scope('Losses/'):\n",
            "        tf.compat.v2.summary.scalar(\n",
            "            name='actor_loss', data=actor_loss, step=self.train_step_counter)\n",
            "\n",
            "    return actor_loss\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\examples\\v2\\train_eval.py",
        "line_number": 108,
        "API": ".join(",
        "context": [
            "    eval_metrics_callback=None):\n",
            "\n",
            "  \"\"\"A simple train and eval for TD3.\"\"\"\n",
            "  root_dir = os.path.expanduser(root_dir)\n",
            "  train_dir = os.path.join(root_dir, 'train')\n",
            "  eval_dir = os.path.join(root_dir, 'eval')\n",
            "\n",
            "  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\agents\\td3\\examples\\v2\\train_eval.py",
        "line_number": 124,
        "API": ".equal(",
        "context": [
            "  ]\n",
            "\n",
            "  global_step = tf.compat.v1.train.get_or_create_global_step()\n",
            "  with tf.compat.v2.summary.record_if(\n",
            "      lambda: tf.math.equal(global_step % summary_interval, 0)):\n",
            "    tf_env = tf_py_environment.TFPyEnvironment(suite_mujoco.load(env_name))\n",
            "    eval_tf_env = tf_py_environment.TFPyEnvironment(suite_mujoco.load(env_name))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\bernoulli_thompson_sampling_agent.py",
        "line_number": 53,
        "API": ".ones(",
        "context": [
            "    \"\"\"\n",
            "    tf.Module.__init__(self, name=name)\n",
            "    # It holds the `alpha` parameter of the beta distribution of each arm.\n",
            "    self.alpha = [\n",
            "        tf.compat.v2.Variable(tf.ones([], dtype=dtype),\n",
            "                              name='alpha_{}'.format(k)) for k in range(\n",
            "                                  num_actions)]\n",
            "    # It holds the `beta` parameter of the beta distribution of each arm.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\bernoulli_thompson_sampling_agent.py",
        "line_number": 58,
        "API": ".ones(",
        "context": [
            "                              name='alpha_{}'.format(k)) for k in range(\n",
            "                                  num_actions)]\n",
            "    # It holds the `beta` parameter of the beta distribution of each arm.\n",
            "    self.beta = [\n",
            "        tf.compat.v2.Variable(tf.ones([], dtype=dtype),\n",
            "                              name='beta_{}'.format(k)) for k in range(\n",
            "                                  num_actions)]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\bernoulli_thompson_sampling_agent.py",
        "line_number": 167,
        "API": ".clip_by_value(",
        "context": [
            "  def _train(self, experience, weights):\n",
            "    experience = self._as_trajectory(experience)\n",
            "    reward, _ = nest_utils.flatten_multi_batched_nested_tensors(\n",
            "        experience.reward, self._time_step_spec.reward)\n",
            "    reward = tf.clip_by_value(reward, clip_value_min=0.0, clip_value_max=1.0)\n",
            "    action, _ = nest_utils.flatten_multi_batched_nested_tensors(\n",
            "        experience.action, self._action_spec)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\bernoulli_thompson_sampling_agent.py",
        "line_number": 175,
        "API": ".cast(",
        "context": [
            "    partitioned_rewards = tf.dynamic_partition(\n",
            "        reward, action, self._num_actions)\n",
            "    for k in range(self._num_actions):\n",
            "      tf.compat.v1.assign_add(\n",
            "          self._alpha[k], tf.cast(\n",
            "              tf.reduce_sum(partitioned_rewards[k]), dtype=self._dtype))\n",
            "      tf.compat.v1.assign_add(\n",
            "          self._beta[k], tf.cast(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\bernoulli_thompson_sampling_agent.py",
        "line_number": 182,
        "API": ".reduce_sum(",
        "context": [
            "          self._beta[k], tf.cast(\n",
            "              tf.reduce_sum(1.0 - partitioned_rewards[k]), dtype=self._dtype))\n",
            "\n",
            "    self.train_step_counter.assign_add(self._batch_size)\n",
            "    loss = -1. * tf.reduce_sum(reward)\n",
            "    return tf_agent.LossInfo(loss=(loss), extra=())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\dropout_thompson_sampling_agent.py",
        "line_number": 182,
        "API": ".scalar(",
        "context": [
            "    if callable(self._dropout_rate):\n",
            "      dropout = self._dropout_rate()\n",
            "    else:\n",
            "      dropout = self._dropout_rate\n",
            "    tf.compat.v2.summary.scalar(\n",
            "        name='dropout', data=dropout, step=self.train_step_counter)\n",
            "    return super(DropoutThompsonSamplingAgent, self)._train(experience, weights)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\exp3_agent.py",
        "line_number": 66,
        "API": ".reduce_sum(",
        "context": [
            "    A vector of size `num_partitions` with the same dtype as `values`. Entry `i`\n",
            "    is the sum of all entries in `values` belonging to partition `i`.\n",
            "  \"\"\"\n",
            "  partitioned_values = tf.dynamic_partition(values, partitions, num_partitions)\n",
            "  return tf.stack([tf.reduce_sum(partition)\n",
            "                   for partition in partitioned_values])\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\exp3_agent.py",
        "line_number": 72,
        "API": ".exp(",
        "context": [
            "\n",
            "\n",
            "def exp3_update_value(reward: types.Float,\n",
            "                      log_prob: types.Float) -> types.Float:\n",
            "  return 1. - (1. - reward) / tf.exp(log_prob)\n",
            "\n",
            "\n",
            "@gin.configurable\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\exp3_agent.py",
        "line_number": 108,
        "API": ".zeros(",
        "context": [
            "    common.tf_agents_gauge.get_cell('TFABandit').set(True)\n",
            "    self._num_actions = policy_utilities.get_num_actions_from_tensor_spec(\n",
            "        action_spec)\n",
            "    self._weights = tf.compat.v2.Variable(\n",
            "        tf.zeros(self._num_actions), name='weights')\n",
            "    self._learning_rate = tf.compat.v2.Variable(\n",
            "        learning_rate, name='learning_rate')\n",
            "    policy = categorical_policy.CategoricalPolicy(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\exp3_agent.py",
        "line_number": 131,
        "API": ".identity(",
        "context": [
            "    return self._num_actions\n",
            "\n",
            "  @property\n",
            "  def weights(self):\n",
            "    return tf.identity(self._weights)\n",
            "\n",
            "  @property\n",
            "  def learning_rate(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\exp3_agent.py",
        "line_number": 139,
        "API": ".assign(",
        "context": [
            "    return tf.identity(self._learning_rate)\n",
            "\n",
            "  @learning_rate.setter\n",
            "  def learning_rate(self, learning_rate):\n",
            "    return tf.compat.v1.assign(self._learning_rate, learning_rate)\n",
            "\n",
            "  def _initialize(self):\n",
            "    tf.compat.v1.variables_initializer(self.variables)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\exp3_agent.py",
        "line_number": 175,
        "API": ".size(",
        "context": [
            "                                  partitions=action,\n",
            "                                  num_partitions=self.num_actions)\n",
            "    tf.compat.v1.assign_add(self._weights, weight_update)\n",
            "\n",
            "    batch_size = tf.cast(tf.size(reward), dtype=tf.int64)\n",
            "    self._train_step_counter.assign_add(batch_size)\n",
            "\n",
            "    return tf_agent.LossInfo(loss=-tf.reduce_sum(experience.reward), extra=())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\exp3_mixture_agent.py",
        "line_number": 153,
        "API": ".zeros_like(",
        "context": [
            "    # agent. The only reason why this value is a tf.Variable is because this way\n",
            "    # the categorical distribution is dynamically parameterized.\n",
            "\n",
            "    self._mixture_weights = tf.Variable(\n",
            "        tf.zeros_like(variable_collection.reward_aggregates))\n",
            "    mixture_distribution = tfd.Categorical(\n",
            "        logits=self._mixture_weights)\n",
            "    super(Exp3MixtureAgent, self).__init__(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\exp3_mixture_agent.py",
        "line_number": 167,
        "API": ".shape(",
        "context": [
            "    policy_choice, _ = nest_utils.flatten_multi_batched_nested_tensors(\n",
            "        experience.policy_info[mixture_policy.MIXTURE_AGENT_ID],\n",
            "        self._time_step_spec.reward)\n",
            "    batch_size = tf.compat.dimension_value(\n",
            "        reward.shape[0]) or tf.shape(reward)[0]\n",
            "    unnormalized_probabilities = tf.exp(self._mixture_weights)\n",
            "    probabilities = unnormalized_probabilities / tf.norm(\n",
            "        unnormalized_probabilities, 1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\exp3_mixture_agent.py",
        "line_number": 172,
        "API": ".reduce_sum(",
        "context": [
            "    unnormalized_probabilities = tf.exp(self._mixture_weights)\n",
            "    probabilities = unnormalized_probabilities / tf.norm(\n",
            "        unnormalized_probabilities, 1)\n",
            "\n",
            "    normalizer = tf.reduce_sum(unnormalized_probabilities)\n",
            "    probabilities = unnormalized_probabilities / normalizer\n",
            "    self._summarize_probabilities(probabilities)\n",
            "    repeated_probs = tf.tile(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\exp3_mixture_agent.py",
        "line_number": 177,
        "API": ".gather(",
        "context": [
            "    probabilities = unnormalized_probabilities / normalizer\n",
            "    self._summarize_probabilities(probabilities)\n",
            "    repeated_probs = tf.tile(\n",
            "        tf.expand_dims(probabilities, axis=0), [batch_size, 1])\n",
            "    probs_per_step = tf.gather(\n",
            "        repeated_probs, policy_choice, batch_dims=1)\n",
            "    per_step_update_term = tf.expand_dims((1 - reward) / probs_per_step, axis=0)\n",
            "    one_hot_policy_choice = tf.one_hot(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\exp3_mixture_agent.py",
        "line_number": 182,
        "API": ".squeeze(",
        "context": [
            "        repeated_probs, policy_choice, batch_dims=1)\n",
            "    per_step_update_term = tf.expand_dims((1 - reward) / probs_per_step, axis=0)\n",
            "    one_hot_policy_choice = tf.one_hot(\n",
            "        policy_choice, depth=self._num_agents)\n",
            "    update_term = 1 - tf.squeeze(\n",
            "        tf.matmul(per_step_update_term, one_hot_policy_choice))\n",
            "    self._update_aggregates(update_term)\n",
            "    self._update_inverse_temperature(batch_size)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\exp3_mixture_agent.py",
        "line_number": 192,
        "API": ".scalar(",
        "context": [
            "        self._variable_collection.inverse_temperature)\n",
            "\n",
            "  def _summarize_probabilities(self, probabilities):\n",
            "    for k in range(self._num_agents):\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='policy_{}_prob'.format(k),\n",
            "          data=probabilities[k],\n",
            "          step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\exp3_mixture_agent.py",
        "line_number": 198,
        "API": ".assign(",
        "context": [
            "          data=probabilities[k],\n",
            "          step=self.train_step_counter)\n",
            "\n",
            "  def _update_aggregates(self, update_term):\n",
            "    self._variable_collection.reward_aggregates.assign(\n",
            "        self._forgetting *\n",
            "        (self._variable_collection.reward_aggregates + update_term))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\exp3_mixture_agent.py",
        "line_number": 203,
        "API": ".assign(",
        "context": [
            "        self._forgetting *\n",
            "        (self._variable_collection.reward_aggregates + update_term))\n",
            "\n",
            "  def _update_inverse_temperature(self, batch_size):\n",
            "    self._variable_collection.inverse_temperature.assign(\n",
            "        tf.maximum(\n",
            "            self._max_inverse_temperature,\n",
            "            tf.sqrt(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\exp3_mixture_agent.py",
        "line_number": 208,
        "API": ".cast(",
        "context": [
            "        tf.maximum(\n",
            "            self._max_inverse_temperature,\n",
            "            tf.sqrt(\n",
            "                tf.square(self._variable_collection.inverse_temperature) +\n",
            "                tf.cast(batch_size, dtype=tf.float32))))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_multi_objective_neural_agent.py",
        "line_number": 147,
        "API": ".format(",
        "context": [
            "    self._num_objectives = len(objective_network_and_loss_fn_sequence)\n",
            "    if self._num_objectives < 2:\n",
            "      raise ValueError(\n",
            "          'Number of objectives should be at least two, but found to be {}'\n",
            "          .format(self._num_objectives))\n",
            "    self._objective_networks, self._error_loss_fns = tuple(\n",
            "        zip(*objective_network_and_loss_fn_sequence))\n",
            "    self._optimizer = optimizer\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_multi_objective_neural_agent.py",
        "line_number": 159,
        "API": ".convert_to_tensor(",
        "context": [
            "    ]\n",
            "    self._laplacian_smoothing_weights = laplacian_smoothing_weights\n",
            "    self._laplacian_matrix = None\n",
            "    if laplacian_matrix is not None:\n",
            "      self._laplacian_matrix = tf.convert_to_tensor(\n",
            "          laplacian_matrix, dtype=tf.float32)\n",
            "      # Check the validity of the laplacian matrix.\n",
            "      tf.debugging.assert_near(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_multi_objective_neural_agent.py",
        "line_number": 165,
        "API": ".reduce_sum(",
        "context": [
            "      # Check the validity of the laplacian matrix.\n",
            "      tf.debugging.assert_near(\n",
            "          0.0, tf.norm(tf.reduce_sum(self._laplacian_matrix, 1)))\n",
            "      tf.debugging.assert_near(\n",
            "          0.0, tf.norm(tf.reduce_sum(self._laplacian_matrix, 0)))\n",
            "      tf.debugging.assert_near(\n",
            "          0.0,\n",
            "          tf.norm(self._laplacian_matrix -\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_multi_objective_neural_agent.py",
        "line_number": 175,
        "API": ".format(",
        "context": [
            "        raise ValueError('laplacian_smoothing_weights cannot be None when the '\n",
            "                         'Laplacian matrix is provided.')\n",
            "      if self._num_objectives != len(self._laplacian_smoothing_weights):\n",
            "        raise ValueError('The length of laplacian smoothing weights: {} '\n",
            "                         'does not equal the number of objectives: {}'.format(\n",
            "                             len(self._laplacian_smoothing_weights),\n",
            "                             self._num_objectives))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_multi_objective_neural_agent.py",
        "line_number": 210,
        "API": ".flatten(",
        "context": [
            "  def _initialize(self):\n",
            "    tf.compat.v1.variables_initializer(self.variables)\n",
            "\n",
            "  def _variables_to_train(self):\n",
            "    variables_to_train = tf.nest.flatten(\n",
            "        [net.trainable_variables for net in self._objective_networks])\n",
            "    return variables_to_train\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_multi_objective_neural_agent.py",
        "line_number": 226,
        "API": ".info(",
        "context": [
            "          training=True)\n",
            "\n",
            "    variables_to_train = self._variables_to_train()\n",
            "    if not variables_to_train:\n",
            "      logging.info('No variable to train in the agent.')\n",
            "      return loss_info\n",
            "\n",
            "    grads = tape.gradient(loss_info.loss, variables_to_train)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_multi_objective_neural_agent.py",
        "line_number": 277,
        "API": ".format(",
        "context": [
            "        if the number of actions is greater than 1.\n",
            "    \"\"\"\n",
            "    if objective_idx >= self._num_objectives or objective_idx < 0:\n",
            "      raise ValueError(\n",
            "          'objective_idx should be between 0 and {}, but is {}'.format(\n",
            "              self._num_objectives, objective_idx))\n",
            "    with tf.name_scope('loss_for_objective_{}'.format(objective_idx)):\n",
            "      objective_network = self._objective_networks[objective_idx]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_multi_objective_neural_agent.py",
        "line_number": 287,
        "API": ".cast(",
        "context": [
            "        predictions, _ = objective_network(observations, training=training)\n",
            "        predicted_values = predictions.q_value_logits\n",
            "        predicted_log_variance = predictions.log_variance\n",
            "        action_predicted_log_variance = common.index_with_actions(\n",
            "            predicted_log_variance, tf.cast(actions, dtype=tf.int32))\n",
            "        sample_weights = sample_weights * 0.5 * tf.exp(\n",
            "            -action_predicted_log_variance)\n",
            "        loss = 0.5 * tf.reduce_mean(action_predicted_log_variance)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_multi_objective_neural_agent.py",
        "line_number": 297,
        "API": ".constant(",
        "context": [
            "        # Bayesian Deep Learning for Computer Vision?.\" Advances in Neural\n",
            "        # Information Processing Systems. 2017. https://arxiv.org/abs/1703.04977\n",
            "      else:\n",
            "        predicted_values, _ = objective_network(observations, training=training)\n",
            "        loss = tf.constant(0.0)\n",
            "\n",
            "      action_predicted_values = common.index_with_actions(\n",
            "          predicted_values,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_multi_objective_neural_agent.py",
        "line_number": 305,
        "API": ".reduce_sum(",
        "context": [
            "          tf.cast(actions, dtype=tf.int32))\n",
            "\n",
            "      # Apply Laplacian smoothing on the estimated rewards, if applicable.\n",
            "      if self._laplacian_matrix is not None:\n",
            "        smoothness_batched = tf.reduce_sum(\n",
            "            predicted_values *\n",
            "            tf.matmul(predicted_values, self._laplacian_matrix),\n",
            "            axis=1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_multi_objective_neural_agent.py",
        "line_number": 311,
        "API": ".reduce_mean(",
        "context": [
            "            tf.matmul(predicted_values, self._laplacian_matrix),\n",
            "            axis=1)\n",
            "        loss += (\n",
            "            self._laplacian_smoothing_weights[objective_idx] *\n",
            "            tf.reduce_mean(smoothness_batched * sample_weights))\n",
            "\n",
            "      # Reduction is done outside of the loss function because non-scalar\n",
            "      # weights with unknown shapes may trigger shape validation that fails\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_multi_objective_neural_agent.py",
        "line_number": 316,
        "API": ".reduce_mean(",
        "context": [
            "\n",
            "      # Reduction is done outside of the loss function because non-scalar\n",
            "      # weights with unknown shapes may trigger shape validation that fails\n",
            "      # XLA compilation.\n",
            "      loss += tf.reduce_mean(\n",
            "          tf.multiply(\n",
            "              self._error_loss_fns[objective_idx](\n",
            "                  single_objective_values,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_multi_objective_neural_agent.py",
        "line_number": 358,
        "API": ".format(",
        "context": [
            "          observations)\n",
            "    if objective_values.shape.rank != 2:\n",
            "      raise ValueError(\n",
            "          'The objectives tensor should be rank-2 [batch_size, num_objectives],'\n",
            "          ' but found to be rank-{}'.format(objective_values.shape.rank))\n",
            "    if objective_values.shape[1] != self._num_objectives:\n",
            "      raise ValueError(\n",
            "          'The number of objectives in the objective_values tensor: {} '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_multi_objective_neural_agent.py",
        "line_number": 374,
        "API": ".reduce_sum(",
        "context": [
            "                                      single_objective_values, weights,\n",
            "                                      training))\n",
            "\n",
            "    self.compute_summaries(objective_losses)\n",
            "    total_loss = tf.reduce_sum(objective_losses)\n",
            "    return tf_agent.LossInfo(total_loss, extra=())\n",
            "\n",
            "  def compute_summaries(self, losses: Sequence[tf.Tensor]):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_multi_objective_neural_agent.py",
        "line_number": 380,
        "API": ".format(",
        "context": [
            "\n",
            "  def compute_summaries(self, losses: Sequence[tf.Tensor]):\n",
            "    if self._num_objectives != len(losses):\n",
            "      raise ValueError('The number of losses: {} does not equal the number '\n",
            "                       'of objectives: {}'.format(\n",
            "                           len(losses), self._num_objectives))\n",
            "    if self.summaries_enabled:\n",
            "      with tf.name_scope('Losses/'):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_multi_objective_neural_agent.py",
        "line_number": 387,
        "API": ".format(",
        "context": [
            "      with tf.name_scope('Losses/'):\n",
            "        for idx in range(self._num_objectives):\n",
            "          name_of_loss = self._objective_networks[idx].name\n",
            "          if not name_of_loss:\n",
            "            name_of_loss = 'loss_{}'.format(idx)\n",
            "          tf.compat.v2.summary.scalar(\n",
            "              name=name_of_loss, data=losses[idx], step=self.train_step_counter)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_multi_objective_neural_agent.py",
        "line_number": 394,
        "API": ".histogram(",
        "context": [
            "\n",
            "      if self._summarize_grads_and_vars:\n",
            "        with tf.name_scope('Variables/'):\n",
            "          for var in self._variables_to_train():\n",
            "            tf.compat.v2.summary.histogram(\n",
            "                name=var.name.replace(':', '_'),\n",
            "                data=var,\n",
            "                step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_reward_prediction_agent.py",
        "line_number": 177,
        "API": ".convert_to_tensor(",
        "context": [
            "    self._heteroscedastic = isinstance(\n",
            "        reward_network, heteroscedastic_q_network.HeteroscedasticQNetwork)\n",
            "    self._laplacian_matrix = None\n",
            "    if laplacian_matrix is not None:\n",
            "      self._laplacian_matrix = tf.convert_to_tensor(\n",
            "          laplacian_matrix, dtype=tf.float32)\n",
            "      # Check the validity of the laplacian matrix.\n",
            "      tf.debugging.assert_near(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_reward_prediction_agent.py",
        "line_number": 183,
        "API": ".reduce_sum(",
        "context": [
            "      # Check the validity of the laplacian matrix.\n",
            "      tf.debugging.assert_near(\n",
            "          0.0, tf.norm(tf.reduce_sum(self._laplacian_matrix, 1)))\n",
            "      tf.debugging.assert_near(\n",
            "          0.0, tf.norm(tf.reduce_sum(self._laplacian_matrix, 0)))\n",
            "    self._laplacian_smoothing_weight = laplacian_smoothing_weight\n",
            "\n",
            "    policy = greedy_reward_policy.GreedyRewardPredictionPolicy(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_reward_prediction_agent.py",
        "line_number": 234,
        "API": ".info(",
        "context": [
            "      loss_info = self._loss(experience, weights=weights, training=True)\n",
            "\n",
            "    variables_to_train = self._variables_to_train()\n",
            "    if not variables_to_train:\n",
            "      logging.info('No variable to train in the agent.')\n",
            "      return loss_info\n",
            "\n",
            "    grads = tape.gradient(loss_info.loss, variables_to_train)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_reward_prediction_agent.py",
        "line_number": 253,
        "API": ".reshape(",
        "context": [
            "\n",
            "    self._optimizer.apply_gradients(grads_and_vars)\n",
            "    self.train_step_counter.assign_add(1)\n",
            "    if self._num_samples_list:\n",
            "      actions_flattened = tf.reshape(experience.action, [-1])\n",
            "      if self._accepts_per_arm_features:\n",
            "        num_samples_per_action_current = [\n",
            "            tf.cast(tf.shape(actions_flattened)[0], dtype=tf.int64)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_reward_prediction_agent.py",
        "line_number": 261,
        "API": ".reduce_sum(",
        "context": [
            "        ]\n",
            "      else:\n",
            "        # Compute the number of samples for each action in the current batch.\n",
            "        num_samples_per_action_current = [\n",
            "            tf.reduce_sum(tf.cast(tf.equal(actions_flattened, k), tf.int64))\n",
            "            for k in range(self._num_actions)\n",
            "        ]\n",
            "      # Update the number of samples for each action.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_reward_prediction_agent.py",
        "line_number": 301,
        "API": ".cast(",
        "context": [
            "                                              training=training)\n",
            "        predicted_values = predictions.q_value_logits\n",
            "        predicted_log_variance = predictions.log_variance\n",
            "        action_predicted_log_variance = common.index_with_actions(\n",
            "            predicted_log_variance, tf.cast(actions, dtype=tf.int32))\n",
            "        sample_weights = sample_weights * 0.5 * tf.exp(\n",
            "            -action_predicted_log_variance)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_reward_prediction_agent.py",
        "line_number": 313,
        "API": ".constant(",
        "context": [
            "        # Information Processing Systems. 2017. https://arxiv.org/abs/1703.04977\n",
            "      else:\n",
            "        predicted_values, _ = self._reward_network(observations,\n",
            "                                                   training=training)\n",
            "        loss = tf.constant(0.0)\n",
            "\n",
            "      action_predicted_values = common.index_with_actions(\n",
            "          predicted_values,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_reward_prediction_agent.py",
        "line_number": 321,
        "API": ".matmul(",
        "context": [
            "          tf.cast(actions, dtype=tf.int32))\n",
            "\n",
            "      # Apply Laplacian smoothing on the estimated rewards, if applicable.\n",
            "      if self._laplacian_matrix is not None:\n",
            "        smoothness_batched = tf.matmul(\n",
            "            predicted_values,\n",
            "            tf.matmul(self._laplacian_matrix, predicted_values,\n",
            "                      transpose_b=True))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_reward_prediction_agent.py",
        "line_number": 331,
        "API": ".reduce_mean(",
        "context": [
            "\n",
            "      # Reduction is done outside of the loss function because non-scalar\n",
            "      # weights with unknown shapes may trigger shape validation that fails\n",
            "      # XLA compilation.\n",
            "      loss += tf.reduce_mean(\n",
            "          tf.multiply(\n",
            "              self._error_loss_fn(\n",
            "                  rewards,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_reward_prediction_agent.py",
        "line_number": 375,
        "API": ".constant(",
        "context": [
            "      rewards_tensor = rewards\n",
            "    reward_loss = self.reward_loss(\n",
            "        observations, actions, rewards_tensor, weights, training)\n",
            "\n",
            "    constraint_loss = tf.constant(0.0)\n",
            "    for i, c in enumerate(self._constraints, 0):\n",
            "      constraint_targets = rewards[\n",
            "          bandit_spec_utils.CONSTRAINTS_SPEC_KEY][:, i]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\greedy_reward_prediction_agent.py",
        "line_number": 396,
        "API": ".scalar(",
        "context": [
            "                        loss: types.Tensor,\n",
            "                        constraint_loss: Optional[types.Tensor] = None):\n",
            "    if self.summaries_enabled:\n",
            "      with tf.name_scope('Losses/'):\n",
            "        tf.compat.v2.summary.scalar(\n",
            "            name='loss', data=loss, step=self.train_step_counter)\n",
            "        if constraint_loss is not None:\n",
            "          tf.compat.v2.summary.scalar(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 74,
        "API": ".zeros(",
        "context": [
            "      name: (string) the name of this instance.\n",
            "    \"\"\"\n",
            "    tf.Module.__init__(self, name=name)\n",
            "    self.theta = tf.compat.v2.Variable(\n",
            "        tf.zeros([num_models, context_dim], dtype=dtype), name='theta')\n",
            "    self.cov_matrix_list = []\n",
            "    self.data_vector_list = []\n",
            "    self.eig_matrix_list = []\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 83,
        "API": ".zeros(",
        "context": [
            "    self.num_samples_list = []\n",
            "    for k in range(num_models):\n",
            "      self.cov_matrix_list.append(\n",
            "          tf.compat.v2.Variable(\n",
            "              tf.zeros([context_dim, context_dim], dtype=dtype),\n",
            "              name='a_' + str(k)))\n",
            "      self.data_vector_list.append(\n",
            "          tf.compat.v2.Variable(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 90,
        "API": ".format(",
        "context": [
            "          tf.compat.v2.Variable(\n",
            "              tf.zeros(context_dim, dtype=dtype), name='b_{}'.format(k)))\n",
            "      self.num_samples_list.append(\n",
            "          tf.compat.v2.Variable(\n",
            "              tf.zeros([], dtype=dtype), name='num_samples_{}'.format(k)))\n",
            "      if use_eigendecomp:\n",
            "        self.eig_matrix_list.append(\n",
            "            tf.compat.v2.Variable(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 95,
        "API": ".format(",
        "context": [
            "      if use_eigendecomp:\n",
            "        self.eig_matrix_list.append(\n",
            "            tf.compat.v2.Variable(\n",
            "                tf.eye(context_dim, dtype=dtype),\n",
            "                name='eig_matrix{}'.format(k)))\n",
            "        self.eig_vals_list.append(\n",
            "            tf.compat.v2.Variable(\n",
            "                tf.zeros([context_dim], dtype=dtype),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 103,
        "API": ".format(",
        "context": [
            "                name='eig_vals{}'.format(k)))\n",
            "      else:\n",
            "        self.eig_matrix_list.append(\n",
            "            tf.compat.v2.Variable(\n",
            "                tf.constant([], dtype=dtype), name='eig_matrix{}'.format(k)))\n",
            "        self.eig_vals_list.append(\n",
            "            tf.compat.v2.Variable(\n",
            "                tf.constant([], dtype=dtype), name='eig_vals{}'.format(k)))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 129,
        "API": ".matmul(",
        "context": [
            "\n",
            "  Returns:\n",
            "    The updated estimates of `a` and `b`.\n",
            "  \"\"\"\n",
            "  a_new = gamma * a_prev + tf.matmul(x, x, transpose_a=True)\n",
            "  b_new = gamma * b_prev + bandit_utils.sum_reward_weighted_observations(r, x)\n",
            "\n",
            "  return a_new, b_new\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 335,
        "API": ".assign(",
        "context": [
            "  def alpha(self):\n",
            "    return self._alpha\n",
            "\n",
            "  def update_alpha(self, alpha):\n",
            "    return tf.compat.v1.assign(self._alpha, alpha)\n",
            "\n",
            "  @property\n",
            "  def theta(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 349,
        "API": ".matmul(",
        "context": [
            "    for k in range(self._num_models):\n",
            "      if self._use_eigendecomp:\n",
            "        model_index = policy_utilities.get_model_index(\n",
            "            k, self._accepts_per_arm_features)\n",
            "        q_t_b = tf.matmul(\n",
            "            self.eig_matrix[model_index],\n",
            "            tf.expand_dims(self.data_vector[model_index], axis=-1),\n",
            "            transpose_a=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 354,
        "API": ".ones_like(",
        "context": [
            "            self.eig_matrix[model_index],\n",
            "            tf.expand_dims(self.data_vector[model_index], axis=-1),\n",
            "            transpose_a=True)\n",
            "        lambda_inv = tf.divide(\n",
            "            tf.ones_like(self.eig_vals[model_index]),\n",
            "            self.eig_vals[model_index] + self._tikhonov_weight)\n",
            "        theta_k = tf.squeeze(\n",
            "            tf.matmul(self.eig_matrix[model_index],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 361,
        "API": ".squeeze(",
        "context": [
            "            tf.matmul(self.eig_matrix[model_index],\n",
            "                      tf.einsum('j,jk->jk', lambda_inv, q_t_b)),\n",
            "            axis=-1)\n",
            "      else:\n",
            "        theta_k = tf.squeeze(\n",
            "            linalg.conjugate_gradient(\n",
            "                self._cov_matrix_list[k] + self._tikhonov_weight *\n",
            "                tf.eye(self._overall_context_dim, dtype=self._dtype),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 369,
        "API": ".stack(",
        "context": [
            "                tf.expand_dims(self._data_vector_list[k], axis=-1)),\n",
            "            axis=-1)\n",
            "      thetas.append(theta_k)\n",
            "\n",
            "    return tf.stack(thetas, axis=0)\n",
            "\n",
            "  def _initialize(self):\n",
            "    tf.compat.v1.variables_initializer(self.variables)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 377,
        "API": ".scalar(",
        "context": [
            "\n",
            "  def compute_summaries(self, loss: types.Tensor):\n",
            "    if self.summaries_enabled:\n",
            "      with tf.name_scope('Losses/'):\n",
            "        tf.compat.v2.summary.scalar(\n",
            "            name='loss', data=loss, step=self.train_step_counter)\n",
            "\n",
            "      if self._summarize_grads_and_vars:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 384,
        "API": ".histogram(",
        "context": [
            "      if self._summarize_grads_and_vars:\n",
            "        with tf.name_scope('Variables/'):\n",
            "          for var in self.policy.variables():\n",
            "            var_name = var.name.replace(':', '_')\n",
            "            tf.compat.v2.summary.histogram(\n",
            "                name=var_name, data=var, step=self.train_step_counter)\n",
            "            tf.compat.v2.summary.scalar(\n",
            "                name=var_name + '_value_norm',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 393,
        "API": ".unstack(",
        "context": [
            "                step=self.train_step_counter)\n",
            "        if self._add_bias:\n",
            "          thetas = self.theta\n",
            "          biases = thetas[:, self._global_context_dim - 1]\n",
            "          bias_list = tf.unstack(biases, axis=0)\n",
            "          for i in range(self._num_actions):\n",
            "            tf.compat.v2.summary.scalar(\n",
            "                name='bias/action_' + str(i),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 440,
        "API": ".cast(",
        "context": [
            "\n",
            "    if self._observation_and_action_constraint_splitter is not None:\n",
            "      observation, _ = self._observation_and_action_constraint_splitter(\n",
            "          observation)\n",
            "    batch_size = tf.cast(\n",
            "        tf.compat.dimension_value(tf.shape(reward)[0]), dtype=tf.int64)\n",
            "    global_observation = observation[bandit_spec_utils.GLOBAL_FEATURE_KEY]\n",
            "    if self._add_bias:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 445,
        "API": ".concat(",
        "context": [
            "        tf.compat.dimension_value(tf.shape(reward)[0]), dtype=tf.int64)\n",
            "    global_observation = observation[bandit_spec_utils.GLOBAL_FEATURE_KEY]\n",
            "    if self._add_bias:\n",
            "      # The bias is added via a constant 1 feature.\n",
            "      global_observation = tf.concat([\n",
            "          global_observation,\n",
            "          tf.ones([batch_size, 1], dtype=global_observation.dtype)\n",
            "      ],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 454,
        "API": ".zeros(",
        "context": [
            "\n",
            "    # The arm observation we train on needs to be copied from the respective\n",
            "    # policy info field to the per arm observation field. Pretending there was\n",
            "    # only one action, we fill the action field with zeros.\n",
            "    action = tf.zeros(shape=[batch_size], dtype=tf.int64)\n",
            "    chosen_action, _ = nest_utils.flatten_multi_batched_nested_tensors(\n",
            "        experience.policy_info.chosen_arm_features,\n",
            "        self.policy.info_spec.chosen_arm_features)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 459,
        "API": ".concat(",
        "context": [
            "    chosen_action, _ = nest_utils.flatten_multi_batched_nested_tensors(\n",
            "        experience.policy_info.chosen_arm_features,\n",
            "        self.policy.info_spec.chosen_arm_features)\n",
            "    arm_observation = chosen_action\n",
            "    overall_observation = tf.concat([global_observation, arm_observation],\n",
            "                                    axis=1)\n",
            "    overall_observation = tf.reshape(\n",
            "        tf.cast(overall_observation, self._dtype), [batch_size, -1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 492,
        "API": ".cast(",
        "context": [
            "    observation, _ = nest_utils.flatten_multi_batched_nested_tensors(\n",
            "        experience.observation, self.training_data_spec.observation)\n",
            "    action, _ = nest_utils.flatten_multi_batched_nested_tensors(\n",
            "        experience.action, self._action_spec)\n",
            "    batch_size = tf.cast(\n",
            "        tf.compat.dimension_value(tf.shape(reward)[0]), dtype=tf.int64)\n",
            "\n",
            "    if self._observation_and_action_constraint_splitter is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 500,
        "API": ".concat(",
        "context": [
            "      observation, _ = self._observation_and_action_constraint_splitter(\n",
            "          observation)\n",
            "    if self._add_bias:\n",
            "      # The bias is added via a constant 1 feature.\n",
            "      observation = tf.concat(\n",
            "          [observation,\n",
            "           tf.ones([batch_size, 1], dtype=observation.dtype)],\n",
            "          axis=1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 505,
        "API": ".reshape(",
        "context": [
            "          [observation,\n",
            "           tf.ones([batch_size, 1], dtype=observation.dtype)],\n",
            "          axis=1)\n",
            "\n",
            "    observation = tf.reshape(\n",
            "        tf.cast(observation, self._dtype), [batch_size, -1])\n",
            "    reward = tf.cast(reward, self._dtype)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 518,
        "API": ".sqrt(",
        "context": [
            "    \"\"\"Optionally applies per-example weight to observation and rewards.\"\"\"\n",
            "    if weights is None:\n",
            "      return (observation, reward)\n",
            "    else:\n",
            "      w_sqrt = tf.sqrt(tf.cast(weights, dtype=self._dtype))\n",
            "      return (tf.reshape(w_sqrt, [-1, 1]) * observation, w_sqrt * reward)\n",
            "\n",
            "  def _loss(self,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 530,
        "API": ".gather(",
        "context": [
            "    experience_reward, action, experience_observation, _ = (\n",
            "        self._process_experience(experience))\n",
            "    observation, reward = self._maybe_apply_per_example_weight(\n",
            "        experience_observation, experience_reward, weights)\n",
            "    theta_for_action = tf.gather(params=self._theta, indices=action)\n",
            "    pred_rewards_for_action = tf.einsum('ij,ij->i', observation,\n",
            "                                        theta_for_action)\n",
            "    loss = tf.losses.mean_squared_error(reward, pred_rewards_for_action)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 548,
        "API": ".cast(",
        "context": [
            "    loss = self._loss(experience, weights, training=True)\n",
            "\n",
            "    for k in range(self._num_models):\n",
            "      diag_mask = tf.linalg.tensor_diag(\n",
            "          tf.cast(tf.equal(action, k), self._dtype))\n",
            "      observations_for_arm = tf.matmul(diag_mask, observation)\n",
            "      rewards_for_arm = tf.matmul(diag_mask, tf.reshape(reward, [-1, 1]))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 553,
        "API": ".matmul(",
        "context": [
            "      observations_for_arm = tf.matmul(diag_mask, observation)\n",
            "      rewards_for_arm = tf.matmul(diag_mask, tf.reshape(reward, [-1, 1]))\n",
            "\n",
            "      # Compute local updates for the matrix A and b of this arm.\n",
            "      cov_matrix_local_update = tf.matmul(\n",
            "          observations_for_arm, observations_for_arm, transpose_a=True)\n",
            "      data_vector_local_update = bandit_utils.sum_reward_weighted_observations(\n",
            "          rewards_for_arm, observations_for_arm)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 578,
        "API": ".assign(",
        "context": [
            "\n",
            "        # Compute the eigendecomposition, if needed.\n",
            "        if self._use_eigendecomp:\n",
            "          eig_vals, eig_matrix = tf.linalg.eigh(self._cov_matrix_list[k])\n",
            "          self._eig_vals_list[k].assign(eig_vals)\n",
            "          self._eig_matrix_list[k].assign(eig_matrix)\n",
            "\n",
            "      # Passes the local_updates to the _merge_fn() above that performs custom\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 589,
        "API": ".control_dependencies(",
        "context": [
            "      replica_context = tf.distribute.get_replica_context()\n",
            "      replica_context.merge_call(\n",
            "          _merge_fn, args=(cov_matrix_local_update, data_vector_local_update))\n",
            "\n",
            "    with tf.control_dependencies([loss.loss]):\n",
            "      self._theta.assign(self.theta)\n",
            "    return loss\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 622,
        "API": ".equal(",
        "context": [
            "        self._process_experience(experience))\n",
            "    observation, reward = self._maybe_apply_per_example_weight(\n",
            "        experience_observation, experience_reward, weights)\n",
            "    for k in range(self._num_models):\n",
            "      action_mask = tf.equal(action, k)\n",
            "      observations_for_arm = tf.boolean_mask(observation, action_mask, axis=0)\n",
            "      rewards_for_arm = tf.boolean_mask(\n",
            "          tf.reshape(reward, [-1, 1]), action_mask, axis=0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 627,
        "API": ".reduce_sum(",
        "context": [
            "      observations_for_arm = tf.boolean_mask(observation, action_mask, axis=0)\n",
            "      rewards_for_arm = tf.boolean_mask(\n",
            "          tf.reshape(reward, [-1, 1]), action_mask, axis=0)\n",
            "\n",
            "      num_samples_for_arm_current = tf.reduce_sum(\n",
            "          tf.cast(action_mask, self._dtype))\n",
            "      tf.compat.v1.assign_add(self._num_samples_list[k],\n",
            "                              num_samples_for_arm_current)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 640,
        "API": ".cond(",
        "context": [
            "        return update_a_and_b_with_forgetting(cov_matrix, data_vector,\n",
            "                                              rewards_for_arm,\n",
            "                                              observations_for_arm, self._gamma)\n",
            "\n",
            "      a_new, b_new = tf.cond(\n",
            "          tf.squeeze(num_samples_for_arm_total) > 0,\n",
            "          lambda: update(self._cov_matrix_list[k], self._data_vector_list[k]),\n",
            "          lambda: (self._cov_matrix_list[k], self._data_vector_list[k]))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 645,
        "API": ".assign(",
        "context": [
            "          tf.squeeze(num_samples_for_arm_total) > 0,\n",
            "          lambda: update(self._cov_matrix_list[k], self._data_vector_list[k]),\n",
            "          lambda: (self._cov_matrix_list[k], self._data_vector_list[k]))\n",
            "      tf.compat.v1.assign(self._cov_matrix_list[k], a_new)\n",
            "      tf.compat.v1.assign(self._data_vector_list[k], b_new)\n",
            "\n",
            "    if self._use_eigendecomp:\n",
            "      eigenvalues, eigenvectors = tf.linalg.eigh(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 650,
        "API": ".unstack(",
        "context": [
            "\n",
            "    if self._use_eigendecomp:\n",
            "      eigenvalues, eigenvectors = tf.linalg.eigh(\n",
            "          tf.stack(self._cov_matrix_list))\n",
            "      eigenvalues_list = tf.unstack(eigenvalues)\n",
            "      eigenvectors_list = tf.unstack(eigenvectors)\n",
            "      for k in range(self._num_models):\n",
            "        tf.compat.v1.assign(self._eig_vals_list[k], eigenvalues_list[k])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 657,
        "API": ".assign(",
        "context": [
            "        tf.compat.v1.assign(self._eig_vals_list[k], eigenvalues_list[k])\n",
            "        tf.compat.v1.assign(self._eig_matrix_list[k], eigenvectors_list[k])\n",
            "    else:\n",
            "      for k in range(self._num_models):\n",
            "        tf.compat.v1.assign(self._eig_vals_list[k],\n",
            "                            tf.constant([], dtype=self._dtype))\n",
            "        tf.compat.v1.assign(self._eig_matrix_list[k],\n",
            "                            tf.constant([], dtype=self._dtype))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\linear_bandit_agent.py",
        "line_number": 662,
        "API": ".control_dependencies(",
        "context": [
            "                            tf.constant([], dtype=self._dtype))\n",
            "        tf.compat.v1.assign(self._eig_matrix_list[k],\n",
            "                            tf.constant([], dtype=self._dtype))\n",
            "\n",
            "    with tf.control_dependencies([loss.loss]):\n",
            "      self._theta.assign(self.theta)\n",
            "    self._train_step_counter.assign_add(batch_size)\n",
            "    return loss\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\mixture_agent.py",
        "line_number": 54,
        "API": ".flatten(",
        "context": [
            "\n",
            "  Returns:\n",
            "    A list of nested tensors with the same structure as `nested_tensor`.\n",
            "  \"\"\"\n",
            "  flattened_tensors = tf.nest.flatten(nested_tensor)\n",
            "  if not flattened_tensors:\n",
            "    return [nested_tensor] * num_partitions\n",
            "  partitioned_flat_tensors = [\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\mixture_agent.py",
        "line_number": 175,
        "API": ".zeros_like(",
        "context": [
            "          observation=partitioned_nested_observations[k],\n",
            "          action=partitioned_nested_actions[k],\n",
            "          policy_info=partitioned_nested_infos[k],\n",
            "          reward=partitioned_nested_rewards[k],\n",
            "          discount=tf.zeros_like(partitioned_nested_rewards[k]))\n",
            "      loss_info = self._agents[k].train(per_policy_experience)\n",
            "      loss += loss_info.loss\n",
            "    common.function_in_tf1()(self._update_mixture_distribution)(experience)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 80,
        "API": ".zeros(",
        "context": [
            "\n",
            "    for k in range(num_actions):\n",
            "      self.cov_matrix_list.append(\n",
            "          tf.compat.v2.Variable(\n",
            "              tf.zeros([encoding_dim, encoding_dim], dtype=dtype),\n",
            "              name='a_{}'.format(k)))\n",
            "      self.data_vector_list.append(\n",
            "          tf.compat.v2.Variable(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 87,
        "API": ".format(",
        "context": [
            "          tf.compat.v2.Variable(\n",
            "              tf.zeros(encoding_dim, dtype=dtype), name='b_{}'.format(k)))\n",
            "      self.num_samples_list.append(\n",
            "          tf.compat.v2.Variable(\n",
            "              tf.zeros([], dtype=dtype), name='num_samples_{}'.format(k)))\n",
            "\n",
            "\n",
            "@gin.configurable\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 298,
        "API": ".assign(",
        "context": [
            "            self._reward_layer.trainable_weights + [self.train_step_counter])\n",
            "\n",
            "  @alpha.setter\n",
            "  def update_alpha(self, alpha):\n",
            "    return tf.compat.v1.assign(self._alpha, alpha)\n",
            "\n",
            "  def _initialize(self):\n",
            "    tf.compat.v1.variables_initializer(self.variables)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 305,
        "API": ".scalar(",
        "context": [
            "    tf.compat.v1.variables_initializer(self.variables)\n",
            "\n",
            "  def compute_summaries(self, loss):\n",
            "    with tf.name_scope('Losses/'):\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name='total_loss', data=loss, step=self.train_step_counter)\n",
            "\n",
            "    if self._summarize_grads_and_vars:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 314,
        "API": ".histogram(",
        "context": [
            "        trainable_variables = (\n",
            "            self._encoding_network.trainable_weights +\n",
            "            self._reward_layer.trainable_weights)\n",
            "        for var in trainable_variables:\n",
            "          tf.compat.v2.summary.histogram(\n",
            "              name=var.name.replace(':', '_'),\n",
            "              data=var,\n",
            "              step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 342,
        "API": ".reshape(",
        "context": [
            "    \"\"\"\n",
            "    with tf.name_scope('loss'):\n",
            "      encoded_observation, _ = self._encoding_network(\n",
            "          observations, training=training)\n",
            "      encoded_observation = tf.reshape(\n",
            "          encoded_observation,\n",
            "          shape=[-1, self._encoding_dim])\n",
            "      predicted_rewards = self._reward_layer(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 349,
        "API": ".cast(",
        "context": [
            "      predicted_rewards = self._reward_layer(\n",
            "          encoded_observation, training=training)\n",
            "      chosen_actions_predicted_rewards = common.index_with_actions(\n",
            "          predicted_rewards,\n",
            "          tf.cast(actions, dtype=tf.int32))\n",
            "\n",
            "      loss = self._error_loss_fn(\n",
            "          rewards, chosen_actions_predicted_rewards,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 357,
        "API": ".cast(",
        "context": [
            "          1 if weights is None else weights)\n",
            "      if self._summarize_grads_and_vars:\n",
            "        with tf.name_scope('Per_arm_loss/'):\n",
            "          for k in range(self._num_models):\n",
            "            loss_mask_for_arm = tf.cast(tf.equal(actions, k), tf.float32)\n",
            "            loss_for_arm = self._error_loss_fn(\n",
            "                rewards,\n",
            "                chosen_actions_predicted_rewards,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 362,
        "API": ".scalar(",
        "context": [
            "            loss_for_arm = self._error_loss_fn(\n",
            "                rewards,\n",
            "                chosen_actions_predicted_rewards,\n",
            "                weights=loss_mask_for_arm)\n",
            "            tf.compat.v2.summary.scalar(\n",
            "                name='loss_arm_' + str(k),\n",
            "                data=loss_for_arm,\n",
            "                step=self.train_step_counter)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 395,
        "API": ".scalar(",
        "context": [
            "    with tf.GradientTape() as tape:\n",
            "      loss_info = self._loss_using_reward_layer(\n",
            "          observation, action, reward, weights, training=training)\n",
            "    tf.debugging.check_numerics(loss_info[0], 'Loss is inf or nan')\n",
            "    tf.compat.v2.summary.scalar(\n",
            "        name='using_reward_layer', data=1, step=self.train_step_counter)\n",
            "    if self._summarize_grads_and_vars:\n",
            "      self.compute_summaries(loss_info.loss)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 445,
        "API": ".cast(",
        "context": [
            "\n",
            "    # The network is trained now. Update the covariance matrix.\n",
            "    encoded_observation, _ = self._encoding_network(\n",
            "        observation, training=training)\n",
            "    encoded_observation = tf.cast(encoded_observation, dtype=self._dtype)\n",
            "    encoded_observation = tf.reshape(\n",
            "        encoded_observation, shape=[-1, self._encoding_dim])\n",
            "    for k in range(self._num_models):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 450,
        "API": ".cast(",
        "context": [
            "    encoded_observation = tf.reshape(\n",
            "        encoded_observation, shape=[-1, self._encoding_dim])\n",
            "    for k in range(self._num_models):\n",
            "      diag_mask = tf.linalg.tensor_diag(\n",
            "          tf.cast(tf.equal(action, k), self._dtype))\n",
            "      observations_for_arm = tf.matmul(diag_mask, encoded_observation)\n",
            "      rewards_for_arm = tf.matmul(diag_mask, tf.reshape(reward, [-1, 1]))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 465,
        "API": ".cond(",
        "context": [
            "        a_new, b_new = linear_agent.update_a_and_b_with_forgetting(\n",
            "            cov_matrix, data_vector, rewards_for_arm, observations_for_arm,\n",
            "            self._gamma)\n",
            "        return a_new, b_new\n",
            "      a_new, b_new = tf.cond(\n",
            "          tf.squeeze(num_samples_for_arm_total) > 0,\n",
            "          lambda: update(self.cov_matrix[k], self.data_vector[k]),\n",
            "          lambda: (self.cov_matrix[k], self.data_vector[k]))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 470,
        "API": ".assign(",
        "context": [
            "          tf.squeeze(num_samples_for_arm_total) > 0,\n",
            "          lambda: update(self.cov_matrix[k], self.data_vector[k]),\n",
            "          lambda: (self.cov_matrix[k], self.data_vector[k]))\n",
            "      tf.compat.v1.assign(self.cov_matrix[k], a_new)\n",
            "      tf.compat.v1.assign(self.data_vector[k], b_new)\n",
            "\n",
            "    loss_tensor = tf.cast(-1. * tf.reduce_sum(reward), dtype=tf.float32)\n",
            "    loss_info = tf_agent.LossInfo(loss=loss_tensor, extra=())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 503,
        "API": ".cast(",
        "context": [
            "\n",
            "    # The network is trained now. Update the covariance matrix.\n",
            "    encoded_observation, _ = self._encoding_network(\n",
            "        observation, training=training)\n",
            "    encoded_observation = tf.cast(encoded_observation, dtype=self._dtype)\n",
            "    encoded_observation = tf.reshape(\n",
            "        encoded_observation, shape=[-1, self._encoding_dim])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 511,
        "API": ".cast(",
        "context": [
            "    self._train_step_counter.assign_add(1)\n",
            "\n",
            "    for k in range(self._num_models):\n",
            "      diag_mask = tf.linalg.tensor_diag(\n",
            "          tf.cast(tf.equal(action, k), self._dtype))\n",
            "      observations_for_arm = tf.matmul(diag_mask, encoded_observation)\n",
            "      rewards_for_arm = tf.matmul(diag_mask, tf.reshape(reward, [-1, 1]))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 516,
        "API": ".matmul(",
        "context": [
            "      observations_for_arm = tf.matmul(diag_mask, encoded_observation)\n",
            "      rewards_for_arm = tf.matmul(diag_mask, tf.reshape(reward, [-1, 1]))\n",
            "\n",
            "      # Compute local updates for the matrix A and b of this arm.\n",
            "      cov_matrix_local_udpate = tf.matmul(\n",
            "          observations_for_arm, observations_for_arm, transpose_a=True)\n",
            "      data_vector_local_update = bandit_utils.sum_reward_weighted_observations(\n",
            "          rewards_for_arm, observations_for_arm)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 535,
        "API": ".assign(",
        "context": [
            "        reduced_updates = strategy.extended.batch_reduce_to(\n",
            "            tf.distribute.ReduceOp.SUM, updates_and_vars)\n",
            "\n",
            "        # Update the model variables.\n",
            "        self.cov_matrix[k].assign(\n",
            "            self._gamma * self.cov_matrix[k] + reduced_updates[0])\n",
            "        self.data_vector[k].assign(\n",
            "            self._gamma * self.data_vector[k] + reduced_updates[1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 549,
        "API": ".reduce_sum(",
        "context": [
            "      replica_context.merge_call(\n",
            "          _merge_fn,\n",
            "          args=(cov_matrix_local_udpate, data_vector_local_update))\n",
            "\n",
            "    loss = -1. * tf.reduce_sum(reward)\n",
            "    return tf_agent.LossInfo(loss=(loss), extra=())\n",
            "\n",
            "  def _train(self, experience, weights=None):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 577,
        "API": ".cast(",
        "context": [
            "         experience, self._accepts_per_arm_features, self.training_data_spec)\n",
            "    if self._observation_and_action_constraint_splitter is not None:\n",
            "      observation, _ = self._observation_and_action_constraint_splitter(\n",
            "          observation)\n",
            "    reward = tf.cast(reward, self._dtype)\n",
            "\n",
            "    if tf.distribute.has_strategy():\n",
            "      if self._distributed_train_encoding_network:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 588,
        "API": ".assign(",
        "context": [
            "        loss_info = self.compute_loss_using_linucb_distributed(\n",
            "            observation, action, reward, weights, training=True)\n",
            "      return loss_info\n",
            "\n",
            "    tf.compat.v1.assign(\n",
            "        self.actions_from_reward_layer,\n",
            "        tf.less(self._train_step_counter,\n",
            "                self._encoding_network_num_train_steps))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\neural_linucb_agent.py",
        "line_number": 601,
        "API": ".cond(",
        "context": [
            "    def no_actions_from_reward_layer():\n",
            "      return self.compute_loss_using_linucb(\n",
            "          observation, action, reward, weights, training=True)\n",
            "\n",
            "    loss_info = tf.cond(\n",
            "        self.actions_from_reward_layer,\n",
            "        use_actions_from_reward_layer,\n",
            "        no_actions_from_reward_layer)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\ranking_agent.py",
        "line_number": 86,
        "API": ".sequence_mask(",
        "context": [
            "  Returns:\n",
            "    A tensor of shape `[batch_size, num_slots]`, with scores for every item in\n",
            "    the recommendation.\n",
            "  \"\"\"\n",
            "  negatives = tf.sequence_mask(\n",
            "      chosen_index, maxlen=num_slots, dtype=tf.float32)\n",
            "\n",
            "  chosen_onehot = tf.one_hot(chosen_index, num_slots)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\ranking_agent.py",
        "line_number": 91,
        "API": ".matmul(",
        "context": [
            "      chosen_index, maxlen=num_slots, dtype=tf.float32)\n",
            "\n",
            "  chosen_onehot = tf.one_hot(chosen_index, num_slots)\n",
            "  diag_value = tf.linalg.diag(chosen_value)\n",
            "  values = tf.linalg.matmul(diag_value, chosen_onehot)\n",
            "  return values + non_click_score * negatives\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\ranking_agent.py",
        "line_number": 243,
        "API": ".format(",
        "context": [
            "      policy = ranking_policy.DescendingScoreRankingPolicy(\n",
            "          self._num_items, self._num_slots, time_step_spec, scoring_network)\n",
            "    else:\n",
            "      raise NotImplementedError(\n",
            "          'Policy type {} is not implemented'.format(policy_type))\n",
            "    use_num_actions = isinstance(\n",
            "        time_step_spec.observation,\n",
            "        dict) and 'num_actions' in time_step_spec.observation\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\ranking_agent.py",
        "line_number": 307,
        "API": ".where(",
        "context": [
            "    elif self._feedback_model == FeedbackModel.SCORE_VECTOR:\n",
            "      score = flat_reward\n",
            "    weights = self._construct_sample_weights(flat_reward, flat_obs, weights)\n",
            "    if self._positional_bias_positive_only:\n",
            "      weights = tf.where(score > 0, weights, 1.0)\n",
            "\n",
            "    est_reward = self._scoring_network(flat_obs, training)[0]\n",
            "    loss_output = self._error_loss_fn(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\ranking_agent.py",
        "line_number": 317,
        "API": ".reduce_sum(",
        "context": [
            "      # In case the loss is an aggregate over all slots, we only use one weight\n",
            "      # per iteration.\n",
            "      weights = weights[:, 0]\n",
            "    return tf_agent.LossInfo(\n",
            "        tf.reduce_sum(\n",
            "            tf.multiply(loss_output, weights)) /\n",
            "        tf.reduce_sum(weights),\n",
            "        extra=())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\ranking_agent.py",
        "line_number": 327,
        "API": ".scalar(",
        "context": [
            "    with tf.GradientTape() as tape:\n",
            "      loss = self._loss(experience, weights, training=True).loss\n",
            "    if self.summaries_enabled:\n",
            "      with tf.name_scope('Losses/'):\n",
            "        tf.compat.v2.summary.scalar(\n",
            "            name='loss', data=loss, step=self.train_step_counter)\n",
            "    gradients = tape.gradient(loss, self._variables_to_train())\n",
            "    grads_and_vars = tuple(zip(gradients, self._variables_to_train()))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\ranking_agent.py",
        "line_number": 357,
        "API": ".shape(",
        "context": [
            "      A tensor of shape `[batch_size, num_slots]` containing the constructed\n",
            "        output weights, depending on the feedback model and the positional bias\n",
            "        model.\n",
            "    \"\"\"\n",
            "    batch_size = tf.shape(tf.nest.flatten(reward)[0])[0]\n",
            "    if weights is None:\n",
            "      weights = tf.ones([batch_size, self._num_slots])\n",
            "    elif not list(weights.shape):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\ranking_agent.py",
        "line_number": 363,
        "API": ".shape(",
        "context": [
            "      weights = tf.ones([batch_size, self._num_slots])\n",
            "    elif not list(weights.shape):\n",
            "      weights = weights * tf.ones([batch_size, self._num_slots])\n",
            "    else:\n",
            "      tf.debugging.assert_equal(tf.shape(weights), [batch_size])\n",
            "      weights = tf.reshape(weights, shape=[-1, 1])\n",
            "      weights = tf.tile(weights, multiples=[1, self._num_slots])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\ranking_agent.py",
        "line_number": 369,
        "API": ".sequence_mask(",
        "context": [
            "      weights = tf.tile(weights, multiples=[1, self._num_slots])\n",
            "\n",
            "    if self._use_num_actions:\n",
            "      num_slotted_items = observation[bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY]\n",
            "      weights = tf.sequence_mask(\n",
            "          num_slotted_items, self._num_slots, dtype=tf.float32) * weights\n",
            "    if self._feedback_model == FeedbackModel.CASCADING:\n",
            "      chosen_index = tf.reshape(reward[CHOSEN_INDEX], shape=[-1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\ranking_agent.py",
        "line_number": 377,
        "API": ".broadcast_to(",
        "context": [
            "      multiplier = tf.sequence_mask(\n",
            "          chosen_index + 1, self._num_slots, dtype=tf.float32)\n",
            "      weights = multiplier * weights\n",
            "    if self._positional_bias_type is not None:\n",
            "      batched_range = tf.broadcast_to(\n",
            "          tf.range(self._num_slots, dtype=tf.float32),\n",
            "          tf.shape(weights))\n",
            "      if self._positional_bias_type == 'base':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\ranking_agent.py",
        "line_number": 385,
        "API": ".pow(",
        "context": [
            "        position_bias_multipliers = tf.pow(\n",
            "            batched_range + 1, self._positional_bias_severity\n",
            "        )\n",
            "      elif self._positional_bias_type == 'exponent':\n",
            "        position_bias_multipliers = tf.pow(\n",
            "            self._positional_bias_severity, batched_range\n",
            "        )\n",
            "      else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\utils.py",
        "line_number": 51,
        "API": ".shape(",
        "context": [
            "  Returns:\n",
            "    The update that needs to be added to `b`. Has the same shape as `b`.\n",
            "    If the observation matrix `x` is empty, a zero vector is returned.\n",
            "  \"\"\"\n",
            "  batch_size = tf.shape(x)[0]\n",
            "\n",
            "  return tf.reduce_sum(tf.reshape(r, [batch_size, 1]) * x, axis=0)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\utils.py",
        "line_number": 80,
        "API": ".zeros(",
        "context": [
            "    ValueError: if `action_spec` is not a bounded scalar int32 or int64 spec\n",
            "      with minimum 0.\n",
            "  \"\"\"\n",
            "  num_actions = policy_utilities.get_num_actions_from_tensor_spec(action_spec)\n",
            "  adjacency_matrix = np.zeros([num_actions, num_actions])\n",
            "  for i in range(num_actions - 1):\n",
            "    adjacency_matrix[i, i + 1] = 1.0\n",
            "    adjacency_matrix[i + 1, i] = 1.0\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\utils.py",
        "line_number": 103,
        "API": ".reduce_sum(",
        "context": [
            "    The (squared) pairwise distances matrix. A dense float `Tensor` of shape\n",
            "    [`num_vectors`, `num_vectors`], where `num_vectors` is the number of input\n",
            "    embedding vectors.\n",
            "  \"\"\"\n",
            "  r = tf.reduce_sum(input_vecs * input_vecs, axis=1, keepdims=True)\n",
            "  pdistance_matrix = (\n",
            "      r - 2 * tf.matmul(input_vecs, input_vecs, transpose_b=True)\n",
            "      + tf.transpose(r))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\utils.py",
        "line_number": 128,
        "API": ".shape(",
        "context": [
            "    The graph Laplacian matrix. A dense float `Tensor` of shape\n",
            "    `[num_vectors, num_vectors]`, where `num_vectors` is the number of input\n",
            "    embedding vectors (`Tensor`).\n",
            "  \"\"\"\n",
            "  num_actions = tf.shape(input_vecs)[0]\n",
            "  pdistance_matrix = compute_pairwise_distances(input_vecs)\n",
            "  sorted_indices = tf.argsort(values=pdistance_matrix)\n",
            "  selected_indices = tf.reshape(sorted_indices[:, 1 : k + 1], [-1, 1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\utils.py",
        "line_number": 133,
        "API": ".expand_dims(",
        "context": [
            "  pdistance_matrix = compute_pairwise_distances(input_vecs)\n",
            "  sorted_indices = tf.argsort(values=pdistance_matrix)\n",
            "  selected_indices = tf.reshape(sorted_indices[:, 1 : k + 1], [-1, 1])\n",
            "  rng = tf.tile(\n",
            "      tf.expand_dims(tf.range(num_actions), axis=-1), [1, k])\n",
            "  rng = tf.reshape(rng, [-1, 1])\n",
            "  full_indices = tf.concat([rng, selected_indices], axis=1)\n",
            "  adjacency_matrix = tf.zeros([num_actions, num_actions], dtype=tf.float32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\utils.py",
        "line_number": 140,
        "API": ".ones(",
        "context": [
            "  adjacency_matrix = tf.zeros([num_actions, num_actions], dtype=tf.float32)\n",
            "  adjacency_matrix = tf.tensor_scatter_nd_update(\n",
            "      tensor=adjacency_matrix,\n",
            "      indices=full_indices,\n",
            "      updates=tf.ones([k * num_actions], dtype=tf.float32))\n",
            "  # Symmetrize it.\n",
            "  adjacency_matrix = adjacency_matrix + tf.transpose(adjacency_matrix)\n",
            "  adjacency_matrix = tf.minimum(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\utils.py",
        "line_number": 145,
        "API": ".reduce_sum(",
        "context": [
            "  # Symmetrize it.\n",
            "  adjacency_matrix = adjacency_matrix + tf.transpose(adjacency_matrix)\n",
            "  adjacency_matrix = tf.minimum(\n",
            "      adjacency_matrix, tf.ones_like(adjacency_matrix))\n",
            "  degree_matrix = tf.linalg.tensor_diag(tf.reduce_sum(adjacency_matrix, axis=1))\n",
            "  laplacian_matrix = degree_matrix - adjacency_matrix\n",
            "  return laplacian_matrix\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\utils.py",
        "line_number": 183,
        "API": ".map_structure(",
        "context": [
            "  # The arm observation we train on needs to be copied from the respective\n",
            "  # policy info field to the per arm observation field. Pretending there was\n",
            "  # only one action, we fill the action field with zeros.\n",
            "  chosen_arm_features = flattened_experience.policy_info.chosen_arm_features\n",
            "  observation[bandit_spec_utils.PER_ARM_FEATURE_KEY] = tf.nest.map_structure(\n",
            "      lambda t: tf.expand_dims(t, axis=1), chosen_arm_features)\n",
            "  action = tf.zeros_like(action)\n",
            "  if bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY in observation:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\utils.py",
        "line_number": 191,
        "API": ".ones_like(",
        "context": [
            "    # This change is not crucial but since in training there will be only one\n",
            "    # action per sample, it's good to follow the convention that the feature\n",
            "    # value for `num_actions` be less than or equal to the maximum available\n",
            "    # number of actions.\n",
            "    observation[bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY] = tf.ones_like(\n",
            "        observation[bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY])\n",
            "\n",
            "  return observation, action, reward\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\trainer.py",
        "line_number": 58,
        "API": ".map_structure(",
        "context": [
            "          'input_tensor is expected to be of rank-2, but found otherwise: '\n",
            "          f'input_tensor={input_tensor}, tensor_shape={tensor_shape}')\n",
            "    tensor_shape[1] = steps\n",
            "    input_tensor.set_shape(tensor_shape)\n",
            "  tf.nest.map_structure(lambda t: set_time_dim(t, num_steps), experience)\n",
            "\n",
            "\n",
            "def _get_training_loop(driver, replay_buffer, agent, steps,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\trainer.py",
        "line_number": 120,
        "API": ".info(",
        "context": [
            "                                                  directory=root_dir,\n",
            "                                                  max_to_keep=5)\n",
            "  latest = checkpoint_manager.latest_checkpoint\n",
            "  if latest is not None:\n",
            "    logging.info('Restoring checkpoint from %s.', latest)\n",
            "    checkpoint.restore(latest)\n",
            "    logging.info('Successfully restored to step %s.', step_metric.result())\n",
            "  else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\trainer.py",
        "line_number": 268,
        "API": ".save(",
        "context": [
            "    starting_loop = 0\n",
            "\n",
            "  for i in range(starting_loop, training_loops):\n",
            "    training_loop(train_step=i, metrics=metrics)\n",
            "    checkpoint_manager.save()\n",
            "    if save_policy & (i % 100 == 0):\n",
            "      saver.save(os.path.join(root_dir, 'policy_%d' % step_metric.result()))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_bernoulli.py",
        "line_number": 30,
        "API": ".getenv(",
        "context": [
            "from tf_agents.bandits.environments import bernoulli_py_environment as bern_env\n",
            "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
            "from tf_agents.environments import tf_py_environment\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_enum('agent', 'BernTS', ['BernTS'], 'Which agent to use.')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_bernoulli.py",
        "line_number": 51,
        "API": ".max(",
        "context": [
            "      batch_size=BATCH_SIZE)\n",
            "  environment = tf_py_environment.TFPyEnvironment(env)\n",
            "\n",
            "  def optimal_reward_fn(unused_observation):\n",
            "    return np.max(means)\n",
            "\n",
            "  def optimal_action_fn(unused_observation):\n",
            "    return np.int32(np.argmax(means))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_covertype.py",
        "line_number": 47,
        "API": ".getenv(",
        "context": [
            "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
            "from tf_agents.networks import q_network\n",
            "\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_enum(\n",
            "    'agent', 'epsGreedy', ['LinUCB', 'LinTS', 'epsGreedy'],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_covertype.py",
        "line_number": 73,
        "API": ".device(",
        "context": [
            "\n",
            "def main(unused_argv):\n",
            "  tf.compat.v1.enable_v2_behavior()  # The trainer only runs with V2 enabled.\n",
            "\n",
            "  with tf.device('/CPU:0'):  # due to b/128333994\n",
            "\n",
            "    covertype_dataset = dataset_utilities.convert_covertype_dataset(\n",
            "        FLAGS.covertype_csv)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_covertype.py",
        "line_number": 78,
        "API": ".eye(",
        "context": [
            "\n",
            "    covertype_dataset = dataset_utilities.convert_covertype_dataset(\n",
            "        FLAGS.covertype_csv)\n",
            "    covertype_reward_distribution = tfd.Independent(\n",
            "        tfd.Deterministic(tf.eye(7)), reinterpreted_batch_ndims=2)\n",
            "    environment = ce.ClassificationBanditEnvironment(\n",
            "        covertype_dataset, covertype_reward_distribution, BATCH_SIZE)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_dqn.py",
        "line_number": 41,
        "API": ".getenv(",
        "context": [
            "from tf_agents.networks import q_network\n",
            "from tf_agents.utils import common\n",
            "\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "\n",
            "FLAGS = flags.FLAGS\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_dqn.py",
        "line_number": 57,
        "API": ".device(",
        "context": [
            "\n",
            "def main(unused_argv):\n",
            "  tf.compat.v1.enable_v2_behavior()  # The trainer only runs with V2 enabled.\n",
            "\n",
            "  with tf.device('/CPU:0'):  # due to b/128333994\n",
            "    action_reward_fns = (\n",
            "        environment_utilities.sliding_linear_reward_fn_generator(\n",
            "            CONTEXT_DIM, NUM_ACTIONS, REWARD_NOISE_VARIANCE))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_drifting_linear.py",
        "line_number": 36,
        "API": ".getenv(",
        "context": [
            "from tf_agents.bandits.environments import non_stationary_stochastic_environment as nse\n",
            "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
            "\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_enum(\n",
            "    'agent', 'LinUCB', ['LinUCB', 'LinTS'],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_drifting_linear.py",
        "line_number": 60,
        "API": ".device(",
        "context": [
            "\n",
            "def main(unused_argv):\n",
            "  tf.compat.v1.enable_v2_behavior()  # The trainer only runs with V2 enabled.\n",
            "\n",
            "  with tf.device('/CPU:0'):  # due to b/128333994\n",
            "    observation_shape = [CONTEXT_DIM]\n",
            "    overall_shape = [BATCH_SIZE] + observation_shape\n",
            "    observation_distribution = tfd.Normal(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_drifting_linear.py",
        "line_number": 68,
        "API": ".zeros(",
        "context": [
            "        loc=tf.zeros(overall_shape), scale=tf.ones(overall_shape))\n",
            "    action_shape = [NUM_ACTIONS]\n",
            "    observation_to_reward_shape = observation_shape + action_shape\n",
            "    observation_to_reward_distribution = tfd.Normal(\n",
            "        loc=tf.zeros(observation_to_reward_shape),\n",
            "        scale=tf.ones(observation_to_reward_shape))\n",
            "    drift_distribution = tfd.Normal(loc=DRIFT_MEAN, scale=DRIFT_VARIANCE)\n",
            "    additive_reward_distribution = tfd.Normal(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_drifting_linear.py",
        "line_number": 73,
        "API": ".ones(",
        "context": [
            "        scale=tf.ones(observation_to_reward_shape))\n",
            "    drift_distribution = tfd.Normal(loc=DRIFT_MEAN, scale=DRIFT_VARIANCE)\n",
            "    additive_reward_distribution = tfd.Normal(\n",
            "        loc=tf.zeros(action_shape),\n",
            "        scale=(REWARD_NOISE_VARIANCE * tf.ones(action_shape)))\n",
            "    environment_dynamics = dle.DriftingLinearDynamics(\n",
            "        observation_distribution,\n",
            "        observation_to_reward_distribution,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_movielens.py",
        "line_number": 40,
        "API": ".getenv(",
        "context": [
            "from tf_agents.bandits.networks import global_and_arm_feature_network\n",
            "from tf_agents.environments import tf_py_environment\n",
            "from tf_agents.networks import q_network\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_string(\n",
            "    'data_path', '',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_movielens.py",
        "line_number": 145,
        "API": ".maximum(",
        "context": [
            "  elif FLAGS.agent == 'DropoutTS':\n",
            "    train_step_counter = tf.compat.v1.train.get_or_create_global_step()\n",
            "\n",
            "    def dropout_fn():\n",
            "      return tf.math.maximum(\n",
            "          tf.math.reciprocal_no_nan(1.01 +\n",
            "                                    tf.cast(train_step_counter, tf.float32)),\n",
            "          0.0003)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_mushroom.py",
        "line_number": 38,
        "API": ".getenv(",
        "context": [
            "from tf_agents.bandits.environments import environment_utilities as env_util\n",
            "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
            "\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_enum(\n",
            "    'agent', 'LinUCB', ['LinUCB', 'LinTS'],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_mushroom.py",
        "line_number": 60,
        "API": ".device(",
        "context": [
            "\n",
            "def main(unused_argv):\n",
            "  tf.compat.v1.enable_v2_behavior()  # The trainer only runs with V2 enabled.\n",
            "\n",
            "  with tf.device('/CPU:0'):  # due to b/128333994\n",
            "\n",
            "    mushroom_reward_distribution = (\n",
            "        dataset_utilities.mushroom_reward_distribution(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_per_arm_stationary_linear.py",
        "line_number": 40,
        "API": ".getenv(",
        "context": [
            "from tf_agents.bandits.specs import utils as bandit_spec_utils\n",
            "from tf_agents.environments import tf_py_environment\n",
            "from tf_agents.policies import utils as policy_utilities\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_enum(\n",
            "    'agent', 'LinUCB', ['LinUCB', 'LinTS', 'epsGreedy', 'NeuralLinUCB'],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_per_arm_stationary_linear.py",
        "line_number": 98,
        "API": ".normal(",
        "context": [
            "      self.theta = theta\n",
            "\n",
            "    def __call__(self, x):\n",
            "      mu = np.dot(x, self.theta)\n",
            "      return np.random.normal(mu, 1)\n",
            "\n",
            "  def _global_context_sampling_fn():\n",
            "    return np.random.randint(-10, 10, [4]).astype(np.float32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_per_arm_stationary_linear.py",
        "line_number": 182,
        "API": ".cast(",
        "context": [
            "        emit_policy_info=policy_utilities.InfoFields.PREDICTED_REWARDS_MEAN)\n",
            "\n",
            "  def _all_rewards(observation, hidden_param):\n",
            "    \"\"\"Outputs rewards for all actions, given an observation.\"\"\"\n",
            "    hidden_param = tf.cast(hidden_param, dtype=tf.float32)\n",
            "    global_obs = observation[bandit_spec_utils.GLOBAL_FEATURE_KEY]\n",
            "    per_arm_obs = observation[bandit_spec_utils.PER_ARM_FEATURE_KEY]\n",
            "    num_actions = tf.shape(per_arm_obs)[1]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_per_arm_stationary_linear.py",
        "line_number": 187,
        "API": ".expand_dims(",
        "context": [
            "    global_obs = observation[bandit_spec_utils.GLOBAL_FEATURE_KEY]\n",
            "    per_arm_obs = observation[bandit_spec_utils.PER_ARM_FEATURE_KEY]\n",
            "    num_actions = tf.shape(per_arm_obs)[1]\n",
            "    tiled_global = tf.tile(\n",
            "        tf.expand_dims(global_obs, axis=1), [1, num_actions, 1])\n",
            "    concatenated = tf.concat([tiled_global, per_arm_obs], axis=-1)\n",
            "    rewards = tf.linalg.matvec(concatenated, hidden_param)\n",
            "    return rewards\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_per_arm_stationary_linear.py",
        "line_number": 193,
        "API": ".reduce_max(",
        "context": [
            "    rewards = tf.linalg.matvec(concatenated, hidden_param)\n",
            "    return rewards\n",
            "\n",
            "  def optimal_reward(observation, hidden_param):\n",
            "    return tf.reduce_max(_all_rewards(observation, hidden_param), axis=1)\n",
            "\n",
            "  def optimal_action(observation, hidden_param):\n",
            "    return tf.argmax(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_piecewise_linear.py",
        "line_number": 36,
        "API": ".getenv(",
        "context": [
            "from tf_agents.bandits.environments import piecewise_stochastic_environment as pse\n",
            "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
            "\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_enum(\n",
            "    'agent', 'LinUCB', ['LinUCB', 'LinTS'],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_piecewise_linear.py",
        "line_number": 71,
        "API": ".zeros(",
        "context": [
            "    interval_distribution = tfd.Deterministic(STATIONARITY_LENGTH)\n",
            "    action_shape = [NUM_ACTIONS]\n",
            "    observation_to_reward_shape = observation_shape + action_shape\n",
            "    observation_to_reward_distribution = tfd.Normal(\n",
            "        loc=tf.zeros(observation_to_reward_shape),\n",
            "        scale=tf.ones(observation_to_reward_shape))\n",
            "    additive_reward_distribution = tfd.Normal(\n",
            "        loc=tf.zeros(action_shape),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_ranking.py",
        "line_number": 31,
        "API": ".getenv(",
        "context": [
            "from tf_agents.specs import bandit_spec_utils\n",
            "from tf_agents.specs import tensor_spec\n",
            "from tf_agents.trajectories import trajectory\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_string('policy_type', 'cosine_distance',\n",
            "                    'The type of policy used.')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_ranking.py",
        "line_number": 75,
        "API": ".norm(",
        "context": [
            "    return np.random.randint(-1, 1, [GLOBAL_DIM]).astype(np.float32)\n",
            "\n",
            "  def _item_sampling_fn():\n",
            "    unnormalized = np.random.randint(-2, 3, [ITEM_DIM]).astype(np.float32)\n",
            "    return unnormalized / np.linalg.norm(unnormalized)\n",
            "\n",
            "  def _relevance_fn(global_obs, item_obs):\n",
            "    min_dim = min(GLOBAL_DIM, ITEM_DIM)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_ranking.py",
        "line_number": 81,
        "API": ".exp(",
        "context": [
            "  def _relevance_fn(global_obs, item_obs):\n",
            "    min_dim = min(GLOBAL_DIM, ITEM_DIM)\n",
            "    dot_prod = np.dot(global_obs[:min_dim],\n",
            "                      item_obs[:min_dim]).astype(np.float32)\n",
            "    return 1 / (1 + np.exp(-dot_prod))\n",
            "\n",
            "  if FLAGS.env_type == 'exp_pos_bias':\n",
            "    positional_biases = list(1.0 / np.arange(1, NUM_SLOTS + 1)**1.3)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_ranking.py",
        "line_number": 95,
        "API": ".eye(",
        "context": [
            "        batch_size=BATCH_SIZE)\n",
            "    feedback_model = ranking_agent.FeedbackModel.SCORE_VECTOR\n",
            "  elif FLAGS.env_type == 'base':\n",
            "    # Inner product with the excess dimensions ignored.\n",
            "    scores_weight_matrix = np.eye(ITEM_DIM, GLOBAL_DIM, dtype=np.float32)\n",
            "\n",
            "    feedback_model = ranking_agent.FeedbackModel.SCORE_VECTOR\n",
            "    if FLAGS.feedback_model == 'cascading':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_ranking.py",
        "line_number": 101,
        "API": ".format(",
        "context": [
            "    feedback_model = ranking_agent.FeedbackModel.SCORE_VECTOR\n",
            "    if FLAGS.feedback_model == 'cascading':\n",
            "      feedback_model = ranking_agent.FeedbackModel.CASCADING\n",
            "    else:\n",
            "      raise NotImplementedError('Feedback model {} not implemented'.format(\n",
            "          FLAGS.feedback_model))\n",
            "    if FLAGS.click_model == 'ghost_actions':\n",
            "      click_model = ranking_environment.ClickModel.GHOST_ACTIONS\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_ranking.py",
        "line_number": 108,
        "API": ".format(",
        "context": [
            "      click_model = ranking_environment.ClickModel.GHOST_ACTIONS\n",
            "    elif FLAGS.click_model == 'distance_based':\n",
            "      click_model = ranking_environment.ClickModel.DISTANCE_BASED\n",
            "    else:\n",
            "      raise NotImplementedError('Diversity mode {} not implemented'.format(\n",
            "          FLAGS.click_mode))\n",
            "\n",
            "    env = ranking_environment.RankingPyEnvironment(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_ranking.py",
        "line_number": 137,
        "API": ".format(",
        "context": [
            "    policy_type = ranking_agent.RankingPolicyType.NO_PENALTY\n",
            "  elif FLAGS.policy_type == 'descending_scores':\n",
            "    policy_type = ranking_agent.RankingPolicyType.DESCENDING_SCORES\n",
            "  else:\n",
            "    raise NotImplementedError('Policy type {} is not implemented'.format(\n",
            "        FLAGS.policy_type))\n",
            "  positional_bias_type = FLAGS.bias_type or None\n",
            "  agent = ranking_agent.RankingAgent(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_ranking.py",
        "line_number": 184,
        "API": ".gather(",
        "context": [
            "          bandit_spec_utils.PER_ARM_FEATURE_KEY:\n",
            "              tensor_spec.TensorSpec(dtype=dtype, shape=shape)\n",
            "      }\n",
            "    else:\n",
            "      slotted_items = tf.gather(item_obs, action, batch_dims=1)\n",
            "      new_observation = {\n",
            "          bandit_spec_utils.GLOBAL_FEATURE_KEY:\n",
            "              orig_trajectory.observation[bandit_spec_utils.GLOBAL_FEATURE_KEY],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_sparse_features.py",
        "line_number": 36,
        "API": ".getenv(",
        "context": [
            "from tf_agents.bandits.specs import utils as bandit_spec_utils\n",
            "from tf_agents.environments import tf_py_environment\n",
            "from tf_agents.policies import utils as policy_utilities\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "\n",
            "flags.DEFINE_enum(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_sparse_features.py",
        "line_number": 80,
        "API": ".array(",
        "context": [
            "\n",
            "def main(unused_argv):\n",
            "  tf.compat.v1.enable_v2_behavior()  # The trainer only runs with V2 enabled.\n",
            "\n",
            "  feature_dict = np.array([str(i) for i in range(DICTIONARY_SIZE)])\n",
            "  def _global_context_sampling_fn():\n",
            "    \"\"\"Generates one sample of global features.\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_sparse_features.py",
        "line_number": 100,
        "API": ".format(",
        "context": [
            "    \"\"\"\n",
            "    generated_features = feature_dict[np.random.randint(0, DICTIONARY_SIZE,\n",
            "                                                        [NUM_GLOBAL_FEATURES])]\n",
            "    global_features = {\n",
            "        'global_feature_{}'.format(i): generated_features[[i]]\n",
            "        for i in range(NUM_GLOBAL_FEATURES)\n",
            "    }\n",
            "    return global_features\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_sparse_features.py",
        "line_number": 125,
        "API": ".format(",
        "context": [
            "    \"\"\"\n",
            "    generated_features = feature_dict[np.random.randint(\n",
            "        0, DICTIONARY_SIZE, [NUM_ARM_FEATURES])]\n",
            "    arm_features = {\n",
            "        'arm_feature_{}'.format(i): generated_features[[i]]\n",
            "        for i in range(NUM_ARM_FEATURES)\n",
            "    }\n",
            "    return arm_features\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_sparse_features.py",
        "line_number": 167,
        "API": ".format(",
        "context": [
            "        tf.feature_column.categorical_column_with_vocabulary_list(\n",
            "            name, feature_dict))\n",
            "\n",
            "  global_columns = [\n",
            "      make_string_feature('global_feature_{}'.format(i))\n",
            "      for i in range(NUM_GLOBAL_FEATURES)\n",
            "  ]\n",
            "  arm_columns = [\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_stationary_linear.py",
        "line_number": 41,
        "API": ".getenv(",
        "context": [
            "from tf_agents.environments import wrappers\n",
            "from tf_agents.networks import q_network\n",
            "from tf_agents.policies import utils as policy_utilities\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_enum(\n",
            "    'agent', 'LinUCB',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_stationary_linear.py",
        "line_number": 75,
        "API": ".device(",
        "context": [
            "\n",
            "def main(unused_argv):\n",
            "  tf.compat.v1.enable_v2_behavior()  # The trainer only runs with V2 enabled.\n",
            "\n",
            "  with tf.device('/CPU:0'):  # due to b/128333994\n",
            "    if FLAGS.normalize_reward_fns:\n",
            "      action_reward_fns = (\n",
            "          environment_utilities.normalized_sliding_linear_reward_fn_generator(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_stationary_linear.py",
        "line_number": 169,
        "API": ".format(",
        "context": [
            "      agent.policy.step = train_step_counter\n",
            "    elif FLAGS.agent == 'BoltzmannGumbel':\n",
            "      num_samples_list = [tf.compat.v2.Variable(\n",
            "          0, dtype=tf.int64,\n",
            "          name='num_samples_{}'.format(k)) for k in range(NUM_ACTIONS)]\n",
            "      agent = neural_boltzmann_agent.NeuralBoltzmannAgent(\n",
            "          time_step_spec=environment.time_step_spec(),\n",
            "          action_spec=environment.action_spec(),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_structured_linear.py",
        "line_number": 38,
        "API": ".getenv(",
        "context": [
            "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
            "from tf_agents.environments import tf_py_environment\n",
            "from tf_agents.networks import q_network\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_enum(\n",
            "    'agent', 'LinUCB', ['LinUCB', 'LinTS', 'epsGreedy'],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_structured_linear.py",
        "line_number": 61,
        "API": ".device(",
        "context": [
            "\n",
            "def main(unused_argv):\n",
            "  tf.compat.v1.enable_v2_behavior()  # The trainer only runs with V2 enabled.\n",
            "\n",
            "  with tf.device('/CPU:0'):  # due to b/128333994\n",
            "    action_reward_fns = (\n",
            "        environment_utilities.structured_linear_reward_fn_generator(\n",
            "            CONTEXT_DIM, NUM_ACTIONS, REWARD_NOISE_VARIANCE))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_wheel.py",
        "line_number": 40,
        "API": ".getenv(",
        "context": [
            "from tf_agents.networks import q_network\n",
            "from tf_agents.policies import utils as policy_utilities\n",
            "\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_enum(\n",
            "    'agent', 'LinUCB', ['LinUCB', 'LinTS', 'epsGreedy', 'random', 'Mix'],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\agents\\examples\\v2\\train_eval_wheel.py",
        "line_number": 78,
        "API": ".device(",
        "context": [
            "    raise ValueError('The reward for action1 (MU_BASE[0]) should always be '\n",
            "                     'less than the reward for the optimal outer action '\n",
            "                     '(MU_HIGH).')\n",
            "\n",
            "  with tf.device('/CPU:0'):  # due to b/128333994\n",
            "    env = wheel_py_environment.WheelPyEnvironment(DELTA, MU_BASE, STD_BASE,\n",
            "                                                  MU_HIGH, STD_HIGH, BATCH_SIZE)\n",
            "    environment = tf_py_environment.TFPyEnvironment(env)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\bandit_py_environment.py",
        "line_number": 93,
        "API": ".map_structure(",
        "context": [
            "  def reward_spec(self) -> types.NestedArraySpec:\n",
            "    return self._reward_spec\n",
            "\n",
            "  def _empty_observation(self):\n",
            "    return tf.nest.map_structure(lambda x: np.zeros(x.shape, x.dtype),\n",
            "                                 self.observation_spec())\n",
            "\n",
            "  @abc.abstractmethod\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\bandit_tf_environment.py",
        "line_number": 44,
        "API": ".range(",
        "context": [
            "  Example usage with eager mode:\n",
            "  ```\n",
            "    # reset() creates the initial time_step and resets the environment.\n",
            "    time_step = environment.reset()\n",
            "    for _ in tf.range(num_steps):\n",
            "      action_step = policy.action(time_step)\n",
            "      time_step = environment.step(action_step.action)\n",
            "  ```\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\bandit_tf_environment.py",
        "line_number": 100,
        "API": ".assign(",
        "context": [
            "        action_spec=action_spec,\n",
            "        batch_size=batch_size)\n",
            "\n",
            "  def _update_time_step(self, time_step):\n",
            "    tf.nest.map_structure(lambda var, value: var.assign(value),\n",
            "                          self._time_step_variables, time_step)\n",
            "\n",
            "  @common.function()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\bandit_tf_environment.py",
        "line_number": 106,
        "API": ".map_structure(",
        "context": [
            "\n",
            "  @common.function()\n",
            "  def _current_time_step(self) -> ts.TimeStep:\n",
            "    def true_fn():\n",
            "      return tf.nest.map_structure(tf.identity, self._time_step_variables)\n",
            "    def false_fn():\n",
            "      current_time_step = self.reset()\n",
            "      return current_time_step\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\bandit_tf_environment.py",
        "line_number": 111,
        "API": ".cond(",
        "context": [
            "    def false_fn():\n",
            "      current_time_step = self.reset()\n",
            "      return current_time_step\n",
            "\n",
            "    return tf.cond(self._reset_called, true_fn, false_fn)\n",
            "\n",
            "  @common.function\n",
            "  def _reset(self) -> ts.TimeStep:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\bandit_tf_environment.py",
        "line_number": 118,
        "API": ".assign(",
        "context": [
            "  def _reset(self) -> ts.TimeStep:\n",
            "    current_time_step = ts.restart(\n",
            "        self._observe(), batch_size=self.batch_size,\n",
            "        reward_spec=self.time_step_spec().reward)\n",
            "    tf.compat.v1.assign(self._reset_called, True)\n",
            "    self._update_time_step(current_time_step)\n",
            "    return current_time_step\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\bernoulli_action_mask_tf_environment.py",
        "line_number": 60,
        "API": ".shape(",
        "context": [
            "\n",
            "@common.function\n",
            "def _maybe_add_one_action(mask):\n",
            "  \"\"\"For time steps where the mask is all zeros, adds one action randomly.\"\"\"\n",
            "  batch_size = tf.shape(mask)[0]\n",
            "  num_actions = tf.shape(mask)[1]\n",
            "  extra_actions = tf.one_hot(\n",
            "      tf.random.uniform([batch_size], 0, num_actions, dtype=tf.int32),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\bernoulli_action_mask_tf_environment.py",
        "line_number": 66,
        "API": ".reduce_max(",
        "context": [
            "  extra_actions = tf.one_hot(\n",
            "      tf.random.uniform([batch_size], 0, num_actions, dtype=tf.int32),\n",
            "      depth=num_actions,\n",
            "      dtype=tf.int32)\n",
            "  cond = tf.cast(tf.equal(tf.reduce_max(mask, axis=1), 1), tf.bool)\n",
            "  return tf.compat.v1.where(cond, mask, extra_actions)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\bernoulli_action_mask_tf_environment.py",
        "line_number": 109,
        "API": ".ones(",
        "context": [
            "        observation_spec_without_mask, mask_spec)\n",
            "    time_step_spec = ts.time_step_spec(joined_observation_spec)\n",
            "\n",
            "    self._current_mask = tf.compat.v2.Variable(\n",
            "        tf.ones([self.batch_size, self._num_actions], dtype=tf.int32))\n",
            "\n",
            "    super(BernoulliActionMaskTFEnvironment, self).__init__(\n",
            "        time_step_spec=time_step_spec,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\bernoulli_action_mask_tf_environment.py",
        "line_number": 122,
        "API": ".gather(",
        "context": [
            "    return self._original_environment\n",
            "\n",
            "  @common.function\n",
            "  def _check_action_with_mask(self, action):\n",
            "    is_allowed = tf.gather(\n",
            "        self._current_mask, tf.expand_dims(action, axis=1), batch_dims=1)\n",
            "    tf.assert_equal(is_allowed, 1, message='Action not in allowed action set.')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\bernoulli_action_mask_tf_environment.py",
        "line_number": 140,
        "API": ".assign(",
        "context": [
            "    original_observation = self._original_environment._observe()\n",
            "    mask = tfd.Bernoulli(self._action_probability).sample(\n",
            "        sample_shape=[self._batch_size, self._num_actions])\n",
            "    mask = _maybe_add_one_action(mask)\n",
            "    tf.compat.v1.assign(self._current_mask, mask)\n",
            "\n",
            "    return self._action_constraint_join_fn(original_observation, mask)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\bernoulli_py_environment.py",
        "line_number": 66,
        "API": ".ones(",
        "context": [
            "        observation_spec,\n",
            "        action_spec)\n",
            "\n",
            "  def _observe(self) -> types.NestedArray:\n",
            "    return np.ones(\n",
            "        shape=[self._batch_size] + list(self.observation_spec().shape),\n",
            "        dtype=self.observation_spec().dtype)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\bernoulli_py_environment.py",
        "line_number": 71,
        "API": ".array(",
        "context": [
            "        shape=[self._batch_size] + list(self.observation_spec().shape),\n",
            "        dtype=self.observation_spec().dtype)\n",
            "\n",
            "  def _apply_action(self, action: types.NestedArray) -> types.Float:\n",
            "    return np.array(\n",
            "        [np.floor(self._means[i] + np.random.random()) for i in action])\n",
            "\n",
            "  @property\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\classification_environment.py",
        "line_number": 48,
        "API": ".group(",
        "context": [
            "      the range `[0, t - 1]`.\n",
            "  Returns:\n",
            "    A `Tensor` `x` with shape `[r]` where `x[i] = tbl[i, row[i], col[i]`.\n",
            "  \"\"\"\n",
            "  assert_correct_shapes = tf.group(\n",
            "      tf.assert_equal(tf.shape(row), tf.shape(col)),\n",
            "      tf.assert_equal(tf.shape(row)[0], tf.shape(tbl)[0]))\n",
            "  rng = tf.range(tf.shape(row)[0])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\classification_environment.py",
        "line_number": 53,
        "API": ".control_dependencies(",
        "context": [
            "      tf.assert_equal(tf.shape(row), tf.shape(col)),\n",
            "      tf.assert_equal(tf.shape(row)[0], tf.shape(tbl)[0]))\n",
            "  rng = tf.range(tf.shape(row)[0])\n",
            "  idx = tf.stack([rng, row, col], axis=-1)\n",
            "  with tf.control_dependencies([assert_correct_shapes]):\n",
            "    values = tf.gather_nd(tbl, idx)\n",
            "  return values\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\classification_environment.py",
        "line_number": 102,
        "API": ".format(",
        "context": [
            "    event_shape = reward_distribution.event_shape\n",
            "    if len(event_shape) != 2:\n",
            "      raise ValueError(\n",
            "          'reward_distribution must have event shape of rank 2; '\n",
            "          'got event shape {}'.format(event_shape))\n",
            "    _, num_actions = event_shape\n",
            "    action_spec = tensor_spec.BoundedTensorSpec(shape=(),\n",
            "                                                dtype=tf.int32,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\classification_environment.py",
        "line_number": 113,
        "API": ".format(",
        "context": [
            "    output_shapes = tf.compat.v1.data.get_output_shapes(dataset)\n",
            "\n",
            "    # Computing `time_step_spec`.\n",
            "    if len(output_shapes) != 2:\n",
            "      raise ValueError('Dataset must have exactly two outputs; got {}'.format(\n",
            "          len(output_shapes)))\n",
            "    context_shape = output_shapes[0]\n",
            "    context_dtype, lbl_dtype = tf.compat.v1.data.get_output_types(dataset)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\classification_environment.py",
        "line_number": 134,
        "API": ".repeat(",
        "context": [
            "      dataset = dataset.shuffle(buffer_size=shuffle_buffer_size,\n",
            "                                seed=seed,\n",
            "                                reshuffle_each_iteration=True)\n",
            "    if repeat_dataset:\n",
            "      dataset = dataset.repeat()\n",
            "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
            "    if prefetch_size:\n",
            "      dataset = dataset.prefetch(prefetch_size)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\classification_environment.py",
        "line_number": 140,
        "API": ".zeros(",
        "context": [
            "    if prefetch_size:\n",
            "      dataset = dataset.prefetch(prefetch_size)\n",
            "    self._data_iterator = eager_utils.dataset_iterator(dataset)\n",
            "    self._current_label = tf.compat.v2.Variable(\n",
            "        tf.zeros(batch_size, dtype=lbl_dtype))\n",
            "    self._previous_label = tf.compat.v2.Variable(\n",
            "        tf.zeros(batch_size, dtype=lbl_dtype))\n",
            "    self._reward_distribution = reward_distribution\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\classification_environment.py",
        "line_number": 146,
        "API": ".mean(",
        "context": [
            "        tf.zeros(batch_size, dtype=lbl_dtype))\n",
            "    self._reward_distribution = reward_distribution\n",
            "    self._label_dtype = lbl_dtype\n",
            "\n",
            "    reward_means = self._reward_distribution.mean()\n",
            "    self._optimal_action_table = tf.argmax(\n",
            "        reward_means, axis=1, output_type=self._action_spec.dtype)\n",
            "    self._optimal_reward_table = tf.reduce_max(reward_means, axis=1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\classification_environment.py",
        "line_number": 153,
        "API": ".assign(",
        "context": [
            "    self._optimal_reward_table = tf.reduce_max(reward_means, axis=1)\n",
            "\n",
            "  def _observe(self) -> types.NestedTensor:\n",
            "    context, lbl = eager_utils.get_next(self._data_iterator)\n",
            "    self._previous_label.assign(self._current_label)\n",
            "    self._current_label.assign(tf.reshape(\n",
            "        tf.cast(lbl, dtype=self._label_dtype), shape=[self._batch_size]))\n",
            "    return tf.reshape(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\classification_environment.py",
        "line_number": 161,
        "API": ".reshape(",
        "context": [
            "        context,\n",
            "        shape=[self._batch_size] + self._time_step_spec.observation.shape)\n",
            "\n",
            "  def _apply_action(self, action: types.NestedTensor) -> types.NestedTensor:\n",
            "    action = tf.reshape(\n",
            "        action, shape=[self._batch_size] + self._action_spec.shape)\n",
            "    reward_samples = self._reward_distribution.sample(tf.shape(action))\n",
            "    return _batched_table_lookup(reward_samples, self._current_label, action)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\classification_environment.py",
        "line_number": 167,
        "API": ".gather(",
        "context": [
            "    reward_samples = self._reward_distribution.sample(tf.shape(action))\n",
            "    return _batched_table_lookup(reward_samples, self._current_label, action)\n",
            "\n",
            "  def compute_optimal_action(self) -> types.NestedTensor:\n",
            "    return tf.gather(\n",
            "        params=self._optimal_action_table, indices=self._previous_label)\n",
            "\n",
            "  def compute_optimal_reward(self) -> types.NestedTensor:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\dataset_utilities.py",
        "line_number": 63,
        "API": ".shape(",
        "context": [
            "    A 0/1 numpy array of rank 2 containing the same number of rows as the input.\n",
            "    The number of columns is equal to the sum of distinct elements per column of\n",
            "    the input array.\n",
            "  \"\"\"\n",
            "  num_rows, num_cols = np.shape(data)\n",
            "  encoded = np.array([], dtype=np.int32).reshape((num_rows, 0))\n",
            "  for i in range(num_cols):\n",
            "    vocabulary = sorted(list(set(data[:, i])))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\dataset_utilities.py",
        "line_number": 68,
        "API": ".array(",
        "context": [
            "  encoded = np.array([], dtype=np.int32).reshape((num_rows, 0))\n",
            "  for i in range(num_cols):\n",
            "    vocabulary = sorted(list(set(data[:, i])))\n",
            "    lookup = dict(list(zip(vocabulary, list(range(len(vocabulary))))))\n",
            "    int_encoded = np.array([lookup[x] for x in data[:, i]])\n",
            "    new_cols = np.eye(len(vocabulary), dtype=np.int32)[int_encoded]\n",
            "    encoded = np.append(encoded, new_cols, axis=1)\n",
            "  return encoded\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\dataset_utilities.py",
        "line_number": 99,
        "API": ".cast(",
        "context": [
            "    nd = np.genfromtxt(infile, dtype=str, delimiter=',')\n",
            "  _validate_mushroom_data(nd)\n",
            "  encoded = _one_hot(nd)\n",
            "  contexts = encoded[:, 2:]\n",
            "  context_tensor = tf.cast(contexts, tf.float32)\n",
            "  labels = encoded[:, 0]\n",
            "  label_tensor = tf.cast(labels, tf.int32)\n",
            "  dataset = tf.data.Dataset.from_tensor_slices((context_tensor, label_tensor))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\dataset_utilities.py",
        "line_number": 145,
        "API": ".cast(",
        "context": [
            "def convert_covertype_dataset(file_path, buffer_size=40000):\n",
            "  with tf.io.gfile.GFile(file_path, 'r') as infile:\n",
            "    data_array = np.genfromtxt(infile, dtype=int, delimiter=',')\n",
            "  contexts = data_array[:, :-1]\n",
            "  context_tensor = tf.cast(contexts, tf.float32)\n",
            "  labels = data_array[:, -1] - 1  # Classes are from [1, 7].\n",
            "  label_tensor = tf.cast(labels, tf.int32)\n",
            "  return tf.data.Dataset.from_tensor_slices(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\dataset_utilities.py",
        "line_number": 154,
        "API": ".zeros(",
        "context": [
            "\n",
            "\n",
            "def load_movielens_data(data_file, delimiter=','):\n",
            "  \"\"\"Loads the movielens data and returns the ratings matrix.\"\"\"\n",
            "  ratings_matrix = np.zeros([MOVIELENS_NUM_USERS, MOVIELENS_NUM_MOVIES])\n",
            "  with tf.io.gfile.GFile(data_file, 'r') as infile:\n",
            "    # The file is a csv with rows containing:\n",
            "    # user id | item id | rating | timestamp\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\drifting_linear_environment.py",
        "line_number": 31,
        "API": ".format(",
        "context": [
            "\n",
            "\n",
            "def _raise_batch_shape_error(tensor_name, batch_shape):\n",
            "  raise ValueError('`{tensor_name}` must have batch shape with length 1; '\n",
            "                   'got {batch_shape}.'.format(\n",
            "                       tensor_name=tensor_name,\n",
            "                       batch_shape=batch_shape))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\drifting_linear_environment.py",
        "line_number": 47,
        "API": ".shape(",
        "context": [
            "\n",
            "  Returns:\n",
            "    The updated tensor (same shape as `x`).\n",
            "  \"\"\"\n",
            "  n = tf.compat.dimension_value(input_x.shape[1]) or tf.shape(input_x)[1]\n",
            "  indices = tf.concat(\n",
            "      [row_index * tf.ones([n, 1], dtype=tf.int32),\n",
            "       tf.reshape(tf.range(n, dtype=tf.int32), [n, 1])], axis=-1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\drifting_linear_environment.py",
        "line_number": 52,
        "API": ".squeeze(",
        "context": [
            "  indices = tf.concat(\n",
            "      [row_index * tf.ones([n, 1], dtype=tf.int32),\n",
            "       tf.reshape(tf.range(n, dtype=tf.int32), [n, 1])], axis=-1)\n",
            "  return tf.tensor_scatter_nd_update(\n",
            "      tensor=input_x, indices=indices, updates=tf.squeeze(updates))\n",
            "\n",
            "\n",
            "def _apply_givens_rotation(cosa, sina, axis_i, axis_j, input_x):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\drifting_linear_environment.py",
        "line_number": 135,
        "API": ".format(",
        "context": [
            "    if reward_batch_shape.rank != 1:\n",
            "      _raise_batch_shape_error(\n",
            "          'additive_reward_distribution', reward_batch_shape)\n",
            "    if additive_reward_distribution.dtype != tf.float32:\n",
            "      raise ValueError('Reward  must have dtype float32; got {}'.format(\n",
            "          self._reward.dtype))\n",
            "    self._observation_dim = self._observation_distribution.batch_shape[1]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\drifting_linear_environment.py",
        "line_number": 150,
        "API": ".format(",
        "context": [
            "        for x in observation_to_reward_distribution.batch_shape]\n",
            "    if (observation_to_reward_shape !=\n",
            "        expected_observation_to_reward_shape):\n",
            "      raise ValueError(\n",
            "          'Observation to reward has {} as expected shape; got {}'.format(\n",
            "              expected_observation_to_reward_shape,\n",
            "              observation_to_reward_shape))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\drifting_linear_environment.py",
        "line_number": 193,
        "API": ".uniform(",
        "context": [
            "             observation: types.NestedTensor,\n",
            "             t: types.Int) -> types.NestedTensor:\n",
            "    # Apply the drift.\n",
            "    theta = self._drift_distribution.sample()\n",
            "    random_i = tf.random.uniform(\n",
            "        [], minval=0, maxval=self._observation_dim - 1, dtype=tf.int32)\n",
            "    random_j = tf.math.mod(random_i + 1, self._observation_dim)\n",
            "    tf.compat.v1.assign(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\drifting_linear_environment.py",
        "line_number": 199,
        "API": ".cos(",
        "context": [
            "    random_j = tf.math.mod(random_i + 1, self._observation_dim)\n",
            "    tf.compat.v1.assign(\n",
            "        self._current_observation_to_reward,\n",
            "        _apply_givens_rotation(\n",
            "            tf.cos(theta), tf.sin(theta), random_i, random_j,\n",
            "            self._current_observation_to_reward))\n",
            "    tf.compat.v1.assign(self._current_additive_reward,\n",
            "                        self._additive_reward_distribution.sample())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\drifting_linear_environment.py",
        "line_number": 204,
        "API": ".matmul(",
        "context": [
            "            self._current_observation_to_reward))\n",
            "    tf.compat.v1.assign(self._current_additive_reward,\n",
            "                        self._additive_reward_distribution.sample())\n",
            "\n",
            "    reward = (tf.matmul(observation, self._current_observation_to_reward) +\n",
            "              self._current_additive_reward)\n",
            "    return reward\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\drifting_linear_environment.py",
        "line_number": 211,
        "API": ".matmul(",
        "context": [
            "\n",
            "  @gin.configurable\n",
            "  def compute_optimal_reward(\n",
            "      self, observation: types.NestedTensor) -> types.NestedTensor:\n",
            "    deterministic_reward = tf.matmul(\n",
            "        observation, self._current_observation_to_reward)\n",
            "    optimal_action_reward = tf.reduce_max(deterministic_reward, axis=-1)\n",
            "    return optimal_action_reward\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\drifting_linear_environment.py",
        "line_number": 219,
        "API": ".matmul(",
        "context": [
            "\n",
            "  @gin.configurable\n",
            "  def compute_optimal_action(\n",
            "      self, observation: types.NestedTensor) -> types.NestedTensor:\n",
            "    deterministic_reward = tf.matmul(\n",
            "        observation, self._current_observation_to_reward)\n",
            "    optimal_action = tf.argmax(\n",
            "        deterministic_reward, axis=-1, output_type=tf.int32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\environment_utilities.py",
        "line_number": 51,
        "API": ".normal(",
        "context": [
            "      A scalar value: the reward.\n",
            "    \"\"\"\n",
            "    mu = np.dot(x, self.theta)\n",
            "    if enable_noise:\n",
            "      return np.random.normal(mu, self.sigma)\n",
            "    return mu\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\environment_utilities.py",
        "line_number": 138,
        "API": ".copy(",
        "context": [
            "  theta_list.append(theta_previous)\n",
            "  for _ in range(1, num_actions):\n",
            "    drift = np.random.rand(context_dim)\n",
            "    theta_new = theta_previous + drift_coefficient * drift\n",
            "    theta_previous = theta_new.copy()\n",
            "    theta_list.append(theta_new)\n",
            "\n",
            "  return linear_reward_fn_generator(theta_list, variance)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\environment_utilities.py",
        "line_number": 166,
        "API": ".matmul(",
        "context": [
            "  def __call__(self, x, enable_noise=False):\n",
            "    # `x` is of shape [`batch_size`, 'context_dim']\n",
            "    # `theta` is of shape [`context_dim`, 'num_rewards']\n",
            "    # The result `predicted_rewards` has shape [`batch_size`, `num_rewards`]\n",
            "    predicted_rewards = np.matmul(x, self.thetas)\n",
            "    return predicted_rewards\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\environment_utilities.py",
        "line_number": 196,
        "API": ".squeeze(",
        "context": [
            "\n",
            "  def _gen_multiple_rewards_for_action():\n",
            "    params = np.random.rand(context_dim, num_rewards)\n",
            "    if squeeze_dims:\n",
            "      return np.squeeze(params)\n",
            "    else:\n",
            "      return params\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\environment_utilities.py",
        "line_number": 219,
        "API": ".stack(",
        "context": [
            "  Returns:\n",
            "    The optimal reward.\n",
            "  \"\"\"\n",
            "  num_actions = len(per_action_reward_fns)\n",
            "  rewards = np.stack(\n",
            "      [per_action_reward_fns[a](observation, enable_noise)\n",
            "       for a in range(num_actions)],\n",
            "      axis=-1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\environment_utilities.py",
        "line_number": 224,
        "API": ".max(",
        "context": [
            "      [per_action_reward_fns[a](observation, enable_noise)\n",
            "       for a in range(num_actions)],\n",
            "      axis=-1)\n",
            "  # `rewards` should be of shape [`batch_size`, `num_actions`].\n",
            "  optimal_action_reward = np.max(rewards, axis=-1)\n",
            "  return optimal_action_reward\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\environment_utilities.py",
        "line_number": 237,
        "API": ".py_function(",
        "context": [
            "  compute_optimal_reward_fn = functools.partial(\n",
            "      compute_optimal_reward,\n",
            "      per_action_reward_fns=per_action_reward_fns,\n",
            "      enable_noise=enable_noise)\n",
            "  return tf.py_function(compute_optimal_reward_fn, [observation], tf.float32)\n",
            "\n",
            "\n",
            "@gin.configurable\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\environment_utilities.py",
        "line_number": 256,
        "API": ".stack(",
        "context": [
            "  Returns:\n",
            "    The optimal action, that is, the one with the highest reward.\n",
            "  \"\"\"\n",
            "  num_actions = len(per_action_reward_fns)\n",
            "  rewards = np.stack([\n",
            "      per_action_reward_fns[a](observation, enable_noise)\n",
            "      for a in range(num_actions)\n",
            "  ],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\environment_utilities.py",
        "line_number": 262,
        "API": ".argmax(",
        "context": [
            "      for a in range(num_actions)\n",
            "  ],\n",
            "                     axis=-1)\n",
            "\n",
            "  optimal_action = np.argmax(rewards, axis=-1)\n",
            "  return optimal_action\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\environment_utilities.py",
        "line_number": 276,
        "API": ".py_function(",
        "context": [
            "  compute_optimal_action_fn = functools.partial(\n",
            "      compute_optimal_action,\n",
            "      per_action_reward_fns=per_action_reward_fns,\n",
            "      enable_noise=enable_noise)\n",
            "  return tf.py_function(compute_optimal_action_fn, [observation], action_dtype)\n",
            "\n",
            "\n",
            "@gin.configurable\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\environment_utilities.py",
        "line_number": 332,
        "API": ".py_function(",
        "context": [
            "def tf_wheel_bandit_compute_optimal_action(observation,\n",
            "                                           delta,\n",
            "                                           action_dtype=tf.int32):\n",
            "  \"\"\"TF wrapper around `compute_optimal_action` to be used in `tf_metrics`.\"\"\"\n",
            "  return tf.py_function(wheel_py_environment.compute_optimal_action,\n",
            "                        [observation, delta], action_dtype)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\environment_utilities.py",
        "line_number": 340,
        "API": ".py_function(",
        "context": [
            "@gin.configurable\n",
            "def tf_wheel_bandit_compute_optimal_reward(observation, delta, mu_inside,\n",
            "                                           mu_high):\n",
            "  \"\"\"TF wrapper around `compute_optimal_reward` to be used in `tf_metrics`.\"\"\"\n",
            "  return tf.py_function(wheel_py_environment.compute_optimal_reward,\n",
            "                        [observation, delta, mu_inside, mu_high], tf.float32)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\environment_utilities.py",
        "line_number": 348,
        "API": ".py_function(",
        "context": [
            "@gin.configurable\n",
            "def compute_optimal_reward_with_movielens_environment(observation, environment):\n",
            "  \"\"\"Helper function for gin configurable Regret metric.\"\"\"\n",
            "  del observation\n",
            "  return tf.py_function(environment.compute_optimal_reward, [], tf.float32)\n",
            "\n",
            "\n",
            "@gin.configurable\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\environment_utilities.py",
        "line_number": 357,
        "API": ".py_function(",
        "context": [
            "                                                      environment,\n",
            "                                                      action_dtype=tf.int32):\n",
            "  \"\"\"Helper function for gin configurable SuboptimalArms metric.\"\"\"\n",
            "  del observation\n",
            "  return tf.py_function(environment.compute_optimal_action, [], action_dtype)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\movielens_per_arm_py_environment.py",
        "line_number": 81,
        "API": ".svd(",
        "context": [
            "        data_dir, delimiter=csv_delimiter)\n",
            "    self._num_users, self._num_movies = self._data_matrix.shape\n",
            "\n",
            "    # Compute the SVD.\n",
            "    u, s, vh = np.linalg.svd(self._data_matrix, full_matrices=False)\n",
            "\n",
            "    # Keep only the largest singular values.\n",
            "    self._u_hat = u[:, :rank_k].astype(np.float32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\movielens_per_arm_py_environment.py",
        "line_number": 86,
        "API": ".transpose(",
        "context": [
            "\n",
            "    # Keep only the largest singular values.\n",
            "    self._u_hat = u[:, :rank_k].astype(np.float32)\n",
            "    self._s_hat = s[:rank_k].astype(np.float32)\n",
            "    self._v_hat = np.transpose(vh[:rank_k]).astype(np.float32)\n",
            "\n",
            "    self._approx_ratings_matrix = np.matmul(self._u_hat * self._s_hat,\n",
            "                                            np.transpose(self._v_hat))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\movielens_per_arm_py_environment.py",
        "line_number": 106,
        "API": ".zeros(",
        "context": [
            "                shape=[num_actions, rank_k], dtype=np.float32),\n",
            "    }\n",
            "    self._time_step_spec = ts.time_step_spec(observation_spec)\n",
            "\n",
            "    self._current_user_indices = np.zeros(batch_size, dtype=np.int32)\n",
            "    self._previous_user_indices = np.zeros(batch_size, dtype=np.int32)\n",
            "\n",
            "    self._current_movie_indices = np.zeros([batch_size, num_actions],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\movielens_per_arm_py_environment.py",
        "line_number": 111,
        "API": ".zeros(",
        "context": [
            "    self._previous_user_indices = np.zeros(batch_size, dtype=np.int32)\n",
            "\n",
            "    self._current_movie_indices = np.zeros([batch_size, num_actions],\n",
            "                                           dtype=np.int32)\n",
            "    self._previous_movie_indices = np.zeros([batch_size, num_actions],\n",
            "                                            dtype=np.int32)\n",
            "\n",
            "    self._observation = {\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\movielens_per_arm_py_environment.py",
        "line_number": 116,
        "API": ".zeros(",
        "context": [
            "                                            dtype=np.int32)\n",
            "\n",
            "    self._observation = {\n",
            "        GLOBAL_KEY:\n",
            "            np.zeros([batch_size, rank_k]),\n",
            "        PER_ARM_KEY:\n",
            "            np.zeros([batch_size, num_actions, rank_k]),\n",
            "    }\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\movielens_per_arm_py_environment.py",
        "line_number": 138,
        "API": ".array(",
        "context": [
            "        self._num_users, size=self._batch_size)\n",
            "    self._previous_user_indices = self._current_user_indices\n",
            "    self._current_user_indices = sampled_user_indices\n",
            "\n",
            "    sampled_movie_indices = np.array([\n",
            "        random.sample(range(self._num_movies), self._num_actions)\n",
            "        for _ in range(self._batch_size)\n",
            "    ])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\movielens_per_arm_py_environment.py",
        "line_number": 144,
        "API": ".reshape(",
        "context": [
            "        for _ in range(self._batch_size)\n",
            "    ])\n",
            "    movie_index_vector = sampled_movie_indices.reshape(-1)\n",
            "    flat_movie_list = self._v_hat[movie_index_vector]\n",
            "    current_movies = flat_movie_list.reshape(\n",
            "        [self._batch_size, self._num_actions, self._context_dim])\n",
            "\n",
            "    self._previous_movie_indices = self._current_movie_indices\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\movielens_per_arm_py_environment.py",
        "line_number": 166,
        "API": ".expand_dims(",
        "context": [
            "                                       chosen_arm_indices]\n",
            "\n",
            "  def _rewards_for_all_actions(self):\n",
            "    rewards_matrix = self._approx_ratings_matrix[\n",
            "        np.expand_dims(self._previous_user_indices, axis=-1),\n",
            "        self._previous_movie_indices]\n",
            "    return rewards_matrix\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\movielens_per_arm_py_environment.py",
        "line_number": 171,
        "API": ".argmax(",
        "context": [
            "        self._previous_movie_indices]\n",
            "    return rewards_matrix\n",
            "\n",
            "  def compute_optimal_action(self):\n",
            "    return np.argmax(self._rewards_for_all_actions(), axis=-1)\n",
            "\n",
            "  def compute_optimal_reward(self):\n",
            "    return np.max(self._rewards_for_all_actions(), axis=-1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\movielens_py_environment.py",
        "line_number": 73,
        "API": ".sum(",
        "context": [
            "        data_dir, delimiter=csv_delimiter)\n",
            "    # Keep only the first items.\n",
            "    self._data_matrix = self._data_matrix[:, :num_movies]\n",
            "    # Filter the users with no iterm rated.\n",
            "    nonzero_users = list(np.nonzero(np.sum(self._data_matrix, axis=1) > 0.0)[0])\n",
            "    self._data_matrix = self._data_matrix[nonzero_users, :]\n",
            "    self._effective_num_users = len(nonzero_users)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\movielens_py_environment.py",
        "line_number": 78,
        "API": ".svd(",
        "context": [
            "    self._data_matrix = self._data_matrix[nonzero_users, :]\n",
            "    self._effective_num_users = len(nonzero_users)\n",
            "\n",
            "    # Compute the SVD.\n",
            "    u, s, vh = np.linalg.svd(self._data_matrix, full_matrices=False)\n",
            "\n",
            "    # Keep only the largest singular values.\n",
            "    self._u_hat = u[:, :rank_k] * np.sqrt(s[:rank_k])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\movielens_py_environment.py",
        "line_number": 83,
        "API": ".sqrt(",
        "context": [
            "\n",
            "    # Keep only the largest singular values.\n",
            "    self._u_hat = u[:, :rank_k] * np.sqrt(s[:rank_k])\n",
            "    self._v_hat = np.transpose(\n",
            "        np.transpose(vh[:rank_k, :]) * np.sqrt(s[:rank_k]))\n",
            "    self._approx_ratings_matrix = np.matmul(self._u_hat, self._v_hat)\n",
            "\n",
            "    self._current_users = np.zeros(batch_size)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\movielens_py_environment.py",
        "line_number": 98,
        "API": ".zeros(",
        "context": [
            "        name='action')\n",
            "    observation_spec = array_spec.ArraySpec(\n",
            "        shape=(self._context_dim,), dtype=np.float64, name='observation')\n",
            "    self._time_step_spec = ts.time_step_spec(observation_spec)\n",
            "    self._observation = np.zeros((self._batch_size, self._context_dim))\n",
            "\n",
            "    self._optimal_action_table = np.argmax(\n",
            "        self._approx_ratings_matrix, axis=1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\movielens_py_environment.py",
        "line_number": 130,
        "API": ".array(",
        "context": [
            "    \"\"\"Computes the reward for the input actions.\"\"\"\n",
            "    rewards = []\n",
            "    for i, j in zip(self._current_users, action):\n",
            "      rewards.append(self._approx_ratings_matrix[i, j])\n",
            "    return np.array(rewards)\n",
            "\n",
            "  def compute_optimal_action(self):\n",
            "    return self._optimal_action_table[self._previous_users]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\non_stationary_stochastic_environment.py",
        "line_number": 100,
        "API": ".map_structure(",
        "context": [
            "    return common.create_variable(\n",
            "        name=spec.name,\n",
            "        dtype=spec.dtype,\n",
            "        shape=[batch_size] + spec.shape.as_list())\n",
            "  return tf.nest.map_structure(create_variable, specs)\n",
            "\n",
            "\n",
            "def _assign_variable_nest(variables, values):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\non_stationary_stochastic_environment.py",
        "line_number": 110,
        "API": ".map_structure(",
        "context": [
            "                               values)\n",
            "\n",
            "\n",
            "def _read_value_nest(variables):\n",
            "  return tf.nest.map_structure(lambda variable: variable.read_value(),\n",
            "                               variables)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\non_stationary_stochastic_environment.py",
        "line_number": 153,
        "API": ".cast(",
        "context": [
            "                                                     self._env_time)\n",
            "    tf.compat.v1.assign_add(self._env_time,\n",
            "                            self._environment_dynamics.batch_size)\n",
            "    return common.index_with_actions(\n",
            "        self._reward, tf.cast(action, dtype=tf.int32))\n",
            "\n",
            "  def _observe(self) -> types.NestedTensor:\n",
            "    _assign_variable_nest(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\piecewise_bernoulli_py_environment.py",
        "line_number": 96,
        "API": ".any(",
        "context": [
            "        actions.\n",
            "    \"\"\"\n",
            "    self._batch_size = batch_size\n",
            "    self._piece_means = np.asarray(piece_means, dtype=np.float32)\n",
            "    if np.any(self._piece_means > 1.0) or np.any(self._piece_means < 0):\n",
            "      raise ValueError('All parameters should be floats in [0, 1].')\n",
            "    self._num_pieces, self._num_actions = self._piece_means.shape\n",
            "    self._change_duration_generator = change_duration_generator\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\piecewise_bernoulli_py_environment.py",
        "line_number": 130,
        "API": ".format(",
        "context": [
            "    while self._current_time >= self._next_change:\n",
            "      duration = int(next(self._change_duration_generator))  # pytype: disable=wrong-arg-types  # trace-all-classes\n",
            "      if duration < 0:\n",
            "        raise ValueError(\n",
            "            'Generated duration must be non-negative. Got {}.'.format(duration))\n",
            "      self._next_change += duration\n",
            "      self._current_piece = (self._current_piece + 1) % self._num_pieces\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\piecewise_bernoulli_py_environment.py",
        "line_number": 135,
        "API": ".zeros(",
        "context": [
            "      self._next_change += duration\n",
            "      self._current_piece = (self._current_piece + 1) % self._num_pieces\n",
            "\n",
            "  def _observe(self) -> types.NestedArray:\n",
            "    return np.zeros(\n",
            "        shape=[self._batch_size, 1],\n",
            "        dtype=self.observation_spec().dtype)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\piecewise_bernoulli_py_environment.py",
        "line_number": 140,
        "API": ".floor(",
        "context": [
            "        shape=[self._batch_size, 1],\n",
            "        dtype=self.observation_spec().dtype)\n",
            "\n",
            "  def _apply_action(self, action: types.NestedArray) -> types.NestedArray:\n",
            "    reward = np.floor(self._piece_means[self._current_piece, action] +\n",
            "                      np.random.random((self._batch_size,)))\n",
            "    self._increment_time()\n",
            "    return reward\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\piecewise_stochastic_environment.py",
        "line_number": 90,
        "API": ".format(",
        "context": [
            "      _raise_batch_shape_error(\n",
            "          'additive_reward_distribution', reward_batch_shape)\n",
            "\n",
            "    if additive_reward_distribution.dtype != tf.float32:\n",
            "      raise ValueError('Reward  must have dtype float32; got {}'.format(\n",
            "          self._reward.dtype))\n",
            "\n",
            "    expected_observation_to_reward_shape = [\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\piecewise_stochastic_environment.py",
        "line_number": 106,
        "API": ".format(",
        "context": [
            "\n",
            "    if (observation_to_reward_shape !=\n",
            "        expected_observation_to_reward_shape):\n",
            "      raise ValueError(\n",
            "          'Observation to reward has {} as expected shape; got {}'.format(\n",
            "              observation_to_reward_shape,\n",
            "              expected_observation_to_reward_shape))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\piecewise_stochastic_environment.py",
        "line_number": 111,
        "API": ".cast(",
        "context": [
            "              observation_to_reward_shape,\n",
            "              expected_observation_to_reward_shape))\n",
            "\n",
            "    self._current_interval = tf.compat.v2.Variable(\n",
            "        tf.cast(interval_distribution.sample(), dtype=tf.int64),\n",
            "        dtype=tf.int64, name='interval')\n",
            "    self._current_observation_to_reward = tf.compat.v2.Variable(\n",
            "        observation_to_reward_distribution.sample(),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\piecewise_stochastic_environment.py",
        "line_number": 167,
        "API": ".cast(",
        "context": [
            "        The pair of `tf.Tensor` `(observation_to_reward, additive_reward)`.\n",
            "      \"\"\"\n",
            "      tf.compat.v1.assign_add(\n",
            "          self._current_interval,\n",
            "          tf.cast(self._interval_distribution.sample(), dtype=tf.int64))\n",
            "      tf.compat.v1.assign(self._current_observation_to_reward,\n",
            "                          self._observation_to_reward_distribution.sample())\n",
            "      tf.compat.v1.assign(self._current_additive_reward,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\piecewise_stochastic_environment.py",
        "line_number": 176,
        "API": ".cond(",
        "context": [
            "\n",
            "      return [self._current_observation_to_reward,\n",
            "              self._current_additive_reward]\n",
            "\n",
            "    observation_to_reward, additive_reward = tf.cond(\n",
            "        t < self._current_interval,\n",
            "        same_interval_parameters,\n",
            "        new_interval_parameters)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\piecewise_stochastic_environment.py",
        "line_number": 181,
        "API": ".matmul(",
        "context": [
            "        t < self._current_interval,\n",
            "        same_interval_parameters,\n",
            "        new_interval_parameters)\n",
            "\n",
            "    reward = (tf.matmul(observation, observation_to_reward) +\n",
            "              tf.reshape(additive_reward, [1, -1]))\n",
            "    return reward\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\random_bandit_environment.py",
        "line_number": 36,
        "API": ".format(",
        "context": [
            "def _raise_batch_shape_error(distribution_name, batch_shape):\n",
            "  raise ValueError('`{distribution_name}` must have batch shape with length 1; '\n",
            "                   'got {batch_shape}. Consider using '\n",
            "                   '`tensorflow_probability.distributions.Independent` '\n",
            "                   'to manipulate batch and event shapes.'.format(\n",
            "                       distribution_name=distribution_name,\n",
            "                       batch_shape=batch_shape))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\random_bandit_environment.py",
        "line_number": 56,
        "API": ".zeros(",
        "context": [
            "    have length exactly 1. `tensorflow_probability.distributions.Independent` is\n",
            "    useful for manipulating batch and event shapes. For example,\n",
            "\n",
            "    ```python\n",
            "    observation_distribution = tfd.Independent(tfd.Normal(tf.zeros([12, 3, 4]),\n",
            "                                                          tf.ones([12, 3, 4])))\n",
            "    env = RandomBanditEnvironment(observation_distribution, ...)\n",
            "    env.observation_spec  # tensor_spec.TensorSpec(shape=[3, 4], ...)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\random_bandit_environment.py",
        "line_number": 89,
        "API": ".format(",
        "context": [
            "          'reward_distribution', observation_batch_shape)\n",
            "\n",
            "    if reward_event_shape.rank != 0:\n",
            "      raise ValueError('`reward_distribution` must have event_shape (); '\n",
            "                       'got {}'.format(reward_event_shape))\n",
            "\n",
            "    if reward_distribution.dtype != tf.float32:\n",
            "      raise ValueError('`reward_distribution` must have dtype float32; '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\random_bandit_environment.py",
        "line_number": 98,
        "API": ".format(",
        "context": [
            "\n",
            "    if observation_batch_shape[0] != reward_batch_shape[0]:\n",
            "      raise ValueError(\n",
            "          '`reward_distribution` and `observation_distribution` must have the '\n",
            "          'same batch shape; got {} and {}'.format(\n",
            "              reward_batch_shape, observation_batch_shape))\n",
            "    batch_size = tf.compat.dimension_value(observation_batch_shape[0])\n",
            "    self._observation_distribution = observation_distribution\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\ranking_environment.py",
        "line_number": 182,
        "API": ".format(",
        "context": [
            "      reward_spec = array_spec.ArraySpec(\n",
            "          shape=[num_slots], dtype=np.float32, name='score_vector')\n",
            "    else:\n",
            "      raise NotImplementedError(\n",
            "          'Feedback model {} not implemented'.format(feedback_model))\n",
            "\n",
            "    super(RankingPyEnvironment, self).__init__(\n",
            "        observation_spec, action_spec, reward_spec, name=name)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\ranking_environment.py",
        "line_number": 195,
        "API": ".stack(",
        "context": [
            "  def batch_size(self) -> int:\n",
            "    return self._batch_size\n",
            "\n",
            "  def _observe(self) -> types.NestedArray:\n",
            "    global_obs = np.stack(\n",
            "        [self._global_sampling_fn() for _ in range(self._batch_size)])\n",
            "    item_obs = np.reshape([\n",
            "        self._item_sampling_fn()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\ranking_environment.py",
        "line_number": 210,
        "API": ".expand_dims(",
        "context": [
            "      raise ValueError('Number of actions must match batch size.')\n",
            "    global_obs = self._observation[GLOBAL_KEY]\n",
            "    item_obs = self._observation[PER_ARM_KEY]\n",
            "    batch_size_range = range(self.batch_size)\n",
            "    slotted_items = item_obs[np.expand_dims(batch_size_range, axis=1), action]\n",
            "    if self._click_model == ClickModel.GHOST_ACTIONS:\n",
            "      chosen_items = self._choose_items_ghost_actions(global_obs, slotted_items)\n",
            "    elif self._click_model == ClickModel.DISTANCE_BASED:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\ranking_environment.py",
        "line_number": 217,
        "API": ".format(",
        "context": [
            "    elif self._click_model == ClickModel.DISTANCE_BASED:\n",
            "      chosen_items = self._choose_items_distance_based(global_obs,\n",
            "                                                       slotted_items)\n",
            "    else:\n",
            "      raise NotImplementedError('Diversity model {} not implemented'.format(\n",
            "          self._click_model))\n",
            "\n",
            "    if self._feedback_model == FeedbackModel.CASCADING:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\ranking_environment.py",
        "line_number": 229,
        "API": ".zeros(",
        "context": [
            "      chosen_values = (chosen_items < self._num_slots).astype(np.float32)\n",
            "      return self._cascading_to_scorevector(chosen_items, chosen_values)\n",
            "\n",
            "  def _cascading_to_scorevector(self, chosen_items, chosen_values):\n",
            "    scores = np.zeros((self.batch_size, self._num_slots + 1), dtype=np.float32)\n",
            "    r = np.arange(self.batch_size)\n",
            "    scores[r, chosen_items] = chosen_values\n",
            "    return scores[:, :-1]  # The last column is for samples with no click.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\ranking_environment.py",
        "line_number": 240,
        "API": ".map_structure(",
        "context": [
            "    # TODO(b/199824775): The trajectory module assumes all reward is float32.\n",
            "    # Sort this out with TF-Agents.\n",
            "    output = super(RankingPyEnvironment, self)._step(action)\n",
            "    reward = output.reward\n",
            "    new_reward = tf.nest.map_structure(lambda x, t: x.astype(t), reward,\n",
            "                                       self.reward_spec())\n",
            "    return ts.TimeStep(\n",
            "        step_type=output.step_type,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\ranking_environment.py",
        "line_number": 249,
        "API": ".matmul(",
        "context": [
            "        discount=output.discount,\n",
            "        observation=output.observation)\n",
            "\n",
            "  def _batched_inner_product(self, global_obs, item_obs):\n",
            "    left = np.matmul(item_obs, self._scores_weight_matrix)\n",
            "    expanded_left = np.expand_dims(left, axis=-2)\n",
            "    expanded_globals = np.reshape(\n",
            "        global_obs, newshape=[self._batch_size, 1, self._global_dim, 1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\ranking_environment.py",
        "line_number": 254,
        "API": ".matmul(",
        "context": [
            "    expanded_left = np.expand_dims(left, axis=-2)\n",
            "    expanded_globals = np.reshape(\n",
            "        global_obs, newshape=[self._batch_size, 1, self._global_dim, 1])\n",
            "    scores = np.reshape(\n",
            "        np.matmul(expanded_left, expanded_globals),\n",
            "        newshape=[self._batch_size, -1])\n",
            "    return scores\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\ranking_environment.py",
        "line_number": 262,
        "API": ".broadcast_to(",
        "context": [
            "  def _choose_items_ghost_actions(self, global_obs, slotted_items):\n",
            "    # If one of the unit vectors gets chosen, it means no-click.\n",
            "    slotted_items_with_units = np.concatenate([\n",
            "        slotted_items,\n",
            "        np.broadcast_to(\n",
            "            np.identity(self._item_dim),\n",
            "            [self._batch_size, self._item_dim, self._item_dim])\n",
            "    ],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\ranking_environment.py",
        "line_number": 269,
        "API": ".normal(",
        "context": [
            "    ],\n",
            "                                              axis=1)\n",
            "\n",
            "    scores = self._batched_inner_product(global_obs, slotted_items_with_units)\n",
            "    perturbed_scores = np.random.normal(loc=scores, scale=1)\n",
            "    unnormalized_probabilities = 1 / (1 + np.exp(-perturbed_scores))\n",
            "    probabilities = unnormalized_probabilities / np.expand_dims(\n",
            "        np.linalg.norm(unnormalized_probabilities, ord=1, axis=-1), axis=1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\ranking_environment.py",
        "line_number": 274,
        "API": ".minimum(",
        "context": [
            "    unnormalized_probabilities = 1 / (1 + np.exp(-perturbed_scores))\n",
            "    probabilities = unnormalized_probabilities / np.expand_dims(\n",
            "        np.linalg.norm(unnormalized_probabilities, ord=1, axis=-1), axis=1)\n",
            "\n",
            "    return np.minimum([\n",
            "        np.random.choice(np.arange(self._num_slots + self._item_dim), p=p)\n",
            "        for p in probabilities\n",
            "    ], self._num_slots)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\ranking_environment.py",
        "line_number": 283,
        "API": ".ones(",
        "context": [
            "  def _choose_items_distance_based(self, global_obs, slotted_items):\n",
            "    scores = self._batched_inner_product(global_obs, slotted_items)\n",
            "    scores = np.concatenate(\n",
            "        [scores,\n",
            "         np.ones([self._batch_size, 1]) * self._distance_threshold], axis=1)\n",
            "    if self._real_cascade:\n",
            "      return np.array(\n",
            "          [np.min(np.where(s >= self._distance_threshold)) for s in scores])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\ranking_environment.py",
        "line_number": 336,
        "API": ".array(",
        "context": [
            "    self._item_sampling_fn = item_sampling_fn\n",
            "    self._num_items = num_items\n",
            "\n",
            "    self._num_slots = len(observation_probs)\n",
            "    self._observation_probs = np.array(observation_probs)\n",
            "    if np.any(self._observation_probs > 1) or np.any(\n",
            "        self._observation_probs < 0):\n",
            "      raise ValueError('Observation probabilities need to be in [0, 1].')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\ranking_environment.py",
        "line_number": 363,
        "API": ".stack(",
        "context": [
            "    super(ExplicitPositionalBiasRankingEnvironment, self).__init__(\n",
            "        observation_spec, action_spec, reward_spec, name=name)\n",
            "\n",
            "  def _observe(self) -> types.NestedArray:\n",
            "    global_obs = np.stack(\n",
            "        [self._global_sampling_fn() for _ in range(self._batch_size)])\n",
            "    item_obs = np.reshape([\n",
            "        self._item_sampling_fn()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\ranking_environment.py",
        "line_number": 378,
        "API": ".expand_dims(",
        "context": [
            "      raise ValueError('Number of actions must match batch size.')\n",
            "    global_obs = self._observation[GLOBAL_KEY]\n",
            "    item_obs = self._observation[PER_ARM_KEY]\n",
            "    batch_size_range = range(self.batch_size)\n",
            "    slotted_items = item_obs[np.expand_dims(batch_size_range, axis=1), action]\n",
            "    relevances = self._get_relevances(global_obs, slotted_items)\n",
            "\n",
            "    # The `relevances` array is of shape `[batch_size, num_slots]`, the\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\ranking_environment.py",
        "line_number": 393,
        "API": ".array(",
        "context": [
            "    \"\"\"Returns the relevance of each item in a batched action.\"\"\"\n",
            "    s_range = range(self._num_slots)\n",
            "    b_range = range(self._batch_size)\n",
            "\n",
            "    relevances = np.array([[\n",
            "        self._relevance_fn(global_obs[i], slotted_items[i, j]) for j in s_range\n",
            "    ] for i in b_range])\n",
            "    clipped_relevances = np.clip(relevances, 0., 1.)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\stationary_stochastic_per_arm_py_environment.py",
        "line_number": 136,
        "API": ".stack(",
        "context": [
            "  def batch_size(self) -> int:\n",
            "    return self._batch_size\n",
            "\n",
            "  def _observe(self) -> types.NestedArray:\n",
            "    global_obs = np.stack(\n",
            "        [self._global_context_sampling_fn() for _ in range(self._batch_size)])\n",
            "    arm_obs = np.reshape([\n",
            "        self._arm_context_sampling_fn()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\stationary_stochastic_per_arm_py_environment.py",
        "line_number": 146,
        "API": ".maximum(",
        "context": [
            "    self._observation = {GLOBAL_KEY: global_obs, PER_ARM_KEY: arm_obs}\n",
            "\n",
            "    if self._num_actions_fn:\n",
            "      num_actions = [self._num_actions_fn() for _ in range(self._batch_size)]\n",
            "      num_actions = np.maximum(num_actions, 1)\n",
            "      num_actions = np.minimum(num_actions, self._max_num_actions)\n",
            "      self._observation.update({NUM_ACTIONS_KEY: num_actions})\n",
            "    return self._observation\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\stationary_stochastic_per_arm_py_environment.py",
        "line_number": 157,
        "API": ".stack(",
        "context": [
            "      raise ValueError('Number of actions must match batch size.')\n",
            "    global_obs = self._observation[GLOBAL_KEY]  # pytype: disable=attribute-error  # trace-all-classes\n",
            "    batch_size_range = range(self.batch_size)\n",
            "    arm_obs = self._observation[PER_ARM_KEY][batch_size_range, action, :]  # pytype: disable=attribute-error  # trace-all-classes\n",
            "    reward = np.stack([\n",
            "        self._reward_fn(np.concatenate((global_obs[b, :], arm_obs[b, :])))\n",
            "        for b in batch_size_range\n",
            "    ])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\stationary_stochastic_py_environment.py",
        "line_number": 122,
        "API": ".stack(",
        "context": [
            "\n",
            "  def _apply_action(self, action: types.NestedArray) -> types.NestedArray:\n",
            "    if len(action) != self.batch_size:\n",
            "      raise ValueError('Number of actions must match batch size.')\n",
            "    reward = np.stack(\n",
            "        [self._reward_fns[a](o) for a, o in zip(action, self._observation)])  # pytype: disable=attribute-error  # trace-all-classes\n",
            "    if self._constraint_fns is not None:\n",
            "      constraint = np.stack(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\stationary_stochastic_structured_py_environment.py",
        "line_number": 104,
        "API": ".map_structure(",
        "context": [
            "    global_example = global_context_sampling_fn()\n",
            "    arm_example = arm_context_sampling_fn()\n",
            "    observation_spec = {\n",
            "        GLOBAL_KEY:\n",
            "            tf.nest.map_structure(array_spec.ArraySpec.from_array,\n",
            "                                  global_example),\n",
            "        PER_ARM_KEY:\n",
            "            array_spec.add_outer_dims_nest(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\stationary_stochastic_structured_py_environment.py",
        "line_number": 138,
        "API": ".map_structure(",
        "context": [
            "    global_obs = self._generate_batch_of_observations(\n",
            "        self._global_context_sampling_fn, self._batch_size)\n",
            "    arm_obs = self._generate_batch_of_observations(\n",
            "        self._arm_context_sampling_fn, self._batch_size * self._num_actions)\n",
            "    arm_obs = tf.nest.map_structure(\n",
            "        lambda x: x.reshape((self.batch_size, self._num_actions) + x.shape[1:]),\n",
            "        arm_obs)\n",
            "    self._observation = {GLOBAL_KEY: global_obs, PER_ARM_KEY: arm_obs}\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\stationary_stochastic_structured_py_environment.py",
        "line_number": 149,
        "API": ".map_structure(",
        "context": [
            "    if len(action) != self.batch_size:\n",
            "      raise ValueError('Number of actions must match batch size.')\n",
            "    global_obs = self._observation[GLOBAL_KEY]  # pytype: disable=attribute-error  # trace-all-classes\n",
            "    batch_size_range = list(range(self.batch_size))\n",
            "    arm_obs = tf.nest.map_structure(lambda x: x[batch_size_range, action, :],\n",
            "                                    self._observation[PER_ARM_KEY])  # pytype: disable=attribute-error  # trace-all-classes\n",
            "    def _get_element_from_batch(structure, index):\n",
            "      return tf.nest.map_structure(lambda x: x[index], structure)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\stationary_stochastic_structured_py_environment.py",
        "line_number": 154,
        "API": ".stack(",
        "context": [
            "                                    self._observation[PER_ARM_KEY])  # pytype: disable=attribute-error  # trace-all-classes\n",
            "    def _get_element_from_batch(structure, index):\n",
            "      return tf.nest.map_structure(lambda x: x[index], structure)\n",
            "\n",
            "    reward = np.stack([\n",
            "        self._reward_fn(\n",
            "            _get_element_from_batch(global_obs, b),\n",
            "            _get_element_from_batch(arm_obs, b)) for b in batch_size_range  # pytype: disable=wrong-arg-count  # trace-all-classes\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\wheel_py_environment.py",
        "line_number": 41,
        "API": ".zeros(",
        "context": [
            "@gin.configurable\n",
            "def compute_optimal_action(observation: np.ndarray,\n",
            "                           delta: float) -> types.Array:\n",
            "  batch_size = observation.shape[0]\n",
            "  optimal_actions = np.zeros(batch_size, dtype=np.int32)\n",
            "  is_outer = np.int32(np.linalg.norm(observation, ord=2, axis=1) > delta)\n",
            "  signs = np.sign(observation)\n",
            "  optimal_actions += is_outer * [_SIGNS_TO_OPT_ACTION[tuple(x)] for x in signs]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\wheel_py_environment.py",
        "line_number": 54,
        "API": ".norm(",
        "context": [
            "    observation: np.ndarray,\n",
            "    delta: float,\n",
            "    mu_inside: float,\n",
            "    mu_high: float) -> types.Array:\n",
            "  is_inside = np.float32(np.linalg.norm(observation, ord=2, axis=1) <= delta)\n",
            "  return is_inside * mu_inside + (1 - is_inside) * mu_high\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\wheel_py_environment.py",
        "line_number": 122,
        "API": ".format(",
        "context": [
            "\n",
            "    # The first action should have higher mean reward that the other actions.\n",
            "    if self._mu_base[0] != max(self._mu_base):\n",
            "      raise ValueError('The first action in mu_base should have the highest '\n",
            "                       'reward; got {}'.format(self._mu_base))\n",
            "\n",
            "    action_spec = array_spec.BoundedArraySpec(\n",
            "        shape=(),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\wheel_py_environment.py",
        "line_number": 133,
        "API": ".zeros(",
        "context": [
            "        name='action')\n",
            "    observation_spec = array_spec.ArraySpec(\n",
            "        shape=(_CONTEXT_DIM,), dtype=np.float32, name='observation')\n",
            "    self._time_step_spec = ts.time_step_spec(observation_spec)\n",
            "    self._observation = np.zeros((self._batch_size, _CONTEXT_DIM))\n",
            "    super(WheelPyEnvironment, self).__init__(\n",
            "        observation_spec, action_spec, name=name)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\wheel_py_environment.py",
        "line_number": 147,
        "API": ".normal(",
        "context": [
            "    return True\n",
            "\n",
            "  def _reward_fn(self, observation, action):\n",
            "    # Sample rewards for all actions.\n",
            "    r_all = np.random.normal(\n",
            "        self._mu_base, self._std_base, size=(self._batch_size, _NUM_ACTIONS))\n",
            "\n",
            "    # Compute the reward inside.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\wheel_py_environment.py",
        "line_number": 154,
        "API": ".arange(",
        "context": [
            "    # Compute the reward inside.\n",
            "    row_norms = np.linalg.norm(observation, ord=2, axis=1)\n",
            "    is_norm_below_delta = np.float32(row_norms <= self._delta)\n",
            "    reward_inside = (\n",
            "        is_norm_below_delta * r_all[np.arange(self._batch_size), action])\n",
            "\n",
            "    # Compute the reward outside.\n",
            "    high_reward = np.random.normal(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\wheel_py_environment.py",
        "line_number": 159,
        "API": ".sign(",
        "context": [
            "\n",
            "    # Compute the reward outside.\n",
            "    high_reward = np.random.normal(\n",
            "        self._mu_high, self._std_high, size=(self._batch_size))\n",
            "    signs = np.sign(observation)\n",
            "    optimal_actions = [_SIGNS_TO_OPT_ACTION[tuple(x)] for x in signs]\n",
            "    r_outside = r_all\n",
            "    r_outside[np.arange(self._batch_size), optimal_actions] = high_reward\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\wheel_py_environment.py",
        "line_number": 165,
        "API": ".arange(",
        "context": [
            "    r_outside = r_all\n",
            "    r_outside[np.arange(self._batch_size), optimal_actions] = high_reward\n",
            "\n",
            "    reward_outside = ((1.0 - is_norm_below_delta) *\n",
            "                      r_outside[np.arange(self._batch_size), action])\n",
            "\n",
            "    reward_final = reward_inside + reward_outside\n",
            "    return reward_final\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\environments\\wheel_py_environment.py",
        "line_number": 172,
        "API": ".uniform(",
        "context": [
            "    return reward_final\n",
            "\n",
            "  def _observe(self) -> types.NestedArray:\n",
            "    \"\"\"Returns 2-dim samples falling in the unit circle.\"\"\"\n",
            "    theta = np.random.uniform(0.0, 2.0 * np.pi, (self._batch_size))\n",
            "    r = np.sqrt(np.random.uniform(size=self._batch_size))\n",
            "    batched_observations = np.stack(\n",
            "        [r * np.cos(theta), r * np.sin(theta)], axis=1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\metrics\\tf_metrics.py",
        "line_number": 74,
        "API": ".assign(",
        "context": [
            "    trajectory_reward = trajectory.reward\n",
            "    if isinstance(trajectory.reward, dict):\n",
            "      trajectory_reward = trajectory.reward[bandit_spec_utils.REWARD_SPEC_KEY]\n",
            "    trajectory_regret = baseline_reward - trajectory_reward\n",
            "    self.regret.assign(tf.reduce_mean(trajectory_regret))\n",
            "    return trajectory\n",
            "\n",
            "  def result(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\metrics\\tf_metrics.py",
        "line_number": 114,
        "API": ".cast(",
        "context": [
            "    Returns:\n",
            "      The arguments, for easy chaining.\n",
            "    \"\"\"\n",
            "    baseline_action = self._baseline_action_fn(trajectory.observation)\n",
            "    disagreement = tf.cast(\n",
            "        tf.not_equal(baseline_action, trajectory.action), tf.float32)\n",
            "    self.suboptimal_arms.assign(tf.reduce_mean(disagreement))\n",
            "    return trajectory\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\metrics\\tf_metrics.py",
        "line_number": 120,
        "API": ".identity(",
        "context": [
            "    self.suboptimal_arms.assign(tf.reduce_mean(disagreement))\n",
            "    return trajectory\n",
            "\n",
            "  def result(self):\n",
            "    return tf.identity(\n",
            "        self.suboptimal_arms, name=self.name)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\metrics\\tf_metrics.py",
        "line_number": 163,
        "API": ".cast(",
        "context": [
            "    \"\"\"\n",
            "    feasibility_prob_all_actions = self._constraint(trajectory.observation)\n",
            "    feasibility_prob_selected_actions = common.index_with_actions(\n",
            "        feasibility_prob_all_actions,\n",
            "        tf.cast(trajectory.action, dtype=tf.int32))\n",
            "    self.constraint_violations.assign(tf.reduce_mean(\n",
            "        1.0 - feasibility_prob_selected_actions))\n",
            "    return trajectory\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\metrics\\tf_metrics.py",
        "line_number": 169,
        "API": ".identity(",
        "context": [
            "        1.0 - feasibility_prob_selected_actions))\n",
            "    return trajectory\n",
            "\n",
            "  def result(self):\n",
            "    return tf.identity(self.constraint_violations, name=self.name)\n",
            "\n",
            "\n",
            "@gin.configurable\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\metrics\\tf_metrics.py",
        "line_number": 212,
        "API": ".reduce_max(",
        "context": [
            "    Returns:\n",
            "      The arguments, for easy chaining.\n",
            "    \"\"\"\n",
            "    all_estimated_rewards = self._estimated_reward_fn(trajectory.observation)\n",
            "    max_estimated_rewards = tf.reduce_max(all_estimated_rewards, axis=-1)\n",
            "    estimated_action_rewards = tf.gather(\n",
            "        all_estimated_rewards, trajectory.action, batch_dims=1)\n",
            "    self.safe_explore.assign(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\metrics\\tf_metrics.py",
        "line_number": 220,
        "API": ".identity(",
        "context": [
            "        tf.reduce_mean(max_estimated_rewards - estimated_action_rewards))\n",
            "    return trajectory\n",
            "\n",
            "  def result(self):\n",
            "    return tf.identity(self.safe_explore, name=self.name)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\multi_objective\\multi_objective_scalarizer.py",
        "line_number": 53,
        "API": ".convert_to_tensor(",
        "context": [
            "      tensor or `Sequence`, and has shape that does not match the shape of\n",
            "      `multi_objectives`.\n",
            "  \"\"\"\n",
            "  for param_name, param_value in params.items():\n",
            "    param_shape = tf.convert_to_tensor(param_value).shape\n",
            "    if param_shape.rank != 1 and not multi_objectives.shape.is_compatible_with(\n",
            "        param_shape):\n",
            "      raise ValueError(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\multi_objective\\multi_objective_scalarizer.py",
        "line_number": 58,
        "API": ".format(",
        "context": [
            "    if param_shape.rank != 1 and not multi_objectives.shape.is_compatible_with(\n",
            "        param_shape):\n",
            "      raise ValueError(\n",
            "          'The shape of multi_objectives: {} does not match the shape of '\n",
            "          'scalarization parameter: {}, which is {}'.format(\n",
            "              multi_objectives.shape, param_name, param_shape))\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\multi_objective\\multi_objective_scalarizer.py",
        "line_number": 119,
        "API": ".format(",
        "context": [
            "    \"\"\"\n",
            "    if not isinstance(num_of_objectives, int):\n",
            "      raise ValueError(\n",
            "          'Scalarizer should be initialized with an integer representing the '\n",
            "          'number of objectives, but the type of the input is {}.'.format(\n",
            "              type(num_of_objectives)))\n",
            "    if num_of_objectives < 2:\n",
            "      raise ValueError(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\multi_objective\\multi_objective_scalarizer.py",
        "line_number": 124,
        "API": ".format(",
        "context": [
            "              type(num_of_objectives)))\n",
            "    if num_of_objectives < 2:\n",
            "      raise ValueError(\n",
            "          'Scalarizer should be used with at least two objectives, but only {}'\n",
            "          ' are given.'.format(num_of_objectives))\n",
            "    self._num_of_objectives = num_of_objectives\n",
            "\n",
            "  def __call__(self, multi_objectives: tf.Tensor) -> tf.Tensor:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\multi_objective\\multi_objective_scalarizer.py",
        "line_number": 141,
        "API": ".format(",
        "context": [
            "      ValueError: if\n",
            "        `multi_objectives.shape.dims[1] != self._num_of_objectives`.\n",
            "    \"\"\"\n",
            "    if multi_objectives.shape.rank != 2:\n",
            "      raise ValueError('The rank of the input should be 2, but is {}'.format(\n",
            "          multi_objectives.shape.rank))\n",
            "    if multi_objectives.shape.dims[1] != self._num_of_objectives:\n",
            "      raise ValueError(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\multi_objective\\multi_objective_scalarizer.py",
        "line_number": 166,
        "API": ".format(",
        "context": [
            "    for param_name, param in params.items():\n",
            "      if param.shape.rank != 2:\n",
            "        raise ValueError(\n",
            "            'Scalarization parameter: {} should be a rank-2 tensor with shape '\n",
            "            '[batch_size, num_of_objectives], but found to be: {}'.format(\n",
            "                param_name, param))\n",
            "      elif param.shape.dims[-1] != self._num_of_objectives:\n",
            "        raise ValueError(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\multi_objective\\multi_objective_scalarizer.py",
        "line_number": 171,
        "API": ".format(",
        "context": [
            "                param_name, param))\n",
            "      elif param.shape.dims[-1] != self._num_of_objectives:\n",
            "        raise ValueError(\n",
            "            'The number of objectives in scalarization parameter: {} should '\n",
            "            'be {}, but found to be {}.'.format(param_name,\n",
            "                                                self._num_of_objectives,\n",
            "                                                param.shape.dims[-1]))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\multi_objective\\multi_objective_scalarizer.py",
        "line_number": 223,
        "API": ".reduce_sum(",
        "context": [
            "\n",
            "  def _scalarize(self, transformed_multi_objectives: tf.Tensor) -> tf.Tensor:\n",
            "    _validate_scalarization_parameter_shape(transformed_multi_objectives,\n",
            "                                            {'weights': self._weights})\n",
            "    return tf.reduce_sum(transformed_multi_objectives * self._weights, axis=1)\n",
            "\n",
            "  def set_parameters(self, weights: tf.Tensor):  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks\n",
            "    \"\"\"Set the scalarization parameter of the LinearScalarizer.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\multi_objective\\multi_objective_scalarizer.py",
        "line_number": 267,
        "API": ".format(",
        "context": [
            "      ValueError: if `len(weights) != len(reference_point)`.\n",
            "    \"\"\"\n",
            "    if len(weights) != len(reference_point):\n",
            "      raise ValueError(\n",
            "          'weights has {} elements but reference_point has {}.'.format(\n",
            "              len(weights), len(reference_point)))\n",
            "    self._weights = copy.deepcopy(weights)\n",
            "    self._reference_point = reference_point\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\multi_objective\\multi_objective_scalarizer.py",
        "line_number": 278,
        "API": ".reduce_min(",
        "context": [
            "    _validate_scalarization_parameter_shape(transformed_multi_objectives, {\n",
            "        'weights': self._weights,\n",
            "        'reference_point': self._reference_point\n",
            "    })\n",
            "    return tf.reduce_min(\n",
            "        (transformed_multi_objectives - self._reference_point) * self._weights,\n",
            "        axis=-1)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\multi_objective\\multi_objective_scalarizer.py",
        "line_number": 355,
        "API": ".join(",
        "context": [
            "    \"\"\"\n",
            "    if any([x < 0 for x in direction]):\n",
            "      raise ValueError(\n",
            "          'direction should be in the positive orthant, but has negative '\n",
            "          'coordinates: [{}].'.format(', '.join(map(str, direction))))\n",
            "    length = np.sqrt(sum([x * x for x in direction]))\n",
            "    if length < self.ALMOST_ZERO:\n",
            "      raise ValueError(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\multi_objective\\multi_objective_scalarizer.py",
        "line_number": 363,
        "API": ".format(",
        "context": [
            "          'direction found to be a nearly-zero vector, but should not be.')\n",
            "    self._direction = [x / length for x in direction]\n",
            "    if len(transform_params) != len(self._direction):\n",
            "      raise ValueError(\n",
            "          'direction has {} elements but transform_params has {}.'.format(\n",
            "              len(direction), len(transform_params)))\n",
            "    self._slopes, self._offsets = zip(\n",
            "        *[(p.slope, p.offset) for p in transform_params])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\multi_objective\\multi_objective_scalarizer.py",
        "line_number": 390,
        "API": ".maximum(",
        "context": [
            "        })\n",
            "    return self._transformer(multi_objectives, self._slopes, self._offsets)\n",
            "\n",
            "  def _scalarize(self, transformed_multi_objectives: tf.Tensor) -> tf.Tensor:\n",
            "    transformed_multi_objectives = tf.maximum(transformed_multi_objectives, 0)\n",
            "    nonzero_mask = tf.broadcast_to(\n",
            "        tf.cast(tf.abs(self._direction) >= self.ALMOST_ZERO, dtype=tf.bool),\n",
            "        tf.shape(transformed_multi_objectives))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\multi_objective\\multi_objective_scalarizer.py",
        "line_number": 395,
        "API": ".where(",
        "context": [
            "    nonzero_mask = tf.broadcast_to(\n",
            "        tf.cast(tf.abs(self._direction) >= self.ALMOST_ZERO, dtype=tf.bool),\n",
            "        tf.shape(transformed_multi_objectives))\n",
            "    return tf.reduce_min(\n",
            "        tf.where(nonzero_mask, transformed_multi_objectives / self._direction,\n",
            "                 transformed_multi_objectives.dtype.max),\n",
            "        axis=1)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\multi_objective\\multi_objective_scalarizer.py",
        "line_number": 432,
        "API": ".format(",
        "context": [
            "        self._offsets = param\n",
            "      else:\n",
            "        raise ValueError(\n",
            "            'All transform_params keys should be {} or {}, but one key is not:'\n",
            "            ' {}'.format(self.SLOPE_KEY, self.OFFSET_KEY, key))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\networks\\global_and_arm_feature_network.py",
        "line_number": 228,
        "API": ".shape(",
        "context": [
            "    arm_output, arm_state = self._arm_network(\n",
            "        arm_obs, step_type=step_type, network_state=network_state)\n",
            "\n",
            "    # Reshape arm output to rank 3 tensor.\n",
            "    arm_output_shape = tf.shape(arm_output)\n",
            "    batch_size = arm_output_shape[0]\n",
            "    inner_dim = 1\n",
            "    outer_dim = arm_output_shape[-1]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\networks\\global_and_arm_feature_network.py",
        "line_number": 238,
        "API": ".prod(",
        "context": [
            "      inner_dims = arm_output.shape[1:-1]\n",
            "      if any(d is None for d in inner_dims):\n",
            "        raise ValueError('inner dimensions of arm output cannot be unknown; '\n",
            "                         f'arm_output.shape: {arm_output.shape}')\n",
            "      inner_dim = np.prod(inner_dims)\n",
            "    arm_output = tf.reshape(\n",
            "        arm_output, shape=[batch_size, inner_dim, outer_dim])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\networks\\global_and_arm_feature_network.py",
        "line_number": 245,
        "API": ".shape(",
        "context": [
            "\n",
            "    global_output, global_state = self._global_network(\n",
            "        global_obs, step_type=step_type, network_state=network_state)\n",
            "\n",
            "    num_actions = tf.shape(arm_output)[1]\n",
            "    global_output = tf.tile(\n",
            "        tf.expand_dims(global_output, axis=1), [1, num_actions, 1])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\networks\\global_and_arm_feature_network.py",
        "line_number": 254,
        "API": ".squeeze(",
        "context": [
            "\n",
            "    output, state = self._common_network(common_input,\n",
            "                                         (global_state, arm_state))\n",
            "    if isinstance(self._common_network, q_network.QNetwork):\n",
            "      output = tf.squeeze(output, axis=-1)\n",
            "    return output, state\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\networks\\heteroscedastic_q_network.py",
        "line_number": 104,
        "API": ".flatten(",
        "context": [
            "      ValueError: If `input_tensor_spec` contains more than one observation. Or\n",
            "        if `action_spec` contains more than one action.\n",
            "    \"\"\"\n",
            "    q_network.validate_specs(action_spec, input_tensor_spec)\n",
            "    action_spec = tf.nest.flatten(action_spec)[0]\n",
            "    num_actions = action_spec.maximum - action_spec.minimum + 1\n",
            "    encoder_input_tensor_spec = input_tensor_spec\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\networks\\heteroscedastic_q_network.py",
        "line_number": 160,
        "API": ".clip_by_value(",
        "context": [
            "    \"\"\"\n",
            "    state, network_state = self._encoder(\n",
            "        observation, step_type=step_type, network_state=network_state)\n",
            "\n",
            "    log_variance = tf.clip_by_value(\n",
            "        self._log_variance_layer(state), tf.math.log(self._min_variance),\n",
            "        tf.math.log(self._max_variance))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\bernoulli_thompson_sampling_policy.py",
        "line_number": 76,
        "API": ".flatten(",
        "context": [
            "    Raises:\n",
            "      NotImplementedError: If `action_spec` contains more than one\n",
            "        `BoundedTensorSpec` or the `BoundedTensorSpec` is not valid.\n",
            "    \"\"\"\n",
            "    flat_action_spec = tf.nest.flatten(action_spec)\n",
            "    if len(flat_action_spec) > 1:\n",
            "      raise NotImplementedError(\n",
            "          'action_spec can only contain a single BoundedTensorSpec.')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\bernoulli_thompson_sampling_policy.py",
        "line_number": 88,
        "API": ".format(",
        "context": [
            "        action_spec.shape.rank > 1 or\n",
            "        action_spec.shape.num_elements() != 1):\n",
            "      raise NotImplementedError(\n",
            "          'action_spec must be a BoundedTensorSpec of integer type and '\n",
            "          'shape (). Found {}.'.format(action_spec))\n",
            "    self._expected_num_actions = action_spec.maximum - action_spec.minimum + 1\n",
            "\n",
            "    if len(alpha) != self._expected_num_actions:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\bernoulli_thompson_sampling_policy.py",
        "line_number": 94,
        "API": ".format(",
        "context": [
            "\n",
            "    if len(alpha) != self._expected_num_actions:\n",
            "      raise ValueError(\n",
            "          'The size of alpha parameters is expected to be equal '\n",
            "          'to the number of actions, but found to be {}'.format(len(alpha)))\n",
            "    self._alpha = alpha\n",
            "    if len(alpha) != len(beta):\n",
            "      raise ValueError(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\bernoulli_thompson_sampling_policy.py",
        "line_number": 129,
        "API": ".is_tensor(",
        "context": [
            "    return self._alpha + self._beta  # pytype: disable=unsupported-operands  # trace-all-classes\n",
            "\n",
            "  def _distribution(self, time_step, policy_state):\n",
            "    if time_step.step_type.shape:\n",
            "      if tf.is_tensor(\n",
            "          time_step.step_type) and time_step.step_type.shape.rank > 0:\n",
            "        batch_size = time_step.step_type.get_shape().as_list()[0]\n",
            "      else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\bernoulli_thompson_sampling_policy.py",
        "line_number": 139,
        "API": ".stack(",
        "context": [
            "      batch_size = 1\n",
            "    # Sample from the posterior distribution.\n",
            "    posterior_dist = tfd.Beta(self._alpha, self._beta)\n",
            "    predicted_reward_sampled = posterior_dist.sample([batch_size])\n",
            "    predicted_reward_means_1d = tf.stack([\n",
            "        self._alpha[k] / (self._alpha[k] + self._beta[k]) for k in range(\n",
            "            self._expected_num_actions)], axis=-1)\n",
            "    predicted_reward_means = tf.stack([\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\bernoulli_thompson_sampling_policy.py",
        "line_number": 155,
        "API": ".argmax(",
        "context": [
            "    if mask is not None:\n",
            "      actions = policy_utilities.masked_argmax(\n",
            "          predicted_reward_sampled, mask, output_type=self.action_spec.dtype)\n",
            "    else:\n",
            "      actions = tf.argmax(\n",
            "          predicted_reward_sampled, axis=-1, output_type=self.action_spec.dtype)\n",
            "\n",
            "    policy_info = policy_utilities.populate_policy_info(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\bernoulli_thompson_sampling_policy.py",
        "line_number": 160,
        "API": ".cast(",
        "context": [
            "          predicted_reward_sampled, axis=-1, output_type=self.action_spec.dtype)\n",
            "\n",
            "    policy_info = policy_utilities.populate_policy_info(\n",
            "        arm_observations=(), chosen_actions=actions,\n",
            "        rewards_for_argmax=tf.cast(predicted_reward_sampled, tf.float32),\n",
            "        est_rewards=tf.cast(predicted_reward_means, tf.float32),\n",
            "        emit_policy_info=self._emit_policy_info,\n",
            "        accepts_per_arm_features=False)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\bernoulli_thompson_sampling_policy.py",
        "line_number": 166,
        "API": ".zeros(",
        "context": [
            "        emit_policy_info=self._emit_policy_info,\n",
            "        accepts_per_arm_features=False)\n",
            "    if policy_utilities.InfoFields.LOG_PROBABILITY in self._emit_policy_info:\n",
            "      policy_info._replace(\n",
            "          log_probability=tf.zeros([batch_size], tf.float32))\n",
            "\n",
            "    return policy_step.PolicyStep(\n",
            "        tfp.distributions.Deterministic(loc=actions), policy_state, policy_info)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\boltzmann_reward_prediction_policy.py",
        "line_number": 125,
        "API": ".maximum(",
        "context": [
            "            ' does not match the expected number of actions:',\n",
            "            self._expected_num_actions)\n",
            "\n",
            "  def _get_temperature_value(self):\n",
            "    return tf.math.maximum(\n",
            "        _MIN_TEMPERATURE,\n",
            "        self._temperature()\n",
            "        if callable(self._temperature) else self._temperature)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\boltzmann_reward_prediction_policy.py",
        "line_number": 131,
        "API": ".shape(",
        "context": [
            "        self._temperature()\n",
            "        if callable(self._temperature) else self._temperature)\n",
            "\n",
            "  def _action_distribution(self, mask, predicted_rewards):\n",
            "    batch_size = tf.shape(predicted_rewards)[0]\n",
            "    if self._boltzmann_gumbel_exploration_constant is not None:\n",
            "      logits = predicted_rewards\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\boltzmann_reward_prediction_policy.py",
        "line_number": 138,
        "API": ".constant(",
        "context": [
            "\n",
            "      # Apply masking if needed. Overwrite the logits for invalid actions to\n",
            "      # logits.dtype.min.\n",
            "      if mask is not None:\n",
            "        almost_neg_inf = tf.constant(logits.dtype.min, dtype=logits.dtype)\n",
            "        logits = tf.compat.v2.where(\n",
            "            tf.cast(mask, tf.bool), logits, almost_neg_inf)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\boltzmann_reward_prediction_policy.py",
        "line_number": 143,
        "API": ".shape(",
        "context": [
            "        logits = tf.compat.v2.where(\n",
            "            tf.cast(mask, tf.bool), logits, almost_neg_inf)\n",
            "\n",
            "      gumbel_dist = tfp.distributions.Gumbel(loc=0., scale=1.)\n",
            "      gumbel_samples = gumbel_dist.sample(tf.shape(logits))\n",
            "      num_samples_list_float = tf.stack(\n",
            "          [tf.cast(x.read_value(), tf.float32) for x in self._num_samples_list],\n",
            "          axis=-1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\boltzmann_reward_prediction_policy.py",
        "line_number": 148,
        "API": ".cast(",
        "context": [
            "      num_samples_list_float = tf.stack(\n",
            "          [tf.cast(x.read_value(), tf.float32) for x in self._num_samples_list],\n",
            "          axis=-1)\n",
            "      exploration_weights = tf.math.divide_no_nan(\n",
            "          tf.cast(self._boltzmann_gumbel_exploration_constant, tf.float32),\n",
            "          tf.sqrt(num_samples_list_float))\n",
            "      final_logits = logits + exploration_weights * gumbel_samples\n",
            "      actions = tf.cast(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\boltzmann_reward_prediction_policy.py",
        "line_number": 179,
        "API": ".fill(",
        "context": [
            "        distribution = tfp.distributions.Categorical(\n",
            "            logits=logits,\n",
            "            dtype=self._action_spec.dtype)\n",
            "\n",
            "    bandit_policy_values = tf.fill([batch_size, 1],\n",
            "                                   policy_utilities.BanditPolicyType.BOLTZMANN)\n",
            "    return distribution, bandit_policy_values\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\categorical_policy.py",
        "line_number": 37,
        "API": ".format(",
        "context": [
            "\n",
            "def _validate_weights(weights: types.Tensor):\n",
            "  if len(weights.shape) != 1:\n",
            "    raise ValueError(\n",
            "        'Expected a 1D `Tensor` of weights; got {}.'.format(weights))\n",
            "\n",
            "\n",
            "class CategoricalPolicy(tf_policy.TFPolicy):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\categorical_policy.py",
        "line_number": 93,
        "API": ".format(",
        "context": [
            "    self._inverse_temperature = inverse_temperature\n",
            "    if action_spec.maximum + 1 != tf.compat.dimension_value(weights.shape[0]):\n",
            "      raise ValueError(\n",
            "          'Number of actions ({}) does not match weights dimension ({}).'\n",
            "          .format(action_spec.maximum + 1,\n",
            "                  tf.compat.dimension_value(weights.shape[0])))\n",
            "    super(CategoricalPolicy, self).__init__(\n",
            "        time_step_spec=time_step_spec,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\categorical_policy.py",
        "line_number": 128,
        "API": ".flatten(",
        "context": [
            "        self._inverse_temperature *\n",
            "        common.replicate(self._weights, outer_shape))\n",
            "    action_distribution = tfd.Independent(\n",
            "        tfd.Categorical(\n",
            "            logits=logits, dtype=tf.nest.flatten(self.action_spec)[0].dtype))\n",
            "    return policy_step.PolicyStep(action_distribution, policy_state)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\constraints.py",
        "line_number": 62,
        "API": ".format(",
        "context": [
            "    super(BaseConstraint, self).__init__(name=name)\n",
            "    if not isinstance(time_step_spec, ts.TimeStep):\n",
            "      raise ValueError(\n",
            "          'The `time_step_spec` must be an instance of `TimeStep`, but is `{}`.'\n",
            "          .format(type(time_step_spec)))\n",
            "\n",
            "    self._time_step_spec = time_step_spec\n",
            "    self._action_spec = action_spec\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\constraints.py",
        "line_number": 171,
        "API": ".cast(",
        "context": [
            "      predicted_values, _ = self._constraint_network(\n",
            "          observations, training=training)\n",
            "      action_predicted_values = common.index_with_actions(\n",
            "          predicted_values,\n",
            "          tf.cast(actions, dtype=tf.int32))\n",
            "      # Reduction is done outside of the loss function because non-scalar\n",
            "      # weights with unknown shapes may trigger shape validation that fails\n",
            "      # XLA compilation.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\constraints.py",
        "line_number": 176,
        "API": ".multiply(",
        "context": [
            "      # Reduction is done outside of the loss function because non-scalar\n",
            "      # weights with unknown shapes may trigger shape validation that fails\n",
            "      # XLA compilation.\n",
            "      return tf.reduce_mean(\n",
            "          tf.multiply(\n",
            "              self._error_loss_fn(\n",
            "                  rewards,\n",
            "                  action_predicted_values,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\constraints.py",
        "line_number": 185,
        "API": ".reshape(",
        "context": [
            "              sample_weights))\n",
            "\n",
            "  def _reshape_and_broadcast(self, input_tensor: types.Tensor,\n",
            "                             to_shape: types.Tensor) -> types.Tensor:\n",
            "    input_tensor = tf.reshape(input_tensor, [-1, 1])\n",
            "    return tf.broadcast_to(input_tensor, to_shape)\n",
            "\n",
            "  # Subclasses can override this function.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\constraints.py",
        "line_number": 193,
        "API": ".concat(",
        "context": [
            "  def __call__(self, observation, actions=None):\n",
            "    \"\"\"Returns the probability of input actions being feasible.\"\"\"\n",
            "    batch_dims = nest_utils.get_outer_shape(\n",
            "        observation, self._time_step_spec.observation)\n",
            "    shape = tf.concat([batch_dims, tf.constant(\n",
            "        self._num_actions, shape=[1], dtype=batch_dims.dtype)], axis=-1)\n",
            "    return tf.ones(shape)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\constraints.py",
        "line_number": 263,
        "API": ".zeros(",
        "context": [
            "    if self._baseline_action_fn is not None:\n",
            "      baseline_action = self._baseline_action_fn(observation)\n",
            "      baseline_action.shape.assert_is_compatible_with(batch_dims)\n",
            "    else:\n",
            "      baseline_action = tf.zeros(batch_dims, dtype=tf.int32)\n",
            "\n",
            "    predicted_values_for_baseline_actions = common.index_with_actions(\n",
            "        predicted_values,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\constraints.py",
        "line_number": 269,
        "API": ".shape(",
        "context": [
            "    predicted_values_for_baseline_actions = common.index_with_actions(\n",
            "        predicted_values,\n",
            "        tf.cast(baseline_action, dtype=tf.int32))\n",
            "    predicted_values_for_baseline_actions = self._reshape_and_broadcast(\n",
            "        predicted_values_for_baseline_actions, tf.shape(predicted_values))\n",
            "    is_satisfied = self._comparator_fn(\n",
            "        predicted_values,\n",
            "        (1 - self._margin) * predicted_values_for_baseline_actions)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\constraints.py",
        "line_number": 330,
        "API": ".cast(",
        "context": [
            "    predicted_values, _ = self._constraint_network(\n",
            "        observation, training=False)\n",
            "    is_satisfied = self._comparator_fn(\n",
            "        predicted_values, self._absolute_value)\n",
            "    return tf.cast(is_satisfied, tf.float32)\n",
            "\n",
            "\n",
            "class QuantileConstraint(NeuralConstraint):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\constraints.py",
        "line_number": 389,
        "API": ".cast(",
        "context": [
            "    predicted_quantiles, _ = self._constraint_network(\n",
            "        observation, training=False)\n",
            "    is_satisfied = self._comparator_fn(\n",
            "        predicted_quantiles, self._quantile_value)\n",
            "    return tf.cast(is_satisfied, tf.float32)\n",
            "\n",
            "\n",
            "class RelativeQuantileConstraint(NeuralConstraint):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\constraints.py",
        "line_number": 453,
        "API": ".zeros(",
        "context": [
            "    if self._baseline_action_fn is not None:\n",
            "      baseline_action = self._baseline_action_fn(observation)\n",
            "      baseline_action.shape.assert_is_compatible_with(batch_dims)\n",
            "    else:\n",
            "      baseline_action = tf.zeros(batch_dims, dtype=tf.int32)\n",
            "\n",
            "    predicted_quantiles_for_baseline_actions = common.index_with_actions(\n",
            "        predicted_quantiles,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\constraints.py",
        "line_number": 459,
        "API": ".shape(",
        "context": [
            "    predicted_quantiles_for_baseline_actions = common.index_with_actions(\n",
            "        predicted_quantiles,\n",
            "        tf.cast(baseline_action, dtype=tf.int32))\n",
            "    predicted_quantiles_for_baseline_actions = self._reshape_and_broadcast(\n",
            "        predicted_quantiles_for_baseline_actions, tf.shape(predicted_quantiles))\n",
            "    is_satisfied = self._comparator_fn(\n",
            "        predicted_quantiles, predicted_quantiles_for_baseline_actions)\n",
            "    return tf.cast(is_satisfied, tf.float32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\constraints.py",
        "line_number": 512,
        "API": ".constant(",
        "context": [
            "                   rewards: types.Tensor,\n",
            "                   weights: Optional[types.TensorOrArray] = None,\n",
            "                   training: bool = False) -> types.Tensor:\n",
            "    with tf.name_scope('constraint_loss'):\n",
            "      return tf.constant(0.0)\n",
            "\n",
            "  # Subclasses must implement these methods.\n",
            "  @abc.abstractmethod\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\constraints.py",
        "line_number": 527,
        "API": ".ones(",
        "context": [
            "    batch_size: types.Int,\n",
            "    num_actions: int,\n",
            "    action_mask: Optional[types.Tensor] = None) -> types.Float:\n",
            "  \"\"\"Helper function to compute the action feasibility probability.\"\"\"\n",
            "  feasibility_prob = tf.ones([batch_size, num_actions])\n",
            "  if action_mask is not None:\n",
            "    feasibility_prob = tf.cast(action_mask, tf.float32)\n",
            "  for c in constraints:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\constraints.py",
        "line_number": 569,
        "API": ".sequence_mask(",
        "context": [
            "    observation, mask = observation_and_action_constraint_splitter(observation)\n",
            "  elif (isinstance(observation, dict) and\n",
            "        bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY in observation):\n",
            "    number_of_actions = observation[bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY]\n",
            "    mask = tf.sequence_mask(\n",
            "        lengths=number_of_actions, maxlen=max_num_actions, dtype=tf.int32)\n",
            "\n",
            "  first_observation = tf.nest.flatten(observation)[0]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 116,
        "API": ".greater_equal(",
        "context": [
            "    )\n",
            "\n",
            "  # A [batch_size, d] bool tensor indicating which elements of\n",
            "  # `greedy_action_prob` satisfy the `max_exploration_prob` constraint.\n",
            "  valid_gamma_mask = tf.greater_equal(\n",
            "      greedy_action_prob, 1.0 - max_exploration_prob\n",
            "  )\n",
            "  # A [batch_size] bool tensor indicating the batch members that have at least\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 121,
        "API": ".greater(",
        "context": [
            "      greedy_action_prob, 1.0 - max_exploration_prob\n",
            "  )\n",
            "  # A [batch_size] bool tensor indicating the batch members that have at least\n",
            "  # one valid entry in `greedy_action_prob`.\n",
            "  feasible = tf.greater(\n",
            "      tf.reduce_sum(tf.cast(valid_gamma_mask, tf.float32), axis=1), 0.0\n",
            "  )\n",
            "  # We mask the probability entries corresponding to invalid gamma values by 2.0\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 127,
        "API": ".where(",
        "context": [
            "  )\n",
            "  # We mask the probability entries corresponding to invalid gamma values by 2.0\n",
            "  # so that they will not be selected as minimizers. See further details in the\n",
            "  # comment below.\n",
            "  greedy_action_prob_masked = tf.where(\n",
            "      valid_gamma_mask,\n",
            "      greedy_action_prob,\n",
            "      2.0 * tf.ones_like(greedy_action_prob),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 138,
        "API": ".where(",
        "context": [
            "  # greedy action probability) subject to the constraint via masking.\n",
            "  # For batch members where the `max_exploration_prob` constraint is infeasible,\n",
            "  # we simply minimize the exploration probability (or equivalently, maximize\n",
            "  # the greedy action probability).\n",
            "  gamma_indices = tf.where(\n",
            "      feasible,\n",
            "      tf.argmin(greedy_action_prob_masked, axis=1),\n",
            "      tf.argmax(greedy_action_prob, axis=1),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 143,
        "API": ".expand_dims(",
        "context": [
            "      feasible,\n",
            "      tf.argmin(greedy_action_prob_masked, axis=1),\n",
            "      tf.argmax(greedy_action_prob, axis=1),\n",
            "  )\n",
            "  gamma_indices = tf.expand_dims(gamma_indices, axis=-1)\n",
            "  greedy_action_prob = tf.gather(\n",
            "      greedy_action_prob, gamma_indices, axis=1, batch_dims=1\n",
            "  )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 148,
        "API": ".gather(",
        "context": [
            "  greedy_action_prob = tf.gather(\n",
            "      greedy_action_prob, gamma_indices, axis=1, batch_dims=1\n",
            "  )\n",
            "  num_actions = tf.shape(other_actions_probs)[1]\n",
            "  other_actions_probs = tf.gather(\n",
            "      other_actions_probs,\n",
            "      tf.tile(gamma_indices, [1, num_actions]),\n",
            "      axis=2,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 265,
        "API": ".cast(",
        "context": [
            "\n",
            "  def _get_exploitation_coefficient(self) -> types.FloatOrReturningFloat:\n",
            "    coef = self._exploitation_coefficient() if callable(\n",
            "        self._exploitation_coefficient) else self._exploitation_coefficient\n",
            "    coef = tf.cast(coef, dtype=tf.float32)\n",
            "    return tf.maximum(coef, 0.0)\n",
            "\n",
            "  @property\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 288,
        "API": ".cast(",
        "context": [
            "    Returns:\n",
            "      The number of allowed actions. It can be either a scalar (when `mask` is\n",
            "      None), or a tensor shaped as [batch_size].\n",
            "    \"\"\"\n",
            "    return (tf.cast(self._expected_num_actions, dtype=tf.float32)\n",
            "            if mask is None else tf.reduce_sum(\n",
            "                tf.cast(tf.cast(mask, tf.bool), tf.float32), axis=1))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 316,
        "API": ".maximum(",
        "context": [
            "      a 1-D grid of `exploitation_coefficient` in log2 scale between 0 and\n",
            "      `_MAX_LOG2_EXPLOITATION_COEF` inclusively, and `d` corresponds to the grid\n",
            "      size.\n",
            "    \"\"\"\n",
            "    num_samples_list_float = tf.maximum(\n",
            "        [tf.cast(x.read_value(), tf.float32) for x in self.num_samples_list],\n",
            "        0.0)\n",
            "    num_trainable_elements_float = tf.cast(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 325,
        "API": ".pow(",
        "context": [
            "    num_allowed_actions = self._get_number_of_allowed_actions(mask)\n",
            "    exploitation_coefficient = (\n",
            "        self._get_exploitation_coefficient()\n",
            "        if self._max_exploration_probability_hint is None\n",
            "        else tf.pow(2.0, range(_MAX_LOG2_EXPLOITATION_COEF + 1))\n",
            "    )\n",
            "    gamma = tf.sqrt(\n",
            "        num_allowed_actions\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 332,
        "API": ".minimum(",
        "context": [
            "        num_allowed_actions\n",
            "        * tf.reduce_sum(num_samples_list_float)\n",
            "        / num_trainable_elements_float\n",
            "    )\n",
            "    return tf.minimum(\n",
            "        _MAX_GAMMA,\n",
            "        tf.reshape(gamma, [-1, 1])\n",
            "        * tf.ones(shape=[batch_size, 1], dtype=dtype)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 340,
        "API": ".shape(",
        "context": [
            "        * tf.reshape(exploitation_coefficient, [1, -1]),\n",
            "    )\n",
            "\n",
            "  def _action_distribution(self, mask, predicted_rewards):\n",
            "    batch_size = tf.shape(predicted_rewards)[0]\n",
            "    gamma = self._compute_gamma(mask, predicted_rewards.dtype, batch_size)\n",
            "    # Replace predicted rewards of masked actions with -inf.\n",
            "    predictions = predicted_rewards if mask is None else tf.where(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 345,
        "API": ".ones_like(",
        "context": [
            "    gamma = self._compute_gamma(mask, predicted_rewards.dtype, batch_size)\n",
            "    # Replace predicted rewards of masked actions with -inf.\n",
            "    predictions = predicted_rewards if mask is None else tf.where(\n",
            "        tf.cast(mask, tf.bool), predicted_rewards, -float('Inf') *\n",
            "        tf.ones_like(predicted_rewards))\n",
            "\n",
            "    # Get the predicted rewards of the greedy actions.\n",
            "    greedy_action_predictions = tf.reshape(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 355,
        "API": ".reshape(",
        "context": [
            "    # `other_actions_probs` is a tensor shaped as [batch_size, num_actions, d]\n",
            "    # that contains valid sampling probabilities for all non-greedy actions.\n",
            "    # The last dimension corresponds to different gamma parameters.\n",
            "    if mask is not None:\n",
            "      num_allowed_actions = tf.reshape(\n",
            "          self._get_number_of_allowed_actions(mask), [batch_size, 1, 1]\n",
            "      )\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 364,
        "API": ".matmul(",
        "context": [
            "    prediction_delta = greedy_action_predictions - predictions\n",
            "    other_actions_probs = tf.math.divide_no_nan(\n",
            "        1.0,\n",
            "        num_allowed_actions\n",
            "        + tf.matmul(\n",
            "            tf.expand_dims(prediction_delta, axis=-1),\n",
            "            tf.expand_dims(gamma, axis=1),\n",
            "        ),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 372,
        "API": ".where(",
        "context": [
            "    )\n",
            "    # Although `predictions` has accounted for the action mask, we still need\n",
            "    # to mask the action probabilities in the case of zero gamma.\n",
            "    if mask is not None:\n",
            "      other_actions_probs = tf.where(\n",
            "          tf.repeat(\n",
            "              input=tf.expand_dims(tf.cast(mask, tf.bool), axis=-1),\n",
            "              repeats=[tf.shape(other_actions_probs)[-1]],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 379,
        "API": ".zeros_like(",
        "context": [
            "              repeats=[tf.shape(other_actions_probs)[-1]],\n",
            "              axis=2,\n",
            "          ),\n",
            "          other_actions_probs,\n",
            "          tf.zeros_like(other_actions_probs),\n",
            "      )\n",
            "\n",
            "    # Get the greedy action.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 384,
        "API": ".argmax(",
        "context": [
            "      )\n",
            "\n",
            "    # Get the greedy action.\n",
            "    greedy_actions = tf.reshape(\n",
            "        tf.argmax(predictions, axis=-1, output_type=self.action_spec.dtype),\n",
            "        [-1, 1])\n",
            "\n",
            "    # Compute the probabilities of sampling the greedy actions, which is\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 392,
        "API": ".reduce_sum(",
        "context": [
            "    # 1 - (the total probability of sampling other actions),\n",
            "    # shaped [batch_size, d].\n",
            "    greedy_action_prob = (\n",
            "        1.0\n",
            "        - tf.reduce_sum(other_actions_probs, axis=1)\n",
            "        + tf.squeeze(\n",
            "            tf.gather(\n",
            "                other_actions_probs, greedy_actions, axis=1, batch_dims=1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 402,
        "API": ".clip_by_value(",
        "context": [
            "        )\n",
            "    )\n",
            "\n",
            "    if self._max_exploration_probability_hint is not None:\n",
            "      max_exploration_prob = tf.clip_by_value(\n",
            "          self._max_exploration_probability_hint,\n",
            "          clip_value_min=0.0,\n",
            "          clip_value_max=1.0,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 411,
        "API": ".squeeze(",
        "context": [
            "      greedy_action_prob, other_actions_probs = _find_action_probabilities(\n",
            "          greedy_action_prob, other_actions_probs, max_exploration_prob\n",
            "      )\n",
            "    else:\n",
            "      other_actions_probs = tf.squeeze(other_actions_probs, axis=2)\n",
            "\n",
            "    # Compute the sampling probabilities for all actions by combining\n",
            "    # `greedy_action_prob` and `other_actions_probs`.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 416,
        "API": ".tile(",
        "context": [
            "\n",
            "    # Compute the sampling probabilities for all actions by combining\n",
            "    # `greedy_action_prob` and `other_actions_probs`.\n",
            "    greedy_action_mask = tf.equal(\n",
            "        tf.tile([\n",
            "            tf.range(self._expected_num_actions, dtype=self.action_spec.dtype)\n",
            "        ], [batch_size, 1]), greedy_actions)\n",
            "    action_probs = tf.where(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 421,
        "API": ".tile(",
        "context": [
            "            tf.range(self._expected_num_actions, dtype=self.action_spec.dtype)\n",
            "        ], [batch_size, 1]), greedy_actions)\n",
            "    action_probs = tf.where(\n",
            "        greedy_action_mask,\n",
            "        tf.tile(greedy_action_prob, [1, self._expected_num_actions]),\n",
            "        other_actions_probs)\n",
            "\n",
            "    if self._action_offset != 0:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\falcon_reward_prediction_policy.py",
        "line_number": 433,
        "API": ".fill(",
        "context": [
            "    else:\n",
            "      distribution = tfp.distributions.Categorical(\n",
            "          probs=action_probs, dtype=self._action_spec.dtype)\n",
            "\n",
            "    bandit_policy_values = tf.fill([batch_size, 1],\n",
            "                                   policy_utilities.BanditPolicyType.FALCON)\n",
            "    return distribution, bandit_policy_values\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\greedy_multi_objective_neural_policy.py",
        "line_number": 67,
        "API": ".format(",
        "context": [
            "    ValueError: If `objectives_tensor` is not rank-3.\n",
            "  \"\"\"\n",
            "  if objectives_tensor.shape.rank != 3:\n",
            "    raise ValueError(\n",
            "        'The objectives_tensor should be rank-3, but is rank-{}'.format(\n",
            "            objectives_tensor.shape.rank))\n",
            "  return tf.transpose(\n",
            "      tf.map_fn(scalarizer, tf.transpose(objectives_tensor, perm=[2, 0, 1])))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\greedy_multi_objective_neural_policy.py",
        "line_number": 133,
        "API": ".flatten(",
        "context": [
            "        None.\n",
            "    \"\"\"\n",
            "    policy_utilities.check_no_mask_with_arm_features(\n",
            "        accepts_per_arm_features, observation_and_action_constraint_splitter)\n",
            "    flat_action_spec = tf.nest.flatten(action_spec)\n",
            "    if len(flat_action_spec) > 1:\n",
            "      raise NotImplementedError(\n",
            "          'action_spec can only contain a single BoundedTensorSpec.')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\greedy_multi_objective_neural_policy.py",
        "line_number": 145,
        "API": ".format(",
        "context": [
            "        action_spec.shape.rank > 1 or\n",
            "        action_spec.shape.num_elements() != 1):\n",
            "      raise NotImplementedError(\n",
            "          'action_spec must be a BoundedTensorSpec of type int32 and shape (). '\n",
            "          'Found {}.'.format(action_spec))\n",
            "    self._expected_num_actions = action_spec.maximum - action_spec.minimum + 1\n",
            "    self._action_offset = action_spec.minimum\n",
            "    policy_state_spec = []\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\greedy_multi_objective_neural_policy.py",
        "line_number": 158,
        "API": ".format(",
        "context": [
            "    self._num_objectives = len(self._objective_networks)\n",
            "    if self._num_objectives < 2:\n",
            "      raise ValueError(\n",
            "          'Number of objectives should be at least two, but found to be {}'\n",
            "          .format(self._num_objectives))\n",
            "\n",
            "    self._emit_policy_info = emit_policy_info\n",
            "    predicted_rewards_mean = ()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\greedy_multi_objective_neural_policy.py",
        "line_number": 251,
        "API": ".format(",
        "context": [
            "          network,\n",
            "          heteroscedastic_q_network.HeteroscedasticQNetwork) else prediction\n",
            "      if predicted_value.shape.rank != 2:\n",
            "        raise ValueError('Prediction from network {} shoud be a rank-2 tensor, '\n",
            "                         ' but has shape {}'.format(idx, predicted_value.shape))\n",
            "      if predicted_value.shape[1] is not None and predicted_value.shape[\n",
            "          1] != self._expected_num_actions:\n",
            "        raise ValueError(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\greedy_multi_objective_neural_policy.py",
        "line_number": 256,
        "API": ".format(",
        "context": [
            "      if predicted_value.shape[1] is not None and predicted_value.shape[\n",
            "          1] != self._expected_num_actions:\n",
            "        raise ValueError(\n",
            "            'The number of actions ({}) does not match objective network {}'\n",
            "            ' output size ({}).'.format(self._expected_num_actions, idx,\n",
            "                                        predicted_value.shape[1]))\n",
            "      predicted_objective_values.append(predicted_value)\n",
            "    # Stack the list of predicted objective tensors into a rank-3 tensor shaped\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\greedy_multi_objective_neural_policy.py",
        "line_number": 261,
        "API": ".stack(",
        "context": [
            "                                        predicted_value.shape[1]))\n",
            "      predicted_objective_values.append(predicted_value)\n",
            "    # Stack the list of predicted objective tensors into a rank-3 tensor shaped\n",
            "    # as [batch_size, num_of_objectives, num_of_actions].\n",
            "    predicted_objectives_tensor = tf.stack(predicted_objective_values, axis=1)\n",
            "    return predicted_objectives_tensor, updated_policy_state\n",
            "\n",
            "  def _distribution(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\greedy_multi_objective_neural_policy.py",
        "line_number": 277,
        "API": ".shape(",
        "context": [
            "    scalarized_reward = scalarize_objectives(predicted_objective_values_tensor,\n",
            "                                             self._scalarizer)\n",
            "    # Preserve static batch size values when they are available.\n",
            "    batch_size = (tf.compat.dimension_value(scalarized_reward.shape[0])\n",
            "                  or tf.shape(scalarized_reward)[0])\n",
            "    mask = constraints.construct_mask_from_multiple_sources(\n",
            "        time_step.observation, self._observation_and_action_constraint_splitter,\n",
            "        (), self._expected_num_actions)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\greedy_multi_objective_neural_policy.py",
        "line_number": 287,
        "API": ".argmax(",
        "context": [
            "    if mask is not None:\n",
            "      actions = policy_utilities.masked_argmax(\n",
            "          scalarized_reward, mask, output_type=self.action_spec.dtype)\n",
            "    else:\n",
            "      actions = tf.argmax(\n",
            "          scalarized_reward, axis=-1, output_type=self.action_spec.dtype)\n",
            "\n",
            "    actions += self._action_offset\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\greedy_multi_objective_neural_policy.py",
        "line_number": 292,
        "API": ".fill(",
        "context": [
            "          scalarized_reward, axis=-1, output_type=self.action_spec.dtype)\n",
            "\n",
            "    actions += self._action_offset\n",
            "\n",
            "    bandit_policy_values = tf.fill([batch_size, 1],\n",
            "                                   policy_utilities.BanditPolicyType.GREEDY)\n",
            "\n",
            "    if self._accepts_per_arm_features:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\greedy_multi_objective_neural_policy.py",
        "line_number": 298,
        "API": ".gather(",
        "context": [
            "\n",
            "    if self._accepts_per_arm_features:\n",
            "      # Saving the features for the chosen action to the policy_info.\n",
            "      def gather_observation(obs):\n",
            "        return tf.gather(params=obs, indices=actions, batch_dims=1)\n",
            "\n",
            "      chosen_arm_features = tf.nest.map_structure(\n",
            "          gather_observation,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\greedy_multi_objective_neural_policy.py",
        "line_number": 304,
        "API": ".zeros(",
        "context": [
            "      chosen_arm_features = tf.nest.map_structure(\n",
            "          gather_observation,\n",
            "          observation[bandit_spec_utils.PER_ARM_FEATURE_KEY])\n",
            "      policy_info = policy_utilities.PerArmPolicyInfo(\n",
            "          log_probability=tf.zeros([batch_size], tf.float32)\n",
            "          if policy_utilities.InfoFields.LOG_PROBABILITY\n",
            "          in self._emit_policy_info else (),\n",
            "          predicted_rewards_mean=(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\greedy_multi_objective_neural_policy.py",
        "line_number": 321,
        "API": ".zeros(",
        "context": [
            "                              in self._emit_policy_info else ()),\n",
            "          chosen_arm_features=chosen_arm_features)\n",
            "    else:\n",
            "      policy_info = policy_utilities.PolicyInfo(\n",
            "          log_probability=tf.zeros([batch_size], tf.float32)\n",
            "          if policy_utilities.InfoFields.LOG_PROBABILITY\n",
            "          in self._emit_policy_info else (),\n",
            "          predicted_rewards_mean=(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\greedy_reward_prediction_policy.py",
        "line_number": 31,
        "API": ".shape(",
        "context": [
            "\n",
            "  def _action_distribution(self, mask, predicted_rewards):\n",
            "    \"\"\"Returns the action with largest predicted reward.\"\"\"\n",
            "    # Argmax.\n",
            "    batch_size = tf.shape(predicted_rewards)[0]\n",
            "    if mask is not None:\n",
            "      actions = policy_utilities.masked_argmax(\n",
            "          predicted_rewards, mask, output_type=self.action_spec.dtype)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\greedy_reward_prediction_policy.py",
        "line_number": 36,
        "API": ".argmax(",
        "context": [
            "    if mask is not None:\n",
            "      actions = policy_utilities.masked_argmax(\n",
            "          predicted_rewards, mask, output_type=self.action_spec.dtype)\n",
            "    else:\n",
            "      actions = tf.argmax(\n",
            "          predicted_rewards, axis=-1, output_type=self.action_spec.dtype)\n",
            "\n",
            "    actions += self._action_offset\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\greedy_reward_prediction_policy.py",
        "line_number": 41,
        "API": ".fill(",
        "context": [
            "          predicted_rewards, axis=-1, output_type=self.action_spec.dtype)\n",
            "\n",
            "    actions += self._action_offset\n",
            "\n",
            "    bandit_policy_values = tf.fill([batch_size, 1],\n",
            "                                   policy_utilities.BanditPolicyType.GREEDY)\n",
            "    return tfp.distributions.Deterministic(loc=actions), bandit_policy_values\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linalg.py",
        "line_number": 29,
        "API": ".format(",
        "context": [
            "\n",
            "def _cg_check_shapes(a_mat, b_mat):\n",
            "  if a_mat.shape[0] != a_mat.shape[1] or a_mat.shape.rank != 2:\n",
            "    raise ValueError('`a_mat` must be rank 2 square matrix; '\n",
            "                     'got shape {}.'.format(a_mat.shape))\n",
            "  if a_mat.shape[1] != b_mat.shape[0]:\n",
            "    raise ValueError('The dims of `a_mat` and `b_mat` are not compatible; '\n",
            "                     'got shapes {} and {}.'.format(a_mat.shape, b_mat.shape))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linalg.py",
        "line_number": 58,
        "API": ".shape(",
        "context": [
            "    ValueError: if `a_mat` is not square or `a_mat` and `b_mat` have\n",
            "    incompatible shapes.\n",
            "  \"\"\"\n",
            "  _cg_check_shapes(a_mat, b_mat)\n",
            "  n = tf.shape(b_mat)[0]\n",
            "  x = tf.zeros_like(b_mat)\n",
            "\n",
            "  r = b_mat - tf.matmul(a_mat, x)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linalg.py",
        "line_number": 63,
        "API": ".einsum(",
        "context": [
            "  x = tf.zeros_like(b_mat)\n",
            "\n",
            "  r = b_mat - tf.matmul(a_mat, x)\n",
            "  p = r\n",
            "  rs_old = tf.einsum('ij,ij->j', r, r)\n",
            "  rs_new = rs_old\n",
            "\n",
            "  def body_fn(i, x, p, r, rs_old, rs_new):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linalg.py",
        "line_number": 73,
        "API": ".tile(",
        "context": [
            "    # columns corresponding to large residuals. We only update variables\n",
            "    # corresponding to those columns to avoid numerical issues.\n",
            "    active_columns_mask = (rs_old > tol)\n",
            "    # Replicate the mask along axis 0 to be of shape [n, k].\n",
            "    active_columns_tiled_mask = tf.tile(\n",
            "        tf.expand_dims(active_columns_mask, axis=0),\n",
            "        multiples=[tf.shape(b_mat)[0], 1])\n",
            "    a_x_p = tf.matmul(a_mat, p)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linalg.py",
        "line_number": 78,
        "API": ".multiply(",
        "context": [
            "        tf.expand_dims(active_columns_mask, axis=0),\n",
            "        multiples=[tf.shape(b_mat)[0], 1])\n",
            "    a_x_p = tf.matmul(a_mat, p)\n",
            "    alpha = rs_old / tf.reduce_sum(p * a_x_p, axis=0)\n",
            "    x = tf.where(active_columns_tiled_mask, x + tf.multiply(p, alpha), x)\n",
            "    r = tf.where(active_columns_tiled_mask, r - tf.multiply(a_x_p, alpha), r)\n",
            "    rs_new = tf.where(active_columns_mask, tf.einsum('ij,ij->j', r, r), rs_new)\n",
            "    p = tf.where(active_columns_tiled_mask, r + tf.multiply(p, rs_new / rs_old),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linalg.py",
        "line_number": 83,
        "API": ".where(",
        "context": [
            "    r = tf.where(active_columns_tiled_mask, r - tf.multiply(a_x_p, alpha), r)\n",
            "    rs_new = tf.where(active_columns_mask, tf.einsum('ij,ij->j', r, r), rs_new)\n",
            "    p = tf.where(active_columns_tiled_mask, r + tf.multiply(p, rs_new / rs_old),\n",
            "                 p)\n",
            "    rs_old = tf.where(active_columns_mask, rs_new, rs_old)\n",
            "    i = i + 1\n",
            "    return i, x, p, r, rs_old, rs_new\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linalg.py",
        "line_number": 93,
        "API": ".less(",
        "context": [
            "    del x  # unused\n",
            "    del p  # unused\n",
            "    del r  # unused\n",
            "    del rs_old  # unused\n",
            "    i_cond = tf.less(i, n)\n",
            "    residual_cond = tf.greater(tf.reduce_max(tf.sqrt(rs_new)), tol)\n",
            "    return tf.logical_and(i_cond, residual_cond)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linalg.py",
        "line_number": 99,
        "API": ".constant(",
        "context": [
            "    return tf.logical_and(i_cond, residual_cond)\n",
            "\n",
            "  _, x, _, _, _, _ = tf.while_loop(\n",
            "      while_exit_cond,\n",
            "      body_fn, [tf.constant(0), x, p, r, rs_old, rs_new],\n",
            "      parallel_iterations=1)\n",
            "  return x\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linalg.py",
        "line_number": 107,
        "API": ".format(",
        "context": [
            "\n",
            "def _check_shapes(a_inv: types.Tensor, u: types.Tensor):\n",
            "  if a_inv.shape[0] != a_inv.shape[1] or a_inv.shape.rank != 2:\n",
            "    raise ValueError('`a_inv` must be rank 2 square matrix; '\n",
            "                     'got shape {}.'.format(a_inv.shape))\n",
            "  if u.shape.rank != 2:\n",
            "    raise ValueError('`u` must be rank 2 matrix; '\n",
            "                     'got shape {}.'.format(u.shape))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linalg.py",
        "line_number": 113,
        "API": ".format(",
        "context": [
            "    raise ValueError('`u` must be rank 2 matrix; '\n",
            "                     'got shape {}.'.format(u.shape))\n",
            "  if a_inv.shape[1] != u.shape[1]:\n",
            "    raise ValueError('`a_inv` and `u` must have shapes [m, m] and [n, m]; '\n",
            "                     'got shapes {} and {}.'.format(a_inv.shape, u.shape))\n",
            "\n",
            "\n",
            "def simplified_woodbury_update(a_inv: types.Float,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linalg.py",
        "line_number": 139,
        "API": ".matmul(",
        "context": [
            "    ValueError: if `a_inv` is not square or `a_inv` and `u` have incompatible\n",
            "    shapes.\n",
            "  \"\"\"\n",
            "  _check_shapes(a_inv, u)\n",
            "  u_x_a_inv = tf.matmul(u, a_inv)\n",
            "  capacitance = (\n",
            "      tf.eye(tf.shape(u)[0], dtype=u.dtype) +\n",
            "      tf.matmul(u_x_a_inv, u, transpose_b=True))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linalg.py",
        "line_number": 144,
        "API": ".solve(",
        "context": [
            "  capacitance = (\n",
            "      tf.eye(tf.shape(u)[0], dtype=u.dtype) +\n",
            "      tf.matmul(u_x_a_inv, u, transpose_b=True))\n",
            "  return -1. * tf.matmul(\n",
            "      u_x_a_inv, tf.linalg.solve(capacitance, u_x_a_inv), transpose_a=True)\n",
            "\n",
            "\n",
            "def update_inverse(a_inv: types.Float, x: types.Float) -> types.Float:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linalg.py",
        "line_number": 165,
        "API": ".shape(",
        "context": [
            "  Returns:\n",
            "    The update that needs to be added to 'a_inv' to compute the inverse.\n",
            "    If `x` is empty, a zero matrix is returned.\n",
            "  \"\"\"\n",
            "  batch_size = tf.shape(x)[0]\n",
            "\n",
            "  def true_fn():\n",
            "    return tf.zeros_like(a_inv)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linalg.py",
        "line_number": 173,
        "API": ".cond(",
        "context": [
            "\n",
            "  def false_fn():\n",
            "    return simplified_woodbury_update(a_inv, x)\n",
            "\n",
            "  a_inv_update = tf.cond(tf.equal(batch_size, 0), true_fn, false_fn)\n",
            "  return a_inv_update\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 200,
        "API": ".format(",
        "context": [
            "    cov_matrix_dim = tf.compat.dimension_value(cov_matrix[0].shape[0])\n",
            "    if self._overall_context_dim != cov_matrix_dim:\n",
            "      raise ValueError('The dimension of matrix `cov_matrix` must match '\n",
            "                       'overall context dimension {}. '\n",
            "                       'Got {} for `cov_matrix`.'.format(\n",
            "                           self._overall_context_dim, cov_matrix_dim))\n",
            "\n",
            "    data_vector_dim = tf.compat.dimension_value(data_vector[0].shape[0])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 207,
        "API": ".format(",
        "context": [
            "    data_vector_dim = tf.compat.dimension_value(data_vector[0].shape[0])\n",
            "    if self._overall_context_dim != data_vector_dim:\n",
            "      raise ValueError('The dimension of vector `data_vector` must match '\n",
            "                       'context  dimension {}. '\n",
            "                       'Got {} for `data_vector`.'.format(\n",
            "                           self._overall_context_dim, data_vector_dim))\n",
            "\n",
            "    self._dtype = self._data_vector[0].dtype\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 238,
        "API": ".flatten(",
        "context": [
            "    all_vars = [\n",
            "        self._cov_matrix, self._data_vector, self._theta, self._num_samples,\n",
            "        self._eig_matrix, self._eig_vals\n",
            "    ]\n",
            "    return [v for v in tf.nest.flatten(all_vars) if isinstance(v, tf.Variable)]\n",
            "\n",
            "  def _predict_mean_reward(self, theta: tf.Tensor,\n",
            "                           global_observation: tf.Tensor,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 261,
        "API": ".stack(",
        "context": [
            "    \"\"\"\n",
            "    if self._accepts_per_arm_features:\n",
            "      # The stacked observations are shaped as\n",
            "      # [num_actions, batch_size, overall_context_dim].\n",
            "      stacked_observations = tf.stack([\n",
            "          self._get_current_observation(\n",
            "              global_observation, arm_observations, arm_index=k)\n",
            "          for k in range(self._num_actions)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 269,
        "API": ".matmul(",
        "context": [
            "      ])\n",
            "      # theta is shaped as [1, overall_context_dim], because `num_models` is 1\n",
            "      # for per-arm-features.\n",
            "      # predicted_rewards is shaped [num_actions, batch_size, 1].\n",
            "      predicted_rewards = tf.matmul(\n",
            "          stacked_observations, theta, transpose_b=True)\n",
            "      # Squeeze and transpose the predicted rewards to return a matrix of\n",
            "      # shape [batch_size, num_actions].\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 278,
        "API": ".matmul(",
        "context": [
            "    else:\n",
            "      # global_observation: shaped [batch_size, global_context_dim].\n",
            "      # self._theta: shaped [num_actions, global_context_dim].\n",
            "      # predicted rewards shaped as [batch_size, num_actions].\n",
            "      return tf.matmul(global_observation, theta, transpose_b=True)\n",
            "\n",
            "  def _predict_mean_reward_and_variance(\n",
            "      self, global_observation: tf.Tensor,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 299,
        "API": ".matrix_transpose(",
        "context": [
            "    \"\"\"\n",
            "    est_reward = []\n",
            "    est_variance = []\n",
            "    for k in range(self._num_actions):\n",
            "      current_observation = tf.linalg.matrix_transpose(\n",
            "          self._get_current_observation(global_observation, arm_observations,\n",
            "                                        k))\n",
            "      model_index = policy_utilities.get_model_index(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 305,
        "API": ".matmul(",
        "context": [
            "                                        k))\n",
            "      model_index = policy_utilities.get_model_index(\n",
            "          k, self._accepts_per_arm_features)\n",
            "      if self._use_eigendecomp:\n",
            "        q_t_b = tf.matmul(\n",
            "            self._eig_matrix[model_index],\n",
            "            current_observation,\n",
            "            transpose_a=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 310,
        "API": ".ones_like(",
        "context": [
            "            self._eig_matrix[model_index],\n",
            "            current_observation,\n",
            "            transpose_a=True)\n",
            "        lambda_inv = tf.divide(\n",
            "            tf.ones_like(self._eig_vals[model_index]),\n",
            "            self._eig_vals[model_index] + self._tikhonov_weight)\n",
            "        a_inv_x = tf.matmul(self._eig_matrix[model_index],\n",
            "                            tf.einsum('j,jk->jk', lambda_inv, q_t_b))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 317,
        "API": ".eye(",
        "context": [
            "                            tf.einsum('j,jk->jk', lambda_inv, q_t_b))\n",
            "      else:\n",
            "        a_inv_x = linalg.conjugate_gradient(\n",
            "            self._cov_matrix[model_index] + self._tikhonov_weight *\n",
            "            tf.eye(self._overall_context_dim, dtype=self._dtype),\n",
            "            current_observation)\n",
            "      est_mean_reward = tf.einsum('j,jk->k', self._data_vector[model_index],\n",
            "                                  a_inv_x)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 322,
        "API": ".einsum(",
        "context": [
            "            current_observation)\n",
            "      est_mean_reward = tf.einsum('j,jk->k', self._data_vector[model_index],\n",
            "                                  a_inv_x)\n",
            "      est_reward.append(est_mean_reward)\n",
            "      var = tf.einsum('ij,ij->j', current_observation, a_inv_x)\n",
            "      est_variance.append(var)\n",
            "\n",
            "    return (tf.stack(est_reward, axis=-1), tf.stack(est_variance, axis=-1))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 332,
        "API": ".cast(",
        "context": [
            "    observation = time_step.observation\n",
            "    if self.observation_and_action_constraint_splitter is not None:\n",
            "      observation, _ = self.observation_and_action_constraint_splitter(\n",
            "          observation)\n",
            "    observation = tf.nest.map_structure(lambda o: tf.cast(o, dtype=self._dtype),\n",
            "                                        observation)\n",
            "    global_observation, arm_observations = self._split_observation(observation)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 338,
        "API": ".concat(",
        "context": [
            "    global_observation, arm_observations = self._split_observation(observation)\n",
            "\n",
            "    if self._add_bias:\n",
            "      # The bias is added via a constant 1 feature.\n",
            "      global_observation = tf.concat([\n",
            "          global_observation,\n",
            "          tf.ones([tf.shape(global_observation)[0], 1], dtype=self._dtype)\n",
            "      ],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 348,
        "API": ".format(",
        "context": [
            "    # batched.\n",
            "    if not global_observation.shape.is_compatible_with(\n",
            "        [None, self._global_context_dim]):\n",
            "      raise ValueError(\n",
            "          'Global observation shape is expected to be {}. Got {}.'.format(\n",
            "              [None, self._global_context_dim],\n",
            "              global_observation.shape.as_list()))\n",
            "    global_observation = tf.reshape(global_observation,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 364,
        "API": ".sqrt(",
        "context": [
            "    else:\n",
            "      est_rewards, est_variances = self._predict_mean_reward_and_variance(\n",
            "          global_observation, arm_observations)\n",
            "      if self._exploration_strategy == ExplorationStrategy.optimistic:\n",
            "        rewards_for_argmax = est_rewards + self._alpha * tf.sqrt(est_variances)\n",
            "      elif self._exploration_strategy == ExplorationStrategy.sampling:\n",
            "        mu_sampler = tfd.Normal(\n",
            "            loc=est_rewards, scale=self._alpha * tf.sqrt(est_variances))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 380,
        "API": ".flatten(",
        "context": [
            "    if mask is not None:\n",
            "      chosen_actions = policy_utilities.masked_argmax(\n",
            "          rewards_for_argmax,\n",
            "          mask,\n",
            "          output_type=tf.nest.flatten(self._action_spec)[0].dtype)\n",
            "    else:\n",
            "      chosen_actions = tf.argmax(\n",
            "          rewards_for_argmax,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 385,
        "API": ".flatten(",
        "context": [
            "    else:\n",
            "      chosen_actions = tf.argmax(\n",
            "          rewards_for_argmax,\n",
            "          axis=-1,\n",
            "          output_type=tf.nest.flatten(self._action_spec)[0].dtype)\n",
            "\n",
            "    action_distributions = tfp.distributions.Deterministic(loc=chosen_actions)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 400,
        "API": ".format(",
        "context": [
            "  def _check_input_variables(self):\n",
            "    if len(self._cov_matrix) != len(self._data_vector):\n",
            "      raise ValueError('The size of list cov_matrix must match the size of '\n",
            "                       'list data_vector. Got {} for cov_matrix and {} '\n",
            "                       'for data_vector'.format(\n",
            "                           len(self._cov_matrix), len((self._data_vector))))\n",
            "    if len(self._num_samples) != len(self._cov_matrix):\n",
            "      raise ValueError('The size of num_samples must match the size of '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 405,
        "API": ".format(",
        "context": [
            "                           len(self._cov_matrix), len((self._data_vector))))\n",
            "    if len(self._num_samples) != len(self._cov_matrix):\n",
            "      raise ValueError('The size of num_samples must match the size of '\n",
            "                       'list cov_matrix. Got {} for num_samples and {} '\n",
            "                       'for cov_matrix'.format(\n",
            "                           len(self._num_samples), len((self._cov_matrix))))\n",
            "\n",
            "    if self._accepts_per_arm_features:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 412,
        "API": ".format(",
        "context": [
            "    if self._accepts_per_arm_features:\n",
            "      if len(self._cov_matrix) != 1:\n",
            "        raise ValueError(\n",
            "            'If the policy accepts per-arm features, the length of `cov_matrix`'\n",
            "            ' list must be 1. Got {} instead.'.format(len(self._cov_matrix)))\n",
            "    else:\n",
            "      if self._num_actions != len(self._cov_matrix):\n",
            "        raise ValueError(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 417,
        "API": ".format(",
        "context": [
            "    else:\n",
            "      if self._num_actions != len(self._cov_matrix):\n",
            "        raise ValueError(\n",
            "            'The number of elements in `cov_matrix` ({}) must match '\n",
            "            'the number of actions derived from `action_spec` ({}).'.format(\n",
            "                len(self._cov_matrix), self._num_actions))\n",
            "\n",
            "  def _populate_policy_info_spec(self, observation_spec,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 477,
        "API": ".concat(",
        "context": [
            "      observation for arm `arm_index`.\n",
            "    \"\"\"\n",
            "    if self._accepts_per_arm_features:\n",
            "      current_arm = arm_observations[:, arm_index, :]\n",
            "      current_observation = tf.concat([global_observation, current_arm],\n",
            "                                      axis=-1)\n",
            "      return current_observation\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\linear_bandit_policy.py",
        "line_number": 491,
        "API": ".format(",
        "context": [
            "      arm_observations = observation[bandit_spec_utils.PER_ARM_FEATURE_KEY]\n",
            "      if not arm_observations.shape.is_compatible_with(\n",
            "          [None, self._num_actions, self._arm_context_dim]):\n",
            "        raise ValueError(\n",
            "            'Arm observation shape is expected to be {}. Got {}.'.format(\n",
            "                [None, self._num_actions, self._arm_context_dim],\n",
            "                arm_observations.shape.as_list()))\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\loss_utils.py",
        "line_number": 86,
        "API": ".cast(",
        "context": [
            "  if y_pred is None:\n",
            "    raise ValueError('y_pred must not be None.')\n",
            "  with tf.compat.v1.name_scope(scope, 'pinball_loss',\n",
            "                               (y_pred, y_true, weights)) as scope:\n",
            "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
            "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
            "    y_pred.get_shape().assert_is_compatible_with(y_true.get_shape())\n",
            "    error = tf.subtract(y_true, y_pred)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\mixture_policy.py",
        "line_number": 101,
        "API": ".flatten(",
        "context": [
            "    raise NotImplementedError(\n",
            "        '_distribution is not implemented for this policy.')\n",
            "\n",
            "  def _action(self, time_step, policy_state, seed=None):\n",
            "    first_obs = tf.nest.flatten(time_step.observation)[0]\n",
            "    batch_size = tf.compat.dimension_value(\n",
            "        first_obs.shape[0]) or tf.shape(first_obs)[0]\n",
            "    policy_choice = self._mixture_distribution.sample(batch_size)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\mixture_policy.py",
        "line_number": 113,
        "API": ".expand_dims(",
        "context": [
            "        [step.action for step in policy_steps], axis=-1)\n",
            "    policy_infos = nest_utils.stack_nested_tensors(\n",
            "        [step.info for step in policy_steps], axis=-1)\n",
            "\n",
            "    expanded_choice = tf.expand_dims(policy_choice, axis=-1)\n",
            "    mixture_action = tf.nest.map_structure(\n",
            "        lambda t: tf.gather(t, policy_choice, batch_dims=1), policy_actions)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\mixture_policy.py",
        "line_number": 118,
        "API": ".gather(",
        "context": [
            "    mixture_action = tf.nest.map_structure(\n",
            "        lambda t: tf.gather(t, policy_choice, batch_dims=1), policy_actions)\n",
            "\n",
            "    expanded_mixture_info = tf.nest.map_structure(\n",
            "        lambda t: tf.gather(t, expanded_choice, batch_dims=1, axis=-1),\n",
            "        policy_infos)\n",
            "    mixture_info = tf.nest.map_structure(lambda t: tf.squeeze(t, axis=-1),\n",
            "                                         expanded_mixture_info)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\neural_linucb_policy.py",
        "line_number": 135,
        "API": ".format(",
        "context": [
            "    self._encoding_dim = encoding_dim\n",
            "\n",
            "    if accepts_per_arm_features and reward_layer.units != 1:\n",
            "      raise ValueError('The output dimension of the reward layer must be 1, got'\n",
            "                       ' {}'.format(reward_layer.units))\n",
            "\n",
            "    if not isinstance(cov_matrix, (list, tuple)):\n",
            "      raise ValueError('cov_matrix must be a list of matrices (Tensors).')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\neural_linucb_policy.py",
        "line_number": 158,
        "API": ".format(",
        "context": [
            "\n",
            "    if len(cov_matrix) != len(data_vector):\n",
            "      raise ValueError('The size of list cov_matrix must match the size of '\n",
            "                       'list data_vector. Got {} for cov_matrix and {} '\n",
            "                       'for data_vector'.format(\n",
            "                           len(self._cov_matrix), len((data_vector))))\n",
            "    if len(num_samples) != len(cov_matrix):\n",
            "      raise ValueError('The size of num_samples must match the size of '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\neural_linucb_policy.py",
        "line_number": 163,
        "API": ".format(",
        "context": [
            "                           len(self._cov_matrix), len((data_vector))))\n",
            "    if len(num_samples) != len(cov_matrix):\n",
            "      raise ValueError('The size of num_samples must match the size of '\n",
            "                       'list cov_matrix. Got {} for num_samples and {} '\n",
            "                       'for cov_matrix'.format(\n",
            "                           len(self._num_samples), len((cov_matrix))))\n",
            "\n",
            "    self._accepts_per_arm_features = accepts_per_arm_features\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\neural_linucb_policy.py",
        "line_number": 173,
        "API": ".flatten(",
        "context": [
            "          time_step_spec.observation)\n",
            "    else:\n",
            "      context_spec = time_step_spec.observation\n",
            "    if accepts_per_arm_features:\n",
            "      self._num_actions = tf.nest.flatten(context_spec[\n",
            "          bandit_spec_utils.PER_ARM_FEATURE_KEY])[0].shape.as_list()[0]\n",
            "      self._num_models = 1\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\neural_linucb_policy.py",
        "line_number": 183,
        "API": ".format(",
        "context": [
            "    cov_matrix_dim = tf.compat.dimension_value(cov_matrix[0].shape[0])\n",
            "    if self._encoding_dim != cov_matrix_dim:\n",
            "      raise ValueError('The dimension of matrix `cov_matrix` must match '\n",
            "                       'encoding dimension {}.'\n",
            "                       'Got {} for `cov_matrix`.'.format(\n",
            "                           self._encoding_dim, cov_matrix_dim))\n",
            "    data_vector_dim = tf.compat.dimension_value(data_vector[0].shape[0])\n",
            "    if self._encoding_dim != data_vector_dim:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\neural_linucb_policy.py",
        "line_number": 189,
        "API": ".format(",
        "context": [
            "    data_vector_dim = tf.compat.dimension_value(data_vector[0].shape[0])\n",
            "    if self._encoding_dim != data_vector_dim:\n",
            "      raise ValueError('The dimension of vector `data_vector` must match '\n",
            "                       'encoding  dimension {}. '\n",
            "                       'Got {} for `data_vector`.'.format(\n",
            "                           self._encoding_dim, data_vector_dim))\n",
            "    action_spec = tensor_spec.BoundedTensorSpec(\n",
            "        shape=(),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\neural_linucb_policy.py",
        "line_number": 237,
        "API": ".flatten(",
        "context": [
            "    all_variables = [self._cov_matrix, self._data_vector,\n",
            "                     self._num_samples, self._actions_from_reward_layer,\n",
            "                     self._encoding_network.variables,\n",
            "                     self._reward_layer.variables]\n",
            "    return [v for v in tf.nest.flatten(all_variables)\n",
            "            if isinstance(v, tf.Variable)]\n",
            "\n",
            "  def _get_actions_from_reward_layer(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\neural_linucb_policy.py",
        "line_number": 244,
        "API": ".reshape(",
        "context": [
            "  def _get_actions_from_reward_layer(\n",
            "      self, encoded_observation: types.Float, mask: Optional[types.Tensor]\n",
            "  ) -> Tuple[types.Int, types.Float, types.Float]:\n",
            "    # Get the predicted expected reward.\n",
            "    est_mean_reward = tf.reshape(self._reward_layer(encoded_observation),\n",
            "                                 shape=[-1, self._num_actions])\n",
            "    if mask is None:\n",
            "      greedy_actions = tf.argmax(est_mean_reward, axis=-1, output_type=tf.int32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\neural_linucb_policy.py",
        "line_number": 255,
        "API": ".shape(",
        "context": [
            "\n",
            "    # Add epsilon greedy on top, if needed.\n",
            "    if self._epsilon_greedy:\n",
            "      batch_size = (tf.compat.dimension_value(encoded_observation.shape[0]) or\n",
            "                    tf.shape(encoded_observation)[0])\n",
            "      if mask is None:\n",
            "        random_actions = tf.random.uniform(\n",
            "            [batch_size], maxval=self._num_actions,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\neural_linucb_policy.py",
        "line_number": 261,
        "API": ".cast(",
        "context": [
            "        random_actions = tf.random.uniform(\n",
            "            [batch_size], maxval=self._num_actions,\n",
            "            dtype=tf.int32)\n",
            "      else:\n",
            "        zero_logits = tf.cast(tf.zeros_like(mask), tf.float32)\n",
            "        masked_categorical = masked.MaskedCategorical(\n",
            "            zero_logits, mask, dtype=tf.int32)\n",
            "        random_actions = masked_categorical.sample()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\neural_linucb_policy.py",
        "line_number": 266,
        "API": ".uniform(",
        "context": [
            "        masked_categorical = masked.MaskedCategorical(\n",
            "            zero_logits, mask, dtype=tf.int32)\n",
            "        random_actions = masked_categorical.sample()\n",
            "\n",
            "      rng = tf.random.uniform([batch_size], maxval=1.0)\n",
            "      cond = tf.greater(rng, self._epsilon_greedy)\n",
            "      chosen_actions = tf.compat.v1.where(cond, greedy_actions, random_actions)\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\neural_linucb_policy.py",
        "line_number": 277,
        "API": ".cast(",
        "context": [
            "\n",
            "  def _get_actions_from_linucb(\n",
            "      self, encoded_observation: types.Float, mask: Optional[types.Tensor]\n",
            "  ) -> Tuple[types.Int, types.Float, types.Float]:\n",
            "    encoded_observation = tf.cast(encoded_observation, dtype=self._dtype)\n",
            "\n",
            "    p_values = []\n",
            "    est_rewards = []\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\neural_linucb_policy.py",
        "line_number": 288,
        "API": ".eye(",
        "context": [
            "      model_index = policy_utilities.get_model_index(\n",
            "          k, self._accepts_per_arm_features)\n",
            "      a_inv_x = linalg.conjugate_gradient(\n",
            "          self._cov_matrix[model_index] +\n",
            "          tf.eye(self._encoding_dim, dtype=self._dtype),\n",
            "          tf.linalg.matrix_transpose(encoded_observation_for_arm))\n",
            "      mean_reward_est = tf.einsum('j,jk->k', self._data_vector[model_index],\n",
            "                                  a_inv_x)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\neural_linucb_policy.py",
        "line_number": 294,
        "API": ".reshape(",
        "context": [
            "      mean_reward_est = tf.einsum('j,jk->k', self._data_vector[model_index],\n",
            "                                  a_inv_x)\n",
            "      est_rewards.append(mean_reward_est)\n",
            "\n",
            "      ci = tf.reshape(\n",
            "          tf.linalg.tensor_diag_part(\n",
            "              tf.matmul(encoded_observation_for_arm, a_inv_x)), [-1, 1])\n",
            "      p_values.append(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\neural_linucb_policy.py",
        "line_number": 300,
        "API": ".stack(",
        "context": [
            "              tf.matmul(encoded_observation_for_arm, a_inv_x)), [-1, 1])\n",
            "      p_values.append(\n",
            "          tf.reshape(mean_reward_est, [-1, 1]) + self._alpha * tf.sqrt(ci))\n",
            "\n",
            "    stacked_p_values = tf.squeeze(tf.stack(p_values, axis=-1), axis=[1])\n",
            "    if mask is None:\n",
            "      chosen_actions = tf.argmax(\n",
            "          stacked_p_values,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\neural_linucb_policy.py",
        "line_number": 310,
        "API": ".stack(",
        "context": [
            "    else:\n",
            "      chosen_actions = policy_utilities.masked_argmax(\n",
            "          stacked_p_values, mask, output_type=tf.int32)\n",
            "\n",
            "    est_mean_reward = tf.cast(tf.stack(est_rewards, axis=-1), tf.float32)\n",
            "    return chosen_actions, est_mean_reward, tf.cast(stacked_p_values,\n",
            "                                                    tf.float32)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\neural_linucb_policy.py",
        "line_number": 328,
        "API": ".cast(",
        "context": [
            "        time_step.observation, self._observation_and_action_constraint_splitter,\n",
            "        (), self._num_actions)\n",
            "    # Pass the observations through the encoding network.\n",
            "    encoded_observation, _ = self._encoding_network(observation)\n",
            "    encoded_observation = tf.cast(encoded_observation, dtype=self._dtype)\n",
            "\n",
            "    if tf.distribute.has_strategy():\n",
            "      if self._distributed_use_reward_layer:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\neural_linucb_policy.py",
        "line_number": 338,
        "API": ".cond(",
        "context": [
            "      else:\n",
            "        chosen_actions, est_mean_rewards, est_rewards_optimistic = (\n",
            "            self._get_actions_from_linucb(encoded_observation, mask))\n",
            "    else:\n",
            "      chosen_actions, est_mean_rewards, est_rewards_optimistic = tf.cond(\n",
            "          self._actions_from_reward_layer,\n",
            "          # pylint: disable=g-long-lambda\n",
            "          lambda: self._get_actions_from_reward_layer(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\ranking_policy.py",
        "line_number": 74,
        "API": ".convert_to_tensor(",
        "context": [
            "    \"\"\"\n",
            "    raise NotImplementedError()\n",
            "\n",
            "  def _sample_n(self, n, seed=None):\n",
            "    logits = tf.convert_to_tensor(self.scores)\n",
            "    sample_shape = tf.concat([[n], tf.shape(logits)], axis=0)\n",
            "    slots = []\n",
            "    for _ in range(self._num_slots):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\ranking_policy.py",
        "line_number": 80,
        "API": ".one_hot(",
        "context": [
            "    slots = []\n",
            "    for _ in range(self._num_slots):\n",
            "      items = tfd.Categorical(logits=logits).sample()\n",
            "      slots.append(items)\n",
            "      logits -= tf.one_hot(items, sample_shape[-1], on_value=np.inf)\n",
            "      logits = self._penalizer_fn(logits, self._features, slots)\n",
            "    sample = tf.expand_dims(tf.stack(slots, axis=-1), axis=0)\n",
            "    return sample\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\ranking_policy.py",
        "line_number": 95,
        "API": ".stack(",
        "context": [
            "\n",
            "  def _penalizer_fn(self, logits, features, slots):\n",
            "    num_items = logits.shape[-1]\n",
            "    num_slotted = len(slots)\n",
            "    slot_tensor = tf.stack(slots, axis=-1)\n",
            "    # The tfd.Categorical distribution will give the sample `num_items` if all\n",
            "    # the logits are `-inf`. Hence, we need to apply minimum. This happens when\n",
            "    # `num_actions` is less than `num_slots`. To this end, the action taken by\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\ranking_policy.py",
        "line_number": 101,
        "API": ".gather(",
        "context": [
            "    # the logits are `-inf`. Hence, we need to apply minimum. This happens when\n",
            "    # `num_actions` is less than `num_slots`. To this end, the action taken by\n",
            "    # the policy always has to be taken together with the `num_actions`\n",
            "    # observation, to know how many slots are filled with valid items.\n",
            "    slotted_features = tf.gather(\n",
            "        features, tf.minimum(slot_tensor, num_items - 1), batch_dims=1)\n",
            "\n",
            "    # Calculate the similarity between all pairs from\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\ranking_policy.py",
        "line_number": 107,
        "API": ".repeat(",
        "context": [
            "\n",
            "    # Calculate the similarity between all pairs from\n",
            "    # `slotted_features x all_features`.\n",
            "    all_sims = tf.keras.losses.cosine_similarity(\n",
            "        tf.repeat(features, num_slotted, axis=1),\n",
            "        tf.tile(slotted_features, [1, num_items, 1])) - 1\n",
            "\n",
            "    sim_matrix = tf.reshape(all_sims, shape=[-1, num_items, num_slotted])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\ranking_policy.py",
        "line_number": 135,
        "API": ".exp(",
        "context": [
            "        Shape is `[num_items]`.\n",
            "      penalty_mixture_coefficient: Unused for this distribution.\n",
            "    \"\"\"\n",
            "    self._num_slots = num_slots\n",
            "    super(NoPenaltyPlackettLuce, self).__init__(scores=tf.math.exp(logits))\n",
            "\n",
            "  def sample(self, sample_shape=(), seed=None, name='sample', **kwargs):\n",
            "    return super(NoPenaltyPlackettLuce, self).sample(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\ranking_policy.py",
        "line_number": 226,
        "API": ".where(",
        "context": [
            "    scores, _ = self._network(observation, time_step.step_type, policy_state)\n",
            "    if self._use_num_actions:\n",
            "      num_actions = time_step.observation[\n",
            "          bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY]\n",
            "      masked_scores = tf.where(\n",
            "          tf.sequence_mask(num_actions, maxlen=self._num_items), scores,\n",
            "          tf.fill(tf.shape(scores), -np.inf))\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\reward_prediction_base_policy.py",
        "line_number": 86,
        "API": ".flatten(",
        "context": [
            "        `BoundedTensorSpec` or the `BoundedTensorSpec` is not valid.\n",
            "    \"\"\"\n",
            "    policy_utilities.check_no_mask_with_arm_features(\n",
            "        accepts_per_arm_features, observation_and_action_constraint_splitter)\n",
            "    flat_action_spec = tf.nest.flatten(action_spec)\n",
            "    if len(flat_action_spec) > 1:\n",
            "      raise NotImplementedError(\n",
            "          'action_spec can only contain a single BoundedTensorSpec.')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\reward_prediction_base_policy.py",
        "line_number": 98,
        "API": ".format(",
        "context": [
            "        action_spec.shape.rank > 1 or\n",
            "        action_spec.shape.num_elements() != 1):\n",
            "      raise NotImplementedError(\n",
            "          'action_spec must be a BoundedTensorSpec of type int32 and shape (). '\n",
            "          'Found {}.'.format(action_spec))\n",
            "    self._expected_num_actions = action_spec.maximum - action_spec.minimum + 1\n",
            "    self._action_offset = action_spec.minimum\n",
            "    reward_network.create_variables()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\reward_prediction_base_policy.py",
        "line_number": 197,
        "API": ".map_structure(",
        "context": [
            "      `step` otherwise.\n",
            "    \"\"\"\n",
            "    if self.accepts_per_arm_features:\n",
            "      # Saving the features for the chosen action to the policy_info.\n",
            "      chosen_arm_features = tf.nest.map_structure(\n",
            "          lambda obs: tf.gather(params=obs, indices=action, batch_dims=1),\n",
            "          time_step.observation[bandit_spec_utils.PER_ARM_FEATURE_KEY])\n",
            "      step = step._replace(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\reward_prediction_base_policy.py",
        "line_number": 248,
        "API": ".format(",
        "context": [
            "        -1] is not None and predicted_reward_values.shape[\n",
            "            -1] != self._expected_num_actions:\n",
            "      raise ValueError(\n",
            "          'The number of actions ({}) does not match the reward_network output'\n",
            "          ' size ({}).'.format(self._expected_num_actions,\n",
            "                               predicted_reward_values.shape[1]))\n",
            "\n",
            "    mask = constr.construct_mask_from_multiple_sources(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\bandits\\policies\\reward_prediction_base_policy.py",
        "line_number": 262,
        "API": ".map_structure(",
        "context": [
            "      # The actual action sampling hasn't happened yet, so we leave\n",
            "      # `log_probability` empty and set `chosen_arm_features` to dummy values of\n",
            "      # all zeros. We need to save dummy chosen arm features to make the\n",
            "      # returned policy step have the same structure as the policy state spec.\n",
            "      dummy_chosen_arm_features = tf.nest.map_structure(\n",
            "          lambda obs: tf.zeros_like(obs[:, 0, ...]),\n",
            "          time_step.observation[bandit_spec_utils.PER_ARM_FEATURE_KEY])\n",
            "      policy_info = policy_utilities.PerArmPolicyInfo(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\cql_sac_benchmark.py",
        "line_number": 66,
        "API": ".join(",
        "context": [
            "        data_parallel_reads=500,\n",
            "        data_prefetch=1000000,\n",
            "        eval_interval=10000)\n",
            "    wall_time_sec = time.time() - start_time_sec\n",
            "    event_file = utils.find_event_log(os.path.join(output_dir, 'eval'))\n",
            "    values, _ = utils.extract_event_log_values(\n",
            "        event_file, 'Metrics/AverageReturn', start_step=10000)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\cql_sac_benchmark.py",
        "line_number": 86,
        "API": ".set_verbosity(",
        "context": [
            "        wall_time=wall_time_sec, metrics=[metric_500k], extras={})\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "  logging.set_verbosity(logging.INFO)\n",
            "  tf.test.main()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\distribution_strategy_utils.py",
        "line_number": 48,
        "API": ".lower(",
        "context": [
            "  \"\"\"\n",
            "  if num_gpus < 0:\n",
            "    raise ValueError(\"`num_gpus` can not be negative.\")\n",
            "\n",
            "  distribution_strategy = distribution_strategy.lower()\n",
            "  if distribution_strategy == \"off\":\n",
            "    if num_gpus > 1:\n",
            "      raise ValueError(\"When {} GPUs are specified, distribution_strategy \"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\dqn_benchmark.py",
        "line_number": 55,
        "API": ".join(",
        "context": [
            "    start_time_sec = time.time()\n",
            "    dqn_train_eval_atari.train_eval(\n",
            "        output_dir, eval_interval=10000, num_iterations=750000)\n",
            "    wall_time_sec = time.time() - start_time_sec\n",
            "    event_file = utils.find_event_log(os.path.join(output_dir, 'eval'))\n",
            "    values, _ = utils.extract_event_log_values(\n",
            "        event_file, 'Metrics/AverageReturn/EnvironmentSteps')\n",
            "    # Min/Max ranges are very large to only hard fail if very broken. The system\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\dqn_benchmark.py",
        "line_number": 71,
        "API": ".main(",
        "context": [
            "        wall_time=wall_time_sec, metrics=[metric_3m], extras={})\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "  tf.test.main()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\perfzero_benchmark.py",
        "line_number": 43,
        "API": ".getenv(",
        "context": [
            "    Args:\n",
            "      output_dir: Base directory to store all output for the test.\n",
            "    \"\"\"\n",
            "    # MLCompass sets this value, but PerfZero OSS passes it as an arg.\n",
            "    if os.getenv('BENCHMARK_OUTPUT_DIR'):\n",
            "      self.output_dir = os.getenv('BENCHMARK_OUTPUT_DIR')\n",
            "    elif output_dir:\n",
            "      self.output_dir = output_dir\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\perfzero_benchmark.py",
        "line_number": 52,
        "API": ".join(",
        "context": [
            "      self.output_dir = '/tmp'\n",
            "\n",
            "  def _get_test_output_dir(self, folder_name):\n",
            "    \"\"\"Returns directory to store info, e.g. saved model and event log.\"\"\"\n",
            "    return os.path.join(self.output_dir, folder_name)\n",
            "\n",
            "  def setUp(self):\n",
            "    \"\"\"Sets up and resets flags before each test.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\ppo_benchmark.py",
        "line_number": 84,
        "API": ".format(",
        "context": [
            "    self.setUp()\n",
            "    output_dir = self._get_test_output_dir('training_env')\n",
            "    start_time_sec = time.time()\n",
            "    bindings = [\n",
            "        'schulman17.train_eval_lib.train_eval.env_name= \"{}\"'.format(\n",
            "            training_env),\n",
            "        'schulman17.train_eval_lib.train_eval.eval_episodes = 100'\n",
            "    ]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\ppo_benchmark.py",
        "line_number": 92,
        "API": ".join(",
        "context": [
            "    gin.parse_config(bindings)\n",
            "    ppo_clip_train_eval.ppo_clip_train_eval(\n",
            "        output_dir, eval_interval=10000, num_iterations=489)\n",
            "    wall_time_sec = time.time() - start_time_sec\n",
            "    event_file = utils.find_event_log(os.path.join(output_dir, 'eval'))\n",
            "    values, _ = utils.extract_event_log_values(\n",
            "        event_file, 'Metrics/AverageReturn/EnvironmentSteps')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\ppo_benchmark.py",
        "line_number": 108,
        "API": ".main(",
        "context": [
            "    self._tearDown()\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "  tf.test.main()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\sac_benchmark.py",
        "line_number": 55,
        "API": ".join(",
        "context": [
            "        env_name='HalfCheetah-v2',\n",
            "        eval_interval=50000,\n",
            "        num_iterations=3000000)\n",
            "    wall_time_sec = time.time() - start_time_sec\n",
            "    event_file = utils.find_event_log(os.path.join(output_dir, 'eval'))\n",
            "    values, _ = utils.extract_event_log_values(event_file,\n",
            "                                               'Metrics/AverageReturn')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\sac_benchmark.py",
        "line_number": 78,
        "API": ".main(",
        "context": [
            "        wall_time=wall_time_sec, metrics=[metric_1m, metric_3m], extras={})\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "  tf.test.main()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\utils.py",
        "line_number": 81,
        "API": ".format(",
        "context": [
            "    self.batch_index = batch_index\n",
            "    self.timestamp = timestamp\n",
            "\n",
            "  def __repr__(self):\n",
            "    return \"'BatchTimestamp<batch_index: {}, timestamp: {}>'\".format(\n",
            "        self.batch_index, self.timestamp)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\utils.py",
        "line_number": 123,
        "API": ".format(",
        "context": [
            "      step_time = elapsed_time / self.log_steps\n",
            "      self.timestamp_log.append(BatchTimestamp(self.global_steps, timestamp))\n",
            "      print(\"BenchmarkMetric: '{{global step':{}, \"\n",
            "            \"'steps_per_second':{:.5g}, step_time:{:.5g}, \"\n",
            "            \"'examples_per_second':{:.3f}}}\".format(self.global_steps,\n",
            "                                                    steps_per_second, step_time,\n",
            "                                                    examples_per_second))\n",
            "      self.start_time = timestamp\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\utils.py",
        "line_number": 182,
        "API": ".format(",
        "context": [
            "def get_variable_value(agent, name):\n",
            "  \"\"\"Returns the value of the trainable variable with the given name.\"\"\"\n",
            "  policy_vars = agent.policy.variables()\n",
            "  tf_vars = [v for v in policy_vars if name in v.name]\n",
            "  assert tf_vars, 'Variable \"{}\" does not exist. Found: {}'.format(\n",
            "      name, policy_vars)\n",
            "  if tf.executing_eagerly() and len(tf_vars) > 1:\n",
            "    var = tf_vars[0]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\utils.py",
        "line_number": 187,
        "API": ".format(",
        "context": [
            "      name, policy_vars)\n",
            "  if tf.executing_eagerly() and len(tf_vars) > 1:\n",
            "    var = tf_vars[0]\n",
            "  else:\n",
            "    assert len(tf_vars) == 1, 'More than one variable with name {}. {}'.format(\n",
            "        name, [(v.name, v.shape) for v in tf_vars])\n",
            "    var = tf_vars[0]\n",
            "  return var.numpy() if tf.executing_eagerly() else var.eval()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\utils.py",
        "line_number": 204,
        "API": ".allclose(",
        "context": [
            "  final_values = [get_variable_value(agent, var_name) for var_name in \\\n",
            "                    check_value_changes]\n",
            "  for var_name, initial, final in zip(check_value_changes, initial_values,\n",
            "                                      final_values):\n",
            "    all_close = np.allclose(initial, final)\n",
            "    assert not all_close, ('[{}] Variable \"{}\" did not change: {} -> {}'.format(\n",
            "        name, var_name, initial, final))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\utils.py",
        "line_number": 233,
        "API": ".join(",
        "context": [
            "  Raises:\n",
            "    FileNotFoundError: If an event log is not found in the event log\n",
            "      directory.\n",
            "  \"\"\"\n",
            "  event_log_path = os.path.join(eventlog_dir, log_file_pattern)\n",
            "\n",
            "  # In OSS tf.io.gfile.glob throws `NotFoundError` vs returning an empty\n",
            "  # list. Catching `NotFoundError` and doing the check yields a consistent\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\utils.py",
        "line_number": 239,
        "API": ".glob(",
        "context": [
            "  # In OSS tf.io.gfile.glob throws `NotFoundError` vs returning an empty\n",
            "  # list. Catching `NotFoundError` and doing the check yields a consistent\n",
            "  # message.\n",
            "  try:\n",
            "    event_files = tf.io.gfile.glob(event_log_path)\n",
            "  except tf.errors.NotFoundError:\n",
            "    event_files = []\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\utils.py",
        "line_number": 247,
        "API": ".format(",
        "context": [
            "  if not event_files:\n",
            "    raise FileNotFoundError(f'No files found matching pattern:{event_log_path}')\n",
            "\n",
            "  assert len(event_files) == 1, (\n",
            "      'Found {} event files({}) matching \"{}\" pattern and expected 1.'.format(\n",
            "          len(event_files), ','.join(event_files), event_log_path))\n",
            "\n",
            "  return event_files[0]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\utils.py",
        "line_number": 277,
        "API": ".info(",
        "context": [
            "  \"\"\"\n",
            "  current_step = 0\n",
            "  start_time = 0\n",
            "  max_wall_time = 0.0\n",
            "  logging.info('Processing event file: %s', event_file)\n",
            "  event_values = {}\n",
            "  for summary in summary_iterator(event_file):\n",
            "    current_step = summary.step\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\utils.py",
        "line_number": 286,
        "API": ".item(",
        "context": [
            "    for value in summary.summary.value:\n",
            "      if value.tag == event_tag:\n",
            "        ndarray = tf.make_ndarray(value.tensor)\n",
            "        if current_step >= start_step:\n",
            "          event_values[summary.step] = ndarray.item(0)\n",
            "          if current_step == start_step:\n",
            "            start_time = summary.wall_time\n",
            "            logging.info(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\benchmark\\utils.py",
        "line_number": 307,
        "API": ".info(",
        "context": [
            "  if end_step and current_step < end_step:\n",
            "    raise ValueError('Error: Final step was less than the requested end_step.')\n",
            "\n",
            "  elapse_time = (max_wall_time - start_time) / 60\n",
            "  logging.info(\n",
            "      'training end (step %d): %s', current_step,\n",
            "      datetime.datetime.fromtimestamp(max_wall_time).strftime(\n",
            "          '%Y-%m-%d %H:%M:%S.%f'))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\gumbel_softmax.py",
        "line_number": 93,
        "API": ".cast(",
        "context": [
            "  def _log_prob(self, x):\n",
            "    if x.dtype != self.distribution.logits.dtype:\n",
            "      # Calculate log_prob using the underlying categorical distribution when\n",
            "      # the input is discrete.\n",
            "      x = tf.cast(x, self.distribution.logits.dtype)\n",
            "      return tf.reduce_sum(\n",
            "          x * tf.math.log_softmax(self.distribution.logits), axis=-1)\n",
            "    # Add an epsilon to prevent INF.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\gumbel_softmax.py",
        "line_number": 101,
        "API": ".one_hot(",
        "context": [
            "    x += 1e-10\n",
            "    return super(GumbelSoftmax, self)._log_prob(x)\n",
            "\n",
            "  def convert_to_one_hot(self, samples):\n",
            "    return tf.one_hot(\n",
            "        tf.argmax(samples, axis=-1),\n",
            "        self.distribution.event_size, dtype=self._output_dtype)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\masked.py",
        "line_number": 68,
        "API": ".convert_to_tensor(",
        "context": [
            "        Otherwise use given value.\n",
            "      name: Python `str` name prefixed to Ops created by this class.\n",
            "    \"\"\"\n",
            "    parameters = dict(locals())\n",
            "    logits = tf.convert_to_tensor(value=logits)\n",
            "    mask = tf.convert_to_tensor(value=mask)\n",
            "    self._mask = tf.cast(mask, tf.bool)  # Nonzero values are True\n",
            "    self._neg_inf = neg_inf\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\masked.py",
        "line_number": 79,
        "API": ".cast(",
        "context": [
            "                       ' Categorical distribution. Given `%s`.' % probs)\n",
            "\n",
            "    if neg_inf is None:\n",
            "      neg_inf = logits.dtype.min\n",
            "    neg_inf = tf.cast(\n",
            "        tf.fill(dims=tf.shape(input=logits), value=neg_inf), logits.dtype)\n",
            "    logits = tf.compat.v2.where(self._mask, logits, neg_inf)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\masked.py",
        "line_number": 93,
        "API": ".log_softmax(",
        "context": [
            "        name=name)\n",
            "    self._parameters = parameters\n",
            "\n",
            "  def _entropy(self):\n",
            "    entropy = tf.nn.log_softmax(self.logits) * self.probs_parameter()\n",
            "    # Replace the (potentially -inf) values with 0s before summing.\n",
            "    entropy = tf.compat.v1.where(self._mask, entropy, tf.zeros_like(entropy))\n",
            "    return -tf.reduce_sum(input_tensor=entropy, axis=-1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\reparameterized_sampling.py",
        "line_number": 41,
        "API": ".format(",
        "context": [
            "  if reparam:\n",
            "    if (distribution.reparameterization_type !=\n",
            "        tfp.distributions.FULLY_REPARAMETERIZED):\n",
            "      raise ValueError('This distribution cannot be reparameterized'\n",
            "                       ': {}'.format(distribution))\n",
            "    else:\n",
            "      return distribution.sample(**kwargs)\n",
            "  else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\tanh_bijector_stable.py",
        "line_number": 54,
        "API": ".tanh(",
        "context": [
            "        parameters=parameters,\n",
            "        name=name)\n",
            "\n",
            "  def _forward(self, x):\n",
            "    return tf.nn.tanh(x)\n",
            "\n",
            "  def _inverse(self, y):\n",
            "    # 0.99999997 is the maximum value such that atanh(x) is valid for both\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\tanh_bijector_stable.py",
        "line_number": 59,
        "API": ".abs(",
        "context": [
            "\n",
            "  def _inverse(self, y):\n",
            "    # 0.99999997 is the maximum value such that atanh(x) is valid for both\n",
            "    # tf.float32 and tf.float64\n",
            "    y = tf.where(tf.less_equal(tf.abs(y), 1.),\n",
            "                 tf.clip_by_value(y, -0.99999997, 0.99999997),\n",
            "                 y)\n",
            "    return tf.atanh(y)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\tanh_bijector_stable.py",
        "line_number": 66,
        "API": ".square(",
        "context": [
            "    return tf.atanh(y)\n",
            "\n",
            "  def _forward_log_det_jacobian(self, x):\n",
            "    #  This formula is mathematically equivalent to\n",
            "    #  `tf.log1p(-tf.square(tf.tanh(x)))`, however this code is more numerically\n",
            "    #  stable.\n",
            "\n",
            "    #  Derivation:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\tanh_bijector_stable.py",
        "line_number": 77,
        "API": ".log(",
        "context": [
            "    #    = 2 * log(2e^-x / (e^-2x + 1))\n",
            "    #    = 2 * (log(2) - x - log(e^-2x + 1))\n",
            "    #    = 2 * (log(2) - x - softplus(-2x))\n",
            "    return 2.0 * (\n",
            "        tf.math.log(tf.constant(2.0, dtype=x.dtype)) - x - tf.nn.softplus(\n",
            "            -2.0 * x))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 86,
        "API": ".format(",
        "context": [
            "    if not isinstance(\n",
            "        distribution,\n",
            "        (tfp.distributions.Normal, tfp.distributions.MultivariateNormalDiag)):\n",
            "      raise ValueError(\"Input distribution must be a normal distribution, \"\n",
            "                       \"got {} instead\".format(distribution))\n",
            "    self.action_means, self.action_magnitudes = common.spec_means_and_magnitudes(\n",
            "        spec)\n",
            "    self._distribution = distribution\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 127,
        "API": ".format(",
        "context": [
            "  def kl_divergence(self, other, name=\"kl_divergence\"):\n",
            "    \"\"\"Computes the KL Divergence between two SquashToSpecNormal distributions.\"\"\"\n",
            "    if not isinstance(other, SquashToSpecNormal):\n",
            "      raise ValueError(\"other distribution should be of type \"\n",
            "                       \"SquashToSpecNormal, got {}\".format(other))\n",
            "    if (np.any(self.action_means != other.action_means) or\n",
            "        np.any(self.action_magnitudes != other.action_magnitudes)):\n",
            "      raise ValueError(\"Other distribution does not have same action mean \"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 132,
        "API": ".format(",
        "context": [
            "    if (np.any(self.action_means != other.action_means) or\n",
            "        np.any(self.action_magnitudes != other.action_magnitudes)):\n",
            "      raise ValueError(\"Other distribution does not have same action mean \"\n",
            "                       \"and magnitude. This mean {}, this magnitude {}, \"\n",
            "                       \"other mean {}, other magnitude {}.\".format(\n",
            "                           self.action_means, self.action_magnitudes,\n",
            "                           other.action_means, other.action_magnitudes))\n",
            "    return self.input_distribution.kl_divergence(other.input_distribution, name)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 151,
        "API": ".tanh(",
        "context": [
            "    return self._squashed_distribution.prob(value, name)\n",
            "\n",
            "  def stddev(self, name=\"stddev\"):\n",
            "    \"\"\"Compute stddev of the SquashToSpecNormal distribution.\"\"\"\n",
            "    stddev = self.action_magnitudes * tf.tanh(self.input_distribution.stddev())\n",
            "    return stddev\n",
            "\n",
            "  def mode(self, name=\"mode\"):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 156,
        "API": ".tanh(",
        "context": [
            "    return stddev\n",
            "\n",
            "  def mode(self, name=\"mode\"):\n",
            "    \"\"\"Compute mean of the SquashToSpecNormal distribution.\"\"\"\n",
            "    mean = self.action_magnitudes * tf.tanh(self.input_distribution.mode()) + \\\n",
            "        self.action_means\n",
            "    return mean\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 241,
        "API": ".format(",
        "context": [
            "  type_: Type[Any]  # Any class that has a .parameters.\n",
            "  params: Mapping[Text, Any]\n",
            "\n",
            "  def __str__(self):\n",
            "    return \"<Params: type={}, params={}>\".format(self.type_, self.params)\n",
            "\n",
            "  def __repr__(self):\n",
            "    return str(self)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 278,
        "API": ".format(",
        "context": [
            "  parameters = getattr(value, \"parameters\", None)\n",
            "  if not isinstance(parameters, Mapping):\n",
            "    raise TypeError(\n",
            "        \"value.parameters is not available or is not a dict; \"\n",
            "        \"value: {}; parameters: {}\".format(value, parameters))\n",
            "  type_ = type(value)\n",
            "  params = {}\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 296,
        "API": ".map_structure(",
        "context": [
            "        for (k, v) in default_values\n",
            "        if v.default is not inspect.Parameter.empty\n",
            "    }\n",
            "    params = {\n",
            "        k: tf.nest.map_structure(process_parameter, v)\n",
            "        for k, v in value.parameters.items()\n",
            "        if v is not default_values.get(k, None)\n",
            "    }\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 354,
        "API": ".map_structure(",
        "context": [
            "  def make_from_params_or_identity(v_):\n",
            "    return make_from_parameters(v_) if isinstance(v_, Params) else v_\n",
            "\n",
            "  params = {\n",
            "      k: tf.nest.map_structure(make_from_params_or_identity, v)\n",
            "      for k, v in value.params.items()\n",
            "  }\n",
            "  return value.type_(**params)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 424,
        "API": ".is_tensor(",
        "context": [
            "  output_entries = {}\n",
            "\n",
            "  # All tensors and TensorSpecs will be included for purposes of this test.\n",
            "  is_tensor_or_spec = (\n",
            "      lambda p: tf.is_tensor(p) or isinstance(p, tf.TypeSpec))\n",
            "\n",
            "  def convert(key, p):\n",
            "    if isinstance(p, Params):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 436,
        "API": ".format(",
        "context": [
            "  for k, v in value.params.items():\n",
            "    if tf.nest.is_nested(v):\n",
            "      flattened_params = nest_utils.flatten_with_joined_paths(v)\n",
            "      for (param_k, param_v) in flattened_params:\n",
            "        key = \"{}:{}\".format(k, param_k)\n",
            "        convert(key, param_v)\n",
            "    else:\n",
            "      convert(k, v)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 502,
        "API": ".format(",
        "context": [
            "    converted = set()\n",
            "\n",
            "    def convert(params_k, p):\n",
            "      if params_k is not None:\n",
            "        params_key = \"{}:{}\".format(k, params_k)\n",
            "        visited.add(params_key)\n",
            "        params_dict_value = params_dict.get(params_key, None)\n",
            "        if params_dict_value is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 509,
        "API": ".get(",
        "context": [
            "        if params_dict_value is not None:\n",
            "          converted.add(params_key)\n",
            "      else:\n",
            "        params_key = k\n",
            "        params_dict_value = params_dict.get(k, None)\n",
            "      processed_params.add(params_key)\n",
            "      if isinstance(p, Params):\n",
            "        return merge_to_parameters_from_dict(p, params_dict_value)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 524,
        "API": ".format(",
        "context": [
            "        raise KeyError(\n",
            "            \"Only saw partial information from the dictionary for nested \"\n",
            "            \"key '{}' in params_dict.  Entries provided: {}.  \"\n",
            "            \"Entries required: {}\"\n",
            "            .format(k, sorted(converted), sorted(visited)))\n",
            "    else:\n",
            "      new_params[k] = convert(None, v)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 532,
        "API": ".format(",
        "context": [
            "  unvisited_params_keys = set(params_dict) - processed_params\n",
            "  if unvisited_params_keys:\n",
            "    raise ValueError(\n",
            "        \"params_dict had keys that were not part of value.params.  \"\n",
            "        \"params_dict keys: {}, value.params processed keys: {}\".format(\n",
            "            sorted(params_dict.keys()), sorted(processed_params)))\n",
            "\n",
            "  return Params(type_=value.type_, params=new_params)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 539,
        "API": ".flatten(",
        "context": [
            "  return Params(type_=value.type_, params=new_params)\n",
            "\n",
            "\n",
            "def _check_no_tensors(parameters: Params):\n",
            "  flat_params = tf.nest.flatten(parameters.params)\n",
            "  for p in flat_params:\n",
            "    if isinstance(p, Params):\n",
            "      _check_no_tensors(p)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 545,
        "API": ".format(",
        "context": [
            "    if isinstance(p, Params):\n",
            "      _check_no_tensors(p)\n",
            "    if tf.is_tensor(p):\n",
            "      raise TypeError(\n",
            "          \"Saw a `Tensor` value in parameters:\\n  {}\".format(parameters))\n",
            "\n",
            "\n",
            "class DistributionSpecV2(object):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 569,
        "API": ".is_tensor(",
        "context": [
            "      parameters: The recursive parameters of the distribution, with\n",
            "        tensors having directly been converted to `tf.TypeSpec` objects.\n",
            "\n",
            "    Raises:\n",
            "      TypeError: If for any entry `x` in `parameters`: `tf.is_tensor(x)`.\n",
            "    \"\"\"\n",
            "    _check_no_tensors(parameters)\n",
            "    self._event_shape = event_shape\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 601,
        "API": ".format(",
        "context": [
            "            and self._parameters == other._parameters)\n",
            "\n",
            "  def __str__(self):\n",
            "    return (\"<DistributionSpecV2: event_shape={}, dtype={}, parameters={}>\"\n",
            "            .format(self.event_shape, self.dtype, self.parameters))\n",
            "\n",
            "  def __repr__(self):\n",
            "    return str(self)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 626,
        "API": ".map_structure(",
        "context": [
            "  def to_event(s):\n",
            "    return (s.event_spec if isinstance(s, DistributionSpecV2)\n",
            "            else tensor_spec.from_spec(s))\n",
            "\n",
            "  event_spec = tf.nest.map_structure(to_event, network_output_spec)\n",
            "\n",
            "  nest_utils.assert_same_structure(\n",
            "      event_spec,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 631,
        "API": ".format(",
        "context": [
            "\n",
            "  nest_utils.assert_same_structure(\n",
            "      event_spec,\n",
            "      spec,\n",
            "      message=(\"{}:\\n{}\\nvs.\\n{}\".format(message_prefix, event_spec, spec)))\n",
            "\n",
            "  def compare_output_to_spec(s1, s2):\n",
            "    if not s1.is_compatible_with(s2):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\distributions\\utils.py",
        "line_number": 638,
        "API": ".map_structure(",
        "context": [
            "    if not s1.is_compatible_with(s2):\n",
            "      raise ValueError(\"{}:\\n{}\\nvs.\\n{}\".format(message_prefix, event_spec,\n",
            "                                                 spec))\n",
            "\n",
            "  tf.nest.map_structure(compare_output_to_spec, event_spec, spec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\drivers\\dynamic_episode_driver.py",
        "line_number": 112,
        "API": ".reduce_sum(",
        "context": [
            "\n",
            "      Returns:\n",
            "        tf.bool tensor, shape (), indicating whether while loop should continue.\n",
            "      \"\"\"\n",
            "      return tf.less(tf.reduce_sum(input_tensor=counter), num_episodes)\n",
            "\n",
            "    return loop_cond\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\drivers\\dynamic_episode_driver.py",
        "line_number": 139,
        "API": ".control_dependencies(",
        "context": [
            "      # TODO(b/134487572): TF2 while_loop seems to either ignore\n",
            "      # parallel_iterations or doesn't properly propagate control dependencies\n",
            "      # from one step to the next. Without this dep, self.env.step() is called\n",
            "      # in parallel.\n",
            "      with tf.control_dependencies(tf.nest.flatten([time_step])):\n",
            "        next_time_step = self.env.step(action_step.action)\n",
            "\n",
            "      policy_state = action_step.state\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\drivers\\dynamic_episode_driver.py",
        "line_number": 148,
        "API": ".shape(",
        "context": [
            "      if self._is_bandit_env:\n",
            "        # For Bandits we create episodes of length 1.\n",
            "        # Since the `next_time_step` is always of type LAST we need to replace\n",
            "        # the step type of the current `time_step` to FIRST.\n",
            "        batch_size = tf.shape(input=time_step.discount)\n",
            "        time_step = time_step._replace(\n",
            "            step_type=tf.fill(batch_size, ts.StepType.FIRST))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\drivers\\dynamic_episode_driver.py",
        "line_number": 158,
        "API": ".control_dependencies(",
        "context": [
            "      transition_observer_ops = [\n",
            "          observer((time_step, action_step, next_time_step))\n",
            "          for observer in self._transition_observers\n",
            "      ]\n",
            "      with tf.control_dependencies(\n",
            "          [tf.group(observer_ops + transition_observer_ops)]):\n",
            "        time_step, next_time_step, policy_state = tf.nest.map_structure(\n",
            "            tf.identity, (time_step, next_time_step, policy_state))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\drivers\\dynamic_episode_driver.py",
        "line_number": 166,
        "API": ".ones(",
        "context": [
            "\n",
            "      # While loop counter is only incremented for episode reset episodes.\n",
            "      # For Bandits, this is every trajectory, for MDPs, this is at boundaries.\n",
            "      if self._is_bandit_env:\n",
            "        counter += tf.ones(batch_size, dtype=tf.int32)\n",
            "      else:\n",
            "        counter += tf.cast(traj.is_boundary(), dtype=tf.int32)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\drivers\\dynamic_episode_driver.py",
        "line_number": 232,
        "API": ".zeros(",
        "context": [
            "    # Batch dim should be first index of tensors during data\n",
            "    # collection.\n",
            "    batch_dims = nest_utils.get_outer_shape(time_step,\n",
            "                                            self.env.time_step_spec())\n",
            "    counter = tf.zeros(batch_dims, tf.int32)\n",
            "\n",
            "    num_episodes = num_episodes or self._num_episodes\n",
            "    [_, time_step, policy_state] = tf.nest.map_structure(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\drivers\\dynamic_episode_driver.py",
        "line_number": 237,
        "API": ".while_loop(",
        "context": [
            "\n",
            "    num_episodes = num_episodes or self._num_episodes\n",
            "    [_, time_step, policy_state] = tf.nest.map_structure(\n",
            "        tf.stop_gradient,\n",
            "        tf.while_loop(\n",
            "            cond=self._loop_condition_fn(num_episodes),\n",
            "            body=self._loop_body_fn(),\n",
            "            loop_vars=[counter, time_step, policy_state],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\drivers\\dynamic_step_driver.py",
        "line_number": 112,
        "API": ".reduce_sum(",
        "context": [
            "\n",
            "      Returns:\n",
            "        tf.bool tensor, shape (), indicating whether while loop should continue.\n",
            "      \"\"\"\n",
            "      return tf.less(tf.reduce_sum(input_tensor=counter), self._num_steps)\n",
            "\n",
            "    return loop_cond\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\drivers\\dynamic_step_driver.py",
        "line_number": 141,
        "API": ".shape(",
        "context": [
            "      if self._is_bandit_env:\n",
            "        # For Bandits we create episodes of length 1.\n",
            "        # Since the `next_time_step` is always of type LAST we need to replace\n",
            "        # the step type of the current `time_step` to FIRST.\n",
            "        batch_size = tf.shape(input=time_step.discount)\n",
            "        time_step = time_step._replace(\n",
            "            step_type=tf.fill(batch_size, ts.StepType.FIRST)\n",
            "        )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\drivers\\dynamic_step_driver.py",
        "line_number": 151,
        "API": ".executing_eagerly(",
        "context": [
            "\n",
            "      observer_ops = []\n",
            "      for observer in self._observers:\n",
            "        observer(traj)\n",
            "        if not tf.executing_eagerly():\n",
            "          # Latest op in graph is the call op for above fn so get it.\n",
            "          observer_ops.append(\n",
            "              tf.compat.v1.get_default_graph().get_operations()[-1]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\drivers\\dynamic_step_driver.py",
        "line_number": 161,
        "API": ".control_dependencies(",
        "context": [
            "      transition_observer_ops = [\n",
            "          observer((time_step, action_step, next_time_step))\n",
            "          for observer in self._transition_observers\n",
            "      ]\n",
            "      with tf.control_dependencies(\n",
            "          [tf.group(observer_ops + transition_observer_ops)]\n",
            "      ):\n",
            "        time_step, next_time_step, policy_state = tf.nest.map_structure(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\drivers\\dynamic_step_driver.py",
        "line_number": 169,
        "API": ".cast(",
        "context": [
            "            tf.identity, (time_step, next_time_step, policy_state)\n",
            "        )\n",
            "\n",
            "      # While loop counter should not be incremented for episode reset steps.\n",
            "      counter += tf.cast(~traj.is_boundary(), dtype=tf.int32)\n",
            "\n",
            "      return [counter, next_time_step, policy_state]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\drivers\\dynamic_step_driver.py",
        "line_number": 210,
        "API": ".zeros(",
        "context": [
            "    # Batch dim should be first index of tensors during data collection.\n",
            "    batch_dims = nest_utils.get_outer_shape(\n",
            "        time_step, self.env.time_step_spec()\n",
            "    )\n",
            "    counter = tf.zeros(batch_dims, tf.int32)\n",
            "\n",
            "    [_, time_step, policy_state] = tf.nest.map_structure(\n",
            "        tf.stop_gradient,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\drivers\\py_driver.py",
        "line_number": 134,
        "API": ".sum(",
        "context": [
            "      for observer in self.info_observers:\n",
            "        observer(self.env.get_info())\n",
            "\n",
            "      if self._end_episode_on_boundary:\n",
            "        num_episodes += np.sum(traj.is_boundary())\n",
            "      else:\n",
            "        num_episodes += np.sum(traj.is_last())\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\drivers\\tf_driver.py",
        "line_number": 96,
        "API": ".function(",
        "context": [
            "    self._max_steps = max_steps or np.inf\n",
            "    self._max_episodes = max_episodes or np.inf\n",
            "\n",
            "    if not disable_tf_function:\n",
            "      self.run = common.function(self.run, autograph=True)\n",
            "\n",
            "  def run(  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks\n",
            "      self, time_step: ts.TimeStep,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\drivers\\tf_driver.py",
        "line_number": 111,
        "API": ".constant(",
        "context": [
            "\n",
            "    Returns:\n",
            "      A tuple (final time_step, final policy_state).\n",
            "    \"\"\"\n",
            "    num_steps = tf.constant(0.0)\n",
            "    num_episodes = tf.constant(0.0)\n",
            "\n",
            "    while num_steps < self._max_steps and num_episodes < self._max_episodes:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\drivers\\tf_driver.py",
        "line_number": 124,
        "API": ".reduce_sum(",
        "context": [
            "        observer((time_step, action_step, next_time_step))\n",
            "      for observer in self.observers:\n",
            "        observer(traj)\n",
            "\n",
            "      num_episodes += tf.math.reduce_sum(\n",
            "          tf.cast(traj.is_boundary(), tf.float32))\n",
            "      num_steps += tf.math.reduce_sum(tf.cast(~traj.is_boundary(), tf.float32))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\atari_preprocessing.py",
        "line_number": 85,
        "API": ".format(",
        "context": [
            "        dtype=np.uint8)\n",
            "\n",
            "    if frame_skip <= 0:\n",
            "      raise ValueError(\n",
            "          'Frame skip should be strictly positive, got {}'.format(frame_skip))\n",
            "    if screen_size <= 0:\n",
            "      raise ValueError('Target screen size should be strictly positive, got {}'\n",
            "                       .format(screen_size))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\atari_preprocessing.py",
        "line_number": 98,
        "API": ".empty(",
        "context": [
            "    obs_dims = self.env.observation_space\n",
            "    # Stores temporary observations used for pooling over two successive\n",
            "    # frames.\n",
            "    self.screen_buffer = [\n",
            "        np.empty((obs_dims.shape[0], obs_dims.shape[1]), dtype=np.uint8),\n",
            "        np.empty((obs_dims.shape[0], obs_dims.shape[1]), dtype=np.uint8)\n",
            "    ]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\atari_preprocessing.py",
        "line_number": 116,
        "API": ".fill(",
        "context": [
            "    self.env.reset()\n",
            "    self.lives = self.env.ale.lives()\n",
            "    self.game_over = False\n",
            "    self._fetch_grayscale_observation(self.screen_buffer[0])\n",
            "    self.screen_buffer[1].fill(0)\n",
            "    return self._pool_and_resize()\n",
            "\n",
            "  def step(self, action: np.ndarray) -> np.ndarray:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\atari_preprocessing.py",
        "line_number": 194,
        "API": ".maximum(",
        "context": [
            "      transformed_screen: numpy array, pooled, resized screen.\n",
            "    \"\"\"\n",
            "    # Pool if there are enough screens to do so.\n",
            "    if self.frame_skip > 1:\n",
            "      np.maximum(\n",
            "          self.screen_buffer[0],\n",
            "          self.screen_buffer[1],\n",
            "          out=self.screen_buffer[0])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\atari_preprocessing.py",
        "line_number": 203,
        "API": ".expand_dims(",
        "context": [
            "    transformed_image = cv2.resize(\n",
            "        self.screen_buffer[0], (self.screen_size, self.screen_size),\n",
            "        interpolation=cv2.INTER_AREA)\n",
            "    int_image = np.asarray(transformed_image, dtype=np.uint8)\n",
            "    return np.expand_dims(int_image, axis=2)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\batched_py_environment.py",
        "line_number": 190,
        "API": ".join(",
        "context": [
            "    \"\"\"Send close messages to the external process and join them.\"\"\"\n",
            "    self._execute(lambda env: env.close(), self._envs)\n",
            "    if self._parallel_execution:\n",
            "      self._pool.close()\n",
            "      self._pool.join()\n",
            "\n",
            "\n",
            "def unstack_actions(batched_actions: types.NestedArray) -> types.NestedArray:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\batched_py_environment.py",
        "line_number": 195,
        "API": ".flatten(",
        "context": [
            "\n",
            "\n",
            "def unstack_actions(batched_actions: types.NestedArray) -> types.NestedArray:\n",
            "  \"\"\"Returns a list of actions from potentially nested batch of actions.\"\"\"\n",
            "  flattened_actions = tf.nest.flatten(batched_actions)\n",
            "  unstacked_actions = [\n",
            "      tf.nest.pack_sequence_as(batched_actions, actions)\n",
            "      for actions in zip(*flattened_actions)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\dm_control_wrapper.py",
        "line_number": 47,
        "API": ".map_structure(",
        "context": [
            "  discount = time_step.discount\n",
            "  if discount is None:\n",
            "    discount = 1.0\n",
            "\n",
            "  observation = tf.nest.map_structure(_maybe_float32, time_step.observation)\n",
            "  return ts.TimeStep(\n",
            "      ts.StepType(time_step.step_type),\n",
            "      _as_float32_array(reward),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\dm_control_wrapper.py",
        "line_number": 75,
        "API": ".map_structure(",
        "context": [
            "    super(DmControlWrapper, self).__init__(env)\n",
            "    render_kwargs = render_kwargs or {}\n",
            "    self._render_kwargs = render_kwargs\n",
            "\n",
            "    self._observation_spec = tf.nest.map_structure(convert_spec,\n",
            "                                                   self._env.observation_spec())\n",
            "    self._action_spec = tf.nest.map_structure(convert_spec,\n",
            "                                              self._env.action_spec())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\dm_control_wrapper.py",
        "line_number": 88,
        "API": ".map_structure(",
        "context": [
            "  def _reset(self):\n",
            "    return convert_time_step(self._env.reset())\n",
            "\n",
            "  def _step(self, action):\n",
            "    action = tf.nest.map_structure(lambda a, s: np.asarray(a, dtype=s.dtype),\n",
            "                                   action, self._env.action_spec())\n",
            "    return convert_time_step(self._env.step(action))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\gym_wrapper.py",
        "line_number": 73,
        "API": ".item(",
        "context": [
            "  # We try to simplify redundant arrays to make logging and debugging less\n",
            "  # verbose and easier to read since the printed spec bounds may be large.\n",
            "  def try_simplify_array_to_value(np_array):\n",
            "    \"\"\"If given numpy array has all the same values, returns that value.\"\"\"\n",
            "    first_value = np_array.item(0)\n",
            "    if np.all(np_array == first_value):\n",
            "      return np.array(first_value, dtype=np_array.dtype)\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\gym_wrapper.py",
        "line_number": 90,
        "API": ".get(",
        "context": [
            "    # Discrete spaces span the set {0, 1, ... , n-1} while Bounded Array specs\n",
            "    # are inclusive on their bounds.\n",
            "    maximum = space.n - 1\n",
            "    # TODO(oars): change to use dtype in space once Gym is updated.\n",
            "    dtype = dtype_map.get(gym.spaces.Discrete, np.int64)\n",
            "    return specs.BoundedArraySpec(\n",
            "        shape=(), dtype=dtype, minimum=0, maximum=maximum, name=name)\n",
            "  elif isinstance(space, gym.spaces.MultiDiscrete):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\gym_wrapper.py",
        "line_number": 100,
        "API": ".get(",
        "context": [
            "        np.asarray(space.nvec - 1, dtype=dtype))\n",
            "    return specs.BoundedArraySpec(\n",
            "        shape=space.shape, dtype=dtype, minimum=0, maximum=maximum, name=name)\n",
            "  elif isinstance(space, gym.spaces.MultiBinary):\n",
            "    dtype = dtype_map.get(gym.spaces.MultiBinary, np.int32)\n",
            "    # Can remove this once we update gym.\n",
            "    if isinstance(space.n, int):\n",
            "      shape = (space.n,)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\gym_wrapper.py",
        "line_number": 112,
        "API": ".get(",
        "context": [
            "  elif isinstance(space, gym.spaces.Box):\n",
            "    if hasattr(space, 'dtype') and gym.spaces.Box not in dtype_map:\n",
            "      dtype = space.dtype\n",
            "    else:\n",
            "      dtype = dtype_map.get(gym.spaces.Box, np.float32)\n",
            "    if dtype == tf.string:\n",
            "      return specs.ArraySpec(shape=space.shape, dtype=dtype, name=name)\n",
            "    minimum = np.asarray(space.low, dtype=dtype)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\gym_wrapper.py",
        "line_number": 140,
        "API": ".format(",
        "context": [
            "        (key, nested_spec(s, key)) for key, s in space.spaces.items()\n",
            "    ])\n",
            "  else:\n",
            "    raise ValueError(\n",
            "        'The gym space {} is currently not supported.'.format(space))\n",
            "\n",
            "\n",
            "class GymWrapper(py_environment.PyEnvironment):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\gym_wrapper.py",
        "line_number": 172,
        "API": ".flatten(",
        "context": [
            "        'observation')\n",
            "    self._action_spec = spec_from_gym_space(self._gym_env.action_space,\n",
            "                                            spec_dtype_map, simplify_box_bounds,\n",
            "                                            'action')\n",
            "    self._flat_obs_spec = tf.nest.flatten(self._observation_spec)\n",
            "    self._render_kwargs = render_kwargs or {}\n",
            "    self._info = None\n",
            "    self._done = True\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\gym_wrapper.py",
        "line_number": 211,
        "API": ".item(",
        "context": [
            "    # transition probability so it has to be hashable. In the case of discrete\n",
            "    # actions we have a numpy scalar (e.g array(2)) which is not hashable\n",
            "    # in this case, we simply pull out the scalar value which will be hashable.\n",
            "    if self._action_is_discrete and isinstance(action, np.ndarray):\n",
            "      action = action.item()\n",
            "\n",
            "    # TODO(oars): Figure out how tuple or dict actions will be generated by the\n",
            "    # agents and if we can pass them through directly to gym.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\gym_wrapper.py",
        "line_number": 259,
        "API": ".seed(",
        "context": [
            "  def close(self) -> None:\n",
            "    return self._gym_env.close()\n",
            "\n",
            "  def seed(self, seed: types.Seed) -> types.Seed:\n",
            "    seed_value = self._gym_env.seed(seed)\n",
            "    if seed_value is None:\n",
            "      return 0\n",
            "    return seed_value\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\parallel_py_environment.py",
        "line_number": 82,
        "API": ".format(",
        "context": [
            "    if any([not callable(ctor) for ctor in env_constructors]):\n",
            "      raise TypeError(\n",
            "          'Found non-callable `env_constructors` in `ParallelPyEnvironment` '\n",
            "          '__init__ call. Did you accidentally pass in environment instances '\n",
            "          'instead of constructors? Got: {}'.format(env_constructors))\n",
            "    self._envs = [ProcessPyEnvironment(ctor, flatten=flatten)\n",
            "                  for ctor in env_constructors]\n",
            "    self._num_envs = len(env_constructors)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\parallel_py_environment.py",
        "line_number": 100,
        "API": ".info(",
        "context": [
            "      raise ValueError('All environments must have the same time_step_spec.')\n",
            "    self._flatten = flatten\n",
            "\n",
            "  def start(self) -> None:\n",
            "    logging.info('Spawning all processes.')\n",
            "    for env in self._envs:\n",
            "      env.start(wait_to_start=self._start_serially)\n",
            "    if not self._start_serially:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\parallel_py_environment.py",
        "line_number": 107,
        "API": ".info(",
        "context": [
            "    if not self._start_serially:\n",
            "      logging.info('Waiting for all processes to start.')\n",
            "      for env in self._envs:\n",
            "        env.wait_start()\n",
            "    logging.info('All processes started.')\n",
            "\n",
            "  @property\n",
            "  def batched(self) -> bool:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\parallel_py_environment.py",
        "line_number": 163,
        "API": ".info(",
        "context": [
            "    return self._stack_time_steps(time_steps)\n",
            "\n",
            "  def close(self) -> None:\n",
            "    \"\"\"Close all external process.\"\"\"\n",
            "    logging.info('Closing all processes.')\n",
            "    for env in self._envs:\n",
            "      env.close()\n",
            "    logging.info('All processes closed.')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\parallel_py_environment.py",
        "line_number": 172,
        "API": ".stack(",
        "context": [
            "  def _stack_time_steps(self, time_steps):\n",
            "    \"\"\"Given a list of TimeStep, combine to one with a batch dimension.\"\"\"\n",
            "    if self._flatten:\n",
            "      return nest_utils.fast_map_structure_flatten(\n",
            "          lambda *arrays: np.stack(arrays), self._time_step_spec, *time_steps)\n",
            "    else:\n",
            "      return nest_utils.fast_map_structure(\n",
            "          lambda *arrays: np.stack(arrays), *time_steps)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\parallel_py_environment.py",
        "line_number": 179,
        "API": ".flatten(",
        "context": [
            "          lambda *arrays: np.stack(arrays), *time_steps)\n",
            "\n",
            "  def _unstack_actions(self, batched_actions):\n",
            "    \"\"\"Returns a list of actions from potentially nested batch of actions.\"\"\"\n",
            "    flattened_actions = tf.nest.flatten(batched_actions)\n",
            "    if self._flatten:\n",
            "      unstacked_actions = zip(*flattened_actions)\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\parallel_py_environment.py",
        "line_number": 268,
        "API": ".get_context(",
        "context": [
            "\n",
            "    Args:\n",
            "      wait_to_start: Whether the call should wait for an env initialization.\n",
            "    \"\"\"\n",
            "    mp_context = multiprocessing.get_context()\n",
            "    self._conn, conn = mp_context.Pipe()\n",
            "    self._process = mp_context.Process(target=self._worker, args=(conn,))  # pytype: disable=attribute-error  # re-none\n",
            "    atexit.register(self.close)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\parallel_py_environment.py",
        "line_number": 281,
        "API": ".join(",
        "context": [
            "    \"\"\"Wait for the started process to finish initialization.\"\"\"\n",
            "    result = self._conn.recv()\n",
            "    if isinstance(result, Exception):\n",
            "      self._conn.close()\n",
            "      self._process.join(5)\n",
            "      raise result\n",
            "    assert result == self._READY, result\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\parallel_py_environment.py",
        "line_number": 366,
        "API": ".join(",
        "context": [
            "    except IOError:\n",
            "      # The connection was already closed.\n",
            "      pass\n",
            "    if self._process.is_alive():\n",
            "      self._process.join(5)\n",
            "\n",
            "  def step(self,\n",
            "           action: types.NestedArray,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\parallel_py_environment.py",
        "line_number": 445,
        "API": ".format(",
        "context": [
            "      raise Exception(stacktrace)\n",
            "    if message == self._RESULT:\n",
            "      return payload\n",
            "    self.close()\n",
            "    raise KeyError('Received message of unexpected type {}'.format(message))\n",
            "\n",
            "  def _worker(self, conn):\n",
            "    \"\"\"The process waits for actions and sends back environment results.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\parallel_py_environment.py",
        "line_number": 479,
        "API": ".flatten(",
        "context": [
            "          if self._flatten and name == 'step':\n",
            "            args = [tf.nest.pack_sequence_as(action_spec, args[0])]\n",
            "          result = getattr(env, name)(*args, **kwargs)\n",
            "          if self._flatten and name in ['step', 'reset']:\n",
            "            result = tf.nest.flatten(result)\n",
            "          conn.send((self._RESULT, result))\n",
            "          continue\n",
            "        if message == self._CLOSE:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\parallel_py_environment.py",
        "line_number": 486,
        "API": ".format(",
        "context": [
            "        if message == self._CLOSE:\n",
            "          assert payload is None\n",
            "          env.close()\n",
            "          break\n",
            "        raise KeyError('Received message of unknown type {}'.format(message))\n",
            "    except Exception:  # pylint: disable=broad-except\n",
            "      etype, evalue, tb = sys.exc_info()\n",
            "      stacktrace = ''.join(traceback.format_exception(etype, evalue, tb))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\parallel_py_environment.py",
        "line_number": 491,
        "API": ".error(",
        "context": [
            "    except Exception:  # pylint: disable=broad-except\n",
            "      etype, evalue, tb = sys.exc_info()\n",
            "      stacktrace = ''.join(traceback.format_exception(etype, evalue, tb))\n",
            "      message = 'Error in environment process: {}'.format(stacktrace)\n",
            "      logging.error(message)\n",
            "      conn.send((self._EXCEPTION, stacktrace))\n",
            "    finally:\n",
            "      conn.close()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\py_environment.py",
        "line_number": 78,
        "API": ".all(",
        "context": [
            "\n",
            "    When batched, the left-most dimension is not part of the action_spec\n",
            "    or the observation_spec and corresponds to the batch dimension.\n",
            "\n",
            "    When batched and handle_auto_reset, it checks `np.all(steps.is_last())`.\n",
            "\n",
            "    Returns:\n",
            "      A boolean indicating whether the environment is batched or not.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\py_environment.py",
        "line_number": 115,
        "API": ".all(",
        "context": [
            "    Returns:\n",
            "      A bool indicating whether the Environment should reset or not.\n",
            "    \"\"\"\n",
            "    handle_auto_reset = getattr(self, '_handle_auto_reset', False)\n",
            "    return handle_auto_reset and np.all(current_time_step.is_last())\n",
            "\n",
            "  @abc.abstractmethod\n",
            "  def observation_spec(self) -> types.NestedArraySpec:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\py_to_dm_wrapper.py",
        "line_number": 65,
        "API": ".map_structure(",
        "context": [
            "    self._environment = env\n",
            "    if env.batched:\n",
            "      raise NotImplementedError(\n",
            "          'Batched environments cannot be converted to dm environments.')\n",
            "    self._observation_spec = tree.map_structure(_convert_spec,\n",
            "                                                env.observation_spec())\n",
            "    self._action_spec = tree.map_structure(_convert_spec, env.action_spec())\n",
            "    self._discount_spec = tree.map_structure(_convert_spec, env.discount_spec())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\random_py_environment.py",
        "line_number": 91,
        "API": ".tile(",
        "context": [
            "    discount = np.asarray(discount, dtype=np.float32)\n",
            "\n",
            "    if self._batch_size:\n",
            "      if not discount.shape:\n",
            "        discount = np.tile(discount, self._batch_size)\n",
            "      if self._batch_size != len(discount):\n",
            "        raise ValueError('Size of discounts must equal the batch size.')\n",
            "    self._discount = discount\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\random_py_environment.py",
        "line_number": 102,
        "API": ".zeros(",
        "context": [
            "      if self._batch_size is None:\n",
            "        self._reward_fn = lambda *_: np.asarray(0.0, dtype=np.float32)\n",
            "      else:\n",
            "        self._reward_fn = (\n",
            "            lambda *_: np.zeros(self._batch_size, dtype=np.float32))\n",
            "    else:\n",
            "      self._reward_fn = reward_fn\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\random_py_environment.py",
        "line_number": 155,
        "API": ".uniform(",
        "context": [
            "      self._done = False\n",
            "    elif self._max_duration and self._num_steps >= self._max_duration:\n",
            "      self._done = True\n",
            "    else:\n",
            "      self._done = self._rng.uniform() < self._episode_end_probability\n",
            "\n",
            "    if self._done:\n",
            "      reward = self._reward_fn(ts.StepType.LAST, action, observation)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\random_py_environment.py",
        "line_number": 178,
        "API": ".seed(",
        "context": [
            "\n",
            "    return self._rng.randint(0, 256, size=self._render_size, dtype=np.uint8)\n",
            "\n",
            "  def seed(self, seed: types.Seed) -> None:\n",
            "    self._rng.seed(seed)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\random_tf_environment.py",
        "line_number": 73,
        "API": ".map_structure(",
        "context": [
            "        time_step_spec, variables)\n",
            "\n",
            "  def _current_time_step(self):\n",
            "    \"\"\"Returns the current `TimeStep`.\"\"\"\n",
            "    return tf.nest.map_structure(tf.identity, self._time_step_variables)\n",
            "\n",
            "  def _update_time_step(self, time_step):\n",
            "    tf.nest.map_structure(lambda var, value: var.assign(value),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\random_tf_environment.py",
        "line_number": 95,
        "API": ".function(",
        "context": [
            "        obs, self._batch_size, reward_spec=self._time_step_spec.reward)\n",
            "    self._update_time_step(time_step)\n",
            "    return self._current_time_step()\n",
            "\n",
            "  @common.function(autograph=True)\n",
            "  def _step(self, action):\n",
            "    \"\"\"Steps the environment according to the action.\"\"\"\n",
            "    # Make sure the given action is compatible with the spec. We compare it to\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\random_tf_environment.py",
        "line_number": 100,
        "API": ".map_structure(",
        "context": [
            "  def _step(self, action):\n",
            "    \"\"\"Steps the environment according to the action.\"\"\"\n",
            "    # Make sure the given action is compatible with the spec. We compare it to\n",
            "    # t[0] as the spec doesn't have a batch dim.\n",
            "    tf.nest.map_structure(\n",
            "        lambda spec, t: tf.Assert(spec.is_compatible_with(t[0]), [t]),\n",
            "        self._action_spec, action)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\random_tf_environment.py",
        "line_number": 111,
        "API": ".uniform(",
        "context": [
            "      return self.reset()\n",
            "\n",
            "    obs, reward = self._sample_obs_and_reward()\n",
            "    # Note: everything in the batch terminates at the same time.\n",
            "    if tf.random.uniform(()) < self._episode_end_probability:\n",
            "      time_step = ts.termination(obs, reward)\n",
            "    else:\n",
            "      time_step = ts.transition(obs, reward)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\suite_atari.py",
        "line_number": 43,
        "API": ".constant(",
        "context": [
            "# As soon as this functionality in TF-Agents is ready and verified, this set of\n",
            "# wrappers will be removed.\n",
            "DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING = DEFAULT_ATARI_GYM_WRAPPERS + (\n",
            "    atari_wrappers.FrameStack4,)\n",
            "gin.constant('DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING',\n",
            "             DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\suite_atari.py",
        "line_number": 67,
        "API": ".format(",
        "context": [
            "  assert obs_type in ['image', 'ram']\n",
            "  assert mode in ['', 'NoFrameskip', 'Deterministic']\n",
            "  assert version in ['v0', 'v4']\n",
            "  if obs_type == 'ram':\n",
            "    name = '{}-ram'.format(name)\n",
            "  return '{}{}-{}'.format(name, mode, version)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\suite_dm_control.py",
        "line_number": 81,
        "API": ".load(",
        "context": [
            "  \"\"\"\n",
            "\n",
            "  if not is_available():\n",
            "    raise ImportError('dm_control module is not available.')\n",
            "  return suite.load(\n",
            "      domain_name,\n",
            "      task_name,\n",
            "      task_kwargs=task_kwargs,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\suite_mujoco.py",
        "line_number": 89,
        "API": ".load(",
        "context": [
            "  \"\"\"\n",
            "  if spec_dtype_map is None:\n",
            "    # Use float32 for Observations.\n",
            "    spec_dtype_map = {gym.spaces.Box: np.float32}\n",
            "  return suite_gym.load(environment_name, discount, max_episode_steps,\n",
            "                        gym_env_wrappers, env_wrappers, spec_dtype_map)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\tf_environment.py",
        "line_number": 96,
        "API": ".control_dependencies(",
        "context": [
            "    tf_env = TFEnvironment()\n",
            "\n",
            "    # Action needs to depend on the time_step using control_dependencies.\n",
            "    time_step = tf_env.current_time_step()\n",
            "    with tf.control_dependencies([time_step.step_type]):\n",
            "      action = tensor_spec.sample_bounded_spec(tf_env.action_spec())\n",
            "    next_time_step = tf_env.step(action)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\tf_environment.py",
        "line_number": 108,
        "API": ".logical_not(",
        "context": [
            "    tf_env = TFEnvironment()\n",
            "\n",
            "    # reset() creates the initial time_step\n",
            "    time_step = tf_env.reset()\n",
            "    c = lambda t: tf.logical_not(t.is_last())\n",
            "    body = lambda t: [tf_env.step(t.observation)]\n",
            "\n",
            "    final_time_step = tf.while_loop(c, body, [time_step])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\tf_py_environment.py",
        "line_number": 41,
        "API": ".flatten(",
        "context": [
            "\n",
            "def _pack_named_sequence(flat_inputs, input_spec, batch_shape):\n",
            "  \"\"\"Assembles back a nested structure that has been flattened.\"\"\"\n",
            "  named_inputs = []\n",
            "  for flat_input, spec in zip(flat_inputs, tf.nest.flatten(input_spec)):\n",
            "    named_input = tf.identity(flat_input, name=spec.name)\n",
            "    if not tf.executing_eagerly():\n",
            "      named_input.set_shape(batch_shape.concatenate(spec.shape))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\tf_py_environment.py",
        "line_number": 134,
        "API": ".format(",
        "context": [
            "      self._pool = pool.ThreadPool(1)\n",
            "    else:\n",
            "      raise TypeError(\n",
            "          'isolation should be True, False, or an instance of '\n",
            "          'a multiprocessing Pool or ThreadPool.  Saw: {}'.format(isolation))\n",
            "\n",
            "    if callable(environment):\n",
            "      environment = self._execute(environment)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\tf_py_environment.py",
        "line_number": 151,
        "API": ".warning(",
        "context": [
            "    self._env = environment\n",
            "    self._check_dims = check_dims\n",
            "\n",
            "    if isolation and getattr(self._env, '_parallel_execution', None):\n",
            "      logging.warning(\n",
            "          'Wrapped environment is executing in parallel.  '\n",
            "          'Perhaps it is a BatchedPyEnvironment with multithreading=True, '\n",
            "          'or it is a ParallelPyEnvironment.  This conflicts with the '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\tf_py_environment.py",
        "line_number": 170,
        "API": ".flatten(",
        "context": [
            "                                          batch_size)\n",
            "\n",
            "    # Gather all the dtypes and shapes of the elements in time_step.\n",
            "    self._time_step_dtypes = [\n",
            "        s.dtype for s in tf.nest.flatten(self.time_step_spec())\n",
            "    ]\n",
            "\n",
            "    self._time_step = None\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\tf_py_environment.py",
        "line_number": 199,
        "API": ".join(",
        "context": [
            "    Only closes pool when `isolation` was provided at init time.\n",
            "    \"\"\"\n",
            "    self._env.close()\n",
            "    if self._pool:\n",
            "      self._pool.join()\n",
            "      self._pool.close()\n",
            "      self._pool = None\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\tf_py_environment.py",
        "line_number": 230,
        "API": ".flatten(",
        "context": [
            "    def _current_time_step_py():\n",
            "      with _check_not_called_concurrently(self._lock):\n",
            "        if self._time_step is None:\n",
            "          self._time_step = self._env.reset()\n",
            "        return tf.nest.flatten(self._time_step)\n",
            "\n",
            "    def _isolated_current_time_step_py():\n",
            "      return self._execute(_current_time_step_py)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\tf_py_environment.py",
        "line_number": 269,
        "API": ".control_dependencies(",
        "context": [
            "          _isolated_reset_py,\n",
            "          [],  # No inputs.\n",
            "          [],\n",
            "          name='reset_py_func')\n",
            "      with tf.control_dependencies([reset_op]):\n",
            "        return self.current_time_step()\n",
            "\n",
            "  def _step(self, actions):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\tf_py_environment.py",
        "line_number": 298,
        "API": ".flatten(",
        "context": [
            "      with _check_not_called_concurrently(self._lock):\n",
            "        packed = tf.nest.pack_sequence_as(\n",
            "            structure=self.action_spec(), flat_sequence=flattened_actions)\n",
            "        self._time_step = self._env.step(packed)\n",
            "        return tf.nest.flatten(self._time_step)\n",
            "\n",
            "    def _isolated_step_py(*flattened_actions):\n",
            "      return self._execute(_step_py, *flattened_actions)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\tf_py_environment.py",
        "line_number": 304,
        "API": ".identity(",
        "context": [
            "    def _isolated_step_py(*flattened_actions):\n",
            "      return self._execute(_step_py, *flattened_actions)\n",
            "\n",
            "    with tf.name_scope('step'):\n",
            "      flat_actions = [tf.identity(x) for x in tf.nest.flatten(actions)]\n",
            "      if self._check_dims:\n",
            "        for action in flat_actions:\n",
            "          dim_value = tensor_shape.dimension_value(action.shape[0])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\tf_py_environment.py",
        "line_number": 359,
        "API": ".zeros(",
        "context": [
            "        return img\n",
            "      elif mode == 'human':\n",
            "        # Generate mock img to keep outputs the same.\n",
            "        self._env.render(mode)\n",
            "        return np.zeros(self._render_shape, dtype=np.uint8)\n",
            "\n",
            "    img = tf.numpy_function(\n",
            "        lambda mode: self._execute(_render, mode), [mode], [tf.uint8],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\tf_py_environment.py",
        "line_number": 365,
        "API": ".executing_eagerly(",
        "context": [
            "    img = tf.numpy_function(\n",
            "        lambda mode: self._execute(_render, mode), [mode], [tf.uint8],\n",
            "        name='render_py_func')\n",
            "\n",
            "    if not tf.executing_eagerly():\n",
            "      # Extract from list returned from np_function.\n",
            "      img = img[0]\n",
            "      img.set_shape(tf.TensorShape(self._render_shape))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\tf_wrappers.py",
        "line_number": 85,
        "API": ".format(",
        "context": [
            "    def _validate(action_spec):\n",
            "      if action_spec.dtype.is_integer and len(action_spec.shape.as_list()) > 1:\n",
            "        raise ValueError(\n",
            "            'OneHotActionWrapper only supports actions with at most one '\n",
            "            'dimension! action_spec: {}'.format(action_spec))\n",
            "\n",
            "    tf.nest.map_structure(_validate, self._env.action_spec())\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\tf_wrappers.py",
        "line_number": 106,
        "API": ".map_structure(",
        "context": [
            "            name='one_hot_action_spec')\n",
            "      else:\n",
            "        return action_spec\n",
            "\n",
            "    return tf.nest.map_structure(convert_to_one_hot, self._env.action_spec())\n",
            "\n",
            "  def _step(self, action):\n",
            "    action = tf.argmax(action, axis=-1, output_type=action.dtype)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\trajectory_replay.py",
        "line_number": 97,
        "API": ".format(",
        "context": [
            "\n",
            "    if tf.compat.dimension_value(outer_dims.shape[0]) != 2:\n",
            "      raise ValueError(\n",
            "          \"Expected two outer dimensions, but saw '{}' dimensions.\\n\"\n",
            "          \"Trajectory:\\n{}.\\nTrajectory spec from policy:\\n{}.\".format(\n",
            "              tf.compat.dimension_value(outer_dims.shape[0]), trajectory,\n",
            "              trajectory_spec))\n",
            "    if self._time_major:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\trajectory_replay.py",
        "line_number": 119,
        "API": ".map_structure(",
        "context": [
            "                                       self._policy.policy_state_spec)\n",
            "\n",
            "    if not self._time_major:\n",
            "      # Make trajectory time-major.\n",
            "      trajectory = tf.nest.map_structure(common.transpose_batch_time,\n",
            "                                         trajectory)\n",
            "\n",
            "    trajectory_tas = tf.nest.map_structure(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\trajectory_replay.py",
        "line_number": 132,
        "API": ".map_structure(",
        "context": [
            "          spec.dtype, size=sequence_length,\n",
            "          element_shape=(tf.TensorShape([static_batch_size])\n",
            "                         .concatenate(spec.shape)))\n",
            "\n",
            "    output_action_tas = tf.nest.map_structure(create_output_ta,\n",
            "                                              trajectory_spec.action)\n",
            "    output_policy_info_tas = tf.nest.map_structure(create_output_ta,\n",
            "                                                   trajectory_spec.policy_info)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\trajectory_replay.py",
        "line_number": 138,
        "API": ".zeros_like(",
        "context": [
            "    output_policy_info_tas = tf.nest.map_structure(create_output_ta,\n",
            "                                                   trajectory_spec.policy_info)\n",
            "\n",
            "    read0 = lambda ta: ta.read(0)\n",
            "    zeros_like0 = lambda t: tf.zeros_like(t[0])\n",
            "    ones_like0 = lambda t: tf.ones_like(t[0])\n",
            "    time_step = ts.TimeStep(\n",
            "        step_type=read0(trajectory_tas.step_type),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\trajectory_replay.py",
        "line_number": 144,
        "API": ".map_structure(",
        "context": [
            "    time_step = ts.TimeStep(\n",
            "        step_type=read0(trajectory_tas.step_type),\n",
            "        reward=tf.nest.map_structure(zeros_like0, trajectory.reward),\n",
            "        discount=ones_like0(trajectory.discount),\n",
            "        observation=tf.nest.map_structure(read0, trajectory_tas.observation))\n",
            "\n",
            "    def process_step(time, time_step, policy_state,\n",
            "                     output_action_tas, output_policy_info_tas):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\trajectory_replay.py",
        "line_number": 167,
        "API": ".write(",
        "context": [
            "        next_output_policy_info_tas: Updated `output_policy_info_tas`.\n",
            "      \"\"\"\n",
            "      action_step = self._policy.action(time_step, policy_state)\n",
            "      policy_state = action_step.state\n",
            "      write_ta = lambda ta, t: ta.write(time - 1, t)\n",
            "      next_output_action_tas = tf.nest.map_structure(\n",
            "          write_ta, output_action_tas, action_step.action)\n",
            "      next_output_policy_info_tas = tf.nest.map_structure(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\trajectory_replay.py",
        "line_number": 203,
        "API": ".map_structure(",
        "context": [
            "      ta_read = lambda ta: ta.read(time)\n",
            "      ta_read_prev = lambda ta: ta.read(time - 1)\n",
            "      time_step = ts.TimeStep(\n",
            "          step_type=ta_read(trajectory_tas.step_type),\n",
            "          observation=tf.nest.map_structure(ta_read,\n",
            "                                            trajectory_tas.observation),\n",
            "          reward=tf.nest.map_structure(ta_read_prev, trajectory_tas.reward),\n",
            "          discount=ta_read_prev(trajectory_tas.discount))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\trajectory_replay.py",
        "line_number": 211,
        "API": ".constant(",
        "context": [
            "\n",
            "      return (time + 1, time_step, policy_state,\n",
            "              next_output_action_tas, next_output_policy_info_tas)\n",
            "\n",
            "    time = tf.constant(1)\n",
            "    time, time_step, policy_state, output_action_tas, output_policy_info_tas = (\n",
            "        tf.while_loop(\n",
            "            cond=lambda time, *_: time < sequence_length,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\trajectory_replay.py",
        "line_number": 227,
        "API": ".stack(",
        "context": [
            "        process_step(time, time_step, policy_state,\n",
            "                     output_action_tas, output_policy_info_tas))\n",
            "\n",
            "    def stack_ta(ta):\n",
            "      t = ta.stack()\n",
            "      if not self._time_major:\n",
            "        t = common.transpose_batch_time(t)\n",
            "      return t\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\trajectory_replay.py",
        "line_number": 232,
        "API": ".map_structure(",
        "context": [
            "      if not self._time_major:\n",
            "        t = common.transpose_batch_time(t)\n",
            "      return t\n",
            "\n",
            "    stacked_output_actions = tf.nest.map_structure(stack_ta, output_action_tas)\n",
            "    stacked_output_policy_info = tf.nest.map_structure(stack_ta,\n",
            "                                                       output_policy_info_tas)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\utils.py",
        "line_number": 84,
        "API": ".sum(",
        "context": [
            "\n",
            "    action = random_policy.action(time_step).action\n",
            "    time_step = environment.step(action)\n",
            "\n",
            "    episode_count += np.sum(time_step.is_last())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 86,
        "API": ".seed(",
        "context": [
            "  def render(self, mode: Text = 'rgb_array') -> types.NestedArray:\n",
            "    return self._env.render(mode)\n",
            "\n",
            "  def seed(self, seed: types.Seed) -> types.Seed:\n",
            "    return self._env.seed(seed)\n",
            "\n",
            "  def wrapped_env(self) -> Any:\n",
            "    return self._env\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 253,
        "API": ".format(",
        "context": [
            "    \"\"\"\n",
            "    super(ActionRepeat, self).__init__(env)\n",
            "    if times <= 1:\n",
            "      raise ValueError(\n",
            "          'Times parameter ({}) should be greater than 1'.format(times))\n",
            "    self._times = times\n",
            "\n",
            "  def _step(self, action):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 290,
        "API": ".flatten(",
        "context": [
            "        set.\n",
            "    \"\"\"\n",
            "    super(FlattenActionWrapper, self).__init__(env)\n",
            "    self._original_action_spec = env.action_spec()\n",
            "    flat_action_spec = tf.nest.flatten(env.action_spec())\n",
            "\n",
            "    if any([len(s.shape) > 1 for s in flat_action_spec]):\n",
            "      raise ValueError('ActionSpec shapes should all have ndim == 1.')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 306,
        "API": ".broadcast_to(",
        "context": [
            "\n",
            "    if all(\n",
            "        [isinstance(s, array_spec.BoundedArraySpec) for s in flat_action_spec]):\n",
            "      minimums = [\n",
            "          np.broadcast_to(s.minimum, shape=s.shape) for s in flat_action_spec\n",
            "      ]\n",
            "      maximums = [\n",
            "          np.broadcast_to(s.maximum, shape=s.shape) for s in flat_action_spec\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 339,
        "API": ".array(",
        "context": [
            "\n",
            "      if s.shape is ():  # pylint: disable=literal-comparison\n",
            "        current_action = current_action[0]\n",
            "\n",
            "      split_actions.append(np.array(current_action, s.dtype))\n",
            "      start = end\n",
            "\n",
            "    structured_action = tf.nest.pack_sequence_as(self._original_action_spec,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 373,
        "API": ".array(",
        "context": [
            "      ValueError: If no index is provided.\n",
            "      ValueError: If one of the indexes is out of bounds.\n",
            "    \"\"\"\n",
            "    super(ObservationFilterWrapper, self).__init__(env)\n",
            "    idx = np.array(idx)\n",
            "    if tf.nest.is_nested(env.observation_spec()):\n",
            "      raise ValueError('ObservationFilterWrapper only works with single-array '\n",
            "                       'observations (not nested).')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 382,
        "API": ".all(",
        "context": [
            "      raise ValueError('ObservationFilterWrapper only works with '\n",
            "                       'single-dimensional indexes for filtering.')\n",
            "    if idx.shape[0] < 1:\n",
            "      raise ValueError('At least one index needs to be provided for filtering.')\n",
            "    if not np.all(idx < env.observation_spec().shape[0]):\n",
            "      raise ValueError('One of the indexes is out of bounds.')\n",
            "\n",
            "    self._idx = idx\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 391,
        "API": ".array(",
        "context": [
            "\n",
            "  def _step(self, action):\n",
            "    time_step = self._env.step(action)\n",
            "    return time_step._replace(observation=\n",
            "                              np.array(time_step.observation)[self._idx])\n",
            "\n",
            "  def observation_spec(self) -> types.NestedArraySpec:\n",
            "    return self._observation_spec\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 399,
        "API": ".array(",
        "context": [
            "\n",
            "  def _reset(self):\n",
            "    time_step = self._env.reset()\n",
            "    return time_step._replace(observation=\n",
            "                              np.array(time_step.observation)[self._idx])\n",
            "\n",
            "\n",
            "@gin.configurable\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 480,
        "API": ".flatten(",
        "context": [
            "      ValueError: IF the action_spec shape and the limits shape are not equal.\n",
            "    \"\"\"\n",
            "    super(ActionDiscretizeWrapper, self).__init__(env)\n",
            "\n",
            "    action_spec = tf.nest.flatten(env.action_spec())\n",
            "    if len(action_spec) != 1:\n",
            "      raise ValueError(\n",
            "          'ActionDiscretizeWrapper only supports environments with a single '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 488,
        "API": ".broadcast_to(",
        "context": [
            "          'action spec. Got {}'.format(env.action_spec()))\n",
            "\n",
            "    action_spec = action_spec[0]\n",
            "    self._original_spec = action_spec\n",
            "    self._num_actions = np.broadcast_to(num_actions, action_spec.shape)\n",
            "\n",
            "    if action_spec.shape != self._num_actions.shape:\n",
            "      raise ValueError('Spec {} and limit shape do not match. Got {}'.format(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 509,
        "API": ".all(",
        "context": [
            "      Tuple with the discrete_spec along with a list of lists mapping actions.\n",
            "    Raises:\n",
            "      ValueError: If not all limits value are >=2.\n",
            "    \"\"\"\n",
            "    if not np.all(limits >= 2):\n",
            "      raise ValueError('num_actions should all be at least size 2.')\n",
            "\n",
            "    limits = np.asarray(limits)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 514,
        "API": ".all(",
        "context": [
            "      raise ValueError('num_actions should all be at least size 2.')\n",
            "\n",
            "    limits = np.asarray(limits)\n",
            "    # Simplify shape of bounds if they are all equal.\n",
            "    if np.all(limits == limits.flat[0]):\n",
            "      discrete_spec_max_limit = limits.flat[0]\n",
            "    else:\n",
            "      discrete_spec_max_limit = limits\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 528,
        "API": ".broadcast_to(",
        "context": [
            "        minimum=0,\n",
            "        maximum=discrete_spec_max_limit - 1,\n",
            "        name=spec.name)\n",
            "\n",
            "    minimum = np.broadcast_to(spec.minimum, shape)\n",
            "    maximum = np.broadcast_to(spec.maximum, shape)\n",
            "\n",
            "    action_map = [\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 558,
        "API": ".format(",
        "context": [
            "    \"\"\"\n",
            "    action = np.asarray(action)\n",
            "    if action.shape != self._discrete_spec.shape:\n",
            "      raise ValueError(\n",
            "          'Received action with incorrect shape. Got {}, expected {}'.format(\n",
            "              action.shape, self._discrete_spec.shape))\n",
            "\n",
            "    mapped_action = [action_map[i][a] for i, a in enumerate(action.flatten())]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 632,
        "API": ".zeros(",
        "context": [
            "                       'action specs.')\n",
            "\n",
            "  def action_spec(self) -> types.NestedArraySpec:\n",
            "    spec = self._env.action_spec()\n",
            "    minimum = np.zeros(shape=spec.shape, dtype=spec.dtype)\n",
            "    maximum = spec.maximum - spec.minimum\n",
            "    return array_spec.BoundedArraySpec(spec.shape, spec.dtype, minimum=minimum,\n",
            "                                       maximum=maximum)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 728,
        "API": ".prod(",
        "context": [
            "\n",
            "    # Compute the observation length after flattening the observation items and\n",
            "    # nested structure. Observation specs are not batched.\n",
            "    observation_total_len = sum(\n",
            "        int(np.prod(observation.shape))\n",
            "        for observation in self._flatten_nested_observations(\n",
            "            observations_spec, is_batched=False))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 799,
        "API": ".reshape(",
        "context": [
            "    def np_flatten(x):\n",
            "      # Check if observations are batch, and if so keep the batch dimension and\n",
            "      # flatten the all other dimensions into one.\n",
            "      if is_batched:\n",
            "        return np.reshape(x, [x.shape[0], -1])\n",
            "      else:\n",
            "        return np.reshape(x, [-1])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 805,
        "API": ".flatten(",
        "context": [
            "        return np.reshape(x, [-1])\n",
            "\n",
            "    # Flatten the individual observations if they are multi-dimensional and then\n",
            "    # flatten the nested structure.\n",
            "    flat_observations = [np_flatten(x) for x in tf.nest.flatten(observations)]\n",
            "    axis = 1 if is_batched else 0\n",
            "    return np.concatenate(flat_observations, axis=axis)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 959,
        "API": ".map_structure(",
        "context": [
            "\n",
            "    def _update_shape(spec):\n",
            "      return spec.replace(shape=(self._history_length,) + spec.shape)\n",
            "\n",
            "    observation_spec = tf.nest.map_structure(_update_shape,\n",
            "                                             self._env.observation_spec())\n",
            "\n",
            "    if self._include_actions:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 975,
        "API": ".zeros(",
        "context": [
            "\n",
            "  def _zeros_from_spec(self, spec):\n",
            "\n",
            "    def _zeros(spec):\n",
            "      return np.zeros(spec.shape, dtype=spec.dtype)\n",
            "\n",
            "    return tf.nest.map_structure(_zeros, spec)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 999,
        "API": ".copy(",
        "context": [
            "    time_step = self._env.reset()\n",
            "\n",
            "    if self._tile_first_step_obs:\n",
            "      self._observation_history.extend([\n",
            "          time_step.observation.copy() for _ in range(self._history_length - 1)\n",
            "      ])\n",
            "    else:\n",
            "      self._observation_history.extend([self._zero_observation] *\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 1029,
        "API": ".format(",
        "context": [
            "      \"\"\"Convert spec to one_hot format.\"\"\"\n",
            "      if np.issubdtype(spec.dtype, np.integer):\n",
            "        if len(spec.shape) > 1:\n",
            "          raise ValueError('OneHotActionWrapper only supports single action!'\n",
            "                           'action_spec: {}'.format(spec))\n",
            "\n",
            "        num_actions = spec.maximum - spec.minimum + 1\n",
            "        output_shape = spec.shape + (num_actions,)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 1043,
        "API": ".map_structure(",
        "context": [
            "            name='one_hot_action_spec')\n",
            "      else:\n",
            "        return spec\n",
            "\n",
            "    self._one_hot_action_spec = tf.nest.map_structure(\n",
            "        convert_to_one_hot, self._env.action_spec())\n",
            "\n",
            "  def action_spec(self) -> types.NestedArraySpec:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 1055,
        "API": ".format(",
        "context": [
            "    def convert_back(action, inner_spec, spec):\n",
            "      if action.shape != inner_spec.shape or action.dtype != inner_spec.dtype:\n",
            "        raise ValueError('Action shape/dtype different from its definition in '\n",
            "                         'the inner_spec. Action: {action}. Inner_spec: '\n",
            "                         '{spec}.'.format(action=action, spec=spec))\n",
            "      if np.issubdtype(action.dtype, np.integer):\n",
            "        action = spec.minimum + np.argmax(action, axis=-1)\n",
            "      return action\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 1060,
        "API": ".map_structure(",
        "context": [
            "      if np.issubdtype(action.dtype, np.integer):\n",
            "        action = spec.minimum + np.argmax(action, axis=-1)\n",
            "      return action\n",
            "\n",
            "    action = tf.nest.map_structure(\n",
            "        convert_back, action, self._one_hot_action_spec,\n",
            "        self._env.action_spec())\n",
            "    return self._env.step(action)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\wrappers.py",
        "line_number": 1100,
        "API": ".array(",
        "context": [
            "    mask_spec = array_spec.ArraySpec(\n",
            "        shape=[self._action_spec.maximum - self._action_spec.minimum + 1],\n",
            "        dtype=np.int64)\n",
            "    self._masked_observation_spec = (env.observation_spec(), mask_spec)\n",
            "    self._constant_mask = np.array(\n",
            "        [[1] * (orig_action_spec.maximum - orig_action_spec.minimum + 1) +\n",
            "         [0] * num_extra_actions] * self.batch_size)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\examples\\masked_cartpole.py",
        "line_number": 39,
        "API": ".array(",
        "context": [
            "  \"\"\"\n",
            "\n",
            "  def __init__(self):\n",
            "    super(MaskedCartPoleEnv, self).__init__()\n",
            "    high = np.array([\n",
            "        self.x_threshold * 2,\n",
            "        self.theta_threshold_radians * 2,\n",
            "    ])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\examples\\tic_tac_toe_environment.py",
        "line_number": 67,
        "API": ".zeros(",
        "context": [
            "  def observation_spec(self):\n",
            "    return BoundedArraySpec((3, 3), np.int32, minimum=0, maximum=2)\n",
            "\n",
            "  def _reset(self):\n",
            "    self._states = np.zeros((3, 3), np.int32)\n",
            "    return TimeStep(StepType.FIRST, np.asarray(0.0, dtype=np.float32),\n",
            "                    self._discount, self._states)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\examples\\tic_tac_toe_environment.py",
        "line_number": 72,
        "API": ".where(",
        "context": [
            "    return TimeStep(StepType.FIRST, np.asarray(0.0, dtype=np.float32),\n",
            "                    self._discount, self._states)\n",
            "\n",
            "  def _legal_actions(self, states: np.ndarray):\n",
            "    return list(zip(*np.where(states == 0)))\n",
            "\n",
            "  def _opponent_play(self, states: np.ndarray):\n",
            "    actions = self._legal_actions(np.array(states))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\examples\\tic_tac_toe_environment.py",
        "line_number": 114,
        "API": ".all(",
        "context": [
            "\n",
            "    is_final, reward = self._check_states(self._states)\n",
            "\n",
            "    step_type = StepType.MID\n",
            "    if np.all(self._states == 0):\n",
            "      step_type = StepType.FIRST\n",
            "    elif is_final:\n",
            "      step_type = StepType.LAST\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\environments\\examples\\tic_tac_toe_environment.py",
        "line_number": 132,
        "API": ".array(",
        "context": [
            "      A tuple of (is_final, reward) where is_final means whether the states\n",
            "      are final are not, and reward is the reward for stepping into the states\n",
            "      The meaning of reward: 0 = not decided or draw, 1 = win, -1 = loss\n",
            "    \"\"\"\n",
            "    seqs = np.array([\n",
            "        # each row\n",
            "        states[0, :], states[1, :], states[2, :],\n",
            "        # each column\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\eval\\metric_utils.py",
        "line_number": 45,
        "API": ".format(",
        "context": [
            "    return collections.OrderedDict(results)\n",
            "\n",
            "\n",
            "def log_metrics(metrics, prefix=''):\n",
            "  log = ['{0} = {1}'.format(m.name, m.result()) for m in metrics]\n",
            "  logging.info('%s \\n\\t\\t %s', prefix, '\\n\\t\\t '.join(log))\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\eval\\metric_utils.py",
        "line_number": 114,
        "API": ".format(",
        "context": [
            "  results = compute(metrics, environment, policy, num_episodes)\n",
            "  if tf_summaries:\n",
            "    py_metric.run_summaries(metrics)\n",
            "  if log:\n",
            "    log_metrics(metrics, prefix='Step = {}'.format(global_step))\n",
            "  if callback is not None:\n",
            "    callback(results, global_step)\n",
            "  return results\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\eval\\metric_utils.py",
        "line_number": 162,
        "API": ".function(",
        "context": [
            "      policy,\n",
            "      observers=metrics,\n",
            "      num_episodes=num_episodes)\n",
            "  if use_function:\n",
            "    common.function(driver.run)(time_step, policy_state)\n",
            "  else:\n",
            "    driver.run(time_step, policy_state)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\eval\\metric_utils.py",
        "line_number": 172,
        "API": ".scalar(",
        "context": [
            "  if train_step is not None and summary_writer:\n",
            "    with summary_writer.as_default():\n",
            "      for m in metrics:\n",
            "        tag = common.join_scope(summary_prefix, m.name)\n",
            "        tf.compat.v2.summary.scalar(name=tag, data=m.result(), step=train_step)\n",
            "  # TODO(b/130249101): Add an option to log metrics.\n",
            "  return collections.OrderedDict(results)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\cql_sac_train_eval.py",
        "line_number": 58,
        "API": ".getenv(",
        "context": [
            "from tf_agents.typing import types\n",
            "\n",
            "FLAGS = flags.FLAGS\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "_REVERB_PORT = flags.DEFINE_integer(\n",
            "    'reverb_port', None,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\cql_sac_train_eval.py",
        "line_number": 192,
        "API": ".info(",
        "context": [
            "    summarize_grads_and_vars: If True, gradient and network variable summaries\n",
            "      will be written during training.\n",
            "    seed: Optional seed for tf.random.\n",
            "  \"\"\"\n",
            "  logging.info('Training CQL-SAC on: %s', env_name)\n",
            "  tf.random.set_seed(seed)\n",
            "  np.random.seed(seed)\n",
            "  # Load environment.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\cql_sac_train_eval.py",
        "line_number": 249,
        "API": ".clip_by_value(",
        "context": [
            "        An RLDS step after applying action clipping and reward shift.\n",
            "      \"\"\"\n",
            "      rlds_step[rlds.REWARD] = rlds_step[rlds.REWARD] + reward_shift\n",
            "      if action_clipping:\n",
            "        rlds_step[rlds.ACTION] = tf.clip_by_value(\n",
            "            rlds_step[rlds.ACTION],\n",
            "            clip_value_min=action_clipping[0],\n",
            "            clip_value_max=action_clipping[1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\cql_sac_train_eval.py",
        "line_number": 326,
        "API": ".join(",
        "context": [
            "        train_step_counter=train_step)\n",
            "    agent.initialize()\n",
            "\n",
            "  # Create learner.\n",
            "  saved_model_dir = os.path.join(root_dir, learner.POLICY_SAVED_MODEL_DIR)\n",
            "  collect_env_step_metric = py_metrics.EnvironmentSteps()\n",
            "  learning_triggers = [\n",
            "      triggers.PolicySavedModelTrigger(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\cql_sac_train_eval.py",
        "line_number": 357,
        "API": ".join(",
        "context": [
            "      env,\n",
            "      eval_greedy_policy,\n",
            "      train_step,\n",
            "      metrics=actor.eval_metrics(eval_episodes),\n",
            "      summary_dir=os.path.join(root_dir, 'eval'),\n",
            "      episodes_per_run=eval_episodes)\n",
            "\n",
            "  # Run.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\cql_sac_train_eval.py",
        "line_number": 375,
        "API": ".set_verbosity(",
        "context": [
            "      eval_actor.run_and_log()\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  logging.set_verbosity(logging.INFO)\n",
            "\n",
            "  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_param)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\data_utils.py",
        "line_number": 36,
        "API": ".clip_by_value(",
        "context": [
            "                   use_trajectories: bool):\n",
            "  \"\"\"Create a sample with reward_shift and action_clipping.\"\"\"\n",
            "\n",
            "  def _clip_actions(actions):\n",
            "    return tf.clip_by_value(\n",
            "        actions,\n",
            "        clip_value_min=action_clipping[0],\n",
            "        clip_value_max=action_clipping[1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\data_utils.py",
        "line_number": 46,
        "API": ".map_structure(",
        "context": [
            "    # Update trajectory.\n",
            "    shifted_reward = sample.reward + reward_shift\n",
            "    if action_clipping:\n",
            "      return sample._replace(\n",
            "          action=tf.nest.map_structure(_clip_actions, sample.action),\n",
            "          reward=shifted_reward)\n",
            "    else:\n",
            "      return sample._replace(reward=shifted_reward)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\data_utils.py",
        "line_number": 61,
        "API": ".map_structure(",
        "context": [
            "        observation=next_time_step.observation)\n",
            "    action_step = sample.action_step\n",
            "    if action_clipping:\n",
            "      action_step = action_step._replace(\n",
            "          action=tf.nest.map_structure(_clip_actions, action_step.action))\n",
            "    return trajectory.Transition(\n",
            "        time_step=sample.time_step,\n",
            "        action_step=action_step,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\data_utils.py",
        "line_number": 171,
        "API": ".array(",
        "context": [
            "  initial_len = len(filenames)\n",
            "  remainder = initial_len % num_shards\n",
            "  for _ in range(num_shards - remainder):\n",
            "    filenames.append(filenames[np.random.randint(low=0, high=initial_len)])\n",
            "  filenames = np.array(filenames)\n",
            "  np.random.shuffle(filenames)\n",
            "  filenames = np.array_split(filenames, num_shards)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\data_utils.py",
        "line_number": 176,
        "API": ".repeat(",
        "context": [
            "  np.random.shuffle(filenames)\n",
            "  filenames = np.array_split(filenames, num_shards)\n",
            "\n",
            "  record_file_ds = tf.data.Dataset.from_tensor_slices(filenames)\n",
            "  record_file_ds = record_file_ds.repeat().shuffle(len(filenames))\n",
            "\n",
            "  spec_path = filenames[0][0] + '.spec'\n",
            "  record_spec = example_encoding_dataset.parse_encoded_spec_from_file(spec_path)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\data_utils.py",
        "line_number": 202,
        "API": ".batch(",
        "context": [
            "\n",
            "  use_tpu = isinstance(\n",
            "      strategy,\n",
            "      (tf.distribute.experimental.TPUStrategy, tf.distribute.TPUStrategy))\n",
            "  example_ds = example_ds.batch(\n",
            "      batch_size, drop_remainder=use_tpu).prefetch(num_prefetch)\n",
            "  return example_ds\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\dataset\\dataset_generator.py",
        "line_number": 51,
        "API": ".set_verbosity(",
        "context": [
            "FLAGS = flags.FLAGS\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  logging.set_verbosity(logging.INFO)\n",
            "\n",
            "  d4rl_env = gym.make(FLAGS.env_name)\n",
            "  d4rl_dataset = d4rl_env.get_dataset()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\dataset\\dataset_generator.py",
        "line_number": 62,
        "API": ".info(",
        "context": [
            "      d4rl_dataset,\n",
            "      FLAGS.exclude_timeouts,\n",
            "      observation_dtype=d4rl_env.observation_space.dtype)\n",
            "  num_episodes = len(dataset_dict['episode_start_index'])\n",
            "  logging.info('Found %d episodes, %s total steps.', num_episodes,\n",
            "               len(dataset_dict['states']))\n",
            "\n",
            "  collect_data_spec = dataset_utils.create_collect_data_spec(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\dataset\\dataset_generator.py",
        "line_number": 67,
        "API": ".info(",
        "context": [
            "               len(dataset_dict['states']))\n",
            "\n",
            "  collect_data_spec = dataset_utils.create_collect_data_spec(\n",
            "      dataset_dict, use_trajectories=FLAGS.use_trajectories)\n",
            "  logging.info('Collect data spec %s', collect_data_spec)\n",
            "\n",
            "  num_replicas = FLAGS.replicas or 1\n",
            "  interval_size = num_episodes // num_replicas + 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\dataset\\dataset_generator.py",
        "line_number": 81,
        "API": ".join(",
        "context": [
            "    end_index = min((FLAGS.replica_id + 1) * interval_size, num_episodes)\n",
            "    file_utils.write_samples_to_tfrecord(\n",
            "        dataset_dict=dataset_dict,\n",
            "        collect_data_spec=collect_data_spec,\n",
            "        dataset_path=os.path.join(root_dir, file_name),\n",
            "        start_episode=start_index,\n",
            "        end_episode=end_index,\n",
            "        use_trajectories=FLAGS.use_trajectories)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\dataset\\dataset_generator.py",
        "line_number": 88,
        "API": ".get_context(",
        "context": [
            "        use_trajectories=FLAGS.use_trajectories)\n",
            "  else:\n",
            "    # Otherwise, parallelize with tf_agents.system.multiprocessing.\n",
            "    jobs = []\n",
            "    context = multiprocessing.get_context()\n",
            "\n",
            "    for i in range(num_replicas):\n",
            "      if num_replicas == 1:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\dataset\\dataset_generator.py",
        "line_number": 95,
        "API": ".join(",
        "context": [
            "      if num_replicas == 1:\n",
            "        file_name = '%s.tfrecord' % FLAGS.env_name\n",
            "      else:\n",
            "        file_name = '%s_%d.tfrecord' % (FLAGS.env_name, i)\n",
            "      dataset_path = os.path.join(root_dir, file_name)\n",
            "      start_index = i * interval_size\n",
            "      end_index = min((i + 1) * interval_size, num_episodes)\n",
            "      kwargs = dict(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\dataset\\dataset_generator.py",
        "line_number": 111,
        "API": ".join(",
        "context": [
            "      job.start()\n",
            "      jobs.append(job)\n",
            "\n",
            "    for job in jobs:\n",
            "      job.join()\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\dataset\\file_utils.py",
        "line_number": 56,
        "API": ".zeros_like(",
        "context": [
            ") -> trajectory.Transition:\n",
            "  \"\"\"Creates a Transition from current and next state information.\"\"\"\n",
            "  tfagents_time_step = ts.TimeStep(\n",
            "      step_type=step_type,\n",
            "      reward=np.zeros_like(reward),  # unknown\n",
            "      discount=np.zeros_like(discount),  # unknown\n",
            "      observation=state)\n",
            "  action_step = policy_step.PolicyStep(action=action, state=(), info=())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\dataset\\file_utils.py",
        "line_number": 107,
        "API": ".zeros_like(",
        "context": [
            "\n",
            "      # Set next state.\n",
            "      # If at the last step in the episode, create a dummy next step.\n",
            "      if step_type == ts.StepType.LAST:\n",
            "        next_state = np.zeros_like(states[step_i])\n",
            "        next_step_type = ts.StepType.FIRST\n",
            "      else:\n",
            "        next_state = states[step_i + 1]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\cql_sac\\kumar20\\dataset\\file_utils.py",
        "line_number": 135,
        "API": ".info(",
        "context": [
            "            next_step_type=next_step_type)\n",
            "      tfrecord_observer(sample)\n",
            "\n",
            "  tfrecord_observer.close()\n",
            "  logging.info('Wrote episodes [%d-%d] to %s', start_episode, end_episode,\n",
            "               dataset_path)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\dqn\\dqn_train_eval.py",
        "line_number": 56,
        "API": ".getenv(",
        "context": [
            "from tf_agents.utils import common\n",
            "\n",
            "FLAGS = flags.FLAGS\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_integer(\n",
            "    'reverb_port', None,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\dqn\\dqn_train_eval.py",
        "line_number": 95,
        "API": ".load(",
        "context": [
            "    policy_save_interval=1000,\n",
            "    eval_interval=1000,\n",
            "    eval_episodes=10):\n",
            "  \"\"\"Trains and evaluates DQN.\"\"\"\n",
            "  collect_env = suite_gym.load(env_name)\n",
            "  eval_env = suite_gym.load(env_name)\n",
            "\n",
            "  time_step_tensor_spec = tensor_spec.from_spec(collect_env.time_step_spec())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\dqn\\dqn_train_eval.py",
        "line_number": 163,
        "API": ".join(",
        "context": [
            "      num_parallel_calls=3, sample_batch_size=batch_size,\n",
            "      num_steps=2).prefetch(3)\n",
            "  experience_dataset_fn = lambda: dataset\n",
            "\n",
            "  saved_model_dir = os.path.join(root_dir, learner.POLICY_SAVED_MODEL_DIR)\n",
            "  env_step_metric = py_metrics.EnvironmentSteps()\n",
            "\n",
            "  learning_triggers = [\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\dqn\\dqn_train_eval.py",
        "line_number": 193,
        "API": ".info(",
        "context": [
            "      random_policy,\n",
            "      train_step,\n",
            "      steps_per_run=initial_collect_steps,\n",
            "      observers=[rb_observer])\n",
            "  logging.info('Doing initial collect.')\n",
            "  initial_collect_actor.run()\n",
            "\n",
            "  tf_collect_policy = agent.collect_policy\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\dqn\\dqn_train_eval.py",
        "line_number": 207,
        "API": ".join(",
        "context": [
            "      train_step,\n",
            "      steps_per_run=1,\n",
            "      observers=[rb_observer, env_step_metric],\n",
            "      metrics=actor.collect_metrics(10),\n",
            "      summary_dir=os.path.join(root_dir, learner.TRAIN_DIR),\n",
            "  )\n",
            "\n",
            "  tf_greedy_policy = agent.policy\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\dqn\\dqn_train_eval.py",
        "line_number": 220,
        "API": ".join(",
        "context": [
            "      greedy_policy,\n",
            "      train_step,\n",
            "      episodes_per_run=eval_episodes,\n",
            "      metrics=actor.eval_metrics(eval_episodes),\n",
            "      summary_dir=os.path.join(root_dir, 'eval'),\n",
            "  )\n",
            "\n",
            "  if eval_interval:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\dqn\\dqn_train_eval.py",
        "line_number": 227,
        "API": ".info(",
        "context": [
            "  if eval_interval:\n",
            "    logging.info('Evaluating.')\n",
            "    eval_actor.run_and_log()\n",
            "\n",
            "  logging.info('Training.')\n",
            "  for _ in range(num_iterations):\n",
            "    collect_actor.run()\n",
            "    dqn_learner.run(iterations=1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\dqn\\dqn_train_eval.py",
        "line_number": 233,
        "API": ".info(",
        "context": [
            "    collect_actor.run()\n",
            "    dqn_learner.run(iterations=1)\n",
            "\n",
            "    if eval_interval and dqn_learner.train_step_numpy % eval_interval == 0:\n",
            "      logging.info('Evaluating.')\n",
            "      eval_actor.run_and_log()\n",
            "\n",
            "  rb_observer.close()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\dqn\\dqn_train_eval.py",
        "line_number": 241,
        "API": ".set_verbosity(",
        "context": [
            "  reverb_server.stop()\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  logging.set_verbosity(logging.INFO)\n",
            "  tf.enable_v2_behavior()\n",
            "\n",
            "  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_bindings)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\dqn\\dqn_train_eval_rnn.py",
        "line_number": 62,
        "API": ".getenv(",
        "context": [
            "from tf_agents.utils import common\n",
            "\n",
            "FLAGS = flags.FLAGS\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_string('env_name', None, 'Name of the environment.')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\dqn\\dqn_train_eval_rnn.py",
        "line_number": 124,
        "API": ".load(",
        "context": [
            "    eval_interval=1000,\n",
            "    eval_episodes=10):\n",
            "  \"\"\"Trains and evaluates DQN.\"\"\"\n",
            "\n",
            "  collect_env = suite_gym.load(env_name)\n",
            "  eval_env = suite_gym.load(env_name)\n",
            "\n",
            "  unused_observation_tensor_spec, action_tensor_spec, time_step_tensor_spec = (\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\dqn\\dqn_train_eval_rnn.py",
        "line_number": 176,
        "API": ".join(",
        "context": [
            "  def experience_dataset_fn():\n",
            "    return reverb_replay.as_dataset(\n",
            "        sample_batch_size=batch_size, num_steps=sequence_length)\n",
            "\n",
            "  saved_model_dir = os.path.join(root_dir, learner.POLICY_SAVED_MODEL_DIR)\n",
            "  env_step_metric = py_metrics.EnvironmentSteps()\n",
            "\n",
            "  learning_triggers = [\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\dqn\\dqn_train_eval_rnn.py",
        "line_number": 220,
        "API": ".join(",
        "context": [
            "      train_step,\n",
            "      steps_per_run=collect_steps_per_iteration,\n",
            "      observers=[rb_observer, env_step_metric],\n",
            "      metrics=actor.collect_metrics(10),\n",
            "      summary_dir=os.path.join(root_dir, learner.TRAIN_DIR),\n",
            "  )\n",
            "\n",
            "  tf_greedy_policy = agent.policy\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\dqn\\dqn_train_eval_rnn.py",
        "line_number": 268,
        "API": ".set_verbosity(",
        "context": [
            "        scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  logging.set_verbosity(logging.INFO)\n",
            "  tf.enable_v2_behavior()\n",
            "\n",
            "  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_bindings)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\dqn\\mnih15\\dqn_train_eval_atari.py",
        "line_number": 132,
        "API": ".load(",
        "context": [
            "    eval_episodes=30,\n",
            "    debug_summaries=True):\n",
            "  \"\"\"Trains and evaluates DQN.\"\"\"\n",
            "\n",
            "  collect_env = suite_atari.load(\n",
            "      env_name,\n",
            "      max_episode_steps=max_episode_frames_collect,\n",
            "      gym_env_wrappers=suite_atari.DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\dqn\\mnih15\\dqn_train_eval_atari.py",
        "line_number": 195,
        "API": ".join(",
        "context": [
            "  dataset = reverb_replay.as_dataset(\n",
            "      sample_batch_size=batch_size, num_steps=2).prefetch(3)\n",
            "  experience_dataset_fn = lambda: dataset\n",
            "\n",
            "  saved_model_dir = os.path.join(root_dir, learner.POLICY_SAVED_MODEL_DIR)\n",
            "  env_step_metric = py_metrics.EnvironmentSteps()\n",
            "\n",
            "  learning_triggers = [\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\dqn\\mnih15\\dqn_train_eval_atari.py",
        "line_number": 240,
        "API": ".join(",
        "context": [
            "      steps_per_run=update_frequency,\n",
            "      observers=[rb_observer, env_step_metric],\n",
            "      metrics=actor.collect_metrics(10),\n",
            "      reference_metrics=[env_step_metric],\n",
            "      summary_dir=os.path.join(root_dir, learner.TRAIN_DIR),\n",
            "  )\n",
            "\n",
            "  tf_greedy_policy = agent.policy\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\dqn\\mnih15\\dqn_train_eval_atari.py",
        "line_number": 254,
        "API": ".join(",
        "context": [
            "      train_step,\n",
            "      episodes_per_run=eval_episodes,\n",
            "      metrics=actor.eval_metrics(eval_episodes),\n",
            "      reference_metrics=[env_step_metric],\n",
            "      summary_dir=os.path.join(root_dir, 'eval'),\n",
            "  )\n",
            "\n",
            "  if eval_interval:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\ppo\\schulman17\\ppo_clip_train_eval.py",
        "line_number": 34,
        "API": ".getenv(",
        "context": [
            "from tf_agents.examples.ppo.schulman17 import train_eval_lib\n",
            "\n",
            "FLAGS = flags.FLAGS\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_integer(\n",
            "    'reverb_port', None,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\ppo\\schulman17\\ppo_clip_train_eval.py",
        "line_number": 77,
        "API": ".set_verbosity(",
        "context": [
            "      eval_interval=eval_interval)\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  logging.set_verbosity(logging.INFO)\n",
            "  tf.enable_v2_behavior()\n",
            "\n",
            "  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_bindings)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\ppo\\schulman17\\train_eval_lib.py",
        "line_number": 171,
        "API": ".load(",
        "context": [
            "    eval_episodes: Number of episodes to evaluate over.\n",
            "    debug_summaries: Boolean for whether to gather debug summaries.\n",
            "    summarize_grads_and_vars: If true, gradient summaries will be written.\n",
            "  \"\"\"\n",
            "  collect_env = suite_mujoco.load(env_name)\n",
            "  eval_env = suite_mujoco.load(env_name)\n",
            "  num_environments = 1\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\ppo\\schulman17\\train_eval_lib.py",
        "line_number": 249,
        "API": ".format(",
        "context": [
            "  reverb_replay_train = reverb_replay_buffer.ReverbReplayBuffer(\n",
            "      agent.collect_data_spec,\n",
            "      sequence_length=collect_sequence_length,\n",
            "      table_name='training_table',\n",
            "      server_address='localhost:{}'.format(reverb_server.port),\n",
            "      # The only collected sequence is used to populate the batches.\n",
            "      max_cycle_length=1,\n",
            "      rate_limiter_timeout_ms=1000)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\ppo\\schulman17\\train_eval_lib.py",
        "line_number": 257,
        "API": ".format(",
        "context": [
            "  reverb_replay_normalization = reverb_replay_buffer.ReverbReplayBuffer(\n",
            "      agent.collect_data_spec,\n",
            "      sequence_length=collect_sequence_length,\n",
            "      table_name='normalization_table',\n",
            "      server_address='localhost:{}'.format(reverb_server.port),\n",
            "      # The only collected sequence is used to populate the batches.\n",
            "      max_cycle_length=1,\n",
            "      rate_limiter_timeout_ms=1000)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\ppo\\schulman17\\train_eval_lib.py",
        "line_number": 267,
        "API": ".join(",
        "context": [
            "      reverb_replay_train.py_client, ['training_table', 'normalization_table'],\n",
            "      sequence_length=collect_sequence_length,\n",
            "      stride_length=collect_sequence_length)\n",
            "\n",
            "  saved_model_dir = os.path.join(root_dir, learner.POLICY_SAVED_MODEL_DIR)\n",
            "  collect_env_step_metric = py_metrics.EnvironmentSteps()\n",
            "  learning_triggers = [\n",
            "      triggers.PolicySavedModelTrigger(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\ppo\\schulman17\\train_eval_lib.py",
        "line_number": 315,
        "API": ".join(",
        "context": [
            "      steps_per_run=collect_sequence_length,\n",
            "      observers=[rb_observer],\n",
            "      metrics=actor.collect_metrics(buffer_size=10) + [collect_env_step_metric],\n",
            "      reference_metrics=[collect_env_step_metric],\n",
            "      summary_dir=os.path.join(root_dir, learner.TRAIN_DIR),\n",
            "      summary_interval=summary_interval)\n",
            "\n",
            "  eval_greedy_policy = py_tf_eager_policy.PyTFEagerPolicy(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\ppo\\schulman17\\train_eval_lib.py",
        "line_number": 322,
        "API": ".info(",
        "context": [
            "  eval_greedy_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
            "      agent.policy, use_tf_function=True)\n",
            "\n",
            "  if eval_interval:\n",
            "    logging.info('Intial evaluation.')\n",
            "    eval_actor = actor.Actor(\n",
            "        eval_env,\n",
            "        eval_greedy_policy,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\ppo\\schulman17\\train_eval_lib.py",
        "line_number": 329,
        "API": ".join(",
        "context": [
            "        eval_greedy_policy,\n",
            "        train_step,\n",
            "        metrics=actor.eval_metrics(eval_episodes),\n",
            "        reference_metrics=[collect_env_step_metric],\n",
            "        summary_dir=os.path.join(root_dir, 'eval'),\n",
            "        episodes_per_run=eval_episodes)\n",
            "\n",
            "    eval_actor.run_and_log()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\ppo\\schulman17\\train_eval_lib.py",
        "line_number": 334,
        "API": ".info(",
        "context": [
            "        episodes_per_run=eval_episodes)\n",
            "\n",
            "    eval_actor.run_and_log()\n",
            "\n",
            "  logging.info('Training on %s', env_name)\n",
            "  last_eval_step = 0\n",
            "  for i in range(num_iterations):\n",
            "    collect_actor.run()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\ppo\\schulman17\\train_eval_lib.py",
        "line_number": 359,
        "API": ".info(",
        "context": [
            "    # does not return after every train step.\n",
            "    if (eval_interval and\n",
            "        (agent_learner.train_step_numpy >= eval_interval + last_eval_step\n",
            "         or i == num_iterations - 1)):\n",
            "      logging.info('Evaluating.')\n",
            "      eval_actor.run_and_log()\n",
            "      last_eval_step = agent_learner.train_step_numpy\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\sac\\haarnoja18\\sac_train_eval.py",
        "line_number": 52,
        "API": ".getenv(",
        "context": [
            "from tf_agents.train.utils import train_utils\n",
            "\n",
            "FLAGS = flags.FLAGS\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_integer(\n",
            "    'reverb_port', None,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\sac\\haarnoja18\\sac_train_eval.py",
        "line_number": 136,
        "API": ".map_structure(",
        "context": [
            "\n",
            "def create_sequential_actor_network(actor_fc_layers, action_tensor_spec):\n",
            "  \"\"\"Create a sequential actor network.\"\"\"\n",
            "  def tile_as_nest(non_nested_output):\n",
            "    return tf.nest.map_structure(lambda _: non_nested_output,\n",
            "                                 action_tensor_spec)\n",
            "\n",
            "  return sequential.Sequential(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\sac\\haarnoja18\\sac_train_eval.py",
        "line_number": 143,
        "API": ".map_structure(",
        "context": [
            "  return sequential.Sequential(\n",
            "      [dense(num_units) for num_units in actor_fc_layers] +\n",
            "      [tf.keras.layers.Lambda(tile_as_nest)] + [\n",
            "          nest_map.NestMap(\n",
            "              tf.nest.map_structure(_TanhNormalProjectionNetworkWrapper,\n",
            "                                    action_tensor_spec))\n",
            "      ])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\sac\\haarnoja18\\sac_train_eval.py",
        "line_number": 180,
        "API": ".info(",
        "context": [
            "    eval_episodes=30,\n",
            "    debug_summaries=False,\n",
            "    summarize_grads_and_vars=False):\n",
            "  \"\"\"Trains and evaluates SAC.\"\"\"\n",
            "  logging.info('Training SAC on: %s', env_name)\n",
            "  collect_env = suite_mujoco.load(env_name)\n",
            "  eval_env = suite_mujoco.load(env_name)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\sac\\haarnoja18\\sac_train_eval.py",
        "line_number": 227,
        "API": ".join(",
        "context": [
            "      sampler=reverb.selectors.Uniform(),\n",
            "      remover=reverb.selectors.Fifo(),\n",
            "      rate_limiter=reverb.rate_limiters.MinSize(1))\n",
            "\n",
            "  reverb_checkpoint_dir = os.path.join(root_dir, learner.TRAIN_DIR,\n",
            "                                       learner.REPLAY_BUFFER_CHECKPOINT_DIR)\n",
            "  reverb_checkpointer = reverb.platform.checkpointers_lib.DefaultCheckpointer(\n",
            "      path=reverb_checkpoint_dir)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\sac\\haarnoja18\\sac_train_eval.py",
        "line_number": 249,
        "API": ".join(",
        "context": [
            "  def experience_dataset_fn():\n",
            "    return reverb_replay.as_dataset(\n",
            "        sample_batch_size=batch_size, num_steps=2).prefetch(50)\n",
            "\n",
            "  saved_model_dir = os.path.join(root_dir, learner.POLICY_SAVED_MODEL_DIR)\n",
            "  env_step_metric = py_metrics.EnvironmentSteps()\n",
            "  learning_triggers = [\n",
            "      triggers.PolicySavedModelTrigger(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\sac\\haarnoja18\\sac_train_eval.py",
        "line_number": 295,
        "API": ".join(",
        "context": [
            "      collect_policy,\n",
            "      train_step,\n",
            "      steps_per_run=1,\n",
            "      metrics=actor.collect_metrics(10),\n",
            "      summary_dir=os.path.join(root_dir, learner.TRAIN_DIR),\n",
            "      observers=[rb_observer, env_step_metric])\n",
            "\n",
            "  tf_greedy_policy = greedy_policy.GreedyPolicy(agent.policy)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\sac\\haarnoja18\\sac_train_eval.py",
        "line_number": 308,
        "API": ".join(",
        "context": [
            "      eval_greedy_policy,\n",
            "      train_step,\n",
            "      episodes_per_run=eval_episodes,\n",
            "      metrics=actor.eval_metrics(eval_episodes),\n",
            "      summary_dir=os.path.join(root_dir, 'eval'),\n",
            "  )\n",
            "\n",
            "  if eval_interval:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\sac\\haarnoja18\\sac_train_eval.py",
        "line_number": 315,
        "API": ".info(",
        "context": [
            "  if eval_interval:\n",
            "    logging.info('Evaluating.')\n",
            "    eval_actor.run_and_log()\n",
            "\n",
            "  logging.info('Training.')\n",
            "  for _ in range(num_iterations):\n",
            "    collect_actor.run()\n",
            "    agent_learner.run(iterations=1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\sac\\haarnoja18\\sac_train_eval.py",
        "line_number": 321,
        "API": ".info(",
        "context": [
            "    collect_actor.run()\n",
            "    agent_learner.run(iterations=1)\n",
            "\n",
            "    if eval_interval and agent_learner.train_step_numpy % eval_interval == 0:\n",
            "      logging.info('Evaluating.')\n",
            "      eval_actor.run_and_log()\n",
            "\n",
            "  rb_observer.close()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\examples\\sac\\haarnoja18\\sac_train_eval.py",
        "line_number": 329,
        "API": ".set_verbosity(",
        "context": [
            "  reverb_server.stop()\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  logging.set_verbosity(logging.INFO)\n",
            "  tf.compat.v1.enable_v2_behavior()\n",
            "\n",
            "  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_bindings)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\reverb_variable_container.py",
        "line_number": 76,
        "API": ".format(",
        "context": [
            "      table_info = server_info[table]\n",
            "      if table_info.max_size != 1:\n",
            "        raise ValueError(\n",
            "            'The max_size of the table {} is {} which different from the '\n",
            "            'expected capacity 1.'.format(table, table_info.max_size))\n",
            "      if not table_info.signature:\n",
            "        raise TypeError('Signature is not defined for table {}.'.format(table))\n",
            "      self._dtypes[table] = tf.nest.map_structure(lambda spec: spec.dtype,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\reverb_variable_container.py",
        "line_number": 100,
        "API": ".format(",
        "context": [
            "        differences (excluding the type differences of sequences in nest), and\n",
            "        type differences.\n",
            "    \"\"\"\n",
            "    if table not in self._dtypes:\n",
            "      raise KeyError('Could not find the table {}. Available tables: {}'.format(\n",
            "          table, self._dtypes.keys()))\n",
            "\n",
            "    # Sequence type check is turned off in Reverb client allowing sequence type\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\reverb_variable_container.py",
        "line_number": 107,
        "API": ".flatten(",
        "context": [
            "    # Sequence type check is turned off in Reverb client allowing sequence type\n",
            "    # differences in the signature. This is required to be able work with\n",
            "    # policies loaded from file which often change tuple to e.g. `ListWrapper`.\n",
            "    self._tf_client.insert(\n",
            "        data=tf.nest.flatten(values),\n",
            "        tables=tf.constant([table]),\n",
            "        priorities=tf.constant([1.0], dtype=tf.float64))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\reverb_variable_container.py",
        "line_number": 144,
        "API": ".flatten(",
        "context": [
            "              check_types: bool = False) -> None:\n",
            "    \"\"\"Assigns the nested values to variables.\"\"\"\n",
            "    nest_utils.assert_same_structure(variables, values, check_types=check_types)\n",
            "    for variable, value in zip(\n",
            "        tf.nest.flatten(variables), tf.nest.flatten(values)):\n",
            "      variable.assign(value)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\examples\\eval_job.py",
        "line_number": 39,
        "API": ".getenv(",
        "context": [
            "from tf_agents.train import actor\n",
            "from tf_agents.train import learner\n",
            "from tf_agents.train.utils import train_utils\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_string('variable_container_server_address', None,\n",
            "                    'Variable container server address.')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\examples\\eval_job.py",
        "line_number": 130,
        "API": ".info(",
        "context": [
            "    # It is very possible when logging a specific interval that the steps evaled\n",
            "    # will not be exact, e.g. 1001 and then 2003 vs. 1000 and then 2000.\n",
            "    if (train_step.numpy() == 0 or\n",
            "        train_step.numpy() >= eval_interval + last_eval_step):\n",
            "      logging.info('Evaluating using greedy policy at step: %d',\n",
            "                   train_step.numpy())\n",
            "      eval_actor.run()\n",
            "      last_eval_step = train_step.numpy()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\examples\\eval_job.py",
        "line_number": 163,
        "API": ".join(",
        "context": [
            "    return_reporting_fn: Optional callback function of the form `fn(train_step,\n",
            "      average_return)` which reports the average return to a custom destination.\n",
            "  \"\"\"\n",
            "  # Wait for the greedy policy to become available, then load it.\n",
            "  greedy_policy_dir = os.path.join(root_dir, learner.POLICY_SAVED_MODEL_DIR,\n",
            "                                   learner.GREEDY_POLICY_SAVED_MODEL_DIR)\n",
            "  policy = train_utils.wait_for_policy(\n",
            "      greedy_policy_dir, load_specs_from_pbtxt=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\examples\\eval_job.py",
        "line_number": 175,
        "API": ".join(",
        "context": [
            "      FLAGS.variable_container_server_address,\n",
            "      table_names=[reverb_variable_container.DEFAULT_TABLE])\n",
            "\n",
            "  # Prepare summary directory.\n",
            "  summary_dir = os.path.join(FLAGS.root_dir, learner.TRAIN_DIR, 'eval',\n",
            "                             str(FLAGS.task))\n",
            "\n",
            "  # Run the evaluation.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\examples\\eval_job.py",
        "line_number": 188,
        "API": ".set_verbosity(",
        "context": [
            "      return_reporting_fn=return_reporting_fn)\n",
            "\n",
            "\n",
            "def main(unused_argv: Sequence[Text]) -> None:\n",
            "  logging.set_verbosity(logging.INFO)\n",
            "\n",
            "  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_bindings)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\examples\\sac\\sac_collect.py",
        "line_number": 44,
        "API": ".getenv(",
        "context": [
            "from tf_agents.train import actor\n",
            "from tf_agents.train import learner\n",
            "from tf_agents.train.utils import train_utils\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_string('env_name', None, 'Name of the environment')\n",
            "flags.DEFINE_string('replay_buffer_server_address', None,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\examples\\sac\\sac_collect.py",
        "line_number": 99,
        "API": ".info(",
        "context": [
            "      random_policy,\n",
            "      train_step,\n",
            "      steps_per_run=initial_collect_steps,\n",
            "      observers=[rb_observer])\n",
            "  logging.info('Doing initial collect.')\n",
            "  initial_collect_actor.run()\n",
            "\n",
            "  env_step_metric = py_metrics.EnvironmentSteps()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\examples\\sac\\sac_collect.py",
        "line_number": 114,
        "API": ".info(",
        "context": [
            "      observers=[rb_observer, env_step_metric])\n",
            "\n",
            "  # Run the experience collection loop.\n",
            "  while train_step.numpy() < max_train_steps:\n",
            "    logging.info('Collecting with policy at step: %d', train_step.numpy())\n",
            "    collect_actor.run()\n",
            "    variable_container.update(variables)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\examples\\sac\\sac_collect.py",
        "line_number": 120,
        "API": ".set_verbosity(",
        "context": [
            "    variable_container.update(variables)\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  logging.set_verbosity(logging.INFO)\n",
            "  tf.enable_v2_behavior()\n",
            "\n",
            "  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_bindings)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\examples\\sac\\sac_collect.py",
        "line_number": 126,
        "API": ".join(",
        "context": [
            "\n",
            "  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_bindings)\n",
            "\n",
            "  # Wait for the collect policy to become available, then load it.\n",
            "  collect_policy_dir = os.path.join(FLAGS.root_dir,\n",
            "                                    learner.POLICY_SAVED_MODEL_DIR,\n",
            "                                    learner.COLLECT_POLICY_SAVED_MODEL_DIR)\n",
            "  collect_policy = train_utils.wait_for_policy(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\examples\\sac\\sac_collect.py",
        "line_number": 133,
        "API": ".join(",
        "context": [
            "  collect_policy = train_utils.wait_for_policy(\n",
            "      collect_policy_dir, load_specs_from_pbtxt=True)\n",
            "\n",
            "  # Prepare summary directory.\n",
            "  summary_dir = os.path.join(FLAGS.root_dir, learner.TRAIN_DIR, str(FLAGS.task))\n",
            "\n",
            "  # Perform collection.\n",
            "  collect(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\examples\\sac\\sac_reverb_server.py",
        "line_number": 35,
        "API": ".getenv(",
        "context": [
            "from tf_agents.specs import tensor_spec\n",
            "from tf_agents.train import learner\n",
            "from tf_agents.train.utils import train_utils\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_integer('min_table_size_before_sampling', 1,\n",
            "                     'Minimum number of elements in table before sampling.')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\examples\\sac\\sac_reverb_server.py",
        "line_number": 54,
        "API": ".set_verbosity(",
        "context": [
            "_SAMPLES_PER_INSERT_TOLERANCE_RATIO = 0.1\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  logging.set_verbosity(logging.INFO)\n",
            "\n",
            "  # Wait for the collect policy to become available, then load it.\n",
            "  collect_policy_dir = os.path.join(FLAGS.root_dir,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\examples\\sac\\sac_reverb_server.py",
        "line_number": 72,
        "API": ".map_structure(",
        "context": [
            "  variables = {\n",
            "      reverb_variable_container.POLICY_KEY: collect_policy.variables(),\n",
            "      reverb_variable_container.TRAIN_STEP_KEY: train_step\n",
            "  }\n",
            "  variable_container_signature = tf.nest.map_structure(\n",
            "      lambda variable: tf.TensorSpec(variable.shape, dtype=variable.dtype),\n",
            "      variables)\n",
            "  logging.info('Signature of variables: \\n%s', variable_container_signature)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\examples\\sac\\sac_reverb_server.py",
        "line_number": 81,
        "API": ".info(",
        "context": [
            "  # Create the signature for the replay buffer holding observed experience.\n",
            "  replay_buffer_signature = tensor_spec.from_spec(\n",
            "      collect_policy.collect_data_spec)\n",
            "  replay_buffer_signature = tensor_spec.add_outer_dim(replay_buffer_signature)\n",
            "  logging.info('Signature of experience: \\n%s', replay_buffer_signature)\n",
            "\n",
            "  if samples_per_insert is not None:\n",
            "    # Use SamplesPerInsertRatio limiter\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\examples\\sac\\sac_train.py",
        "line_number": 48,
        "API": ".getenv(",
        "context": [
            "from tf_agents.train.utils import train_utils\n",
            "from tf_agents.trajectories import time_step as ts\n",
            "from tf_agents.typing import types\n",
            "\n",
            "flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
            "                    'Root directory for writing logs/summaries/checkpoints.')\n",
            "flags.DEFINE_string('env_name', None, 'Name of the environment')\n",
            "flags.DEFINE_string('replay_buffer_server_address', None,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\examples\\sac\\sac_train.py",
        "line_number": 115,
        "API": ".info(",
        "context": [
            "    num_iterations: int = 2000000,\n",
            "    learner_iterations_per_call: int = 1) -> None:\n",
            "  \"\"\"Trains a SAC agent.\"\"\"\n",
            "  # Get the specs from the environment.\n",
            "  logging.info('Training SAC with learning rate: %f', learning_rate)\n",
            "  env = suite_load_fn(environment_name)\n",
            "  observation_tensor_spec, action_tensor_spec, time_step_tensor_spec = (\n",
            "      spec_utils.get_tensor_specs(env))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\examples\\sac\\sac_train.py",
        "line_number": 132,
        "API": ".join(",
        "context": [
            "        learning_rate=learning_rate)\n",
            "\n",
            "  # Create the policy saver which saves the initial model now, then it\n",
            "  # periodically checkpoints the policy weigths.\n",
            "  saved_model_dir = os.path.join(root_dir, learner.POLICY_SAVED_MODEL_DIR)\n",
            "  save_model_trigger = triggers.PolicySavedModelTrigger(\n",
            "      saved_model_dir, agent, train_step, interval=1000)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\experimental\\distributed\\examples\\sac\\sac_train.py",
        "line_number": 179,
        "API": ".set_verbosity(",
        "context": [
            "    variable_container.push(variables)\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  logging.set_verbosity(logging.INFO)\n",
            "  tf.enable_v2_behavior()\n",
            "\n",
            "  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_bindings)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\bias_layer.py",
        "line_number": 46,
        "API": ".get(",
        "context": [
            "    if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
            "      kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
            "\n",
            "    super(BiasLayer, self).__init__(**kwargs)\n",
            "    self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n",
            "\n",
            "    self.supports_masking = True\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\bias_layer.py",
        "line_number": 67,
        "API": ".expand_dims(",
        "context": [
            "    self.built = True\n",
            "\n",
            "  def call(self, inputs):\n",
            "    if inputs.shape.rank == 1:\n",
            "      expanded_inputs = tf.expand_dims(inputs, -1)\n",
            "      with_bias = tf.nn.bias_add(expanded_inputs, self.bias)\n",
            "      return with_bias[..., 0]\n",
            "    return tf.nn.bias_add(inputs, self.bias)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\dynamic_unroll_layer.py",
        "line_number": 69,
        "API": ".flatten(",
        "context": [
            "  \"\"\"\n",
            "  if explicit_dtype is not None:\n",
            "    return explicit_dtype\n",
            "  elif tf.nest.is_nested(state):\n",
            "    inferred_dtypes = [element.dtype for element in tf.nest.flatten(state)]\n",
            "    if not inferred_dtypes:\n",
            "      raise ValueError(\"Unable to infer dtype from empty state.\")\n",
            "    all_same = all([x == inferred_dtypes[0] for x in inferred_dtypes])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\dynamic_unroll_layer.py",
        "line_number": 107,
        "API": ".shape(",
        "context": [
            "    batch_size = shape.dims[1].value\n",
            "    if batch_size is not None:\n",
            "      return batch_size\n",
            "  # Fallback to the dynamic batch size of the first input.\n",
            "  return tf.shape(input=flat_input[0])[1]\n",
            "\n",
            "\n",
            "class DynamicUnroll(tf.keras.layers.Layer):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\dynamic_unroll_layer.py",
        "line_number": 272,
        "API": ".flatten(",
        "context": [
            "    if initial_state_missing and self.dtype is None:\n",
            "      raise ValueError(\"Must provide either dtype or initial_state\")\n",
            "\n",
            "    inputs_flat = [\n",
            "        tf.convert_to_tensor(x, name=\"input\") for x in tf.nest.flatten(inputs)]\n",
            "    has_time_axis = all(\n",
            "        [x.shape.ndims is None or x.shape.ndims > 2 for x in inputs_flat])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\dynamic_unroll_layer.py",
        "line_number": 279,
        "API": ".expand_dims(",
        "context": [
            "\n",
            "    if not has_time_axis:\n",
            "      # No time axis; and we're converting to time major anyway; add a time axis\n",
            "      # at the front.\n",
            "      inputs_flat = [tf.expand_dims(x, 0) for x in inputs_flat]\n",
            "    else:\n",
            "      # Assume all inputs are batch major.  Convert to time major.\n",
            "      inputs_flat = [common.transpose_batch_time(x) for x in inputs_flat]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\dynamic_unroll_layer.py",
        "line_number": 292,
        "API": ".transpose(",
        "context": [
            "    inputs = tf.nest.pack_sequence_as(inputs, inputs_flat)\n",
            "\n",
            "    # reset_mask is batch major.  Convert to time major.\n",
            "    if reset_mask is not None:\n",
            "      reset_mask = tf.transpose(a=reset_mask)\n",
            "\n",
            "    for shape in inputs_static_shapes:\n",
            "      got_batch_size = tf.compat.dimension_value(shape[1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\dynamic_unroll_layer.py",
        "line_number": 315,
        "API": ".shape(",
        "context": [
            "\n",
            "    # Try to get the iteration count statically; if that's not possible,\n",
            "    # access it dynamically at runtime.\n",
            "    iterations = tf.compat.dimension_value(inputs_flat[0].shape[0])\n",
            "    iterations = iterations or tf.shape(input=inputs_flat[0])[0]\n",
            "\n",
            "    if not tf.is_tensor(iterations) and iterations == 1:\n",
            "      # Take exactly one time step\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\dynamic_unroll_layer.py",
        "line_number": 342,
        "API": ".map_structure(",
        "context": [
            "          training=training)\n",
            "\n",
            "    if not has_time_axis:\n",
            "      # Remove the time axis.\n",
            "      outputs = tf.nest.map_structure(\n",
            "          lambda o: tf.squeeze(o, axis=1), outputs)\n",
            "\n",
            "    return outputs, new_state\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\dynamic_unroll_layer.py",
        "line_number": 350,
        "API": ".where(",
        "context": [
            "\n",
            "\n",
            "def _maybe_reset_state(reset, s_zero, s):\n",
            "  if not isinstance(s, tf.TensorArray) and s.shape.rank > 0:\n",
            "    return tf.compat.v1.where(reset, s_zero, s)\n",
            "  else:\n",
            "    return s\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\dynamic_unroll_layer.py",
        "line_number": 364,
        "API": ".squeeze(",
        "context": [
            "                               training):\n",
            "  \"\"\"Helper for dynamic_unroll which runs a single step.\"\"\"\n",
            "  def _squeeze(t):\n",
            "    if not isinstance(t, tf.TensorArray) and t.shape.rank > 0:\n",
            "      return tf.squeeze(t, [0])\n",
            "    else:\n",
            "      return t\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\dynamic_unroll_layer.py",
        "line_number": 369,
        "API": ".map_structure(",
        "context": [
            "    else:\n",
            "      return t\n",
            "\n",
            "  # Remove time dimension.\n",
            "  inputs = tf.nest.map_structure(_squeeze, inputs)\n",
            "  if reset_mask is not None:\n",
            "    reset_mask = _squeeze(reset_mask)\n",
            "    state = tf.nest.map_structure(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\dynamic_unroll_layer.py",
        "line_number": 377,
        "API": ".expand_dims(",
        "context": [
            "        lambda s, s_zero: _maybe_reset_state(reset_mask, s_zero, s), state,\n",
            "        zero_state)\n",
            "\n",
            "  outputs, final_state = cell(inputs, state, training=training)\n",
            "  outputs = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1), outputs)\n",
            "\n",
            "  return (outputs, final_state)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\dynamic_unroll_layer.py",
        "line_number": 400,
        "API": ".unstack(",
        "context": [
            "  def ta_and_unstack(x):\n",
            "    return (tf.TensorArray(dtype=x.dtype,\n",
            "                           size=iterations,\n",
            "                           element_shape=x.shape[1:])\n",
            "            .unstack(x))\n",
            "\n",
            "  inputs_tas = tf.nest.map_structure(ta_and_unstack, inputs)\n",
            "  if reset_mask is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\dynamic_unroll_layer.py",
        "line_number": 416,
        "API": ".map_structure(",
        "context": [
            "        size=iterations,\n",
            "        element_shape=(tf.TensorShape([const_batch_size])\n",
            "                       .concatenate(_maybe_tensor_shape_from_tensor(s))))\n",
            "\n",
            "  output_tas = tf.nest.map_structure(create_output_ta, cell.output_size)\n",
            "\n",
            "  def pred(time, *unused_args):\n",
            "    return time < iterations\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\dynamic_unroll_layer.py",
        "line_number": 435,
        "API": ".map_structure(",
        "context": [
            "      - state: rnn state @ time + 1\n",
            "      - output_tas: output tensorarrays with values written @ time\n",
            "      - masks_ta: optional mask tensorarray with mask written @ time\n",
            "    \"\"\"\n",
            "    input_ = tf.nest.map_structure(lambda ta: ta.read(time), inputs_tas)\n",
            "    if reset_mask_ta is not None:\n",
            "      is_reset = reset_mask_ta.read(time)\n",
            "      state = tf.nest.map_structure(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\dynamic_unroll_layer.py",
        "line_number": 444,
        "API": ".write(",
        "context": [
            "          state)\n",
            "\n",
            "    outputs, next_state = cell(input_, state, training=training)\n",
            "\n",
            "    output_tas = tf.nest.map_structure(lambda ta, x: ta.write(time, x),\n",
            "                                       output_tas, outputs)\n",
            "\n",
            "    return (time + 1, next_state, output_tas)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\dynamic_unroll_layer.py",
        "line_number": 454,
        "API": ".executing_eagerly(",
        "context": [
            "  # determined by the parent scope, or is set to place the cached\n",
            "  # Variable using the same placement as for the rest of the RNN.\n",
            "  with tf.compat.v1.variable_scope(\n",
            "      tf.compat.v1.get_variable_scope()) as varscope:\n",
            "    if (not tf.executing_eagerly() and varscope.caching_device is None):\n",
            "      varscope.set_caching_device(lambda op: op.device)\n",
            "\n",
            "    _, final_state, output_tas = (\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\dynamic_unroll_layer.py",
        "line_number": 461,
        "API": ".constant(",
        "context": [
            "    _, final_state, output_tas = (\n",
            "        tf.while_loop(\n",
            "            cond=pred,\n",
            "            body=body,\n",
            "            loop_vars=(tf.constant(0, name=\"time\"), initial_state, output_tas),\n",
            "            parallel_iterations=parallel_iterations,\n",
            "            swap_memory=swap_memory,\n",
            "            maximum_iterations=iterations))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\dynamic_unroll_layer.py",
        "line_number": 466,
        "API": ".stack(",
        "context": [
            "            parallel_iterations=parallel_iterations,\n",
            "            swap_memory=swap_memory,\n",
            "            maximum_iterations=iterations))\n",
            "\n",
            "  outputs = tf.nest.map_structure(lambda ta: ta.stack(), output_tas)\n",
            "\n",
            "  if isinstance(iterations, int):\n",
            "    # TensorArray.stack() doesn't set a static value for dimension 0,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\dynamic_unroll_layer.py",
        "line_number": 472,
        "API": ".map_structure(",
        "context": [
            "  if isinstance(iterations, int):\n",
            "    # TensorArray.stack() doesn't set a static value for dimension 0,\n",
            "    # even if the size is known. Set the shapes here.\n",
            "    iterations_shape = tf.TensorShape([iterations])\n",
            "    tf.nest.map_structure(\n",
            "        lambda t: t.set_shape(iterations_shape.concatenate(t.shape[1:])),\n",
            "        outputs)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\dynamic_unroll_layer.py",
        "line_number": 477,
        "API": ".map_structure(",
        "context": [
            "        lambda t: t.set_shape(iterations_shape.concatenate(t.shape[1:])),\n",
            "        outputs)\n",
            "\n",
            "  # Convert everything back to batch major\n",
            "  outputs = tf.nest.map_structure(common.transpose_batch_time, outputs)\n",
            "\n",
            "  return (outputs, final_state)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\inner_reshape.py",
        "line_number": 90,
        "API": ".format(",
        "context": [
            "      and (current_shape.num_elements()\n",
            "           != new_shape.num_elements())):\n",
            "    raise ValueError(\n",
            "        'Mismatched number of elements in current and new inner shapes: '\n",
            "        '{} vs. {}'.format(current_shape, new_shape))\n",
            "\n",
            "  def reshape(t):\n",
            "    return _reshape_inner_dims(t, current_shape, new_shape)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\inner_reshape.py",
        "line_number": 95,
        "API": ".map_structure(",
        "context": [
            "\n",
            "  def reshape(t):\n",
            "    return _reshape_inner_dims(t, current_shape, new_shape)\n",
            "  return tf.keras.layers.Lambda(\n",
            "      lambda inputs: tf.nest.map_structure(reshape, inputs), **kwargs)\n",
            "\n",
            "\n",
            "def _reshape_inner_dims(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\inner_reshape.py",
        "line_number": 103,
        "API": ".shape(",
        "context": [
            "    tensor: tf.Tensor,\n",
            "    shape: tf.TensorShape,\n",
            "    new_shape: tf.TensorShape) -> tf.Tensor:\n",
            "  \"\"\"Reshapes tensor to: shape(tensor)[:-len(shape)] + new_shape.\"\"\"\n",
            "  tensor_shape = tf.shape(tensor)\n",
            "  ndims = shape.rank\n",
            "  tensor.shape[-ndims:].assert_is_compatible_with(shape)\n",
            "  new_shape_inner_tensor = tf.cast(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\inner_reshape.py",
        "line_number": 108,
        "API": ".cast(",
        "context": [
            "  ndims = shape.rank\n",
            "  tensor.shape[-ndims:].assert_is_compatible_with(shape)\n",
            "  new_shape_inner_tensor = tf.cast(\n",
            "      [-1 if d is None else d for d in new_shape.as_list()], tf.int32)\n",
            "  new_shape_outer_tensor = tf.cast(\n",
            "      tensor_shape[:-ndims], tf.int32)\n",
            "  full_new_shape = tf.concat(\n",
            "      (new_shape_outer_tensor, new_shape_inner_tensor), axis=0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\permanent_variable_rate_dropout.py",
        "line_number": 47,
        "API": ".dropout(",
        "context": [
            "      training = tf.keras.backend.learning_phase()\n",
            "\n",
            "    if training:\n",
            "      rate = self._get_dropout_value()\n",
            "      outputs = tf.nn.dropout(\n",
            "          inputs,\n",
            "          noise_shape=self._get_noise_shape(inputs),\n",
            "          seed=self.seed,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\rnn_wrapper.py",
        "line_number": 47,
        "API": ".format(",
        "context": [
            "        `return_sequences == False`.\n",
            "    \"\"\"\n",
            "    if not isinstance(layer, tf.keras.layers.RNN):\n",
            "      raise TypeError(\n",
            "          'layer is not a subclass of tf.keras.layers.RNN.  Layer: {}'.format(\n",
            "              layer))\n",
            "    layer_config = layer.get_config()\n",
            "    if not layer_config.get('return_state', False):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\rnn_wrapper.py",
        "line_number": 54,
        "API": ".format(",
        "context": [
            "    if not layer_config.get('return_state', False):\n",
            "      # This is an RNN layer that doesn't return state.\n",
            "      raise NotImplementedError(\n",
            "          'Provided a Keras RNN layer with return_state==False. '\n",
            "          'This configuration is not supported.  Layer: {}'.format(layer))\n",
            "    if not layer_config.get('return_sequences', False):\n",
            "      raise NotImplementedError(\n",
            "          'Provided a Keras RNN layer with return_sequences==False. '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\rnn_wrapper.py",
        "line_number": 133,
        "API": ".convert_to_tensor(",
        "context": [
            "    self.built = True\n",
            "\n",
            "  def get_initial_state(self, inputs=None):\n",
            "    inputs_flat = [\n",
            "        tf.convert_to_tensor(x, name='input', dtype_hint=self.dtype)\n",
            "        for x in tf.nest.flatten(inputs)\n",
            "    ]\n",
            "    has_time_axis = all(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\rnn_wrapper.py",
        "line_number": 139,
        "API": ".expand_dims(",
        "context": [
            "    ]\n",
            "    has_time_axis = all(\n",
            "        [x.shape.ndims is None or x.shape.ndims > 2 for x in inputs_flat])\n",
            "    if not has_time_axis:\n",
            "      inputs_flat = [tf.expand_dims(t, axis=1) for t in inputs_flat]\n",
            "    inputs = tf.nest.pack_sequence_as(inputs, inputs_flat)\n",
            "    return self._layer.get_initial_state(inputs)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\rnn_wrapper.py",
        "line_number": 162,
        "API": ".convert_to_tensor(",
        "context": [
            "         is typically a tensor shaped `[batch_size, n, ...]`.\n",
            "       - `final_state` contains the final state.\n",
            "    \"\"\"\n",
            "    inputs_flat = [\n",
            "        tf.convert_to_tensor(x, name='input', dtype_hint=self.dtype)\n",
            "        for x in tf.nest.flatten(inputs)\n",
            "    ]\n",
            "    has_time_axis = all(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\rnn_wrapper.py",
        "line_number": 168,
        "API": ".expand_dims(",
        "context": [
            "    ]\n",
            "    has_time_axis = all(\n",
            "        [x.shape.ndims is None or x.shape.ndims > 2 for x in inputs_flat])\n",
            "    if not has_time_axis:\n",
            "      inputs_flat = [tf.expand_dims(t, axis=1) for t in inputs_flat]\n",
            "    inputs = tf.nest.pack_sequence_as(inputs, inputs_flat)\n",
            "\n",
            "    if not common.safe_has_state(initial_state):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\rnn_wrapper.py",
        "line_number": 182,
        "API": ".flatten(",
        "context": [
            "\n",
            "    # Keras RNN's outputs[1:] does not match the nest structure of its cells'\n",
            "    # state_size property.  Restructure the output state to match.\n",
            "    new_state = tf.nest.pack_sequence_as(\n",
            "        self.state_size, tf.nest.flatten(new_state))\n",
            "\n",
            "    if not has_time_axis:\n",
            "      output = tf.nest.map_structure(lambda t: tf.squeeze(t, axis=1), output)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\squashed_outer_wrapper.py",
        "line_number": 46,
        "API": ".normal(",
        "context": [
            "  ```python\n",
            "  batch_norm = tf.keras.layers.BatchNormalization(axis=-1)\n",
            "  layer = SquashedOuterWrapper(wrapped=batch_norm, inner_rank=3)\n",
            "\n",
            "  inputs_0 = tf.random.normal((B, H, W, C))\n",
            "  # batch_norm sees tensor of shape [B, H, W, C]\n",
            "  # outputs_1 shape is [B, H, W, C]\n",
            "  outputs_0 = layer(inputs_0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\squashed_outer_wrapper.py",
        "line_number": 51,
        "API": ".normal(",
        "context": [
            "  # batch_norm sees tensor of shape [B, H, W, C]\n",
            "  # outputs_1 shape is [B, H, W, C]\n",
            "  outputs_0 = layer(inputs_0)\n",
            "\n",
            "  inputs_1 = tf.random.normal((B, T, H, W, C))\n",
            "  # batch_norm sees a tensor of shape [B * T, H, W, C]\n",
            "  # outputs_1 shape is [B, T, H, W, C]\n",
            "  outputs_1 = layer(inputs_1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\squashed_outer_wrapper.py",
        "line_number": 56,
        "API": ".normal(",
        "context": [
            "  # batch_norm sees a tensor of shape [B * T, H, W, C]\n",
            "  # outputs_1 shape is [B, T, H, W, C]\n",
            "  outputs_1 = layer(inputs_1)\n",
            "\n",
            "  inputs_2 = tf.random.normal((B1, B2, T, H, W, C))\n",
            "  # batch_norm sees a tensor of shape [B1 * B2 * T, H, W, C]\n",
            "  # outputs_2 shape is [B1, B2, T, H, W, C]\n",
            "  outputs_2 = layer(inputs_2)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\squashed_outer_wrapper.py",
        "line_number": 84,
        "API": ".format(",
        "context": [
            "    if getattr(wrapped, 'get_initial_state', None) is not None:\n",
            "      raise ValueError(\n",
            "          '`wrapped` has method `get_initial_state`, which means its inputs '\n",
            "          'will include separate state tensors.  This is not supported by '\n",
            "          '`SquashedOuterWrapper`.  wrapped: {}'.format(wrapped))\n",
            "    self._inner_rank = inner_rank\n",
            "    self._wrapped = wrapped\n",
            "    super(SquashedOuterWrapper, self).__init__(**kwargs)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\squashed_outer_wrapper.py",
        "line_number": 101,
        "API": ".format(",
        "context": [
            "  def build(self, input_shape):\n",
            "    input_shape = tf.TensorShape(input_shape)\n",
            "    if input_shape.rank is None:\n",
            "      raise ValueError(\n",
            "          'inputs must have known rank; input shape: {}'.format(input_shape))\n",
            "    batch_shape = input_shape[:-self.inner_rank]\n",
            "    inner_shape = input_shape[-self.inner_rank:]\n",
            "    if batch_shape.is_fully_defined():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\squashed_outer_wrapper.py",
        "line_number": 115,
        "API": ".format(",
        "context": [
            "  def call(self, inputs, training=False):\n",
            "    static_rank = inputs.shape.rank\n",
            "    if static_rank is None:\n",
            "      raise ValueError(\n",
            "          'inputs must have known rank; inputs: {}'.format(inputs))\n",
            "    squash_dims = static_rank - self.inner_rank\n",
            "    bs = utils.BatchSquash(squash_dims)\n",
            "    squashed_inputs = bs.flatten(inputs)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\keras_layers\\squashed_outer_wrapper.py",
        "line_number": 144,
        "API": ".format(",
        "context": [
            "  def compute_output_shape(self, input_shape):\n",
            "    input_shape = tf.TensorShape(input_shape)\n",
            "    if input_shape.rank is None:\n",
            "      raise ValueError(\n",
            "          'inputs must have known rank; input shape: {}'.format(input_shape))\n",
            "    batch_shape = input_shape[:-self.inner_rank]\n",
            "    inner_shape = input_shape[-self.inner_rank:]\n",
            "    if batch_shape.is_fully_defined():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\batched_py_metric.py",
        "line_number": 84,
        "API": ".format(",
        "context": [
            "    if batch_size != len(self._metrics):\n",
            "      raise ValueError('Batch size {} does not match previously set batch '\n",
            "                       'size {}. Make sure your batch size is set correctly '\n",
            "                       'in BatchedPyMetric initialization and that the batch '\n",
            "                       'size remains constant.'.format(batch_size,\n",
            "                                                       len(self._metrics)))\n",
            "\n",
            "    for metric, trajectory in zip(self._metrics, trajectories):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\batched_py_metric.py",
        "line_number": 101,
        "API": ".array(",
        "context": [
            "    \"\"\"Evaluates the current value of the metric.\"\"\"\n",
            "    if self._built:\n",
            "      return self._metric_class.aggregate(self._metrics)\n",
            "    else:\n",
            "      return np.array(0.0, dtype=self._dtype)\n",
            "\n",
            "  @staticmethod\n",
            "  def aggregate(metrics):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\export_utils.py",
        "line_number": 30,
        "API": ".info(",
        "context": [
            "    loss_info: An optional instance of `LossInfo` whose value is logged.\n",
            "  \"\"\"\n",
            "  def logging_at_step_fn(name, value):\n",
            "    logging_msg = f'[step={step}] {name} = {value}.'\n",
            "    logging.info(logging_msg)\n",
            "\n",
            "  for metric in metrics:\n",
            "    logging_at_step_fn(metric.name, metric.result())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\py_metric.py",
        "line_number": 66,
        "API": ".format(",
        "context": [
            "  for metric in metrics:\n",
            "    if metric.summary_op is None:\n",
            "      raise RuntimeError('metric.tf_summaries() must be called on py_metric '\n",
            "                         '{} before attempting to run '\n",
            "                         'summaries.'.format(metric.name))\n",
            "  summary_ops = [metric.summary_op for metric in metrics]\n",
            "  feed_dict = dict(\n",
            "      (metric.summary_placeholder, metric.result()) for metric in metrics)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\py_metric.py",
        "line_number": 99,
        "API": ".info(",
        "context": [
            "    \"\"\"Evaluates the current value of the metric.\"\"\"\n",
            "\n",
            "  def log(self):\n",
            "    tag = common.join_scope(self.prefix, self.name)\n",
            "    logging.info('%s', '{0} = {1}'.format(tag, self.result()))\n",
            "\n",
            "  def tf_summaries(self,\n",
            "                   train_step: types.Int = None,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\py_metric.py",
        "line_number": 127,
        "API": ".scalar(",
        "context": [
            "      raise RuntimeError('metric.tf_summaries() can only be called once.')\n",
            "\n",
            "    tag = common.join_scope(self.prefix, self.name)\n",
            "    summaries = []\n",
            "    summaries.append(tf.compat.v2.summary.scalar(\n",
            "        name=tag, data=self.summary_placeholder, step=train_step))\n",
            "    prefix = self.prefix\n",
            "    if prefix:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\py_metric.py",
        "line_number": 136,
        "API": ".format(",
        "context": [
            "    for step_metric in step_metrics:\n",
            "      # Skip plotting the metrics against itself.\n",
            "      if self.name == step_metric.name:\n",
            "        continue\n",
            "      step_tag = '{}vs_{}/{}'.format(prefix, step_metric.name, self.name)\n",
            "      if isinstance(step_metric, PyMetric):\n",
            "        step_tensor = step_metric.summary_placeholder\n",
            "      elif isinstance(step_metric, tf_metric.TFStepMetric):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\py_metric.py",
        "line_number": 143,
        "API": ".format(",
        "context": [
            "      elif isinstance(step_metric, tf_metric.TFStepMetric):\n",
            "        step_tensor = step_metric.result()\n",
            "      else:\n",
            "        raise ValueError('step_metric is not PyMetric or TFStepMetric: '\n",
            "                         '{}'.format(step_metric))\n",
            "      summaries.append(tf.compat.v2.summary.scalar(\n",
            "          name=step_tag,\n",
            "          data=self.summary_placeholder,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\py_metric.py",
        "line_number": 149,
        "API": ".group(",
        "context": [
            "          name=step_tag,\n",
            "          data=self.summary_placeholder,\n",
            "          step=step_tensor))\n",
            "\n",
            "    self._summary_op = tf.group(*summaries)\n",
            "    return self._summary_op\n",
            "\n",
            "  @property\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\py_metric.py",
        "line_number": 158,
        "API": ".array(",
        "context": [
            "    \"\"\"TF placeholder to be used for the result of this metric.\"\"\"\n",
            "    if self._summary_placeholder is None:\n",
            "      result = self.result()\n",
            "      if not isinstance(result, (np.ndarray, np.generic)):\n",
            "        result = np.array(result)\n",
            "      dtype = tf.as_dtype(result.dtype)\n",
            "      shape = result.shape\n",
            "      self._summary_placeholder = tf.compat.v1.placeholder(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\py_metric.py",
        "line_number": 181,
        "API": ".mean(",
        "context": [
            "      metrics: a list of metrics, of the same class.\n",
            "    Returns:\n",
            "      The result of aggregating this metric.\n",
            "    \"\"\"\n",
            "    return np.mean([metric.result() for metric in metrics])\n",
            "\n",
            "  def __call__(self, *args):\n",
            "    \"\"\"Method to update the metric contents.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\py_metrics.py",
        "line_number": 49,
        "API": ".array(",
        "context": [
            "      dtype: Data type of deque elements.\n",
            "    \"\"\"\n",
            "    self._start_index = np.int64(0)\n",
            "    self._len = np.int64(0)\n",
            "    self._maxlen = np.array(maxlen)\n",
            "    initial_len = 10 if np.isinf(self._maxlen) else self._maxlen\n",
            "    self._buffer = np.zeros(shape=(initial_len,), dtype=dtype)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\py_metrics.py",
        "line_number": 61,
        "API": ".isinf(",
        "context": [
            "  def add(self, value: Any):\n",
            "    insert_idx = int((self._start_index + self._len) % self._maxlen)\n",
            "\n",
            "    # Increase buffer size if necessary.\n",
            "    if np.isinf(self._maxlen) and insert_idx >= self._buffer.shape[0]:\n",
            "      new_len = self._buffer.shape[0] * 2\n",
            "      self._buffer.resize((new_len,), refcheck=False)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\py_metrics.py",
        "line_number": 73,
        "API": ".add(",
        "context": [
            "      self._start_index = np.mod(self._start_index + 1, self._maxlen)\n",
            "\n",
            "  def extend(self, values: Iterable[Any]):\n",
            "    for value in values:\n",
            "      self.add(value)\n",
            "\n",
            "  @property\n",
            "  def last(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\py_metrics.py",
        "line_number": 88,
        "API": ".mean(",
        "context": [
            "    return self._len\n",
            "\n",
            "  def mean(self, dtype: Optional[np.dtype] = None):\n",
            "    if self._len == self._buffer.shape[0]:\n",
            "      return np.mean(self._buffer, dtype=dtype)\n",
            "\n",
            "    assert self._start_index == 0\n",
            "    return np.mean(self._buffer[:self._len], dtype=dtype)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\py_metrics.py",
        "line_number": 132,
        "API": ".mean(",
        "context": [
            "\n",
            "  def result(self) -> np.float32:\n",
            "    \"\"\"Returns the value of this metric.\"\"\"\n",
            "    if self._buffer:\n",
            "      return self._buffer.mean(dtype=np.float32)\n",
            "    return np.array(0.0, dtype=np.float32)\n",
            "\n",
            "  @abc.abstractmethod\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\py_metrics.py",
        "line_number": 170,
        "API": ".zeros(",
        "context": [
            "                                              batch_size=batch_size)\n",
            "\n",
            "  def _reset(self, batch_size):\n",
            "    \"\"\"Resets stat gathering variables.\"\"\"\n",
            "    self._np_state.episode_return = np.zeros(\n",
            "        shape=(batch_size,), dtype=np.float64)\n",
            "\n",
            "  def _batched_call(self, trajectory):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\py_metrics.py",
        "line_number": 181,
        "API": ".where(",
        "context": [
            "      trajectory: a tf_agents.trajectory.Trajectory.\n",
            "    \"\"\"\n",
            "    episode_return = self._np_state.episode_return\n",
            "\n",
            "    is_first = np.where(trajectory.is_first())\n",
            "    episode_return[is_first] = 0\n",
            "\n",
            "    episode_return += trajectory.reward\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\py_metrics.py",
        "line_number": 186,
        "API": ".where(",
        "context": [
            "    episode_return[is_first] = 0\n",
            "\n",
            "    episode_return += trajectory.reward\n",
            "\n",
            "    is_last = np.where(trajectory.is_last())\n",
            "    self.add_to_buffer(episode_return[is_last])\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\py_metrics.py",
        "line_number": 208,
        "API": ".zeros(",
        "context": [
            "        name, buffer_size=buffer_size, batch_size=batch_size)\n",
            "\n",
            "  def _reset(self, batch_size):\n",
            "    \"\"\"Resets stat gathering variables.\"\"\"\n",
            "    self._np_state.episode_steps = np.zeros(\n",
            "        shape=(batch_size,), dtype=np.float64)\n",
            "\n",
            "  def _batched_call(self, trajectory):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\py_metrics.py",
        "line_number": 220,
        "API": ".where(",
        "context": [
            "    \"\"\"\n",
            "    episode_steps = self._np_state.episode_steps\n",
            "\n",
            "    # Each non-boundary trajectory (first, mid or last) represents a step.\n",
            "    episode_steps[np.where(~trajectory.is_boundary())] += 1\n",
            "    self.add_to_buffer(episode_steps[np.where(trajectory.is_last())])\n",
            "    episode_steps[np.where(trajectory.is_last())] = 0\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\py_metrics.py",
        "line_number": 244,
        "API": ".sum(",
        "context": [
            "  def call(self, trajectory: traj.Trajectory):\n",
            "    if trajectory.step_type.ndim == 0:\n",
            "      trajectory = nest_utils.batch_nested_array(trajectory)\n",
            "\n",
            "    new_steps = np.sum((~trajectory.is_boundary()).astype(np.int64))\n",
            "    self._np_state.environment_steps += new_steps\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\py_metrics.py",
        "line_number": 267,
        "API": ".sum(",
        "context": [
            "  def call(self, trajectory: traj.Trajectory):\n",
            "    if trajectory.step_type.ndim == 0:\n",
            "      trajectory = nest_utils.batch_nested_array(trajectory)\n",
            "\n",
            "    completed_episodes = np.sum(trajectory.is_last().astype(np.int64))\n",
            "    self._np_state.number_episodes += completed_episodes\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metric.py",
        "line_number": 74,
        "API": ".executing_eagerly(",
        "context": [
            "      If using graph execution, this returns an op to perform the\n",
            "      initialization. Under eager execution, the variables are reset to their\n",
            "      initial values as a side effect and this function returns None.\n",
            "    \"\"\"\n",
            "    if not tf.executing_eagerly():\n",
            "      return tf.compat.v1.group([v.initializer for v in self.variables])\n",
            "\n",
            "  @common.function\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metric.py",
        "line_number": 112,
        "API": ".scalar(",
        "context": [
            "    tag = common.join_scope(prefix, self.name)\n",
            "    result = self.result()\n",
            "    if train_step is not None:\n",
            "      summaries.append(\n",
            "          tf.compat.v2.summary.scalar(name=tag, data=result, step=train_step))\n",
            "    if prefix:\n",
            "      prefix += '_'\n",
            "    for step_metric in step_metrics:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metric.py",
        "line_number": 119,
        "API": ".format(",
        "context": [
            "    for step_metric in step_metrics:\n",
            "      # Skip plotting the metrics against itself.\n",
            "      if self.name == step_metric.name:\n",
            "        continue\n",
            "      step_tag = '{}vs_{}/{}'.format(prefix, step_metric.name, self.name)\n",
            "      # Summaries expect the step value to be an int64.\n",
            "      step = tf.cast(step_metric.result(), tf.int64)\n",
            "      summaries.append(tf.compat.v2.summary.scalar(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metric.py",
        "line_number": 154,
        "API": ".histogram(",
        "context": [
            "    tag = common.join_scope(prefix, self.name)\n",
            "    result = self.result()\n",
            "    if train_step is not None:\n",
            "      summaries.append(\n",
            "          tf.compat.v2.summary.histogram(\n",
            "              name=tag, data=result, step=train_step))\n",
            "    if prefix:\n",
            "      prefix += '_'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metric.py",
        "line_number": 162,
        "API": ".format(",
        "context": [
            "    for step_metric in step_metrics:\n",
            "      # Skip plotting the metrics against itself.\n",
            "      if self.name == step_metric.name:\n",
            "        continue\n",
            "      step_tag = '{}vs_{}/{}'.format(prefix, step_metric.name, self.name)\n",
            "      # Summaries expect the step value to be an int64.\n",
            "      step = tf.cast(step_metric.result(), tf.int64)\n",
            "      summaries.append(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metric.py",
        "line_number": 198,
        "API": ".flatten(",
        "context": [
            "\n",
            "    Returns:\n",
            "      A list of scalar summaries.\n",
            "    \"\"\"\n",
            "    result_list = tf.nest.flatten(self.result())\n",
            "    if len(result_list) == 1:\n",
            "      # For the special case when the multiple metrics come from a single but\n",
            "      # non-scalar tensor.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metric.py",
        "line_number": 224,
        "API": ".scalar(",
        "context": [
            "        metric_name = self.metric_names[metric_index]\n",
            "      tag = common.join_scope(tag, metric_name)\n",
            "      if train_step is not None:\n",
            "        summaries.append(\n",
            "            tf.compat.v2.summary.scalar(name=tag, data=result, step=train_step))\n",
            "    if prefix:\n",
            "      prefix += '_'\n",
            "    for metric_index, result in enumerate(result_list):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metric.py",
        "line_number": 241,
        "API": ".format(",
        "context": [
            "        if (metric_index < len(self.metric_names) and\n",
            "            len(result_list) == len(self.metric_names) and\n",
            "            self.metric_names[metric_index] is not None):\n",
            "          metric_name = self.metric_names[metric_index]\n",
            "        step_tag = '{}vs_{}/{}/{}'.format(prefix, step_metric.name,\n",
            "                                          self.name, metric_name)\n",
            "        # Summaries expect the step value to be an int64.\n",
            "        step = tf.cast(step_metric.result(), tf.int64)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 36,
        "API": ".convert_to_tensor(",
        "context": [
            "class TFDeque(object):\n",
            "  \"\"\"Deque backed by tf.Variable storage.\"\"\"\n",
            "\n",
            "  def __init__(self, max_len, dtype, shape=(), name='TFDeque'):\n",
            "    self._max_len = tf.convert_to_tensor(max_len, dtype=tf.int32)\n",
            "    self._spec = tf.TensorSpec(shape, dtype, name='Buffer')\n",
            "    self._buffer = table.Table(self._spec, capacity=max_len)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 45,
        "API": ".range(",
        "context": [
            "        initial_value=0, dtype=tf.int32, shape=(), name=name + 'Head')\n",
            "\n",
            "  @property\n",
            "  def data(self):\n",
            "    return self._buffer.read(tf.range(self.length))\n",
            "\n",
            "  @common.function(autograph=True)\n",
            "  def extend(self, value):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 50,
        "API": ".add(",
        "context": [
            "\n",
            "  @common.function(autograph=True)\n",
            "  def extend(self, value):\n",
            "    for v in value:\n",
            "      self.add(v)\n",
            "\n",
            "  @common.function(autograph=True)\n",
            "  def add(self, value):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 55,
        "API": ".write(",
        "context": [
            "\n",
            "  @common.function(autograph=True)\n",
            "  def add(self, value):\n",
            "    position = tf.math.mod(self._head, self._max_len)\n",
            "    self._buffer.write(position, value)\n",
            "    self._head.assign_add(1)\n",
            "\n",
            "  @property\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 60,
        "API": ".minimum(",
        "context": [
            "    self._head.assign_add(1)\n",
            "\n",
            "  @property\n",
            "  def length(self):\n",
            "    return tf.minimum(self._head, self._max_len)\n",
            "\n",
            "  @common.function\n",
            "  def clear(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 66,
        "API": ".function(",
        "context": [
            "  @common.function\n",
            "  def clear(self):\n",
            "    self._head.assign(0)\n",
            "\n",
            "  @common.function(autograph=True)\n",
            "  def mean(self):\n",
            "    if tf.equal(self._head, 0):\n",
            "      return tf.zeros(self._spec.shape, self._spec.dtype)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 72,
        "API": ".function(",
        "context": [
            "    if tf.equal(self._head, 0):\n",
            "      return tf.zeros(self._spec.shape, self._spec.dtype)\n",
            "    return tf.math.reduce_mean(self.data, axis=0)\n",
            "\n",
            "  @common.function(autograph=True)\n",
            "  def max(self):\n",
            "    if tf.equal(self._head, 0):\n",
            "      return tf.fill(self._spec.shape, self._spec.dtype.min)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 78,
        "API": ".function(",
        "context": [
            "    if tf.equal(self._head, 0):\n",
            "      return tf.fill(self._spec.shape, self._spec.dtype.min)\n",
            "    return tf.math.reduce_max(self.data, axis=0)\n",
            "\n",
            "  @common.function(autograph=True)\n",
            "  def min(self):\n",
            "    if tf.equal(self._head, 0):\n",
            "      return tf.fill(self._spec.shape, self._spec.dtype.max)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 108,
        "API": ".cast(",
        "context": [
            "    Returns:\n",
            "      The arguments, for easy chaining.\n",
            "    \"\"\"\n",
            "    # The __call__ will execute this.\n",
            "    num_steps = tf.cast(~trajectory.is_boundary(), self.dtype)\n",
            "    num_steps = tf.reduce_sum(input_tensor=num_steps)\n",
            "    self.environment_steps.assign_add(num_steps)\n",
            "    return trajectory\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 114,
        "API": ".identity(",
        "context": [
            "    self.environment_steps.assign_add(num_steps)\n",
            "    return trajectory\n",
            "\n",
            "  def result(self):\n",
            "    return tf.identity(self.environment_steps, name=self.name)\n",
            "\n",
            "  @common.function\n",
            "  def reset(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 143,
        "API": ".cast(",
        "context": [
            "    Returns:\n",
            "      The arguments, for easy chaining.\n",
            "    \"\"\"\n",
            "    # The __call__ will execute this.\n",
            "    num_episodes = tf.cast(trajectory.is_last(), self.dtype)\n",
            "    num_episodes = tf.reduce_sum(input_tensor=num_episodes)\n",
            "    self.number_episodes.assign_add(num_episodes)\n",
            "    return trajectory\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 149,
        "API": ".identity(",
        "context": [
            "    self.number_episodes.assign_add(num_episodes)\n",
            "    return trajectory\n",
            "\n",
            "  def result(self):\n",
            "    return tf.identity(self.number_episodes, name=self.name)\n",
            "\n",
            "  @common.function\n",
            "  def reset(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 172,
        "API": ".function(",
        "context": [
            "    self._dtype = dtype\n",
            "    self._return_accumulator = common.create_variable(\n",
            "        initial_value=0, dtype=dtype, shape=(batch_size,), name='Accumulator')\n",
            "\n",
            "  @common.function(autograph=True)\n",
            "  def call(self, trajectory):\n",
            "    # Zero out batch indices where a new episode is starting.\n",
            "    self._return_accumulator.assign(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 182,
        "API": ".reduce_sum(",
        "context": [
            "\n",
            "    # Update accumulator with received rewards. We are summing over all\n",
            "    # non-batch dimensions in case the reward is a vector.\n",
            "    self._return_accumulator.assign_add(\n",
            "        tf.reduce_sum(\n",
            "            trajectory.reward, axis=range(1, len(trajectory.reward.shape))))\n",
            "\n",
            "    # Add final returns to buffer.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 188,
        "API": ".add(",
        "context": [
            "\n",
            "    # Add final returns to buffer.\n",
            "    last_episode_indices = tf.squeeze(tf.where(trajectory.is_last()), axis=-1)\n",
            "    for indx in last_episode_indices:\n",
            "      self._buffer.add(self._return_accumulator[indx])\n",
            "\n",
            "    return trajectory\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 193,
        "API": ".mean(",
        "context": [
            "\n",
            "    return trajectory\n",
            "\n",
            "  def result(self):\n",
            "    return self._buffer.mean()\n",
            "\n",
            "  @common.function\n",
            "  def reset(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 198,
        "API": ".assign(",
        "context": [
            "\n",
            "  @common.function\n",
            "  def reset(self):\n",
            "    self._buffer.clear()\n",
            "    self._return_accumulator.assign(tf.zeros_like(self._return_accumulator))\n",
            "\n",
            "\n",
            "@gin.configurable(module='tf_agents')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 228,
        "API": ".squeeze(",
        "context": [
            "    # Update accumulator with received rewards.\n",
            "    self._return_accumulator.assign_add(trajectory.reward)\n",
            "\n",
            "    # Add final returns to buffer.\n",
            "    last_episode_indices = tf.squeeze(tf.where(trajectory.is_last()), axis=-1)\n",
            "    for indx in last_episode_indices:\n",
            "      self._buffer.add(self._return_accumulator[indx])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 235,
        "API": ".max(",
        "context": [
            "\n",
            "    return trajectory\n",
            "\n",
            "  def result(self):\n",
            "    return self._buffer.max()\n",
            "\n",
            "  @common.function\n",
            "  def reset(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 277,
        "API": ".min(",
        "context": [
            "\n",
            "    return trajectory\n",
            "\n",
            "  def result(self):\n",
            "    return self._buffer.min()\n",
            "\n",
            "  @common.function\n",
            "  def reset(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 301,
        "API": ".function(",
        "context": [
            "    self._dtype = dtype\n",
            "    self._length_accumulator = common.create_variable(\n",
            "        initial_value=0, dtype=dtype, shape=(batch_size,), name='Accumulator')\n",
            "\n",
            "  @common.function(autograph=True)\n",
            "  def call(self, trajectory):\n",
            "    # Each non-boundary trajectory (first, mid or last) represents a step.\n",
            "    non_boundary_indices = tf.squeeze(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 308,
        "API": ".ones_like(",
        "context": [
            "    non_boundary_indices = tf.squeeze(\n",
            "        tf.where(tf.logical_not(trajectory.is_boundary())), axis=-1)\n",
            "    self._length_accumulator.scatter_add(\n",
            "        tf.IndexedSlices(\n",
            "            tf.ones_like(\n",
            "                non_boundary_indices, dtype=self._length_accumulator.dtype),\n",
            "            non_boundary_indices))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 313,
        "API": ".squeeze(",
        "context": [
            "                non_boundary_indices, dtype=self._length_accumulator.dtype),\n",
            "            non_boundary_indices))\n",
            "\n",
            "    # Add lengths to buffer when we hit end of episode\n",
            "    last_indices = tf.squeeze(tf.where(trajectory.is_last()), axis=-1)\n",
            "    for indx in last_indices:\n",
            "      self._buffer.add(self._length_accumulator[indx])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 320,
        "API": ".zeros_like(",
        "context": [
            "\n",
            "    # Clear length accumulator at the end of episodes.\n",
            "    self._length_accumulator.scatter_update(\n",
            "        tf.IndexedSlices(\n",
            "            tf.zeros_like(last_indices, dtype=self._dtype), last_indices))\n",
            "\n",
            "    return trajectory\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 330,
        "API": ".assign(",
        "context": [
            "\n",
            "  @common.function\n",
            "  def reset(self):\n",
            "    self._buffer.clear()\n",
            "    self._length_accumulator.assign(tf.zeros_like(self._length_accumulator))\n",
            "\n",
            "\n",
            "@gin.configurable(module='tf_agents')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 371,
        "API": ".map_structure(",
        "context": [
            "               dtype=tf.float32,\n",
            "               batch_size=1,\n",
            "               buffer_size=10):\n",
            "    self._batch_size = batch_size\n",
            "    self._buffer = tf.nest.map_structure(\n",
            "        lambda r: TFDeque(buffer_size, r.dtype, r.shape), reward_spec)\n",
            "    metric_names = _get_metric_names_from_spec(reward_spec)\n",
            "    self._dtype = dtype\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 377,
        "API": ".zeros(",
        "context": [
            "    metric_names = _get_metric_names_from_spec(reward_spec)\n",
            "    self._dtype = dtype\n",
            "    def create_acc(spec):\n",
            "      return common.create_variable(\n",
            "          initial_value=np.zeros((batch_size,) + spec.shape),\n",
            "          shape=(batch_size,) + spec.shape,\n",
            "          dtype=spec.dtype,\n",
            "          name='Accumulator/' + spec.name)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 386,
        "API": ".function(",
        "context": [
            "    self._reward_spec = reward_spec\n",
            "    super(AverageReturnMultiMetric, self).__init__(\n",
            "        name=name, prefix=prefix, metric_names=metric_names)\n",
            "\n",
            "  @common.function(autograph=True)\n",
            "  def call(self, trajectory):\n",
            "    nest_utils.assert_same_structure(trajectory.reward, self._reward_spec)\n",
            "    for buf, return_acc, reward in zip(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 391,
        "API": ".flatten(",
        "context": [
            "  def call(self, trajectory):\n",
            "    nest_utils.assert_same_structure(trajectory.reward, self._reward_spec)\n",
            "    for buf, return_acc, reward in zip(\n",
            "        tf.nest.flatten(self._buffer),\n",
            "        tf.nest.flatten(self._return_accumulator),\n",
            "        tf.nest.flatten(trajectory.reward)):\n",
            "      # Zero out batch indices where a new episode is starting.\n",
            "      is_start = trajectory.is_first()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 396,
        "API": ".reshape(",
        "context": [
            "        tf.nest.flatten(trajectory.reward)):\n",
            "      # Zero out batch indices where a new episode is starting.\n",
            "      is_start = trajectory.is_first()\n",
            "      if reward.shape.rank > 1:\n",
            "        is_start = tf.broadcast_to(tf.reshape(trajectory.is_first(), [-1, 1]),\n",
            "                                   tf.shape(return_acc))\n",
            "      return_acc.assign(\n",
            "          tf.where(is_start, tf.zeros_like(return_acc),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 406,
        "API": ".squeeze(",
        "context": [
            "      # Update accumulator with received rewards.\n",
            "      return_acc.assign_add(reward)\n",
            "\n",
            "      # Add final returns to buffer.\n",
            "      last_episode_indices = tf.squeeze(tf.where(trajectory.is_last()), axis=-1)\n",
            "      for indx in last_episode_indices:\n",
            "        buf.add(return_acc[indx])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 413,
        "API": ".map_structure(",
        "context": [
            "\n",
            "    return trajectory\n",
            "\n",
            "  def result(self):\n",
            "    return tf.nest.map_structure(lambda b: b.mean(), self._buffer)\n",
            "\n",
            "  @common.function\n",
            "  def reset(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 418,
        "API": ".assign(",
        "context": [
            "\n",
            "  @common.function\n",
            "  def reset(self):\n",
            "    tf.nest.map_structure(lambda b: b.clear(), self._buffer)\n",
            "    tf.nest.map_structure(lambda acc: acc.assign(tf.zeros_like(acc)),\n",
            "                          self._return_accumulator)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 423,
        "API": ".log(",
        "context": [
            "                          self._return_accumulator)\n",
            "\n",
            "\n",
            "def log_metrics(metrics, prefix=''):\n",
            "  log = ['{0} = {1}'.format(m.name, m.log().numpy()) for m in metrics]\n",
            "  logging.info('%s', '{0} \\n\\t\\t {1}'.format(prefix, '\\n\\t\\t '.join(log)))\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_metrics.py",
        "line_number": 428,
        "API": ".flatten(",
        "context": [
            "  logging.info('%s', '{0} \\n\\t\\t {1}'.format(prefix, '\\n\\t\\t '.join(log)))\n",
            "\n",
            "\n",
            "def _get_metric_names_from_spec(reward_spec):\n",
            "  reward_spec_flat = tf.nest.flatten(reward_spec)\n",
            "  metric_names_list = tf.nest.map_structure(lambda r: r.name, reward_spec_flat)\n",
            "  return metric_names_list\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_py_metric.py",
        "line_number": 76,
        "API": ".flatten(",
        "context": [
            "        packed_trajectories = tf.nest.pack_sequence_as(\n",
            "            structure=(trajectory), flat_sequence=flat_sequence)\n",
            "        return self._py_metric(packed_trajectories)\n",
            "\n",
            "    flattened_trajectories = tf.nest.flatten(trajectory)\n",
            "    metric_op = tf.py_function(\n",
            "        _call,\n",
            "        flattened_trajectories,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_py_metric.py",
        "line_number": 83,
        "API": ".control_dependencies(",
        "context": [
            "        flattened_trajectories,\n",
            "        [],\n",
            "        name='metric_call_py_func')\n",
            "\n",
            "    with tf.control_dependencies([metric_op]):\n",
            "      return tf.nest.map_structure(tf.identity, trajectory)\n",
            "\n",
            "  def result(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_py_metric.py",
        "line_number": 91,
        "API": ".py_function(",
        "context": [
            "    def _result():\n",
            "      with _check_not_called_concurrently(self._lock):\n",
            "        return self._py_metric.result()\n",
            "\n",
            "    result_value = tf.py_function(\n",
            "        _result,\n",
            "        [],\n",
            "        self._dtype,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_py_metric.py",
        "line_number": 96,
        "API": ".executing_eagerly(",
        "context": [
            "        _result,\n",
            "        [],\n",
            "        self._dtype,\n",
            "        name='metric_result_py_func')\n",
            "    if not tf.executing_eagerly():\n",
            "      result_value.set_shape(())\n",
            "    return result_value\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\metrics\\tf_py_metric.py",
        "line_number": 105,
        "API": ".py_function(",
        "context": [
            "    def _reset():\n",
            "      with _check_not_called_concurrently(self._lock):\n",
            "        return self._py_metric.reset()\n",
            "\n",
            "    return tf.py_function(\n",
            "        _reset, [], [],\n",
            "        name='metric_reset_py_func')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\actor_distribution_network.py",
        "line_number": 44,
        "API": ".log(",
        "context": [
            "                           init_action_stddev=0.35,\n",
            "                           init_means_output_factor=0.1,\n",
            "                           seed_stream_class=tfp.util.SeedStream,\n",
            "                           seed=None):\n",
            "  std_bias_initializer_value = np.log(np.exp(init_action_stddev) - 1)\n",
            "\n",
            "  return normal_projection_network.NormalProjectionNetwork(\n",
            "      action_spec,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\actor_distribution_network.py",
        "line_number": 159,
        "API": ".map_structure(",
        "context": [
            "          kwargs['seed'] = seed\n",
            "          kwargs['seed_stream_class'] = seed_stream_class\n",
            "        return continuous_projection_net(spec, **kwargs)\n",
            "\n",
            "    projection_networks = tf.nest.map_structure(map_proj, output_tensor_spec)\n",
            "    output_spec = tf.nest.map_structure(lambda proj_net: proj_net.output_spec,\n",
            "                                        projection_networks)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\actor_distribution_network.py",
        "line_number": 195,
        "API": ".map_structure(",
        "context": [
            "      distribution, _ = proj_net(\n",
            "          state, outer_rank, training=training, mask=mask)\n",
            "      return distribution\n",
            "\n",
            "    output_actions = tf.nest.map_structure(\n",
            "        call_projection_net, self._projection_networks)\n",
            "    return output_actions, network_state\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\actor_distribution_rnn_network.py",
        "line_number": 42,
        "API": ".log(",
        "context": [
            "\n",
            "def _normal_projection_net(action_spec,\n",
            "                           init_action_stddev=0.35,\n",
            "                           init_means_output_factor=0.1):\n",
            "  std_bias_initializer_value = np.log(np.exp(init_action_stddev) - 1)\n",
            "\n",
            "  return normal_projection_network.NormalProjectionNetwork(\n",
            "      action_spec,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\actor_distribution_rnn_network.py",
        "line_number": 159,
        "API": ".map_structure(",
        "context": [
            "        return discrete_projection_net(spec)\n",
            "      else:\n",
            "        return continuous_projection_net(spec)\n",
            "\n",
            "    projection_networks = tf.nest.map_structure(map_proj, output_tensor_spec)\n",
            "    output_spec = tf.nest.map_structure(lambda proj_net: proj_net.output_spec,\n",
            "                                        projection_networks)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\actor_distribution_rnn_network.py",
        "line_number": 182,
        "API": ".map_structure(",
        "context": [
            "    state, network_state = self._lstm_encoder(\n",
            "        observation, step_type=step_type, network_state=network_state,\n",
            "        training=training)\n",
            "    outer_rank = nest_utils.get_outer_rank(observation, self.input_tensor_spec)\n",
            "    output_actions = tf.nest.map_structure(\n",
            "        lambda proj_net: proj_net(state, outer_rank, training=training)[0],\n",
            "        self._projection_networks)\n",
            "    return output_actions, network_state\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\categorical_projection_network.py",
        "line_number": 48,
        "API": ".unique(",
        "context": [
            "      logits_init_output_factor: Output factor for initializing kernel logits\n",
            "        weights.\n",
            "      name: A string representing name of the network.\n",
            "    \"\"\"\n",
            "    unique_num_actions = np.unique(sample_spec.maximum - sample_spec.minimum +\n",
            "                                   1)\n",
            "    if len(unique_num_actions) > 1 or np.any(unique_num_actions <= 0):\n",
            "      raise ValueError('Bounds on discrete actions must be the same for all '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\categorical_projection_network.py",
        "line_number": 105,
        "API": ".flatten(",
        "context": [
            "    # outer_rank is needed because the projection is not done on the raw\n",
            "    # observations so getting the outer rank is hard as there is no spec to\n",
            "    # compare to.\n",
            "    batch_squash = utils.BatchSquash(outer_rank)\n",
            "    inputs = batch_squash.flatten(inputs)\n",
            "    inputs = tf.cast(inputs, tf.float32)\n",
            "\n",
            "    logits = self._projection_layer(inputs, training=training)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\categorical_projection_network.py",
        "line_number": 118,
        "API": ".expand_dims(",
        "context": [
            "      # dimension so the final shape is (B, 1, A), where A is the number of\n",
            "      # actions. This will make Categorical emit events shaped (B, 1) rather\n",
            "      # than (B,). Using axis -2 to allow for (B, T, 1, A) shaped q_values.\n",
            "      if mask.shape.rank < logits.shape.rank:\n",
            "        mask = tf.expand_dims(mask, -2)\n",
            "\n",
            "      # Overwrite the logits for invalid actions to a very large negative\n",
            "      # number. We do not use -inf because it produces NaNs in many tfp\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\categorical_projection_network.py",
        "line_number": 123,
        "API": ".constant(",
        "context": [
            "\n",
            "      # Overwrite the logits for invalid actions to a very large negative\n",
            "      # number. We do not use -inf because it produces NaNs in many tfp\n",
            "      # functions.\n",
            "      almost_neg_inf = tf.constant(logits.dtype.min, dtype=logits.dtype)\n",
            "      logits = tf.compat.v2.where(\n",
            "          tf.cast(mask, tf.bool), logits, almost_neg_inf)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\categorical_q_network.py",
        "line_number": 44,
        "API": ".linspace(",
        "context": [
            "  The first element of the output is a batch of logits based on the distribution\n",
            "  called C51 from Bellemare et al., 2017 (https://arxiv.org/abs/1707.06887). The\n",
            "  logits are used to compute approximate probability distributions for Q-values\n",
            "  for each potential action, by computing the probabilities at the 51 points\n",
            "  (called atoms) in np.linspace(-10.0, 10.0, 51).\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\categorical_q_network.py",
        "line_number": 138,
        "API": ".reshape(",
        "context": [
            "      A tuple `(logits, network_state)`.\n",
            "    \"\"\"\n",
            "    logits, network_state = self._q_network(\n",
            "        observation, step_type, network_state, training=training)\n",
            "    logits = tf.reshape(logits, [-1, self._num_actions, self._num_atoms])\n",
            "    return logits, network_state\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\encoding_network.py",
        "line_number": 69,
        "API": ".warning(",
        "context": [
            "  if type(layer) == tf.compat.v1.keras.layers.DenseFeatures:\n",
            "    raise ValueError('DenseFeatures V1 is not supported. '\n",
            "                     'Use tf.compat.v2.keras.layers.DenseFeatures instead.')\n",
            "  if layer.built:\n",
            "    logging.warning(\n",
            "        'Beware: Copying a layer that has already been built: \\'%s\\'.  '\n",
            "        'This can lead to subtle bugs because the original layer\\'s weights '\n",
            "        'will not be used in the copy.', layer.name)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\encoding_network.py",
        "line_number": 137,
        "API": ".copy(",
        "context": [
            "    ]\n",
            "    ```\n",
            "\n",
            "    **NOTE** `preprocessing_layers` and `preprocessing_combiner` are not allowed\n",
            "    to have already been built.  This ensures calls to `network.copy()` in the\n",
            "    future always have an unbuilt, fresh set of parameters.  Furtheremore,\n",
            "    a shallow copy of the layers is always created by the Network, so the\n",
            "    layer objects passed to the network are never modified.  For more details\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\encoding_network.py",
        "line_number": 192,
        "API": ".flatten(",
        "context": [
            "    if preprocessing_layers is None:\n",
            "      flat_preprocessing_layers = None\n",
            "    else:\n",
            "      flat_preprocessing_layers = [\n",
            "          _copy_layer(layer) for layer in tf.nest.flatten(preprocessing_layers)\n",
            "      ]\n",
            "      # Assert shallow structure is the same. This verifies preprocessing\n",
            "      # layers can be applied on expected input nests.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\encoding_network.py",
        "line_number": 204,
        "API": ".flatten(",
        "context": [
            "      if not nest.is_nested(input_tensor_spec):\n",
            "        input_nest = [input_tensor_spec]\n",
            "      nest.assert_shallow_structure(preprocessing_layers, input_nest)\n",
            "\n",
            "    if (len(tf.nest.flatten(input_tensor_spec)) > 1 and\n",
            "        preprocessing_combiner is None):\n",
            "      raise ValueError(\n",
            "          'preprocessing_combiner layer is required when more than 1 '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\encoding_network.py",
        "line_number": 291,
        "API": ".map_structure(",
        "context": [
            "\n",
            "    # Pull out the nest structure of the preprocessing layers. This avoids\n",
            "    # saving the original kwarg layers as a class attribute which Keras would\n",
            "    # then track.\n",
            "    self._preprocessing_nest = tf.nest.map_structure(lambda l: None,\n",
            "                                                     preprocessing_layers)\n",
            "    self._flat_preprocessing_layers = flat_preprocessing_layers\n",
            "    self._preprocessing_combiner = preprocessing_combiner\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\encoding_network.py",
        "line_number": 306,
        "API": ".map_structure(",
        "context": [
            "    if self._batch_squash:\n",
            "      outer_rank = nest_utils.get_outer_rank(\n",
            "          observation, self.input_tensor_spec)\n",
            "      batch_squash = utils.BatchSquash(outer_rank)\n",
            "      observation = tf.nest.map_structure(batch_squash.flatten, observation)\n",
            "\n",
            "    if self._flat_preprocessing_layers is None:\n",
            "      processed = observation\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\encoding_network.py",
        "line_number": 330,
        "API": ".map_structure(",
        "context": [
            "    for layer in self._postprocessing_layers:\n",
            "      states = layer(states, training=training)\n",
            "\n",
            "    if self._batch_squash:\n",
            "      states = tf.nest.map_structure(batch_squash.unflatten, states)\n",
            "\n",
            "    return states, network_state\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\expand_dims_layer.py",
        "line_number": 64,
        "API": ".expand_dims(",
        "context": [
            "\n",
            "  def call(self, inputs):\n",
            "    if self.axis < 0:\n",
            "      # Negative axis, so expand starting from the right\n",
            "      return tf.expand_dims(inputs, self.axis)\n",
            "    else:\n",
            "      # Perform the expansion from the left, but skip the batch dimension.\n",
            "      return tf.expand_dims(inputs, self.axis + 1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\layer_utils.py",
        "line_number": 42,
        "API": ".prod(",
        "context": [
            "  weight_shapes = [w.shape.as_list() for w in unique_weights]\n",
            "  standardized_weight_shapes = [\n",
            "      [0 if w_i is None else w_i for w_i in w] for w in weight_shapes\n",
            "  ]\n",
            "  return int(sum(np.prod(p) for p in standardized_weight_shapes))\n",
            "\n",
            "\n",
            "def _weight_memory_size(weights):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\layer_utils.py",
        "line_number": 63,
        "API": ".prod(",
        "context": [
            "    if not hasattr(w, \"shape\"):\n",
            "      continue\n",
            "    elif None in w.shape.as_list():\n",
            "      continue\n",
            "    weight_shape = np.prod(w.shape.as_list())\n",
            "    per_param_size = w.dtype.size\n",
            "    total_memory_size += weight_shape * per_param_size\n",
            "  return total_memory_size\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\layer_utils.py",
        "line_number": 129,
        "API": ".format(",
        "context": [
            "  units = [\"Byte\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n",
            "  scale = 1024\n",
            "  for unit in units:\n",
            "    if weight_memory_size / scale < 1:\n",
            "      return \"{:.2f} {}\".format(weight_memory_size, unit)\n",
            "    else:\n",
            "      weight_memory_size /= scale\n",
            "  return \"{:.2f} {}\".format(weight_memory_size, units[-1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\layer_utils.py",
        "line_number": 169,
        "API": ".get(",
        "context": [
            "    reduced_sharding_spec = list(sorted(set(layout.sharding_specs)))\n",
            "    if tf.experimental.dtensor.UNSHARDED in reduced_sharding_spec:\n",
            "      reduced_sharding_spec.remove(tf.experimental.dtensor.UNSHARDED)\n",
            "    reduced_sharding_spec = tuple(reduced_sharding_spec)  # For dict key\n",
            "    weight_count, memory_size = per_sharing_spec_result.get(\n",
            "        reduced_sharding_spec, (0, 0)\n",
            "    )\n",
            "    reduced_weight_shape = np.prod(w.shape.as_list())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\layer_utils.py",
        "line_number": 213,
        "API": ".prod(",
        "context": [
            "            \"are fully replicated\"\n",
            "        )\n",
            "        per_device_size = memory_size\n",
            "      else:\n",
            "        sharding_factor = np.prod([mesh.dim_size(s) for s in sharding_spec])\n",
            "        per_device_size = memory_size / sharding_factor\n",
            "        print_fn(\n",
            "            f\"{count} / {total_weight_count} params \"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\layer_utils.py",
        "line_number": 227,
        "API": ".format(",
        "context": [
            "        \"Overall per device memory usage: \"\n",
            "        f\"{_readable_memory_size(total_per_device_memory_size)}\"\n",
            "    )\n",
            "    print_fn(\n",
            "        \"Overall sharding factor: {:.2f}\".format(\n",
            "            total_memory_size / total_per_device_memory_size\n",
            "        )\n",
            "    )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\layer_utils.py",
        "line_number": 281,
        "API": ".flatten(",
        "context": [
            "    nodes_by_depth = model._nodes_by_depth.values()  # pylint: disable=protected-access\n",
            "    nodes = []\n",
            "    for v in nodes_by_depth:\n",
            "      if (len(v) > 1) or (\n",
            "          len(v) == 1 and len(tf.nest.flatten(v[0].keras_inputs)) > 1\n",
            "      ):\n",
            "        # if the model has multiple nodes\n",
            "        # or if the nodes have multiple inbound_layers\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\lstm_encoding_network.py",
        "line_number": 194,
        "API": ".map_structure(",
        "context": [
            "      counter[0] += 1\n",
            "      return tensor_spec.TensorSpec(\n",
            "          size, dtype=dtype, name='network_state_%d' % counter[0])\n",
            "\n",
            "    state_spec = tf.nest.map_structure(create_spec,\n",
            "                                       lstm_network.cell.state_size)\n",
            "\n",
            "    super(LSTMEncodingNetwork, self).__init__(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\lstm_encoding_network.py",
        "line_number": 244,
        "API": ".equal(",
        "context": [
            "        observation, step_type=step_type, network_state=(), training=training)\n",
            "\n",
            "    network_kwargs = {}\n",
            "    if isinstance(self._lstm_network, dynamic_unroll_layer.DynamicUnroll):\n",
            "      network_kwargs['reset_mask'] = tf.equal(step_type,\n",
            "                                              time_step.StepType.FIRST,\n",
            "                                              name='mask')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\lstm_encoding_network.py",
        "line_number": 260,
        "API": ".flatten(",
        "context": [
            "      state, network_state = output\n",
            "    else:\n",
            "      state = output[0]\n",
            "      network_state = tf.nest.pack_sequence_as(\n",
            "          self._lstm_network.cell.state_size, tf.nest.flatten(output[1:]))\n",
            "\n",
            "    for layer in self._output_encoder:\n",
            "      state = layer(state, training=training)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\lstm_encoding_network.py",
        "line_number": 267,
        "API": ".squeeze(",
        "context": [
            "      state = layer(state, training=training)\n",
            "\n",
            "    if not has_time_dim:\n",
            "      # Remove time dimension from the state.\n",
            "      state = tf.squeeze(state, [1])\n",
            "\n",
            "    return state, network_state\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\mask_splitter_network.py",
        "line_number": 141,
        "API": ".copy(",
        "context": [
            "    full_kwargs = dict(self._saved_kwargs, **kwargs)\n",
            "    if 'wrapped_network' not in kwargs:\n",
            "      # In the case of `wrapped_network` provided in `kwargs` copy is not called\n",
            "      # since it is assume it is already ready to use as it is.\n",
            "      full_kwargs['wrapped_network'] = self._wrapped_network.copy()\n",
            "    return type(self)(**full_kwargs)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\nest_map.py",
        "line_number": 93,
        "API": ".executing_eagerly(",
        "context": [
            "    Raises:\n",
            "      TypeError: If any of the layers are not instances of keras `Layer`.\n",
            "      ValueError: If `input_spec` is provided but its nest structure does\n",
            "        not match that of `nested_layers`.\n",
            "      RuntimeError: If not `tf.executing_eagerly()`; as this is required to\n",
            "        be able to create deep copies of layers in `layers`.\n",
            "    \"\"\"\n",
            "    if not tf.executing_eagerly():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\nest_map.py",
        "line_number": 100,
        "API": ".flatten(",
        "context": [
            "    if not tf.executing_eagerly():\n",
            "      raise RuntimeError(\n",
            "          'Not executing eagerly - cannot make deep copies of `nested_layers`.')\n",
            "\n",
            "    flat_nested_layers = tf.nest.flatten(nested_layers)\n",
            "    for layer in flat_nested_layers:\n",
            "      if not isinstance(layer, tf.keras.layers.Layer):\n",
            "        raise TypeError(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\nest_map.py",
        "line_number": 105,
        "API": ".format(",
        "context": [
            "    for layer in flat_nested_layers:\n",
            "      if not isinstance(layer, tf.keras.layers.Layer):\n",
            "        raise TypeError(\n",
            "            'Expected all layers to be instances of keras Layer, but saw'\n",
            "            ': \\'{}\\''.format(layer))\n",
            "\n",
            "    if input_spec is not None:\n",
            "      nest_utils.assert_same_structure(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\nest_map.py",
        "line_number": 113,
        "API": ".flatten(",
        "context": [
            "          nested_layers, input_spec,\n",
            "          message=(\n",
            "              '`nested_layers` and `input_spec` do not have matching structures'\n",
            "          ))\n",
            "      flat_input_spec = tf.nest.flatten(input_spec)\n",
            "    else:\n",
            "      flat_input_spec = [None] * len(flat_nested_layers)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\nest_map.py",
        "line_number": 147,
        "API": ".map_structure(",
        "context": [
            "\n",
            "  @property\n",
            "  def nested_layers(self) -> types.NestedNetwork:\n",
            "    # Return a shallow copy so users don't modify the layers list.\n",
            "    return tf.nest.map_structure(lambda m: m, self._nested_layers)\n",
            "\n",
            "  def copy(self, **kwargs) -> 'NestMap':\n",
            "    \"\"\"Make a copy of a `NestMap` instance.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\nest_map.py",
        "line_number": 185,
        "API": ".map_structure(",
        "context": [
            "          message=(\n",
            "              'network_state and state_spec do not have matching structure'))\n",
            "      nested_layers_state = network_state\n",
            "    else:\n",
            "      nested_layers_state = tf.nest.map_structure(\n",
            "          lambda _: (), self._nested_layers)\n",
            "\n",
            "    # Here we must use map_structure_up_to because nested_layers_state has a\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\network.py",
        "line_number": 74,
        "API": ".get(",
        "context": [
            "    if baseclasses[0] == tf.keras.layers.Layer:\n",
            "      # This is just Network below.  Return early.\n",
            "      return abc.ABCMeta.__new__(mcs, classname, baseclasses, attrs)\n",
            "\n",
            "    init = attrs.get(\"__init__\", None)\n",
            "\n",
            "    if not init:\n",
            "      # This wrapper class does not define an __init__.  When someone creates\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\network.py",
        "line_number": 140,
        "API": ".copy(",
        "context": [
            "  ```\n",
            "\n",
            "  To create a copy of the network:\n",
            "  ```python\n",
            "  cloned_net = net.copy()\n",
            "  cloned_net.variables  # Raises ValueError: cloned net does not share weights.\n",
            "  cloned_net.create_variables(...)\n",
            "  cloned_net.variables  # Now new variables have been created.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\network.py",
        "line_number": 217,
        "API": ".fill(",
        "context": [
            "\n",
            "    random_input = tensor_spec.sample_spec_nest(\n",
            "        input_tensor_spec, outer_dims=(1,))\n",
            "    initial_state = self.get_initial_state(batch_size=1)\n",
            "    step_type = tf.fill((1,), time_step.StepType.FIRST)\n",
            "    outputs = self.__call__(\n",
            "        random_input,\n",
            "        step_type=step_type,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\network.py",
        "line_number": 243,
        "API": ".map_structure(",
        "context": [
            "      else:\n",
            "        return tensor_spec.remove_outer_dims_nest(\n",
            "            tf.type_spec_from_value(x), num_outer_dims=1)\n",
            "\n",
            "    self._network_output_spec = tf.nest.map_structure(\n",
            "        _calc_unbatched_spec, outputs[0])\n",
            "    return self._network_output_spec\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\network.py",
        "line_number": 402,
        "API": ".get(",
        "context": [
            "    # Convert *args, **kwargs to a canonical kwarg representation.\n",
            "    normalized_kwargs = tf_inspect.getcallargs(\n",
            "        self.call, inputs, *args, **kwargs)\n",
            "    # TODO(b/156315434): Rename network_state to just state.\n",
            "    network_state = normalized_kwargs.get(\"network_state\", None)\n",
            "    normalized_kwargs.pop(\"self\", None)\n",
            "\n",
            "    if common.safe_has_state(network_state):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\network.py",
        "line_number": 418,
        "API": ".is_tensor(",
        "context": [
            "    if \"step_type\" not in call_argspec.args and not call_argspec.keywords:\n",
            "      normalized_kwargs.pop(\"step_type\", None)\n",
            "\n",
            "    # network_state can be a (), None, Tensor or NestedTensors.\n",
            "    if (not tf.is_tensor(network_state)\n",
            "        and network_state in (None, ())\n",
            "        and \"network_state\" not in call_argspec.args\n",
            "        and not call_argspec.keywords):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\network.py",
        "line_number": 503,
        "API": ".add(",
        "context": [
            "  while to_visit:\n",
            "    obj = to_visit.pop()\n",
            "    if obj in existing:\n",
            "      continue\n",
            "    existing.add(obj)\n",
            "    if _is_layer(obj):\n",
            "      yield obj\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\network.py",
        "line_number": 538,
        "API": ".is_tensor(",
        "context": [
            "  \"\"\"\n",
            "  def _maybe_convert_to_spec(p):\n",
            "    if isinstance(p, distribution_utils.Params):\n",
            "      return _convert_to_spec_and_remove_singleton_batch_dim(p, outer_ndim)\n",
            "    elif tf.is_tensor(p):\n",
            "      return tensor_spec.remove_outer_dims_nest(\n",
            "          tf.type_spec_from_value(p), num_outer_dims=outer_ndim)\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\network.py",
        "line_number": 546,
        "API": ".map_structure(",
        "context": [
            "      return p\n",
            "\n",
            "  return distribution_utils.Params(\n",
            "      type_=parameters.type_,\n",
            "      params=tf.nest.map_structure(_maybe_convert_to_spec, parameters.params))\n",
            "\n",
            "\n",
            "def create_variables(module: typing.Union[Network, tf.keras.layers.Layer],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\network.py",
        "line_number": 584,
        "API": ".format(",
        "context": [
            "  # Generic keras layer\n",
            "  if input_spec is None:\n",
            "    raise ValueError(\n",
            "        \"Module is a Keras layer; an input_spec is required but saw \"\n",
            "        \"None: {}\".format(module))\n",
            "\n",
            "  if isinstance(module, tf.keras.layers.RNN):\n",
            "    raise TypeError(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\network.py",
        "line_number": 589,
        "API": ".format(",
        "context": [
            "\n",
            "  if isinstance(module, tf.keras.layers.RNN):\n",
            "    raise TypeError(\n",
            "        \"Keras RNN layers (non-cell layers) must be wrapped in \"\n",
            "        \"tf_agents.keras_layers.RNNWrapper.  Layer: {}\".format(module))\n",
            "\n",
            "  maybe_spec = getattr(module, \"_network_output_spec\", None)\n",
            "  if maybe_spec is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\network.py",
        "line_number": 612,
        "API": ".map_structure(",
        "context": [
            "      # Convert tensor to its type-spec, and remove the batch dimension\n",
            "      # from the spec.\n",
            "      spec = tf.type_spec_from_value(t)\n",
            "      return nest_utils.remove_singleton_batch_spec_dim(spec, outer_ndim=1)\n",
            "    state_spec = tf.nest.map_structure(remove_singleton_batch_spec_dim, state)\n",
            "\n",
            "    outputs = module(random_input, state, **kwargs)\n",
            "    # tf.keras.layers.{LSTMCell, ...} return (output, [state1, state2,...]).\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\network.py",
        "line_number": 634,
        "API": ".map_structure(",
        "context": [
            "      return tensor_spec.remove_outer_dims_nest(\n",
            "          tf.type_spec_from_value(x), num_outer_dims=outer_ndim)\n",
            "\n",
            "  # pylint: disable=protected-access\n",
            "  module._network_output_spec = tf.nest.map_structure(_calc_unbatched_spec,\n",
            "                                                      output)\n",
            "  module._network_state_spec = state_spec\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\network.py",
        "line_number": 650,
        "API": ".format(",
        "context": [
            "    raise TypeError(\n",
            "        \"Saw a tf.keras.layers.RNN layer nested inside e.g. a keras Sequential \"\n",
            "        \"layer.  This is not directly supported.  Please wrap your layer \"\n",
            "        \"inside a `tf_agents.keras_layers.RNNWrapper` or use \"\n",
            "        \"`tf_agents.networks.Sequential`.  Layer: {}\".format(layer))\n",
            "  if isinstance(layer, tf.keras.layers.TimeDistributed):\n",
            "    return 1 + _get_input_outer_ndim(layer.layer, input_spec)\n",
            "  if isinstance(layer, tf.keras.Sequential):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\network.py",
        "line_number": 671,
        "API": ".flatten(",
        "context": [
            "  if outer_ndim is None:\n",
            "    return 1\n",
            "\n",
            "  if input_spec:\n",
            "    input_spec = tf.nest.flatten(input_spec)[0]\n",
            "    if outer_ndim >= input_spec.shape.ndims:\n",
            "      # We can capture the \"outer batch size\" as the diff between the\n",
            "      # expected input rank and the rank of the non-batched spec passed in the\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\network.py",
        "line_number": 702,
        "API": ".format(",
        "context": [
            "    return layer.state_spec\n",
            "\n",
            "  if isinstance(layer, tf.keras.layers.RNN):\n",
            "    raise TypeError(\"RNN Layer must be wrapped inside \"\n",
            "                    \"`tf_agents.keras_layers.RNNWrapper`: {}\".format(layer))\n",
            "\n",
            "  initial_state = getattr(layer, \"get_initial_state\", None)\n",
            "  state_size = getattr(layer, \"state_size\", None)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\network.py",
        "line_number": 709,
        "API": ".format(",
        "context": [
            "  state_size = getattr(layer, \"state_size\", None)\n",
            "  if initial_state is not None and state_size is None:\n",
            "    raise ValueError(\n",
            "        \"Layer lacks a `state_size` property.  Unable to extract state \"\n",
            "        \"spec: {}\".format(layer))\n",
            "  state_spec = ()\n",
            "  if state_size is not None:\n",
            "    state_spec = tf.nest.map_structure(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\normal_projection_network.py",
        "line_number": 39,
        "API": ".tanh(",
        "context": [
            "  \"\"\"Maps inputs with arbitrary range to range defined by spec using `tanh`.\"\"\"\n",
            "  means = (spec.maximum + spec.minimum) / 2.0\n",
            "  magnitudes = (spec.maximum - spec.minimum) / 2.0\n",
            "\n",
            "  return means + magnitudes * tf.tanh(inputs)\n",
            "\n",
            "\n",
            "@gin.configurable\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\normal_projection_network.py",
        "line_number": 91,
        "API": ".flatten(",
        "context": [
            "        tfp.util.SeedStream, except for in unit testing, when one may want to\n",
            "        seed all the layers deterministically.\n",
            "      name: A string representing name of the network.\n",
            "    \"\"\"\n",
            "    if len(tf.nest.flatten(sample_spec)) != 1:\n",
            "      raise ValueError('Normal Projection network only supports single spec '\n",
            "                       'samples.')\n",
            "    self._scale_distribution = scale_distribution\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\normal_projection_network.py",
        "line_number": 159,
        "API": ".copy(",
        "context": [
            "        # `param_static_shapes`, even when using MVNDiag the spec\n",
            "        # continues to use the terms 'loc' and 'scale'.  Here we have to massage\n",
            "        # the construction to use 'scale' for kwarg 'scale_diag'.  Since they\n",
            "        # have the same shape and dtype expectationts, this is okay.\n",
            "        kwargs = kwargs.copy()\n",
            "        kwargs['scale_diag'] = kwargs['scale']\n",
            "        del kwargs['scale']\n",
            "        distribution = tfp.distributions.MultivariateNormalDiag(*args, **kwargs)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\normal_projection_network.py",
        "line_number": 181,
        "API": ".format(",
        "context": [
            "\n",
            "    if mask is not None:\n",
            "      raise NotImplementedError(\n",
            "          'NormalProjectionNetwork does not yet implement action masking; got '\n",
            "          'mask={}'.format(mask))\n",
            "\n",
            "    # outer_rank is needed because the projection is not done on the raw\n",
            "    # observations so getting the outer rank is hard as there is no spec to\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\normal_projection_network.py",
        "line_number": 187,
        "API": ".flatten(",
        "context": [
            "    # outer_rank is needed because the projection is not done on the raw\n",
            "    # observations so getting the outer rank is hard as there is no spec to\n",
            "    # compare to.\n",
            "    batch_squash = network_utils.BatchSquash(outer_rank)\n",
            "    inputs = batch_squash.flatten(inputs)\n",
            "\n",
            "    means = self._means_projection_layer(inputs, training=training)\n",
            "    means = tf.reshape(means, [-1] + self._sample_spec.shape.as_list())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\normal_projection_network.py",
        "line_number": 195,
        "API": ".cast(",
        "context": [
            "\n",
            "    # If scaling the distribution later, use a normalized mean.\n",
            "    if not self._scale_distribution and self._mean_transform is not None:\n",
            "      means = self._mean_transform(means, self._sample_spec)\n",
            "    means = tf.cast(means, self._sample_spec.dtype)\n",
            "\n",
            "    if self._state_dependent_std:\n",
            "      stds = self._stddev_projection_layer(inputs, training=training)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\normal_projection_network.py",
        "line_number": 200,
        "API": ".zeros_like(",
        "context": [
            "\n",
            "    if self._state_dependent_std:\n",
            "      stds = self._stddev_projection_layer(inputs, training=training)\n",
            "    else:\n",
            "      stds = self._bias(tf.zeros_like(means), training=training)\n",
            "      stds = tf.reshape(stds, [-1] + self._sample_spec.shape.as_list())\n",
            "\n",
            "    if self._std_transform is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\normal_projection_network.py",
        "line_number": 205,
        "API": ".cast(",
        "context": [
            "      stds = tf.reshape(stds, [-1] + self._sample_spec.shape.as_list())\n",
            "\n",
            "    if self._std_transform is not None:\n",
            "      stds = self._std_transform(stds)\n",
            "    stds = tf.cast(stds, self._sample_spec.dtype)\n",
            "\n",
            "    means = batch_squash.unflatten(means)\n",
            "    stds = batch_squash.unflatten(stds)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\q_network.py",
        "line_number": 32,
        "API": ".flatten(",
        "context": [
            "def validate_specs(action_spec, observation_spec):\n",
            "  \"\"\"Validates the spec contains a single action.\"\"\"\n",
            "  del observation_spec  # not currently validated\n",
            "\n",
            "  flat_action_spec = tf.nest.flatten(action_spec)\n",
            "  if len(flat_action_spec) > 1:\n",
            "    raise ValueError('Network only supports action_specs with a single action.')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\q_network.py",
        "line_number": 101,
        "API": ".flatten(",
        "context": [
            "      ValueError: If `input_tensor_spec` contains more than one observation. Or\n",
            "        if `action_spec` contains more than one action.\n",
            "    \"\"\"\n",
            "    validate_specs(action_spec, input_tensor_spec)\n",
            "    action_spec = tf.nest.flatten(action_spec)[0]\n",
            "    num_actions = action_spec.maximum - action_spec.minimum + 1\n",
            "    encoder_input_tensor_spec = input_tensor_spec\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\q_rnn_network.py",
        "line_number": 99,
        "API": ".flatten(",
        "context": [
            "      ValueError: If neither `lstm_size` nor `rnn_construction_fn` are provided.\n",
            "      ValueError: If both `lstm_size` and `rnn_construction_fn` are provided.\n",
            "    \"\"\"\n",
            "    q_network.validate_specs(action_spec, input_tensor_spec)\n",
            "    action_spec = tf.nest.flatten(action_spec)[0]\n",
            "    num_actions = action_spec.maximum - action_spec.minimum + 1\n",
            "\n",
            "    q_projection = layers.Dense(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\sequential.py",
        "line_number": 106,
        "API": ".format(",
        "context": [
            "      TypeError: If any of the layers are not instances of keras `Layer`.\n",
            "    \"\"\"\n",
            "    if not layers:\n",
            "      raise ValueError(\n",
            "          '`layers` must not be empty; saw: {}'.format(layers))\n",
            "    for layer in layers:\n",
            "      if not isinstance(layer, tf.keras.layers.Layer):\n",
            "        raise TypeError(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\sequential.py",
        "line_number": 111,
        "API": ".format(",
        "context": [
            "    for layer in layers:\n",
            "      if not isinstance(layer, tf.keras.layers.Layer):\n",
            "        raise TypeError(\n",
            "            'Expected all layers to be instances of keras Layer, but saw'\n",
            "            ': \\'{}\\''.format(layer))\n",
            "\n",
            "    layers = [\n",
            "        rnn_wrapper.RNNWrapper(layer) if isinstance(layer, tf.keras.layers.RNN)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\sequential.py",
        "line_number": 124,
        "API": ".flatten(",
        "context": [
            "\n",
            "    # Now we remove all of the empty state specs so if there are no RNN layers,\n",
            "    # our state spec is empty.  layer_has_state is a list of bools telling us\n",
            "    # which layers have a non-empty state and which don't.\n",
            "    flattened_specs = [tf.nest.flatten(s) for s in state_spec]\n",
            "    layer_has_state = [bool(fs) for fs in flattened_specs]\n",
            "    state_spec = tuple(\n",
            "        s for s, has_state in zip(state_spec, layer_has_state) if has_state)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\sequential.py",
        "line_number": 137,
        "API": ".copy(",
        "context": [
            "\n",
            "  @property\n",
            "  def layers(self) -> List[tf.keras.layers.Layer]:\n",
            "    # Return a shallow copy so users don't modify the layers list.\n",
            "    return copy.copy(self._sequential_layers)\n",
            "\n",
            "  def copy(self, **kwargs) -> 'Sequential':\n",
            "    \"\"\"Make a copy of a `Sequential` instance.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\sequential.py",
        "line_number": 154,
        "API": ".executing_eagerly(",
        "context": [
            "    Returns:\n",
            "      A deep copy of this network.\n",
            "\n",
            "    Raises:\n",
            "      RuntimeError: If not `tf.executing_eagerly()`; as this is required to\n",
            "        be able to create deep copies of layers in `layers`.\n",
            "    \"\"\"\n",
            "    if not tf.executing_eagerly():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\sequential.py",
        "line_number": 167,
        "API": ".is_tensor(",
        "context": [
            "      new_kwargs['layers'] = new_layers\n",
            "    return type(self)(**new_kwargs)\n",
            "\n",
            "  def call(self, inputs, network_state=(), **kwargs):\n",
            "    if not tf.is_tensor(network_state) and not network_state:\n",
            "      network_state = ((),) * len(self.state_spec)\n",
            "    next_network_state = [()] * len(self.state_spec)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\sequential.py",
        "line_number": 172,
        "API": ".copy(",
        "context": [
            "      network_state = ((),) * len(self.state_spec)\n",
            "    next_network_state = [()] * len(self.state_spec)\n",
            "\n",
            "    # Only Networks are expected to know about step_type; not Keras layers.\n",
            "    layer_kwargs = kwargs.copy()\n",
            "    layer_kwargs.pop('step_type', None)\n",
            "\n",
            "    stateful_layer_idx = 0\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\utils.py",
        "line_number": 40,
        "API": ".format(",
        "context": [
            "          and output_spec.dtype.is_floating):\n",
            "    raise ValueError(\n",
            "        'Expected {} to emit a floating point tensor with inner dims '\n",
            "        '{}; but saw network output spec: {}'\n",
            "        .format(label, expected_output_shape, output_spec))\n",
            "\n",
            "\n",
            "class BatchSquash(object):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\utils.py",
        "line_number": 72,
        "API": ".shape(",
        "context": [
            "    with tf.name_scope('batch_flatten'):\n",
            "      if self._batch_dims == 1:\n",
            "        return tensor\n",
            "\n",
            "      self._original_tensor_shape = composite.shape(tensor)\n",
            "\n",
            "      if tensor.shape[self._batch_dims:].is_fully_defined():\n",
            "        return composite.reshape(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\utils.py",
        "line_number": 78,
        "API": ".reshape(",
        "context": [
            "      if tensor.shape[self._batch_dims:].is_fully_defined():\n",
            "        return composite.reshape(\n",
            "            tensor, [-1] + tensor.shape[self._batch_dims:].as_list())\n",
            "\n",
            "      reshaped = composite.reshape(\n",
            "          tensor,\n",
            "          tf.concat([[-1], composite.shape(tensor)[self._batch_dims:]], axis=0),\n",
            "      )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\utils.py",
        "line_number": 92,
        "API": ".prod(",
        "context": [
            "      if (isinstance(tensor, tf.Tensor) and\n",
            "          tensor.shape[:self._batch_dims].is_fully_defined()):\n",
            "        return tf.ensure_shape(\n",
            "            reshaped,\n",
            "            [np.prod(tensor.shape[:self._batch_dims], dtype=np.int64)] +\n",
            "            tensor.shape[self._batch_dims:])\n",
            "      return reshaped\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\utils.py",
        "line_number": 106,
        "API": ".reshape(",
        "context": [
            "      if self._original_tensor_shape is None:\n",
            "        raise ValueError('Please call flatten before unflatten.')\n",
            "\n",
            "      # pyformat: disable\n",
            "      return composite.reshape(\n",
            "          tensor,\n",
            "          tf.concat([\n",
            "              self._original_tensor_shape[:self._batch_dims],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\utils.py",
        "line_number": 167,
        "API": ".join(",
        "context": [
            "            kernel_size=kernel_size,\n",
            "            strides=strides,\n",
            "            activation=activation_fn,\n",
            "            kernel_initializer=clone_initializer(kernel_initializer),\n",
            "            name='/'.join([name, 'conv2d']) if name else None)\n",
            "        for (filters, kernel_size, strides) in conv_layer_params\n",
            "    ])\n",
            "  layers.append(tf.keras.layers.Flatten())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\utils.py",
        "line_number": 199,
        "API": ".join(",
        "context": [
            "          num_units,\n",
            "          activation=activation_fn,\n",
            "          kernel_initializer=clone_initializer(kernel_initializer),\n",
            "          kernel_regularizer=kernel_regularizer,\n",
            "          name='/'.join([name, 'dense%d' % i]) if name else None))\n",
            "      if not isinstance(dropout_params, dict):\n",
            "        dropout_params = {'rate': dropout_params} if dropout_params else None\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\value_network.py",
        "line_number": 127,
        "API": ".squeeze(",
        "context": [
            "    state, network_state = self._encoder(\n",
            "        observation, step_type=step_type, network_state=network_state,\n",
            "        training=training)\n",
            "    value = self._postprocessing_layers(state, training=training)\n",
            "    return tf.squeeze(value, -1), network_state\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\networks\\value_rnn_network.py",
        "line_number": 131,
        "API": ".squeeze(",
        "context": [
            "    state, network_state = self._lstm_encoder(\n",
            "        observation, step_type=step_type, network_state=network_state,\n",
            "        training=training)\n",
            "    value = self._postprocessing_layers(state, training=training)\n",
            "    return tf.squeeze(value, -1), network_state\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\actor_policy.py",
        "line_number": 107,
        "API": ".format(",
        "context": [
            "    action_spec = tensor_spec.from_spec(action_spec)\n",
            "\n",
            "    if not isinstance(actor_network, network.Network):\n",
            "      raise ValueError('actor_network must be a network.Network. Found '\n",
            "                       '{}.'.format(type(actor_network)))\n",
            "\n",
            "    # Create variables regardless of if we use the output spec.\n",
            "    actor_output_spec = actor_network.create_variables(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\actor_policy.py",
        "line_number": 114,
        "API": ".map_structure(",
        "context": [
            "    actor_output_spec = actor_network.create_variables(\n",
            "        time_step_spec.observation)\n",
            "\n",
            "    if isinstance(actor_network, network.DistributionNetwork):\n",
            "      actor_output_spec = tf.nest.map_structure(\n",
            "          lambda o: o.sample_spec, actor_network.output_spec)\n",
            "\n",
            "    distribution_utils.assert_specs_are_compatible(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\actor_policy.py",
        "line_number": 126,
        "API": ".flatten(",
        "context": [
            "    self._observation_normalizer = observation_normalizer\n",
            "    self._training = training\n",
            "\n",
            "    if observation_and_action_constraint_splitter is not None:\n",
            "      if len(tf.nest.flatten(action_spec)) > 1 or (\n",
            "          not tensor_spec.is_discrete(action_spec)):\n",
            "        raise NotImplementedError(\n",
            "            'Action constraints for ActorPolicy are currently only supported '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\actor_policy.py",
        "line_number": 149,
        "API": ".normalize(",
        "context": [
            "\n",
            "  def _apply_actor_network(self, observation, step_type, policy_state,\n",
            "                           mask=None):\n",
            "    if self._observation_normalizer:\n",
            "      observation = self._observation_normalizer.normalize(observation)\n",
            "    if mask is None:\n",
            "      return self._actor_network(\n",
            "          observation, step_type=step_type, network_state=policy_state,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\actor_policy.py",
        "line_number": 189,
        "API": ".map_structure(",
        "context": [
            "        # This is an action tensor, so wrap it in a deterministic distribution.\n",
            "        return tfp.distributions.Deterministic(loc=action_or_distribution)\n",
            "      return action_or_distribution\n",
            "\n",
            "    distributions = tf.nest.map_structure(_to_distribution,\n",
            "                                          actions_or_distributions)\n",
            "    return policy_step.PolicyStep(distributions, policy_state)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\async_policy_saver.py",
        "line_number": 56,
        "API": ".info(",
        "context": [
            "          self._save_condition_variable.wait()\n",
            "          if self._join_save_thread:\n",
            "            return\n",
            "        if self._saving_checkpoint:\n",
            "          logging.info(\"Saving checkpoint to %s\", self._export_dir)\n",
            "          self._policy_saver.save_checkpoint(self._export_dir)\n",
            "        else:\n",
            "          logging.info(\"Saving policy to %s\", self._export_dir)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\async_policy_saver.py",
        "line_number": 113,
        "API": ".info(",
        "context": [
            "\n",
            "    if blocking:\n",
            "      with self._save_condition_variable:\n",
            "        while self._export_dir:\n",
            "          logging.info(\"Waiting for AsyncPolicySaver to finish.\")\n",
            "          self._save_condition_variable.wait()\n",
            "        if saving_checkpoint:\n",
            "          self._policy_saver.save_checkpoint(export_dir)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\async_policy_saver.py",
        "line_number": 118,
        "API": ".save(",
        "context": [
            "          self._save_condition_variable.wait()\n",
            "        if saving_checkpoint:\n",
            "          self._policy_saver.save_checkpoint(export_dir)\n",
            "        else:\n",
            "          self._policy_saver.save(export_dir)\n",
            "      return\n",
            "\n",
            "    if not self._save_condition_variable.acquire(blocking=False):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\async_policy_saver.py",
        "line_number": 135,
        "API": ".info(",
        "context": [
            "  def flush(self):\n",
            "    \"\"\"Blocks until there is no saving happening.\"\"\"\n",
            "    with self._save_condition_variable:\n",
            "      while self._export_dir:\n",
            "        logging.info(\"Waiting for AsyncPolicySaver to finish.\")\n",
            "        self._save_condition_variable.wait()\n",
            "\n",
            "  def close(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\async_policy_saver.py",
        "line_number": 142,
        "API": ".info(",
        "context": [
            "  def close(self):\n",
            "    \"\"\"Blocks until there is no saving happening and kills the save_thread.\"\"\"\n",
            "    with self._save_condition_variable:\n",
            "      while self._export_dir:\n",
            "        logging.info(\"Waiting for AsyncPolicySaver to finish.\")\n",
            "        self._save_condition_variable.wait()\n",
            "      self._join_save_thread = True\n",
            "      self._save_condition_variable.notify()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\batched_py_policy.py",
        "line_number": 118,
        "API": ".join(",
        "context": [
            "  def __del__(self):\n",
            "    \"\"\"Join external processes, if necessary.\"\"\"\n",
            "    if self._parallel_execution:  # pytype: disable=attribute-error  # trace-all-classes\n",
            "      self._pool.close()  # pytype: disable=attribute-error  # trace-all-classes\n",
            "      self._pool.join()  # pytype: disable=attribute-error  # trace-all-classes\n",
            "\n",
            "  def _validate_spec(self, policy_spec_method, spec_to_match):\n",
            "    # pytype: disable=attribute-error\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\batched_py_policy.py",
        "line_number": 166,
        "API": ".format(",
        "context": [
            "      NotImplementedError: if `seed` is not None.\n",
            "    \"\"\"\n",
            "    if seed is not None:\n",
            "      raise NotImplementedError(\n",
            "          \"seed is not supported; but saw seed: {}\".format(seed))\n",
            "    if self._num_policies == 1:  # pytype: disable=attribute-error  # trace-all-classes\n",
            "      time_step = nest_utils.unbatch_nested_array(time_step)\n",
            "      policy_state = nest_utils.unbatch_nested_array(policy_state)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\boltzmann_policy.py",
        "line_number": 75,
        "API": ".copy(",
        "context": [
            "\n",
            "  def _apply_temperature(self, dist):\n",
            "    \"\"\"Change the action distribution to incorporate the temperature.\"\"\"\n",
            "    logits = dist.logits / self._get_temperature_value()\n",
            "    return dist.copy(logits=logits)\n",
            "\n",
            "  def _distribution(self, time_step, policy_state):\n",
            "    distribution_step = self._wrapped_policy.distribution(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\boltzmann_policy.py",
        "line_number": 83,
        "API": ".map_structure(",
        "context": [
            "        time_step, policy_state)\n",
            "    if self._temperature is None:\n",
            "      return distribution_step\n",
            "\n",
            "    action_dist = tf.nest.map_structure(self._apply_temperature,\n",
            "                                        distribution_step.action)\n",
            "    return distribution_step._replace(action=action_dist)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\categorical_q_policy.py",
        "line_number": 106,
        "API": ".format(",
        "context": [
            "      raise ValueError(\n",
            "          'Action specs should have minimum of 0, but saw: {0}.  If collecting '\n",
            "          'from a python environment, consider using '\n",
            "          'tf_agents.environments.wrappers.ActionOffsetWrapper.'\n",
            "          .format(action_spec))\n",
            "\n",
            "    num_actions = action_spec.maximum - action_spec.minimum + 1\n",
            "    try:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\categorical_q_policy.py",
        "line_number": 127,
        "API": ".convert_to_tensor(",
        "context": [
            "        policy_state_spec=q_network.state_spec,\n",
            "        observation_and_action_constraint_splitter=(\n",
            "            observation_and_action_constraint_splitter))\n",
            "\n",
            "    self._temperature = tf.convert_to_tensor(temperature, dtype=tf.float32)\n",
            "    self._q_network = q_network\n",
            "\n",
            "    # Generate support in numpy so that we can assign it to a constant and avoid\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\categorical_q_policy.py",
        "line_number": 132,
        "API": ".linspace(",
        "context": [
            "    self._q_network = q_network\n",
            "\n",
            "    # Generate support in numpy so that we can assign it to a constant and avoid\n",
            "    # having a tensor property.\n",
            "    support = np.linspace(min_q_value, max_q_value, self._num_atoms,\n",
            "                          dtype=np.float32)\n",
            "    self._support = tf.constant(support, dtype=tf.float32)\n",
            "    self._action_dtype = action_spec.dtype\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\categorical_q_policy.py",
        "line_number": 172,
        "API": ".constant(",
        "context": [
            "    logits = q_values\n",
            "\n",
            "    if observation_and_action_constraint_splitter is not None:\n",
            "      # Overwrite the logits for invalid actions to -inf.\n",
            "      neg_inf = tf.constant(-np.inf, dtype=logits.dtype)\n",
            "      logits = tf.compat.v2.where(tf.cast(mask, tf.bool), logits, neg_inf)\n",
            "\n",
            "    action_spec = cast(tf.TensorSpec, self.action_spec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\epsilon_greedy_policy.py",
        "line_number": 118,
        "API": ".uniform(",
        "context": [
            "    greedy_action = self._greedy_policy.action(time_step, policy_state)\n",
            "    random_action = self._random_policy.action(time_step, (), seed_stream())\n",
            "\n",
            "    outer_shape = nest_utils.get_outer_shape(time_step, self._time_step_spec)\n",
            "    rng = tf.random.uniform(\n",
            "        outer_shape, maxval=1.0, seed=seed_stream(), name='epsilon_rng')\n",
            "    cond = tf.greater_equal(rng, self._get_epsilon())\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\epsilon_greedy_policy.py",
        "line_number": 131,
        "API": ".map_structure(",
        "context": [
            "    outer_ndims = int(outer_shape.shape[0])\n",
            "    if outer_ndims >= 2:\n",
            "      raise ValueError(\n",
            "          'Only supports batched time steps with a single batch dimension')\n",
            "    action = tf.nest.map_structure(lambda g, r: tf.compat.v1.where(cond, g, r),\n",
            "                                   greedy_action.action, random_action.action)\n",
            "\n",
            "    if greedy_action.info:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\epsilon_greedy_policy.py",
        "line_number": 138,
        "API": ".where(",
        "context": [
            "    if greedy_action.info:\n",
            "      if not random_action.info:\n",
            "        raise ValueError('Incompatible info field')\n",
            "      # Note that the objects in PolicyInfo may have different shapes, so we\n",
            "      # need to call nest_utils.where() on each type of object.\n",
            "      info = tf.nest.map_structure(lambda x, y: nest_utils.where(cond, x, y),\n",
            "                                   greedy_action.info, random_action.info)\n",
            "      if self._emit_log_probability:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\epsilon_greedy_policy.py",
        "line_number": 146,
        "API": ".map_structure(",
        "context": [
            "        # At this point, info.log_probability contains the log prob of the\n",
            "        # action chosen, conditioned on the policy that was chosen. We want to\n",
            "        # emit the full log probability of the action, so we'll add in the log\n",
            "        # probability of choosing the policy.\n",
            "        random_log_prob = tf.nest.map_structure(\n",
            "            lambda t: tf.math.log(tf.zeros_like(t) + self._get_epsilon()),\n",
            "            info.log_probability)\n",
            "        greedy_log_prob = tf.nest.map_structure(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\epsilon_greedy_policy.py",
        "line_number": 152,
        "API": ".where(",
        "context": [
            "            info.log_probability)\n",
            "        greedy_log_prob = tf.nest.map_structure(\n",
            "            lambda t: tf.math.log(tf.ones_like(t) - self._get_epsilon()),\n",
            "            random_log_prob)\n",
            "        log_prob_of_chosen_policy = nest_utils.where(cond, greedy_log_prob,\n",
            "                                                     random_log_prob)\n",
            "        log_prob = tf.nest.map_structure(lambda a, b: a + b,\n",
            "                                         log_prob_of_chosen_policy,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\epsilon_greedy_policy.py",
        "line_number": 163,
        "API": ".reshape(",
        "context": [
            "      if policy_utilities.has_bandit_policy_type(info, check_for_tensor=True):\n",
            "        # Generate mask of the same shape as bandit_policy_type (batch_size, 1).\n",
            "        # This is the opposite of `cond`, which is 1-D bool tensor (batch_size,)\n",
            "        # that is true when greedy policy was used, otherwise `cond` is false.\n",
            "        random_policy_mask = tf.reshape(tf.logical_not(cond),\n",
            "                                        tf.shape(info.bandit_policy_type))  # pytype: disable=attribute-error\n",
            "        bandit_policy_type = policy_utilities.bandit_policy_uniform_mask(\n",
            "            info.bandit_policy_type, mask=random_policy_mask)  # pytype: disable=attribute-error\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\fixed_policy.py",
        "line_number": 71,
        "API": ".convert_to_tensor(",
        "context": [
            "                                      emit_log_probability=emit_log_probability)\n",
            "    nest_utils.assert_same_structure(self._action_spec, actions)\n",
            "\n",
            "    def convert(action, spec):\n",
            "      return tf.convert_to_tensor(value=action, dtype=spec.dtype)\n",
            "\n",
            "    self._action_value = tf.nest.map_structure(convert, actions,\n",
            "                                               self._action_spec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\fixed_policy.py",
        "line_number": 76,
        "API": ".map_structure(",
        "context": [
            "\n",
            "    self._action_value = tf.nest.map_structure(convert, actions,\n",
            "                                               self._action_spec)\n",
            "    if self._emit_log_probability:\n",
            "      log_probability = tf.nest.map_structure(\n",
            "          lambda t: tf.constant(0.0, tf.float32), self._action_spec)\n",
            "      self._policy_info = policy_step.set_log_probability(policy_info,\n",
            "                                                          log_probability)  # pytype: disable=wrong-arg-types\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\fixed_policy.py",
        "line_number": 90,
        "API": ".map_structure(",
        "context": [
            "  def _get_policy_info_and_action(self, time_step):\n",
            "    outer_shape = nest_utils.get_outer_shape(time_step, self._time_step_spec)\n",
            "\n",
            "    if self._emit_log_probability:\n",
            "      log_probability = tf.nest.map_structure(\n",
            "          lambda _: tf.zeros(outer_shape, tf.float32), self._action_spec)\n",
            "      policy_info = policy_step.set_log_probability(\n",
            "          self._policy_info, log_probability=log_probability)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\fixed_policy.py",
        "line_number": 96,
        "API": ".map_structure(",
        "context": [
            "      policy_info = policy_step.set_log_probability(\n",
            "          self._policy_info, log_probability=log_probability)\n",
            "    else:\n",
            "      policy_info = self._policy_info\n",
            "    action = tf.nest.map_structure(lambda t: common.replicate(t, outer_shape),\n",
            "                                   self._action_value)\n",
            "    return policy_info, action\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\fixed_policy.py",
        "line_number": 112,
        "API": ".map_structure(",
        "context": [
            "    def dist_fn(action):\n",
            "      \"\"\"Return a categorical distribution with all density on fixed action.\"\"\"\n",
            "      return tfp.distributions.Deterministic(loc=action)\n",
            "    return policy_step.PolicyStep(\n",
            "        tf.nest.map_structure(dist_fn, action), policy_state, policy_info)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\gaussian_policy.py",
        "line_number": 56,
        "API": ".map_structure(",
        "context": [
            "      if not tensor_spec.is_continuous(action_spec):\n",
            "        raise ValueError(\n",
            "            'Gaussian Noise is applicable only to continuous actions.')\n",
            "\n",
            "    tf.nest.map_structure(_validate_action_spec, wrapped_policy.action_spec)\n",
            "\n",
            "    super(GaussianPolicy, self).__init__(\n",
            "        wrapped_policy.time_step_spec,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\gaussian_policy.py",
        "line_number": 69,
        "API": ".zeros(",
        "context": [
            "    self._wrapped_policy = wrapped_policy\n",
            "\n",
            "    def _create_normal_distribution(action_spec):\n",
            "      return tfd.Normal(\n",
            "          loc=tf.zeros(action_spec.shape, dtype=action_spec.dtype),\n",
            "          scale=tf.ones(action_spec.shape, dtype=action_spec.dtype) * scale)\n",
            "\n",
            "    self._noise_distribution = tf.nest.map_structure(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\gaussian_policy.py",
        "line_number": 87,
        "API": ".map_structure(",
        "context": [
            "\n",
            "    def _add_noise(action, distribution):\n",
            "      return action + distribution.sample(seed=seed_stream())\n",
            "\n",
            "    actions = tf.nest.map_structure(_add_noise, action_step.action,\n",
            "                                    self._noise_distribution)\n",
            "    return policy_step.PolicyStep(actions, action_step.state, action_step.info)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\greedy_policy.py",
        "line_number": 37,
        "API": ".log(",
        "context": [
            "  \"\"\"Thin wrapper around Deterministic that supports taking log_prob.\"\"\"\n",
            "\n",
            "  def _log_prob(self, x):\n",
            "    \"\"\"Takes log-probs by casting to tf.float32 instead of self.dtype.\"\"\"\n",
            "    return tf.math.log(tf.cast(self.prob(x), dtype=tf.float32))\n",
            "\n",
            "\n",
            "@gin.configurable(module='tf_agents', denylist=['policy'])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\greedy_policy.py",
        "line_number": 82,
        "API": ".map_structure(",
        "context": [
            "\n",
            "    distribution_step = self._wrapped_policy.distribution(\n",
            "        time_step, policy_state)\n",
            "    return policy_step.PolicyStep(\n",
            "        tf.nest.map_structure(dist_fn, distribution_step.action),\n",
            "        distribution_step.state, distribution_step.info)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\ou_noise_policy.py",
        "line_number": 57,
        "API": ".map_structure(",
        "context": [
            "    def _validate_action_spec(action_spec):\n",
            "      if not tensor_spec.is_continuous(action_spec):\n",
            "        raise ValueError('OU Noise is applicable only to continuous actions.')\n",
            "\n",
            "    tf.nest.map_structure(_validate_action_spec, wrapped_policy.action_spec)\n",
            "\n",
            "    super(OUNoisePolicy, self).__init__(\n",
            "        wrapped_policy.time_step_spec,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\ou_noise_policy.py",
        "line_number": 79,
        "API": ".zeros(",
        "context": [
            "    seed_stream = tfp.util.SeedStream(seed=seed, salt='ou_noise')\n",
            "\n",
            "    def _create_ou_process(action_spec):\n",
            "      return common.OUProcess(\n",
            "          lambda: tf.zeros(action_spec.shape, dtype=action_spec.dtype),\n",
            "          self._ou_damping,\n",
            "          self._ou_stddev,\n",
            "          seed=seed_stream())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\ou_noise_policy.py",
        "line_number": 85,
        "API": ".map_structure(",
        "context": [
            "          self._ou_stddev,\n",
            "          seed=seed_stream())\n",
            "\n",
            "    if self._ou_process is None:\n",
            "      self._ou_process = tf.nest.map_structure(_create_ou_process,\n",
            "                                               self._action_spec)\n",
            "\n",
            "    action_step = self._wrapped_policy.action(time_step, policy_state,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\ou_noise_policy.py",
        "line_number": 94,
        "API": ".map_structure(",
        "context": [
            "\n",
            "    def _add_ou_noise(action, ou_process):\n",
            "      return action + ou_process()\n",
            "\n",
            "    actions = tf.nest.map_structure(_add_ou_noise, action_step.action,\n",
            "                                    self._ou_process)\n",
            "    return policy_step.PolicyStep(actions, action_step.state, action_step.info)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_info_updater_wrapper.py",
        "line_number": 82,
        "API": ".squeeze(",
        "context": [
            "\n",
            "  # Helper function to verify the compatibility between `current_info` and\n",
            "  # `_info_spec`.\n",
            "  def _check_value(self, tensor: tf.Tensor, tensorspec: tf.TensorSpec):\n",
            "    if not tf.TensorShape(tf.squeeze(tensor.get_shape())).is_compatible_with(\n",
            "        tensorspec.shape):\n",
            "      raise ValueError(\n",
            "          'Tensor {} is not compatible with specification {}.'.format(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_loader.py",
        "line_number": 68,
        "API": ".join(",
        "context": [
            "  return policy\n",
            "\n",
            "\n",
            "def _copy_file(from_dir, name, to_dir):\n",
            "  tf.io.gfile.copy(os.path.join(from_dir, name), os.path.join(to_dir, name))\n",
            "\n",
            "\n",
            "def _copy_dir(from_dir, name, to_dir):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_loader.py",
        "line_number": 73,
        "API": ".join(",
        "context": [
            "\n",
            "\n",
            "def _copy_dir(from_dir, name, to_dir):\n",
            "  from_dir_name = os.path.join(from_dir, name)\n",
            "  to_dir_name = os.path.join(to_dir, name)\n",
            "  tf.io.gfile.mkdir(to_dir_name)\n",
            "  for file_name in tf.io.gfile.listdir(from_dir_name):\n",
            "    _copy_file(from_dir_name, file_name, to_dir_name)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_loader.py",
        "line_number": 111,
        "API": ".load(",
        "context": [
            "     * `variables/variables.index`\n",
            "     * `variables/variables.data-00000-of-00001`:\n",
            "\n",
            "  After running this function you can pass `output_path` to\n",
            "  `policy_loader.load()` to load the policy.\n",
            "\n",
            "  Example usage:\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_loader.py",
        "line_number": 128,
        "API": ".load(",
        "context": [
            "      '/path/policies/checkpoint/policy_checkpoint_0001340',\n",
            "      '/path/policies/collect_policy/prod')\n",
            "  ...\n",
            "  # Later, load a model from the assembled model\n",
            "  collect_policy = policy_loader.load('/path/policies/collect_policy/prod')\n",
            "  ```\n",
            "\n",
            "  Args:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_loader.py",
        "line_number": 139,
        "API": ".makedirs(",
        "context": [
            "    output_path: string. Path where to save the materialized full saved model.\n",
            "  \"\"\"\n",
            "  if tf.io.gfile.exists(output_path):\n",
            "    raise ValueError('Output path already exists: %s' % output_path)\n",
            "  tf.io.gfile.makedirs(output_path)\n",
            "  _copy_dir(checkpoint_path, tf.saved_model.VARIABLES_DIRECTORY, output_path)\n",
            "  _copy_dir(saved_model_path, tf.saved_model.ASSETS_DIRECTORY, output_path)\n",
            "  _copy_file(saved_model_path, tf.saved_model.SAVED_MODEL_FILENAME_PB,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 47,
        "API": ".add(",
        "context": [
            "\n",
            "def _true_if_missing_or_collision(spec, spec_names):\n",
            "  if not spec.name or spec.name in spec_names:\n",
            "    return True\n",
            "  spec_names.add(spec.name)\n",
            "  return False\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 64,
        "API": ".flatten(",
        "context": [
            "  \"\"\"Checks for missing or colliding names in specs.\"\"\"\n",
            "  spec_names = set()\n",
            "  checked = [\n",
            "      _true_if_missing_or_collision(s, spec_names)\n",
            "      for s in tf.nest.flatten(spec)\n",
            "  ]\n",
            "  if any(checked):\n",
            "    raise ValueError(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 70,
        "API": ".map_structure(",
        "context": [
            "  if any(checked):\n",
            "    raise ValueError(\n",
            "        'Specs contain either a missing name or a name collision.\\n  '\n",
            "        'Spec names: %s\\n' %\n",
            "        (tf.nest.map_structure(lambda s: s.name or '<MISSING>', spec),))\n",
            "\n",
            "\n",
            "def _check_compatible(spec, tensor, ignore_outer_dims=True):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 80,
        "API": ".format(",
        "context": [
            "    tensor = tensor_spec.remove_outer_dims_nest(\n",
            "        tensor, tensor.shape.ndims - spec.shape.ndims)\n",
            "  if not spec.is_compatible_with(tensor):\n",
            "    raise ValueError('Tensor is incompatible with spec. spec = {0}, '\n",
            "                     'tensor = {1}'.format(spec, tensor))\n",
            "\n",
            "\n",
            "def add_batch_dim(spec, outer_dims):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 116,
        "API": ".save(",
        "context": [
            "\n",
            "  for i in range(...):\n",
            "    agent.train(...)\n",
            "    if i % 100 == 0:\n",
            "      saver.save('policy_%d' % global_step)\n",
            "  ```\n",
            "\n",
            "  To load and use the saved policy directly:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 122,
        "API": ".load(",
        "context": [
            "\n",
            "  To load and use the saved policy directly:\n",
            "\n",
            "  ```python\n",
            "  saved_policy = tf.compat.v2.saved_model.load('policy_0')\n",
            "  policy_state = saved_policy.get_initial_state(batch_size=3)\n",
            "  time_step = ...\n",
            "  while True:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 136,
        "API": ".load(",
        "context": [
            "  or to use the distributional form, e.g.:\n",
            "\n",
            "  ```python\n",
            "  batch_size = 3\n",
            "  saved_policy = tf.compat.v2.saved_model.load('policy_0')\n",
            "  policy_state = saved_policy.get_initial_state(batch_size=batch_size)\n",
            "  time_step = ...\n",
            "  while True:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 150,
        "API": ".load(",
        "context": [
            "  If using the flattened (signature) version, you will be limited to using\n",
            "  dicts keyed by the specs' name fields.\n",
            "\n",
            "  ```python\n",
            "  saved_policy = tf.compat.v2.saved_model.load('policy_0')\n",
            "  get_initial_state_fn = saved_policy.signatures['get_initial_state']\n",
            "  action_fn = saved_policy.signatures['action']\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 229,
        "API": ".copy(",
        "context": [
            "      _check_spec(action_fn_input_spec)\n",
            "\n",
            "    # Make a shallow copy as we'll be making some changes in-place.\n",
            "    saved_policy = tf.Module()\n",
            "    saved_policy.collect_data_spec = copy.copy(policy.collect_data_spec)\n",
            "    saved_policy.policy_state_spec = copy.copy(policy.policy_state_spec)\n",
            "\n",
            "    if train_step is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 267,
        "API": ".function(",
        "context": [
            "      get_initial_state_fn = functools.partial(\n",
            "          policy.get_initial_state, batch_size=batch_size)\n",
            "      get_initial_state_input_specs = ()\n",
            "\n",
            "    get_initial_state_fn = common.function()(get_initial_state_fn)\n",
            "\n",
            "    original_action_fn = policy.action\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 285,
        "API": ".map_structure(",
        "context": [
            "      try:\n",
            "        time_step = cast(ts.TimeStep, time_step)\n",
            "        outs = policy.distribution(\n",
            "            time_step=time_step, policy_state=policy_state)\n",
            "        return tf.nest.map_structure(_check_composite_distribution, outs)\n",
            "      except (TypeError, NotImplementedError) as e:\n",
            "        # TODO(b/156526399): Move this to just the policy.distribution() call\n",
            "        # once tfp.experimental.as_composite() properly handles LinearOperator*\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 290,
        "API": ".warning(",
        "context": [
            "      except (TypeError, NotImplementedError) as e:\n",
            "        # TODO(b/156526399): Move this to just the policy.distribution() call\n",
            "        # once tfp.experimental.as_composite() properly handles LinearOperator*\n",
            "        # components as well as TransformedDistributions.\n",
            "        logging.warning(\n",
            "            'WARNING: Could not serialize policy.distribution() for policy '\n",
            "            '\"%s\". Calling saved_model.distribution() will raise the following '\n",
            "            'assertion error: %s', policy, e)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 304,
        "API": ".function(",
        "context": [
            "    # We call get_concrete_function() for its side effect: to ensure the proper\n",
            "    # ConcreteFunction is stored in the SavedModel.\n",
            "    get_initial_state_fn.get_concrete_function(*get_initial_state_input_specs)\n",
            "\n",
            "    train_step_fn = common.function(\n",
            "        lambda: saved_policy.train_step).get_concrete_function()\n",
            "    get_metadata_fn = common.function(\n",
            "        lambda: saved_policy.metadata).get_concrete_function()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 309,
        "API": ".map_structure(",
        "context": [
            "        lambda: saved_policy.train_step).get_concrete_function()\n",
            "    get_metadata_fn = common.function(\n",
            "        lambda: saved_policy.metadata).get_concrete_function()\n",
            "\n",
            "    batched_time_step_spec = tf.nest.map_structure(\n",
            "        lambda spec: add_batch_dim(spec, [batch_size]), policy.time_step_spec)\n",
            "    batched_time_step_spec = cast(ts.TimeStep, batched_time_step_spec)\n",
            "    batched_policy_state_spec = tf.nest.map_structure(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 334,
        "API": ".function(",
        "context": [
            "      _check_spec(policy_state_spec)\n",
            "\n",
            "    if input_fn_and_spec is not None:\n",
            "      # Store a signature based on input_fn_and_spec\n",
            "      @common.function()\n",
            "      def polymorphic_action_fn(example):\n",
            "        action_inputs = input_fn_and_spec[0](example)\n",
            "        tf.nest.map_structure(_check_compatible, action_fn_input_spec,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 341,
        "API": ".function(",
        "context": [
            "        tf.nest.map_structure(_check_compatible, action_fn_input_spec,\n",
            "                              action_inputs)\n",
            "        return action_fn(*action_inputs)\n",
            "\n",
            "      @common.function()\n",
            "      def polymorphic_distribution_fn(example):\n",
            "        action_inputs = input_fn_and_spec[0](example)\n",
            "        tf.nest.map_structure(_check_compatible, action_fn_input_spec,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 348,
        "API": ".map_structure(",
        "context": [
            "        tf.nest.map_structure(_check_compatible, action_fn_input_spec,\n",
            "                              action_inputs)\n",
            "        return distribution_fn(*action_inputs)\n",
            "\n",
            "      batched_input_spec = tf.nest.map_structure(\n",
            "          lambda spec: add_batch_dim(spec, [batch_size]), input_fn_and_spec[1])\n",
            "      # We call get_concrete_function() for its side effect: to ensure the\n",
            "      # proper ConcreteFunction is stored in the SavedModel.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 362,
        "API": ".function(",
        "context": [
            "    else:\n",
            "      action_input_spec = action_fn_input_spec\n",
            "      if batched_policy_state_spec:\n",
            "        # Store the signature with a required policy state spec\n",
            "        polymorphic_action_fn = common.function()(action_fn)\n",
            "        polymorphic_action_fn.get_concrete_function(\n",
            "            time_step=batched_time_step_spec,\n",
            "            policy_state=batched_policy_state_spec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 367,
        "API": ".function(",
        "context": [
            "        polymorphic_action_fn.get_concrete_function(\n",
            "            time_step=batched_time_step_spec,\n",
            "            policy_state=batched_policy_state_spec)\n",
            "\n",
            "        polymorphic_distribution_fn = common.function()(distribution_fn)\n",
            "        polymorphic_distribution_fn.get_concrete_function(\n",
            "            time_step=batched_time_step_spec,\n",
            "            policy_state=batched_policy_state_spec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 377,
        "API": ".function(",
        "context": [
            "        #  restored.action(time_step)\n",
            "        # or\n",
            "        #  restored.action(time_step, ())\n",
            "        # (without retracing the inner action twice)\n",
            "        @common.function()\n",
            "        def polymorphic_action_fn(time_step,\n",
            "                                  policy_state=batched_policy_state_spec):\n",
            "          return action_fn(time_step, policy_state)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 388,
        "API": ".function(",
        "context": [
            "            policy_state=batched_policy_state_spec)\n",
            "        polymorphic_action_fn.get_concrete_function(\n",
            "            time_step=batched_time_step_spec)\n",
            "\n",
            "        @common.function()\n",
            "        def polymorphic_distribution_fn(time_step,\n",
            "                                        policy_state=batched_policy_state_spec):\n",
            "          return distribution_fn(time_step, policy_state)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 425,
        "API": ".map_structure(",
        "context": [
            "        'get_metadata':\n",
            "            _function_with_flat_signature(\n",
            "                get_metadata_fn,\n",
            "                input_specs=(),\n",
            "                output_spec=tf.nest.map_structure(lambda v: v.dtype,\n",
            "                                                  self._metadata),\n",
            "                include_batch_dimension=False),\n",
            "    }\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 447,
        "API": ".warning(",
        "context": [
            "          name: ref\n",
            "          for name, ref in policy._unconditional_checkpoint_dependencies}  # pylint: disable=protected-access\n",
            "    except AttributeError as e:\n",
            "      if '_self_unconditional' in str(e):\n",
            "        logging.warning(\n",
            "            'Unable to capture all trackable objects in policy \"%s\".  This '\n",
            "            'may be okay.  Error: %s', policy, e)\n",
            "      else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 519,
        "API": ".executing_eagerly(",
        "context": [
            "\n",
            "    Returns:\n",
            "      An integer.\n",
            "    \"\"\"\n",
            "    if tf.executing_eagerly():\n",
            "      return self._train_step.numpy()\n",
            "    else:\n",
            "      return tf.identity(self._train_step)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 530,
        "API": ".executing_eagerly(",
        "context": [
            "\n",
            "    Returns:\n",
            "      An a dictionary of tf.Variable.\n",
            "    \"\"\"\n",
            "    if tf.executing_eagerly():\n",
            "      return {k: self._metadata[k].numpy() for k in self._metadata}\n",
            "    else:\n",
            "      return self._metadata\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 560,
        "API": ".map_structure(",
        "context": [
            "    if getattr(self._policy, name, None) is not None:\n",
            "      raise ValueError('Policy already has an attribute registered with: %s' %\n",
            "                       name)\n",
            "\n",
            "    batched_spec = tf.nest.map_structure(lambda s: add_batch_dim(s, outer_dims),\n",
            "                                         input_spec)\n",
            "    tf_fn = common.function(fn)\n",
            "    # We call get_concrete_function() for its side effect: to ensure the proper\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 608,
        "API": ".save(",
        "context": [
            "    Args:\n",
            "      export_dir: Directory to save the policy to.\n",
            "      options: Optional `tf.saved_model.SaveOptions` object.\n",
            "    \"\"\"\n",
            "    tf.compat.v2.saved_model.save(\n",
            "        self._policy, export_dir, signatures=self._signatures, options=options)\n",
            "\n",
            "    temp_spec_file_name = '{}_temp'.format(POLICY_SPECS_PBTXT)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 618,
        "API": ".join(",
        "context": [
            "        'collect_data_spec': self._policy.collect_data_spec,\n",
            "        'policy_state_spec': self._policy.policy_state_spec\n",
            "    }\n",
            "    tensor_spec.to_pbtxt_file(temp_spec_output_path, specs)\n",
            "    spec_output_path = os.path.join(export_dir, POLICY_SPECS_PBTXT)\n",
            "    # By moving the file to its final location makes it safer to wait for the\n",
            "    # file (e.g. from a separate binary). The parameter `overwrite=True`\n",
            "    # reproduces the exact previous behavior.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 659,
        "API": ".join(",
        "context": [
            "        model_variables=self._policy.model_variables,\n",
            "        train_step=self._train_step)\n",
            "    # Use write() to make sure that the file prefix is not modified by appending\n",
            "    # a save counter value.\n",
            "    file_prefix = os.path.join(export_dir, tf.saved_model.VARIABLES_DIRECTORY,\n",
            "                               tf.saved_model.VARIABLES_FILENAME)\n",
            "    checkpoint.write(file_prefix, options=options)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 694,
        "API": ".flatten(",
        "context": [
            "          dtype=spec.dtype)\n",
            "    else:\n",
            "      return spec\n",
            "\n",
            "  flat_input_spec = [_with_batch(spec) for spec in tf.nest.flatten(input_specs)]\n",
            "\n",
            "  def as_dict(outputs, output_spec):\n",
            "    nest_utils.assert_same_structure(outputs, output_spec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\policy_saver.py",
        "line_number": 699,
        "API": ".flatten(",
        "context": [
            "\n",
            "  def as_dict(outputs, output_spec):\n",
            "    nest_utils.assert_same_structure(outputs, output_spec)\n",
            "    flat_outputs = tf.nest.flatten(outputs)\n",
            "    flat_names = [s.name for s in tf.nest.flatten(output_spec)]\n",
            "    return dict(zip(flat_names, flat_outputs))\n",
            "\n",
            "  @common.function(input_signature=flat_input_spec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_epsilon_greedy_policy.py",
        "line_number": 81,
        "API": ".format(",
        "context": [
            "    self._epsilon = epsilon\n",
            "    self._epsilon_decay_end_count = epsilon_decay_end_count\n",
            "    if epsilon_decay_end_count is not None:\n",
            "      if epsilon_decay_end_value is None or epsilon_decay_end_value >= epsilon:\n",
            "        raise ValueError('Invalid value for epsilon_decay_end_value {}'.format(\n",
            "            epsilon_decay_end_value))\n",
            "      self._epsilon_decay_step_factor = float(\n",
            "          epsilon - epsilon_decay_end_value) / epsilon_decay_end_count\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_epsilon_greedy_policy.py",
        "line_number": 121,
        "API": ".format(",
        "context": [
            "              policy_state=(),\n",
            "              seed: Optional[types.Seed] = None):\n",
            "    if seed is not None:\n",
            "      raise NotImplementedError(\n",
            "          'seed is not supported; but saw seed: {}'.format(seed))\n",
            "    self._count += 1\n",
            "    # _random_function()'s range should be [0, 1), so if epsilon is 1,\n",
            "    # we should always use random policy, and if epislon is 0, it\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_policy.py",
        "line_number": 276,
        "API": ".zeros(",
        "context": [
            "      if batch_size is None:\n",
            "        shape = spec.shape\n",
            "      else:\n",
            "        shape = (batch_size,) + spec.shape\n",
            "      return np.zeros(shape, spec.dtype)\n",
            "\n",
            "    return tf.nest.map_structure(_zero_array, self._policy_state_spec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_tf_eager_policy.py",
        "line_number": 77,
        "API": ".function(",
        "context": [
            "    \"\"\"\n",
            "    self._policy = policy\n",
            "    self._use_tf_function = use_tf_function\n",
            "    if self._use_tf_function:\n",
            "      self._policy_action_fn = common.function(policy.action)\n",
            "    else:\n",
            "      self._policy_action_fn = policy.action\n",
            "    self._batch_time_steps = batch_time_steps\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_tf_eager_policy.py",
        "line_number": 85,
        "API": ".map_structure(",
        "context": [
            "    super(PyTFEagerPolicyBase, self).__init__(time_step_spec, action_spec,\n",
            "                                              policy_state_spec, info_spec)\n",
            "\n",
            "  def variables(self):\n",
            "    return tf.nest.map_structure(lambda t: t.numpy(), self._policy.variables())\n",
            "\n",
            "  def _get_initial_state(self, batch_size):\n",
            "    if batch_size is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_tf_eager_policy.py",
        "line_number": 94,
        "API": ".warning(",
        "context": [
            "    return self._policy.get_initial_state(batch_size=batch_size)\n",
            "\n",
            "  def _action(self, time_step, policy_state, seed: Optional[types.Seed] = None):\n",
            "    if seed is not None and self._use_tf_function:\n",
            "      logging.warning(\n",
            "          'Using `seed` may force a retrace for each call to `action`.')\n",
            "    if self._batch_time_steps:\n",
            "      time_step = nest_utils.batch_nested_array(time_step)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_tf_eager_policy.py",
        "line_number": 99,
        "API": ".map_structure(",
        "context": [
            "          'Using `seed` may force a retrace for each call to `action`.')\n",
            "    if self._batch_time_steps:\n",
            "      time_step = nest_utils.batch_nested_array(time_step)\n",
            "    # Avoid passing numpy arrays to avoid retracing of the tf.function.\n",
            "    time_step = tf.nest.map_structure(tf.convert_to_tensor, time_step)\n",
            "    if seed is not None:\n",
            "      policy_step = self._policy_action_fn(time_step, policy_state, seed=seed)\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_tf_eager_policy.py",
        "line_number": 178,
        "API": ".load(",
        "context": [
            "        file generated by the `policy_saver`.\n",
            "      use_tf_function: See PyTFEagerPolicyBase.\n",
            "      batch_time_steps: See PyTFEagerPolicyBase.\n",
            "    \"\"\"\n",
            "    policy = tf.compat.v2.saved_model.load(model_path)\n",
            "    self._checkpoint = tf.train.Checkpoint(policy=policy)\n",
            "    if not (time_step_spec or load_specs_from_pbtxt):\n",
            "      raise ValueError(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_tf_eager_policy.py",
        "line_number": 186,
        "API": ".join(",
        "context": [
            "          'To load a SavedModel policy you have to provide the specs, or'\n",
            "          'enable loading from proto.')\n",
            "    policy_specs = None\n",
            "    if not time_step_spec and load_specs_from_pbtxt:\n",
            "      spec_path = os.path.join(model_path, policy_saver.POLICY_SPECS_PBTXT)\n",
            "      policy_specs = policy_saver.specs_from_collect_data_spec(\n",
            "          tensor_spec.from_pbtxt_file(spec_path))\n",
            "      time_step_spec = policy_specs['time_step_spec']\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_tf_eager_policy.py",
        "line_number": 204,
        "API": ".map_structure(",
        "context": [
            "\n",
            "    # Call action function of the saved_model policy w/ dummy data to\n",
            "    # initialize the graph. Otherwise the first call to the action function may\n",
            "    # be 10x slower than the rest of the calls on GPU.\n",
            "    dummy_time_step = tf.nest.map_structure(\n",
            "        lambda s: tf.zeros(s.shape, s.dtype), time_step_spec)\n",
            "    dummy_time_step = nest_utils.batch_nested_tensors(dummy_time_step)\n",
            "    dummy_policy_state = tf.nest.map_structure(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_tf_eager_policy.py",
        "line_number": 231,
        "API": ".save(",
        "context": [
            "\n",
            "  def update_from_checkpoint(self, checkpoint_path: Text):\n",
            "    \"\"\"Allows users to update saved_model variables directly from a checkpoint.\n",
            "\n",
            "    `checkpoint_path` is a path that was passed to either `PolicySaver.save()`\n",
            "    or `PolicySaver.save_checkpoint()`. The policy looks for set of checkpoint\n",
            "    files with the file prefix `<checkpoint_path>/variables/variables'\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_tf_eager_policy.py",
        "line_number": 239,
        "API": ".join(",
        "context": [
            "    Args:\n",
            "      checkpoint_path: Path to the checkpoint to restore and use to udpate this\n",
            "        policy.\n",
            "    \"\"\"\n",
            "    file_prefix = os.path.join(checkpoint_path,\n",
            "                               tf.saved_model.VARIABLES_DIRECTORY,\n",
            "                               tf.saved_model.VARIABLES_FILENAME)\n",
            "    status = self._checkpoint.read(file_prefix)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_tf_eager_policy.py",
        "line_number": 249,
        "API": ".split(",
        "context": [
            "    # checkpoint. We split based on '_' and take the last item which gives us\n",
            "    # the train step.\n",
            "    try:\n",
            "      self._train_step_from_last_restored_checkpoint_path = int(\n",
            "          checkpoint_path.split('_')[-1])\n",
            "    except ValueError:\n",
            "      # In case the checkpoint format is not as expected.\n",
            "      self._train_step_from_last_restored_checkpoint_path = None\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_tf_policy.py",
        "line_number": 60,
        "API": ".warning(",
        "context": [
            "      batch_size: (deprecated)\n",
            "      seed: Seed to use if policy performs random actions (optional).\n",
            "    \"\"\"\n",
            "    if not isinstance(policy, tf_policy.TFPolicy):\n",
            "      logging.warning('Policy should implement tf_policy.TFPolicy')\n",
            "\n",
            "    if batch_size is not None:\n",
            "      logging.warning('In PyTFPolicy constructor, `batch_size` is deprecated, '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_tf_policy.py",
        "line_number": 96,
        "API": ".map_structure(",
        "context": [
            "          self._tf_policy.time_step_spec, outer_dims=outer_dims)\n",
            "      self._tf_initial_state = self._tf_policy.get_initial_state(\n",
            "          batch_size=self._batch_size or 1)\n",
            "\n",
            "      self._policy_state = tf.nest.map_structure(\n",
            "          lambda ps: tf.compat.v1.placeholder(  # pylint: disable=g-long-lambda\n",
            "              ps.dtype,\n",
            "              ps.shape,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_tf_policy.py",
        "line_number": 115,
        "API": ".flatten(",
        "context": [
            "    if not graph:\n",
            "      graph = tf.compat.v1.get_default_graph()\n",
            "\n",
            "    self._construct(batch_size, graph)\n",
            "    var_list = tf.nest.flatten(self._tf_policy.variables())\n",
            "    common.initialize_uninitialized_variables(self.session, var_list)\n",
            "    self._built = True\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_tf_policy.py",
        "line_number": 134,
        "API": ".save(",
        "context": [
            "      policy_checkpointer = common.Checkpointer(\n",
            "          ckpt_dir=policy_dir, policy=self._tf_policy, global_step=global_step)\n",
            "      policy_checkpointer.initialize_or_restore(self.session)\n",
            "      with self.session.as_default():\n",
            "        policy_checkpointer.save(global_step)\n",
            "\n",
            "  def restore(self,\n",
            "              policy_dir: Text,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_tf_policy.py",
        "line_number": 185,
        "API": ".format(",
        "context": [
            "      self.initialize(None)\n",
            "    else:\n",
            "      raise ValueError(\n",
            "          'Cannot handle more than one outer dimension. Saw {} outer '\n",
            "          'dimensions: {}'.format(len(outer_shape), outer_shape))\n",
            "\n",
            "  def _get_initial_state(self, batch_size):\n",
            "    if not self._built:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_tf_policy.py",
        "line_number": 194,
        "API": ".format(",
        "context": [
            "\n",
            "    if batch_size not in [self._batch_size, self._batch_size or 1]:\n",
            "      raise ValueError(\n",
            "          '`batch_size` argument is different from the batch size provided '\n",
            "          'previously. Expected {}, but saw {}.'.format(self._batch_size,\n",
            "                                                        batch_size))\n",
            "    return self.session.run(self._tf_initial_state)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_tf_policy.py",
        "line_number": 210,
        "API": ".format(",
        "context": [
            "      batch_size = time_step.step_type.shape[0]\n",
            "    if self._batch_size != batch_size:\n",
            "      raise ValueError(\n",
            "          'The batch size of time_step is different from the batch size '\n",
            "          'provided previously. Expected {}, but saw {}.'.format(\n",
            "              self._batch_size, batch_size))\n",
            "\n",
            "    if not self._batched:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\py_tf_policy.py",
        "line_number": 224,
        "API": ".flatten(",
        "context": [
            "    feed_dict = {self._time_step: time_step}\n",
            "    if policy_state is not None:\n",
            "      # Flatten policy_state to handle specs that are not hashable due to lists.\n",
            "      for state_ph, state in zip(\n",
            "          tf.nest.flatten(self._policy_state), tf.nest.flatten(policy_state)):\n",
            "        feed_dict[state_ph] = state\n",
            "\n",
            "    action_step = self.session.run(self._action_step, feed_dict)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 71,
        "API": ".array(",
        "context": [
            "    return subset\n",
            "  elif isinstance(nested_list, tuple):\n",
            "    return tuple(convert_nest_lists_to_np_array(v) for v in nested_list)\n",
            "  elif isinstance(nested_list, list):\n",
            "    return np.array(nested_list, np.float32)\n",
            "  elif isinstance(nested_list, np.ndarray):\n",
            "    # Converting all dtype to be float32 makes it easier to do CEM loop.\n",
            "    # The dtype of the final sampled action will be converted back according\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 210,
        "API": ".constant(",
        "context": [
            "      var: A [B, A] sized tensors where each row is the initial_var.\n",
            "    \"\"\"\n",
            "\n",
            "    def broadcast_to_batch(array):\n",
            "      tensor = tf.constant(array)\n",
            "      action_size = tf.shape(tensor)[-1]\n",
            "      return tf.broadcast_to(tensor, [batch_size, action_size])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 217,
        "API": ".map_structure(",
        "context": [
            "\n",
            "    mean = tf.nest.map_structure(\n",
            "        broadcast_to_batch,\n",
            "        convert_nest_lists_to_np_array(self._init_mean))\n",
            "    var = tf.nest.map_structure(\n",
            "        broadcast_to_batch,\n",
            "        convert_nest_lists_to_np_array(self._init_var))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 274,
        "API": ".map_structure(",
        "context": [
            "      \"\"\"\n",
            "      del best_actions, best_scores, best_next_policy_state\n",
            "\n",
            "      # Prevent variance to collapse to 0.0 (stuck in local minimum).\n",
            "      var = tf.nest.map_structure(\n",
            "          lambda v: tf.maximum(v, self._minimal_var), var)\n",
            "\n",
            "      # Sample a batch of actions with the shape of [B, N, A] or [BxT, N, A]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 290,
        "API": ".map_structure(",
        "context": [
            "            observation, actions, step_type, policy_state)  # [B, N]\n",
            "\n",
            "      best_scores, ind = tf.nn.top_k(scores, self._num_elites)  # ind: [B, M]\n",
            "\n",
            "      actions_float = tf.nest.map_structure(\n",
            "          lambda t: tf.cast(t, tf.float32), actions)\n",
            "      mean, var = self._actions_sampler.refit_distribution_to(\n",
            "          ind, actions_float)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 298,
        "API": ".gather(",
        "context": [
            "\n",
            "      best_next_policy_state = next_policy_state\n",
            "\n",
            "      def select_best_actions(actions):\n",
            "        best_actions = tf.gather(actions, ind, batch_dims=1)\n",
            "        return best_actions\n",
            "\n",
            "      best_actions = tf.nest.map_structure(select_best_actions, actions)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 308,
        "API": ".less(",
        "context": [
            "\n",
            "    def cond(mean, var, i, iters, best_actions, best_scores,\n",
            "             best_next_policy_state):\n",
            "      del mean, var, best_actions, best_scores, best_next_policy_state\n",
            "      return tf.less(i, iters)\n",
            "\n",
            "    mean, var = self._initial_params(batch_size)\n",
            "    iters = tf.constant(self._num_iterations)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 314,
        "API": ".zeros(",
        "context": [
            "    mean, var = self._initial_params(batch_size)\n",
            "    iters = tf.constant(self._num_iterations)\n",
            "\n",
            "    def init_best_actions(action_spec):\n",
            "      best_actions = tf.zeros(\n",
            "          [batch_size, self._num_elites, action_spec.shape.as_list()[0]],\n",
            "          dtype=action_spec.dtype)\n",
            "      return best_actions\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 319,
        "API": ".map_structure(",
        "context": [
            "          [batch_size, self._num_elites, action_spec.shape.as_list()[0]],\n",
            "          dtype=action_spec.dtype)\n",
            "      return best_actions\n",
            "\n",
            "    best_actions = tf.nest.map_structure(init_best_actions, self._action_spec)\n",
            "    best_scores = tf.zeros([batch_size, self._num_elites], dtype=tf.float32)\n",
            "\n",
            "    # Run the while loop for CEM in-graph.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 326,
        "API": ".map_structure(",
        "context": [
            "    # Run the while loop for CEM in-graph.\n",
            "    mean_shape = tf.nest.map_structure(\n",
            "        lambda m: [None] + m.get_shape()[1:], mean\n",
            "    )\n",
            "    var_shape = tf.nest.map_structure(lambda v: [None] + v.get_shape()[1:], var)\n",
            "    best_action_shape = tf.nest.map_structure(\n",
            "        lambda a: [None] + a.get_shape()[1:], best_actions\n",
            "    )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 333,
        "API": ".map_structure(",
        "context": [
            "    )\n",
            "    elites_shape = tf.TensorShape([None, self._num_elites])\n",
            "    policy_state_shape = ()\n",
            "    if common.safe_has_state(policy_state):\n",
            "      policy_state_shape = tf.nest.map_structure(\n",
            "          lambda state: state.get_shape(), policy_state\n",
            "      )\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 338,
        "API": ".while_loop(",
        "context": [
            "          lambda state: state.get_shape(), policy_state\n",
            "      )\n",
            "\n",
            "    _, _, _, _, best_actions, best_scores, best_next_policy_state = (\n",
            "        tf.while_loop(\n",
            "            cond=cond,\n",
            "            body=body,\n",
            "            loop_vars=[\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 363,
        "API": ".map_structure(",
        "context": [
            "        )\n",
            "    )\n",
            "\n",
            "    if outer_rank == 2:\n",
            "      best_actions = tf.nest.map_structure(\n",
            "          lambda x: tf.reshape(  # pylint: disable=g-long-lambda\n",
            "              x, [-1, seq_size, self._num_elites, tf.shape(x)[-1]]),\n",
            "          best_actions)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 403,
        "API": ".shape(",
        "context": [
            "    \"\"\"\n",
            "\n",
            "    def expand_to_megabatch(feature):\n",
            "      # Collapse second dimension of megabatch.\n",
            "      dim = tf.shape(feature)[2]\n",
            "      return tf.reshape(feature, [-1, dim])\n",
            "\n",
            "    if self._preprocess_state_action:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 408,
        "API": ".map_structure(",
        "context": [
            "      return tf.reshape(feature, [-1, dim])\n",
            "\n",
            "    if self._preprocess_state_action:\n",
            "      # [B, N, A] -> [BxN, A]\n",
            "      sample_actions = tf.nest.map_structure(\n",
            "          expand_to_megabatch, sample_actions)\n",
            "      # TODO(b/138331671) Move tf.contrib.seq2seq.tile_batch to utils.common\n",
            "      # [B, ...] -> [BxN, ...]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 420,
        "API": ".map_structure(",
        "context": [
            "    scores, next_policy_state = self.compute_target_q(\n",
            "        observation, sample_actions, step_type, policy_state)  # [BxN]\n",
            "\n",
            "    if self._preprocess_state_action:\n",
            "      next_policy_state = tf.nest.map_structure(\n",
            "          lambda x: tf.reshape(x, [-1, self._num_samples] + x.shape.as_list(  # pylint:disable=g-long-lambda\n",
            "          )[1:])[:, 0, ...], next_policy_state)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 455,
        "API": ".shape(",
        "context": [
            "    # Reshape input to be compatible with network: [BxN, T, A]\n",
            "\n",
            "    def expand_to_megabatch(feature):\n",
            "      # Collapse second dimension of megabatch.\n",
            "      dim = tf.shape(feature)[2]\n",
            "      dim_sample = tf.shape(feature)[1]\n",
            "      feature = tf.reshape(feature, [-1, seq_size, dim_sample, dim])\n",
            "      feature = tf.transpose(feature, [0, 2, 1, 3])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 462,
        "API": ".shape(",
        "context": [
            "      feature = tf.transpose(feature, [0, 2, 1, 3])\n",
            "      return tf.reshape(feature, [-1, seq_size, dim])\n",
            "\n",
            "    def decouple_batch_time(feature):\n",
            "      dim = tf.shape(feature)[2]\n",
            "      dim_sample = tf.shape(feature)[1]\n",
            "      return tf.reshape(feature, [-1, seq_size, dim_sample, dim])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 468,
        "API": ".map_structure(",
        "context": [
            "      return tf.reshape(feature, [-1, seq_size, dim_sample, dim])\n",
            "\n",
            "    if self._preprocess_state_action:\n",
            "      # [BxT, N, A] -> [BxN, T, A]\n",
            "      sample_actions = tf.nest.map_structure(\n",
            "          expand_to_megabatch, sample_actions)\n",
            "      # TODO(b/138331671) Move tf.contrib.seq2seq.tile_batch to utils.common\n",
            "      # [B, T, ...] -> [BxN, T, ...]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 477,
        "API": ".map_structure(",
        "context": [
            "      step_type = nest_utils.tile_batch(step_type, self._num_samples)\n",
            "      policy_state = nest_utils.tile_batch(policy_state, self._num_samples)\n",
            "    else:\n",
            "      # [BxT, N, A] -> [B, T, N, A]\n",
            "      sample_actions = tf.nest.map_structure(\n",
            "          decouple_batch_time, sample_actions)\n",
            "\n",
            "    scores, next_policy_state = self.compute_target_q(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 484,
        "API": ".map_structure(",
        "context": [
            "    scores, next_policy_state = self.compute_target_q(\n",
            "        observation, sample_actions, step_type, policy_state)  # [BxN, T]\n",
            "\n",
            "    if self._preprocess_state_action:\n",
            "      next_policy_state = tf.nest.map_structure(\n",
            "          lambda x: tf.reshape(x, [-1, self._num_samples] + x.shape.as_list(  # pylint:disable=g-long-lambda\n",
            "          )[1:])[:, 0, ...], next_policy_state)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 489,
        "API": ".transpose(",
        "context": [
            "          lambda x: tf.reshape(x, [-1, self._num_samples] + x.shape.as_list(  # pylint:disable=g-long-lambda\n",
            "          )[1:])[:, 0, ...], next_policy_state)\n",
            "\n",
            "    scores = tf.reshape(scores, [-1, self._num_samples, seq_size])  # [B, N, T]\n",
            "    scores = tf.transpose(scores, [0, 2, 1])  # [B, T, N]\n",
            "    scores = tf.reshape(scores, [-1, self._num_samples])  # [BxT, N]\n",
            "\n",
            "    return scores, next_policy_state\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 525,
        "API": ".map_structure(",
        "context": [
            "    def select_best_action(actions):\n",
            "      best_action = actions[..., 0, :]  # [B, A]\n",
            "      return best_action\n",
            "\n",
            "    best_action = tf.nest.map_structure(select_best_action, best_actions)\n",
            "\n",
            "    best_action_consider_actor = best_action\n",
            "    best_score_consider_actor = best_scores[..., 0]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 533,
        "API": ".cast(",
        "context": [
            "    if self._actor_policy is not None:\n",
            "      potential_best_action = self._actor_policy.action(time_step).action\n",
            "      potential_best_q, _ = self.compute_target_q(\n",
            "          time_step.observation, potential_best_action)\n",
            "      use_cem = tf.cast(\n",
            "          best_score_consider_actor > potential_best_q, tf.float32)\n",
            "      best_score_consider_actor = (\n",
            "          best_score_consider_actor * use_cem +\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 540,
        "API": ".expand_dims(",
        "context": [
            "          best_score_consider_actor * use_cem +\n",
            "          (tf.ones_like(use_cem, tf.float32) - use_cem) * potential_best_q)\n",
            "\n",
            "      def select_best_action_consider_actor(action1, action2):\n",
            "        use_cem_expanded = tf.expand_dims(\n",
            "            tf.cast(use_cem, action1.dtype), axis=-1)\n",
            "        return (action1 * use_cem_expanded +\n",
            "                action2 * (tf.ones_like(use_cem_expanded, action1.dtype)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 545,
        "API": ".map_structure(",
        "context": [
            "            tf.cast(use_cem, action1.dtype), axis=-1)\n",
            "        return (action1 * use_cem_expanded +\n",
            "                action2 * (tf.ones_like(use_cem_expanded, action1.dtype)\n",
            "                           - use_cem_expanded))\n",
            "      best_action_consider_actor = tf.nest.map_structure(\n",
            "          select_best_action_consider_actor,\n",
            "          best_action_consider_actor, potential_best_action)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 554,
        "API": ".map_structure(",
        "context": [
            "                                         best_action_consider_actor)\n",
            "    if self._info_spec and 'target_q' in self._info_spec:\n",
            "      batch_size = nest_utils.get_outer_shape(\n",
            "          time_step, self._time_step_spec)[0]\n",
            "      info = tf.nest.map_structure(\n",
            "          lambda spec: tf.zeros(tf.concat([[batch_size], spec.shape], axis=-1)),\n",
            "          self._info_spec)\n",
            "      info['target_q'] = best_score_consider_actor\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\qtopt_cem_policy.py",
        "line_number": 561,
        "API": ".map_structure(",
        "context": [
            "      info['target_q'] = best_score_consider_actor\n",
            "    else:\n",
            "      batch_size = nest_utils.get_outer_shape(\n",
            "          time_step, self._time_step_spec)[0]\n",
            "      info = tf.nest.map_structure(\n",
            "          lambda spec: tf.zeros(tf.concat([[batch_size], spec.shape], axis=-1)),\n",
            "          self._info_spec)\n",
            "    return policy_step.PolicyStep(distribution, best_next_policy_state, info)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\q_policy.py",
        "line_number": 104,
        "API": ".flatten(",
        "context": [
            "            'action_spec must be compatible with q_network.action_spec; '\n",
            "            'instead got action_spec=%s, q_network.action_spec=%s' % (\n",
            "                action_spec, network_action_spec))\n",
            "\n",
            "    flat_action_spec = tf.nest.flatten(action_spec)\n",
            "    if len(flat_action_spec) > 1:\n",
            "      raise ValueError(\n",
            "          'Only scalar actions are supported now, but action spec is: {}'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\q_policy.py",
        "line_number": 114,
        "API": ".format(",
        "context": [
            "      spec = flat_action_spec[0]\n",
            "      if spec.shape.rank > 0:\n",
            "        raise ValueError(\n",
            "            'Only scalar actions are supported now, but action spec is: {}'\n",
            "            .format(action_spec))\n",
            "\n",
            "      if spec.minimum != 0:\n",
            "        raise ValueError(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\q_policy.py",
        "line_number": 163,
        "API": ".constant(",
        "context": [
            "    logits = q_values\n",
            "\n",
            "    if observation_and_action_constraint_splitter is not None:\n",
            "      # Overwrite the logits for invalid actions to logits.dtype.min.\n",
            "      almost_neg_inf = tf.constant(logits.dtype.min, dtype=logits.dtype)\n",
            "      logits = tf.compat.v2.where(\n",
            "          tf.cast(mask, tf.bool), logits, almost_neg_inf)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\random_py_policy.py",
        "line_number": 128,
        "API": ".cast(",
        "context": [
            "    if observation_and_action_constraint_splitter is not None:\n",
            "      _, mask = observation_and_action_constraint_splitter(\n",
            "          time_step.observation)\n",
            "\n",
            "      zero_logits = tf.cast(tf.zeros_like(mask), tf.float32)\n",
            "      masked_categorical = masked.MaskedCategorical(zero_logits, mask)\n",
            "      random_action = tf.cast(\n",
            "          masked_categorical.sample() + self.action_spec.minimum,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\random_py_policy.py",
        "line_number": 137,
        "API": ".expand_dims(",
        "context": [
            "\n",
            "      # If the action spec says each action should be shaped (1,), add another\n",
            "      # dimension so the final shape is (B, 1) rather than (B,).\n",
            "      if len(self.action_spec.shape) == 1:\n",
            "        random_action = tf.expand_dims(random_action, axis=-1)\n",
            "    else:\n",
            "      random_action = array_spec.sample_spec_nest(\n",
            "          self._action_spec, self._rng, outer_dims=outer_dims)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\random_tf_policy.py",
        "line_number": 52,
        "API": ".log(",
        "context": [
            "    A tensor of type float32 with shape outer_dims.\n",
            "  \"\"\"\n",
            "  # Equivalent of what a tfp.distribution.Categorical would return.\n",
            "  if action_spec.dtype.is_integer:\n",
            "    log_prob = -tf.math.log(action_spec.maximum - action_spec.minimum + 1.0)\n",
            "  # Equivalent of what a tfp.distribution.Uniform would return.\n",
            "  else:\n",
            "    log_prob = -tf.math.log(action_spec.maximum - action_spec.minimum)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\random_tf_policy.py",
        "line_number": 59,
        "API": ".reduce_sum(",
        "context": [
            "    log_prob = -tf.math.log(action_spec.maximum - action_spec.minimum)\n",
            "\n",
            "  # Note that log_prob may be a vector. We first reduce it to a scalar, and then\n",
            "  # adjust by the number of times that vector is repeated in action_spec.\n",
            "  log_prob = tf.reduce_sum(log_prob) * (\n",
            "      action_spec.shape.num_elements() / log_prob.shape.num_elements())\n",
            "  # Regardless of the type of the action, the log_prob should be float32.\n",
            "  return tf.cast(tf.fill(outer_dims, log_prob), tf.float32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\random_tf_policy.py",
        "line_number": 79,
        "API": ".get(",
        "context": [
            "\n",
            "  def __init__(self, time_step_spec: ts.TimeStep,\n",
            "               action_spec: types.NestedTensorSpec, *args, **kwargs):\n",
            "    observation_and_action_constraint_splitter = (\n",
            "        kwargs.get('observation_and_action_constraint_splitter', None))\n",
            "    self._accepts_per_arm_features = (\n",
            "        kwargs.pop('accepts_per_arm_features', False))\n",
            "    self._stationary_mask = kwargs.pop('stationary_mask', None)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\random_tf_policy.py",
        "line_number": 90,
        "API": ".constant(",
        "context": [
            "            'RandomTFPolicy only supports action constraints for '\n",
            "            'BoundedTensorSpec action specs.')\n",
            "      assert action_spec.dtype.is_integer, ('To use a stationary mask, action '\n",
            "                                            'dtype must be integer.')\n",
            "      self._stationary_mask = tf.constant([self._stationary_mask])\n",
            "      num_actions = action_spec.maximum - action_spec.minimum + 1\n",
            "      assert (self._stationary_mask.shape[-1] == num_actions), (\n",
            "          'Stationary mask should have length equal to the number of actions, '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\random_tf_policy.py",
        "line_number": 95,
        "API": ".format(",
        "context": [
            "      num_actions = action_spec.maximum - action_spec.minimum + 1\n",
            "      assert (self._stationary_mask.shape[-1] == num_actions), (\n",
            "          'Stationary mask should have length equal to the number of actions, '\n",
            "          'but we get {} and {}.'\n",
            "          .format(num_actions, self._stationary_mask.shape[-1]))\n",
            "\n",
            "    if observation_and_action_constraint_splitter is not None:\n",
            "      if not isinstance(action_spec, tensor_spec.BoundedTensorSpec):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\random_tf_policy.py",
        "line_number": 134,
        "API": ".cast(",
        "context": [
            "        mask = mask * self._stationary_mask\n",
            "\n",
            "      action_spec = tensor_spec.from_spec(self.action_spec)\n",
            "      action_spec = cast(tensor_spec.BoundedTensorSpec, action_spec)\n",
            "      zero_logits = tf.cast(tf.zeros_like(mask), tf.float32)\n",
            "      masked_categorical = masked.MaskedCategorical(zero_logits, mask)\n",
            "      action_ = tf.cast(masked_categorical.sample() + action_spec.minimum,\n",
            "                        action_spec.dtype)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\random_tf_policy.py",
        "line_number": 142,
        "API": ".expand_dims(",
        "context": [
            "\n",
            "      # If the action spec says each action should be shaped (1,), add another\n",
            "      # dimension so the final shape is (B, 1) rather than (B,).\n",
            "      if action_spec.shape.rank == 1:\n",
            "        action_ = tf.expand_dims(action_, axis=-1)\n",
            "      policy_info = tensor_spec.sample_spec_nest(\n",
            "          self._info_spec, outer_dims=outer_dims)\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\random_tf_policy.py",
        "line_number": 151,
        "API": ".shape(",
        "context": [
            "      action_spec = cast(tensor_spec.BoundedTensorSpec, self.action_spec)\n",
            "\n",
            "      if self._accepts_per_arm_features:\n",
            "        max_num_arms = action_spec.maximum - action_spec.minimum + 1\n",
            "        batch_size = tf.shape(time_step.step_type)[0]\n",
            "        num_actions = observation.get(\n",
            "            bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY,\n",
            "            tf.ones(shape=(batch_size,), dtype=tf.int32) * max_num_arms)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\random_tf_policy.py",
        "line_number": 156,
        "API": ".cast(",
        "context": [
            "        num_actions = observation.get(\n",
            "            bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY,\n",
            "            tf.ones(shape=(batch_size,), dtype=tf.int32) * max_num_arms)\n",
            "        mask = tf.sequence_mask(num_actions, max_num_arms)\n",
            "        zero_logits = tf.cast(tf.zeros_like(mask), tf.float32)\n",
            "        masked_categorical = masked.MaskedCategorical(zero_logits, mask)\n",
            "        action_ = tf.nest.map_structure(\n",
            "            lambda t: tf.cast(masked_categorical.sample() + t.minimum, t.dtype),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\random_tf_policy.py",
        "line_number": 162,
        "API": ".shape(",
        "context": [
            "        action_ = tf.nest.map_structure(\n",
            "            lambda t: tf.cast(masked_categorical.sample() + t.minimum, t.dtype),\n",
            "            action_spec)\n",
            "      elif self._stationary_mask is not None:\n",
            "        batch_size = tf.shape(time_step.step_type)[0]\n",
            "        mask = tf.tile(self._stationary_mask, [batch_size, 1])\n",
            "        zero_logits = tf.cast(tf.zeros_like(mask), tf.float32)\n",
            "        masked_categorical = masked.MaskedCategorical(zero_logits, mask)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\random_tf_policy.py",
        "line_number": 178,
        "API": ".gather(",
        "context": [
            "\n",
            "    # Update policy info with chosen arm features.\n",
            "    if self._accepts_per_arm_features:\n",
            "      def _gather_fn(t):\n",
            "        return tf.gather(params=t, indices=action_, batch_dims=1)\n",
            "      chosen_arm_features = tf.nest.map_structure(\n",
            "          _gather_fn, observation[bandit_spec_utils.PER_ARM_FEATURE_KEY])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\random_tf_policy.py",
        "line_number": 189,
        "API": ".to_dense(",
        "context": [
            "\n",
            "    # TODO(b/78181147): Investigate why this control dependency is required.\n",
            "    def _maybe_convert_sparse_tensor(t):\n",
            "      if isinstance(t, tf.SparseTensor):\n",
            "        return tf.sparse.to_dense(t)\n",
            "      else:\n",
            "        return t\n",
            "    if time_step is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\random_tf_policy.py",
        "line_number": 194,
        "API": ".flatten(",
        "context": [
            "      else:\n",
            "        return t\n",
            "    if time_step is not None:\n",
            "      with tf.control_dependencies(\n",
            "          tf.nest.flatten(tf.nest.map_structure(_maybe_convert_sparse_tensor,\n",
            "                                                time_step))):\n",
            "        action_ = tf.nest.map_structure(tf.identity, action_)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\random_tf_policy.py",
        "line_number": 206,
        "API": ".map_structure(",
        "context": [
            "        action_spec = cast(tensor_spec.BoundedTensorSpec, self.action_spec)\n",
            "        log_probability = masked_categorical.log_prob(\n",
            "            action_ - action_spec.minimum)\n",
            "      else:\n",
            "        log_probability = tf.nest.map_structure(\n",
            "            lambda s: _calculate_log_probability(outer_dims, s),\n",
            "            self._action_spec)\n",
            "      policy_info = policy_step.set_log_probability(policy_info,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\scripted_py_policy.py",
        "line_number": 120,
        "API": ".format(",
        "context": [
            "\n",
            "    if not array_spec.check_arrays_nest(current_action, self._action_spec):\n",
            "      raise ValueError(\n",
            "          \"Action at index {} does not match the environment's action_spec. \"\n",
            "          \"Got: {}. Expected {}.\".format(action_index, current_action,\n",
            "                                         self._action_spec))\n",
            "\n",
            "    logging.info(\"Policy_state: %r\", policy_state)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\temporal_action_smoothing.py",
        "line_number": 84,
        "API": ".map_structure(",
        "context": [
            "    def _smooth_action_tensor(smoothing_state_tensor, action_tensor):\n",
            "      return (smoothing_state_tensor * self._smoothing_coefficient +\n",
            "              action_tensor * (1.0 - self._smoothing_coefficient))\n",
            "\n",
            "    smoothed_action = tf.nest.map_structure(_smooth_action_tensor,\n",
            "                                            moving_average,\n",
            "                                            wrapped_policy_step.action)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\tf_policy.py",
        "line_number": 185,
        "API": ".format(",
        "context": [
            "    common.assert_members_are_not_overridden(base_cls=TFPolicy, instance=self)\n",
            "    if not isinstance(time_step_spec, ts.TimeStep):\n",
            "      raise ValueError(\n",
            "          'The `time_step_spec` must be an instance of `TimeStep`, but is `{}`.'\n",
            "          .format(type(time_step_spec)))\n",
            "\n",
            "    self._time_step_spec = tensor_spec.from_spec(time_step_spec)\n",
            "    self._action_spec = tensor_spec.from_spec(action_spec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\tf_policy.py",
        "line_number": 200,
        "API": ".map_structure(",
        "context": [
            "          dtype=tf.float32,\n",
            "          maximum=0,\n",
            "          minimum=-float('inf'),\n",
            "          name='log_probability')\n",
            "      log_probability_spec = tf.nest.map_structure(\n",
            "          lambda _: log_probability_spec, action_spec)\n",
            "      info_spec = policy_step.set_log_probability(\n",
            "          info_spec, log_probability_spec)  # pytype: disable=wrong-arg-types\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\tf_policy.py",
        "line_number": 257,
        "API": ".shape(",
        "context": [
            "      return policy_state\n",
            "\n",
            "    batch_size = tf.compat.dimension_value(time_step.discount.shape[0])\n",
            "    if batch_size is None:\n",
            "      batch_size = tf.shape(time_step.discount)[0]\n",
            "\n",
            "    # Make sure we call this with a kwarg as it may be wrapped in tf.function\n",
            "    # which would expect a tensor if it was not a kwarg.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\tf_policy.py",
        "line_number": 268,
        "API": ".where(",
        "context": [
            "    # time_step in the sequence as we can't easily generalize how the policy is\n",
            "    # unrolled over the sequence.\n",
            "    if nest_utils.get_outer_rank(time_step, self._time_step_spec) > 1:\n",
            "      condition = time_step.is_first()[:, 0, ...]\n",
            "    return nest_utils.where(condition, zero_state, policy_state)\n",
            "\n",
            "  def action(self,\n",
            "             time_step: ts.TimeStep,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\tf_policy.py",
        "line_number": 333,
        "API": ".map_structure(",
        "context": [
            "          step.action, self._action_spec,\n",
            "          message='action and action_spec structures do not match')\n",
            "\n",
            "    if self._clip:\n",
            "      clipped_actions = tf.nest.map_structure(clip_action,\n",
            "                                              step.action,\n",
            "                                              self._action_spec)\n",
            "      step = step._replace(action=clipped_actions)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\tf_policy.py",
        "line_number": 349,
        "API": ".flatten(",
        "context": [
            "        return value.dtype.is_compatible_with(spec.dtype)\n",
            "\n",
            "      compatibility = [\n",
            "          compare_to_spec(v, s) for (v, s)\n",
            "          in zip(tf.nest.flatten(step.action),\n",
            "                 tf.nest.flatten(self.action_spec))]\n",
            "\n",
            "      if not all(compatibility):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\tf_policy.py",
        "line_number": 354,
        "API": ".map_structure(",
        "context": [
            "                 tf.nest.flatten(self.action_spec))]\n",
            "\n",
            "      if not all(compatibility):\n",
            "        get_dtype = lambda x: x.dtype\n",
            "        action_dtypes = tf.nest.map_structure(get_dtype, step.action)\n",
            "        spec_dtypes = tf.nest.map_structure(get_dtype, self.action_spec)\n",
            "\n",
            "        raise TypeError('Policy produced an action with a dtype that doesn\\'t '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\tf_policy.py",
        "line_number": 404,
        "API": ".map_structure(",
        "context": [
            "    if self.emit_log_probability:\n",
            "      # This here is set only for compatibility with info_spec in constructor.\n",
            "      info = policy_step.set_log_probability(\n",
            "          step.info,\n",
            "          tf.nest.map_structure(\n",
            "              lambda _: tf.constant(0., dtype=tf.float32),\n",
            "              policy_step.get_log_probability(self._info_spec)))\n",
            "      step = step._replace(info=info)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\tf_policy.py",
        "line_number": 557,
        "API": ".map_structure(",
        "context": [
            "        `info`: Optional side information such as action log probabilities.\n",
            "    \"\"\"\n",
            "    seed_stream = tfp.util.SeedStream(seed=seed, salt='tf_agents_tf_policy')\n",
            "    distribution_step = self._distribution(time_step, policy_state)  # pytype: disable=wrong-arg-types\n",
            "    actions = tf.nest.map_structure(\n",
            "        lambda d: reparameterized_sampling.sample(d, seed=seed_stream()),\n",
            "        distribution_step.action)\n",
            "    info = distribution_step.info\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\tf_policy.py",
        "line_number": 563,
        "API": ".map_structure(",
        "context": [
            "        distribution_step.action)\n",
            "    info = distribution_step.info\n",
            "    if self.emit_log_probability:\n",
            "      try:\n",
            "        log_probability = tf.nest.map_structure(lambda a, d: d.log_prob(a),\n",
            "                                                actions,\n",
            "                                                distribution_step.action)\n",
            "        info = policy_step.set_log_probability(info, log_probability)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\tf_py_policy.py",
        "line_number": 33,
        "API": ".flatten(",
        "context": [
            "from tf_agents.utils import nest_utils\n",
            "\n",
            "\n",
            "def map_tensor_spec_to_dtypes_list(t_spec):\n",
            "  return [spec.dtype for spec in tf.nest.flatten(t_spec)]\n",
            "\n",
            "\n",
            "class TFPyPolicy(tf_policy.TFPolicy):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\tf_py_policy.py",
        "line_number": 71,
        "API": ".map_structure(",
        "context": [
            "    self._py_policy = policy\n",
            "    self._py_policy_is_batched = py_policy_is_batched\n",
            "\n",
            "    (time_step_spec, action_spec,\n",
            "     policy_state_spec, info_spec) = tf.nest.map_structure(\n",
            "         tensor_spec.from_spec,\n",
            "         (policy.time_step_spec, policy.action_spec, policy.policy_state_spec,\n",
            "          policy.info_spec))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\tf_py_policy.py",
        "line_number": 109,
        "API": ".flatten(",
        "context": [
            "      reset_op: a list of Tensors representing the results of py_policy.reset().\n",
            "    \"\"\"\n",
            "\n",
            "    def _get_initial_state_fn(*batch_size):\n",
            "      return tf.nest.flatten(\n",
            "          self._py_policy.get_initial_state(batch_size=batch_size))\n",
            "\n",
            "    with tf.name_scope('get_initial_state'):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\tf_py_policy.py",
        "line_number": 127,
        "API": ".format(",
        "context": [
            "  @common.function\n",
            "  def _action(self, time_step, policy_state, seed: Optional[types.Seed] = None):\n",
            "    if seed is not None:\n",
            "      raise NotImplementedError(\n",
            "          'seed is not supported; but saw seed: {}'.format(seed))\n",
            "\n",
            "    def _action_fn(*flattened_time_step_and_policy_state):\n",
            "      packed_py_time_step, packed_py_policy_state = tf.nest.pack_sequence_as(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\tf_py_policy.py",
        "line_number": 136,
        "API": ".flatten(",
        "context": [
            "                     self._py_policy.policy_state_spec),\n",
            "          flat_sequence=flattened_time_step_and_policy_state)\n",
            "      py_action_step = self._py_policy.action(\n",
            "          time_step=packed_py_time_step, policy_state=packed_py_policy_state)\n",
            "      return tf.nest.flatten(py_action_step)\n",
            "\n",
            "    with tf.name_scope('action'):\n",
            "      if not self._py_policy_is_batched:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\tf_py_policy.py",
        "line_number": 141,
        "API": ".flatten(",
        "context": [
            "\n",
            "    with tf.name_scope('action'):\n",
            "      if not self._py_policy_is_batched:\n",
            "        time_step = nest_utils.unbatch_nested_tensors(time_step)\n",
            "      flattened_input_tensors = tf.nest.flatten((time_step, policy_state))\n",
            "\n",
            "      flat_action_step = tf.numpy_function(\n",
            "          _action_fn,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\utils.py",
        "line_number": 51,
        "API": ".format(",
        "context": [
            "      with minimum 0.\n",
            "  \"\"\"\n",
            "  if not isinstance(action_spec, tensor_spec.BoundedTensorSpec):\n",
            "    raise ValueError('Action spec must be a `BoundedTensorSpec`; '\n",
            "                     'got {}'.format(type(action_spec)))\n",
            "  if action_spec.shape.rank != 0:\n",
            "    raise ValueError('Action spec must be a scalar; '\n",
            "                     'got shape{}'.format(action_spec.shape))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\utils.py",
        "line_number": 57,
        "API": ".format(",
        "context": [
            "    raise ValueError('Action spec must be a scalar; '\n",
            "                     'got shape{}'.format(action_spec.shape))\n",
            "  if action_spec.dtype not in (tf.int32, tf.int64):\n",
            "    raise ValueError('Action spec must be have dtype int32 or int64; '\n",
            "                     'got {}'.format(action_spec.dtype))\n",
            "  if action_spec.minimum != 0:\n",
            "    raise ValueError('Action spec must have minimum 0; '\n",
            "                     'got {}'.format(action_spec.minimum))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\utils.py",
        "line_number": 131,
        "API": ".map_structure(",
        "context": [
            "    A policy info.\n",
            "  \"\"\"\n",
            "  if accepts_per_arm_features:\n",
            "    # Saving the features for the chosen action to the policy_info.\n",
            "    chosen_arm_features = tf.nest.map_structure(\n",
            "        lambda t: tf.gather(params=t, indices=chosen_actions, batch_dims=1),\n",
            "        arm_observations)\n",
            "    policy_info = PerArmPolicyInfo(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\utils.py",
        "line_number": 205,
        "API": ".constant(",
        "context": [
            "    A Tensor of rank 1 and type `output_type`, with the masked argmax of every\n",
            "    row of `input_tensor`.\n",
            "  \"\"\"\n",
            "  input_tensor.shape.assert_is_compatible_with(mask.shape)\n",
            "  neg_inf = tf.constant(-float('Inf'), input_tensor.dtype)\n",
            "  modified_input = tf.compat.v2.where(\n",
            "      tf.cast(mask, tf.bool), input_tensor, neg_inf)\n",
            "  argmax_tensor = tf.argmax(modified_input, axis=-1, output_type=output_type)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\utils.py",
        "line_number": 210,
        "API": ".reduce_max(",
        "context": [
            "  modified_input = tf.compat.v2.where(\n",
            "      tf.cast(mask, tf.bool), input_tensor, neg_inf)\n",
            "  argmax_tensor = tf.argmax(modified_input, axis=-1, output_type=output_type)\n",
            "  # Replace results for invalid mask rows with -1.\n",
            "  reduce_mask = tf.cast(tf.reduce_max(mask, axis=1), tf.bool)\n",
            "  neg_one = tf.constant(-1, output_type)\n",
            "  return tf.compat.v2.where(reduce_mask, argmax_tensor, neg_one)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\utils.py",
        "line_number": 286,
        "API": ".where(",
        "context": [
            "\n",
            "  Returns:\n",
            "    Tensor containing `BanditPolicyType` enumerations with masked values.\n",
            "  \"\"\"\n",
            "  return tf.where(\n",
            "      mask, tf.fill(tf.shape(values), BanditPolicyType.UNIFORM), values)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\utils.py",
        "line_number": 323,
        "API": ".format(",
        "context": [
            "                                   not None):\n",
            "    raise NotImplementedError(\n",
            "        'Action masking is not allowed for Bandit problems with arm features. '\n",
            "        'To implement a policy that handles variable number of actions, please '\n",
            "        'use the `{}` feature key.'.format(\n",
            "            bandit_spec_utils.NUM_ACTIONS_FEATURE_KEY))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous.py",
        "line_number": 51,
        "API": ".constant(",
        "context": [
            "\n",
            "    super(GaussianActionsSampler, self).__init__(action_spec, sample_clippers)\n",
            "\n",
            "    self._sample_rejecters = sample_rejecters\n",
            "    self._max_rejection_iterations = tf.constant(max_rejection_iterations)\n",
            "\n",
            "    self._support_integer = support_integer\n",
            "    if not support_integer:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous.py",
        "line_number": 59,
        "API": ".format(",
        "context": [
            "      for flat_action_spec in tf.nest.flatten(action_spec):\n",
            "        if flat_action_spec.dtype.is_integer:\n",
            "          raise ValueError('Only continuous action is supported by this '\n",
            "                           'sampler. The action_spec: {} contains discrete '\n",
            "                           'action'.format(action_spec))\n",
            "        if flat_action_spec.shape.rank > 1:\n",
            "          raise ValueError('Only 1d action is supported by this sampler. '\n",
            "                           'The action_spec: {} contains action whose rank > 1.'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous.py",
        "line_number": 64,
        "API": ".format(",
        "context": [
            "        if flat_action_spec.shape.rank > 1:\n",
            "          raise ValueError('Only 1d action is supported by this sampler. '\n",
            "                           'The action_spec: {} contains action whose rank > 1.'\n",
            "                           'Consider coverting it into multiple 1d '\n",
            "                           'actions.'.format(action_spec))\n",
            "\n",
            "  def refit_distribution_to(self, target_sample_indices, samples):\n",
            "    \"\"\"Refits distribution according to actions with index of ind.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous.py",
        "line_number": 82,
        "API": ".moments(",
        "context": [
            "        is the refitted var.\n",
            "    \"\"\"\n",
            "\n",
            "    def get_mean(best_samples):\n",
            "      mean, _ = tf.nn.moments(best_samples, axes=1)  # mu, var: [B, A]\n",
            "      return mean\n",
            "\n",
            "    def get_var(best_samples):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous.py",
        "line_number": 89,
        "API": ".map_structure(",
        "context": [
            "    def get_var(best_samples):\n",
            "      _, var = tf.nn.moments(best_samples, axes=1)  # mu, var: [B, A]\n",
            "      return var\n",
            "\n",
            "    best_samples = tf.nest.map_structure(\n",
            "        lambda s: tf.gather(s, target_sample_indices, batch_dims=1), samples)\n",
            "    mean = tf.nest.map_structure(get_mean, best_samples)\n",
            "    var = tf.nest.map_structure(get_var, best_samples)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous.py",
        "line_number": 114,
        "API": ".sqrt(",
        "context": [
            "        shape [B, N, A]\n",
            "    \"\"\"\n",
            "\n",
            "    def sample_and_transpose(mean, var):\n",
            "      dist = tfp.distributions.Normal(loc=mean, scale=tf.sqrt(var))\n",
            "      sample = tf.transpose(dist.sample(num_samples), [1, 0, 2])\n",
            "      return sample\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous.py",
        "line_number": 122,
        "API": ".map_structure(",
        "context": [
            "    batch_size = tf.shape(tf.nest.flatten(mean)[0])[0]\n",
            "\n",
            "    def sample_fn(mean_sample, var_sample, state_sample):\n",
            "      # [B, N, A]\n",
            "      samples_continuous = tf.nest.map_structure(sample_and_transpose,\n",
            "                                                 mean_sample, var_sample)\n",
            "\n",
            "      if self._sample_clippers:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous.py",
        "line_number": 129,
        "API": ".map_structure(",
        "context": [
            "      if self._sample_clippers:\n",
            "        for sample_clipper in self._sample_clippers:\n",
            "          samples_continuous = sample_clipper(samples_continuous, state_sample)\n",
            "\n",
            "      samples_continuous = tf.nest.map_structure(\n",
            "          common.clip_to_spec, samples_continuous, self._action_spec)\n",
            "      return samples_continuous\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous.py",
        "line_number": 135,
        "API": ".map_structure(",
        "context": [
            "      return samples_continuous\n",
            "\n",
            "    @tf.function\n",
            "    def rejection_sampling(sample_rejector):\n",
            "      valid_batch_samples = tf.nest.map_structure(\n",
            "          lambda spec: tf.TensorArray(spec.dtype, size=batch_size),\n",
            "          self._action_spec)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous.py",
        "line_number": 140,
        "API": ".constant(",
        "context": [
            "          lambda spec: tf.TensorArray(spec.dtype, size=batch_size),\n",
            "          self._action_spec)\n",
            "\n",
            "      for b_indx in tf.range(batch_size):\n",
            "        k = tf.constant(0)\n",
            "        # pylint: disable=cell-var-from-loop\n",
            "        valid_samples = tf.nest.map_structure(\n",
            "            lambda spec: tf.TensorArray(spec.dtype, size=num_samples),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous.py",
        "line_number": 146,
        "API": ".constant(",
        "context": [
            "        valid_samples = tf.nest.map_structure(\n",
            "            lambda spec: tf.TensorArray(spec.dtype, size=num_samples),\n",
            "            self._action_spec)\n",
            "\n",
            "        count = tf.constant(0)\n",
            "        while count < self._max_rejection_iterations:\n",
            "          count += 1\n",
            "          mean_sample = tf.nest.map_structure(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous.py",
        "line_number": 151,
        "API": ".map_structure(",
        "context": [
            "        while count < self._max_rejection_iterations:\n",
            "          count += 1\n",
            "          mean_sample = tf.nest.map_structure(\n",
            "              lambda t: tf.expand_dims(tf.gather(t, b_indx), axis=0), mean)\n",
            "          var_sample = tf.nest.map_structure(\n",
            "              lambda t: tf.expand_dims(tf.gather(t, b_indx), axis=0), var)\n",
            "          if state is not None:\n",
            "            state_sample = tf.nest.map_structure(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous.py",
        "line_number": 164,
        "API": ".where(",
        "context": [
            "\n",
            "          mask = sample_rejector(samples, state_sample)\n",
            "\n",
            "          mask = mask[0, ...]\n",
            "          mask_index = tf.where(mask)[:, 0]\n",
            "\n",
            "          num_mask = tf.shape(mask_index)[0]\n",
            "          if num_mask == 0:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous.py",
        "line_number": 170,
        "API": ".map_structure(",
        "context": [
            "          num_mask = tf.shape(mask_index)[0]\n",
            "          if num_mask == 0:\n",
            "            continue\n",
            "\n",
            "          good_samples = tf.nest.map_structure(\n",
            "              lambda t: tf.gather(t, mask_index, axis=1)[0, ...], samples)\n",
            "\n",
            "          for sample_idx in range(num_mask):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous.py",
        "line_number": 176,
        "API": ".map_structure(",
        "context": [
            "\n",
            "          for sample_idx in range(num_mask):\n",
            "            if k >= num_samples:\n",
            "              break\n",
            "            valid_samples = tf.nest.map_structure(\n",
            "                lambda gs, vs: vs.write(k, gs[sample_idx:sample_idx+1, ...]),\n",
            "                good_samples, valid_samples)\n",
            "            k += 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous.py",
        "line_number": 185,
        "API": ".map_structure(",
        "context": [
            "        if k < num_samples:\n",
            "          zero_samples = tensor_spec.zero_spec_nest(\n",
            "              self._action_spec, outer_dims=(num_samples-k,))\n",
            "          for sample_idx in range(num_samples-k):\n",
            "            valid_samples = tf.nest.map_structure(\n",
            "                lambda gs, vs: vs.write(k, gs[sample_idx:sample_idx+1, ...]),\n",
            "                zero_samples, valid_samples)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous.py",
        "line_number": 192,
        "API": ".map_structure(",
        "context": [
            "\n",
            "        valid_samples = tf.nest.map_structure(lambda vs: vs.concat(),\n",
            "                                              valid_samples)\n",
            "\n",
            "        valid_batch_samples = tf.nest.map_structure(\n",
            "            lambda vbs, vs: vbs.write(b_indx, vs), valid_batch_samples,\n",
            "            valid_samples)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous.py",
        "line_number": 197,
        "API": ".stack(",
        "context": [
            "            lambda vbs, vs: vbs.write(b_indx, vs), valid_batch_samples,\n",
            "            valid_samples)\n",
            "\n",
            "      samples_continuous = tf.nest.map_structure(\n",
            "          lambda a: a.stack(), valid_batch_samples)\n",
            "      return samples_continuous\n",
            "\n",
            "    if self._sample_rejecters:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous.py",
        "line_number": 205,
        "API": ".map_structure(",
        "context": [
            "      samples_continuous = rejection_sampling(self._sample_rejecters)\n",
            "      def set_b_n_shape(t):\n",
            "        t.set_shape(tf.TensorShape([None, num_samples] + t.shape[2:].dims))\n",
            "\n",
            "      tf.nest.map_structure(set_b_n_shape, samples_continuous)\n",
            "    else:\n",
            "      samples_continuous = sample_fn(mean, var, state)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous.py",
        "line_number": 210,
        "API": ".map_structure(",
        "context": [
            "    else:\n",
            "      samples_continuous = sample_fn(mean, var, state)\n",
            "\n",
            "    if self._support_integer:\n",
            "      samples_continuous = tf.nest.map_structure(\n",
            "          lambda t, s: tf.cast(t, s.dtype), samples_continuous,\n",
            "          self._action_spec)\n",
            "    return samples_continuous\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 167,
        "API": ".flatten(",
        "context": [
            "    super(GaussianActionsSampler, self).__init__(\n",
            "        action_spec, sample_clippers, sample_rejecters)\n",
            "\n",
            "    num_one_hot_action = 0\n",
            "    for flat_action_spec in tf.nest.flatten(action_spec):\n",
            "      if flat_action_spec.shape.rank != 1:\n",
            "        raise ValueError('Only 1d action is supported by this sampler. '\n",
            "                         'The action_spec: \\n{}\\n contains action whose rank is'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 172,
        "API": ".format(",
        "context": [
            "      if flat_action_spec.shape.rank != 1:\n",
            "        raise ValueError('Only 1d action is supported by this sampler. '\n",
            "                         'The action_spec: \\n{}\\n contains action whose rank is'\n",
            "                         ' not 1. Consider coverting it into multiple 1d '\n",
            "                         'actions.'.format(action_spec))\n",
            "      if flat_action_spec.dtype.is_integer:\n",
            "        num_one_hot_action = num_one_hot_action + 1\n",
            "        # S\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 183,
        "API": ".format(",
        "context": [
            "    if num_one_hot_action != 1:\n",
            "      raise ValueError('Only continuous action + 1 one_hot action is supported'\n",
            "                       ' by this sampler. The action_spec: \\n{}\\n contains '\n",
            "                       'either multiple one_hot actions or no one_hot '\n",
            "                       'action'.format(action_spec))\n",
            "\n",
            "    if sample_clippers is None:\n",
            "      raise ValueError('Sampler clippers must be set!')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 194,
        "API": ".format(",
        "context": [
            "\n",
            "    if len(sample_clippers) != len(sub_actions_fields):\n",
            "      raise ValueError('Number of sample_clippers must be the same as number of'\n",
            "                       ' sub_actions_fields! sample_clippers: {}, '\n",
            "                       'sub_actions_fields: {}'.format(\n",
            "                           sample_clippers, sub_actions_fields))\n",
            "\n",
            "    if self._sample_rejecters is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 200,
        "API": ".constant(",
        "context": [
            "\n",
            "    if self._sample_rejecters is None:\n",
            "      self._sample_rejecters = [None] * len(sub_actions_fields)\n",
            "\n",
            "    self._max_rejection_iterations = tf.constant(max_rejection_iterations)\n",
            "\n",
            "    self._num_sub_actions = len(sample_clippers)\n",
            "    self._sub_actions_fields = sub_actions_fields\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 209,
        "API": ".sort(",
        "context": [
            "    action_spec_keys = list(sorted(self._action_spec.keys()))\n",
            "    sub_actions_fields_keys = [\n",
            "        item for sublist in self._sub_actions_fields for item in sublist  # pylint: disable=g-complex-comprehension\n",
            "    ]\n",
            "    sub_actions_fields_keys.sort()\n",
            "    if action_spec_keys != sub_actions_fields_keys:\n",
            "      raise ValueError('sub_actions_fields must cover all keys in action_spec!'\n",
            "                       'action_spec_keys: {}, sub_actions_fields_keys:'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 242,
        "API": ".ones(",
        "context": [
            "    for i in range(self._num_sub_actions):\n",
            "      mask = {}\n",
            "      for k in self._action_spec.keys():\n",
            "        if k in self._sub_actions_fields[i]:\n",
            "          mask[k] = tf.ones([1])\n",
            "        else:\n",
            "          mask[k] = tf.zeros([1])\n",
            "      self._masks.append(mask)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 267,
        "API": ".shape(",
        "context": [
            "    \"\"\"\n",
            "\n",
            "    def get_mean(best_samples, spec, index_range_min, index_range_max):\n",
            "      if spec.dtype.is_integer:\n",
            "        return tf.zeros([tf.shape(target_sample_indices)[0], spec.shape[0]])\n",
            "      else:\n",
            "        # In the following we use a customized way to calculate mean and var\n",
            "        # from best_samples. The reason why we don't use standard tf.nn.moment\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 282,
        "API": ".reduce_sum(",
        "context": [
            "        # mean = sum_elites_continuou / num_elites_continuous_expanded\n",
            "        # var = sum((best_samples - mean)^2 - (mean)^2 * num_elites_categorical)\n",
            "        # / num_elites_continuous\n",
            "\n",
            "        sum_elites_continuous = tf.reduce_sum(best_samples, axis=1)  # [B, A]\n",
            "\n",
            "        # num_elites_continuous: [B]\n",
            "        num_elites_continuous = tf.reduce_sum(tf.cast(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 287,
        "API": ".less(",
        "context": [
            "\n",
            "        # num_elites_continuous: [B]\n",
            "        num_elites_continuous = tf.reduce_sum(tf.cast(\n",
            "            tf.logical_and(tf.greater_equal(\n",
            "                target_sample_indices, index_range_min), tf.less(\n",
            "                    target_sample_indices, index_range_max)),\n",
            "            tf.float32), axis=1)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 292,
        "API": ".tile(",
        "context": [
            "                    target_sample_indices, index_range_max)),\n",
            "            tf.float32), axis=1)\n",
            "\n",
            "        # num_elites_continuous_expanded: [B, A]\n",
            "        num_elites_continuous_expanded = tf.tile(tf.expand_dims(\n",
            "            num_elites_continuous, 1), [1, spec.shape.as_list()[0]])\n",
            "\n",
            "        # mean: [B, A]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 303,
        "API": ".shape(",
        "context": [
            "        return mean\n",
            "\n",
            "    def get_var(best_samples, mean, spec, index_range_min, index_range_max):\n",
            "      if spec.dtype.is_integer:\n",
            "        return tf.zeros([tf.shape(target_sample_indices)[0], spec.shape[0]])\n",
            "      else:\n",
            "        # num_elites_continuous: [B]\n",
            "        num_elites_continuous = tf.reduce_sum(tf.cast(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 308,
        "API": ".less(",
        "context": [
            "      else:\n",
            "        # num_elites_continuous: [B]\n",
            "        num_elites_continuous = tf.reduce_sum(tf.cast(\n",
            "            tf.logical_and(tf.greater_equal(\n",
            "                target_sample_indices, index_range_min), tf.less(\n",
            "                    target_sample_indices, index_range_max)),\n",
            "            tf.float32), axis=1)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 313,
        "API": ".tile(",
        "context": [
            "                    target_sample_indices, index_range_max)),\n",
            "            tf.float32), axis=1)\n",
            "\n",
            "        # num_elites_continuous_expanded: [B, A]\n",
            "        num_elites_continuous_expanded = tf.tile(tf.expand_dims(\n",
            "            num_elites_continuous, 1), [1, spec.shape.as_list()[0]])\n",
            "\n",
            "        num_elites = tf.cast(tf.shape(target_sample_indices)[1], tf.float32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 322,
        "API": ".ones(",
        "context": [
            "        num_elites_categorical_expanded = (num_elites -\n",
            "                                           num_elites_continuous_expanded)\n",
            "\n",
            "        # mean_expanded: [M, B, A]\n",
            "        mean_expanded = mean * tf.ones(\n",
            "            [tf.shape(target_sample_indices)[1], 1, 1])\n",
            "        # mean_expanded: [B, M, A]\n",
            "        mean_expanded = tf.transpose(mean_expanded, [1, 0, 2])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 328,
        "API": ".reduce_sum(",
        "context": [
            "        # mean_expanded: [B, M, A]\n",
            "        mean_expanded = tf.transpose(mean_expanded, [1, 0, 2])\n",
            "\n",
            "        var = tf.math.divide_no_nan(\n",
            "            tf.reduce_sum(tf.square(best_samples - mean_expanded), axis=1) -\n",
            "            tf.multiply(tf.square(mean), num_elites_categorical_expanded),\n",
            "            num_elites_continuous_expanded)\n",
            "        return var\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 333,
        "API": ".map_structure(",
        "context": [
            "            tf.multiply(tf.square(mean), num_elites_categorical_expanded),\n",
            "            num_elites_continuous_expanded)\n",
            "        return var\n",
            "\n",
            "    best_samples = tf.nest.map_structure(\n",
            "        lambda s: tf.gather(s, target_sample_indices, batch_dims=1), samples)\n",
            "\n",
            "    if not self._index_range_min or not self._index_range_max:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 340,
        "API": ".map_structure(",
        "context": [
            "    if not self._index_range_min or not self._index_range_max:\n",
            "      raise ValueError('sample_batch_and_clip must be called before '\n",
            "                       'refit_distribution_to!')\n",
            "\n",
            "    mean = tf.nest.map_structure(\n",
            "        get_mean, best_samples, self._action_spec,\n",
            "        self._index_range_min, self._index_range_max)\n",
            "    var = tf.nest.map_structure(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 355,
        "API": ".one_hot(",
        "context": [
            "    num_samples = self._number_samples_all[i]\n",
            "\n",
            "    def sample_and_transpose(mean, var, spec, mask):\n",
            "      if spec.dtype.is_integer:\n",
            "        sample = tf.one_hot(\n",
            "            one_hot_index, self._num_mutually_exclusive_actions)\n",
            "        sample = tf.broadcast_to(\n",
            "            sample,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 360,
        "API": ".constant(",
        "context": [
            "            one_hot_index, self._num_mutually_exclusive_actions)\n",
            "        sample = tf.broadcast_to(\n",
            "            sample,\n",
            "            [tf.shape(mean)[0],\n",
            "             tf.constant(num_samples),  # pylint: disable=cell-var-from-loop\n",
            "             tf.shape(mean)[1]])\n",
            "      else:\n",
            "        dist = tfp.distributions.Normal(loc=mean, scale=tf.sqrt(var))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 365,
        "API": ".transpose(",
        "context": [
            "             tf.shape(mean)[1]])\n",
            "      else:\n",
            "        dist = tfp.distributions.Normal(loc=mean, scale=tf.sqrt(var))\n",
            "        # Transpose to [B, N, A]\n",
            "        sample = tf.transpose(\n",
            "            dist.sample(num_samples), [1, 0, 2])  # pylint: disable=cell-var-from-loop\n",
            "        sample = sample * mask\n",
            "      return tf.cast(sample, spec.dtype)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 370,
        "API": ".shape(",
        "context": [
            "            dist.sample(num_samples), [1, 0, 2])  # pylint: disable=cell-var-from-loop\n",
            "        sample = sample * mask\n",
            "      return tf.cast(sample, spec.dtype)\n",
            "\n",
            "    batch_size = tf.shape(tf.nest.flatten(mean)[0])[0]\n",
            "\n",
            "    def sample_fn(mean_sample, var_sample, state_sample):\n",
            "      # [B, N, A]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 383,
        "API": ".map_structure(",
        "context": [
            "      if self._sample_clippers[i]:\n",
            "        for sample_clipper in self._sample_clippers[i]:\n",
            "          samples_continuous = sample_clipper(samples_continuous, state_sample)\n",
            "\n",
            "      samples_continuous = tf.nest.map_structure(\n",
            "          common.clip_to_spec, samples_continuous, self._action_spec)\n",
            "      return samples_continuous\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 438,
        "API": ".one_hot(",
        "context": [
            "\n",
            "        if k < num_samples:\n",
            "          def sample_zero_and_one_hot(spec):\n",
            "            if spec.dtype.is_integer:\n",
            "              sample = tf.one_hot(\n",
            "                  one_hot_index, self._num_mutually_exclusive_actions)\n",
            "            else:\n",
            "              sample = tf.zeros(spec.shape, spec.dtype)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 443,
        "API": ".broadcast_to(",
        "context": [
            "                  one_hot_index, self._num_mutually_exclusive_actions)\n",
            "            else:\n",
            "              sample = tf.zeros(spec.shape, spec.dtype)\n",
            "\n",
            "            sample = tf.broadcast_to(\n",
            "                sample,\n",
            "                tf.TensorShape([num_samples] + sample.shape.dims))\n",
            "            return tf.cast(sample, spec.dtype)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 448,
        "API": ".map_structure(",
        "context": [
            "                sample,\n",
            "                tf.TensorShape([num_samples] + sample.shape.dims))\n",
            "            return tf.cast(sample, spec.dtype)\n",
            "\n",
            "          zero_samples = tf.nest.map_structure(\n",
            "              sample_zero_and_one_hot, self._action_spec)\n",
            "          for sample_idx in range(num_samples-k):\n",
            "            valid_samples = tf.nest.map_structure(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 455,
        "API": ".concat(",
        "context": [
            "            valid_samples = tf.nest.map_structure(\n",
            "                lambda gs, vs: vs.write(k, gs[sample_idx:sample_idx+1, ...]),\n",
            "                zero_samples, valid_samples)\n",
            "\n",
            "        valid_samples = tf.nest.map_structure(lambda vs: vs.concat(),\n",
            "                                              valid_samples)\n",
            "\n",
            "        valid_batch_samples = tf.nest.map_structure(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 462,
        "API": ".map_structure(",
        "context": [
            "        valid_batch_samples = tf.nest.map_structure(\n",
            "            lambda vbs, vs: vbs.write(b_indx, vs), valid_batch_samples,\n",
            "            valid_samples)\n",
            "\n",
            "      samples_continuous = tf.nest.map_structure(\n",
            "          lambda a: a.stack(), valid_batch_samples)\n",
            "      return samples_continuous\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 471,
        "API": ".map_structure(",
        "context": [
            "      samples_continuous = rejection_sampling(self._sample_rejecters[i])\n",
            "      def set_b_n_shape(t):\n",
            "        t.set_shape(tf.TensorShape([None, num_samples] + t.shape[2:].dims))\n",
            "\n",
            "      tf.nest.map_structure(set_b_n_shape, samples_continuous)\n",
            "    else:\n",
            "      samples_continuous = sample_fn(mean, var, state)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 532,
        "API": ".eye(",
        "context": [
            "      if i == self._categorical_index:\n",
            "        # Samples one_hot actions.\n",
            "        def sample_one_hot(mean, spec):\n",
            "          if spec.dtype.is_integer:\n",
            "            full_one_hot = tf.eye(\n",
            "                self._num_mutually_exclusive_actions,\n",
            "                dtype=tf.int32)  # [S, S]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 538,
        "API": ".range(",
        "context": [
            "                dtype=tf.int32)  # [S, S]\n",
            "\n",
            "            categorical_one_hot = tf.gather(\n",
            "                full_one_hot,\n",
            "                tf.range(one_hot_index,\n",
            "                         one_hot_index+self._num_sub_categorical_actions))\n",
            "\n",
            "            return tf.broadcast_to(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 543,
        "API": ".shape(",
        "context": [
            "                         one_hot_index+self._num_sub_categorical_actions))\n",
            "\n",
            "            return tf.broadcast_to(\n",
            "                categorical_one_hot,\n",
            "                [tf.shape(mean)[0],\n",
            "                 self._num_sub_categorical_actions,\n",
            "                 spec.shape[0]])\n",
            "          else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 548,
        "API": ".shape(",
        "context": [
            "                 self._num_sub_categorical_actions,\n",
            "                 spec.shape[0]])\n",
            "          else:\n",
            "            return tf.zeros([\n",
            "                tf.shape(mean)[0],\n",
            "                self._num_sub_categorical_actions,\n",
            "                spec.shape[0]])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 555,
        "API": ".map_structure(",
        "context": [
            "\n",
            "        samples_one_hot = tf.nest.map_structure(\n",
            "            sample_one_hot, mean, self._action_spec)\n",
            "\n",
            "        samples_one_hot = tf.nest.map_structure(\n",
            "            common.clip_to_spec, samples_one_hot, self._action_spec)\n",
            "\n",
            "        samples_all.append(samples_one_hot)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_continuous_and_one_hot.py",
        "line_number": 568,
        "API": ".map_structure(",
        "context": [
            "        samples_all.append(samples_continuous)\n",
            "\n",
            "        one_hot_index += 1\n",
            "\n",
            "    samples_all = tf.nest.map_structure(\n",
            "        lambda *tensors: tf.concat(tensors, axis=1),\n",
            "        *samples_all)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_hybrid.py",
        "line_number": 45,
        "API": ".flatten(",
        "context": [
            "  def __init__(self, action_spec, sample_clippers=None):\n",
            "\n",
            "    super(GaussianActionsSampler, self).__init__(action_spec, sample_clippers)\n",
            "\n",
            "    for flat_action_spec in tf.nest.flatten(action_spec):\n",
            "      if flat_action_spec.shape.rank > 1:\n",
            "        raise ValueError('Only 1d action is supported by this sampler. '\n",
            "                         'The action_spec: {} contains action whose rank > 1. '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_hybrid.py",
        "line_number": 50,
        "API": ".format(",
        "context": [
            "      if flat_action_spec.shape.rank > 1:\n",
            "        raise ValueError('Only 1d action is supported by this sampler. '\n",
            "                         'The action_spec: {} contains action whose rank > 1. '\n",
            "                         'Consider coverting it into multiple 1d '\n",
            "                         'actions.'.format(action_spec))\n",
            "\n",
            "  def refit_distribution_to(self, target_sample_indices, samples):\n",
            "    \"\"\"Refits distribution according to actions with index of ind.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_hybrid.py",
        "line_number": 68,
        "API": ".moments(",
        "context": [
            "        is the refitted var.\n",
            "    \"\"\"\n",
            "\n",
            "    def get_mean(best_samples):\n",
            "      mean, _ = tf.nn.moments(best_samples, axes=1)  # mu, var: [B, A]\n",
            "      return tf.cast(mean, tf.float32)\n",
            "\n",
            "    def get_var(best_samples):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_hybrid.py",
        "line_number": 73,
        "API": ".cast(",
        "context": [
            "      return tf.cast(mean, tf.float32)\n",
            "\n",
            "    def get_var(best_samples):\n",
            "      _, var = tf.nn.moments(best_samples, axes=1)  # mu, var: [B, A]\n",
            "      return tf.cast(var, tf.float32)\n",
            "\n",
            "    best_samples = tf.nest.map_structure(\n",
            "        lambda s: tf.gather(s, target_sample_indices, batch_dims=1), samples)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_hybrid.py",
        "line_number": 78,
        "API": ".map_structure(",
        "context": [
            "\n",
            "    best_samples = tf.nest.map_structure(\n",
            "        lambda s: tf.gather(s, target_sample_indices, batch_dims=1), samples)\n",
            "    mean = tf.nest.map_structure(get_mean, best_samples)\n",
            "    var = tf.nest.map_structure(get_var, best_samples)\n",
            "\n",
            "    return mean, var\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_hybrid.py",
        "line_number": 100,
        "API": ".sqrt(",
        "context": [
            "        shape [B, N, A]\n",
            "    \"\"\"\n",
            "\n",
            "    def sample_and_transpose(mean, var, spec):\n",
            "      dist = tfp.distributions.Normal(loc=mean, scale=tf.sqrt(var))\n",
            "      sample = tf.transpose(dist.sample(num_samples), [1, 0, 2])\n",
            "      return tf.cast(sample, spec.dtype)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\policies\\samplers\\qtopt_cem_actions_sampler_hybrid.py",
        "line_number": 105,
        "API": ".map_structure(",
        "context": [
            "      sample = tf.transpose(dist.sample(num_samples), [1, 0, 2])\n",
            "      return tf.cast(sample, spec.dtype)\n",
            "\n",
            "    # [B, N, A]\n",
            "    samples = tf.nest.map_structure(\n",
            "        sample_and_transpose, mean, var, self._action_spec)\n",
            "\n",
            "    actions = tf.nest.map_structure(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 201,
        "API": ".format(",
        "context": [
            "        if not callable(is_first):\n",
            "          raise TypeError(\n",
            "              'Argument t is not a Trajectory; did you forget to pass the '\n",
            "              'proper begin_episode_fn to EpisodicReplayBuffer?  Saw: \\'{}\\''\n",
            "              .format(t))\n",
            "        return is_first()\n",
            "      begin_episode_fn = _begin_episode_fn\n",
            "    if end_episode_fn is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 211,
        "API": ".format(",
        "context": [
            "        if not callable(is_last):\n",
            "          raise TypeError(\n",
            "              'Argument t is not a Trajectory; did you forget to pass the '\n",
            "              'proper end_episode_fn to EpisodicReplayBuffer?  Saw: \\'{}\\''\n",
            "              .format(t))\n",
            "        return is_last()\n",
            "      end_episode_fn = _end_episode_fn\n",
            "    if not callable(begin_episode_fn):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 216,
        "API": ".format(",
        "context": [
            "        return is_last()\n",
            "      end_episode_fn = _end_episode_fn\n",
            "    if not callable(begin_episode_fn):\n",
            "      raise TypeError(\n",
            "          'begin_episode_fn is not callable: {}'.format(begin_episode_fn))\n",
            "    if not callable(end_episode_fn):\n",
            "      raise TypeError(\n",
            "          'end_episode_fn is not callable: {}'.format(end_episode_fn))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 232,
        "API": ".device(",
        "context": [
            "    self._dataset_window_shift = dataset_window_shift\n",
            "    self._dataset_drop_remainder = dataset_drop_remainder\n",
            "    self._num_writes = common.create_variable('num_writes_counter')\n",
            "\n",
            "    with tf.device(self._device):\n",
            "      self._data_table = episode_table_fn(\n",
            "          self._data_spec, self._capacity, self._name_prefix)\n",
            "      # The episode ids\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 266,
        "API": ".reduce_sum(",
        "context": [
            "    return self._name_prefix\n",
            "\n",
            "  def _num_frames(self):\n",
            "    \"\"\"Returns the number of frames in the buffer.\"\"\"\n",
            "    return tf.math.reduce_sum(self._episode_lengths)\n",
            "\n",
            "  def create_episode_ids(self, num_episodes=None):\n",
            "    \"\"\"Returns a new tensor containing initial invalid episode ID(s).\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 307,
        "API": ".is_tensor(",
        "context": [
            "\n",
            "    Raises:\n",
            "      ValueError: If `num_episodes` is bigger than capacity, or non-scalar.\n",
            "    \"\"\"\n",
            "    if tf.is_tensor(num_episodes):\n",
            "      if num_episodes.shape.rank != 0:\n",
            "        raise ValueError('num_episodes must be a scalar, but saw shape: {}'\n",
            "                         .format(num_episodes.shape))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 313,
        "API": ".convert_to_tensor(",
        "context": [
            "        raise ValueError('num_episodes must be a scalar, but saw shape: {}'\n",
            "                         .format(num_episodes.shape))\n",
            "      return tf.fill(\n",
            "          [num_episodes],\n",
            "          tf.convert_to_tensor(_INVALID_EPISODE_ID, dtype=tf.int64),\n",
            "          name='episode_id')\n",
            "\n",
            "    shape = ()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 320,
        "API": ".format(",
        "context": [
            "    shape = ()\n",
            "    if num_episodes is not None and num_episodes > 0:\n",
            "      if num_episodes > self._capacity:\n",
            "        raise ValueError('Buffer cannot create episode_ids when '\n",
            "                         'num_episodes {} > capacity {}.'.format(\n",
            "                             num_episodes, self._capacity))\n",
            "      shape = (num_episodes,)\n",
            "    return tf.constant(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 341,
        "API": ".device(",
        "context": [
            "      An updated episode id Tensor.  Accessing this episode id value will,\n",
            "      as a side effect, start or end the current episode in the buffer.\n",
            "    \"\"\"\n",
            "    episode_id.shape.assert_has_rank(0)\n",
            "    with tf.device(self._device):\n",
            "      with tf.name_scope('add_steps'):\n",
            "        # If users pass in, e.g., a python list [2, 3, 4] of type int32\n",
            "        # but the data_spec requires an int64, then the user will get a very\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 348,
        "API": ".map_structure(",
        "context": [
            "        # but the data_spec requires an int64, then the user will get a very\n",
            "        # confusing error much deeper in the TensorList code.  Doing the\n",
            "        # conversion here either converts when necessary, or raises an error\n",
            "        # on incompatible types earlier in the run.\n",
            "        items = tf.nest.map_structure(\n",
            "            lambda x, spec: tf.convert_to_tensor(value=x, dtype=spec.dtype),\n",
            "            items, self._data_spec)\n",
            "        item_0 = tf.nest.flatten(items)[0]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 354,
        "API": ".shape(",
        "context": [
            "            items, self._data_spec)\n",
            "        item_0 = tf.nest.flatten(items)[0]\n",
            "        num_steps = tf.cast(\n",
            "            tf.compat.dimension_value(item_0.shape[0]) or\n",
            "            tf.shape(input=item_0)[0], tf.int64)\n",
            "        # If begin_episode is True, then the increment of the episode_id happens\n",
            "        # before trying to add anything to the buffer, regardless of whether the\n",
            "        # item will actually be added.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 369,
        "API": ".control_dependencies(",
        "context": [
            "          \"\"\"Add sequence of items to the buffer.\"\"\"\n",
            "          inc_episode_length = self._increment_episode_length_locked(\n",
            "              episode_location, num_steps)\n",
            "          write_data_op = self._data_table.append(episode_location, items)\n",
            "          with tf.control_dependencies([inc_episode_length, write_data_op]):\n",
            "            return tf.identity(new_episode_id)\n",
            "\n",
            "        # Accessing episode_id may modify\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 375,
        "API": ".control_dependencies(",
        "context": [
            "\n",
            "        # Accessing episode_id may modify\n",
            "        # self._episodes_loc_to_id_map, so ensure it is executed\n",
            "        # before the tf.equal.\n",
            "        with tf.control_dependencies([new_episode_id]):\n",
            "          episode_valid = tf.equal(\n",
            "              self._episodes_loc_to_id_map[episode_location], new_episode_id)\n",
            "        def _maybe_add_steps():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 383,
        "API": ".identity(",
        "context": [
            "          return self._add_episode_critical_section.execute(_add_steps)\n",
            "        return utils.smart_cond(\n",
            "            episode_valid,\n",
            "            _maybe_add_steps,\n",
            "            lambda: tf.identity(new_episode_id),\n",
            "            name='conditioned_add_steps')\n",
            "\n",
            "  def add_batch(self, items, episode_ids):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 402,
        "API": ".device(",
        "context": [
            "      A `Tensor` containing the updated episode ids.  Accessing or executing\n",
            "      this tensor also adds `items` to the replay buffer.\n",
            "    \"\"\"\n",
            "    episode_ids.shape.assert_has_rank(1)\n",
            "    with tf.device(self._device):\n",
            "      with tf.name_scope('add_batch'):\n",
            "        # If begin_episode is True, then the increment of the episode_id happens\n",
            "        # before trying to add anything to the buffer, regardless of whether the\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 416,
        "API": ".control_dependencies(",
        "context": [
            "                                                        end_episode)\n",
            "        episodes_locations = tf.math.mod(batch_episode_ids, self._capacity)\n",
            "        # Accessing episode_id may modify self._episodes_loc_to_id_map, so\n",
            "        # ensure it is executed before\n",
            "        with tf.control_dependencies([episodes_locations]):\n",
            "          episode_valid = tf.equal(\n",
            "              self._episodes_loc_to_id_map.sparse_read(episodes_locations),\n",
            "              batch_episode_ids)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 423,
        "API": ".reshape(",
        "context": [
            "              batch_episode_ids)\n",
            "\n",
            "        def _add_batch():\n",
            "          \"\"\"Add elements to the appropiate episode_locations.\"\"\"\n",
            "          ids_to_update = tf.reshape(tf.compat.v1.where(episode_valid), [-1])\n",
            "          episodes_locations_ = tf.gather(episodes_locations, ids_to_update)\n",
            "          filter_items = lambda item: tf.gather(item, ids_to_update)\n",
            "          items_ = tf.nest.map_structure(filter_items, items)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 431,
        "API": ".control_dependencies(",
        "context": [
            "          write_data_op = self._data_table.add(episodes_locations_, items_)\n",
            "          inc_episode_lengths = self._increment_episode_length_locked(\n",
            "              episodes_locations_)\n",
            "          inc_write_counter_op = self._num_writes.assign_add(1)\n",
            "          with tf.control_dependencies([\n",
            "              write_data_op, inc_episode_lengths, inc_write_counter_op]):\n",
            "            return tf.identity(batch_episode_ids)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 440,
        "API": ".cond(",
        "context": [
            "\n",
            "        def _maybe_add_batch():\n",
            "          return self._add_episode_critical_section.execute(_add_batch)\n",
            "\n",
            "        return tf.cond(\n",
            "            pred=num_adds > 0,\n",
            "            true_fn=_maybe_add_batch,\n",
            "            false_fn=lambda: episode_ids)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 482,
        "API": ".device(",
        "context": [
            "\n",
            "        - An episode sampled uniformly from the buffer.\n",
            "        - BufferInfo NamedTuple, containing the episode id.\n",
            "    \"\"\"\n",
            "    with tf.device(self._device):\n",
            "      with tf.name_scope('get_next'):\n",
            "        episode_id = self._sample_episode_ids(shape=[], seed=self._seed)\n",
            "        row = self._get_episode_id_location(episode_id)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 540,
        "API": ".flatten(",
        "context": [
            "    if sequence_preprocess_fn is not None:\n",
            "      raise NotImplementedError('sequence_preprocess_fn is not supported.')\n",
            "\n",
            "    # data_tf.nest.flatten does not flatten python lists, tf.nest.flatten does.\n",
            "    if tf.nest.flatten(self._data_spec) != data_nest.flatten(self._data_spec):\n",
            "      raise ValueError(\n",
            "          'Cannot perform gather; data spec contains lists and this conflicts '\n",
            "          'with gathering operator.  Convert any lists to tuples.  '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 545,
        "API": ".format(",
        "context": [
            "      raise ValueError(\n",
            "          'Cannot perform gather; data spec contains lists and this conflicts '\n",
            "          'with gathering operator.  Convert any lists to tuples.  '\n",
            "          'For example, if your spec looks like [a, b, c], '\n",
            "          'change it to (a, b, c).  Spec structure is:\\n  {}'.format(\n",
            "              tf.nest.map_structure(lambda spec: spec.dtype, self._data_spec)))\n",
            "\n",
            "    seed_per_episode = distributions_util.gen_new_seed(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 570,
        "API": ".boolean_mask(",
        "context": [
            "            seed=self._seed)\n",
            "      episode_locations = self._get_episode_id_location(episode_ids)\n",
            "\n",
            "      if self._completed_only:\n",
            "        return tf.boolean_mask(\n",
            "            tensor=episode_locations,\n",
            "            mask=self._episode_completed.sparse_read(episode_locations))\n",
            "      else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 591,
        "API": ".flatten(",
        "context": [
            "      def _read_tensor_list_and_id(row):\n",
            "        \"\"\"Read the TensorLists out of the table row, get id and num_frames.\"\"\"\n",
            "        # Return a flattened tensor list\n",
            "        flat_tensor_lists = tuple(\n",
            "            tf.nest.flatten(self._data_table.get_episode_lists(row)))\n",
            "        # Due to race conditions, not all entries may have been written for the\n",
            "        # given episode.  Use the minimum list length to identify the full valid\n",
            "        # available length.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 612,
        "API": ".uniform(",
        "context": [
            "      @tf.autograph.experimental.do_not_convert\n",
            "      def _random_slice(flat_tensor_lists, id_, num_frames):\n",
            "        \"\"\"Take a random slice from the episode, of length num_steps.\"\"\"\n",
            "        # Sample uniformly between [0, num_frames - num_steps]\n",
            "        start_slice = tf.random.uniform((),\n",
            "                                        minval=0,\n",
            "                                        maxval=num_frames - num_steps + 1,\n",
            "                                        dtype=tf.int32,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 619,
        "API": ".flatten(",
        "context": [
            "                                        dtype=tf.int32,\n",
            "                                        seed=seed_per_episode)\n",
            "        end_slice = start_slice + num_steps\n",
            "\n",
            "        flat_spec = tf.nest.flatten(self._data_spec)\n",
            "\n",
            "        # Pull out frames in [start_slice, start_slice + num_steps]\n",
            "        flat = tuple(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 624,
        "API": ".range(",
        "context": [
            "\n",
            "        # Pull out frames in [start_slice, start_slice + num_steps]\n",
            "        flat = tuple(\n",
            "            list_ops.tensor_list_gather(  # pylint: disable=g-complex-comprehension\n",
            "                t, indices=tf.range(start_slice, end_slice),\n",
            "                element_dtype=spec.dtype, element_shape=spec.shape)\n",
            "            for t, spec in zip(flat_tensor_lists, flat_spec))\n",
            "        return flat, id_\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 649,
        "API": ".batch(",
        "context": [
            "      # We set drop_remainder on this batch since the dataset never ends,\n",
            "      # therefore setting this will not cause any lost data and allows the\n",
            "      # output tensors to have a definite leading dimension of\n",
            "      # `sample_batch_size`.\n",
            "      ds = ds.batch(sample_batch_size, drop_remainder=True)\n",
            "\n",
            "    return ds\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 703,
        "API": ".range(",
        "context": [
            "\n",
            "    def get_episode_ids(_):\n",
            "      min_frame_offset, max_frame_offset = _valid_range_ids(\n",
            "          self._get_last_episode_id(), self._capacity)\n",
            "      return tf.data.Dataset.range(min_frame_offset, max_frame_offset)\n",
            "\n",
            "    # Instead of calling get_episode_ids and creating a dataset from this,\n",
            "    # we instead build a dataset whose iterator recalculates the available\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 710,
        "API": ".range(",
        "context": [
            "    # we instead build a dataset whose iterator recalculates the available\n",
            "    # episode_ids in the dataset whenever it is reinitialized.  We\n",
            "    # want to do this because the RB valid episodes can change over time;\n",
            "    # specifically the RB may be empty when this dataset is first created.\n",
            "    episode_ids_ds = tf.data.Dataset.range(1).flat_map(get_episode_ids)\n",
            "\n",
            "    def read_episode(episode_id):\n",
            "      row = self._get_episode_id_location(episode_id)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 724,
        "API": ".map_structure(",
        "context": [
            "        # Disable autograph to make debugging errors easier.\n",
            "        @tf.autograph.experimental.do_not_convert\n",
            "        def group_windows(windowed):\n",
            "          return tf.data.Dataset.zip(\n",
            "              tf.nest.map_structure(\n",
            "                  lambda d: d.batch(num_steps, drop_remainder=drop_remainder),\n",
            "                  windowed))\n",
            "        ds = (ds.unbatch()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 736,
        "API": ".range(",
        "context": [
            "      # per the check at the top of this function.\n",
            "      assert num_steps is not None\n",
            "\n",
            "      # Split up the replay buffer into sample_batch_size parallel datasets.\n",
            "      ds_shards = (tf.data.Dataset.range(sample_batch_size)\n",
            "                   .map(lambda i: ds.shard(sample_batch_size, i)))\n",
            "      # In each dataset, convert different-length episodes to blocks of size\n",
            "      # num_steps.  The very final blocks may be dropped if their size is not a\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 746,
        "API": ".map_structure(",
        "context": [
            "      @tf.autograph.experimental.do_not_convert\n",
            "      def rebatch(ds_):\n",
            "        def batch_nest(window):\n",
            "          return tf.data.Dataset.zip(\n",
            "              tf.nest.map_structure(\n",
            "                  lambda d: d.batch(num_steps, drop_remainder=True),\n",
            "                  window))\n",
            "        return (ds_\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 756,
        "API": ".batch(",
        "context": [
            "                .flat_map(batch_nest))\n",
            "      ds_shards = ds_shards.map(rebatch)\n",
            "      ds = ds_shards.interleave(lambda ds_: ds_)\n",
            "      # Batch by sample_batch_size from the interleaved stream.\n",
            "      ds = ds.batch(sample_batch_size, drop_remainder=drop_remainder)\n",
            "\n",
            "    return ds\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 772,
        "API": ".flatten(",
        "context": [
            "    Raises:\n",
            "      ValueError: If the data spec contains lists that must be converted to\n",
            "        tuples.\n",
            "    \"\"\"\n",
            "    if tf.nest.flatten(self._data_spec) != data_nest.flatten(self._data_spec):\n",
            "      raise ValueError(\n",
            "          'Cannot perform gather; data spec contains lists and this conflicts '\n",
            "          'with gathering operator.  Convert any lists to tuples.  '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 778,
        "API": ".map_structure(",
        "context": [
            "          'Cannot perform gather; data spec contains lists and this conflicts '\n",
            "          'with gathering operator.  Convert any lists to tuples.  '\n",
            "          'For example, if your spec looks like [a, b, c], '\n",
            "          'change it to (a, b, c).  Spec structure is:\\n  %s' %\n",
            "          tf.nest.map_structure(lambda spec: spec.dtype, self._data_spec))\n",
            "\n",
            "    min_val, max_val = _valid_range_ids(self._get_last_episode_id(),\n",
            "                                        self._capacity)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 786,
        "API": ".shape(",
        "context": [
            "\n",
            "    def get_episode_and_id(id_):\n",
            "      row = self._get_episode_id_location(id_)\n",
            "      data = self._data_table.get_episode_values(row)\n",
            "      n = tf.shape(tf.nest.flatten(data)[0])[0]\n",
            "      id_repeated = tf.fill([n], id_)\n",
            "      return (tuple(tf.nest.flatten(data)), id_repeated)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 792,
        "API": ".cast(",
        "context": [
            "      return (tuple(tf.nest.flatten(data)), id_repeated)\n",
            "\n",
            "    episode_lengths = self._episode_lengths.read_value()\n",
            "    if self._completed_only:\n",
            "      episode_lengths *= tf.cast(self._episode_completed, dtype=tf.int64)\n",
            "    total_length = tf.reduce_sum(input_tensor=episode_lengths)\n",
            "\n",
            "    def via_iterator():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 797,
        "API": ".range(",
        "context": [
            "    total_length = tf.reduce_sum(input_tensor=episode_lengths)\n",
            "\n",
            "    def via_iterator():\n",
            "      \"\"\"If total_length > 0, create a dataset iterator to concat episodes.\"\"\"\n",
            "      valid_episodes = tf.range(min_val, max_val)\n",
            "      ds = tf.data.Dataset.from_tensor_slices(valid_episodes)\n",
            "\n",
            "      if self._completed_only:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 803,
        "API": ".cast(",
        "context": [
            "\n",
            "      if self._completed_only:\n",
            "        # Filter out incomplete episodes.\n",
            "        def check_completed(id_):\n",
            "          return tf.cast(self._episode_completed.sparse_read(id_), tf.bool)\n",
            "        ds = ds.filter(check_completed)\n",
            "\n",
            "      def _unflatten(flat_data, id_):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 814,
        "API": ".batch(",
        "context": [
            "          ds.map(get_episode_and_id).unbatch()\n",
            "          # Batch all the frames in the buffer.  Request a larger amount in\n",
            "          # case the buffer grows between the construction of total_length and\n",
            "          # the call to .map().\n",
            "          .batch(10 + 2 * total_length).map(_unflatten)).batch(1)\n",
            "\n",
            "      # Use ds.take(1) in case we haven't requested a large enough batch (the\n",
            "      # replay buffer has grown too quickly), since get_single_element requires\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 825,
        "API": ".zeros(",
        "context": [
            "\n",
            "    def empty():\n",
            "\n",
            "      def _empty_from_spec(spec):\n",
            "        return tf.zeros([0] + spec.shape.as_list(), spec.dtype, name='empty')\n",
            "\n",
            "      empty_data = tf.nest.map_structure(_empty_from_spec, self._data_spec)\n",
            "      empty_id = tf.zeros([], dtype=tf.int64)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 831,
        "API": ".cond(",
        "context": [
            "      empty_data = tf.nest.map_structure(_empty_from_spec, self._data_spec)\n",
            "      empty_id = tf.zeros([], dtype=tf.int64)\n",
            "      return empty_data, empty_id\n",
            "\n",
            "    return tf.cond(pred=total_length > 0, true_fn=via_iterator, false_fn=empty)\n",
            "\n",
            "  def _clear(self, clear_all_variables=False):\n",
            "    \"\"\"Clears the replay buffer.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 844,
        "API": ".assign(",
        "context": [
            "    Returns:\n",
            "      An op to clear the buffer.\n",
            "    \"\"\"\n",
            "    assignments = [\n",
            "        self._episode_lengths.assign(tf.zeros_like(self._episode_lengths))]\n",
            "    assignments += [self._num_writes.assign(tf.zeros_like(self._num_writes))]\n",
            "\n",
            "    if clear_all_variables:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 849,
        "API": ".assign(",
        "context": [
            "    assignments += [self._num_writes.assign(tf.zeros_like(self._num_writes))]\n",
            "\n",
            "    if clear_all_variables:\n",
            "      zero_vars = self._id_table.variables() + [self._episode_completed]\n",
            "      assignments += [var.assign(tf.zeros_like(var)) for var in zero_vars]\n",
            "      neg_one_vars = [self._episodes_loc_to_id_map, self._last_episode]\n",
            "      assignments += [var.assign(_INVALID_EPISODE_ID * tf.ones_like(var))\n",
            "                      for var in neg_one_vars]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 854,
        "API": ".group(",
        "context": [
            "      neg_one_vars = [self._episodes_loc_to_id_map, self._last_episode]\n",
            "      assignments += [var.assign(_INVALID_EPISODE_ID * tf.ones_like(var))\n",
            "                      for var in neg_one_vars]\n",
            "\n",
            "    return tf.group(self._data_table.clear(), assignments, name='clear')\n",
            "\n",
            "  # Other private methods.\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 865,
        "API": ".equal(",
        "context": [
            "    Returns:\n",
            "      An int64 vector of length at most `capacity`.\n",
            "    \"\"\"\n",
            "    def _completed_episodes():\n",
            "      completed_mask = tf.equal(self._episode_completed, 1)\n",
            "      return tf.boolean_mask(\n",
            "          tensor=self._episodes_loc_to_id_map, mask=completed_mask)\n",
            "    return self._add_episode_critical_section.execute(_completed_episodes)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 883,
        "API": ".device(",
        "context": [
            "\n",
            "    Raises:\n",
            "      InvalidArgumentException: (at runtime) if episode_id is not valid.\n",
            "    \"\"\"\n",
            "    with tf.device(self._device), tf.name_scope('get_episode'):\n",
            "      episode_id = tf.convert_to_tensor(\n",
            "          episode_id, dtype=tf.int64, name='episode_id')\n",
            "      episode_location = self._get_episode_id_location(episode_id)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 889,
        "API": ".control_dependencies(",
        "context": [
            "          episode_id, dtype=tf.int64, name='episode_id')\n",
            "      episode_location = self._get_episode_id_location(episode_id)\n",
            "      # Accessing episode_id may modify _episodes_loc_to_id_map upstream, so\n",
            "      # ensure that we've performed that modification *first*.\n",
            "      with tf.control_dependencies([episode_id]):\n",
            "        episode_at_location = self._episodes_loc_to_id_map[episode_location]\n",
            "      episode_valid = tf.equal(episode_at_location, episode_id)\n",
            "      assert_valid = tf.Assert(episode_valid, [\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 897,
        "API": ".control_dependencies(",
        "context": [
            "          'Episode id', episode_id, 'is not valid.  It points to location',\n",
            "          episode_location, 'but the episode at that location is currently id',\n",
            "          episode_at_location\n",
            "      ])\n",
            "      with tf.control_dependencies([assert_valid, episode_location]):\n",
            "        return self._data_table.get_episode_values(episode_location)\n",
            "\n",
            "  def _maybe_end_episode(self, episode_id, end_episode=False):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 915,
        "API": ".group(",
        "context": [
            "\n",
            "    def _maybe_end_episode_id():\n",
            "      \"\"\"Maybe end episode ID.\"\"\"\n",
            "      def _end_episode_id():\n",
            "        return tf.group(\n",
            "            tf.compat.v1.scatter_update(self._episode_completed,\n",
            "                                        [episode_location], 1))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 926,
        "API": ".format(",
        "context": [
            "\n",
            "      pred_value = end_episode & (episode_id >= 0) & episode_valid\n",
            "      if pred_value.shape.rank != 0:\n",
            "        raise ValueError('Invalid condition shape: {} (should be scalar).'\n",
            "                         .format(pred_value.shape))\n",
            "      maybe_end = tf.cond(\n",
            "          pred=pred_value,\n",
            "          true_fn=_end_episode_id,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 932,
        "API": ".control_dependencies(",
        "context": [
            "          pred=pred_value,\n",
            "          true_fn=_end_episode_id,\n",
            "          false_fn=tf.no_op,\n",
            "          name='maybe_end_episode_id')\n",
            "      with tf.control_dependencies([maybe_end]):\n",
            "        return self._episode_completed.sparse_read(episode_location) > 0\n",
            "\n",
            "    return self._add_episode_critical_section.execute(_maybe_end_episode_id)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 952,
        "API": ".equal(",
        "context": [
            "    episodes_location = self._get_episode_id_location(batch_episode_ids)\n",
            "\n",
            "    def _execute():\n",
            "      \"\"\"Maybe end episode ID.\"\"\"\n",
            "      valid_episodes = tf.equal(\n",
            "          batch_episode_ids,\n",
            "          self._episodes_loc_to_id_map.sparse_read(episodes_location))\n",
            "      maybe_end_mask = end_episode & (batch_episode_ids >= 0) & valid_episodes\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 960,
        "API": ".control_dependencies(",
        "context": [
            "      episodes_location_ = tf.boolean_mask(\n",
            "          tensor=episodes_location, mask=maybe_end_mask)\n",
            "      update_completed = tf.compat.v1.scatter_update(self._episode_completed,\n",
            "                                                     episodes_location_, 1)\n",
            "      with tf.control_dependencies([update_completed]):\n",
            "        return self._episode_completed.sparse_read(episodes_location) > 0\n",
            "\n",
            "    return self._add_episode_critical_section.execute(_execute)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 989,
        "API": ".expand_dims(",
        "context": [
            "          self._episodes_loc_to_id_map, episode_location, new_episode_id)\n",
            "      update_completed = tf.compat.v1.scatter_update(self._episode_completed,\n",
            "                                                     episode_location, 0)\n",
            "      reset_data = self._data_table.clear_rows(\n",
            "          tf.expand_dims(episode_location, 0))\n",
            "      reset_length = tf.compat.v1.scatter_update(self._episode_lengths,\n",
            "                                                 episode_location, 0)\n",
            "      with tf.control_dependencies([\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 994,
        "API": ".identity(",
        "context": [
            "      reset_length = tf.compat.v1.scatter_update(self._episode_lengths,\n",
            "                                                 episode_location, 0)\n",
            "      with tf.control_dependencies([\n",
            "          update_mapping, update_completed, reset_data, reset_length]):\n",
            "        return tf.identity(new_episode_id)\n",
            "\n",
            "    def _get_new_episode_id():\n",
            "      return self._add_episode_critical_section.execute(_assign_new_episode_id)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 999,
        "API": ".convert_to_tensor(",
        "context": [
            "\n",
            "    def _get_new_episode_id():\n",
            "      return self._add_episode_critical_section.execute(_assign_new_episode_id)\n",
            "\n",
            "    begin_episode = tf.convert_to_tensor(\n",
            "        value=begin_episode, name='begin_episode')\n",
            "    end_episode = tf.convert_to_tensor(value=end_episode, name='end_episode')\n",
            "    # If episode_id value is still -1 we need to assign a proper value.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1007,
        "API": ".format(",
        "context": [
            "    pred_value = begin_episode | (episode_id < 0)\n",
            "    if pred_value.shape.rank != 0:\n",
            "      raise ValueError('Invalid condition predicate shape: {} '\n",
            "                       '(should be scalar).'\n",
            "                       .format(pred_value.shape))\n",
            "    updated_episode_id = tf.cond(\n",
            "        pred=pred_value,\n",
            "        true_fn=_get_new_episode_id,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1016,
        "API": ".control_dependencies(",
        "context": [
            "        name='get_episode_id')\n",
            "\n",
            "    # _maybe_end_episode acquires the critical section.\n",
            "    mark_completed = self._maybe_end_episode(updated_episode_id, end_episode)\n",
            "    with tf.control_dependencies([mark_completed]):\n",
            "      updated_episode_id = tf.identity(updated_episode_id)\n",
            "    return updated_episode_id\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1051,
        "API": ".convert_to_tensor(",
        "context": [
            "      raise ValueError(\n",
            "          'batch_episode_ids must be a vector with 1 dimension')\n",
            "    # If batch_episode_ids value is still -1 we need to assign a\n",
            "    # proper value.  Find which IDs need to be updated.\n",
            "    begin_episode = tf.convert_to_tensor(value=begin_episode)\n",
            "    end_episode = tf.convert_to_tensor(value=end_episode)\n",
            "    ids_to_update_mask = ((batch_episode_ids < 0) | begin_episode)\n",
            "    if mask is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1059,
        "API": ".reduce_sum(",
        "context": [
            "      ids_to_update_mask &= mask\n",
            "\n",
            "    def _update_batch_episode_ids():\n",
            "      \"\"\"Increment the episode_id inside a critical section.\"\"\"\n",
            "      num_ids = tf.reduce_sum(\n",
            "          input_tensor=tf.cast(ids_to_update_mask, tf.int64))\n",
            "      end_id = self._last_episode.assign_add(num_ids).value() + 1\n",
            "      start_id = end_id - num_ids\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1065,
        "API": ".where(",
        "context": [
            "      end_id = self._last_episode.assign_add(num_ids).value() + 1\n",
            "      start_id = end_id - num_ids\n",
            "      new_batch_episode_ids = tf.range(start_id, end_id)\n",
            "      # Update when b/74385543 is fixed.\n",
            "      ids_to_update = tf.compat.v1.where(ids_to_update_mask)\n",
            "      scattered_updated_episode_ids = tf.scatter_nd(\n",
            "          ids_to_update, new_batch_episode_ids,\n",
            "          shape=tf.shape(batch_episode_ids, out_type=tf.int64))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1082,
        "API": ".control_dependencies(",
        "context": [
            "                                                    [episode_locations], 0)\n",
            "      reset_data = self._data_table.clear_rows(episode_locations)\n",
            "      reset_length = tf.compat.v1.scatter_update(self._episode_lengths,\n",
            "                                                 episode_locations, 0)\n",
            "      with tf.control_dependencies([\n",
            "          update_mapping, reset_completed, reset_data, reset_length]):\n",
            "        return tf.identity(updated_batch_episode_ids)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1092,
        "API": ".control_dependencies(",
        "context": [
            "\n",
            "    # _maybe_end_batch_episodes acquires the critical section.\n",
            "    mark_completed = self._maybe_end_batch_episodes(episode_ids, end_episode)\n",
            "\n",
            "    with tf.control_dependencies([mark_completed]):\n",
            "      episode_ids = tf.identity(episode_ids)\n",
            "    return episode_ids\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1109,
        "API": ".control_dependencies(",
        "context": [
            "        message='EpisodicReplayBuffer is empty. Make sure to add items '\n",
            "        'before sampling the buffer.')\n",
            "    if weigh_by_episode_length:\n",
            "      # Sample episodes proportional to length.\n",
            "      with tf.control_dependencies([assert_nonempty]):\n",
            "        num_episodes = tf.minimum(self._last_episode + 1, self._capacity)\n",
            "        episode_lengths = self._episode_lengths[:num_episodes]\n",
            "        logits = tf.math.log(tf.cast(episode_lengths, tf.float32))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1114,
        "API": ".categorical(",
        "context": [
            "        num_episodes = tf.minimum(self._last_episode + 1, self._capacity)\n",
            "        episode_lengths = self._episode_lengths[:num_episodes]\n",
            "        logits = tf.math.log(tf.cast(episode_lengths, tf.float32))\n",
            "        return tf.reshape(\n",
            "            tf.random.categorical(\n",
            "                [logits],  # shape is: [1, num_episodes]\n",
            "                num_samples=tf.reduce_prod(shape),\n",
            "                seed=seed,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1123,
        "API": ".control_dependencies(",
        "context": [
            "            shape)\n",
            "    else:\n",
            "      min_val, max_val = _valid_range_ids(self._get_last_episode_id(),\n",
            "                                          self._capacity)\n",
            "      with tf.control_dependencies([assert_nonempty]):\n",
            "        return tf.random.uniform(\n",
            "            shape, minval=min_val, maxval=max_val, dtype=tf.int64, seed=seed)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1129,
        "API": ".value(",
        "context": [
            "            shape, minval=min_val, maxval=max_val, dtype=tf.int64, seed=seed)\n",
            "\n",
            "  def _get_last_episode_id(self):\n",
            "    def last_episode():\n",
            "      return self._last_episode.value()\n",
            "\n",
            "    return self._add_episode_critical_section.execute(last_episode)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1153,
        "API": ".control_dependencies(",
        "context": [
            "    def _assign_add():\n",
            "      new_length = self._episode_lengths[episode_location] + increment\n",
            "      update_length = tf.compat.v1.scatter_update(\n",
            "          self._episode_lengths, [episode_location], new_length)\n",
            "      with tf.control_dependencies([update_length]):\n",
            "        return tf.identity(new_length)\n",
            "\n",
            "    def _assign_add_multiple():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1161,
        "API": ".control_dependencies(",
        "context": [
            "      new_length = tf.gather(self._episode_lengths, episode_location)\n",
            "      new_length += increment\n",
            "      update_length = tf.compat.v1.scatter_update(self._episode_lengths,\n",
            "                                                  episode_location, new_length)\n",
            "      with tf.control_dependencies([update_length]):\n",
            "        return tf.identity(new_length)\n",
            "\n",
            "    if episode_location.shape.rank == 0:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1174,
        "API": ".equal(",
        "context": [
            "\n",
            "  def _get_valid_ids_mask_locked(self, episode_ids):\n",
            "    \"\"\"Returns a mask of whether the given IDs are valid. Caller must lock.\"\"\"\n",
            "    episode_locations = self._get_episode_id_location(episode_ids)\n",
            "    location_matches_id = tf.equal(\n",
            "        episode_ids,\n",
            "        self._episodes_loc_to_id_map.sparse_read(episode_locations))\n",
            "    # Note that the above map is initialized with -1s.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1182,
        "API": ".device(",
        "context": [
            "    return (episode_ids >= 0) & location_matches_id\n",
            "\n",
            "  def get_valid_ids_mask(self, episode_ids):\n",
            "    \"\"\"Returns a mask of whether the given IDs are valid.\"\"\"\n",
            "    with tf.device(self._device):\n",
            "      with tf.name_scope('get_valid_ids_mask'):\n",
            "        return self._add_episode_critical_section.execute(\n",
            "            lambda: self._get_valid_ids_mask_locked(episode_ids))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1202,
        "API": ".cast(",
        "context": [
            "    Returns:\n",
            "      episodes: An Episodes object with an outer dimension of the same size as\n",
            "        locations.\n",
            "    \"\"\"\n",
            "    locations = tf.cast(locations, dtype=tf.int64, name='locations')\n",
            "    locations.shape.assert_has_rank(1)\n",
            "\n",
            "    def _extract_locked():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1212,
        "API": ".control_dependencies(",
        "context": [
            "          length=self._episode_lengths.sparse_read(locations),\n",
            "          completed=self._episode_completed.sparse_read(locations),\n",
            "          tensor_lists=self._data_table.get_episode_lists(locations))\n",
            "      if clear_data:\n",
            "        with tf.control_dependencies(tf.nest.flatten(episodes)):\n",
            "          clear_rows = self._data_table.clear_rows(locations)\n",
            "          clear_lengths = tf.compat.v1.scatter_update(self._episode_lengths,\n",
            "                                                      locations, 0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1218,
        "API": ".control_dependencies(",
        "context": [
            "          clear_lengths = tf.compat.v1.scatter_update(self._episode_lengths,\n",
            "                                                      locations, 0)\n",
            "          clear_completed = tf.compat.v1.scatter_update(self._episode_completed,\n",
            "                                                        locations, 0)\n",
            "        with tf.control_dependencies(\n",
            "            [clear_rows, clear_lengths, clear_completed]):\n",
            "          episodes = tf.nest.map_structure(tf.identity, episodes)\n",
            "      return episodes\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1223,
        "API": ".device(",
        "context": [
            "            [clear_rows, clear_lengths, clear_completed]):\n",
            "          episodes = tf.nest.map_structure(tf.identity, episodes)\n",
            "      return episodes\n",
            "\n",
            "    with tf.device(self._device):\n",
            "      with tf.name_scope('extract'):\n",
            "        return self._add_episode_critical_section.execute(_extract_locked)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1248,
        "API": ".convert_to_tensor(",
        "context": [
            "      A `Tensor` containing the updated episode ids.  Accessing or executing\n",
            "      this tensor also extends episodes in the replay buffer.\n",
            "    \"\"\"\n",
            "    episode_ids.shape.assert_has_rank(1)\n",
            "    episode_ids_indices = tf.convert_to_tensor(\n",
            "        value=episode_ids_indices, name='episode_ids_indices')\n",
            "    episode_ids_indices.shape.assert_has_rank(1)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1255,
        "API": ".equal(",
        "context": [
            "\n",
            "    def _extend_locked(episode_ids, expanded_episode_ids):\n",
            "      \"\"\"Does the above within the buffer's critical section.\"\"\"\n",
            "      episode_locations = self._get_episode_id_location(episode_ids)\n",
            "      episode_valid = tf.equal(\n",
            "          self._episodes_loc_to_id_map.sparse_read(episode_locations),\n",
            "          episode_ids)\n",
            "      episode_valid_idx = tf.reshape(tf.compat.v1.where(episode_valid), [-1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1262,
        "API": ".gather(",
        "context": [
            "      episode_valid_idx = tf.reshape(tf.compat.v1.where(episode_valid), [-1])\n",
            "      episode_locations = tf.gather(episode_locations, episode_valid_idx)\n",
            "      increment_lengths = self._increment_episode_length_locked(\n",
            "          episode_locations,\n",
            "          tf.gather(episodes.length, episode_valid_idx))\n",
            "      set_completed = tf.compat.v1.scatter_update(\n",
            "          self._episode_completed, episode_locations,\n",
            "          tf.gather(episodes.completed, episode_valid_idx))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1268,
        "API": ".gather(",
        "context": [
            "          self._episode_completed, episode_locations,\n",
            "          tf.gather(episodes.completed, episode_valid_idx))\n",
            "      extend = self._data_table.extend(\n",
            "          episode_locations,\n",
            "          tf.nest.map_structure(lambda tl: tf.gather(tl, episode_valid_idx),\n",
            "                                episodes.tensor_lists))\n",
            "      with tf.control_dependencies([increment_lengths, set_completed, extend]):\n",
            "        return tf.identity(expanded_episode_ids)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1273,
        "API": ".device(",
        "context": [
            "                                episodes.tensor_lists))\n",
            "      with tf.control_dependencies([increment_lengths, set_completed, extend]):\n",
            "        return tf.identity(expanded_episode_ids)\n",
            "\n",
            "    with tf.device(self._device):\n",
            "      with tf.name_scope('extend'):\n",
            "        episode_ids_indices_shape = tf.shape(episode_ids_indices)\n",
            "        begin_episode = self._begin_episode_fn(episodes)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1279,
        "API": ".reshape(",
        "context": [
            "        episode_ids_indices_shape = tf.shape(episode_ids_indices)\n",
            "        begin_episode = self._begin_episode_fn(episodes)\n",
            "        begin_episode = tf.broadcast_to(\n",
            "            begin_episode, episode_ids_indices_shape, name='begin_episode')\n",
            "        column_indices = tf.reshape(episode_ids_indices, [-1, 1])\n",
            "        episode_ids_shape = tf.shape(input=episode_ids)\n",
            "        # We expand the tensors below from size `num_episodes` (the size of\n",
            "        # episode_ids_indices) to tensors of  size `max_num_episodes` (the size\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1284,
        "API": ".scatter_nd(",
        "context": [
            "        episode_ids_shape = tf.shape(input=episode_ids)\n",
            "        # We expand the tensors below from size `num_episodes` (the size of\n",
            "        # episode_ids_indices) to tensors of  size `max_num_episodes` (the size\n",
            "        # of episode_ids).\n",
            "        expanded_begin_episode = tf.scatter_nd(column_indices, begin_episode,\n",
            "                                               episode_ids_shape)\n",
            "        expanded_mask = tf.scatter_nd(column_indices,\n",
            "                                      tf.fill(episode_ids_indices_shape, True),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1293,
        "API": ".gather(",
        "context": [
            "        expanded_episode_ids = self._get_batch_episode_ids(\n",
            "            episode_ids,\n",
            "            begin_episode=expanded_begin_episode,\n",
            "            mask=expanded_mask)\n",
            "        episode_ids = tf.gather(expanded_episode_ids, episode_ids_indices)\n",
            "        return self._add_episode_critical_section.execute(\n",
            "            lambda: _extend_locked(episode_ids, expanded_episode_ids))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1333,
        "API": ".format(",
        "context": [
            "        replay_buffer.data_spec, replay_buffer.capacity)\n",
            "\n",
            "    if not isinstance(replay_buffer, EpisodicReplayBuffer):\n",
            "      raise TypeError(\n",
            "          'Expected an EpisodicReplayBuffer, saw {}'.format(replay_buffer))\n",
            "    shape = ()\n",
            "    if num_episodes and num_episodes > 0:\n",
            "      if num_episodes > replay_buffer.capacity:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1338,
        "API": ".format(",
        "context": [
            "    shape = ()\n",
            "    if num_episodes and num_episodes > 0:\n",
            "      if num_episodes > replay_buffer.capacity:\n",
            "        raise ValueError('Buffer cannot create episode_ids when '\n",
            "                         'num_episodes {} > capacity {}.'.format(\n",
            "                             num_episodes, replay_buffer.capacity))\n",
            "      shape = (num_episodes,)\n",
            "    self._replay_buffer = replay_buffer\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1367,
        "API": ".assign(",
        "context": [
            "      this varaible also adds `items` to the replay buffer.\n",
            "    \"\"\"\n",
            "    new_episode_ids = self._replay_buffer.add_batch(\n",
            "        items=items, episode_ids=self._episode_ids_var)\n",
            "    self._episode_ids_var.assign(new_episode_ids)\n",
            "    return new_episode_ids\n",
            "\n",
            "  @common.function_in_tf1()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1386,
        "API": ".assign(",
        "context": [
            "      as a side effect, start or end the current episode.\n",
            "    \"\"\"\n",
            "    new_episode_id = self._replay_buffer.add_sequence(\n",
            "        items=items, episode_id=self._episode_ids_var)\n",
            "    self._episode_ids_var.assign(new_episode_id)\n",
            "    return new_episode_id\n",
            "\n",
            "  @common.function_in_tf1()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1411,
        "API": ".assign(",
        "context": [
            "    new_episode_ids = self._replay_buffer.extend_episodes(\n",
            "        episode_ids=self._episode_ids_var,\n",
            "        episode_ids_indices=episode_ids_indices,\n",
            "        episodes=episodes)\n",
            "    self._episode_ids_var.assign(new_episode_ids)\n",
            "    return new_episode_ids\n",
            "\n",
            "  def _get_next(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1439,
        "API": ".constant(",
        "context": [
            "\n",
            "  Returns:\n",
            "    A tuple (min_id, max_id) for the range [min_id, max_id) of valid ids.\n",
            "  \"\"\"\n",
            "  min_id_non_full = tf.constant(0, dtype=tf.int64)\n",
            "  max_id_non_full = tf.maximum(last_id + 1, 0)\n",
            "\n",
            "  min_id_full = tf.cast(last_id + 1 - capacity, dtype=tf.int64)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_replay_buffer.py",
        "line_number": 1446,
        "API": ".where(",
        "context": [
            "  min_id_full = tf.cast(last_id + 1 - capacity, dtype=tf.int64)\n",
            "  max_id_full = tf.cast(last_id + 1, dtype=tf.int64)\n",
            "\n",
            "  return (\n",
            "      tf.where(last_id < capacity, min_id_non_full, min_id_full),\n",
            "      tf.where(last_id < capacity, max_id_non_full, max_id_full))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_table.py",
        "line_number": 35,
        "API": ".convert_to_tensor(",
        "context": [
            "\n",
            "\n",
            "def _empty_slot(spec):\n",
            "  shape = [s if s is not None else -1 for s in spec.shape.as_list()]\n",
            "  shape = tf.convert_to_tensor(value=shape, dtype=tf.int64, name='shape')\n",
            "  return list_ops.empty_tensor_list(shape, spec.dtype)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_table.py",
        "line_number": 67,
        "API": ".map_structure(",
        "context": [
            "        return name_prefix + '.' + name\n",
            "      else:\n",
            "        return _create_unique_slot_name(spec, count + 1)\n",
            "\n",
            "    self._slots = tf.nest.map_structure(_create_unique_slot_name,\n",
            "                                        self._tensor_spec)\n",
            "    self._flattened_slots = tf.nest.flatten(self._slots)\n",
            "    self._flattened_specs = tf.nest.flatten(self._tensor_spec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_table.py",
        "line_number": 81,
        "API": ".map_structure(",
        "context": [
            "          deleted_key=-2,\n",
            "          name=slot_name,\n",
            "          default_value=_empty_slot(spec))\n",
            "\n",
            "    self._storage = tf.nest.map_structure(_create_storage, self._tensor_spec,\n",
            "                                          self._slots)\n",
            "    self._variables = tf.nest.flatten(self._storage)\n",
            "    self._slot2variable_map = dict(zip(self._flattened_slots, self._variables))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_table.py",
        "line_number": 126,
        "API": ".range(",
        "context": [
            "    Returns:\n",
            "      Episodes as TensorLists, stored in nested Tensors.\n",
            "    \"\"\"\n",
            "    if rows is None:\n",
            "      rows = tf.range(self._capacity, dtype=tf.int64)\n",
            "    else:\n",
            "      rows = tf.convert_to_tensor(value=rows, dtype=tf.int64)\n",
            "    values = [\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_table.py",
        "line_number": 146,
        "API": ".convert_to_tensor(",
        "context": [
            "\n",
            "    Returns:\n",
            "      Stacked values at given row.\n",
            "    \"\"\"\n",
            "    row = tf.convert_to_tensor(value=row, dtype=tf.int64)\n",
            "    row.shape.assert_has_rank(0)\n",
            "    return tf.nest.map_structure(self._stack_tensor_list, self.slots,\n",
            "                                 self.get_episode_lists(row))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_table.py",
        "line_number": 162,
        "API": ".convert_to_tensor(",
        "context": [
            "\n",
            "    Returns:\n",
            "      Ops for appending values at the given row.\n",
            "    \"\"\"\n",
            "    row = tf.convert_to_tensor(value=row, dtype=tf.int64)\n",
            "    flattened_values = tf.nest.flatten(values)\n",
            "    append_ops = []\n",
            "    for spec, slot, value in zip(self._flattened_specs, self._flattened_slots,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_table.py",
        "line_number": 169,
        "API": ".cast(",
        "context": [
            "    for spec, slot, value in zip(self._flattened_specs, self._flattened_slots,\n",
            "                                 flattened_values):\n",
            "      var_slot = self._slot2variable_map[slot].lookup(row)\n",
            "      value_as_tl = list_ops.tensor_list_from_tensor(\n",
            "          value, element_shape=tf.cast(spec.shape.as_list(), dtype=tf.int64))\n",
            "      new_value = list_ops.tensor_list_concat_lists(\n",
            "          var_slot, value_as_tl, element_dtype=spec.dtype)\n",
            "      append_ops.append(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_table.py",
        "line_number": 174,
        "API": ".group(",
        "context": [
            "      new_value = list_ops.tensor_list_concat_lists(\n",
            "          var_slot, value_as_tl, element_dtype=spec.dtype)\n",
            "      append_ops.append(\n",
            "          self._slot2variable_map[slot].insert_or_assign(row, new_value))\n",
            "    return tf.group(*append_ops)\n",
            "\n",
            "  def add(self, rows, values):\n",
            "    \"\"\"Returns ops for appending a single frame value to the given rows.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_table.py",
        "line_number": 191,
        "API": ".convert_to_tensor(",
        "context": [
            "\n",
            "    Returns:\n",
            "      Ops for appending values at rows.\n",
            "    \"\"\"\n",
            "    rows = tf.convert_to_tensor(value=rows, dtype=tf.int64)\n",
            "    flattened_values = tf.nest.flatten(values)\n",
            "    write_ops = []\n",
            "    for slot, value in zip(self._flattened_slots, flattened_values):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_table.py",
        "line_number": 199,
        "API": ".group(",
        "context": [
            "      var_slots = self._slot2variable_map[slot].lookup(rows)\n",
            "      new_value = list_ops.tensor_list_push_back_batch(var_slots, value)\n",
            "      write_ops.append(\n",
            "          self._slot2variable_map[slot].insert_or_assign(rows, new_value))\n",
            "    return tf.group(*write_ops)\n",
            "\n",
            "  def extend(self, rows, episode_lists):\n",
            "    \"\"\"Returns ops for extending a set of rows by the given TensorLists.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_table.py",
        "line_number": 214,
        "API": ".convert_to_tensor(",
        "context": [
            "      Ops for extending the table.\n",
            "    \"\"\"\n",
            "    nest_utils.assert_same_structure(self.slots, episode_lists)\n",
            "\n",
            "    rows = tf.convert_to_tensor(value=rows, dtype=tf.int64)\n",
            "    existing_lists = self.get_episode_lists(rows)\n",
            "    flat_existing_lists = tf.nest.flatten(existing_lists)\n",
            "    flat_episode_lists = tf.nest.flatten(episode_lists)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_table.py",
        "line_number": 228,
        "API": ".group(",
        "context": [
            "          existing_list, episode_list, element_dtype=spec.dtype)\n",
            "      slot_variable = self._slot2variable_map[slot]\n",
            "      write_ops.append(\n",
            "          slot_variable.insert_or_assign(rows, extended_list))\n",
            "    return tf.group(*write_ops)\n",
            "\n",
            "  def clear(self):\n",
            "    \"\"\"Returns op for clearing the table and removing all the episodes.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_table.py",
        "line_number": 240,
        "API": ".range(",
        "context": [
            "    clear_ops = []\n",
            "    for slot in self._flattened_slots:\n",
            "      clear_ops.append(\n",
            "          self._slot2variable_map[slot].erase(\n",
            "              tf.range(self._capacity, dtype=tf.int64)))\n",
            "    return tf.group(*clear_ops)\n",
            "\n",
            "  def clear_rows(self, rows):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_table.py",
        "line_number": 251,
        "API": ".convert_to_tensor(",
        "context": [
            "      rows: A list/tensor of location(s) to clear values.\n",
            "    Returns:\n",
            "      Ops for clearing the values at rows.\n",
            "    \"\"\"\n",
            "    rows = tf.convert_to_tensor(value=rows, dtype=tf.int64)\n",
            "    clear_ops = []\n",
            "    for spec, slot in zip(self._flattened_specs, self._flattened_slots):\n",
            "      new_value = tf.fill([tf.size(input=rows)], _empty_slot(spec))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\episodic_table.py",
        "line_number": 257,
        "API": ".group(",
        "context": [
            "    for spec, slot in zip(self._flattened_specs, self._flattened_slots):\n",
            "      new_value = tf.fill([tf.size(input=rows)], _empty_slot(spec))\n",
            "      clear_ops.append(\n",
            "          self._slot2variable_map[slot].insert_or_assign(rows, new_value))\n",
            "    return tf.group(*clear_ops)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\py_hashed_replay_buffer.py",
        "line_number": 77,
        "API": ".split(",
        "context": [
            "\n",
            "  def compress(self, observation, split_axis=-1):\n",
            "    # e.g. When split_axis is -1, turns an array of size 84x84x4\n",
            "    # into a list of arrays of size 84x84x1.\n",
            "    frame_list = np.split(observation, observation.shape[split_axis],\n",
            "                          split_axis)\n",
            "    return np.array([self.add_frame(f) for f in frame_list])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\py_hashed_replay_buffer.py",
        "line_number": 114,
        "API": ".format(",
        "context": [
            "\n",
            "  def __init__(self, data_spec, capacity, log_interval=None):\n",
            "    if not isinstance(data_spec, trajectory.Trajectory):\n",
            "      raise ValueError(\n",
            "          'data_spec must be the spec of a trajectory: {}'.format(data_spec))\n",
            "    super(PyHashedReplayBuffer, self).__init__(\n",
            "        data_spec, capacity)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\py_hashed_replay_buffer.py",
        "line_number": 146,
        "API": ".info(",
        "context": [
            "      observation = self._frame_buffer.compress(traj.observation)\n",
            "\n",
            "    if (self._log_interval and\n",
            "        self._np_state.item_count % self._log_interval == 0):\n",
            "      logging.info('%s', 'Effective Replay buffer frame count: {}'.format(\n",
            "          len(self._frame_buffer)))\n",
            "\n",
            "    return traj._replace(observation=observation)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\py_uniform_replay_buffer.py",
        "line_number": 102,
        "API": ".format(",
        "context": [
            "    outer_shape = nest_utils.get_outer_array_shape(items, self._data_spec)\n",
            "    if outer_shape[0] != 1:\n",
            "      raise NotImplementedError('PyUniformReplayBuffer only supports a batch '\n",
            "                                'size of 1, but received `items` with batch '\n",
            "                                'size {}.'.format(outer_shape[0]))\n",
            "\n",
            "    item = nest_utils.unbatch_nested_array(items)\n",
            "    with self._lock:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\py_uniform_replay_buffer.py",
        "line_number": 108,
        "API": ".get(",
        "context": [
            "    item = nest_utils.unbatch_nested_array(items)\n",
            "    with self._lock:\n",
            "      if self._np_state.size == self._capacity:\n",
            "        # If we are at capacity, we are deleting element cur_id.\n",
            "        self._on_delete(self._storage.get(self._np_state.cur_id))\n",
            "      self._storage.set(self._np_state.cur_id, self._encode(item))\n",
            "      self._np_state.size = np.minimum(self._np_state.size + 1,\n",
            "                                       self._capacity)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\py_uniform_replay_buffer.py",
        "line_number": 125,
        "API": ".empty(",
        "context": [
            "      \"\"\"Gets a single item from the replay buffer.\"\"\"\n",
            "      with self._lock:\n",
            "        if self._np_state.size <= 0:\n",
            "          def empty_item(spec):\n",
            "            return np.empty(spec.shape, dtype=spec.dtype)\n",
            "          if num_steps is not None:\n",
            "            item = [tf.nest.map_structure(empty_item, self.data_spec)\n",
            "                    for n in range(num_steps)]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\py_uniform_replay_buffer.py",
        "line_number": 132,
        "API": ".map_structure(",
        "context": [
            "                    for n in range(num_steps)]\n",
            "            if time_stacked:\n",
            "              item = nest_utils.stack_nested_arrays(item)\n",
            "          else:\n",
            "            item = tf.nest.map_structure(empty_item, self.data_spec)\n",
            "          return item\n",
            "        idx = np.random.randint(self._np_state.size - num_steps_value + 1)\n",
            "        if self._np_state.size == self._capacity:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\py_uniform_replay_buffer.py",
        "line_number": 144,
        "API": ".get(",
        "context": [
            "\n",
            "        if num_steps is not None:\n",
            "          # TODO(b/120242830): Try getting data from numpy in one shot rather\n",
            "          # than num_steps_value.\n",
            "          item = [self._decode(self._storage.get((idx + n) % self._capacity))\n",
            "                  for n in range(num_steps)]\n",
            "        else:\n",
            "          item = self._decode(self._storage.get(idx % self._capacity))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\py_uniform_replay_buffer.py",
        "line_number": 173,
        "API": ".flatten(",
        "context": [
            "      data_spec = array_spec.add_outer_dims_nest(\n",
            "          data_spec, (sample_batch_size,))\n",
            "    if num_steps is not None:\n",
            "      data_spec = (data_spec,) * num_steps\n",
            "    shapes = tuple(s.shape for s in tf.nest.flatten(data_spec))\n",
            "    dtypes = tuple(s.dtype for s in tf.nest.flatten(data_spec))\n",
            "\n",
            "    def generator_fn():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\py_uniform_replay_buffer.py",
        "line_number": 184,
        "API": ".flatten(",
        "context": [
            "                   for _ in range(sample_batch_size)]\n",
            "          item = nest_utils.stack_nested_arrays(batch)\n",
            "        else:\n",
            "          item = self._get_next(num_steps=num_steps, time_stacked=False)\n",
            "        yield tuple(tf.nest.flatten(item))\n",
            "\n",
            "    def time_stack(*structures):\n",
            "      time_axis = 0 if sample_batch_size is None else 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\py_uniform_replay_buffer.py",
        "line_number": 189,
        "API": ".stack(",
        "context": [
            "\n",
            "    def time_stack(*structures):\n",
            "      time_axis = 0 if sample_batch_size is None else 1\n",
            "      return tf.nest.map_structure(\n",
            "          lambda *elements: tf.stack(elements, axis=time_axis), *structures)\n",
            "\n",
            "    ds = tf.data.Dataset.from_generator(\n",
            "        generator_fn, dtypes,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\py_uniform_replay_buffer.py",
        "line_number": 200,
        "API": ".get(",
        "context": [
            "    else:\n",
            "      return ds\n",
            "\n",
            "  def _gather_all(self):\n",
            "    data = [self._decode(self._storage.get(idx))\n",
            "            for idx in range(self._capacity)]\n",
            "    stacked = nest_utils.stack_nested_arrays(data)\n",
            "    batched = tf.nest.map_structure(lambda t: np.expand_dims(t, 0), stacked)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\replay_buffer.py",
        "line_number": 212,
        "API": ".flatten(",
        "context": [
            "      ValueError: If the data spec contains lists that must be converted to\n",
            "        tuples.\n",
            "    \"\"\"\n",
            "    # data_tf.nest.flatten does not flatten python lists, nest.flatten does.\n",
            "    if tf.nest.flatten(self._data_spec) != data_nest.flatten(self._data_spec):\n",
            "      raise ValueError(\n",
            "          'Cannot perform gather; data spec contains lists and this conflicts '\n",
            "          'with gathering operator.  Convert any lists to tuples.  '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\replay_buffer.py",
        "line_number": 217,
        "API": ".format(",
        "context": [
            "      raise ValueError(\n",
            "          'Cannot perform gather; data spec contains lists and this conflicts '\n",
            "          'with gathering operator.  Convert any lists to tuples.  '\n",
            "          'For example, if your spec looks like [a, b, c], '\n",
            "          'change it to (a, b, c).  Spec structure is:\\n  {}'.format(\n",
            "              tf.nest.map_structure(lambda spec: spec.dtype, self._data_spec)))\n",
            "\n",
            "    if single_deterministic_pass:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_replay_buffer.py",
        "line_number": 126,
        "API": ".format(",
        "context": [
            "    self._max_samples_per_stream = max_samples_per_stream\n",
            "    self._rate_limiter_timeout_ms = rate_limiter_timeout_ms\n",
            "\n",
            "    if local_server:\n",
            "      self._server_address = 'localhost:{}'.format(local_server.port)\n",
            "\n",
            "    self._py_client = reverb.Client(self._server_address)\n",
            "    self._tf_client = reverb.TFClient(self._server_address, 'rb_tf_client')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_replay_buffer.py",
        "line_number": 260,
        "API": ".format(",
        "context": [
            "    if (num_parallel_calls and sample_batch_size\n",
            "        and num_parallel_calls > sample_batch_size):\n",
            "      raise ValueError(\n",
            "          'num_parallel_calls cannot be bigger than sample_batch_size '\n",
            "          '{} > {}'.format(num_parallel_calls, sample_batch_size))\n",
            "    num_parallel_calls = num_parallel_calls or tf.data.experimental.AUTOTUNE\n",
            "    total_batch_size = sample_batch_size or 1\n",
            "    # This determines how many parallel Reverb dataset pipelines we create -\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_replay_buffer.py",
        "line_number": 360,
        "API": ".format(",
        "context": [
            "    if not self._deterministic_table:\n",
            "      raise ValueError(\n",
            "          'Unable to perform a single deterministic pass over the dataset, '\n",
            "          'since either the sampler or the remover is not deterministic '\n",
            "          '(FIFO or Heap).  Table info:\\n{}'.format(self._table_info))\n",
            "    self._verify_num_steps(num_steps)\n",
            "\n",
            "    def per_sequence_fn(sample):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_replay_buffer.py",
        "line_number": 431,
        "API": ".format(",
        "context": [
            "        raise ValueError(\n",
            "            'Can not guarantee sequential data for num_steps as sequence '\n",
            "            'length of the data is smaller.  This is not supported.  '\n",
            "            'num_steps > sequence_length ({} vs. {})'\n",
            "            .format(num_steps, self._sequence_length))\n",
            "      if self._sequence_length % num_steps != 0:\n",
            "        raise ValueError(\n",
            "            'Can not guarantee sequential data since sequence_length is not a '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_replay_buffer.py",
        "line_number": 436,
        "API": ".format(",
        "context": [
            "      if self._sequence_length % num_steps != 0:\n",
            "        raise ValueError(\n",
            "            'Can not guarantee sequential data since sequence_length is not a '\n",
            "            'multiple of num_steps ({} vs. {})'\n",
            "            .format(num_steps, self._sequence_length))\n",
            "\n",
            "\n",
            "def make_reverb_dataset(server_address: str,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_replay_buffer.py",
        "line_number": 492,
        "API": ".as_dtype(",
        "context": [
            "  Returns:\n",
            "    A tf.data.Dataset that streams data from the replay server.\n",
            "  \"\"\"\n",
            "  # Extract the shapes and dtypes from these specs.\n",
            "  get_dtype = lambda x: tf.as_dtype(x.dtype)\n",
            "  get_shape = lambda x: (sequence_length,) + x.shape\n",
            "  shapes = tf.nest.map_structure(get_shape, data_spec)\n",
            "  dtypes = tf.nest.map_structure(get_dtype, data_spec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_replay_buffer.py",
        "line_number": 516,
        "API": ".flatten(",
        "context": [
            "    ) -> types.ReverbReplaySample:\n",
            "      # Assumes that the first element of traj is shaped\n",
            "      # (sequence_length, ...); and we extract this length.\n",
            "      info, traj = info_traj\n",
            "      first_elem = tf.nest.flatten(traj)[0]\n",
            "      length = first_elem.shape[0] or tf.shape(first_elem)[0]\n",
            "      info = tf.nest.map_structure(lambda t: tf.repeat(t, [length]), info)\n",
            "      return reverb.ReplaySample(info, traj)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_replay_buffer.py",
        "line_number": 535,
        "API": ".range(",
        "context": [
            "\n",
            "  if cycle_length == 1:\n",
            "    dataset = generate_reverb_dataset(0)\n",
            "  else:\n",
            "    dataset = tf.data.Dataset.range(cycle_length).interleave(\n",
            "        generate_reverb_dataset,\n",
            "        cycle_length=cycle_length,\n",
            "        num_parallel_calls=num_parallel_calls)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_replay_buffer.py",
        "line_number": 548,
        "API": ".batch(",
        "context": [
            "  options.experimental_deterministic = False\n",
            "  dataset = dataset.with_options(options)\n",
            "\n",
            "  if batch_size:\n",
            "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
            "  if prefetch_size:\n",
            "    dataset = dataset.prefetch(prefetch_size)\n",
            "  return dataset\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_replay_buffer.py",
        "line_number": 561,
        "API": ".shape(",
        "context": [
            "  This function takes a structure `sample` and for each tensor `t`, it truncates\n",
            "  the tensor's outer dimension to be the highest possible multiple of\n",
            "  `num_steps`.\n",
            "\n",
            "  This is done by first calculating `rows = tf.shape(t[0]) // num_steps`, then\n",
            "  truncating the `tensor` to shape `t_trunc = t[: (rows * num_steps), ...]`.\n",
            "  For each tensor, it returns `tf.reshape(t_trunc, [rows, num_steps, ...])`.\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_replay_buffer.py",
        "line_number": 572,
        "API": ".flatten(",
        "context": [
            "\n",
            "  Returns:\n",
            "    A next with tensors reshaped to `[rows, num_steps, ...]`.\n",
            "  \"\"\"\n",
            "  first_elem = tf.nest.flatten(sample)[0]\n",
            "  static_sequence_length = tf.compat.dimension_value(first_elem.shape[0])\n",
            "  if static_sequence_length is not None:\n",
            "    num_rows = static_sequence_length // num_steps\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_replay_buffer.py",
        "line_number": 578,
        "API": ".shape(",
        "context": [
            "  if static_sequence_length is not None:\n",
            "    num_rows = static_sequence_length // num_steps\n",
            "    static_num_rows = num_rows\n",
            "  else:\n",
            "    num_rows = tf.shape(first_elem)[0] // num_steps\n",
            "    static_num_rows = None\n",
            "  def _truncate_and_reshape(t):\n",
            "    truncated = t[:(num_rows * num_steps), ...]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_replay_buffer.py",
        "line_number": 584,
        "API": ".shape(",
        "context": [
            "  def _truncate_and_reshape(t):\n",
            "    truncated = t[:(num_rows * num_steps), ...]\n",
            "    reshaped = tf.reshape(\n",
            "        truncated,\n",
            "        tf.concat(([num_rows, num_steps], tf.shape(t)[1:]), axis=0))\n",
            "    reshaped.set_shape([static_num_rows, num_steps] + t.shape[1:])\n",
            "    return reshaped\n",
            "  return tf.nest.map_structure(_truncate_and_reshape, sample)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_utils.py",
        "line_number": 158,
        "API": ".error(",
        "context": [
            "    if (self._cached_steps >= self._max_sequence_length and\n",
            "        not self._overflow_episode):\n",
            "      self._overflow_episode = True\n",
            "      if self._bypass_partial_episodes:\n",
            "        logging.error(\n",
            "            \"The number of trajectories within the same episode exceeds \"\n",
            "            \"`max_sequence_length`. This episode is bypassed and will NOT \"\n",
            "            \"be written into the replay buffer. Consider increasing the \"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_utils.py",
        "line_number": 194,
        "API": ".map_structure(",
        "context": [
            "    \"\"\"\n",
            "    # Only writes to Reverb when the writer has cached trajectories.\n",
            "    if self._writer_has_data:\n",
            "      # No need to truncate since the truncation is done in the class.\n",
            "      trajectory = tf.nest.map_structure(lambda h: h[:], self._writer.history)\n",
            "      for table_name in self._table_names:\n",
            "        self._writer.create_item(\n",
            "            table=table_name,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_utils.py",
        "line_number": 202,
        "API": ".info(",
        "context": [
            "            trajectory=trajectory,\n",
            "            priority=self._priority)\n",
            "      self._writer_has_data = False\n",
            "    else:\n",
            "      logging.info(\"Skipped writing to Reverb because the writer is empty.\")\n",
            "\n",
            "  def flush(self):\n",
            "    \"\"\"Ensures that items are pushed to the service.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_utils.py",
        "line_number": 212,
        "API": ".flush(",
        "context": [
            "    needed when `rate_limiter_timeout_ms` is set for the replay buffer.\n",
            "    By calling this method before the `learner.run()`, we ensure that there is\n",
            "    enough data to be consumed.\n",
            "    \"\"\"\n",
            "    self._writer.flush()\n",
            "\n",
            "  def reset(self,\n",
            "            write_cached_steps: bool = True) -> None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_utils.py",
        "line_number": 394,
        "API": ".map_structure(",
        "context": [
            "    **Note**: The method does *not* clear the cache after writing.\n",
            "    \"\"\"\n",
            "\n",
            "    if self._sequence_lengths_reached():\n",
            "      trajectory = tf.nest.map_structure(\n",
            "          lambda d: d[-self._sequence_length:], self._writer.history)\n",
            "      for table_name in self._table_names:\n",
            "        self._writer.create_item(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_utils.py",
        "line_number": 412,
        "API": ".flush(",
        "context": [
            "    needed when `rate_limiter_timeout_ms` is set for the replay buffer.\n",
            "    By calling this method before the `learner.run()`, we ensure that there is\n",
            "    enough data to be consumed.\n",
            "    \"\"\"\n",
            "    self._writer.flush()\n",
            "\n",
            "  def _get_padding_step(\n",
            "      self, example_trajectory: trajectory_lib.Trajectory\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_utils.py",
        "line_number": 419,
        "API": ".map_structure(",
        "context": [
            "      self, example_trajectory: trajectory_lib.Trajectory\n",
            "  ) -> trajectory_lib.Trajectory:\n",
            "    \"\"\"Get the padding step to append to the cache.\"\"\"\n",
            "    zero_step = trajectory_lib.boundary(\n",
            "        tf.nest.map_structure(tf.zeros_like, example_trajectory.observation),\n",
            "        tf.nest.map_structure(tf.zeros_like, example_trajectory.action),\n",
            "        tf.nest.map_structure(tf.zeros_like, example_trajectory.policy_info),\n",
            "        tf.nest.map_structure(tf.zeros_like, example_trajectory.reward),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\reverb_utils.py",
        "line_number": 464,
        "API": ".format(",
        "context": [
            "      else:\n",
            "        raise ValueError(\n",
            "            \"write_cached_steps is True, but not enough steps remain in the \"\n",
            "            \"cache to write an item with sequence_length={}, consider enabling \"\n",
            "            \"pad_end_of_episodes.\".format(self._sequence_length))\n",
            "\n",
            "    self._cached_steps = 0\n",
            "    self._last_trajectory = None\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\rlds_to_reverb.py",
        "line_number": 201,
        "API": ".constant(",
        "context": [
            "    return step[rlds.IS_LAST] and step[rlds.IS_TERMINAL]\n",
            "\n",
            "  def _get_discount(step: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
            "    \"\"\"Returns 0 for complete episode, else returns the current discount.\"\"\"\n",
            "    return tf.constant(\n",
            "        0.0, dtype=tf.float32) if _is_complete(step) else step[rlds.DISCOUNT]\n",
            "\n",
            "  def _validate_step(current_step: Dict[str, tf.Tensor],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\rlds_to_reverb.py",
        "line_number": 263,
        "API": ".batch(",
        "context": [
            "\n",
            "  # Batch the RLDS data as pair of adjacent steps with an overlap of one step\n",
            "  # and then create tuples of those batches to finally to create TF Agents\n",
            "  # trajectories with correct next step type.\n",
            "  return rlds.transformations.batch(\n",
            "      dataset=rlds_data, size=2, shift=1,\n",
            "      drop_remainder=True).map(_pair_to_tuple).map(_to_trajectory)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\rlds_to_reverb.py",
        "line_number": 319,
        "API": ".info(",
        "context": [
            "  steps = 0\n",
            "  for entry in convert_rlds_to_trajectories(rlds_data, policy_info_fn):\n",
            "    reverb_observer(entry)\n",
            "    steps += 1\n",
            "  logging.info('Successfully wrote %d steps to Reverb.', steps)\n",
            "  return steps\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\table.py",
        "line_number": 53,
        "API": ".map_structure(",
        "context": [
            "\n",
            "    def _create_unique_slot_name(spec):\n",
            "      return tf.compat.v1.get_default_graph().unique_name(spec.name or 'slot')\n",
            "\n",
            "    self._slots = tf.nest.map_structure(_create_unique_slot_name,\n",
            "                                        self._tensor_spec)\n",
            "\n",
            "    def _create_storage(spec, slot_name):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\table.py",
        "line_number": 61,
        "API": ".zeros(",
        "context": [
            "      \"\"\"Create storage for a slot, track it.\"\"\"\n",
            "      shape = [self._capacity] + spec.shape.as_list()\n",
            "      new_storage = common.create_variable(\n",
            "          name=slot_name,\n",
            "          initializer=tf.zeros(shape, dtype=spec.dtype),\n",
            "          shape=None,\n",
            "          dtype=spec.dtype,\n",
            "          unique_name=False)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\table.py",
        "line_number": 68,
        "API": ".map_structure(",
        "context": [
            "          unique_name=False)\n",
            "      return new_storage\n",
            "\n",
            "    with tf.compat.v1.variable_scope(scope):\n",
            "      self._storage = tf.nest.map_structure(_create_storage, self._tensor_spec,\n",
            "                                            self._slots)\n",
            "\n",
            "    self._slot2storage_map = dict(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\table.py",
        "line_number": 79,
        "API": ".flatten(",
        "context": [
            "  def slots(self):\n",
            "    return self._slots\n",
            "\n",
            "  def variables(self):\n",
            "    return tf.nest.flatten(self._storage)\n",
            "\n",
            "  def read(self, rows, slots=None):\n",
            "    \"\"\"Returns values for the given rows.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\table.py",
        "line_number": 100,
        "API": ".flatten(",
        "context": [
            "    Returns:\n",
            "      Values at given rows.\n",
            "    \"\"\"\n",
            "    slots = slots or self._slots\n",
            "    flattened_slots = tf.nest.flatten(slots)\n",
            "    values = [\n",
            "        self._slot2storage_map[slot].sparse_read(rows)\n",
            "        for slot in flattened_slots\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\table.py",
        "line_number": 125,
        "API": ".flatten(",
        "context": [
            "    Returns:\n",
            "      Ops for writing values at rows.\n",
            "    \"\"\"\n",
            "    slots = slots or self._slots\n",
            "    flattened_slots = tf.nest.flatten(slots)\n",
            "    flattened_values = tf.nest.flatten(values)\n",
            "    write_ops = [\n",
            "        tf.compat.v1.scatter_update(self._slot2storage_map[slot], rows,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\table.py",
        "line_number": 132,
        "API": ".group(",
        "context": [
            "        tf.compat.v1.scatter_update(self._slot2storage_map[slot], rows,\n",
            "                                    value).op\n",
            "        for (slot, value) in zip(flattened_slots, flattened_values)\n",
            "    ]\n",
            "    return tf.group(*write_ops)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 143,
        "API": ".range(",
        "context": [
            "\n",
            "    self._id_spec = tensor_spec.TensorSpec([], dtype=tf.int64, name='id')\n",
            "    self._capacity_value = np.int64(self._capacity)\n",
            "    self._batch_offsets = (\n",
            "        tf.range(self._batch_size, dtype=tf.int64) * self._max_length)\n",
            "    self._scope = scope\n",
            "    self._device = device\n",
            "    self._table_fn = table_fn\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 149,
        "API": ".device(",
        "context": [
            "    self._device = device\n",
            "    self._table_fn = table_fn\n",
            "    self._dataset_drop_remainder = dataset_drop_remainder\n",
            "    self._dataset_window_shift = dataset_window_shift\n",
            "    with tf.device(self._device), tf.compat.v1.variable_scope(self._scope):\n",
            "      self._capacity = tf.constant(capacity, dtype=tf.int64)\n",
            "      self._data_table = table_fn(self._data_spec, self._capacity_value)\n",
            "      self._id_table = table_fn(self._id_spec, self._capacity_value)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 178,
        "API": ".minimum(",
        "context": [
            "\n",
            "  def _num_frames(self):\n",
            "    num_items_single_batch_segment = self._get_last_id() + 1\n",
            "    total_frames = num_items_single_batch_segment * self._batch_size\n",
            "    return tf.minimum(total_frames, self._capacity)\n",
            "\n",
            "  def _add_batch(self, items):\n",
            "    \"\"\"Adds a batch of items to the replay buffer.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 197,
        "API": ".map_structure(",
        "context": [
            "    # Calling get_outer_rank here will validate that all items have the same\n",
            "    # outer rank. This was not usually an issue, but now that it's easier to\n",
            "    # call this from an eager context it's easy to make the mistake.\n",
            "    nest_utils.get_outer_rank(\n",
            "        tf.nest.map_structure(tf.convert_to_tensor, items),\n",
            "        self._data_spec)\n",
            "\n",
            "    with tf.device(self._device), tf.name_scope(self._scope):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 203,
        "API": ".write(",
        "context": [
            "\n",
            "    with tf.device(self._device), tf.name_scope(self._scope):\n",
            "      id_ = self._increment_last_id()\n",
            "      write_rows = self._get_rows_for_id(id_)\n",
            "      write_id_op = self._id_table.write(write_rows, id_)\n",
            "      write_data_op = self._data_table.write(write_rows, items)\n",
            "      return tf.group(write_id_op, write_data_op)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 236,
        "API": ".device(",
        "context": [
            "          - The sampling probability of each item.\n",
            "    Raises:\n",
            "      ValueError: if num_steps is bigger than the capacity.\n",
            "    \"\"\"\n",
            "    with tf.device(self._device), tf.name_scope(self._scope):\n",
            "      with tf.name_scope('get_next'):\n",
            "        min_val, max_val = _valid_range_ids(\n",
            "            self._get_last_id(), self._max_length, num_steps)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 246,
        "API": ".control_dependencies(",
        "context": [
            "            max_val,\n",
            "            min_val,\n",
            "            message='TFUniformReplayBuffer is empty. Make sure to add items '\n",
            "            'before sampling the buffer.')\n",
            "        with tf.control_dependencies([assert_nonempty]):\n",
            "          num_ids = max_val - min_val\n",
            "          probability = tf.cond(\n",
            "              pred=tf.equal(num_ids, 0),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 251,
        "API": ".cast(",
        "context": [
            "          num_ids = max_val - min_val\n",
            "          probability = tf.cond(\n",
            "              pred=tf.equal(num_ids, 0),\n",
            "              true_fn=lambda: 0.,\n",
            "              false_fn=lambda: 1. / tf.cast(num_ids * self._batch_size,  # pylint: disable=g-long-lambda\n",
            "                                            tf.float32))\n",
            "          ids = tf.random.uniform(\n",
            "              rows_shape, minval=min_val, maxval=max_val, dtype=tf.int64)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 257,
        "API": ".uniform(",
        "context": [
            "          ids = tf.random.uniform(\n",
            "              rows_shape, minval=min_val, maxval=max_val, dtype=tf.int64)\n",
            "\n",
            "        # Move each id sample to a random batch.\n",
            "        batch_offsets = tf.random.uniform(\n",
            "            rows_shape, minval=0, maxval=self._batch_size, dtype=tf.int64)\n",
            "        batch_offsets *= self._max_length\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 267,
        "API": ".range(",
        "context": [
            "          data = self._data_table.read(rows_to_get)\n",
            "          data_ids = self._id_table.read(rows_to_get)\n",
            "        else:\n",
            "          if time_stacked:\n",
            "            step_range = tf.range(num_steps, dtype=tf.int64)\n",
            "            if sample_batch_size:\n",
            "              step_range = tf.reshape(step_range, [1, num_steps])\n",
            "              step_range = tf.tile(step_range, [sample_batch_size, 1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 274,
        "API": ".reshape(",
        "context": [
            "              step_range = tf.tile(step_range, [sample_batch_size, 1])\n",
            "              ids = tf.tile(tf.expand_dims(ids, -1), [1, num_steps])\n",
            "              batch_offsets = batch_offsets[:, None]\n",
            "            else:\n",
            "              step_range = tf.reshape(step_range, [num_steps])\n",
            "\n",
            "            rows_to_get = tf.math.mod(step_range + ids,\n",
            "                                      self._max_length) + batch_offsets\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 291,
        "API": ".fill(",
        "context": [
            "              data.append(items)\n",
            "              data_ids.append(self._id_table.read(steps_to_get))\n",
            "            data = tuple(data)\n",
            "            data_ids = tuple(data_ids)\n",
            "        probabilities = tf.fill(rows_shape, probability)\n",
            "\n",
            "        buffer_info = BufferInfo(ids=data_ids,\n",
            "                                 probabilities=probabilities)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 389,
        "API": ".format(",
        "context": [
            "      raise ValueError(\n",
            "          'sample_batch_size ({}) > self.batch_size ({}) and '\n",
            "          'dataset_drop_remainder is True.  In '\n",
            "          'this case, ALL data will be dropped by the deterministic dataset.'\n",
            "          .format(static_size, static_self_batch_size))\n",
            "    if (self._dataset_drop_remainder\n",
            "        and static_num_steps is not None\n",
            "        and static_self_max_length is not None\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 398,
        "API": ".format(",
        "context": [
            "      raise ValueError(\n",
            "          'num_steps_size ({}) > self.max_length ({}) and '\n",
            "          'dataset_drop_remainder is True.  In '\n",
            "          'this case, ALL data will be dropped by the deterministic dataset.'\n",
            "          .format(static_num_steps, static_self_max_length))\n",
            "\n",
            "    def get_row_ids(_):\n",
            "      \"\"\"Passed to Dataset.range(self._batch_size).flat_map(.), gets row ids.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 418,
        "API": ".range(",
        "context": [
            "              max_frame_offset,\n",
            "              message='TFUniformReplayBuffer is empty. Make sure to add items '\n",
            "              'before asking the buffer for data.')\n",
            "\n",
            "          min_max_frame_range = tf.range(min_frame_offset, max_frame_offset)\n",
            "\n",
            "          window_shift = self._dataset_window_shift\n",
            "          def group_windows(ds_, drop_remainder=self._dataset_drop_remainder):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 434,
        "API": ".range(",
        "context": [
            "              if num_steps is not None:\n",
            "                ids = (ids.window(num_steps, shift=window_shift)\n",
            "                       .flat_map(group_windows))\n",
            "              return ids\n",
            "            return tf.data.Dataset.range(self._batch_size).flat_map(row_ids)\n",
            "          else:\n",
            "            def batched_row_ids(batch):\n",
            "              # Create a matrix of indices shaped [num_frames, batch_size]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 445,
        "API": ".range(",
        "context": [
            "                  (min_max_frame_range[:, tf.newaxis]\n",
            "                   + batch * self._max_length))\n",
            "\n",
            "            indices_ds = (\n",
            "                tf.data.Dataset.range(self._batch_size)\n",
            "                .batch(sample_batch_size,\n",
            "                       drop_remainder=self._dataset_drop_remainder)\n",
            "                .flat_map(batched_row_ids))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 467,
        "API": ".range(",
        "context": [
            "            return indices_ds\n",
            "\n",
            "    # Get our indices as a dataset; each time we reinitialize the iterator we\n",
            "    # update our min/max id bounds from the state of the replay buffer.\n",
            "    ds = tf.data.Dataset.range(1).flat_map(get_row_ids)\n",
            "\n",
            "    def get_data(id_):\n",
            "      with tf.device(self._device), tf.name_scope(self._scope):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 489,
        "API": ".device(",
        "context": [
            "\n",
            "    Returns:\n",
            "      All the items currently in the buffer.\n",
            "    \"\"\"\n",
            "    with tf.device(self._device), tf.name_scope(self._scope):\n",
            "      with tf.name_scope('gather_all'):\n",
            "        # Make ids, repeated over batch_size. Shape [batch_size, num_ids, ...].\n",
            "        min_val, max_val = _valid_range_ids(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 494,
        "API": ".range(",
        "context": [
            "      with tf.name_scope('gather_all'):\n",
            "        # Make ids, repeated over batch_size. Shape [batch_size, num_ids, ...].\n",
            "        min_val, max_val = _valid_range_ids(\n",
            "            self._get_last_id(), self._max_length)\n",
            "        ids = tf.range(min_val, max_val)\n",
            "        ids = tf.stack([ids] * self._batch_size)\n",
            "        rows = tf.math.mod(ids, self._max_length)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 499,
        "API": ".expand_dims(",
        "context": [
            "        ids = tf.stack([ids] * self._batch_size)\n",
            "        rows = tf.math.mod(ids, self._max_length)\n",
            "\n",
            "        # Make batch_offsets, shape [batch_size, 1], then add to rows.\n",
            "        batch_offsets = tf.expand_dims(\n",
            "            tf.range(self._batch_size, dtype=tf.int64) * self._max_length,\n",
            "            1)\n",
            "        rows += batch_offsets\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 523,
        "API": ".assign(",
        "context": [
            "      op that clears or unlinks the replay buffer contents.\n",
            "    \"\"\"\n",
            "    table_vars = self._data_table.variables() + self._id_table.variables()\n",
            "    def _init_vars():\n",
            "      assignments = [self._last_id.assign(-1)]\n",
            "      if clear_all_variables:\n",
            "        assignments += [v.assign(tf.zeros_like(v)) for v in table_vars]\n",
            "      return tf.group(*assignments, name='clear')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 539,
        "API": ".value(",
        "context": [
            "    Returns:\n",
            "      An op that increments the last_id.\n",
            "    \"\"\"\n",
            "    def _assign_add():\n",
            "      return self._last_id.assign_add(increment).value()\n",
            "    return self._last_id_cs.execute(_assign_add)\n",
            "\n",
            "  def _get_last_id(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 545,
        "API": ".value(",
        "context": [
            "\n",
            "  def _get_last_id(self):\n",
            "\n",
            "    def last_id():\n",
            "      return self._last_id.value()\n",
            "\n",
            "    return self._last_id_cs.execute(last_id)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 569,
        "API": ".constant(",
        "context": [
            "  Returns:\n",
            "    A tuple (min_id, max_id) for the range [min_id, max_id) of valid ids.\n",
            "  \"\"\"\n",
            "  if num_steps is None:\n",
            "    num_steps = tf.constant(1, tf.int64)\n",
            "\n",
            "  min_id_not_full = tf.constant(0, dtype=tf.int64)\n",
            "  max_id_not_full = tf.maximum(last_id + 1 - num_steps + 1, 0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py",
        "line_number": 577,
        "API": ".where(",
        "context": [
            "\n",
            "  min_id_full = last_id + 1 - max_length\n",
            "  max_id_full = last_id + 1 - num_steps + 1\n",
            "\n",
            "  return (tf.where(last_id < max_length, min_id_not_full, min_id_full),\n",
            "          tf.where(last_id < max_length, max_id_not_full, max_id_full))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\array_spec.py",
        "line_number": 37,
        "API": ".as_dtype(",
        "context": [
            "\n",
            "  Returns:\n",
            "    An np.array sample of the requested spec.\n",
            "  \"\"\"\n",
            "  tf_dtype = tf.as_dtype(spec.dtype)\n",
            "  low = spec.minimum\n",
            "  high = spec.maximum\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\array_spec.py",
        "line_number": 42,
        "API": ".any(",
        "context": [
            "  low = spec.minimum\n",
            "  high = spec.maximum\n",
            "\n",
            "  if tf_dtype.is_floating:\n",
            "    if spec.dtype == np.float64 and np.any(np.isinf(high - low)):\n",
            "      # The min-max interval cannot be represented by the np.float64. This is a\n",
            "      # problem only for np.float64, np.float32 works as expected.\n",
            "      # Spec bounds are set to read only so we can't use argumented assignment.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\array_spec.py",
        "line_number": 48,
        "API": ".uniform(",
        "context": [
            "      # problem only for np.float64, np.float32 works as expected.\n",
            "      # Spec bounds are set to read only so we can't use argumented assignment.\n",
            "      low = low / 2\n",
            "      high = high / 2\n",
            "    return rng.uniform(\n",
            "        low,\n",
            "        high,\n",
            "        size=spec.shape,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\array_spec.py",
        "line_number": 55,
        "API": ".any(",
        "context": [
            "        size=spec.shape,\n",
            "    ).astype(spec.dtype)\n",
            "\n",
            "  else:\n",
            "    if spec.dtype == np.int64 and np.any(high - low < 0):\n",
            "      # The min-max interval cannot be represented by the tf_dtype. This is a\n",
            "      # problem only for int64.\n",
            "      low = low / 2\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\array_spec.py",
        "line_number": 61,
        "API": ".any(",
        "context": [
            "      # problem only for int64.\n",
            "      low = low / 2\n",
            "      high = high / 2\n",
            "\n",
            "    if np.any(high < tf_dtype.max):\n",
            "      high = np.where(high < tf_dtype.max, high + 1, high)\n",
            "    elif spec.dtype != np.int64 or spec.dtype != np.uint64:\n",
            "      # We can still +1 the high if we cast it to the larger dtype.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\array_spec.py",
        "line_number": 75,
        "API": ".reshape(",
        "context": [
            "          size=spec.shape,\n",
            "          dtype=spec.dtype,\n",
            "      )\n",
            "    else:\n",
            "      return np.reshape(\n",
            "          np.array([\n",
            "              rng.randint(low, high, size=1, dtype=spec.dtype)\n",
            "              for low, high in zip(low.flatten(), high.flatten())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\array_spec.py",
        "line_number": 102,
        "API": ".map_structure(",
        "context": [
            "        tuple(outer_dims) + tuple(spec.shape), spec.dtype, spec.minimum,\n",
            "        spec.maximum, spec.name)\n",
            "    return sample_bounded_spec(spec, rng)\n",
            "\n",
            "  return tf.nest.map_structure(sample_fn, structure)\n",
            "\n",
            "\n",
            "def check_arrays_nest(arrays, spec):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\array_spec.py",
        "line_number": 143,
        "API": ".map_structure(",
        "context": [
            "    if not spec.check_array(array):\n",
            "      raise ValueError(f'The value \"{array}\" does not match spec: {spec}')\n",
            "\n",
            "  # Check all the elements in arrays match to their spec\n",
            "  tf.nest.map_structure(assert_array_spec, arrays, spec)\n",
            "\n",
            "\n",
            "def add_outer_dims_nest(structure, outer_dims):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\array_spec.py",
        "line_number": 155,
        "API": ".map_structure(",
        "context": [
            "      return BoundedArraySpec(shape, spec.dtype, spec.minimum,\n",
            "                              spec.maximum, name)\n",
            "    return ArraySpec(shape, spec.dtype, name=name)\n",
            "\n",
            "  return tf.nest.map_structure(add_outer_dims, structure)\n",
            "\n",
            "\n",
            "@gin.configurable\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\array_spec.py",
        "line_number": 203,
        "API": ".format(",
        "context": [
            "    \"\"\"Returns the name of the ArraySpec.\"\"\"\n",
            "    return self._name\n",
            "\n",
            "  def __repr__(self):\n",
            "    return 'ArraySpec(shape={}, dtype={}, name={})'.format(\n",
            "        self.shape, repr(self.dtype), repr(self.name))\n",
            "\n",
            "  def __eq__(self, other):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\array_spec.py",
        "line_number": 309,
        "API": ".broadcast_to(",
        "context": [
            "    \"\"\"\n",
            "    super(BoundedArraySpec, self).__init__(shape, dtype, name)\n",
            "\n",
            "    try:\n",
            "      np.broadcast_to(minimum, shape=shape)\n",
            "    except ValueError as numpy_exception:\n",
            "      raise ValueError('minimum is not compatible with shape. '\n",
            "                       'Message: {!r}.'.format(numpy_exception))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\array_spec.py",
        "line_number": 315,
        "API": ".broadcast_to(",
        "context": [
            "      raise ValueError('minimum is not compatible with shape. '\n",
            "                       'Message: {!r}.'.format(numpy_exception))\n",
            "\n",
            "    try:\n",
            "      np.broadcast_to(maximum, shape=shape)\n",
            "    except ValueError as numpy_exception:\n",
            "      raise ValueError('maximum is not compatible with shape. '\n",
            "                       'Message: {!r}.'.format(numpy_exception))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\array_spec.py",
        "line_number": 320,
        "API": ".as_dtype(",
        "context": [
            "    except ValueError as numpy_exception:\n",
            "      raise ValueError('maximum is not compatible with shape. '\n",
            "                       'Message: {!r}.'.format(numpy_exception))\n",
            "\n",
            "    tf_dtype = tf.as_dtype(self._dtype)\n",
            "    low = tf_dtype.min\n",
            "    high = tf_dtype.max\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\array_spec.py",
        "line_number": 329,
        "API": ".array(",
        "context": [
            "      minimum = low\n",
            "    if maximum is None:\n",
            "      maximum = high\n",
            "\n",
            "    self._minimum = np.array(minimum)\n",
            "    self._maximum = np.array(maximum)\n",
            "\n",
            "    if tf_dtype.is_floating:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\array_spec.py",
        "line_number": 340,
        "API": ".any(",
        "context": [
            "\n",
            "      self._maximum[self._maximum == -np.inf] = low\n",
            "      self._maximum[self._maximum == np.inf] = high\n",
            "\n",
            "    if np.any(self._minimum > self._maximum):\n",
            "      raise ValueError(\n",
            "          'Spec bounds min has values greater than max: [{},{}]'.format(\n",
            "              self._minimum, self._maximum))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\array_spec.py",
        "line_number": 345,
        "API": ".any(",
        "context": [
            "      raise ValueError(\n",
            "          'Spec bounds min has values greater than max: [{},{}]'.format(\n",
            "              self._minimum, self._maximum))\n",
            "    if (np.any(self._minimum < low) or np.any(self._minimum > high) or\n",
            "        np.any(self._maximum < low) or np.any(self._maximum > high)):\n",
            "      raise ValueError(\n",
            "          'Spec bounds [{},{}] not within the range [{}, {}] of the given '\n",
            "          'dtype ({})'.format(self._minimum, self._maximum, low, high,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\array_spec.py",
        "line_number": 382,
        "API": ".broadcast_to(",
        "context": [
            "  @property\n",
            "  def num_values(self):\n",
            "    \"\"\"Returns the number of values for discrete BoundedArraySpec.\"\"\"\n",
            "    if is_discrete(self):\n",
            "      return (np.broadcast_to(self.maximum, shape=self.shape) -\n",
            "              np.broadcast_to(self.minimum, shape=self.shape) + 1)\n",
            "\n",
            "  def __repr__(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\array_spec.py",
        "line_number": 388,
        "API": ".format(",
        "context": [
            "\n",
            "  def __repr__(self):\n",
            "    template = ('BoundedArraySpec(shape={}, dtype={}, name={}, '\n",
            "                'minimum={}, maximum={})')\n",
            "    return template.format(self.shape, repr(self.dtype), repr(self.name),\n",
            "                           self._minimum, self._maximum)\n",
            "\n",
            "  def __eq__(self, other):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\array_spec.py",
        "line_number": 395,
        "API": ".all(",
        "context": [
            "  def __eq__(self, other):\n",
            "    if not isinstance(other, BoundedArraySpec):\n",
            "      return False\n",
            "    return (super(BoundedArraySpec, self).__eq__(other) and\n",
            "            (self.minimum == other.minimum).all() and\n",
            "            (self.maximum == other.maximum).all())\n",
            "\n",
            "  def check_array(self, array):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\array_spec.py",
        "line_number": 401,
        "API": ".all(",
        "context": [
            "\n",
            "  def check_array(self, array):\n",
            "    \"\"\"Return true if the given array conforms to the spec.\"\"\"\n",
            "    return (super(BoundedArraySpec, self).check_array(array) and\n",
            "            np.all(array >= self.minimum) and np.all(array <= self.maximum))\n",
            "\n",
            "  def replace(self, shape=None, dtype=None,\n",
            "              minimum=None, maximum=None,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\distribution_spec.py",
        "line_number": 85,
        "API": ".copy(",
        "context": [
            "\n",
            "    Returns:\n",
            "      Distribution instance.\n",
            "    \"\"\"\n",
            "    kwargs = self._distribution_parameters.copy()\n",
            "    kwargs.update(distribution_parameters)\n",
            "    return self._builder(**kwargs)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\distribution_spec.py",
        "line_number": 91,
        "API": ".format(",
        "context": [
            "    return self._builder(**kwargs)\n",
            "\n",
            "  def __repr__(self):\n",
            "    return (\"DistributionSpec(builder={}, input_params_spec={}, \"\n",
            "            \"sample_spec={})\").format(self.builder,\n",
            "                                      repr(self.input_params_spec),\n",
            "                                      repr(self.sample_spec))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 81,
        "API": ".map_structure(",
        "context": [
            "      raise ValueError(\n",
            "          \"No known conversion from type `%s` to a TensorSpec.  Saw:\\n  %s\"\n",
            "          % (type(s), s))\n",
            "\n",
            "  return tf.nest.map_structure(_convert_to_tensor_spec, spec)\n",
            "\n",
            "\n",
            "def to_array_spec(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 104,
        "API": ".map_structure(",
        "context": [
            "      return array_spec.ArraySpec(s.shape.as_list(),\n",
            "                                  s.dtype.as_numpy_dtype,\n",
            "                                  s.name)\n",
            "\n",
            "  return tf.nest.map_structure(_convert, tensor_spec)\n",
            "\n",
            "\n",
            "def to_nest_array_spec(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 125,
        "API": ".placeholder(",
        "context": [
            "  Returns:\n",
            "    An instance of tf.placeholder.\n",
            "  \"\"\"\n",
            "  ph_shape = list(outer_dims) + spec.shape.as_list()\n",
            "  return tf.compat.v1.placeholder(spec.dtype, ph_shape, spec.name)\n",
            "\n",
            "\n",
            "def to_placeholder_with_default(default, spec, outer_dims=()):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 173,
        "API": ".format(",
        "context": [
            "    def to_ph(spec):\n",
            "      shape = list(outer_dims) + spec.shape.as_list()\n",
            "      if isinstance(default, np.ndarray) and list(default.shape) != shape:\n",
            "        raise ValueError(\"Shape mismatch between default value and spec. \"\n",
            "                         \"Got {}, expected {}\".format(default.shape, shape))\n",
            "      const = tf.constant(default, shape=shape, dtype=spec.dtype)\n",
            "      return to_placeholder_with_default(const, spec, outer_dims=outer_dims)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 178,
        "API": ".map_structure(",
        "context": [
            "      const = tf.constant(default, shape=shape, dtype=spec.dtype)\n",
            "      return to_placeholder_with_default(const, spec, outer_dims=outer_dims)\n",
            "\n",
            "  with tf.name_scope(name_scope):\n",
            "    return tf.nest.map_structure(to_ph, nested_tensor_specs)\n",
            "\n",
            "\n",
            "def _random_uniform_int(shape, outer_dims, minval, maxval, dtype, seed=None):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 187,
        "API": ".broadcast_to(",
        "context": [
            "  # maxval in BoundedTensorSpec is bound inclusive.\n",
            "  # tf.random_uniform is upper bound exclusive, +1 to fix the sampling\n",
            "  # behavior.\n",
            "  # However +1 could cause overflow, in such cases we use the original maxval.\n",
            "  maxval = np.broadcast_to(maxval, minval.shape).astype(dtype.as_numpy_dtype)\n",
            "  minval = np.broadcast_to(minval, maxval.shape).astype(dtype.as_numpy_dtype)\n",
            "\n",
            "  sampling_maxval = maxval\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 192,
        "API": ".where(",
        "context": [
            "  minval = np.broadcast_to(minval, maxval.shape).astype(dtype.as_numpy_dtype)\n",
            "\n",
            "  sampling_maxval = maxval\n",
            "  if dtype.is_integer:\n",
            "    sampling_maxval = np.where(maxval < dtype.max, maxval + 1, maxval)\n",
            "\n",
            "  if not np.all(shape[-len(minval.shape):] == minval.shape):\n",
            "    raise ValueError(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 209,
        "API": ".convert_to_tensor(",
        "context": [
            "  #  sample [5, 3] for minval 2.0\n",
            "  #  stack on innermost axis to get [5, 3, 2]\n",
            "  #  reshape to get [5, 3, 2]\n",
            "  samples = []\n",
            "  shape = tf.convert_to_tensor(shape, dtype=tf.int32)\n",
            "  sample_shape = tf.concat((outer_dims, shape[:-len(minval.shape)]), axis=0)\n",
            "  full_shape = tf.concat((outer_dims, shape), axis=0)\n",
            "  for (single_min, single_max) in zip(minval.flat, sampling_maxval.flat):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 214,
        "API": ".uniform(",
        "context": [
            "  sample_shape = tf.concat((outer_dims, shape[:-len(minval.shape)]), axis=0)\n",
            "  full_shape = tf.concat((outer_dims, shape), axis=0)\n",
            "  for (single_min, single_max) in zip(minval.flat, sampling_maxval.flat):\n",
            "    samples.append(\n",
            "        tf.random.uniform(\n",
            "            shape=sample_shape,\n",
            "            minval=single_min,\n",
            "            maxval=single_max,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 220,
        "API": ".stack(",
        "context": [
            "            minval=single_min,\n",
            "            maxval=single_max,\n",
            "            dtype=dtype,\n",
            "            seed=seed))\n",
            "  samples = tf.stack(samples, axis=-1)\n",
            "  samples = tf.reshape(samples, full_shape)\n",
            "  return samples\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 239,
        "API": ".as_dtype(",
        "context": [
            "    A Tensor sample of the requested spec.\n",
            "  \"\"\"\n",
            "  minval = spec.minimum\n",
            "  maxval = spec.maximum\n",
            "  dtype = tf.as_dtype(spec.dtype)\n",
            "\n",
            "  # To sample uint8 we will use int32 and cast later. This is needed for two\n",
            "  # reasons:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 252,
        "API": ".maximum(",
        "context": [
            "\n",
            "  if dtype in [tf.float64, tf.float32]:\n",
            "    # Avoid under/over-flow as random_uniform can't sample over the full range\n",
            "    # for these types.\n",
            "    minval = np.maximum(dtype.min / 8, minval)\n",
            "    maxval = np.minimum(dtype.max / 8, maxval)\n",
            "\n",
            "  if outer_dims is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 258,
        "API": ".convert_to_tensor(",
        "context": [
            "\n",
            "  if outer_dims is None:\n",
            "    outer_dims = tf.constant([], dtype=tf.int32)\n",
            "  else:\n",
            "    outer_dims = tf.convert_to_tensor(outer_dims, dtype=tf.int32)\n",
            "\n",
            "  def _unique_vals(vals):\n",
            "    if vals.size > 0:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 263,
        "API": ".all(",
        "context": [
            "\n",
            "  def _unique_vals(vals):\n",
            "    if vals.size > 0:\n",
            "      if vals.ndim > 0:\n",
            "        return np.all(vals == vals[0])\n",
            "    return True\n",
            "\n",
            "  if (minval.ndim != 0 or\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 277,
        "API": ".item(",
        "context": [
            "        maxval=maxval,\n",
            "        dtype=sampling_dtype,\n",
            "        seed=seed)\n",
            "  else:\n",
            "    minval = minval.item(0) if minval.ndim != 0 else minval\n",
            "    maxval = maxval.item(0) if maxval.ndim != 0 else maxval\n",
            "    # BoundedTensorSpec are bounds inclusive.\n",
            "    # tf.random_uniform is upper bound exclusive, +1 to fix the sampling\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 286,
        "API": ".convert_to_tensor(",
        "context": [
            "    # However +1 will cause overflow, in such cases we use the original maxval.\n",
            "    if sampling_dtype.is_integer and maxval < sampling_dtype.max:\n",
            "      maxval = maxval + 1\n",
            "\n",
            "    shape = tf.convert_to_tensor(spec.shape, dtype=tf.int32)\n",
            "    full_shape = tf.concat((outer_dims, shape), axis=0)\n",
            "    res = tf.random.uniform(\n",
            "        full_shape,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 296,
        "API": ".cast(",
        "context": [
            "        dtype=sampling_dtype,\n",
            "        seed=seed)\n",
            "\n",
            "  if is_uint8:\n",
            "    res = tf.cast(res, dtype=dtype)\n",
            "\n",
            "  return res\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 344,
        "API": ".format(",
        "context": [
            "    if isinstance(spec, tf.SparseTensorSpec):\n",
            "      outer_shape = tf.get_static_value(outer_dims)\n",
            "      if outer_dims is not None and outer_shape is None:\n",
            "        raise NotImplementedError(\n",
            "            \"outer_dims must be statically known, got: {}\".format(outer_dims))\n",
            "      shape = tf.TensorShape(outer_shape or []).concatenate(spec.shape)\n",
            "\n",
            "      if shape.num_elements() == 0 or tf.compat.dimension_value(shape[0]) == 0:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 349,
        "API": ".zeros(",
        "context": [
            "      shape = tf.TensorShape(outer_shape or []).concatenate(spec.shape)\n",
            "\n",
            "      if shape.num_elements() == 0 or tf.compat.dimension_value(shape[0]) == 0:\n",
            "        return tf.SparseTensor(\n",
            "            indices=tf.zeros([0, shape.rank], dtype=tf.int64),\n",
            "            values=tf.zeros([0], dtype=spec.dtype),\n",
            "            dense_shape=shape)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 382,
        "API": ".cast(",
        "context": [
            "                sample_spec, outer_dims=outer_dims, seed=seed_stream()))\n",
            "      elif spec.dtype == tf.bool:\n",
            "        sample_spec = BoundedTensorSpec(\n",
            "            spec.shape, tf.int32, minimum=0, maximum=1)\n",
            "        return tf.cast(sample_bounded_spec(\n",
            "            sample_spec, outer_dims=outer_dims, seed=seed_stream()), tf.bool)\n",
            "      else:\n",
            "        bounded_spec = BoundedTensorSpec.from_spec(spec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 405,
        "API": ".format(",
        "context": [
            "            outer_dims=outer_dims,\n",
            "            seed=seed_stream(),\n",
            "        )\n",
            "    else:\n",
            "      raise TypeError(\"Spec type not supported: '{}'\".format(spec))\n",
            "\n",
            "  return tf.nest.map_structure(sample_fn, structure)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 429,
        "API": ".format(",
        "context": [
            "  \"\"\"\n",
            "\n",
            "  def make_zero(spec):\n",
            "    if not isinstance(spec, TensorSpec):\n",
            "      raise NotImplementedError(\"Spec type not supported: '{}'\".format(spec))\n",
            "    if outer_dims is None:\n",
            "      shape = spec.shape\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 434,
        "API": ".concat(",
        "context": [
            "    if outer_dims is None:\n",
            "      shape = spec.shape\n",
            "    else:\n",
            "      spec_shape = tf.convert_to_tensor(value=spec.shape, dtype=tf.int32)\n",
            "      shape = tf.concat((outer_dims, spec_shape), axis=0)\n",
            "    return tf.zeros(shape, spec.dtype)\n",
            "\n",
            "  if specs:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 439,
        "API": ".constant(",
        "context": [
            "    return tf.zeros(shape, spec.dtype)\n",
            "\n",
            "  if specs:\n",
            "    if outer_dims is None:\n",
            "      outer_dims = tf.constant([], dtype=tf.int32)\n",
            "    else:\n",
            "      outer_dims = tf.convert_to_tensor(outer_dims, dtype=tf.int32)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 476,
        "API": ".map_structure(",
        "context": [
            "      # TODO(b/187478998) Add name when tf.SparseTensorSpec supports it.\n",
            "      return tf.SparseTensorSpec(shape, spec.dtype)\n",
            "    return TensorSpec(shape, spec.dtype, name=name)\n",
            "\n",
            "  return tf.nest.map_structure(add_outer_dims, specs)\n",
            "\n",
            "\n",
            "def add_outer_dim(specs, dim=None):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 513,
        "API": ".map_structure(",
        "context": [
            "      return BoundedTensorSpec(spec.shape, dtype, spec.minimum, spec.maximum,\n",
            "                               spec.name)\n",
            "    return TensorSpec(spec.shape, dtype, name=spec.name)\n",
            "\n",
            "  return tf.nest.map_structure(update_dtype, specs)\n",
            "\n",
            "\n",
            "def remove_outer_dims_nest(specs, num_outer_dims):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 537,
        "API": ".format(",
        "context": [
            "    # TODO(b/187478998) Use spec.name when tf.SparseTensorSpec supports it.\n",
            "    name = getattr(spec, \"name\", None)\n",
            "    if len(spec.shape) < num_outer_dims:\n",
            "      raise ValueError(\"The shape of spec {} has rank lower than the specified \"\n",
            "                       \"num_outer_dims {}\".format(spec, num_outer_dims))\n",
            "    shape = list(spec.shape)[num_outer_dims:]\n",
            "    if hasattr(spec, \"minimum\") and hasattr(spec, \"maximum\"):\n",
            "      if isinstance(spec.minimum,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 557,
        "API": ".map_structure(",
        "context": [
            "      # TODO(b/187478998) Add name when tf.SparseTensorSpec supports it.\n",
            "      return tf.SparseTensorSpec(shape, spec.dtype)\n",
            "    return TensorSpec(shape, spec.dtype, name=name)\n",
            "\n",
            "  return tf.nest.map_structure(remove_outer_dims, specs)\n",
            "\n",
            "\n",
            "def to_proto(spec):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\specs\\tensor_spec.py",
        "line_number": 589,
        "API": ".split(",
        "context": [
            "\n",
            "def to_pbtxt_file(output_path, spec):\n",
            "  \"\"\"Saves a spec encoded as a struct_pb2.StructuredValue in a pbtxt file.\"\"\"\n",
            "  spec_proto = to_proto(spec)\n",
            "  dir_path = os.path.split(output_path)[0]\n",
            "  tf.io.gfile.makedirs(dir_path)\n",
            "  with tf.io.gfile.GFile(output_path, \"wb\") as f:\n",
            "    f.write(text_format.MessageToString(spec_proto))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\system\\system_multiprocessing.py",
        "line_number": 89,
        "API": ".get_context(",
        "context": [
            "    RuntimeError: If main() was not executed via handle_main().\n",
            "  \"\"\"\n",
            "  if not multiprocessing_core.initialized():\n",
            "    raise RuntimeError(_NOT_INITIALIZED_ERROR)\n",
            "  return _rewrite_target_with_state(multiprocessing_core.get_context(method))\n",
            "\n",
            "\n",
            "class _WrappedTargetWithState:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\system\\system_multiprocessing.py",
        "line_number": 124,
        "API": ".error(",
        "context": [
            "    for saver in _STATE_SAVERS:\n",
            "      try:\n",
            "        self._global_state.append(cloudpickle.dumps(saver.collect_state()))\n",
            "      except TypeError as e:\n",
            "        context.get_logger().error(\n",
            "            'Error while pickling global state from saver %s: %s.  Skipping.',\n",
            "            saver, e)\n",
            "        self._global_state.append(None)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\system\\system_multiprocessing.py",
        "line_number": 148,
        "API": ".format(",
        "context": [
            "    try:\n",
            "      if len(_STATE_SAVERS) != len(self._global_state):\n",
            "        raise RuntimeError(\n",
            "            'Expected number of state savers to match count of state values, '\n",
            "            'but saw {} vs. {}'.format(len(_STATE_SAVERS), self._global_state))\n",
            "\n",
            "      # Deserialize and restore global state\n",
            "      for saver, state in zip(_STATE_SAVERS, self._global_state):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\system\\system_multiprocessing.py",
        "line_number": 160,
        "API": ".error(",
        "context": [
            "      target = cloudpickle.loads(self._target)\n",
            "      return target(*args, **kwargs)\n",
            "    except Exception as e:\n",
            "      logger = self._context.log_to_stderr()\n",
            "      logger.error(e)\n",
            "      raise e\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\system\\system_multiprocessing.py",
        "line_number": 278,
        "API": ".format(",
        "context": [
            "  def restore_state(self, state):\n",
            "    if not isinstance(state, gym.envs.registration.EnvRegistry):\n",
            "      raise RuntimeError(\n",
            "          'Expected gym registry object of type {}, but saw state {}'\n",
            "          .format(gym.envs.registration.EnvRegistry, state))\n",
            "    gym.envs.registration.registry = state\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\system\\default\\multiprocessing_core.py",
        "line_number": 53,
        "API": ".get_context(",
        "context": [
            "    pass\n",
            "\n",
            "\n",
            "def get_context(method: Text = None) -> _multiprocessing.context.BaseContext:\n",
            "  return _multiprocessing.get_context(method)\n",
            "\n",
            "\n",
            "def handle_main(parent_main_fn, *args, **kwargs):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\actor.py",
        "line_number": 186,
        "API": ".scalar(",
        "context": [
            "      # Generate summaries against the train_step\n",
            "      for m in self._metrics:\n",
            "        tag = m.name\n",
            "        try:\n",
            "          tf.summary.scalar(\n",
            "              name=os.path.join(\"Metrics/\", self._name, tag),\n",
            "              data=m.result(),\n",
            "              step=self._train_step)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\actor.py",
        "line_number": 191,
        "API": ".error(",
        "context": [
            "              name=os.path.join(\"Metrics/\", self._name, tag),\n",
            "              data=m.result(),\n",
            "              step=self._train_step)\n",
            "        except ValueError:\n",
            "          logging.error(\"Scalar summary could not be written for metric %s\",\n",
            "                        m)\n",
            "        # Generate summaries against the reference_metrics\n",
            "        for reference_metric in self._reference_metrics:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\actor.py",
        "line_number": 197,
        "API": ".scalar(",
        "context": [
            "        # Generate summaries against the reference_metrics\n",
            "        for reference_metric in self._reference_metrics:\n",
            "          tag = \"Metrics/{}/{}\".format(m.name, reference_metric.name)\n",
            "          try:\n",
            "            tf.summary.scalar(\n",
            "                name=os.path.join(self._name, tag),\n",
            "                data=m.result(),\n",
            "                step=reference_metric.result())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\actor.py",
        "line_number": 202,
        "API": ".error(",
        "context": [
            "                name=os.path.join(self._name, tag),\n",
            "                data=m.result(),\n",
            "                step=reference_metric.result())\n",
            "          except ValueError:\n",
            "            logging.error(\n",
            "                \"Scalar summary could not be written for reference_metric %s\",\n",
            "                m)\n",
            "      for m in self._image_metrics:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\actor.py",
        "line_number": 208,
        "API": ".image(",
        "context": [
            "                m)\n",
            "      for m in self._image_metrics:\n",
            "        tag = m.name\n",
            "        try:\n",
            "          tf.summary.image(\n",
            "              name=os.path.join(\"Metrics/\", self._name, tag),\n",
            "              data=m.result(),\n",
            "              step=self._train_step)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\actor.py",
        "line_number": 213,
        "API": ".error(",
        "context": [
            "              name=os.path.join(\"Metrics/\", self._name, tag),\n",
            "              data=m.result(),\n",
            "              step=self._train_step)\n",
            "        except ValueError:\n",
            "          logging.error(\"Image summary could not be written for metric %s\", m)\n",
            "\n",
            "  def log_metrics(self):\n",
            "    \"\"\"Logs metric results to stdout.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\actor.py",
        "line_number": 219,
        "API": ".format(",
        "context": [
            "  def log_metrics(self):\n",
            "    \"\"\"Logs metric results to stdout.\"\"\"\n",
            "    if self._metrics is None:\n",
            "      return\n",
            "    log = [\"{0} = {1}\".format(m.name, m.result()) for m in self._metrics]\n",
            "    logging.info(\"%s \\n\\t\\t %s\", self._name, \"\\n\\t\\t \".join(log))\n",
            "\n",
            "  def reset(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\interval_trigger.py",
        "line_number": 44,
        "API": ".info(",
        "context": [
            "    self._last_trigger_value = start\n",
            "    self._fn = fn\n",
            "\n",
            "    if self._interval <= 0:\n",
            "      logging.info(\n",
            "          'IntervalTrigger will not be triggered because interval is set to %d',\n",
            "          self._interval)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\learner.py",
        "line_number": 143,
        "API": ".warning(",
        "context": [
            "      summary_root_dir: (Optional) Root directory path where summaries will be\n",
            "       written to.\n",
            "    \"\"\"\n",
            "    if checkpoint_interval < 0:\n",
            "      logging.warning(\n",
            "          'Warning: checkpointing the training process is manually disabled.'\n",
            "          'This means training progress will NOT be automatically restored '\n",
            "          'if the job gets preempted.'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\learner.py",
        "line_number": 149,
        "API": ".join(",
        "context": [
            "          'This means training progress will NOT be automatically restored '\n",
            "          'if the job gets preempted.'\n",
            "      )\n",
            "\n",
            "    self._train_dir = os.path.join(root_dir, TRAIN_DIR)\n",
            "    summary_root_dir = (\n",
            "        root_dir if summary_root_dir is None else summary_root_dir)\n",
            "    self._summary_dir = os.path.join(summary_root_dir, TRAIN_DIR)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\learner.py",
        "line_number": 182,
        "API": ".join(",
        "context": [
            "    self._agent.train = tf.autograph.experimental.do_not_convert(agent.train)\n",
            "\n",
            "    self._strategy_run_options = strategy_run_options\n",
            "\n",
            "    checkpoint_dir = os.path.join(self._train_dir, POLICY_CHECKPOINT_DIR)\n",
            "    with self.strategy.scope():\n",
            "      agent.initialize()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\learner.py",
        "line_number": 222,
        "API": ".warn(",
        "context": [
            "\n",
            "        _create_variables.get_concrete_function(batched_specs)\n",
            "      else:\n",
            "        # TODO(b/186052656) Update clients.\n",
            "        logging.warn('run_optimizer_variable_init = False is Deprecated')\n",
            "\n",
            "      self._checkpointer = common.Checkpointer(\n",
            "          checkpoint_dir,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\learner.py",
        "line_number": 236,
        "API": ".constant(",
        "context": [
            "      if hasattr(trigger, 'set_start'):\n",
            "        trigger.set_start(self.train_step.numpy())\n",
            "\n",
            "    self.triggers.append(self._get_checkpoint_trigger(checkpoint_interval))\n",
            "    self.summary_interval = tf.constant(summary_interval, dtype=tf.int64)\n",
            "\n",
            "  @property\n",
            "  def train_step_numpy(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\learner.py",
        "line_number": 252,
        "API": ".save(",
        "context": [
            "  def _get_checkpoint_trigger(self, checkpoint_interval):\n",
            "    if checkpoint_interval <= 0:\n",
            "      return lambda _, force_trigger=False: None\n",
            "\n",
            "    save_fn = lambda: self._checkpointer.save(self.train_step)\n",
            "    return interval_trigger.IntervalTrigger(\n",
            "        checkpoint_interval, save_fn, start=self.train_step.numpy())\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\learner.py",
        "line_number": 276,
        "API": ".equal(",
        "context": [
            "    assert iterations >= 1, (\n",
            "        'Iterations must be greater or equal to 1, was %d' % iterations)\n",
            "    def _summary_record_if():\n",
            "      if self.summary_interval:\n",
            "        return tf.math.equal(\n",
            "            self.train_step % tf.constant(self.summary_interval), 0)\n",
            "      else:\n",
            "        return tf.constant(False)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\learner.py",
        "line_number": 286,
        "API": ".constant(",
        "context": [
            "         common.soft_device_placement(), \\\n",
            "         tf.compat.v2.summary.record_if(_summary_record_if), \\\n",
            "         self.strategy.scope():\n",
            "      iterator = iterator or self._experience_iterator\n",
            "      loss_info = self._train(tf.constant(iterations),\n",
            "                              iterator,\n",
            "                              parallel_iterations)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\learner.py",
        "line_number": 298,
        "API": ".function(",
        "context": [
            "      return loss_info\n",
            "\n",
            "  # Use tf.config.experimental_run_functions_eagerly(True) if you want to\n",
            "  # disable use of tf.function.\n",
            "  @common.function(autograph=True)\n",
            "  def _train(self, iterations, iterator, parallel_iterations):\n",
            "    # Call run explicitly once to get loss info shape for autograph. Because the\n",
            "    # for loop below will get converted to a `tf.while_loop` by autograph we\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\learner.py",
        "line_number": 305,
        "API": ".range(",
        "context": [
            "    # for loop below will get converted to a `tf.while_loop` by autograph we\n",
            "    # need the shape of loss info to be well defined.\n",
            "    loss_info = self.single_train_step(iterator)\n",
            "\n",
            "    for _ in tf.range(iterations - 1):\n",
            "      tf.autograph.experimental.set_loop_options(\n",
            "          parallel_iterations=parallel_iterations)\n",
            "      loss_info = self.single_train_step(iterator)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\learner.py",
        "line_number": 315,
        "API": ".is_tensor(",
        "context": [
            "      rank = None\n",
            "      if isinstance(loss, tf.distribute.DistributedValues):\n",
            "        # If loss is distributed get the rank from the first replica.\n",
            "        rank = loss.values[0].shape.rank\n",
            "      elif tf.is_tensor(loss):\n",
            "        rank = loss.shape.rank\n",
            "      axis = None\n",
            "      if rank:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\learner.py",
        "line_number": 324,
        "API": ".map_structure(",
        "context": [
            "      return self.strategy.reduce(tf.distribute.ReduceOp.SUM, loss, axis=axis)\n",
            "\n",
            "    # We assume all data can be reduced in the loss_info. This means no\n",
            "    # string dtypes are currently allowed as LossInfo Fields.\n",
            "    reduced_loss_info = tf.nest.map_structure(_reduce_loss, loss_info)\n",
            "    return reduced_loss_info\n",
            "\n",
            "  def single_train_step(self, iterator):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\learner.py",
        "line_number": 385,
        "API": ".equal(",
        "context": [
            "      The total loss computed.\n",
            "    \"\"\"\n",
            "\n",
            "    def _summary_record_if():\n",
            "      return tf.math.equal(\n",
            "          self.train_step % tf.constant(self.summary_interval), 0)\n",
            "\n",
            "    with self.train_summary_writer.as_default(), \\\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\learner.py",
        "line_number": 408,
        "API": ".function(",
        "context": [
            "      return loss_info\n",
            "\n",
            "  # Use tf.config.experimental_run_functions_eagerly(True) if you want to\n",
            "  # disable use of tf.function.\n",
            "  @common.function(autograph=True)\n",
            "  def _loss(\n",
            "      self,\n",
            "      experience_and_sample_info: ExperienceAndSampleInfo,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\learner.py",
        "line_number": 438,
        "API": ".is_tensor(",
        "context": [
            "    def _reduce_loss(loss):\n",
            "      rank = None\n",
            "      if isinstance(loss, tf.distribute.DistributedValues):\n",
            "        rank = loss.values[0].shape.rank\n",
            "      elif tf.is_tensor(loss):\n",
            "        rank = loss.shape.rank\n",
            "      axis = None\n",
            "      if rank:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\learner.py",
        "line_number": 447,
        "API": ".map_structure(",
        "context": [
            "      return self.strategy.reduce(reduce_op, loss, axis=axis)\n",
            "\n",
            "    # We assume all data can be reduced in the loss_info. This means no\n",
            "    # string dtypes are currently allowed as LossInfo Fields.\n",
            "    reduced_loss_info = tf.nest.map_structure(_reduce_loss, loss_info)\n",
            "    return reduced_loss_info\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\ppo_learner.py",
        "line_number": 221,
        "API": ".repeat(",
        "context": [
            "      # We take the current batches, repeat for `num_epochs` times and exhaust\n",
            "      # this data in the current learner run. The next time learner runs, new\n",
            "      # batches of data will be sampled, cached and repeated.\n",
            "      # This is enabled by the `Counter().flat_map()` trick below.\n",
            "      train_dataset = train_dataset.cache().repeat(self._num_epochs)\n",
            "\n",
            "      if self._minibatch_size:\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\ppo_learner.py",
        "line_number": 226,
        "API": ".map_structure(",
        "context": [
            "\n",
            "      if self._minibatch_size:\n",
            "\n",
            "        def squash_dataset_element(sequence, info):\n",
            "          return tf.nest.map_structure(\n",
            "              utils.BatchSquash(2).flatten, (sequence, info))\n",
            "\n",
            "        # We unbatch the dataset shaped [B, T, ...] to a new dataset that\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\ppo_learner.py",
        "line_number": 236,
        "API": ".batch(",
        "context": [
            "        # in mini batches that contain subsets from more than one sequences.\n",
            "        # PPO agent can handle mini batches across episode boundaries.\n",
            "        train_dataset = train_dataset.map(squash_dataset_element).unbatch()\n",
            "        train_dataset = train_dataset.shuffle(self._shuffle_buffer_size)\n",
            "        train_dataset = train_dataset.batch(1, drop_remainder=True)\n",
            "        train_dataset = train_dataset.batch(\n",
            "            self._minibatch_size, drop_remainder=True)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\ppo_learner.py",
        "line_number": 271,
        "API": ".assign(",
        "context": [
            "    Returns:\n",
            "      The total loss computed before running the final step.\n",
            "    \"\"\"\n",
            "    num_frames = self._update_normalizers(self._normalization_iterator)\n",
            "    self.num_frames_for_training.assign(num_frames)\n",
            "\n",
            "    if self._minibatch_size:\n",
            "      num_total_batches = int(self.num_frames_for_training.numpy() /\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\ppo_learner.py",
        "line_number": 283,
        "API": ".format(",
        "context": [
            "    iterations = int(num_total_batches / self.num_replicas)\n",
            "    if iterations == 0:\n",
            "      raise ValueError('Cannot distribute {} batches across {} replicas. '\n",
            "                       'Please increase PPOLearner.num_samples. See PPOLeaner.'\n",
            "                       'num_samples documentation for more details.'.format(\n",
            "                           num_total_batches, self.num_replicas))\n",
            "    loss_info = self._generic_learner.run(\n",
            "        iterations,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\ppo_learner.py",
        "line_number": 292,
        "API": ".function(",
        "context": [
            "        parallel_iterations=parallel_iterations)\n",
            "\n",
            "    return loss_info\n",
            "\n",
            "  @common.function(autograph=True)\n",
            "  def _update_normalizers(self, iterator):\n",
            "    \"\"\"Update the normalizers and count the total number of frames.\"\"\"\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\ppo_learner.py",
        "line_number": 314,
        "API": ".range(",
        "context": [
            "    num_frames = 0\n",
            "    traj, _ = next(iterator)\n",
            "    num_frames += _update(traj)\n",
            "\n",
            "    for _ in tf.range(1, self._num_samples):\n",
            "      traj, _ = next(iterator)\n",
            "      num_frames += _update(traj)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\triggers.py",
        "line_number": 148,
        "API": ".join(",
        "context": [
            "    for saver, _ in savers:\n",
            "      for name, fn in extra_concrete_functions:\n",
            "        saver.register_concrete_function(name, fn)\n",
            "\n",
            "    self._checkpoint_dir = os.path.join(saved_model_dir,\n",
            "                                        learner.POLICY_CHECKPOINT_DIR)\n",
            "\n",
            "    # TODO(b/173815037): Use a TF-Agents util to check for whether a saved\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\triggers.py",
        "line_number": 154,
        "API": ".join(",
        "context": [
            "\n",
            "    # TODO(b/173815037): Use a TF-Agents util to check for whether a saved\n",
            "    # policy already exists.\n",
            "    for saver, path in savers:\n",
            "      spec_path = os.path.join(saved_model_dir, path, 'policy_specs.pbtxt')\n",
            "      if not tf.io.gfile.exists(spec_path):\n",
            "        saver.save(os.path.join(saved_model_dir, path))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\triggers.py",
        "line_number": 182,
        "API": ".assign(",
        "context": [
            "\n",
            "  def _save_fn(self) -> None:\n",
            "    self._agent.post_process_policy()\n",
            "    for k, v in self._metadata_metrics.items():\n",
            "      self._metadata[k].assign(v.result())\n",
            "    self._raw_policy_saver.save_checkpoint(\n",
            "        os.path.join(self._checkpoint_dir,\n",
            "                     'policy_checkpoint_%010d' % self._train_step.numpy()))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\triggers.py",
        "line_number": 217,
        "API": ".info(",
        "context": [
            "    steps_per_sec = self._step_timer.steps_per_second()\n",
            "    self._step_timer.restart()\n",
            "    step = self._train_step.numpy()\n",
            "    if self._log_to_terminal:\n",
            "      logging.info('Step: %d, %.3f steps/sec', step, steps_per_sec)\n",
            "    with tf.compat.v2.summary.record_if(True):\n",
            "      with tf.name_scope('RunTime/'):\n",
            "        tf.summary.scalar(name='train_steps_per_sec',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\triggers.py",
        "line_number": 247,
        "API": ".info(",
        "context": [
            "    super(ReverbCheckpointTrigger, self).__init__(interval, self._save_fn)\n",
            "\n",
            "  def _save_fn(self) -> None:\n",
            "    checkpoint_path = self._reverb_client.checkpoint()\n",
            "    logging.info('Checkpointing Reverb data to %s', checkpoint_path)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\utils\\strategy_utils.py",
        "line_number": 45,
        "API": ".info(",
        "context": [
            "  \"\"\"\n",
            "  if tpu and use_gpu:\n",
            "    raise ValueError('Only one of tpu or use_gpu should be provided.')\n",
            "  if tpu or use_gpu:\n",
            "    logging.info('Devices: \\n%s', tf.config.list_logical_devices())\n",
            "    if tpu:\n",
            "      resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu)\n",
            "      tf.config.experimental_connect_to_cluster(resolver)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\utils\\strategy_utils.py",
        "line_number": 54,
        "API": ".info(",
        "context": [
            "\n",
            "      strategy = tf.distribute.TPUStrategy(resolver)\n",
            "    else:\n",
            "      strategy = tf.distribute.MirroredStrategy()\n",
            "    logging.info('Devices after getting strategy:\\n%s',\n",
            "                 tf.config.list_logical_devices())\n",
            "  else:\n",
            "    strategy = tf.distribute.get_strategy()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\utils\\train_utils.py",
        "line_number": 77,
        "API": ".cast(",
        "context": [
            "\n",
            "    # Get the train step in which the experience was observed. This is stored as\n",
            "    # Reverb priority.\n",
            "    # TODO(b/168426331): Check sample info version.\n",
            "    observation_generation_train_step = tf.cast(\n",
            "        sample_info.priority, dtype=tf.int64)\n",
            "\n",
            "    # Get the train step corresponding to the latest outputed policy.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\utils\\train_utils.py",
        "line_number": 83,
        "API": ".cast(",
        "context": [
            "\n",
            "    # Get the train step corresponding to the latest outputed policy.\n",
            "    # Policy is written in every `train_steps_per_policy_update` step, so we\n",
            "    # normalize the value of `train_step` accordingly.\n",
            "    on_policy_train_step = tf.cast(\n",
            "        train_step / train_steps_per_policy_update,\n",
            "        dtype=tf.int64) * train_steps_per_policy_update\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\utils\\train_utils.py",
        "line_number": 90,
        "API": ".reduce_max(",
        "context": [
            "\n",
            "    # An observation is off-policy if its train step delta is greater than 0.\n",
            "    observation_train_step_delta = (\n",
            "        on_policy_train_step - observation_generation_train_step)\n",
            "    max_train_step_delta = tf.reduce_max(observation_train_step_delta)\n",
            "    max_policy_update_delta = tf.cast(\n",
            "        max_train_step_delta / train_steps_per_policy_update, dtype=tf.int64)\n",
            "    num_stale_observations = tf.reduce_sum(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\utils\\train_utils.py",
        "line_number": 99,
        "API": ".scalar(",
        "context": [
            "\n",
            "    # Break out from local name scopes (e.g. the ones intrdouced by while loop).\n",
            "    with tf.name_scope(''):\n",
            "      # Write the summaries for the first replica.\n",
            "      tf.summary.scalar(\n",
            "          name='staleness/max_train_step_delta_in_batch',\n",
            "          data=max_train_step_delta,\n",
            "          step=train_step)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\utils\\train_utils.py",
        "line_number": 107,
        "API": ".scalar(",
        "context": [
            "      tf.summary.scalar(\n",
            "          name='staleness/max_policy_update_delta_in_batch',\n",
            "          data=max_policy_update_delta,\n",
            "          step=train_step)\n",
            "      tf.summary.scalar(\n",
            "          name='staleness/num_stale_obserations_in_batch',\n",
            "          data=num_stale_observations,\n",
            "          step=train_step)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\utils\\train_utils.py",
        "line_number": 141,
        "API": ".join(",
        "context": [
            "    TimeoutError: If the policy does not become available during the number of\n",
            "      retries.\n",
            "  \"\"\"\n",
            "  # TODO(b/173815037): Write and wait for a DONE file instead.\n",
            "  last_written_policy_file = os.path.join(policy_dir, 'policy_specs.pbtxt')\n",
            "  wait_for_file(\n",
            "      last_written_policy_file,\n",
            "      sleep_time_secs=sleep_time_secs,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\utils\\train_utils.py",
        "line_number": 179,
        "API": ".info(",
        "context": [
            "      stat = tf.io.gfile.stat(file_path)\n",
            "    except tf.errors.NotFoundError:\n",
            "      return True\n",
            "    found_file = stat.length <= 0\n",
            "    logging.info(\n",
            "        'Checking for file %s (%s)',\n",
            "        file_path,\n",
            "        'found' if found_file else 'not found',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\utils\\train_utils.py",
        "line_number": 221,
        "API": ".info(",
        "context": [
            "  \"\"\"\n",
            "  retry = 0\n",
            "  while (num_retries is None or retry < num_retries) and wait_predicate_fn():\n",
            "    if sleep_time_secs > 0:\n",
            "      logging.info(\n",
            "          'Waiting for `wait_predicate_fn`. Block execution. Sleeping for %d '\n",
            "          'seconds.', sleep_time_secs)\n",
            "      time.sleep(sleep_time_secs)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\train\\utils\\train_utils.py",
        "line_number": 230,
        "API": ".format(",
        "context": [
            "\n",
            "  if retry >= num_retries:\n",
            "    raise TimeoutError(\n",
            "        'The wait predicate did not return `False` after {} retries waiting {} '\n",
            "        'seconds between retries.'.format(num_retries, sleep_time_secs))\n",
            "\n",
            "  logging.info('The `wait_predicate_fn` returned `False`. Continue execution.')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\policy_step.py",
        "line_number": 101,
        "API": ".get(",
        "context": [
            "\n",
            "def _maybe_get_value_namedtuple_or_dict(\n",
            "    obj: Any, key: Text, default_value: Any) -> Any:\n",
            "  if isinstance(obj, Mapping):\n",
            "    return obj.get(key, default_value)\n",
            "  if getattr(obj, '_fields', None) is not None:\n",
            "    return getattr(obj, key, default_value)\n",
            "  return None\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 36,
        "API": ".sum(",
        "context": [
            "def _as_array(a, t=np.float32):\n",
            "  if t is None:\n",
            "    t = np.float32\n",
            "  r = np.asarray(a, dtype=t)\n",
            "  if np.isnan(np.sum(r)):\n",
            "    raise ValueError('Received a time_step input that converted to a nan array.'\n",
            "                     ' Did you accidentally set some input value to None?.\\n'\n",
            "                     'Got:\\n{}'.format(a))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 77,
        "API": ".is_tensor(",
        "context": [
            "  \"\"\"\n",
            "  __slots__ = ()\n",
            "\n",
            "  def is_first(self) -> types.Bool:\n",
            "    if tf.is_tensor(self.step_type):\n",
            "      return tf.equal(self.step_type, StepType.FIRST)\n",
            "    return np.equal(self.step_type, StepType.FIRST)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 82,
        "API": ".is_tensor(",
        "context": [
            "      return tf.equal(self.step_type, StepType.FIRST)\n",
            "    return np.equal(self.step_type, StepType.FIRST)\n",
            "\n",
            "  def is_mid(self) -> types.Bool:\n",
            "    if tf.is_tensor(self.step_type):\n",
            "      return tf.equal(self.step_type, StepType.MID)\n",
            "    return np.equal(self.step_type, StepType.MID)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 87,
        "API": ".is_tensor(",
        "context": [
            "      return tf.equal(self.step_type, StepType.MID)\n",
            "    return np.equal(self.step_type, StepType.MID)\n",
            "\n",
            "  def is_last(self) -> types.Bool:\n",
            "    if tf.is_tensor(self.step_type):\n",
            "      return tf.equal(self.step_type, StepType.LAST)\n",
            "    return np.equal(self.step_type, StepType.LAST)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 94,
        "API": ".flatten(",
        "context": [
            "\n",
            "  def __hash__(self):\n",
            "    # TODO(b/130243327): Explore performance impact and consider converting\n",
            "    # dicts in the observation into ordered dicts in __new__ call.\n",
            "    return hash(tuple(tf.nest.flatten(self)))\n",
            "\n",
            "  def __repr__(self):\n",
            "    return 'TimeStep(\\n' + pprint.pformat(dict(self._asdict())) + ')'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 138,
        "API": ".flatten(",
        "context": [
            "\n",
            "  Returns:\n",
            "    A `TimeStep`.\n",
            "  \"\"\"\n",
            "  first_observation = tf.nest.flatten(observation)[0]\n",
            "  if not tf.is_tensor(first_observation):\n",
            "    if batch_size is not None:\n",
            "      if reward_spec is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 144,
        "API": ".map_structure(",
        "context": [
            "    if batch_size is not None:\n",
            "      if reward_spec is None:\n",
            "        reward = np.zeros(batch_size, dtype=np.float32)\n",
            "      else:\n",
            "        reward = tf.nest.map_structure(\n",
            "            lambda r: np.zeros([batch_size] + list(r.shape), _get_np_dtype(r)),\n",
            "            reward_spec)\n",
            "      discount = np.ones(batch_size, dtype=np.float32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 155,
        "API": ".map_structure(",
        "context": [
            "      if reward_spec is None:\n",
            "        return TimeStep(StepType.FIRST, _as_array(0.0), _as_array(1.0),\n",
            "                        observation)\n",
            "      else:\n",
            "        reward = tf.nest.map_structure(\n",
            "            lambda r: np.zeros(r.shape, dtype=_get_np_dtype(r)), reward_spec)\n",
            "        return TimeStep(StepType.FIRST, reward, _as_array(1.0), observation)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 162,
        "API": ".fill(",
        "context": [
            "\n",
            "  # TODO(b/130244501): Check leading dimension of first_observation\n",
            "  # against batch_size if all are known statically.\n",
            "  shape = _as_multi_dim(batch_size)\n",
            "  step_type = tf.fill(shape, StepType.FIRST, name='step_type')\n",
            "  if reward_spec is None:\n",
            "    reward = tf.fill(shape, _as_array(0.0), name='reward')\n",
            "  else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 168,
        "API": ".fill(",
        "context": [
            "    reward = tf.fill(shape, _as_array(0.0), name='reward')\n",
            "  else:\n",
            "    reward = tf.nest.map_structure(\n",
            "        # pylint: disable=g-long-lambda\n",
            "        lambda r: tf.fill(\n",
            "            tf.concat([shape, r.shape], axis=-1),\n",
            "            _as_array(0.0, _get_np_dtype(r)),\n",
            "            name='reward'),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 173,
        "API": ".fill(",
        "context": [
            "            tf.concat([shape, r.shape], axis=-1),\n",
            "            _as_array(0.0, _get_np_dtype(r)),\n",
            "            name='reward'),\n",
            "        reward_spec)\n",
            "  discount = tf.fill(shape, _as_array(1.0), name='discount')\n",
            "  return TimeStep(step_type, reward, discount, observation)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 180,
        "API": ".is_tensor(",
        "context": [
            "\n",
            "def _as_multi_dim(maybe_scalar):\n",
            "  if maybe_scalar is None:\n",
            "    shape = ()\n",
            "  elif tf.is_tensor(maybe_scalar) and maybe_scalar.shape.rank > 0:\n",
            "    shape = maybe_scalar\n",
            "  elif np.asarray(maybe_scalar).ndim > 0:\n",
            "    shape = maybe_scalar\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 217,
        "API": ".flatten(",
        "context": [
            "  Raises:\n",
            "    ValueError: If observations are tensors but reward's statically known rank\n",
            "      is not `0` or `1`.\n",
            "  \"\"\"\n",
            "  first_observation = tf.nest.flatten(observation)[0]\n",
            "  if not tf.is_tensor(first_observation):\n",
            "    if outer_dims is not None:\n",
            "      step_type = np.tile(StepType.MID, outer_dims)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 224,
        "API": ".map_structure(",
        "context": [
            "      step_type = np.tile(StepType.MID, outer_dims)\n",
            "      discount = _as_array(discount)\n",
            "      return TimeStep(step_type, reward, discount, observation)\n",
            "    # Infer the batch size.\n",
            "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
            "                                   reward)\n",
            "    first_reward = tf.nest.flatten(reward)[0]\n",
            "    discount = _as_array(discount)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 229,
        "API": ".tile(",
        "context": [
            "                                   reward)\n",
            "    first_reward = tf.nest.flatten(reward)[0]\n",
            "    discount = _as_array(discount)\n",
            "    if first_reward.shape:\n",
            "      step_type = np.tile(StepType.MID, first_reward.shape)\n",
            "    else:\n",
            "      step_type = StepType.MID\n",
            "    return TimeStep(step_type, reward, discount, observation)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 237,
        "API": ".map_structure(",
        "context": [
            "\n",
            "  # TODO(b/130245199): If reward.shape.rank == 2, and static\n",
            "  # batch sizes are available for both first_observation and reward,\n",
            "  # check that these match.\n",
            "  reward = tf.nest.map_structure(\n",
            "      # pylint: disable=g-long-lambda\n",
            "      lambda r: tf.convert_to_tensor(value=r, dtype=r.dtype, name='reward'),\n",
            "      reward)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 244,
        "API": ".flatten(",
        "context": [
            "      reward)\n",
            "  if outer_dims is not None:\n",
            "    shape = outer_dims\n",
            "  else:\n",
            "    first_reward = tf.nest.flatten(reward)[0]\n",
            "    if first_reward.shape.rank == 0:\n",
            "      shape = []\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 249,
        "API": ".shape(",
        "context": [
            "    if first_reward.shape.rank == 0:\n",
            "      shape = []\n",
            "    else:\n",
            "      shape = [tf.compat.dimension_value(first_reward.shape[0]) or\n",
            "               tf.shape(input=first_reward)[0]]\n",
            "  step_type = tf.fill(shape, StepType.MID, name='step_type')\n",
            "  discount = tf.convert_to_tensor(\n",
            "      value=discount, dtype=tf.float32, name='discount')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 254,
        "API": ".fill(",
        "context": [
            "  step_type = tf.fill(shape, StepType.MID, name='step_type')\n",
            "  discount = tf.convert_to_tensor(\n",
            "      value=discount, dtype=tf.float32, name='discount')\n",
            "  if discount.shape.rank == 0:\n",
            "    discount = tf.fill(shape, discount, name='discount_fill')\n",
            "  return TimeStep(step_type, reward, discount, observation)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 275,
        "API": ".flatten(",
        "context": [
            "  Returns:\n",
            "    A `TimeStep`.\n",
            "\n",
            "  \"\"\"\n",
            "  first_observation = tf.nest.flatten(observation)[0]\n",
            "  if not tf.is_tensor(first_observation):\n",
            "    if outer_dims is not None:\n",
            "      step_type = np.tile(StepType.LAST, outer_dims)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 283,
        "API": ".map_structure(",
        "context": [
            "      discount = np.zeros(outer_dims, dtype=np.float32)\n",
            "      return TimeStep(step_type, reward, discount, observation)\n",
            "\n",
            "    # Infer the batch size based on reward\n",
            "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
            "                                   reward)\n",
            "    first_reward = tf.nest.flatten(reward)[0]\n",
            "    if first_reward.shape:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 288,
        "API": ".tile(",
        "context": [
            "                                   reward)\n",
            "    first_reward = tf.nest.flatten(reward)[0]\n",
            "    if first_reward.shape:\n",
            "      batch_size = first_reward.shape[0]\n",
            "      step_type = np.tile(StepType.LAST, batch_size)\n",
            "      discount = np.zeros(batch_size, dtype=np.float32)\n",
            "    else:\n",
            "      step_type = StepType.LAST\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 298,
        "API": ".map_structure(",
        "context": [
            "\n",
            "  # TODO(b/130245199): If reward.shape.rank == 2, and static\n",
            "  # batch sizes are available for both first_observation and reward,\n",
            "  # check that these match.\n",
            "  reward = tf.nest.map_structure(\n",
            "      lambda r: tf.convert_to_tensor(r, dtype=r.dtype, name='reward'), reward)\n",
            "\n",
            "  if outer_dims is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 304,
        "API": ".flatten(",
        "context": [
            "\n",
            "  if outer_dims is not None:\n",
            "    shape = outer_dims\n",
            "  else:\n",
            "    first_reward = tf.nest.flatten(reward)[0]\n",
            "    if first_reward.shape.rank == 0:\n",
            "      shape = []\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 309,
        "API": ".shape(",
        "context": [
            "    if first_reward.shape.rank == 0:\n",
            "      shape = []\n",
            "    else:\n",
            "      shape = [tf.compat.dimension_value(first_reward.shape[0]) or\n",
            "               tf.shape(input=first_reward)[0]]\n",
            "  step_type = tf.fill(shape, StepType.LAST, name='step_type')\n",
            "  discount = tf.fill(shape, _as_array(0.0), name='discount')\n",
            "  return TimeStep(step_type, reward, discount, observation)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 338,
        "API": ".flatten(",
        "context": [
            "\n",
            "  Returns:\n",
            "    A `TimeStep`.\n",
            "  \"\"\"\n",
            "  first_observation = tf.nest.flatten(observation)[0]\n",
            "  if not tf.is_tensor(first_observation):\n",
            "    if outer_dims is not None:\n",
            "      step_type = np.tile(StepType.LAST, outer_dims)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 345,
        "API": ".map_structure(",
        "context": [
            "      step_type = np.tile(StepType.LAST, outer_dims)\n",
            "      discount = _as_array(discount)\n",
            "      return TimeStep(step_type, reward, discount, observation)\n",
            "    # Infer the batch size.\n",
            "    reward = tf.nest.map_structure(lambda x: _as_array(x, _get_np_dtype(x)),\n",
            "                                   reward)\n",
            "    first_reward = tf.nest.flatten(reward)[0]\n",
            "    discount = _as_array(discount)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 350,
        "API": ".tile(",
        "context": [
            "                                   reward)\n",
            "    first_reward = tf.nest.flatten(reward)[0]\n",
            "    discount = _as_array(discount)\n",
            "    if first_reward.shape:\n",
            "      step_type = np.tile(StepType.LAST, first_reward.shape)\n",
            "    else:\n",
            "      step_type = StepType.LAST\n",
            "    return TimeStep(step_type, reward, discount, observation)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 355,
        "API": ".map_structure(",
        "context": [
            "    else:\n",
            "      step_type = StepType.LAST\n",
            "    return TimeStep(step_type, reward, discount, observation)\n",
            "\n",
            "  reward = tf.nest.map_structure(\n",
            "      lambda r: tf.convert_to_tensor(value=r, dtype=r.dtype, name='reward'),\n",
            "      reward)\n",
            "  if outer_dims is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 366,
        "API": ".shape(",
        "context": [
            "    if first_reward.shape.rank == 0:\n",
            "      shape = []\n",
            "    else:\n",
            "      shape = [tf.compat.dimension_value(first_reward.shape[0]) or\n",
            "               tf.shape(input=first_reward)[0]]\n",
            "  step_type = tf.fill(shape, StepType.LAST, name='step_type')\n",
            "  discount = tf.convert_to_tensor(\n",
            "      value=discount, dtype=tf.float32, name='discount')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 371,
        "API": ".fill(",
        "context": [
            "  step_type = tf.fill(shape, StepType.LAST, name='step_type')\n",
            "  discount = tf.convert_to_tensor(\n",
            "      value=discount, dtype=tf.float32, name='discount')\n",
            "  if discount.shape.rank == 0:\n",
            "    discount = tf.fill(shape, discount, name='discount_fill')\n",
            "  return TimeStep(step_type, reward, discount, observation)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 397,
        "API": ".flatten(",
        "context": [
            "  \"\"\"\n",
            "  if observation_spec is None:\n",
            "    return TimeStep(step_type=(), reward=(), discount=(), observation=())\n",
            "\n",
            "  first_observation_spec = tf.nest.flatten(observation_spec)[0]\n",
            "  if reward_spec is not None:\n",
            "    first_reward_spec = tf.nest.flatten(reward_spec)[0]\n",
            "    if (isinstance(first_reward_spec, tf.TypeSpec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\time_step.py",
        "line_number": 405,
        "API": ".format(",
        "context": [
            "        != isinstance(first_observation_spec, tf.TypeSpec)):\n",
            "      raise TypeError(\n",
            "          'Expected observation and reward specs to both be either tensor or '\n",
            "          'array specs, but saw spec values {} vs. {}'\n",
            "          .format(first_observation_spec, first_reward_spec))\n",
            "  if isinstance(first_observation_spec, tf.TypeSpec):\n",
            "    return TimeStep(\n",
            "        step_type=tensor_spec.TensorSpec([], tf.int32, name='step_type'),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 72,
        "API": ".is_tensor(",
        "context": [
            "  \"\"\"\n",
            "  __slots__ = ()\n",
            "\n",
            "  def is_first(self) -> types.Bool:\n",
            "    if tf.is_tensor(self.step_type):\n",
            "      return tf.equal(self.step_type, ts.StepType.FIRST)\n",
            "    return self.step_type == ts.StepType.FIRST\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 77,
        "API": ".is_tensor(",
        "context": [
            "      return tf.equal(self.step_type, ts.StepType.FIRST)\n",
            "    return self.step_type == ts.StepType.FIRST\n",
            "\n",
            "  def is_mid(self) -> types.Bool:\n",
            "    if tf.is_tensor(self.step_type):\n",
            "      return tf.logical_and(\n",
            "          tf.equal(self.step_type, ts.StepType.MID),\n",
            "          tf.equal(self.next_step_type, ts.StepType.MID))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 85,
        "API": ".is_tensor(",
        "context": [
            "    return (self.step_type == ts.StepType.MID) & (\n",
            "        self.next_step_type == ts.StepType.MID)\n",
            "\n",
            "  def is_last(self) -> types.Bool:\n",
            "    if tf.is_tensor(self.next_step_type):\n",
            "      return tf.equal(self.next_step_type, ts.StepType.LAST)\n",
            "    return self.next_step_type == ts.StepType.LAST\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 90,
        "API": ".is_tensor(",
        "context": [
            "      return tf.equal(self.next_step_type, ts.StepType.LAST)\n",
            "    return self.next_step_type == ts.StepType.LAST\n",
            "\n",
            "  def is_boundary(self) -> types.Bool:\n",
            "    if tf.is_tensor(self.step_type):\n",
            "      return tf.equal(self.step_type, ts.StepType.LAST)\n",
            "    return self.step_type == ts.StepType.LAST\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 219,
        "API": ".identity(",
        "context": [
            "  \"\"\"\n",
            "  if nest_utils.has_tensors(\n",
            "      observation, action, policy_info, reward, discount):\n",
            "    with tf.name_scope(name_scope):\n",
            "      discount = tf.identity(discount)\n",
            "      shape = tf.shape(input=discount)\n",
            "      make_tensors = lambda struct: tf.nest.map_structure(tf.identity, struct)\n",
            "      return Trajectory(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 227,
        "API": ".fill(",
        "context": [
            "          step_type=tf.fill(shape, step_type),\n",
            "          observation=make_tensors(observation),\n",
            "          action=make_tensors(action),\n",
            "          policy_info=make_tensors(policy_info),\n",
            "          next_step_type=tf.fill(shape, next_step_type),\n",
            "          reward=make_tensors(reward),\n",
            "          discount=discount)\n",
            "  else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 233,
        "API": ".map_structure(",
        "context": [
            "          discount=discount)\n",
            "  else:\n",
            "    discount = np.asarray(discount)\n",
            "    shape = discount.shape\n",
            "    make_arrays = lambda struct: tf.nest.map_structure(np.asarray, struct)\n",
            "    return Trajectory(\n",
            "        step_type=np.full(shape, step_type),\n",
            "        observation=make_arrays(observation),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 239,
        "API": ".full(",
        "context": [
            "        step_type=np.full(shape, step_type),\n",
            "        observation=make_arrays(observation),\n",
            "        action=make_arrays(action),\n",
            "        policy_info=make_arrays(policy_info),\n",
            "        next_step_type=np.full(shape, next_step_type),\n",
            "        reward=make_arrays(reward),\n",
            "        discount=discount)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 439,
        "API": ".is_tensor(",
        "context": [
            "\n",
            "  Returns:\n",
            "    A python integer or `0-D` scalar tensor with type `int64`.\n",
            "  \"\"\"\n",
            "  assert tf.is_tensor(t), t\n",
            "  if isinstance(t, tf.SparseTensor):\n",
            "    static_shape = tf.get_static_value(t.dense_shape)\n",
            "    if static_shape is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 451,
        "API": ".shape(",
        "context": [
            "    outer_dim = tf.compat.dimension_value(t.shape[0])\n",
            "    return outer_dim if outer_dim is not None else t.nrows()\n",
            "  else:\n",
            "    outer_dim = tf.compat.dimension_value(t.shape[0])\n",
            "    return outer_dim if outer_dim is not None else tf.shape(t)[0]\n",
            "\n",
            "\n",
            "def from_episode(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 468,
        "API": ".flatten(",
        "context": [
            "  If `discount` is not provided, the first entry in `reward` is used to estimate\n",
            "  `T`:\n",
            "\n",
            "  ```\n",
            "  reward_0 = tf.nest.flatten(reward)[0]\n",
            "  T = shape(reward_0)[0]\n",
            "  ```\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 522,
        "API": ".flatten(",
        "context": [
            "    \"\"\"Implementation of from_episode.\"\"\"\n",
            "    if discount is not None:\n",
            "      time_source = discount\n",
            "    else:\n",
            "      time_source = tf.nest.flatten(reward)[0]\n",
            "    if tf.is_tensor(time_source):\n",
            "      num_frames = _maybe_static_outer_dim(time_source)\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 530,
        "API": ".is_tensor(",
        "context": [
            "      num_frames = np.shape(time_source)[0]\n",
            "    if discount is None:\n",
            "      discount = ones_fn([num_frames], dtype=float_dtype)\n",
            "\n",
            "    if not tf.is_tensor(num_frames):\n",
            "\n",
            "      def check_num_frames(t):\n",
            "        if tf.is_tensor(t):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 537,
        "API": ".is_tensor(",
        "context": [
            "        if tf.is_tensor(t):\n",
            "          outer_dim = _maybe_static_outer_dim(t)\n",
            "        else:\n",
            "          outer_dim = t.shape[0]\n",
            "        if not tf.is_tensor(outer_dim) and outer_dim != num_frames:\n",
            "          raise ValueError('Expected first dimension to be {}, '\n",
            "                           'but saw outer dim: {}'.format(num_frames,\n",
            "                                                          outer_dim))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 542,
        "API": ".map_structure(",
        "context": [
            "          raise ValueError('Expected first dimension to be {}, '\n",
            "                           'but saw outer dim: {}'.format(num_frames,\n",
            "                                                          outer_dim))\n",
            "\n",
            "      tf.nest.map_structure(\n",
            "          check_num_frames,\n",
            "          (observation, action, policy_info, reward, discount),\n",
            "          expand_composites=False)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 645,
        "API": ".map_structure(",
        "context": [
            "  if next_trajectory is not None:\n",
            "    _validate_rank(next_trajectory.discount, min_rank=1, max_rank=2)\n",
            "\n",
            "  if next_trajectory is None:\n",
            "    next_trajectory = tf.nest.map_structure(\n",
            "        lambda t: composite.slice_from(t, axis=1, start=1), trajectory)\n",
            "    trajectory = tf.nest.map_structure(\n",
            "        lambda t: composite.slice_to(t, axis=1, end=-1), trajectory)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 654,
        "API": ".map_structure(",
        "context": [
            "      action=trajectory.action, state=(), info=trajectory.policy_info)\n",
            "  # TODO(b/130244652): Consider replacing 0 rewards & discounts with ().\n",
            "  time_steps = ts.TimeStep(\n",
            "      trajectory.step_type,\n",
            "      reward=tf.nest.map_structure(tf.zeros_like, trajectory.reward),  # unknown\n",
            "      discount=tf.zeros_like(trajectory.discount),  # unknown\n",
            "      observation=trajectory.observation)\n",
            "  next_time_steps = ts.TimeStep(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 701,
        "API": ".ones_like(",
        "context": [
            "  to:\n",
            "\n",
            "  ```python\n",
            "  next_time_step.discount = (\n",
            "      gamma**(N-1) * tf.ones_like(trajectory.discount[:, 0]))\n",
            "  next_time_step.reward = (\n",
            "      sum_{n=0}^{N-1} gamma**n * trajectory.reward[:, n])\n",
            "  ```\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 727,
        "API": ".shape(",
        "context": [
            "\n",
            "  # Use static values when available, so that we can use XLA when the time\n",
            "  # dimension is fixed.\n",
            "  time_dim = (tf.compat.dimension_value(trajectory.discount.shape[1])\n",
            "              or tf.shape(trajectory.discount)[1])\n",
            "\n",
            "  static_time_dim = tf.get_static_value(time_dim)\n",
            "  if static_time_dim in (0, 1):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 733,
        "API": ".format(",
        "context": [
            "  static_time_dim = tf.get_static_value(time_dim)\n",
            "  if static_time_dim in (0, 1):\n",
            "    raise ValueError(\n",
            "        'Trajectory frame count must be at least 2, but saw {}.  Shape of '\n",
            "        'trajectory.discount: {}'.format(static_time_dim,\n",
            "                                         trajectory.discount.shape))\n",
            "\n",
            "  n = time_dim - 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 744,
        "API": ".map_structure(",
        "context": [
            "\n",
            "  # pylint: disable=g-long-lambda\n",
            "\n",
            "  # Pull out x[:,0] for x in trajectory\n",
            "  first_frame = tf.nest.map_structure(\n",
            "      lambda t: composite.squeeze(\n",
            "          composite.slice_to(t, axis=1, end=1),\n",
            "          axis=1),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 751,
        "API": ".map_structure(",
        "context": [
            "          axis=1),\n",
            "      trajectory)\n",
            "\n",
            "  # Pull out x[:,-1] for x in trajectory\n",
            "  final_frame = tf.nest.map_structure(\n",
            "      lambda t: composite.squeeze(\n",
            "          composite.slice_from(t, axis=1, start=-1),\n",
            "          axis=1),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 781,
        "API": ".map_structure(",
        "context": [
            "\n",
            "  time_steps = ts.TimeStep(\n",
            "      first_frame.step_type,\n",
            "      # unknown\n",
            "      reward=tf.nest.map_structure(\n",
            "          lambda r: np.nan * tf.ones_like(r), first_frame.reward),\n",
            "      # unknown\n",
            "      discount=np.nan * tf.ones_like(first_frame.discount),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 831,
        "API": ".format(",
        "context": [
            "  \"\"\"\n",
            "  rank = len(variable.shape)\n",
            "  if rank < min_rank or rank > max_rank:\n",
            "    raise ValueError(\n",
            "        'Expect variable within rank [{},{}], but got rank {}.'.format(\n",
            "            min_rank, max_rank, rank))\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\trajectories\\trajectory.py",
        "line_number": 842,
        "API": ".map_structure(",
        "context": [
            "  \"\"\"Break experience to transitions.\"\"\"\n",
            "  transitions = to_transition(experience)\n",
            "\n",
            "  if squeeze_time_dim:\n",
            "    transitions = tf.nest.map_structure(lambda x: composite.squeeze(x, 1),\n",
            "                                        transitions)\n",
            "\n",
            "  return transitions\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 72,
        "API": ".enabled(",
        "context": [
            "def check_tf1_allowed():\n",
            "  \"\"\"Raises an error if running in TF1 (non-eager) mode and this is disabled.\"\"\"\n",
            "  if _TF1_MODE_ALLOWED:\n",
            "    return\n",
            "  if not tf2_checker.enabled():\n",
            "    raise RuntimeError(\n",
            "        'You are using TF1 or running TF with eager mode disabled.  '\n",
            "        'TF-Agents no longer supports TF1 mode (except for a shrinking list of '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 111,
        "API": ".executing_eagerly(",
        "context": [
            "    wrapped = [None]\n",
            "\n",
            "    @functools.wraps(fn)\n",
            "    def preconfigured_function(*fn_args, **fn_kwargs):\n",
            "      if tf.executing_eagerly():\n",
            "        return fn(*fn_args, **fn_kwargs)\n",
            "      if wrapped[0] is None:\n",
            "        wrapped[0] = function(*((fn,) + args), **kwargs)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 128,
        "API": ".function(",
        "context": [
            "\n",
            "  Example:\n",
            "\n",
            "  ```python\n",
            "  @common.function()\n",
            "  def my_eager_code(x, y):\n",
            "    ...\n",
            "  ```\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 142,
        "API": ".function(",
        "context": [
            "    A tf.function wrapper.\n",
            "  \"\"\"\n",
            "  autograph = kwargs.pop('autograph', False)\n",
            "  reduce_retracing = kwargs.pop('reduce_retracing', True)\n",
            "  return tf.function(  # allow-tf-function\n",
            "      *args,\n",
            "      autograph=autograph,\n",
            "      reduce_retracing=reduce_retracing,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 152,
        "API": ".executing_eagerly(",
        "context": [
            "\n",
            "def has_eager_been_enabled():\n",
            "  \"\"\"Returns true iff in TF2 or in TF1 with eager execution enabled.\"\"\"\n",
            "  with tf.init_scope():\n",
            "    return tf.executing_eagerly()\n",
            "\n",
            "\n",
            "def function_in_tf1(*args, **kwargs):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 213,
        "API": ".constant(",
        "context": [
            "  check_tf1_allowed()\n",
            "  if has_eager_been_enabled():\n",
            "    if initializer is None:\n",
            "      if shape:\n",
            "        initial_value = tf.constant(initial_value, shape=shape, dtype=dtype)\n",
            "      else:\n",
            "        initial_value = tf.convert_to_tensor(initial_value, dtype=dtype)\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 227,
        "API": ".constant(",
        "context": [
            "  collections = [tf.compat.v1.GraphKeys.GLOBAL_VARIABLES]\n",
            "  if use_local_variable:\n",
            "    collections = [tf.compat.v1.GraphKeys.LOCAL_VARIABLES]\n",
            "  if initializer is None:\n",
            "    initializer = tf.compat.v1.initializers.constant(initial_value, dtype=dtype)\n",
            "    if shape is None:\n",
            "      shape = tf.convert_to_tensor(initial_value).shape\n",
            "  if unique_name:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 296,
        "API": ".format(",
        "context": [
            "    return tf.no_op(name=op_name)\n",
            "  if len(source_variables) != len(target_variables):\n",
            "    raise ValueError(\n",
            "        'Source and target variable lists have different lengths: '\n",
            "        '{} vs. {}'.format(len(source_variables), len(target_variables)))\n",
            "  if sort_variables_by_name:\n",
            "    source_variables = sorted(source_variables, key=lambda x: x.name)\n",
            "    target_variables = sorted(target_variables, key=lambda x: x.name)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 316,
        "API": ".assign(",
        "context": [
            "      else:\n",
            "        current_tau = tau\n",
            "\n",
            "      if current_tau == 1.0:\n",
            "        return v1.assign(v2)\n",
            "      else:\n",
            "        return v1.assign((1 - current_tau) * v1 + current_tau * v2)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 335,
        "API": ".group(",
        "context": [
            "    else:\n",
            "      update = update_fn(v_t, v_s)\n",
            "\n",
            "    updates.append(update)\n",
            "  return tf.group(*updates, name=op_name)\n",
            "\n",
            "\n",
            "def join_scope(parent_scope, child_scope):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 352,
        "API": ".join(",
        "context": [
            "  if not parent_scope:\n",
            "    return child_scope\n",
            "  if not child_scope:\n",
            "    return parent_scope\n",
            "  return '/'.join([parent_scope, child_scope])\n",
            "\n",
            "\n",
            "# TODO(b/138322868): Add an optional action_spec for validation.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 387,
        "API": ".shape(",
        "context": [
            "    # In the multidimensional case, the last dimension of actions indexes the\n",
            "    # vector of actions for each batch, so exclude it from the batch dimensions.\n",
            "    batch_dims -= 1\n",
            "\n",
            "  outer_shape = tf.shape(input=actions)\n",
            "  batch_indices = tf.meshgrid(\n",
            "      *[tf.range(outer_shape[i]) for i in range(batch_dims)], indexing='ij')\n",
            "  batch_indices = [tf.cast(tf.expand_dims(batch_index, -1), dtype=tf.int32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 393,
        "API": ".expand_dims(",
        "context": [
            "      *[tf.range(outer_shape[i]) for i in range(batch_dims)], indexing='ij')\n",
            "  batch_indices = [tf.cast(tf.expand_dims(batch_index, -1), dtype=tf.int32)\n",
            "                   for batch_index in batch_indices]\n",
            "  if not multi_dim_actions:\n",
            "    actions = tf.expand_dims(actions, -1)\n",
            "  # Cast actions to tf.int32 in order to avoid a TypeError in tf.concat.\n",
            "  actions = tf.cast(actions, dtype=tf.int32)\n",
            "  action_indices = tf.concat(batch_indices + [actions], -1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 418,
        "API": ".group(",
        "context": [
            "\n",
            "  Args:\n",
            "    body: callable that returns the tensorflow op to be performed every time an\n",
            "      internal counter is divisible by the period. The op must have no output\n",
            "      (for example, a tf.group()).\n",
            "    period: inverse frequency with which to perform the op.\n",
            "    name: name of the variable_scope.\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 428,
        "API": ".executing_eagerly(",
        "context": [
            "\n",
            "  Returns:\n",
            "    An op that periodically performs the specified op.\n",
            "  \"\"\"\n",
            "  if tf.executing_eagerly():\n",
            "    if isinstance(period, tf.Variable):\n",
            "      return Periodically(body, period, name)\n",
            "    return EagerPeriodically(body, period)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 457,
        "API": ".group(",
        "context": [
            "\n",
            "    Args:\n",
            "      body: callable that returns the tensorflow op to be performed every time\n",
            "        an internal counter is divisible by the period. The op must have no\n",
            "        output (for example, a tf.group()).\n",
            "      period: inverse frequency with which to perform the op. It can be a Tensor\n",
            "        or a Variable.\n",
            "      name: name of the object.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 483,
        "API": ".cast(",
        "context": [
            "      if self._period is None:\n",
            "        return tf.no_op()\n",
            "      if self._period == 1:\n",
            "        return self._body()\n",
            "      period = tf.cast(self._period, self._counter.dtype)\n",
            "      remainder = tf.math.mod(self._counter.assign_add(1), period)\n",
            "      return tf.cond(\n",
            "          pred=tf.equal(remainder, 0), true_fn=self._body, false_fn=tf.no_op)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 508,
        "API": ".group(",
        "context": [
            "\n",
            "    Args:\n",
            "      body: callable that returns the tensorflow op to be performed every time\n",
            "        an internal counter is divisible by the period. The op must have no\n",
            "        output (for example, a tf.group()).\n",
            "      period: inverse frequency with which to perform the op. Must be a simple\n",
            "        python int/long.\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 544,
        "API": ".clip_by_value(",
        "context": [
            "\n",
            "  Returns:\n",
            "    clipped_value: (tensor) `value` clipped to be compatible with `spec`.\n",
            "  \"\"\"\n",
            "  return tf.clip_by_value(value, spec.minimum, spec.maximum)\n",
            "\n",
            "\n",
            "def spec_means_and_magnitudes(action_spec):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 549,
        "API": ".map_structure(",
        "context": [
            "\n",
            "\n",
            "def spec_means_and_magnitudes(action_spec):\n",
            "  \"\"\"Get the center and magnitude of the ranges in action spec.\"\"\"\n",
            "  action_means = tf.nest.map_structure(\n",
            "      lambda spec: (spec.maximum + spec.minimum) / 2.0, action_spec)\n",
            "  action_magnitudes = tf.nest.map_structure(\n",
            "      lambda spec: (spec.maximum - spec.minimum) / 2.0, action_spec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 554,
        "API": ".array(",
        "context": [
            "      lambda spec: (spec.maximum + spec.minimum) / 2.0, action_spec)\n",
            "  action_magnitudes = tf.nest.map_structure(\n",
            "      lambda spec: (spec.maximum - spec.minimum) / 2.0, action_spec)\n",
            "  return np.array(\n",
            "      action_means, dtype=np.float32), np.array(\n",
            "          action_magnitudes, dtype=np.float32)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 568,
        "API": ".reshape(",
        "context": [
            "\n",
            "  Returns:\n",
            "    A batch scaled the given spec bounds.\n",
            "  \"\"\"\n",
            "  tensor = tf.reshape(tensor, [-1] + spec.shape.as_list())\n",
            "\n",
            "  # Scale the tensor.\n",
            "  means, magnitudes = spec_means_and_magnitudes(spec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 575,
        "API": ".cast(",
        "context": [
            "  means, magnitudes = spec_means_and_magnitudes(spec)\n",
            "  tensor = means + magnitudes * tensor\n",
            "\n",
            "  # Set type.\n",
            "  return tf.cast(tensor, spec.dtype)\n",
            "\n",
            "\n",
            "def ornstein_uhlenbeck_process(initial_value,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 607,
        "API": ".executing_eagerly(",
        "context": [
            "\n",
            "  Returns:\n",
            "    An op that generates noise.\n",
            "  \"\"\"\n",
            "  if tf.executing_eagerly():\n",
            "    return OUProcess(initial_value, damping, stddev, seed, scope)\n",
            "  else:\n",
            "    return OUProcess(initial_value, damping, stddev, seed, scope)()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 652,
        "API": ".normal(",
        "context": [
            "      self._x = tf.compat.v2.Variable(\n",
            "          initial_value=initial_value, trainable=False)\n",
            "\n",
            "  def __call__(self):\n",
            "    noise = tf.random.normal(\n",
            "        shape=self._x.shape,\n",
            "        stddev=self._stddev,\n",
            "        dtype=self._x.dtype,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 657,
        "API": ".assign(",
        "context": [
            "        shape=self._x.shape,\n",
            "        stddev=self._stddev,\n",
            "        dtype=self._x.dtype,\n",
            "        seed=self._seed)\n",
            "    return self._x.assign((1. - self._damping) * self._x + noise)\n",
            "\n",
            "\n",
            "def log_probability(distributions, actions, action_spec):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 678,
        "API": ".reduce_sum(",
        "context": [
            "    # sum log-probs over everything but the batch\n",
            "    single_log_prob = single_distribution.log_prob(single_action)\n",
            "    rank = single_log_prob.shape.rank\n",
            "    reduce_dims = list(range(outer_rank, rank))\n",
            "    return tf.reduce_sum(\n",
            "        input_tensor=single_log_prob,\n",
            "        axis=reduce_dims)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 686,
        "API": ".flatten(",
        "context": [
            "  nest_utils.assert_same_structure(distributions, actions)\n",
            "  log_probs = [\n",
            "      _compute_log_prob(dist, action)\n",
            "      for (dist, action\n",
            "          ) in zip(tf.nest.flatten(distributions), tf.nest.flatten(actions))\n",
            "  ]\n",
            "\n",
            "  # sum log-probs over action tuple\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 711,
        "API": ".map_structure(",
        "context": [
            "    Assumes actions are independent, so that marginal entropies of each action\n",
            "    may be summed.\n",
            "  \"\"\"\n",
            "  if outer_rank is None:\n",
            "    nested_modes = tf.nest.map_structure(lambda d: d.mode(), distributions)\n",
            "    outer_rank = nest_utils.get_outer_rank(nested_modes, action_spec)\n",
            "\n",
            "  def _compute_entropy(single_distribution):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 720,
        "API": ".reduce_sum(",
        "context": [
            "      entropies = single_distribution.entropy()\n",
            "      # Sum entropies over everything but the batch.\n",
            "      rank = entropies.shape.rank\n",
            "      reduce_dims = list(range(outer_rank, rank))\n",
            "      return tf.reduce_sum(input_tensor=entropies, axis=reduce_dims)\n",
            "    except NotImplementedError:\n",
            "      return None\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 725,
        "API": ".flatten(",
        "context": [
            "    except NotImplementedError:\n",
            "      return None\n",
            "\n",
            "  entropies = []\n",
            "  for dist in tf.nest.flatten(distributions):\n",
            "    entropy_dist = _compute_entropy(dist)\n",
            "    if entropy_dist is not None:\n",
            "      entropies.append(entropy_dist)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 734,
        "API": ".add_n(",
        "context": [
            "  # Sum entropies over action tuple.\n",
            "  if not entropies:\n",
            "    return None\n",
            "\n",
            "  return tf.add_n(entropies)\n",
            "\n",
            "\n",
            "def discounted_future_sum(values, gamma, num_steps):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 766,
        "API": ".minimum(",
        "context": [
            "                     values.get_shape().rank)\n",
            "\n",
            "  (batch_size, total_steps) = values.get_shape().as_list()\n",
            "\n",
            "  num_steps = tf.minimum(num_steps, total_steps)\n",
            "  discount_filter = tf.reshape(gamma**tf.cast(tf.range(num_steps), tf.float32),\n",
            "                               [-1, 1, 1])\n",
            "  padded_values = tf.concat([values, tf.zeros([batch_size, num_steps - 1])], 1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 771,
        "API": ".squeeze(",
        "context": [
            "  discount_filter = tf.reshape(gamma**tf.cast(tf.range(num_steps), tf.float32),\n",
            "                               [-1, 1, 1])\n",
            "  padded_values = tf.concat([values, tf.zeros([batch_size, num_steps - 1])], 1)\n",
            "\n",
            "  convolved_values = tf.squeeze(\n",
            "      tf.nn.conv1d(\n",
            "          input=tf.expand_dims(padded_values, -1),\n",
            "          filters=discount_filter,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 806,
        "API": ".cast(",
        "context": [
            "  if total_steps is None:\n",
            "    raise ValueError('total_steps dimension in input '\n",
            "                     'values[batch_size, total_steps] must be fully defined.')\n",
            "\n",
            "  episode_mask = tf.cast(\n",
            "      tf.sequence_mask(episode_lengths, total_steps), tf.float32)\n",
            "  values *= episode_mask\n",
            "  return discounted_future_sum(values, gamma, num_steps)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 835,
        "API": ".minimum(",
        "context": [
            "    raise ValueError('Input must be rank 2 tensor.  Got %d.' %\n",
            "                     values.get_shape().rank)\n",
            "\n",
            "  (batch_size, total_steps) = values.get_shape().as_list()\n",
            "  num_steps = tf.minimum(num_steps, total_steps)\n",
            "\n",
            "  if final_values is None:\n",
            "    final_values = tf.zeros([batch_size])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 840,
        "API": ".expand_dims(",
        "context": [
            "\n",
            "  if final_values is None:\n",
            "    final_values = tf.zeros([batch_size])\n",
            "\n",
            "  padding_exponent = tf.expand_dims(\n",
            "      tf.cast(tf.range(num_steps, 0, -1), tf.float32), 0)\n",
            "  final_pad = tf.expand_dims(final_values, 1) * gamma**padding_exponent\n",
            "  return tf.concat([\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 857,
        "API": ".cast(",
        "context": [
            "\n",
            "  Returns:\n",
            "    A float32 Tensor with 0s where step_type == LAST and 1s otherwise.\n",
            "  \"\"\"\n",
            "  episode_mask = tf.cast(\n",
            "      tf.not_equal(time_steps.step_type, ts.StepType.LAST), tf.float32)\n",
            "  return episode_mask\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 876,
        "API": ".equal(",
        "context": [
            "      only contiguous components. Each row will be of the form\n",
            "      [1.0] * a + [0.0] * b, where a >= 1 and b >= 0, and in which the initial\n",
            "      sequence of ones corresponds to a contiguous sub-episode.\n",
            "  \"\"\"\n",
            "  episode_end = tf.equal(next_time_steps_discount,\n",
            "                         tf.constant(0, dtype=next_time_steps_discount.dtype))\n",
            "  mask = tf.math.cumprod(\n",
            "      1.0 - tf.cast(episode_end, tf.float32), axis=1, exclusive=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 893,
        "API": ".softmax(",
        "context": [
            "\n",
            "  Returns:\n",
            "    A Tensor containing the expected Q-values.\n",
            "  \"\"\"\n",
            "  probabilities = tf.nn.softmax(logits)\n",
            "  return tf.reduce_sum(input_tensor=support * probabilities, axis=-1)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 906,
        "API": ".histogram(",
        "context": [
            "    tensor: The tensor to generate summaries of.\n",
            "    step: Variable to use for summaries.\n",
            "  \"\"\"\n",
            "  with tf.name_scope(tag):\n",
            "    tf.compat.v2.summary.histogram(name='histogram', data=tensor, step=step)\n",
            "    tf.compat.v2.summary.scalar(\n",
            "        name='mean', data=tf.reduce_mean(input_tensor=tensor), step=step)\n",
            "    tf.compat.v2.summary.scalar(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 911,
        "API": ".abs(",
        "context": [
            "    tf.compat.v2.summary.scalar(\n",
            "        name='mean', data=tf.reduce_mean(input_tensor=tensor), step=step)\n",
            "    tf.compat.v2.summary.scalar(\n",
            "        name='mean_abs',\n",
            "        data=tf.reduce_mean(input_tensor=tf.abs(tensor)),\n",
            "        step=step)\n",
            "    tf.compat.v2.summary.scalar(\n",
            "        name='max', data=tf.reduce_max(input_tensor=tensor), step=step)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 916,
        "API": ".reduce_min(",
        "context": [
            "        step=step)\n",
            "    tf.compat.v2.summary.scalar(\n",
            "        name='max', data=tf.reduce_max(input_tensor=tensor), step=step)\n",
            "    tf.compat.v2.summary.scalar(\n",
            "        name='min', data=tf.reduce_min(input_tensor=tensor), step=step)\n",
            "    tf.compat.v2.summary.scalar(\n",
            "        name='std', data=tf.math.reduce_std(input_tensor=tensor), step=step)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 951,
        "API": ".shape(",
        "context": [
            "  \"\"\"\n",
            "  rewards.shape.assert_is_compatible_with(discounts.shape)\n",
            "  if (not rewards.shape.is_fully_defined() or\n",
            "      not discounts.shape.is_fully_defined()):\n",
            "    tf.debugging.assert_equal(tf.shape(input=rewards),\n",
            "                              tf.shape(input=discounts))\n",
            "\n",
            "  def discounted_accumulate_rewards(next_step_return, reward_and_discount):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 960,
        "API": ".transpose(",
        "context": [
            "    return next_step_return * discount + reward\n",
            "\n",
            "  # Support batched rewards and discount via transpose.\n",
            "  if rewards.shape.rank > 1 and not time_major:\n",
            "    rewards = tf.transpose(rewards, perm=[1, 0])\n",
            "    discounts = tf.transpose(discounts, perm=[1, 0])\n",
            "  # Cumulatively sum discounted reward R_t.\n",
            "  #   R_t = r_t + discount * (r_t+1 + discount * (r_t+2 * discount( ...\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 966,
        "API": ".scan(",
        "context": [
            "  # Cumulatively sum discounted reward R_t.\n",
            "  #   R_t = r_t + discount * (r_t+1 + discount * (r_t+2 * discount( ...\n",
            "  # As discount is 0 for terminal states, ends of episode will not include\n",
            "  #   reward from subsequent timesteps.\n",
            "  returns = tf.scan(\n",
            "      discounted_accumulate_rewards, [rewards, discounts],\n",
            "      initializer=tf.zeros_like(rewards[0]),\n",
            "      reverse=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 972,
        "API": ".transpose(",
        "context": [
            "      initializer=tf.zeros_like(rewards[0]),\n",
            "      reverse=True)\n",
            "  # Reverse transpose if needed.\n",
            "  if returns.shape.rank > 1 and not time_major:\n",
            "    returns = tf.transpose(returns, perm=[1, 0])\n",
            "  return returns\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 987,
        "API": ".info(",
        "context": [
            "  for flag, v in zip(is_initialized, var_list):\n",
            "    if not flag:\n",
            "      uninitialized_vars.append(v)\n",
            "  if uninitialized_vars:\n",
            "    logging.info('uninitialized_vars: %s',\n",
            "                 ', '.join([str(x) for x in uninitialized_vars]))\n",
            "    session.run(tf.compat.v1.variables_initializer(uninitialized_vars))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 1009,
        "API": ".makedirs(",
        "context": [
            "    \"\"\"\n",
            "    self._checkpoint = tf.train.Checkpoint(**kwargs)\n",
            "\n",
            "    if not tf.io.gfile.exists(ckpt_dir):\n",
            "      tf.io.gfile.makedirs(ckpt_dir)\n",
            "\n",
            "    self._manager = tf.train.CheckpointManager(\n",
            "        self._checkpoint, directory=ckpt_dir, max_to_keep=max_to_keep)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 1015,
        "API": ".info(",
        "context": [
            "    self._manager = tf.train.CheckpointManager(\n",
            "        self._checkpoint, directory=ckpt_dir, max_to_keep=max_to_keep)\n",
            "\n",
            "    if self._manager.latest_checkpoint is not None:\n",
            "      logging.info('Checkpoint available: %s', self._manager.latest_checkpoint)\n",
            "      self._checkpoint_exists = True\n",
            "    else:\n",
            "      logging.info('No checkpoint available at %s', ckpt_dir)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 1020,
        "API": ".restore(",
        "context": [
            "      self._checkpoint_exists = True\n",
            "    else:\n",
            "      logging.info('No checkpoint available at %s', ckpt_dir)\n",
            "      self._checkpoint_exists = False\n",
            "    self._load_status = self._checkpoint.restore(\n",
            "        self._manager.latest_checkpoint)\n",
            "\n",
            "  @property\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 1040,
        "API": ".save(",
        "context": [
            "\n",
            "  def save(self, global_step: tf.Tensor,\n",
            "           options: tf.train.CheckpointOptions = None):\n",
            "    \"\"\"Save state to checkpoint.\"\"\"\n",
            "    saved_checkpoint = self._manager.save(\n",
            "        checkpoint_number=global_step, options=options)\n",
            "    self._checkpoint_exists = True\n",
            "    logging.info('%s', 'Saved checkpoint: {}'.format(saved_checkpoint))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 1066,
        "API": ".convert_to_tensor(",
        "context": [
            "\n",
            "  Raises:\n",
            "    ValueError: when the outer shape is incorrect.\n",
            "  \"\"\"\n",
            "  outer_shape = tf.convert_to_tensor(value=outer_shape)\n",
            "  if len(outer_shape.shape) != 1:\n",
            "    raise ValueError('The outer shape must be a 1D tensor')\n",
            "  outer_ndims = int(outer_shape.shape[0])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 1077,
        "API": ".shape(",
        "context": [
            "  if outer_ndims == 0:\n",
            "    return tensor\n",
            "\n",
            "  # Calculate target shape of replicated tensor\n",
            "  target_shape = tf.concat([outer_shape, tf.shape(input=tensor)], axis=0)\n",
            "\n",
            "  # tf.tile expects `tensor` to be at least 1D\n",
            "  if tensor_ndims == 0:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 1084,
        "API": ".tile(",
        "context": [
            "  if tensor_ndims == 0:\n",
            "    tensor = tensor[None]\n",
            "\n",
            "  # Replicate tensor \"t\" along the 1st dimension.\n",
            "  tiled_tensor = tf.tile(tensor, [tf.reduce_prod(input_tensor=outer_shape)] +\n",
            "                         [1] * (tensor_ndims - 1))\n",
            "\n",
            "  # Reshape to match outer_shape.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 1135,
        "API": ".format(",
        "context": [
            "  ]\n",
            "  if overridden_members:\n",
            "    raise ValueError(\n",
            "        'Subclasses of {} cannot override most of its base members, but '\n",
            "        '{} overrides: {}'.format(base_cls, instance_type, overridden_members))\n",
            "\n",
            "\n",
            "def element_wise_squared_loss(x, y):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 1164,
        "API": ".rank(",
        "context": [
            "  x_static_shape = x.get_shape()\n",
            "  if x_static_shape.rank is not None and x_static_shape.rank < 2:\n",
            "    return x\n",
            "\n",
            "  x_rank = tf.rank(x)\n",
            "  x_t = tf.transpose(a=x, perm=tf.concat(([1, 0], tf.range(2, x_rank)), axis=0))\n",
            "  x_t.set_shape(\n",
            "      tf.TensorShape(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 1188,
        "API": ".makedirs(",
        "context": [
            "  spec_proto = nested_structure_coder.encode_structure(spec)\n",
            "\n",
            "  dir_path = os.path.dirname(file_path)\n",
            "  if not tf.io.gfile.exists(dir_path):\n",
            "    tf.io.gfile.makedirs(dir_path)\n",
            "\n",
            "  with tf.compat.v2.io.gfile.GFile(file_path, 'wb') as gfile:\n",
            "    gfile.write(spec_proto.SerializeToString())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 1267,
        "API": ".format(",
        "context": [
            "        'layers/weights with the input network.  If you are not intending to '\n",
            "        'share weights make sure all the weights are created inside the Network'\n",
            "        ' since a copy will be created by creating a new Network with the same '\n",
            "        'args but a new name. Shared variables found: '\n",
            "        '\\'{}\\'.'.format(\n",
            "            network_1.name, network_2.name,\n",
            "            [x.name for x in shared_variables]))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 1287,
        "API": ".format(",
        "context": [
            "      (e.g. user must call `create_variables`).\n",
            "  \"\"\"\n",
            "  if network_1.input_tensor_spec != network_2.input_tensor_spec:\n",
            "    raise ValueError('Input tensor specs of network and target network '\n",
            "                     'do not match: {} vs. {}.'.format(\n",
            "                         network_1.input_tensor_spec,\n",
            "                         network_2.input_tensor_spec))\n",
            "  if len(network_1.variables) != len(network_2.variables):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 1293,
        "API": ".format(",
        "context": [
            "                         network_2.input_tensor_spec))\n",
            "  if len(network_1.variables) != len(network_2.variables):\n",
            "    raise ValueError(\n",
            "        'Variables lengths do not match between Q network and target network: '\n",
            "        '{} vs. {}'.format(network_1.variables, network_2.variables))\n",
            "  for v1, v2 in zip(network_1.variables, network_2.variables):\n",
            "    if v1.dtype != v2.dtype or v1.shape != v2.shape:\n",
            "      raise ValueError(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 1305,
        "API": ".copy(",
        "context": [
            "                                          name=None,\n",
            "                                          input_spec=None):\n",
            "  \"\"\"Copies the network into target if None and checks for shared variables.\"\"\"\n",
            "  if target_network is None:\n",
            "    target_network = network.copy(name=name)\n",
            "    target_network.create_variables(input_spec)\n",
            "  # Copy may have been shallow, and variables may inadvertently be shared\n",
            "  # between the target and the original networks. This would be an unusual\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 1347,
        "API": ".convert_to_tensor(",
        "context": [
            "    An AggregatedLosses named tuple with scalar losses to optimize.\n",
            "  \"\"\"\n",
            "  total_loss, weighted_loss, reg_loss = None, None, None\n",
            "  if sample_weight is not None and not isinstance(sample_weight, tf.Tensor):\n",
            "    sample_weight = tf.convert_to_tensor(sample_weight, dtype=tf.float32)\n",
            "\n",
            "  # Compute loss that is scaled by global batch size.\n",
            "  if per_example_loss is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 1358,
        "API": ".expand_dims(",
        "context": [
            "      # Expand `sample_weight` to be broadcastable to the shape of\n",
            "      # `per_example_loss`, to ensure that multiplication works properly.\n",
            "      if weight_rank > 0 and loss_rank > weight_rank:\n",
            "        for dim in range(weight_rank, loss_rank):\n",
            "          sample_weight = tf.expand_dims(sample_weight, dim)\n",
            "      # Sometimes we have an episode boundary or similar, and at this location\n",
            "      # the loss is nonsensical (i.e., inf or nan); and sample_weight is zero.\n",
            "      # In this case, we should respect the zero sample_weight and ignore the\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 1371,
        "API": ".format(",
        "context": [
            "      err_msg = (\n",
            "          'Need to use a loss function that computes losses per sample, ex: '\n",
            "          'replace losses.mean_squared_error with tf.math.squared_difference. '\n",
            "          'Invalid value passed for `per_example_loss`. Expected a tensor '\n",
            "          'tensor with at least rank 1, received: {}'.format(per_example_loss))\n",
            "      if tf.distribute.has_strategy():\n",
            "        raise ValueError(err_msg)\n",
            "      else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 1377,
        "API": ".expand_dims(",
        "context": [
            "        raise ValueError(err_msg)\n",
            "      else:\n",
            "        logging.warning(err_msg)\n",
            "        # Add extra dimension to prevent error in compute_average_loss.\n",
            "        per_example_loss = tf.expand_dims(per_example_loss, 0)\n",
            "    elif loss_rank > 1:\n",
            "      # If per_example_loss is shaped [B, T, ...], we need to compute the mean\n",
            "      # across the extra dimensions, ex. time, as well.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 1383,
        "API": ".cast(",
        "context": [
            "      # If per_example_loss is shaped [B, T, ...], we need to compute the mean\n",
            "      # across the extra dimensions, ex. time, as well.\n",
            "      per_example_loss = tf.reduce_mean(per_example_loss, range(1, loss_rank))\n",
            "\n",
            "    global_batch_size = global_batch_size and tf.cast(global_batch_size,\n",
            "                                                      per_example_loss.dtype)\n",
            "    weighted_loss = tf.nn.compute_average_loss(\n",
            "        per_example_loss,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\common.py",
        "line_number": 1404,
        "API": ".scalar(",
        "context": [
            "  if name_data:\n",
            "    with tf.name_scope(name_scope):\n",
            "      for name, data in name_data.items():\n",
            "        if data is not None:\n",
            "          tf.compat.v2.summary.scalar(\n",
            "              name=name, data=data, step=step)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\composite.py",
        "line_number": 29,
        "API": ".shape(",
        "context": [
            "def shape(tensor):\n",
            "  if isinstance(tensor, tf.SparseTensor):\n",
            "    return tensor.dense_shape\n",
            "  else:\n",
            "    return tf.shape(input=tensor, out_type=tf.int64)\n",
            "\n",
            "\n",
            "def reshape(t, shape):  # pylint: disable=redefined-outer-name\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\composite.py",
        "line_number": 42,
        "API": ".reshape(",
        "context": [
            "\n",
            "  Returns:\n",
            "    The reshaped tensor.\n",
            "  \"\"\"\n",
            "  return (tf.sparse.reshape(t, shape) if isinstance(t, tf.SparseTensor)\n",
            "          else tf.reshape(t, shape))\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\composite.py",
        "line_number": 63,
        "API": ".reduce_all(",
        "context": [
            "  \"\"\"\n",
            "  if isinstance(t, tf.SparseTensor):\n",
            "    # Fill in a dummy value if there are no elements in the tensor.\n",
            "    indices_axis = t.indices[:, axis]\n",
            "    all_zero = tf.reduce_all(tf.equal(indices_axis, 0))\n",
            "    with tf.control_dependencies([\n",
            "        tf.Assert(\n",
            "            all_zero,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\composite.py",
        "line_number": 70,
        "API": ".concat(",
        "context": [
            "            all_zero,\n",
            "            ['Unable to squeeze SparseTensor {} axis {} '\n",
            "             'because indices are not all equal to 0:', indices_axis])]):\n",
            "      return tf.SparseTensor(\n",
            "          indices=tf.concat(\n",
            "              (t.indices[:, :axis], t.indices[:, axis + 1:]),\n",
            "              axis=1),\n",
            "          values=t.values,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\composite.py",
        "line_number": 78,
        "API": ".squeeze(",
        "context": [
            "          dense_shape=tf.concat(\n",
            "              (t.dense_shape[:axis], t.dense_shape[axis + 1:]),\n",
            "              axis=0))\n",
            "  else:\n",
            "    return tf.squeeze(t, [axis])\n",
            "\n",
            "\n",
            "def expand_dims(t, axis):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\composite.py",
        "line_number": 95,
        "API": ".is_tensor(",
        "context": [
            "  Raises:\n",
            "    NotImplementedError: If `t` is a `SparseTensor` and `axis != 0`.\n",
            "  \"\"\"\n",
            "  if isinstance(t, tf.SparseTensor):\n",
            "    if tf.is_tensor(axis) or axis != 0:\n",
            "      raise NotImplementedError(\n",
            "          'Can only expand_dims on SparseTensor {} on static axis 0, '\n",
            "          'but received axis {}'.format(t, axis))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\composite.py",
        "line_number": 101,
        "API": ".shape(",
        "context": [
            "          'Can only expand_dims on SparseTensor {} on static axis 0, '\n",
            "          'but received axis {}'.format(t, axis))\n",
            "    n_elem = (\n",
            "        t.indices.shape[0] or tf.get_static_shape(t.dense_shape)[0]\n",
            "        or tf.shape(t.indices)[0])\n",
            "    shape_ = tf.cast(t.shape, tf.int64)\n",
            "    return tf.SparseTensor(\n",
            "        indices=tf.concat((tf.zeros([n_elem, 1], dtype=tf.int64),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\composite.py",
        "line_number": 108,
        "API": ".concat(",
        "context": [
            "        indices=tf.concat((tf.zeros([n_elem, 1], dtype=tf.int64),\n",
            "                           t.indices),\n",
            "                          axis=1),\n",
            "        values=t.values,\n",
            "        dense_shape=tf.concat(([1], shape_), axis=0))\n",
            "  else:\n",
            "    return tf.expand_dims(t, axis)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\composite.py",
        "line_number": 133,
        "API": ".is_tensor(",
        "context": [
            "  Returns:\n",
            "    The sliced composite tensor.\n",
            "  \"\"\"\n",
            "  if isinstance(tensor, tf.SparseTensor):\n",
            "    if not tf.is_tensor(start) and start < 0:\n",
            "      start = tensor.dense_shape[axis] + start\n",
            "    all_but_first = tf.reshape(\n",
            "        tf.where(tensor.indices[:, axis] >= start),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\composite.py",
        "line_number": 138,
        "API": ".gather(",
        "context": [
            "      start = tensor.dense_shape[axis] + start\n",
            "    all_but_first = tf.reshape(\n",
            "        tf.where(tensor.indices[:, axis] >= start),\n",
            "        [-1])\n",
            "    indices = tf.gather(tensor.indices, all_but_first)\n",
            "    indices = tf.unstack(indices, axis=1)\n",
            "    indices = tf.stack(indices[:axis]\n",
            "                       + [indices[axis] - start]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\composite.py",
        "line_number": 144,
        "API": ".unstack(",
        "context": [
            "    indices = tf.stack(indices[:axis]\n",
            "                       + [indices[axis] - start]\n",
            "                       + indices[axis + 1:],\n",
            "                       axis=1)\n",
            "    new_shape = tf.unstack(tensor.dense_shape)\n",
            "    new_shape[axis] = new_shape[axis] - start\n",
            "    return tf.SparseTensor(\n",
            "        indices=indices,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\composite.py",
        "line_number": 149,
        "API": ".stack(",
        "context": [
            "    new_shape[axis] = new_shape[axis] - start\n",
            "    return tf.SparseTensor(\n",
            "        indices=indices,\n",
            "        values=tf.gather(tensor.values, all_but_first),\n",
            "        dense_shape=tf.stack(new_shape))\n",
            "  else:\n",
            "    ndims = len(tensor.shape)\n",
            "    if ndims is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\composite.py",
        "line_number": 154,
        "API": ".format(",
        "context": [
            "  else:\n",
            "    ndims = len(tensor.shape)\n",
            "    if ndims is None:\n",
            "      raise ValueError(\n",
            "          'Unable to slice a tensor with unknown rank: {}'.format(tensor))\n",
            "    slices = tuple([slice(None)] * axis\n",
            "                   + [slice(start, None)]\n",
            "                   + [slice(None)] * (ndims - axis - 1))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\composite.py",
        "line_number": 181,
        "API": ".is_tensor(",
        "context": [
            "  Returns:\n",
            "    The sliced composite tensor.\n",
            "  \"\"\"\n",
            "  if isinstance(tensor, tf.SparseTensor):\n",
            "    if not tf.is_tensor(end) and end < 0:\n",
            "      end = tensor.dense_shape[axis] + end\n",
            "    all_but_first = tf.reshape(\n",
            "        tf.where(tensor.indices[:, axis] < end),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\composite.py",
        "line_number": 186,
        "API": ".unstack(",
        "context": [
            "      end = tensor.dense_shape[axis] + end\n",
            "    all_but_first = tf.reshape(\n",
            "        tf.where(tensor.indices[:, axis] < end),\n",
            "        [-1])\n",
            "    new_shape = tf.unstack(tensor.dense_shape)\n",
            "    new_shape[axis] = end\n",
            "    return tf.SparseTensor(\n",
            "        indices=tf.gather(tensor.indices, all_but_first),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\composite.py",
        "line_number": 191,
        "API": ".stack(",
        "context": [
            "    new_shape[axis] = end\n",
            "    return tf.SparseTensor(\n",
            "        indices=tf.gather(tensor.indices, all_but_first),\n",
            "        values=tf.gather(tensor.values, all_but_first),\n",
            "        dense_shape=tf.stack(new_shape))\n",
            "  else:\n",
            "    ndims = len(tensor.shape)\n",
            "    if ndims is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\composite.py",
        "line_number": 196,
        "API": ".format(",
        "context": [
            "  else:\n",
            "    ndims = len(tensor.shape)\n",
            "    if ndims is None:\n",
            "      raise ValueError(\n",
            "          'Unable to slice a tensor with unknown rank: {}'.format(tensor))\n",
            "    slices = tuple([slice(None)] * axis\n",
            "                   + [slice(None, end)]\n",
            "                   + [slice(None)] * (ndims - axis - 1))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 95,
        "API": ".copy(",
        "context": [
            "\n",
            "  def __init__(self, func_or_method, *args, **kwargs):\n",
            "    self._func_or_method = func_or_method\n",
            "    self._args = args\n",
            "    self._kwargs = copy.copy(kwargs)\n",
            "    getargspec = inspect.getargspec if six.PY2 else inspect.getfullargspec\n",
            "    arg_names = getargspec(func_or_method).args\n",
            "    self._arg_names = arg_names\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 123,
        "API": ".copy(",
        "context": [
            "      The result of func_or_method(*args, **kwargs).\n",
            "    \"\"\"\n",
            "    # By default use the init args.\n",
            "    call_args = args or self._args\n",
            "    call_kwargs = copy.copy(self._kwargs)\n",
            "    for arg_name in self._arg_names[:len(args)]:\n",
            "      # Remove any original kwargs replaced by new positional args.\n",
            "      call_kwargs.pop(arg_name, None)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 166,
        "API": ".executing_eagerly(",
        "context": [
            "  if not callable(func_or_method):\n",
            "    raise TypeError('func_or_method must be callable.')\n",
            "\n",
            "  def decorator(*args, **kwargs):\n",
            "    if tf.executing_eagerly():\n",
            "      return Future(func_or_method, *args, **kwargs)\n",
            "    else:\n",
            "      return func_or_method(*args, **kwargs)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 188,
        "API": ".histogram(",
        "context": [
            "        var_values = var.values\n",
            "      else:\n",
            "        var_values = var\n",
            "      var_name = var.name.replace(':', '_')\n",
            "      tf.compat.v2.summary.histogram(\n",
            "          name=var_name + '_value', data=var_values, step=step)\n",
            "      tf.compat.v2.summary.scalar(\n",
            "          name=var_name + '_value_norm',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 211,
        "API": ".histogram(",
        "context": [
            "          grad_values = grad.values\n",
            "        else:\n",
            "          grad_values = grad\n",
            "        var_name = var.name.replace(':', '_')\n",
            "        tf.compat.v2.summary.histogram(\n",
            "            name=var_name + '_gradient', data=grad_values, step=step)\n",
            "        tf.compat.v2.summary.scalar(\n",
            "            name=var_name + '_gradient_norm',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 218,
        "API": ".info(",
        "context": [
            "            name=var_name + '_gradient_norm',\n",
            "            data=tf.linalg.global_norm([grad_values]),\n",
            "            step=step)\n",
            "      else:\n",
            "        logging.info('Var %s has no gradient', var.name)\n",
            "\n",
            "\n",
            "def clip_gradient_norms(gradients_to_variables, max_norm):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 235,
        "API": ".clip_by_norm(",
        "context": [
            "  clipped_grads_and_vars = []\n",
            "  for grad, var in gradients_to_variables:\n",
            "    if grad is not None:\n",
            "      if isinstance(grad, tf.IndexedSlices):\n",
            "        tmp = tf.clip_by_norm(grad.values, max_norm)\n",
            "        grad = tf.IndexedSlices(tmp, grad.indices, grad.dense_shape)\n",
            "      else:\n",
            "        grad = tf.clip_by_norm(grad, max_norm)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 305,
        "API": ".executing_eagerly(",
        "context": [
            "  if not callable(total_loss_fn):\n",
            "    raise ValueError('`total_loss_fn` should be a function.')\n",
            "  if not common.resource_variables_enabled():\n",
            "    raise RuntimeError(common.MISSING_RESOURCE_VARIABLES_ERROR)\n",
            "  if not tf.executing_eagerly():\n",
            "    if callable(loss):\n",
            "      loss = loss()\n",
            "    if callable(variables_to_train):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 312,
        "API": ".control_dependencies(",
        "context": [
            "    if callable(variables_to_train):\n",
            "      variables_to_train = variables_to_train()\n",
            "    # Calculate loss first, then calculate train op, then return the original\n",
            "    # loss conditioned on executing the train op.\n",
            "    with tf.control_dependencies(tf.nest.flatten(loss)):\n",
            "      loss = tf.nest.map_structure(\n",
            "          lambda t: tf.identity(t, 'loss_pre_train'), loss)\n",
            "    train_op = create_train_op(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 327,
        "API": ".control_dependencies(",
        "context": [
            "        gate_gradients=gate_gradients,\n",
            "        aggregation_method=aggregation_method,\n",
            "        check_numerics=check_numerics)\n",
            "\n",
            "    with tf.control_dependencies([train_op]):\n",
            "      return tf.nest.map_structure(\n",
            "          lambda t: tf.identity(t, 'loss_post_train'), loss)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 338,
        "API": ".warning(",
        "context": [
            "  if not callable(loss):\n",
            "    raise ValueError('`loss` should be a function in eager mode.')\n",
            "\n",
            "  if not isinstance(loss, Future):\n",
            "    logging.warning('loss should be an instance of eager_utils.Future')\n",
            "\n",
            "  with tf.GradientTape() as tape:\n",
            "    loss_value = loss()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 347,
        "API": ".flatten(",
        "context": [
            "  if variables_to_train is None:\n",
            "    variables_to_train = tape.watched_variables()\n",
            "  elif callable(variables_to_train):\n",
            "    variables_to_train = variables_to_train()\n",
            "  variables_to_train = tf.nest.flatten(variables_to_train)\n",
            "  grads = tape.gradient(total_loss_value, variables_to_train)\n",
            "  grads_and_vars = list(zip(grads, variables_to_train))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 416,
        "API": ".warning(",
        "context": [
            "    update_ops = global_update_ops\n",
            "  else:\n",
            "    update_ops = set(update_ops)\n",
            "  if not global_update_ops.issubset(update_ops):\n",
            "    tf.compat.v1.logging.warning(\n",
            "        'update_ops in create_train_op does not contain all the '\n",
            "        'update_ops in GraphKeys.UPDATE_OPS')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 422,
        "API": ".control_dependencies(",
        "context": [
            "        'update_ops in GraphKeys.UPDATE_OPS')\n",
            "\n",
            "  # Make sure update_ops are computed before total_loss.\n",
            "  if update_ops:\n",
            "    with tf.control_dependencies(update_ops):\n",
            "      barrier = tf.no_op(name='update_barrier')\n",
            "    with tf.control_dependencies([barrier]):\n",
            "      total_loss = tf.identity(total_loss)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 464,
        "API": ".control_dependencies(",
        "context": [
            "      total_loss = tf.debugging.check_numerics(total_loss,\n",
            "                                               'LossTensor is inf or nan')\n",
            "\n",
            "    # Ensure the train_tensor computes grad_updates.\n",
            "    with tf.control_dependencies([grad_updates]):\n",
            "      train_op = tf.identity(total_loss, name='train_op')\n",
            "\n",
            "  return train_op\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 483,
        "API": ".ones(",
        "context": [
            "  In Graph mode it would create different tf.py_function for each combination\n",
            "  of dtype of the inputs and cache them for reuse.\n",
            "\n",
            "  NOTE: In Graph mode: if `output_dtypes` is not provided then `func` would\n",
            "  be called with `np.ones()` to infer the output dtypes, and therefore `func`\n",
            "  should be stateless.\n",
            "\n",
            "  ```python\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 490,
        "API": ".sum(",
        "context": [
            "  ```python\n",
            "  Instead of doing:\n",
            "\n",
            "  def sum(x):\n",
            "    return np.sum(x)\n",
            "  inputs = tf.constant([3, 4])\n",
            "  outputs = tf.py_function(sum, inputs, Tout=[tf.int64])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 495,
        "API": ".py_function(",
        "context": [
            "  inputs = tf.constant([3, 4])\n",
            "  outputs = tf.py_function(sum, inputs, Tout=[tf.int64])\n",
            "\n",
            "  inputs = tf.constant([3., 4.])\n",
            "  outputs = tf.py_function(sum, inputs, Tout=[tf.float32])\n",
            "\n",
            "  Do:\n",
            "  @eager_utils.np_function\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 500,
        "API": ".sum(",
        "context": [
            "\n",
            "  Do:\n",
            "  @eager_utils.np_function\n",
            "  def sum(x):\n",
            "    return np.sum(x)\n",
            "\n",
            "  inputs = tf.constant([3, 4])\n",
            "  outputs = sum(inputs)  # Infers that Tout is tf.int64\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 505,
        "API": ".constant(",
        "context": [
            "\n",
            "  inputs = tf.constant([3, 4])\n",
            "  outputs = sum(inputs)  # Infers that Tout is tf.int64\n",
            "\n",
            "  inputs = tf.constant([3., 4.])\n",
            "  outputs = sum(inputs)  # Infers that Tout is tf.float32\n",
            "\n",
            "  # Output dtype is always float32 for valid input dtypes.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 511,
        "API": ".mean(",
        "context": [
            "\n",
            "  # Output dtype is always float32 for valid input dtypes.\n",
            "  @eager_utils.np_function(output_dtypes=np.float32)\n",
            "  def mean(x):\n",
            "    return np.mean(x)\n",
            "\n",
            "  # Output dtype depends on the input dtype.\n",
            "  @eager_utils.np_function(output_dtypes=lambda x: (x, x))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 519,
        "API": ".constant(",
        "context": [
            "  def repeat(x):\n",
            "    return x, x\n",
            "\n",
            "  with context.graph_mode():\n",
            "    outputs = sum(tf.constant([3, 4]))\n",
            "    outputs2 = sum(tf.constant([3., 4.]))\n",
            "    sess.run(outputs) # np.array(7)\n",
            "    sess.run(outputs2) # np.array(7.)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 525,
        "API": ".constant(",
        "context": [
            "    sess.run(outputs) # np.array(7)\n",
            "    sess.run(outputs2) # np.array(7.)\n",
            "\n",
            "  with context.eager_mode():\n",
            "    inputs = tf.constant([3, 4])\n",
            "    outputs = sum(tf.constant([3, 4])) # tf.Tensor([7])\n",
            "    outputs = sum(tf.constant([3., 4.])) # tf.Tensor([7.])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 548,
        "API": ".flatten(",
        "context": [
            "    def wrapper(*args, **kwargs):\n",
            "      \"\"\"Wrapper to add nested input and outputs support.\"\"\"\n",
            "      func_with_kwargs = functools.partial(func, **kwargs)\n",
            "      def func_flat_outputs(*args):\n",
            "        return tf.nest.flatten(func_with_kwargs(*args))\n",
            "\n",
            "      def compute_output_dtypes(*args):\n",
            "        \"\"\"Calls the func to compute output dtypes.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 553,
        "API": ".map_structure(",
        "context": [
            "\n",
            "      def compute_output_dtypes(*args):\n",
            "        \"\"\"Calls the func to compute output dtypes.\"\"\"\n",
            "        result = func(*args, **kwargs)\n",
            "        return tf.nest.map_structure(lambda x: x.dtype, result)\n",
            "\n",
            "      if tf.executing_eagerly():\n",
            "        result = func_with_kwargs(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 558,
        "API": ".convert_to_tensor(",
        "context": [
            "\n",
            "      if tf.executing_eagerly():\n",
            "        result = func_with_kwargs(\n",
            "            *tf.nest.map_structure(lambda x: x.numpy(), args))\n",
            "        convert = lambda x: x if x is None else tf.convert_to_tensor(value=x)\n",
            "        return tf.nest.map_structure(convert, result)\n",
            "      else:\n",
            "        input_dtypes = tuple([x.dtype for x in tf.nest.flatten(args)])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 564,
        "API": ".map_structure(",
        "context": [
            "      else:\n",
            "        input_dtypes = tuple([x.dtype for x in tf.nest.flatten(args)])\n",
            "        if input_dtypes not in dtype_map:\n",
            "          if output_dtypes is None:\n",
            "            dummy_args = tf.nest.map_structure(\n",
            "                lambda x: np.ones(x.shape, x.dtype.as_numpy_dtype), args)\n",
            "            dtype_map[input_dtypes] = compute_output_dtypes(*dummy_args)\n",
            "          elif isinstance(output_dtypes, (list, tuple)):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 573,
        "API": ".as_dtype(",
        "context": [
            "            dtype_map[input_dtypes] = output_dtypes\n",
            "          else:\n",
            "            try:\n",
            "              # See if output_dtypes define the output dtype directly.\n",
            "              tf.as_dtype(output_dtypes)\n",
            "              dtype_map[input_dtypes] = output_dtypes\n",
            "            except TypeError:\n",
            "              if callable(output_dtypes):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 583,
        "API": ".flatten(",
        "context": [
            "              else:\n",
            "                raise ValueError(\n",
            "                    'output_dtypes not a list of dtypes or a callable.')\n",
            "\n",
            "      flat_output_dtypes = tf.nest.flatten(dtype_map[input_dtypes])\n",
            "      flat_outputs = tf.py_function(func_flat_outputs,\n",
            "                                    inp=args,\n",
            "                                    Tout=flat_output_dtypes)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 613,
        "API": ".executing_eagerly(",
        "context": [
            "  Returns:\n",
            "    A `tf.data.Iterator` if Graph mode is enabled; a tf.data.EagerIterator if\n",
            "    in eager mode.\n",
            "  \"\"\"\n",
            "  if tf.executing_eagerly():\n",
            "    return iter(dataset)\n",
            "  try:\n",
            "    iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\eager_utils.py",
        "line_number": 636,
        "API": ".executing_eagerly(",
        "context": [
            "  Returns:\n",
            "    A `tf.data.Iterator` if Graph mode is enabled; a Python iterator if in eager\n",
            "    mode.\n",
            "  \"\"\"\n",
            "  if tf.executing_eagerly():\n",
            "    return next(iterator)\n",
            "  return iterator.get_next()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\example_encoding.py",
        "line_number": 50,
        "API": ".zeros(",
        "context": [
            "    }\n",
            "\n",
            "    example_encoder = get_example_encoder(spec)\n",
            "    serialized = example_encoder({\n",
            "        'lidar': np.zeros((900,), np.float32),\n",
            "        'joint_positions': {\n",
            "            'arm': np.array([0.0, 1.57, 0.707, 0.2, 0.0, -1.57, 0.0],\n",
            "                            np.float32),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\example_encoding.py",
        "line_number": 86,
        "API": ".flatten(",
        "context": [
            "\n",
            "  # pylint: enable=g-complex-comprehension\n",
            "\n",
            "  def _example_encoder(features_nest):\n",
            "    flat_features = tf.nest.flatten(features_nest)\n",
            "    if len(flat_features) != len(feature_encoders):\n",
            "      raise ValueError(\n",
            "          'Encoding failed: The number of items to encode does not match the '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\example_encoding.py",
        "line_number": 150,
        "API": ".flatten(",
        "context": [
            "          serialized=serialized, features=features_dict)\n",
            "\n",
            "      decoded_features = []\n",
            "\n",
            "      dtypes = [s.dtype for s in tf.nest.flatten(example_spec)]\n",
            "      if len(parsers) != len(dtypes):\n",
            "        raise ValueError(\n",
            "            'Decoding failed: Number of parsers (%d) does not match the number '\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\example_encoding.py",
        "line_number": 158,
        "API": ".map_fn(",
        "context": [
            "            'of items in the example_spec (%d)' % (len(parsers), len(dtypes)))\n",
            "\n",
            "      for (path, parser), dtype in zip(parsers, dtypes):\n",
            "        decoded_features.append(\n",
            "            tf.map_fn(parser, raw_features[path], dtype=dtype))\n",
            "\n",
            "      return tf.nest.pack_sequence_as(example_spec, decoded_features)\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\example_encoding.py",
        "line_number": 189,
        "API": ".as_dtype(",
        "context": [
            "\n",
            "\n",
            "def _validate_dtype(dtype):\n",
            "  \"\"\"Check that dtype is supported by tf.decode_raw.\"\"\"\n",
            "  dtype = tf.as_dtype(dtype)\n",
            "  supported_dtypes = (tf.half, tf.float32, tf.float64, tf.uint8, tf.int8,\n",
            "                      tf.uint16, tf.int16, tf.int32, tf.int64)\n",
            "  if dtype not in supported_dtypes:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\example_encoding.py",
        "line_number": 194,
        "API": ".join(",
        "context": [
            "  supported_dtypes = (tf.half, tf.float32, tf.float64, tf.uint8, tf.int8,\n",
            "                      tf.uint16, tf.int16, tf.int32, tf.int64)\n",
            "  if dtype not in supported_dtypes:\n",
            "    raise ValueError('%s is not supported, dtype must be one of %s' %\n",
            "                     (dtype.name, ', '.join(d.name for d in supported_dtypes)))\n",
            "  return dtype\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\example_encoding.py",
        "line_number": 200,
        "API": ".as_dtype(",
        "context": [
            "\n",
            "\n",
            "def _check_shape_and_dtype(value, shape, dtype):\n",
            "  \"\"\"Check that `value` has expected shape and dtype.\"\"\"\n",
            "  value_dtype = tf.as_dtype(value.dtype.newbyteorder('N'))\n",
            "  if shape != value.shape or dtype != value_dtype:\n",
            "    raise ValueError('Expected shape %s of %s, got: shape %s of %s' %\n",
            "                     (shape, dtype.name, value.shape, value_dtype.name))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\example_encoding.py",
        "line_number": 231,
        "API": ".save(",
        "context": [
            "        im = Image.fromarray(value[:, :, 0])\n",
            "      else:\n",
            "        im = Image.fromarray(value)\n",
            "      out = io.BytesIO()\n",
            "      im.save(out, format='jpeg', quality=image_quality)\n",
            "      return tf.train.Feature(\n",
            "          bytes_list=tf.train.BytesList(value=[out.getvalue()]))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\example_encoding.py",
        "line_number": 244,
        "API": ".flatten(",
        "context": [
            "      value = np.asarray(value)\n",
            "      _check_shape_and_dtype(value, shape, dtype)\n",
            "      return tf.train.Feature(\n",
            "          float_list=tf.train.FloatList(\n",
            "              value=value.flatten(order='C').tolist()))\n",
            "\n",
            "    return _encode_to_float_list\n",
            "  elif dtype == tf.int64:  # Serialize int64 to Int64List.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\example_encoding.py",
        "line_number": 254,
        "API": ".flatten(",
        "context": [
            "      value = np.asarray(value)\n",
            "      _check_shape_and_dtype(value, shape, dtype)\n",
            "      return tf.train.Feature(\n",
            "          int64_list=tf.train.Int64List(\n",
            "              value=value.flatten(order='C').tolist()))\n",
            "\n",
            "    return _encode_to_int64_list\n",
            "  else:  # Serialize anything else to BytesList in little endian order.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\example_encoding.py",
        "line_number": 297,
        "API": ".reshape(",
        "context": [
            "  elif dtype == tf.int64:\n",
            "    return (tf.io.FixedLenFeature(shape=shape, dtype=tf.int64), lambda x: x)\n",
            "\n",
            "  def decode(x):\n",
            "    return tf.reshape(tf.io.decode_raw(x, dtype), shape)\n",
            "\n",
            "  return (tf.io.FixedLenFeature(shape=[], dtype=tf.string), decode)\n",
            "  # pylint: enable=g-long-lambda\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\example_encoding_dataset.py",
        "line_number": 49,
        "API": ".write(",
        "context": [
            "      shape of the non-batched Tensors.\n",
            "  \"\"\"\n",
            "  spec_proto = tensor_spec.to_proto(tensor_data_spec)\n",
            "  with tf.io.TFRecordWriter(output_path) as writer:\n",
            "    writer.write(spec_proto.SerializeToString())\n",
            "\n",
            "\n",
            "def parse_encoded_spec_from_file(input_path):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\example_encoding_dataset.py",
        "line_number": 68,
        "API": ".executing_eagerly(",
        "context": [
            "    raise IOError('Could not find spec file at %s.' % input_path)\n",
            "  dataset = tf.data.TFRecordDataset(input_path, buffer_size=1)\n",
            "  dataset_iterator = eager_utils.dataset_iterator(dataset)\n",
            "  signature_proto_string = eager_utils.get_next(dataset_iterator)\n",
            "  if tf.executing_eagerly():\n",
            "    signature_proto = struct_pb2.StructuredValue.FromString(\n",
            "        signature_proto_string.numpy())\n",
            "  else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\example_encoding_dataset.py",
        "line_number": 98,
        "API": ".function(",
        "context": [
            "    observers=[..., tfrecord_observer],\n",
            "    num_steps=collect_steps_per_iteration).run()\n",
            "\n",
            "  *Note*: Depending on your driver you may have to do\n",
            "    `common.function(tfrecord_observer)` to handle the use of a callable with no\n",
            "    return within a `tf.group` operation.\n",
            "  \"\"\"\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\example_encoding_dataset.py",
        "line_number": 132,
        "API": ".makedirs(",
        "context": [
            "        compress_image=compress_image,\n",
            "        image_quality=image_quality)\n",
            "    # Two output files: a tfrecord file and a file with the serialized spec\n",
            "    self.output_path = output_path\n",
            "    tf.io.gfile.makedirs(os.path.dirname(self.output_path))\n",
            "    self._writer = tf.io.TFRecordWriter(self.output_path)\n",
            "    logging.info('Writing dataset to TFRecord at %s', self.output_path)\n",
            "    # Save the tensor spec used to write the dataset to file\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\example_encoding_dataset.py",
        "line_number": 150,
        "API": ".write(",
        "context": [
            "      structured_data = data\n",
            "    else:\n",
            "      data = nest_utils.unbatch_nested_array(data)\n",
            "      structured_data = tf.nest.pack_sequence_as(self._array_data_spec, data)\n",
            "    self._writer.write(self._encoder(structured_data))\n",
            "\n",
            "  def flush(self):\n",
            "    \"\"\"Manually flush TFRecord writer.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\example_encoding_dataset.py",
        "line_number": 159,
        "API": ".info(",
        "context": [
            "\n",
            "  def close(self):\n",
            "    \"\"\"Close the TFRecord writer.\"\"\"\n",
            "    self._writer.close()\n",
            "    logging.info('Closing TFRecord file at %s', self.output_path)\n",
            "\n",
            "  def __call__(self, data):\n",
            "    \"\"\"If not in py_mode Wraps write() into a TF op for eager execution.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\example_encoding_dataset.py",
        "line_number": 164,
        "API": ".write(",
        "context": [
            "\n",
            "  def __call__(self, data):\n",
            "    \"\"\"If not in py_mode Wraps write() into a TF op for eager execution.\"\"\"\n",
            "    if self._py_mode:\n",
            "      self.write(data)\n",
            "    else:\n",
            "      flat_data = tf.nest.flatten(data)\n",
            "      tf.numpy_function(self.write, flat_data, [], name='encoder_observer')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\example_encoding_dataset.py",
        "line_number": 226,
        "API": ".info(",
        "context": [
            "\n",
            "    decoder = example_encoding.get_example_decoder(\n",
            "        spec, compress_image=compress_image)\n",
            "\n",
            "  logging.info('Loading TFRecord dataset...')\n",
            "  if not num_parallel_reads:\n",
            "    num_parallel_reads = len(dataset_files)\n",
            "  dataset = tf.data.TFRecordDataset(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\example_encoding_dataset.py",
        "line_number": 245,
        "API": ".batch(",
        "context": [
            "    return nest_utils.batch_nested_tensors(sample)\n",
            "\n",
            "  if as_experience:\n",
            "    dataset = dataset.map(\n",
            "        decode_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(\n",
            "            2, drop_remainder=True)\n",
            "  elif add_batch_dim:\n",
            "    dataset = dataset.map(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\lazy_loader.py",
        "line_number": 50,
        "API": ".warning(",
        "context": [
            "    self._parent_module_globals[self._local_name] = module\n",
            "\n",
            "    # Emit a warning if one was specified\n",
            "    if self._warning:\n",
            "      logging.warning(self._warning)\n",
            "      # Make sure to only warn once.\n",
            "      self._warning = None\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 97,
        "API": ".format(",
        "context": [
            "      of their substructures. Only possible if `check_types is True`.\n",
            "  \"\"\"\n",
            "  if not isinstance(check_types, bool):\n",
            "    raise TypeError(\n",
            "        'check_types must be a bool but saw: \\'{}\\''.format(check_types))\n",
            "  if not isinstance(expand_composites, bool):\n",
            "    raise TypeError('expand_composites must be a bool but saw: \\'{}\\''.format(\n",
            "        expand_composites))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 119,
        "API": ".map_structure(",
        "context": [
            "  except (TypeError, ValueError) as e:\n",
            "    exception = type(e)\n",
            "\n",
            "  if exception:\n",
            "    str1 = tf.nest.map_structure(\n",
            "        lambda _: _DOT, nest1, expand_composites=expand_composites)\n",
            "    str2 = tf.nest.map_structure(\n",
            "        lambda _: _DOT, nest2, expand_composites=expand_composites)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 124,
        "API": ".format(",
        "context": [
            "        lambda _: _DOT, nest1, expand_composites=expand_composites)\n",
            "    str2 = tf.nest.map_structure(\n",
            "        lambda _: _DOT, nest2, expand_composites=expand_composites)\n",
            "    raise exception('{}:\\n  {}\\nvs.\\n  {}\\nValues:\\n  {}\\nvs.\\n  {}.'\n",
            "                    .format(message, str1, str2, nest1, nest2))\n",
            "\n",
            "\n",
            "def flatten_with_joined_paths(structure, expand_composites=False):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 132,
        "API": ".join(",
        "context": [
            "  flattened = flatten_with_tuple_paths(\n",
            "      structure, expand_composites=expand_composites)\n",
            "\n",
            "  def stringify_and_join(path_elements):\n",
            "    return '/'.join(str(path_element) for path_element in path_elements)\n",
            "\n",
            "  return [(stringify_and_join(path), value) for (path, value) in flattened]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 138,
        "API": ".get(",
        "context": [
            "  return [(stringify_and_join(path), value) for (path, value) in flattened]\n",
            "\n",
            "\n",
            "def fast_map_structure_flatten(func, structure, *flat_structure, **kwargs):\n",
            "  expand_composites = kwargs.get('expand_composites', False)\n",
            "  entries = zip(*flat_structure)\n",
            "  return tf.nest.pack_sequence_as(\n",
            "      structure, [func(*x) for x in entries],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 146,
        "API": ".get(",
        "context": [
            "      expand_composites=expand_composites)\n",
            "\n",
            "\n",
            "def fast_map_structure(func, *structure, **kwargs):\n",
            "  expand_composites = kwargs.get('expand_composites', False)\n",
            "  flat_structure = [\n",
            "      tf.nest.flatten(s, expand_composites=expand_composites) for s in structure\n",
            "  ]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 158,
        "API": ".any(",
        "context": [
            "      expand_composites=expand_composites)\n",
            "\n",
            "\n",
            "def has_tensors(*x):\n",
            "  return np.any(\n",
            "      [tf.is_tensor(t) for t in tf.nest.flatten(x, expand_composites=True)])\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 306,
        "API": ".format(",
        "context": [
            "  \"\"\"\n",
            "  assert_same_structure(\n",
            "      tensors_1,\n",
            "      tensors_2,\n",
            "      message=('{}: {} and {} do not have matching structures'.format(\n",
            "          caller, tensors_1_name, tensors_2_name)))\n",
            "\n",
            "  def convert_to_tensor(t):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 312,
        "API": ".flatten(",
        "context": [
            "\n",
            "  def convert_to_tensor(t):\n",
            "    return tf.convert_to_tensor(t) if not tf.is_tensor(t) else t\n",
            "\n",
            "  flat_t1 = tf.nest.map_structure(convert_to_tensor, tf.nest.flatten(tensors_1))\n",
            "  flat_t2 = tf.nest.map_structure(convert_to_tensor, tf.nest.flatten(tensors_2))\n",
            "\n",
            "  t1_shapes = [t.shape for t in flat_t1]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 332,
        "API": ".map_structure(",
        "context": [
            "        compatible = False\n",
            "        break\n",
            "\n",
            "  if not compatible:\n",
            "    get_dtypes = lambda v: tf.nest.map_structure(lambda x: x.dtype, v)\n",
            "    get_shapes = lambda v: tf.nest.map_structure(lambda x: x.shape, v)\n",
            "    raise ValueError('{}: Inconsistent dtypes or shapes between {} and {}.\\n'\n",
            "                     'dtypes:\\n{}\\nvs.\\n{}.\\n'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 370,
        "API": ".format(",
        "context": [
            "    tensors_or_specs = prune_extra_keys(specs, tensors_or_specs)\n",
            "  assert_same_structure(\n",
            "      tensors_or_specs,\n",
            "      specs,\n",
            "      message=('{}: {} and {} do not have matching structures'.format(\n",
            "          caller, tensors_name, specs_name)))\n",
            "\n",
            "  flat_tensors = nest.flatten(tensors_or_specs)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 376,
        "API": ".is_tensor(",
        "context": [
            "\n",
            "  flat_tensors = nest.flatten(tensors_or_specs)\n",
            "  flat_specs = tf.nest.flatten(specs)\n",
            "  def _convert(t, s):\n",
            "    if not isinstance(t, tf.TypeSpec) and not tf.is_tensor(t):\n",
            "      t = tf.convert_to_tensor(t, dtype_hint=s.dtype)\n",
            "    return t\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 404,
        "API": ".map_structure(",
        "context": [
            "        compatible = False\n",
            "        break\n",
            "\n",
            "  if not compatible:\n",
            "    get_dtypes = lambda v: tf.nest.map_structure(lambda x: x.dtype, v)\n",
            "    get_shapes = lambda v: tf.nest.map_structure(spec_shape, v)\n",
            "    raise ValueError('{}: Inconsistent dtypes or shapes between {} and {}.\\n'\n",
            "                     'dtypes:\\n{}\\nvs.\\n{}.\\n'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 444,
        "API": ".zeros(",
        "context": [
            "      dimensions.  Default 1.\n",
            "    allow_extra_fields: If `True`, then `tensors` may have extra subfields which\n",
            "      are not in specs.  In this case, the extra subfields\n",
            "      will not be checked.  For example:  ```python\n",
            "      tensors = {\"a\": tf.zeros((3, 4), dtype=tf.float32),\n",
            "                 \"b\": tf.zeros((5, 6), dtype=tf.float32)}\n",
            "      specs = {\"a\": tf.TensorSpec(shape=(4,), dtype=tf.float32)} assert\n",
            "        is_batched_nested_tensors(tensors, specs, allow_extra_fields=True) ```\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 471,
        "API": ".flatten(",
        "context": [
            "  assert_same_structure(\n",
            "      tensors,\n",
            "      specs,\n",
            "      message='Tensors and specs do not have matching structures')\n",
            "  flat_tensors = nest.flatten(tensors)\n",
            "  flat_specs = tf.nest.flatten(specs)\n",
            "\n",
            "  tensor_shapes = [t.shape for t in flat_tensors]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 493,
        "API": ".map_structure(",
        "context": [
            "  ):\n",
            "    packed_tensor_dtypes = tf.nest.pack_sequence_as(specs, tensor_dtypes)\n",
            "    packed_spec_dtypes = tf.nest.pack_sequence_as(specs, spec_dtypes)\n",
            "\n",
            "    mismatched = tf.nest.map_structure(\n",
            "        lambda t, s: t != s, packed_tensor_dtypes, packed_spec_dtypes\n",
            "    )\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 498,
        "API": ".flatten(",
        "context": [
            "        lambda t, s: t != s, packed_tensor_dtypes, packed_spec_dtypes\n",
            "    )\n",
            "\n",
            "    num_mismatched = tf.math.count_nonzero(\n",
            "        tf.nest.flatten(mismatched), dtype=tf.int32\n",
            "    )\n",
            "\n",
            "    raise TypeError(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 503,
        "API": ".format(",
        "context": [
            "    )\n",
            "\n",
            "    raise TypeError(\n",
            "        'Tensor dtypes do not match spec dtypes:\\n{}\\nvs.\\n{}\\nNumber of'\n",
            "        ' mismatched entries: {}\\nMismatch:\\n{}'.format(\n",
            "            tf.nest.pack_sequence_as(specs, tensor_dtypes),\n",
            "            tf.nest.pack_sequence_as(specs, spec_dtypes),\n",
            "            num_mismatched,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 583,
        "API": ".expand_dims(",
        "context": [
            "  Raises:\n",
            "    ValueError: if the tensors and specs have incompatible dimensions or shapes.\n",
            "  \"\"\"\n",
            "  if specs is None:\n",
            "    return tf.nest.map_structure(lambda x: composite.expand_dims(x, 0), tensors)\n",
            "\n",
            "  assert_same_structure(\n",
            "      tensors,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 590,
        "API": ".flatten(",
        "context": [
            "      tensors,\n",
            "      specs,\n",
            "      message='Tensors and specs do not have matching structures')\n",
            "\n",
            "  flat_tensors = tf.nest.flatten(tensors)\n",
            "  flat_shapes = [spec_shape(s) for s in tf.nest.flatten(specs)]\n",
            "  batched_tensors = []\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 598,
        "API": ".expand_dims(",
        "context": [
            "  tensor_rank = lambda tensor: tensor.shape.rank\n",
            "  for tensor, shape in zip(flat_tensors, flat_shapes):\n",
            "    if tensor_rank(tensor) == shape.rank:\n",
            "      tensor.shape.assert_is_compatible_with(shape)\n",
            "      tensor = composite.expand_dims(tensor, 0)\n",
            "    elif tensor_rank(tensor) == shape.rank + 1:\n",
            "      tensor.shape[1:].assert_is_compatible_with(shape)\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 603,
        "API": ".format(",
        "context": [
            "    elif tensor_rank(tensor) == shape.rank + 1:\n",
            "      tensor.shape[1:].assert_is_compatible_with(shape)\n",
            "    else:\n",
            "      raise ValueError('Tensor does not have the correct dimensions. '\n",
            "                       'tensor.shape {} expected shape {}'.format(\n",
            "                           tensor.shape, shape))\n",
            "    batched_tensors.append(tensor)\n",
            "  return tf.nest.pack_sequence_as(tensors, batched_tensors)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 615,
        "API": ".flatten(",
        "context": [
            "  assert_same_structure(\n",
            "      tensors,\n",
            "      specs,\n",
            "      message='Tensors and specs do not have matching structures')\n",
            "  flat_tensors = tf.nest.flatten(tensors)\n",
            "  flat_shapes = [spec_shape(s) for s in tf.nest.flatten(specs)]\n",
            "  for tensor, shape in zip(flat_tensors, flat_shapes):\n",
            "    if tensor.shape.rank == shape.rank:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 624,
        "API": ".format(",
        "context": [
            "    elif tensor.shape.rank == shape.rank + num_outer_dims:\n",
            "      tensor.shape[num_outer_dims:].assert_is_compatible_with(shape)\n",
            "    else:\n",
            "      raise ValueError('Tensor does not have the correct dimensions. '\n",
            "                       'tensor.shape {} expected shape {}'.format(\n",
            "                           tensor.shape, [None] + shape.as_list()))\n",
            "  return flat_tensors, flat_shapes\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 659,
        "API": ".map_structure(",
        "context": [
            "  Raises:\n",
            "    ValueError: if the tensors and specs have incompatible dimensions or shapes.\n",
            "  \"\"\"\n",
            "  if specs is None:\n",
            "    return tf.nest.map_structure(lambda x: composite.squeeze(x, 0), tensors)\n",
            "\n",
            "  unbatched_tensors = []\n",
            "  flat_tensors, flat_shapes = _flatten_and_check_shape_nested_tensors(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 666,
        "API": ".squeeze(",
        "context": [
            "  flat_tensors, flat_shapes = _flatten_and_check_shape_nested_tensors(\n",
            "      tensors, specs)\n",
            "  for tensor, shape in zip(flat_tensors, flat_shapes):\n",
            "    if tensor.shape.rank == shape.rank + 1:\n",
            "      tensor = composite.squeeze(tensor, 0)\n",
            "    unbatched_tensors.append(tensor)\n",
            "  return tf.nest.pack_sequence_as(tensors, unbatched_tensors)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 705,
        "API": ".format(",
        "context": [
            "      if isinstance(tensor, tf.SparseTensor):\n",
            "        if not isinstance(num_or_size_splits, numbers.Number):\n",
            "          raise ValueError(\n",
            "              'Saw a SparseTensor, for which num_or_size_splits must be a '\n",
            "              'scalar.  But it is not: {}'.format(num_or_size_splits))\n",
            "        split_tensors = tf.sparse.split(\n",
            "            sp_input=tensor, num_split=num_or_size_splits, axis=0)\n",
            "      else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 737,
        "API": ".unstack(",
        "context": [
            "  for tensor, shape in zip(flat_tensors, flat_shapes):\n",
            "    if tensor.shape.rank == shape.rank:\n",
            "      raise ValueError('Can only unstack tensors with a batch dimension.')\n",
            "    if tensor.shape.rank == shape.rank + 1:\n",
            "      unstacked_tensors = tf.unstack(tensor)\n",
            "    unstacked_tensor_lists.append(unstacked_tensors)\n",
            "  unstacked_tensors_zipped = zip(*unstacked_tensor_lists)\n",
            "  return [\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 756,
        "API": ".stack(",
        "context": [
            "\n",
            "  Returns:\n",
            "    A stacked nested tensor.\n",
            "  \"\"\"\n",
            "  return tf.nest.map_structure(lambda *tensors: tf.stack(tensors, axis=axis),\n",
            "                               *tensors)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 782,
        "API": ".flatten(",
        "context": [
            "  assert_same_structure(\n",
            "      tensors,\n",
            "      specs,\n",
            "      message='Tensors and specs do not have matching structures')\n",
            "  flat_tensors = tf.nest.flatten(tensors)\n",
            "  flat_spec_shapes = [spec_shape(s) for s in tf.nest.flatten(specs)]\n",
            "  out_tensors = []\n",
            "  batch_dims = []\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 791,
        "API": ".prod(",
        "context": [
            "    if i == 0:  # Set batch_dims based on first tensor.\n",
            "      batch_dims = tensor.shape[:tensor.shape.rank - sp_shape.rank]\n",
            "      if batch_dims.is_fully_defined():\n",
            "        batch_dims = batch_dims.as_list()\n",
            "        batch_prod = np.prod(batch_dims)\n",
            "        batch_dims = tf.constant(batch_dims, dtype=tf.int64)\n",
            "      else:\n",
            "        batch_dims = tf.shape(tensor)[:tensor.shape.rank - sp_shape.rank]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 799,
        "API": ".shape(",
        "context": [
            "        batch_prod = tf.reduce_prod(batch_dims)\n",
            "    if not sp_shape.is_fully_defined():\n",
            "      # When shape of spec is not fully defined, we do not rely on it to\n",
            "      # reshape the tensor but retain the original non-batch dims of tensors.\n",
            "      non_batch_dims = tf.shape(tensor)[tensor.shape.rank - sp_shape.rank:]\n",
            "      reshaped_dims = tf.concat([[batch_prod], non_batch_dims], 0)\n",
            "    else:\n",
            "      reshaped_dims = [batch_prod] + sp_shape.as_list()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 825,
        "API": ".flatten(",
        "context": [
            "  assert_same_structure(\n",
            "      nested_tensor,\n",
            "      spec,\n",
            "      message='Tensors and specs do not have matching structures')\n",
            "  first_tensor = tf.nest.flatten(nested_tensor)[0]\n",
            "  first_spec = tf.nest.flatten(spec)[0]\n",
            "\n",
            "  # Check tensors have same batch shape.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 832,
        "API": ".constant(",
        "context": [
            "  # Check tensors have same batch shape.\n",
            "  num_outer_dims = (len(first_tensor.shape) - len(first_spec.shape))\n",
            "  if not is_batched_nested_tensors(\n",
            "      nested_tensor, spec, num_outer_dims=num_outer_dims, check_dtypes=False):\n",
            "    return tf.constant([], dtype=tf.int32)\n",
            "\n",
            "  return tf.shape(input=first_tensor)[:num_outer_dims]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 863,
        "API": ".flatten(",
        "context": [
            "  assert_same_structure(\n",
            "      tensors,\n",
            "      specs,\n",
            "      message='Tensors and specs do not have matching structures')\n",
            "  tensor_shapes = [t.shape for t in tf.nest.flatten(tensors)]\n",
            "  spec_shapes = [spec_shape(s) for s in tf.nest.flatten(specs)]\n",
            "\n",
            "  if any(s_shape.rank is None for s_shape in spec_shapes):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 924,
        "API": ".expand_dims(",
        "context": [
            "                   (num_outer_dims, tensor_shapes, spec_shapes))\n",
            "\n",
            "\n",
            "def batch_nested_array(nested_array):\n",
            "  return tf.nest.map_structure(lambda x: np.expand_dims(x, 0), nested_array)\n",
            "\n",
            "\n",
            "def unbatch_nested_array(nested_array):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 934,
        "API": ".squeeze(",
        "context": [
            "\n",
            "def unbatch_nested_tensors_to_arrays(nested_tensors):\n",
            "\n",
            "  def _to_unbatched_numpy(tensor):\n",
            "    return np.squeeze(tensor.numpy(), 0)\n",
            "\n",
            "  return tf.nest.map_structure(_to_unbatched_numpy, nested_tensors)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 945,
        "API": ".flatten(",
        "context": [
            "  def _unstack(array):\n",
            "    # Use numpy views instead of np.split, it's 5x+ faster.\n",
            "    return [array[i] for i in range(len(array))]\n",
            "\n",
            "  return zip(*[_unstack(array) for array in tf.nest.flatten(nested_array)])\n",
            "\n",
            "\n",
            "def unstack_nested_arrays(nested_array):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 996,
        "API": ".flatten(",
        "context": [
            "  Returns:\n",
            "    A nested array containing batched items, where each batched item is obtained\n",
            "      by stacking corresponding items from the list of nested_arrays.\n",
            "  \"\"\"\n",
            "  nested_arrays_flattened = [tf.nest.flatten(a) for a in nested_arrays]\n",
            "  batched_nested_array_flattened = [\n",
            "      np.stack(a) for a in zip(*nested_arrays_flattened)\n",
            "  ]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 1006,
        "API": ".flatten(",
        "context": [
            "\n",
            "\n",
            "def get_outer_array_shape(nested_array, spec):\n",
            "  \"\"\"Batch dims of array's batch dimension `dim`.\"\"\"\n",
            "  first_array = tf.nest.flatten(nested_array)[0]\n",
            "  first_spec = tf.nest.flatten(spec)[0]\n",
            "  num_outer_dims = len(first_array.shape) - len(first_spec.shape)\n",
            "  return first_array.shape[:num_outer_dims]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 1040,
        "API": ".flatten(",
        "context": [
            "      true_outputs,\n",
            "      false_outputs,\n",
            "      message='\"true_outputs\" and \"false_outputs\" structures do not match')\n",
            "\n",
            "  if tf.nest.flatten(true_outputs):\n",
            "    condition_rank = tf.rank(condition)\n",
            "    @tf.function  # allow-tf-function\n",
            "    def per_field_where(t, f):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 1045,
        "API": ".rank(",
        "context": [
            "    condition_rank = tf.rank(condition)\n",
            "    @tf.function  # allow-tf-function\n",
            "    def per_field_where(t, f):\n",
            "      tf.debugging.assert_rank_at_least(t, condition_rank)\n",
            "      rank_difference = tf.rank(t) - condition_rank\n",
            "      condition_shape = tf.concat(\n",
            "          [tf.shape(condition),\n",
            "           tf.ones(rank_difference, dtype=tf.int32)], axis=0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 1050,
        "API": ".where(",
        "context": [
            "      condition_shape = tf.concat(\n",
            "          [tf.shape(condition),\n",
            "           tf.ones(rank_difference, dtype=tf.int32)], axis=0)\n",
            "      per_field_condition = tf.reshape(condition, condition_shape)\n",
            "      return tf.compat.v2.where(per_field_condition, t, f)\n",
            "\n",
            "    return tf.nest.map_structure(per_field_where, true_outputs, false_outputs)\n",
            "  else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 1055,
        "API": ".where(",
        "context": [
            "\n",
            "    return tf.nest.map_structure(per_field_where, true_outputs, false_outputs)\n",
            "  else:\n",
            "    return tf.nest.map_structure(\n",
            "        lambda t, f: tf.compat.v2.where(condition, t, f), true_outputs,\n",
            "        false_outputs)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 1086,
        "API": ".format(",
        "context": [
            "    shape = getattr(spec, '_shape', None)\n",
            "  if shape is None:\n",
            "    raise ValueError(\n",
            "        'Could not remove singleton batch dim from spec; it lacks a shape: {}'\n",
            "        .format(spec))\n",
            "  for i in range(outer_ndim):\n",
            "    if len(shape) <= i:\n",
            "      logging.error(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 1094,
        "API": ".error(",
        "context": [
            "          'Could not remove singleton batch dim from spec; len(shape) < %d.  '\n",
            "          'Shape: %s.  Skipping.', i + 1, shape)\n",
            "      break\n",
            "    if tf.compat.dimension_value(shape[i]) != 1:\n",
            "      logging.error(\n",
            "          'Could not remove singleton batch dim from spec; shape[%d] != 1: %s '\n",
            "          '(shape: %s).  Skipping.', i, spec, shape)\n",
            "      break\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 1104,
        "API": ".convert_to_tensor(",
        "context": [
            "\n",
            "\n",
            "def _tile_batch(t, multiplier, ensure_shape=True):\n",
            "  \"\"\"Core single-tensor implementation of tile_batch.\"\"\"\n",
            "  t = tf.convert_to_tensor(t, name='t')\n",
            "  shape_t = tf.shape(t)\n",
            "  if t.shape.ndims is None or t.shape.ndims < 1:\n",
            "    raise ValueError('t must have statically known rank')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 1113,
        "API": ".tile(",
        "context": [
            "  tiling[1] = multiplier\n",
            "  num_batch_dims = tf.compat.dimension_value(t.shape.dims[0])\n",
            "  tiled_static_batch_size = (\n",
            "      num_batch_dims * multiplier if num_batch_dims is not None else None)\n",
            "  tiled = tf.tile(tf.expand_dims(t, 1), tiling)\n",
            "  tiled = tf.reshape(tiled,\n",
            "                     tf.concat(([shape_t[0] * multiplier], shape_t[1:]), 0))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 1149,
        "API": ".is_tensor(",
        "context": [
            "  Raises:\n",
            "    ValueError: if tensor(s) `t` do not have a statically known rank or\n",
            "    the rank is < 1.\n",
            "  \"\"\"\n",
            "  ensure_shape = False if tf.is_tensor(multiplier) else True\n",
            "  return tf.nest.map_structure(\n",
            "      lambda t: _tile_batch(t, multiplier, ensure_shape=ensure_shape), tensors)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 1173,
        "API": ".format(",
        "context": [
            "  def check_value_spec(v):\n",
            "    if not isinstance(v, tf.TensorSpec):\n",
            "      raise ValueError(\n",
            "          '{} emits outputs that are not tensors; spec: {}'\n",
            "          .format(network_name, output_spec))\n",
            "    if v.shape not in ((), (1,)):\n",
            "      raise ValueError(\n",
            "          '{} emits multiple values; spec: {}'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\nest_utils.py",
        "line_number": 1181,
        "API": ".format(",
        "context": [
            "          .format(network_name, output_spec))\n",
            "    if not v.dtype.is_floating:\n",
            "      raise ValueError(\n",
            "          '{} emits outputs that are not real numbers; spec: {}'\n",
            "          .format(network_name, output_spec))\n",
            "\n",
            "  tf.nest.map_structure(check_value_spec, output_spec)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\numpy_storage.py",
        "line_number": 41,
        "API": ".ones(",
        "context": [
            "\n",
            "  ```python\n",
            "  arrays = numpy_storage.NumpyState()\n",
            "  checkpoint = tf.train.Checkpoint(numpy_arrays=arrays)\n",
            "  arrays.x = np.ones([3, 4])\n",
            "  directory = self.get_temp_dir()\n",
            "  prefix = os.path.join(directory, 'ckpt')\n",
            "  save_path = checkpoint.save(prefix)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\numpy_storage.py",
        "line_number": 46,
        "API": ".all(",
        "context": [
            "  directory = self.get_temp_dir()\n",
            "  prefix = os.path.join(directory, 'ckpt')\n",
            "  save_path = checkpoint.save(prefix)\n",
            "  arrays.x[:] = 0.\n",
            "  assert (arrays.x == np.zeros([3, 4])).all()\n",
            "  checkpoint.restore(save_path)\n",
            "  assert (arrays.x == np.ones([3, 4])).all()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\numpy_storage.py",
        "line_number": 53,
        "API": ".restore(",
        "context": [
            "\n",
            "  second_checkpoint = tf.train.Checkpoint(\n",
            "      numpy_arrays=numpy_storage.NumpyState())\n",
            "  # Attributes of NumpyState objects are created automatically by restore()\n",
            "  second_checkpoint.restore(save_path)\n",
            "  assert (second_checkpoint.numpy_arrays.x == np.ones([3, 4])).all()\n",
            "  ```\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\numpy_storage.py",
        "line_number": 66,
        "API": ".restore(",
        "context": [
            "  save, the NumPy array(s) are fed as strings to be saved in the checkpoint (via\n",
            "  a placeholder when graph building, or as a string constant when executing\n",
            "  eagerly). When restoring they skip the TensorFlow graph entirely, and so no\n",
            "  restore ops need be run. This means that restoration always happens eagerly,\n",
            "  rather than waiting for `checkpoint.restore(...).run_restore_ops()` like\n",
            "  TensorFlow variables when graph building.\n",
            "  \"\"\"\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\numpy_storage.py",
        "line_number": 87,
        "API": ".get(",
        "context": [
            "      An existing dependency if one exists, or a new `_NumpyWrapper` placeholder\n",
            "      dependency (which will generally be restored immediately).\n",
            "    \"\"\"\n",
            "    if cached_dependencies is not None:\n",
            "      value = cached_dependencies.get(name)\n",
            "    else:\n",
            "      value = super(NumpyState, self)._lookup_dependency(name)\n",
            "    if value is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\numpy_storage.py",
        "line_number": 153,
        "API": ".save(",
        "context": [
            "  def serialize(self):\n",
            "    \"\"\"Callback to serialize the array.\"\"\"\n",
            "    string_file = io.BytesIO()\n",
            "    try:\n",
            "      np.save(string_file, self.array, allow_pickle=False)\n",
            "      serialized = string_file.getvalue()\n",
            "    finally:\n",
            "      string_file.close()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\numpy_storage.py",
        "line_number": 163,
        "API": ".load(",
        "context": [
            "  def deserialize(self, string_value):\n",
            "    \"\"\"Callback to deserialize the array.\"\"\"\n",
            "    string_file = io.BytesIO(string_value)\n",
            "    try:\n",
            "      self.array = np.load(string_file, allow_pickle=False)\n",
            "    finally:\n",
            "      string_file.close()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\numpy_storage.py",
        "line_number": 192,
        "API": ".flatten(",
        "context": [
            "    \"\"\"\n",
            "    self._capacity = capacity\n",
            "    if not all([\n",
            "        isinstance(spec, array_spec.ArraySpec)\n",
            "        for spec in tf.nest.flatten(data_spec)\n",
            "    ]):\n",
            "      raise ValueError('The data_spec parameter must be an instance or nest of '\n",
            "                       'array_spec.ArraySpec. Got: {}'.format(data_spec))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\numpy_storage.py",
        "line_number": 197,
        "API": ".flatten(",
        "context": [
            "    ]):\n",
            "      raise ValueError('The data_spec parameter must be an instance or nest of '\n",
            "                       'array_spec.ArraySpec. Got: {}'.format(data_spec))\n",
            "    self._data_spec = data_spec\n",
            "    self._flat_specs = tf.nest.flatten(data_spec)\n",
            "    self._np_state = NumpyState()\n",
            "\n",
            "    self._buf_names = data_structures.NoDependency([])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\numpy_storage.py",
        "line_number": 202,
        "API": ".format(",
        "context": [
            "    self._np_state = NumpyState()\n",
            "\n",
            "    self._buf_names = data_structures.NoDependency([])\n",
            "    for idx in range(len(self._flat_specs)):\n",
            "      self._buf_names.append('buffer{}'.format(idx))\n",
            "      # Set each buffer to a sentinel value (real buffers will never be\n",
            "      # scalars) rather than a real value so that if they are restored from\n",
            "      # checkpoint, we don't end up double-initializing. We don't leave them\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\numpy_storage.py",
        "line_number": 216,
        "API": ".zeros(",
        "context": [
            "    array = getattr(self._np_state, self._buf_names[index])\n",
            "    if np.isscalar(array) or array.ndim == 0:\n",
            "      spec = self._flat_specs[index]\n",
            "      shape = (self._capacity,) + spec.shape\n",
            "      array = np.zeros(shape=shape, dtype=spec.dtype)\n",
            "      setattr(self._np_state, self._buf_names[index], array)\n",
            "    return array\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\numpy_storage.py",
        "line_number": 229,
        "API": ".flatten(",
        "context": [
            "    return tf.nest.pack_sequence_as(self._data_spec, encoded_item)\n",
            "\n",
            "  def set(self, table_idx, value):\n",
            "    \"\"\"Set table_idx to value.\"\"\"\n",
            "    for nest_idx, element in enumerate(tf.nest.flatten(value)):\n",
            "      self._array(nest_idx)[table_idx] = element\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\object_identity.py",
        "line_number": 92,
        "API": ".format(",
        "context": [
            "    # weakref.ref(a) in _WeakObjectIdentityWrapper.\n",
            "    return id(self._wrapped)\n",
            "\n",
            "  def __repr__(self):\n",
            "    return \"<{} wrapping {!r}>\".format(type(self).__name__, self._wrapped)\n",
            "\n",
            "\n",
            "class _WeakObjectIdentityWrapper(_ObjectIdentityWrapper):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\object_identity.py",
        "line_number": 215,
        "API": ".add(",
        "context": [
            "  def discard(self, key):\n",
            "    self._storage.discard(self._wrap_key(key))\n",
            "\n",
            "  def add(self, key):\n",
            "    self._storage.add(self._wrap_key(key))\n",
            "\n",
            "  def update(self, items):\n",
            "    self._storage.update([self._wrap_key(item) for item in items])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\session_utils.py",
        "line_number": 53,
        "API": ".constant(",
        "context": [
            "  class MySessionUserClass(SessionUser):\n",
            "\n",
            "    def __init__(self):\n",
            "      self(MySessionUserClass, self).__init__()\n",
            "      self.op = tf.constant(0)\n",
            "\n",
            "    def run_some_op(self):\n",
            "      self.session.run(self.op)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\session_utils.py",
        "line_number": 121,
        "API": ".format(",
        "context": [
            "    default_session = tf.compat.v1.get_default_session()\n",
            "    if default_session is None:\n",
            "      raise AttributeError(\n",
            "          \"No TensorFlow session-like object was set on this {!r}, and none \"\n",
            "          \"could be retrieved using 'tf.get_default_session()'.\".format(\n",
            "              self.__class__.__name__))\n",
            "    return default_session\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\tensor_normalizer.py",
        "line_number": 58,
        "API": ".normalize(",
        "context": [
            "  observation_list = [list of float32 scalars or batches]\n",
            "  normalized_list = []\n",
            "\n",
            "  for o in observation_list:\n",
            "    normalized_list.append(tensor_normalizer.normalize(o))\n",
            "    tensor_normalizer.update(o)\n",
            "  ```\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\tensor_normalizer.py",
        "line_number": 69,
        "API": ".normalize(",
        "context": [
            "      tf.TensorSpec([], tf.float64), dtype=tf.float64)\n",
            "  observation_list = [list of float64 scalars or batches]\n",
            "\n",
            "  for o in observation_list:\n",
            "    normalized_list.append(tensor_normalizer.normalize(o))\n",
            "    tensor_normalizer.update(o)\n",
            "  \"\"\"\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\tensor_normalizer.py",
        "line_number": 85,
        "API": ".flatten(",
        "context": [
            "    super(TensorNormalizer, self).__init__(name=scope)\n",
            "    self._scope = scope\n",
            "    self._tensor_spec = tensor_spec\n",
            "    self._flat_variable_spec = [\n",
            "        self._map_spec_dtype(s) for s in tf.nest.flatten(tensor_spec)\n",
            "    ]\n",
            "    self._create_variables()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\tensor_normalizer.py",
        "line_number": 91,
        "API": ".get(",
        "context": [
            "    self._create_variables()\n",
            "\n",
            "  def map_dtype(self, dtype):\n",
            "    # Use tf.float32 by default\n",
            "    return _DTYPE_CONVERSION.get(dtype, tf.float32)\n",
            "\n",
            "  def _map_spec_dtype(self, spec):\n",
            "    return tensor_spec_lib.with_dtype(spec, self.map_dtype(spec.dtype))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\tensor_normalizer.py",
        "line_number": 127,
        "API": ".map_structure(",
        "context": [
            "        self._tensor_spec,\n",
            "        caller=self,\n",
            "        tensors_name='tensor',\n",
            "        specs_name='tensor_spec')\n",
            "    tensor = tf.nest.map_structure(\n",
            "        lambda t: tf.cast(t, self.map_dtype(t.dtype)), tensor)\n",
            "\n",
            "    return tf.group(self._update_ops(tensor, outer_dims))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\tensor_normalizer.py",
        "line_number": 153,
        "API": ".cast(",
        "context": [
            "    nest_utils.assert_matching_dtypes_and_inner_shapes(\n",
            "        tensor, self._tensor_spec, caller=self,\n",
            "        tensors_name='tensors', specs_name='tensor_spec')\n",
            "    tensor = [\n",
            "        tf.cast(t, self.map_dtype(t.dtype)) for t in tf.nest.flatten(tensor)\n",
            "    ]\n",
            "\n",
            "    with tf.name_scope(self._scope + '/normalize'):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\tensor_normalizer.py",
        "line_number": 159,
        "API": ".map_structure(",
        "context": [
            "\n",
            "    with tf.name_scope(self._scope + '/normalize'):\n",
            "      mean_estimate, var_estimate = self._get_mean_var_estimates()\n",
            "      mean = (\n",
            "          mean_estimate if center_mean else tf.nest.map_structure(\n",
            "              tf.zeros_like, mean_estimate))\n",
            "\n",
            "      def _normalize_single_tensor(single_tensor, single_mean, single_var):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\tensor_normalizer.py",
        "line_number": 184,
        "API": ".clip_by_value(",
        "context": [
            "\n",
            "      if clip_value > 0:\n",
            "\n",
            "        def _clip(t):\n",
            "          return tf.clip_by_value(\n",
            "              t, -clip_value, clip_value, name='clipped_normalized_tensor')\n",
            "\n",
            "        normalized_tensor = tf.nest.map_structure(_clip, normalized_tensor)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\tensor_normalizer.py",
        "line_number": 191,
        "API": ".map_structure(",
        "context": [
            "        normalized_tensor = tf.nest.map_structure(_clip, normalized_tensor)\n",
            "\n",
            "    normalized_tensor = tf.nest.pack_sequence_as(self._tensor_spec,\n",
            "                                                 normalized_tensor)\n",
            "    normalized_tensor = tf.nest.map_structure(\n",
            "        lambda t, spec: tf.cast(t, spec.dtype), normalized_tensor,\n",
            "        self._tensor_spec)\n",
            "    return normalized_tensor\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\tensor_normalizer.py",
        "line_number": 210,
        "API": ".map_structure(",
        "context": [
            "    self._norm_update_rate = norm_update_rate\n",
            "\n",
            "  def _create_variables(self):\n",
            "    \"\"\"Creates the variables needed for EMATensorNormalizer.\"\"\"\n",
            "    self._mean_moving_avg = tf.nest.map_structure(\n",
            "        lambda spec: create_variable('mean', 0, spec.shape, spec.dtype),\n",
            "        self._flat_variable_spec)\n",
            "    self._var_moving_avg = tf.nest.map_structure(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\tensor_normalizer.py",
        "line_number": 247,
        "API": ".reduce_mean(",
        "context": [
            "    def _tensor_update_ops(single_tensor, mean_var, var_var):\n",
            "      \"\"\"Make update ops for a single non-nested tensor.\"\"\"\n",
            "      # Take the moments across batch dimension. Calculate variance with\n",
            "      #   moving avg mean, so that this works even with batch size 1.\n",
            "      mean = tf.reduce_mean(single_tensor, axis=outer_dims)\n",
            "      var = tf.reduce_mean(\n",
            "          tf.math.squared_difference(single_tensor, mean_var), axis=outer_dims)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\tensor_normalizer.py",
        "line_number": 253,
        "API": ".control_dependencies(",
        "context": [
            "          tf.math.squared_difference(single_tensor, mean_var), axis=outer_dims)\n",
            "\n",
            "      # Ops to update moving average. Make sure that all stats are computed\n",
            "      #   before updates are performed.\n",
            "      with tf.control_dependencies([mean, var]):\n",
            "        update_ops = [\n",
            "            mean_var.assign_add(self._norm_update_rate * (mean - mean_var)),\n",
            "            var_var.assign_add(self._norm_update_rate * (var - var_var))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\tensor_normalizer.py",
        "line_number": 261,
        "API": ".flatten(",
        "context": [
            "        ]\n",
            "      return update_ops\n",
            "\n",
            "    # Aggregate update ops for all parts of potentially nested tensor.\n",
            "    tensor = tf.nest.flatten(tensor)\n",
            "    updates = tf.nest.map_structure(_tensor_update_ops, tensor,\n",
            "                                    self._mean_moving_avg, self._var_moving_avg)\n",
            "    all_update_ops = tf.nest.flatten(updates)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\tensor_normalizer.py",
        "line_number": 325,
        "API": ".size(",
        "context": [
            "    outer_shape = nest_utils.get_outer_shape(tensors, self._tensor_spec)\n",
            "    outer_rank_static = tf.compat.dimension_value(outer_shape.shape[0])\n",
            "    outer_axes = (\n",
            "        list(range(outer_rank_static)) if outer_rank_static is not None else\n",
            "        tf.range(tf.size(outer_shape)))\n",
            "\n",
            "    outer_n = tf.reduce_prod(outer_shape)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\tensor_normalizer.py",
        "line_number": 334,
        "API": ".cast(",
        "context": [
            "\n",
            "    update_ops = []\n",
            "\n",
            "    for i, t in enumerate(flat_tensors):\n",
            "      n_a = tf.cast(outer_n, self._count[i].dtype)\n",
            "      avg_a = tf.math.reduce_mean(t, outer_axes)\n",
            "      m2_a = tf.math.reduce_sum(tf.math.squared_difference(t, avg_a),\n",
            "                                outer_axes)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\tensor_normalizer.py",
        "line_number": 345,
        "API": ".control_dependencies(",
        "context": [
            "      m2_b_c = self._m2_carry[i]\n",
            "\n",
            "      n_ab, avg_ab, m2_ab, m2_ab_c = parallel_variance_calculation(\n",
            "          n_a, avg_a, m2_a, n_b, avg_b, m2_b, m2_b_c)\n",
            "      with tf.control_dependencies([n_ab, avg_ab, m2_ab, m2_ab_c]):\n",
            "        update_ops.extend([\n",
            "            self._count[i].assign(n_ab),\n",
            "            self._avg[i].assign(avg_ab),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\tensor_normalizer.py",
        "line_number": 350,
        "API": ".assign(",
        "context": [
            "        update_ops.extend([\n",
            "            self._count[i].assign(n_ab),\n",
            "            self._avg[i].assign(avg_ab),\n",
            "            self._m2[i].assign(m2_ab),\n",
            "            self._m2_carry[i].assign(m2_ab_c),\n",
            "        ])\n",
            "\n",
            "    return update_ops\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\tensor_normalizer.py",
        "line_number": 365,
        "API": ".assign(",
        "context": [
            "    \"\"\"Reset the count, mean and variance to its initial state.\"\"\"\n",
            "    reset_ops = []\n",
            "    for i in range(len(self._count)):\n",
            "      reset_ops.extend([\n",
            "          self._count[i].assign(_EPS * tf.ones_like(self._count[i])),\n",
            "          self._avg[i].assign(tf.zeros_like(self._avg[i])),\n",
            "          self._m2[i].assign(tf.zeros_like(self._m2[i])),\n",
            "          self._m2_carry[i].assign(tf.zeros_like(self._m2_carry[i])),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\tensor_normalizer.py",
        "line_number": 402,
        "API": ".shape(",
        "context": [
            "\n",
            "  To get `avg_a` and `m2_a` from a tensor `x` of shape `[n_a, ...]`, use:\n",
            "\n",
            "  ```\n",
            "  n_a = tf.shape(x)[0]\n",
            "  avg_a = tf.math.reduce_mean(x, axis=[0])\n",
            "  m2_a = tf.math.reduce_sum(tf.math.squared_difference(t, avg_a), axis=[0])\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\value_ops.py",
        "line_number": 42,
        "API": ".ones_like(",
        "context": [
            "  `T`: number of steps per trajectory.  This is equal to `N - n` in the equation\n",
            "       above.\n",
            "\n",
            "  **Note** To replicate the calculation `Q_n` exactly, use\n",
            "  `discounts = gamma * tf.ones_like(rewards)` and `provide_all_returns=False`.\n",
            "\n",
            "  Args:\n",
            "    rewards: Tensor with shape `[T, B]` (or `[T]`) representing rewards.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\value_ops.py",
        "line_number": 65,
        "API": ".transpose(",
        "context": [
            "      A tensor with shape `[B]` (or []) representing the discounted returns.\n",
            "  \"\"\"\n",
            "  if not time_major:\n",
            "    with tf.name_scope(\"to_time_major_tensors\"):\n",
            "      discounts = tf.transpose(discounts)\n",
            "      rewards = tf.transpose(rewards)\n",
            "\n",
            "  if final_value is None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\value_ops.py",
        "line_number": 76,
        "API": ".map_structure(",
        "context": [
            "    reward, discount = reward_discount\n",
            "    return accumulated_discounted_reward * discount + reward\n",
            "\n",
            "  if provide_all_returns:\n",
            "    returns = tf.nest.map_structure(\n",
            "        tf.stop_gradient,\n",
            "        tf.scan(\n",
            "            fn=discounted_return_fn,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\value_ops.py",
        "line_number": 86,
        "API": ".transpose(",
        "context": [
            "            initializer=final_value))\n",
            "\n",
            "    if not time_major:\n",
            "      with tf.name_scope(\"to_batch_major_tensors\"):\n",
            "        returns = tf.transpose(returns)\n",
            "  else:\n",
            "    returns = tf.foldr(\n",
            "        fn=discounted_return_fn,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\value_ops.py",
        "line_number": 94,
        "API": ".stop_gradient(",
        "context": [
            "        elems=(rewards, discounts),\n",
            "        initializer=final_value,\n",
            "        back_prop=False)\n",
            "\n",
            "  return tf.stop_gradient(returns)\n",
            "\n",
            "\n",
            "def generalized_advantage_estimation(values,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\value_ops.py",
        "line_number": 133,
        "API": ".transpose(",
        "context": [
            "  \"\"\"\n",
            "\n",
            "  if not time_major:\n",
            "    with tf.name_scope(\"to_time_major_tensors\"):\n",
            "      discounts = tf.transpose(discounts)\n",
            "      rewards = tf.transpose(rewards)\n",
            "      values = tf.transpose(values)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\value_ops.py",
        "line_number": 139,
        "API": ".concat(",
        "context": [
            "      values = tf.transpose(values)\n",
            "\n",
            "  with tf.name_scope(\"gae\"):\n",
            "\n",
            "    next_values = tf.concat(\n",
            "        [values[1:], tf.expand_dims(final_value, 0)], axis=0)\n",
            "    delta = rewards + discounts * next_values - values\n",
            "    weighted_discounts = discounts * td_lambda\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\value_ops.py",
        "line_number": 148,
        "API": ".map_structure(",
        "context": [
            "    def weighted_cumulative_td_fn(accumulated_td, reversed_weights_td_tuple):\n",
            "      weighted_discount, td = reversed_weights_td_tuple\n",
            "      return td + weighted_discount * accumulated_td\n",
            "\n",
            "    advantages = tf.nest.map_structure(\n",
            "        tf.stop_gradient,\n",
            "        tf.scan(\n",
            "            fn=weighted_cumulative_td_fn,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\value_ops.py",
        "line_number": 153,
        "API": ".zeros_like(",
        "context": [
            "        tf.stop_gradient,\n",
            "        tf.scan(\n",
            "            fn=weighted_cumulative_td_fn,\n",
            "            elems=(weighted_discounts, delta),\n",
            "            initializer=tf.zeros_like(final_value),\n",
            "            reverse=True))\n",
            "\n",
            "  if not time_major:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\value_ops.py",
        "line_number": 158,
        "API": ".transpose(",
        "context": [
            "            reverse=True))\n",
            "\n",
            "  if not time_major:\n",
            "    with tf.name_scope(\"to_batch_major_tensors\"):\n",
            "      advantages = tf.transpose(advantages)\n",
            "\n",
            "  return tf.stop_gradient(advantages)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\xla.py",
        "line_number": 36,
        "API": ".constant(",
        "context": [
            "  global _IS_XLA_AVAILABLE\n",
            "  # There's unfortunately no cleaner way to get the device other than creating a\n",
            "  # new op and querying it.\n",
            "  with tf.name_scope(\"is_xla_available\"):\n",
            "    device = tf.constant(0.0).device\n",
            "  if device not in _IS_XLA_AVAILABLE:\n",
            "    try:\n",
            "      # Take ourselves outside of any tf.function calls.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\xla.py",
        "line_number": 46,
        "API": ".device(",
        "context": [
            "        with tf.compat.v1.Graph().as_default():\n",
            "          # We'll use a session so we can be compatible with both TF1 and TF2\n",
            "          with tf.compat.v1.Session() as sess:\n",
            "            # Check for XLA on the given device.\n",
            "            with tf.device(device):\n",
            "              sess.run(tf.xla.experimental.compile(lambda: tf.constant(0.0)))\n",
            "    except (ValueError, tf.errors.InvalidArgumentError):\n",
            "      _IS_XLA_AVAILABLE[device] = False\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\xla.py",
        "line_number": 133,
        "API": ".map_structure(",
        "context": [
            "\n",
            "\n",
            "def _compiled(*args, _fn=None, _self=None, **kwargs):\n",
            "  \"\"\"Helper function for optionally XLA compiling `fn`.\"\"\"\n",
            "  args = tf.nest.map_structure(tf.convert_to_tensor, args)\n",
            "  kwargs = tf.nest.map_structure(tf.convert_to_tensor, kwargs)\n",
            "  if tf.compat.v1.executing_eagerly() or not is_xla_available():\n",
            "    if _self is not None:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tf_agents\\utils\\xla.py",
        "line_number": 141,
        "API": ".function(",
        "context": [
            "      return _fn(_self, *args, **kwargs)\n",
            "    else:\n",
            "      return _fn(*args, **kwargs)\n",
            "  else:\n",
            "    @tf.function(jit_compile=True)  # allow-tf-function\n",
            "    def _call_fn(*args, **kwargs):\n",
            "      if _self is not None:\n",
            "        return _fn(_self, *args, **kwargs)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\graph_builder.py",
        "line_number": 112,
        "API": ".format(",
        "context": [
            "      self.graph_agg = np.mean\n",
            "    elif graph_agg == GraphAggTypes.MEDIAN:\n",
            "      self.graph_agg = np.median\n",
            "    else:\n",
            "      raise ValueError('Unknown graph_agg:{}'.format(graph_agg))\n",
            "\n",
            "    # Makes the output path absolute for clarity.\n",
            "    self.output_dir = os.path.abspath(output_path)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\graph_builder.py",
        "line_number": 135,
        "API": ".info(",
        "context": [
            "    \"\"\"\n",
            "    data_collector, walltimes = [], []\n",
            "    for eventlog_dir in self.eventlog_dirs:\n",
            "      event_file = utils.find_event_log(eventlog_dir)\n",
            "      logging.info('Processing event file: %s', event_file)\n",
            "      data, total_time = utils.extract_event_log_values(event_file,\n",
            "                                                        self.event_tag,\n",
            "                                                        self.end_step)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\graph_builder.py",
        "line_number": 169,
        "API": ".get(",
        "context": [
            "    for key, value in sorted(base_data.items()):\n",
            "      entry = [key]\n",
            "      values = [value]\n",
            "      for data in data_collector[1:]:\n",
            "        values.append(data.get(key, -1))\n",
            "      mean_val = np.mean(values)\n",
            "      median_val = np.median(values)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\graph_builder.py",
        "line_number": 187,
        "API": ".join(",
        "context": [
            "    Args:\n",
            "      agg_data: 2d array of data to export to csv.\n",
            "    \"\"\"\n",
            "    # Outputs csv with aggregated data for each step.\n",
            "    csv_path = os.path.join(self.output_path,\n",
            "                            self.output_prefix + '_summary.csv')\n",
            "    with open(csv_path, 'w', newline='') as f:\n",
            "      writer = csv.writer(f)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\graph_builder.py",
        "line_number": 207,
        "API": ".debug(",
        "context": [
            "    # csv contains aggregate info that will get excluded in the pd.melt.\n",
            "    columns.extend(['median', 'mean'])\n",
            "    print(columns)\n",
            "    df = pd.DataFrame(agg_data, columns=columns)\n",
            "    logging.debug('Dataframes datatypes: %s', df.dtypes)\n",
            "    new_pd = pd.melt(\n",
            "        df,\n",
            "        id_vars='step',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\graph_builder.py",
        "line_number": 214,
        "API": ".info(",
        "context": [
            "        id_vars='step',\n",
            "        value_vars=list(df.columns[1:num_runs + 1]),\n",
            "        var_name='run',\n",
            "        value_name=self.yaxis_title)\n",
            "    logging.info('DataFrame to graph:\\n%s', new_pd)\n",
            "    # Build graph\n",
            "    plt.figure(figsize=(10, 5))\n",
            "    ax = sns.lineplot(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\graph_builder.py",
        "line_number": 222,
        "API": ".join(",
        "context": [
            "        data=new_pd, x='step', y=self.yaxis_title, estimator=self.graph_agg)\n",
            "    ax.set_title(self.title)\n",
            "    ax.set(xlabel=self.xaxis_title)\n",
            "    plt.ticklabel_format(style='plain', axis='x')\n",
            "    graph_path = os.path.join(self.output_path,\n",
            "                              self.output_prefix + '_graph.png')\n",
            "    plt.savefig(graph_path)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\graph_builder.py",
        "line_number": 234,
        "API": ".show(",
        "context": [
            "    self._output_csv(agg_data)\n",
            "    self._output_graph(agg_data, len(data_collector))\n",
            "\n",
            "    if self.show_graph:\n",
            "      plt.show()\n",
            "\n",
            "\n",
            "def main(_):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\release_builder.py",
        "line_number": 81,
        "API": ".format(",
        "context": [
            "      branch_hash: Git hash to use to create the new branch if needed.\n",
            "    \"\"\"\n",
            "    self.major, self.minor, self.patch, self.release = (\n",
            "        self._parse_version_input(release_number))\n",
            "    self.branch_name = 'r{}.{}.{}'.format(self.major, self.minor, self.patch)\n",
            "    self.tag_name = 'v{}.{}.{}'.format(self.major, self.minor, self.patch)\n",
            "    self.branch_hash = branch_hash\n",
            "    self.repo = self._get_repo(git_repo, working_dir)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\release_builder.py",
        "line_number": 86,
        "API": ".join(",
        "context": [
            "    self.tag_name = 'v{}.{}.{}'.format(self.major, self.minor, self.patch)\n",
            "    self.branch_hash = branch_hash\n",
            "    self.repo = self._get_repo(git_repo, working_dir)\n",
            "    if version_file:\n",
            "      self.version_file = os.path.join(self.repo.working_tree_dir, version_file)\n",
            "\n",
            "  def create_release_branch(self):\n",
            "    \"\"\"Creates a release branch and optionally an updated version file.\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\release_builder.py",
        "line_number": 91,
        "API": ".info(",
        "context": [
            "\n",
            "  def create_release_branch(self):\n",
            "    \"\"\"Creates a release branch and optionally an updated version file.\"\"\"\n",
            "    logging.info('Create release branch %s.', self.branch_name)\n",
            "    logging.info('Starting active branch:%s.', self.repo.active_branch)\n",
            "\n",
            "    self._checkout_or_create_branch()\n",
            "    if self.version_file:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\release_builder.py",
        "line_number": 98,
        "API": ".info(",
        "context": [
            "    if self.version_file:\n",
            "      updated = self._update_version_file()\n",
            "      if updated:\n",
            "        self.repo.remotes.origin.push(self.branch_name)\n",
            "        logging.info('Version file updated and pushed to remote %s',\n",
            "                     self.repo.remotes.origin.url)\n",
            "\n",
            "  def create_tag(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\release_builder.py",
        "line_number": 103,
        "API": ".info(",
        "context": [
            "                     self.repo.remotes.origin.url)\n",
            "\n",
            "  def create_tag(self):\n",
            "    \"\"\"Creates a tag from the branch.\"\"\"\n",
            "    logging.info('Create tag %s', self.tag_name)\n",
            "    logging.info('Starting active branch:%s', self.repo.active_branch)\n",
            "\n",
            "    self._checkout_or_create_branch()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\release_builder.py",
        "line_number": 108,
        "API": ".info(",
        "context": [
            "    logging.info('Starting active branch:%s', self.repo.active_branch)\n",
            "\n",
            "    self._checkout_or_create_branch()\n",
            "    if not any(x.name == self.tag_name for x in self.repo.tags):\n",
            "      logging.info('Create %s tag.', self.tag_name)\n",
            "      self.repo.create_tag(self.tag_name)\n",
            "    self.repo.remotes.origin.push(self.tag_name)\n",
            "    logging.info('Created tag %s', self.tag_name)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\release_builder.py",
        "line_number": 127,
        "API": ".split(",
        "context": [
            "        minor: Minor version number.\n",
            "        patch: Patch version number.\n",
            "        release: Suffix for the release, e.g. rc0. Often an empty string.\n",
            "    \"\"\"\n",
            "    parts = release_number.split('.')\n",
            "    if len(parts) < 3:\n",
            "      logging.error('FLAGS.version_number must be x.x.x or x.x.x.y')\n",
            "    elif len(parts) == 3:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\release_builder.py",
        "line_number": 145,
        "API": ".join(",
        "context": [
            "\n",
            "    Returns:\n",
            "      True if commit takes place or false if no change.\n",
            "    \"\"\"\n",
            "    file_paths = [os.path.join(self.repo.working_tree_dir, file)]\n",
            "    if self.repo.index.diff(None, paths=file_paths):\n",
            "      index = self.repo.index\n",
            "      index.add(file_paths)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\release_builder.py",
        "line_number": 150,
        "API": ".info(",
        "context": [
            "    if self.repo.index.diff(None, paths=file_paths):\n",
            "      index = self.repo.index\n",
            "      index.add(file_paths)\n",
            "      index.commit(msg)\n",
            "      logging.info('%s committed locally.', file)\n",
            "      return True\n",
            "    else:\n",
            "      return False\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\release_builder.py",
        "line_number": 161,
        "API": ".join(",
        "context": [
            "\n",
            "    Returns:\n",
            "      True if the version file was committed, False if no change was needed.\n",
            "    \"\"\"\n",
            "    file_path = os.path.join(self.repo.working_tree_dir, self.version_file)\n",
            "    self._update_version_numbers(file_path)\n",
            "    return self._commit_file(\n",
            "        self.version_file, 'Version updated for release {}.{}.{}{}.'.format(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\release_builder.py",
        "line_number": 176,
        "API": ".input(",
        "context": [
            "\n",
            "    Args:\n",
            "      file_path: path to the version file.\n",
            "    \"\"\"\n",
            "    with fileinput.input(files=(file_path), inplace=True) as f:\n",
            "      for line in f:\n",
            "        logging.info(line)\n",
            "        if line.startswith('_MAJOR_VERSION '):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\release_builder.py",
        "line_number": 184,
        "API": ".format(",
        "context": [
            "          print(\"_MAJOR_VERSION = '{}'\".format(self.major))\n",
            "          continue\n",
            "\n",
            "        if line.startswith('_MINOR_VERSION '):\n",
            "          print(\"_MINOR_VERSION = '{}'\".format(self.minor))\n",
            "          continue\n",
            "\n",
            "        if line.startswith('_PATCH_VERSION '):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\release_builder.py",
        "line_number": 192,
        "API": ".format(",
        "context": [
            "          print(\"_PATCH_VERSION = '{}'\".format(self.patch))\n",
            "          continue\n",
            "\n",
            "        if line.startswith('_REL_SUFFIX '):\n",
            "          print(\"_REL_SUFFIX = '{}'\".format(self.release))\n",
            "          continue\n",
            "\n",
            "        print(line, end='')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\release_builder.py",
        "line_number": 217,
        "API": ".info(",
        "context": [
            "        self.repo.git.checkout('origin/' + self.branch_name, b=self.branch_name)\n",
            "      except GitCommandError:\n",
            "        self.repo.create_head(\n",
            "            self.branch_name, commit=self.branch_hash).checkout()\n",
            "        logging.info('Created branch %s from hash %s.', self.repo.active_branch,\n",
            "                     self.branch_hash)\n",
            "\n",
            "    self.repo.git.push('--set-upstream', 'origin', self.branch_name)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\release_builder.py",
        "line_number": 222,
        "API": ".info(",
        "context": [
            "                     self.branch_hash)\n",
            "\n",
            "    self.repo.git.push('--set-upstream', 'origin', self.branch_name)\n",
            "    logging.info('Branch pushed to remote %s.', self.repo.remotes.origin.url)\n",
            "    logging.info('Active branch changed to:%s', self.repo.active_branch)\n",
            "\n",
            "  def _get_repo(self, git_repo, working_dir):\n",
            "    \"\"\"Clones repo of points to existing repo.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\release_builder.py",
        "line_number": 234,
        "API": ".makedirs(",
        "context": [
            "\n",
            "    Returns:\n",
            "      Repo object representing the git repository.\n",
            "    \"\"\"\n",
            "    os.makedirs(working_dir, exist_ok=True)\n",
            "    try:\n",
            "      return Repo(working_dir)\n",
            "    except InvalidGitRepositoryError:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\agents\\tools\\release_builder.py",
        "line_number": 242,
        "API": ".set_verbosity(",
        "context": [
            "      return Repo.clone_from(git_repo, working_dir)\n",
            "\n",
            "\n",
            "def main(_):\n",
            "  logging.set_verbosity(logging.INFO)\n",
            "  release_build = ReleaseBuilder(FLAGS.git_repo, FLAGS.version_file,\n",
            "                                 FLAGS.release_number, FLAGS.working_dir,\n",
            "                                 FLAGS.branch_hash)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\game.py",
        "line_number": 13,
        "API": ".get(",
        "context": [
            "class Board(object):\n",
            "    \"\"\"board for the game\"\"\"\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.width = int(kwargs.get('width', 8))\n",
            "        self.height = int(kwargs.get('height', 8))\n",
            "        # board states stored as a dict,\n",
            "        # key: move as location on the board,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\game.py",
        "line_number": 20,
        "API": ".get(",
        "context": [
            "        # key: move as location on the board,\n",
            "        # value: player as pieces type\n",
            "        self.states = {}\n",
            "        # need how many pieces in a row to win\n",
            "        self.n_in_row = int(kwargs.get('n_in_row', 5))\n",
            "        self.players = [1, 2]  # player1 and player2\n",
            "\n",
            "    def init_board(self, start_player=0):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\game.py",
        "line_number": 26,
        "API": ".format(",
        "context": [
            "\n",
            "    def init_board(self, start_player=0):\n",
            "        if self.width < self.n_in_row or self.height < self.n_in_row:\n",
            "            raise Exception('board width and height can not be '\n",
            "                            'less than {}'.format(self.n_in_row))\n",
            "        self.current_player = self.players[start_player]  # start player\n",
            "        # keep available moves in a list\n",
            "        self.availables = list(range(self.width * self.height))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\game.py",
        "line_number": 60,
        "API": ".zeros(",
        "context": [
            "        \"\"\"return the board state from the perspective of the current player.\n",
            "        state shape: 4*width*height\n",
            "        \"\"\"\n",
            "\n",
            "        square_state = np.zeros((4, self.width, self.height))\n",
            "        if self.states:\n",
            "            moves, players = np.array(list(zip(*self.states.items())))\n",
            "            move_curr = moves[players == self.current_player]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\game.py",
        "line_number": 101,
        "API": ".get(",
        "context": [
            "            w = m % width\n",
            "            player = states[m]\n",
            "\n",
            "            if (w in range(width - n + 1) and\n",
            "                    len(set(states.get(i, -1) for i in range(m, m + n))) == 1):\n",
            "                return True, player\n",
            "\n",
            "            if (h in range(height - n + 1) and\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\game.py",
        "line_number": 109,
        "API": ".get(",
        "context": [
            "                    len(set(states.get(i, -1) for i in range(m, m + n * width, width))) == 1):\n",
            "                return True, player\n",
            "\n",
            "            if (w in range(width - n + 1) and h in range(height - n + 1) and\n",
            "                    len(set(states.get(i, -1) for i in range(m, m + n * (width + 1), width + 1))) == 1):\n",
            "                return True, player\n",
            "\n",
            "            if (w in range(n - 1, width) and h in range(height - n + 1) and\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\game.py",
        "line_number": 146,
        "API": ".format(",
        "context": [
            "        print(\"Player\", player1, \"with X\".rjust(3))\n",
            "        print(\"Player\", player2, \"with O\".rjust(3))\n",
            "        print()\n",
            "        for x in range(width):\n",
            "            print(\"{0:8}\".format(x), end='')\n",
            "        print('\\r\\n')\n",
            "        for i in range(height - 1, -1, -1):\n",
            "            print(\"{0:4d}\".format(i), end='')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\game.py",
        "line_number": 152,
        "API": ".get(",
        "context": [
            "        for i in range(height - 1, -1, -1):\n",
            "            print(\"{0:4d}\".format(i), end='')\n",
            "            for j in range(width):\n",
            "                loc = i * width + j\n",
            "                p = board.states.get(loc, -1)\n",
            "                if p == player1:\n",
            "                    print('X'.center(8), end='')\n",
            "                elif p == player2:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\game.py",
        "line_number": 211,
        "API": ".zeros(",
        "context": [
            "                self.graphic(self.board, p1, p2)\n",
            "            end, winner = self.board.game_end()\n",
            "            if end:\n",
            "                # winner from the perspective of the current player of each state\n",
            "                winners_z = np.zeros(len(current_players))\n",
            "                if winner != -1:\n",
            "                    winners_z[np.array(current_players) == winner] = 1.0\n",
            "                    winners_z[np.array(current_players) != winner] = -1.0\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\human_play.py",
        "line_number": 35,
        "API": ".split(",
        "context": [
            "    def get_action(self, board):\n",
            "        try:\n",
            "            location = input(\"Your move: \")\n",
            "            if isinstance(location, str):  # for python3\n",
            "                location = [int(n, 10) for n in location.split(\",\")]\n",
            "            move = board.location_to_move(location)\n",
            "        except Exception as e:\n",
            "            move = -1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\human_play.py",
        "line_number": 45,
        "API": ".format(",
        "context": [
            "            move = self.get_action(board)\n",
            "        return move\n",
            "\n",
            "    def __str__(self):\n",
            "        return \"Human {}\".format(self.player)\n",
            "\n",
            "\n",
            "def run():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\human_play.py",
        "line_number": 64,
        "API": ".load(",
        "context": [
            "        # mcts_player = MCTSPlayer(best_policy.policy_value_fn, c_puct=5, n_playout=400)\n",
            "\n",
            "        # load the provided model (trained in Theano/Lasagne) into a MCTS player written in pure numpy\n",
            "        try:\n",
            "            policy_param = pickle.load(open(model_file, 'rb'))\n",
            "        except:\n",
            "            policy_param = pickle.load(open(model_file, 'rb'),\n",
            "                                       encoding='bytes')  # To support python3\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\mcts_alphaZero.py",
        "line_number": 13,
        "API": ".max(",
        "context": [
            "import copy\n",
            "\n",
            "\n",
            "def softmax(x):\n",
            "    probs = np.exp(x - np.max(x))\n",
            "    probs /= np.sum(probs)\n",
            "    return probs\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\mcts_alphaZero.py",
        "line_number": 76,
        "API": ".sqrt(",
        "context": [
            "        c_puct: a number in (0, inf) controlling the relative impact of\n",
            "            value Q, and prior probability P, on this node's score.\n",
            "        \"\"\"\n",
            "        self._u = (c_puct * self._P *\n",
            "                   np.sqrt(self._parent._n_visits) / (1 + self._n_visits))\n",
            "        return self._Q + self._u\n",
            "\n",
            "    def is_leaf(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\mcts_alphaZero.py",
        "line_number": 125,
        "API": ".expand(",
        "context": [
            "        action_probs, leaf_value = self._policy(state)\n",
            "        # Check for end of game.\n",
            "        end, winner = state.game_end()\n",
            "        if not end:\n",
            "            node.expand(action_probs)\n",
            "        else:\n",
            "            # for end state\uff0creturn the \"true\" leaf_value\n",
            "            if winner == -1:  # tie\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\mcts_alphaZero.py",
        "line_number": 152,
        "API": ".log(",
        "context": [
            "        # calc the move probabilities based on visit counts at the root node\n",
            "        act_visits = [(act, node._n_visits)\n",
            "                      for act, node in self._root._children.items()]\n",
            "        acts, visits = zip(*act_visits)\n",
            "        act_probs = softmax(1.0/temp * np.log(np.array(visits) + 1e-10))\n",
            "\n",
            "        return acts, act_probs\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\mcts_alphaZero.py",
        "line_number": 187,
        "API": ".zeros(",
        "context": [
            "\n",
            "    def get_action(self, board, temp=1e-3, return_prob=0):\n",
            "        sensible_moves = board.availables\n",
            "        # the pi vector returned by MCTS as in the alphaGo Zero paper\n",
            "        move_probs = np.zeros(board.width*board.height)\n",
            "        if len(sensible_moves) > 0:\n",
            "            acts, probs = self.mcts.get_move_probs(board, temp)\n",
            "            move_probs[list(acts)] = probs\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\mcts_alphaZero.py",
        "line_number": 196,
        "API": ".ones(",
        "context": [
            "                # add Dirichlet Noise for exploration (needed for\n",
            "                # self-play training)\n",
            "                move = np.random.choice(\n",
            "                    acts,\n",
            "                    p=0.75*probs + 0.25*np.random.dirichlet(0.3*np.ones(len(probs)))\n",
            "                )\n",
            "                # update the root node and reuse the search tree\n",
            "                self.mcts.update_with_move(move)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\mcts_alphaZero.py",
        "line_number": 217,
        "API": ".format(",
        "context": [
            "        else:\n",
            "            print(\"WARNING: the board is full\")\n",
            "\n",
            "    def __str__(self):\n",
            "        return \"MCTS {}\".format(self.player)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\mcts_pure.py",
        "line_number": 23,
        "API": ".ones(",
        "context": [
            "def policy_value_fn(board):\n",
            "    \"\"\"a function that takes in a state and outputs a list of (action, probability)\n",
            "    tuples and a score for the state\"\"\"\n",
            "    # return uniform probabilities and 0 score for pure MCTS\n",
            "    action_probs = np.ones(len(board.availables))/len(board.availables)\n",
            "    return zip(board.availables, action_probs), 0\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\mcts_pure.py",
        "line_number": 131,
        "API": ".expand(",
        "context": [
            "        action_probs, _ = self._policy(state)\n",
            "        # Check for end of game\n",
            "        end, winner = state.game_end()\n",
            "        if not end:\n",
            "            node.expand(action_probs)\n",
            "        # Evaluate the leaf node by random rollout\n",
            "        leaf_value = self._evaluate_rollout(state)\n",
            "        # Update value and visit count of nodes in this traversal.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net.py",
        "line_number": 19,
        "API": ".scalar(",
        "context": [
            "    \"\"\"policy-value network \"\"\"\n",
            "    def __init__(self, board_width, board_height, model_file=None):\n",
            "        self.board_width = board_width\n",
            "        self.board_height = board_height\n",
            "        self.learning_rate = T.scalar('learning_rate')\n",
            "        self.l2_const = 1e-4  # coef of l2 penalty\n",
            "        self.create_policy_value_net()\n",
            "        self._loss_train_op()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net.py",
        "line_number": 25,
        "API": ".load(",
        "context": [
            "        self.create_policy_value_net()\n",
            "        self._loss_train_op()\n",
            "        if model_file:\n",
            "            try:\n",
            "                net_params = pickle.load(open(model_file, 'rb'))\n",
            "            except:\n",
            "                # To support loading pretrained model in python3\n",
            "                net_params = pickle.load(open(model_file, 'rb'),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net.py",
        "line_number": 66,
        "API": ".function(",
        "context": [
            "                nonlinearity=lasagne.nonlinearities.tanh)\n",
            "        # get action probs and state score value\n",
            "        self.action_probs, self.value = lasagne.layers.get_output(\n",
            "                [self.policy_net, self.value_net])\n",
            "        self.policy_value = theano.function([self.state_input],\n",
            "                                            [self.action_probs, self.value],\n",
            "                                            allow_input_downcast=True)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net.py",
        "line_number": 79,
        "API": ".reshape(",
        "context": [
            "        \"\"\"\n",
            "        legal_positions = board.availables\n",
            "        current_state = board.current_state()\n",
            "        act_probs, value = self.policy_value(\n",
            "            current_state.reshape(-1, 4, self.board_width, self.board_height)\n",
            "            )\n",
            "        act_probs = zip(legal_positions, act_probs.flatten()[legal_positions])\n",
            "        return act_probs, value[0][0]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net.py",
        "line_number": 92,
        "API": ".flatten(",
        "context": [
            "        \"\"\"\n",
            "        params = lasagne.layers.get_all_params(\n",
            "                [self.policy_net, self.value_net], trainable=True)\n",
            "        value_loss = lasagne.objectives.squared_error(\n",
            "                self.winner, self.value.flatten())\n",
            "        policy_loss = lasagne.objectives.categorical_crossentropy(\n",
            "                self.action_probs, self.mcts_probs)\n",
            "        l2_penalty = lasagne.regularization.apply_penalty(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net.py",
        "line_number": 100,
        "API": ".sum(",
        "context": [
            "                params, lasagne.regularization.l2)\n",
            "        self.loss = self.l2_const*l2_penalty + lasagne.objectives.aggregate(\n",
            "                value_loss + policy_loss, mode='mean')\n",
            "        # policy entropy\uff0cfor monitoring only\n",
            "        self.entropy = -T.mean(T.sum(\n",
            "                self.action_probs * T.log(self.action_probs + 1e-10), axis=1))\n",
            "        # get the train op\n",
            "        updates = lasagne.updates.adam(self.loss, params,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net.py",
        "line_number": 105,
        "API": ".function(",
        "context": [
            "                self.action_probs * T.log(self.action_probs + 1e-10), axis=1))\n",
            "        # get the train op\n",
            "        updates = lasagne.updates.adam(self.loss, params,\n",
            "                                       learning_rate=self.learning_rate)\n",
            "        self.train_step = theano.function(\n",
            "            [self.state_input, self.mcts_probs, self.winner, self.learning_rate],\n",
            "            [self.loss, self.entropy],\n",
            "            updates=updates,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_keras.py",
        "line_number": 36,
        "API": ".load(",
        "context": [
            "        self.create_policy_value_net()   \n",
            "        self._loss_train_op()\n",
            "\n",
            "        if model_file:\n",
            "            net_params = pickle.load(open(model_file, 'rb'))\n",
            "            self.model.set_weights(net_params)\n",
            "        \n",
            "    def create_policy_value_net(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_keras.py",
        "line_number": 60,
        "API": ".array(",
        "context": [
            "\n",
            "        self.model = Model(in_x, [self.policy_net, self.value_net])\n",
            "        \n",
            "        def policy_value(state_input):\n",
            "            state_input_union = np.array(state_input)\n",
            "            results = self.model.predict_on_batch(state_input_union)\n",
            "            return results\n",
            "        self.policy_value = policy_value\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_keras.py",
        "line_number": 72,
        "API": ".reshape(",
        "context": [
            "        output: a list of (action, probability) tuples for each available action and the score of the board state\n",
            "        \"\"\"\n",
            "        legal_positions = board.availables\n",
            "        current_state = board.current_state()\n",
            "        act_probs, value = self.policy_value(current_state.reshape(-1, 4, self.board_width, self.board_height))\n",
            "        act_probs = zip(legal_positions, act_probs.flatten()[legal_positions])\n",
            "        return act_probs, value[0][0]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_keras.py",
        "line_number": 85,
        "API": ".compile(",
        "context": [
            "\n",
            "        # get the train op   \n",
            "        opt = Adam()\n",
            "        losses = ['categorical_crossentropy', 'mean_squared_error']\n",
            "        self.model.compile(optimizer=opt, loss=losses)\n",
            "\n",
            "        def self_entropy(probs):\n",
            "            return -np.mean(np.sum(probs * np.log(probs + 1e-10), axis=1))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_keras.py",
        "line_number": 91,
        "API": ".array(",
        "context": [
            "        def self_entropy(probs):\n",
            "            return -np.mean(np.sum(probs * np.log(probs + 1e-10), axis=1))\n",
            "\n",
            "        def train_step(state_input, mcts_probs, winner, learning_rate):\n",
            "            state_input_union = np.array(state_input)\n",
            "            mcts_probs_union = np.array(mcts_probs)\n",
            "            winner_union = np.array(winner)\n",
            "            loss = self.model.evaluate(state_input_union, [mcts_probs_union, winner_union], batch_size=len(state_input), verbose=0)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_numpy.py",
        "line_number": 14,
        "API": ".max(",
        "context": [
            "\n",
            "\n",
            "# some utility functions\n",
            "def softmax(x):\n",
            "    probs = np.exp(x - np.max(x))\n",
            "    probs /= np.sum(probs)\n",
            "    return probs\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_numpy.py",
        "line_number": 20,
        "API": ".maximum(",
        "context": [
            "    return probs\n",
            "\n",
            "\n",
            "def relu(X):\n",
            "    out = np.maximum(X, 0)\n",
            "    return out\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_numpy.py",
        "line_number": 35,
        "API": ".reshape(",
        "context": [
            "    w_out = (w_x - w_filter + 2 * padding) / stride + 1\n",
            "    h_out, w_out = int(h_out), int(w_out)\n",
            "    X_col = im2col_indices(X, h_filter, w_filter,\n",
            "                           padding=padding, stride=stride)\n",
            "    W_col = W.reshape(n_filters, -1)\n",
            "    out = (np.dot(W_col, X_col).T + b).T\n",
            "    out = out.reshape(n_filters, h_out, w_out, n_x)\n",
            "    out = out.transpose(3, 0, 1, 2)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_numpy.py",
        "line_number": 56,
        "API": ".repeat(",
        "context": [
            "    assert (W + 2 * padding - field_height) % stride == 0\n",
            "    out_height = int((H + 2 * padding - field_height) / stride + 1)\n",
            "    out_width = int((W + 2 * padding - field_width) / stride + 1)\n",
            "\n",
            "    i0 = np.repeat(np.arange(field_height), field_width)\n",
            "    i0 = np.tile(i0, C)\n",
            "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
            "    j0 = np.tile(np.arange(field_width), field_height * C)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_numpy.py",
        "line_number": 61,
        "API": ".reshape(",
        "context": [
            "    i0 = np.tile(i0, C)\n",
            "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
            "    j0 = np.tile(np.arange(field_width), field_height * C)\n",
            "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
            "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
            "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
            "\n",
            "    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_numpy.py",
        "line_number": 73,
        "API": ".pad(",
        "context": [
            "def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
            "    \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
            "    # Zero-pad the input\n",
            "    p = padding\n",
            "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
            "\n",
            "    k, i, j = get_im2col_indices(x.shape, field_height,\n",
            "                                 field_width, padding, stride)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_numpy.py",
        "line_number": 80,
        "API": ".reshape(",
        "context": [
            "                                 field_width, padding, stride)\n",
            "\n",
            "    cols = x_padded[:, k, i, j]\n",
            "    C = x.shape[1]\n",
            "    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
            "    return cols\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_numpy.py",
        "line_number": 100,
        "API": ".reshape(",
        "context": [
            "        \"\"\"\n",
            "        legal_positions = board.availables\n",
            "        current_state = board.current_state()\n",
            "\n",
            "        X = current_state.reshape(-1, 4, self.board_width, self.board_height)\n",
            "        # first 3 conv layers with ReLu nonlinearity\n",
            "        for i in [0, 2, 4]:\n",
            "            X = relu(conv_forward(X, self.params[i], self.params[i+1]))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_numpy.py",
        "line_number": 106,
        "API": ".flatten(",
        "context": [
            "        for i in [0, 2, 4]:\n",
            "            X = relu(conv_forward(X, self.params[i], self.params[i+1]))\n",
            "        # policy head\n",
            "        X_p = relu(conv_forward(X, self.params[6], self.params[7], padding=0))\n",
            "        X_p = fc_forward(X_p.flatten(), self.params[8], self.params[9])\n",
            "        act_probs = softmax(X_p)\n",
            "        # value head\n",
            "        X_v = relu(conv_forward(X, self.params[10],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_numpy.py",
        "line_number": 111,
        "API": ".flatten(",
        "context": [
            "        act_probs = softmax(X_p)\n",
            "        # value head\n",
            "        X_v = relu(conv_forward(X, self.params[10],\n",
            "                                self.params[11], padding=0))\n",
            "        X_v = relu(fc_forward(X_v.flatten(), self.params[12], self.params[13]))\n",
            "        value = np.tanh(fc_forward(X_v, self.params[14], self.params[15]))[0]\n",
            "        act_probs = zip(legal_positions, act_probs.flatten()[legal_positions])\n",
            "        return act_probs, value\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_pytorch.py",
        "line_number": 44,
        "API": ".relu(",
        "context": [
            "        self.val_fc2 = nn.Linear(64, 1)\n",
            "\n",
            "    def forward(self, state_input):\n",
            "        # common layers\n",
            "        x = F.relu(self.conv1(state_input))\n",
            "        x = F.relu(self.conv2(x))\n",
            "        x = F.relu(self.conv3(x))\n",
            "        # action policy layers\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_pytorch.py",
        "line_number": 49,
        "API": ".view(",
        "context": [
            "        x = F.relu(self.conv2(x))\n",
            "        x = F.relu(self.conv3(x))\n",
            "        # action policy layers\n",
            "        x_act = F.relu(self.act_conv1(x))\n",
            "        x_act = x_act.view(-1, 4*self.board_width*self.board_height)\n",
            "        x_act = F.log_softmax(self.act_fc1(x_act))\n",
            "        # state value layers\n",
            "        x_val = F.relu(self.val_conv1(x))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_pytorch.py",
        "line_number": 54,
        "API": ".relu(",
        "context": [
            "        x_act = F.log_softmax(self.act_fc1(x_act))\n",
            "        # state value layers\n",
            "        x_val = F.relu(self.val_conv1(x))\n",
            "        x_val = x_val.view(-1, 2*self.board_width*self.board_height)\n",
            "        x_val = F.relu(self.val_fc1(x_val))\n",
            "        x_val = F.tanh(self.val_fc2(x_val))\n",
            "        return x_act, x_val\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_pytorch.py",
        "line_number": 76,
        "API": ".load(",
        "context": [
            "        self.optimizer = optim.Adam(self.policy_value_net.parameters(),\n",
            "                                    weight_decay=self.l2_const)\n",
            "\n",
            "        if model_file:\n",
            "            net_params = torch.load(model_file)\n",
            "            self.policy_value_net.load_state_dict(net_params)\n",
            "\n",
            "    def policy_value(self, state_batch):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_pytorch.py",
        "line_number": 87,
        "API": ".exp(",
        "context": [
            "        \"\"\"\n",
            "        if self.use_gpu:\n",
            "            state_batch = Variable(torch.FloatTensor(state_batch).cuda())\n",
            "            log_act_probs, value = self.policy_value_net(state_batch)\n",
            "            act_probs = np.exp(log_act_probs.data.cpu().numpy())\n",
            "            return act_probs, value.data.cpu().numpy()\n",
            "        else:\n",
            "            state_batch = Variable(torch.FloatTensor(state_batch))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_pytorch.py",
        "line_number": 92,
        "API": ".exp(",
        "context": [
            "            return act_probs, value.data.cpu().numpy()\n",
            "        else:\n",
            "            state_batch = Variable(torch.FloatTensor(state_batch))\n",
            "            log_act_probs, value = self.policy_value_net(state_batch)\n",
            "            act_probs = np.exp(log_act_probs.data.numpy())\n",
            "            return act_probs, value.data.numpy()\n",
            "\n",
            "    def policy_value_fn(self, board):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_pytorch.py",
        "line_number": 102,
        "API": ".reshape(",
        "context": [
            "        output: a list of (action, probability) tuples for each available\n",
            "        action and the score of the board state\n",
            "        \"\"\"\n",
            "        legal_positions = board.availables\n",
            "        current_state = np.ascontiguousarray(board.current_state().reshape(\n",
            "                -1, 4, self.board_width, self.board_height))\n",
            "        if self.use_gpu:\n",
            "            log_act_probs, value = self.policy_value_net(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_pytorch.py",
        "line_number": 107,
        "API": ".exp(",
        "context": [
            "                -1, 4, self.board_width, self.board_height))\n",
            "        if self.use_gpu:\n",
            "            log_act_probs, value = self.policy_value_net(\n",
            "                    Variable(torch.from_numpy(current_state)).cuda().float())\n",
            "            act_probs = np.exp(log_act_probs.data.cpu().numpy().flatten())\n",
            "        else:\n",
            "            log_act_probs, value = self.policy_value_net(\n",
            "                    Variable(torch.from_numpy(current_state)).float())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_pytorch.py",
        "line_number": 137,
        "API": ".view(",
        "context": [
            "        # forward\n",
            "        log_act_probs, value = self.policy_value_net(state_batch)\n",
            "        # define the loss = (z - v)^2 - pi^T * log(p) + c||theta||^2\n",
            "        # Note: the L2 penalty is incorporated in optimizer\n",
            "        value_loss = F.mse_loss(value.view(-1), winner_batch)\n",
            "        policy_loss = -torch.mean(torch.sum(mcts_probs*log_act_probs, 1))\n",
            "        loss = value_loss + policy_loss\n",
            "        # backward and optimize\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_pytorch.py",
        "line_number": 144,
        "API": ".mean(",
        "context": [
            "        # backward and optimize\n",
            "        loss.backward()\n",
            "        self.optimizer.step()\n",
            "        # calc policy entropy, for monitoring only\n",
            "        entropy = -torch.mean(\n",
            "                torch.sum(torch.exp(log_act_probs) * log_act_probs, 1)\n",
            "                )\n",
            "        return loss.data[0], entropy.data[0]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_pytorch.py",
        "line_number": 149,
        "API": ".item(",
        "context": [
            "                torch.sum(torch.exp(log_act_probs) * log_act_probs, 1)\n",
            "                )\n",
            "        return loss.data[0], entropy.data[0]\n",
            "        #for pytorch version >= 0.5 please use the following line instead.\n",
            "        #return loss.item(), entropy.item()\n",
            "\n",
            "    def get_policy_param(self):\n",
            "        net_params = self.policy_value_net.state_dict()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_pytorch.py",
        "line_number": 158,
        "API": ".save(",
        "context": [
            "\n",
            "    def save_model(self, model_file):\n",
            "        \"\"\" save model params to file \"\"\"\n",
            "        net_params = self.get_policy_param()  # get model params\n",
            "        torch.save(net_params, model_file)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_tensorflow.py",
        "line_number": 19,
        "API": ".placeholder(",
        "context": [
            "        self.board_height = board_height\n",
            "\n",
            "        # Define the tensorflow neural network\n",
            "        # 1. Input:\n",
            "        self.input_states = tf.placeholder(\n",
            "                tf.float32, shape=[None, 4, board_height, board_width])\n",
            "        self.input_state = tf.transpose(self.input_states, [0, 2, 3, 1])\n",
            "        # 2. Common Networks Layers\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_tensorflow.py",
        "line_number": 27,
        "API": ".conv2d(",
        "context": [
            "        self.conv1 = tf.layers.conv2d(inputs=self.input_state,\n",
            "                                      filters=32, kernel_size=[3, 3],\n",
            "                                      padding=\"same\", data_format=\"channels_last\",\n",
            "                                      activation=tf.nn.relu)\n",
            "        self.conv2 = tf.layers.conv2d(inputs=self.conv1, filters=64,\n",
            "                                      kernel_size=[3, 3], padding=\"same\",\n",
            "                                      data_format=\"channels_last\",\n",
            "                                      activation=tf.nn.relu)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_tensorflow.py",
        "line_number": 36,
        "API": ".conv2d(",
        "context": [
            "                                      kernel_size=[3, 3], padding=\"same\",\n",
            "                                      data_format=\"channels_last\",\n",
            "                                      activation=tf.nn.relu)\n",
            "        # 3-1 Action Networks\n",
            "        self.action_conv = tf.layers.conv2d(inputs=self.conv3, filters=4,\n",
            "                                            kernel_size=[1, 1], padding=\"same\",\n",
            "                                            data_format=\"channels_last\",\n",
            "                                            activation=tf.nn.relu)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_tensorflow.py",
        "line_number": 41,
        "API": ".reshape(",
        "context": [
            "                                            kernel_size=[1, 1], padding=\"same\",\n",
            "                                            data_format=\"channels_last\",\n",
            "                                            activation=tf.nn.relu)\n",
            "        # Flatten the tensor\n",
            "        self.action_conv_flat = tf.reshape(\n",
            "                self.action_conv, [-1, 4 * board_height * board_width])\n",
            "        # 3-2 Full connected layer, the output is the log probability of moves\n",
            "        # on each slot on the board\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_tensorflow.py",
        "line_number": 49,
        "API": ".conv2d(",
        "context": [
            "        self.action_fc = tf.layers.dense(inputs=self.action_conv_flat,\n",
            "                                         units=board_height * board_width,\n",
            "                                         activation=tf.nn.log_softmax)\n",
            "        # 4 Evaluation Networks\n",
            "        self.evaluation_conv = tf.layers.conv2d(inputs=self.conv3, filters=2,\n",
            "                                                kernel_size=[1, 1],\n",
            "                                                padding=\"same\",\n",
            "                                                data_format=\"channels_last\",\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_tensorflow.py",
        "line_number": 54,
        "API": ".reshape(",
        "context": [
            "                                                kernel_size=[1, 1],\n",
            "                                                padding=\"same\",\n",
            "                                                data_format=\"channels_last\",\n",
            "                                                activation=tf.nn.relu)\n",
            "        self.evaluation_conv_flat = tf.reshape(\n",
            "                self.evaluation_conv, [-1, 2 * board_height * board_width])\n",
            "        self.evaluation_fc1 = tf.layers.dense(inputs=self.evaluation_conv_flat,\n",
            "                                              units=64, activation=tf.nn.relu)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_tensorflow.py",
        "line_number": 64,
        "API": ".placeholder(",
        "context": [
            "                                              units=1, activation=tf.nn.tanh)\n",
            "\n",
            "        # Define the Loss function\n",
            "        # 1. Label: the array containing if the game wins or not for each state\n",
            "        self.labels = tf.placeholder(tf.float32, shape=[None, 1])\n",
            "        # 2. Predictions: the array containing the evaluation score of each state\n",
            "        # which is self.evaluation_fc2\n",
            "        # 3-1. Value Loss function\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_tensorflow.py",
        "line_number": 71,
        "API": ".placeholder(",
        "context": [
            "        # 3-1. Value Loss function\n",
            "        self.value_loss = tf.losses.mean_squared_error(self.labels,\n",
            "                                                       self.evaluation_fc2)\n",
            "        # 3-2. Policy Loss function\n",
            "        self.mcts_probs = tf.placeholder(\n",
            "                tf.float32, shape=[None, board_height * board_width])\n",
            "        self.policy_loss = tf.negative(tf.reduce_mean(\n",
            "                tf.reduce_sum(tf.multiply(self.mcts_probs, self.action_fc), 1)))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_tensorflow.py",
        "line_number": 78,
        "API": ".add_n(",
        "context": [
            "                tf.reduce_sum(tf.multiply(self.mcts_probs, self.action_fc), 1)))\n",
            "        # 3-3. L2 penalty (regularization)\n",
            "        l2_penalty_beta = 1e-4\n",
            "        vars = tf.trainable_variables()\n",
            "        l2_penalty = l2_penalty_beta * tf.add_n(\n",
            "            [tf.nn.l2_loss(v) for v in vars if 'bias' not in v.name.lower()])\n",
            "        # 3-4 Add up to be the Loss function\n",
            "        self.loss = self.value_loss + self.policy_loss + l2_penalty\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_tensorflow.py",
        "line_number": 84,
        "API": ".placeholder(",
        "context": [
            "        # 3-4 Add up to be the Loss function\n",
            "        self.loss = self.value_loss + self.policy_loss + l2_penalty\n",
            "\n",
            "        # Define the optimizer we use for training\n",
            "        self.learning_rate = tf.placeholder(tf.float32)\n",
            "        self.optimizer = tf.train.AdamOptimizer(\n",
            "                learning_rate=self.learning_rate).minimize(self.loss)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_tensorflow.py",
        "line_number": 92,
        "API": ".reduce_mean(",
        "context": [
            "        # Make a session\n",
            "        self.session = tf.Session()\n",
            "\n",
            "        # calc policy entropy, for monitoring only\n",
            "        self.entropy = tf.negative(tf.reduce_mean(\n",
            "                tf.reduce_sum(tf.exp(self.action_fc) * self.action_fc, 1)))\n",
            "\n",
            "        # Initialize variables\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_tensorflow.py",
        "line_number": 113,
        "API": ".exp(",
        "context": [
            "        log_act_probs, value = self.session.run(\n",
            "                [self.action_fc, self.evaluation_fc2],\n",
            "                feed_dict={self.input_states: state_batch}\n",
            "                )\n",
            "        act_probs = np.exp(log_act_probs)\n",
            "        return act_probs, value\n",
            "\n",
            "    def policy_value_fn(self, board):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_tensorflow.py",
        "line_number": 123,
        "API": ".reshape(",
        "context": [
            "        output: a list of (action, probability) tuples for each available\n",
            "        action and the score of the board state\n",
            "        \"\"\"\n",
            "        legal_positions = board.availables\n",
            "        current_state = np.ascontiguousarray(board.current_state().reshape(\n",
            "                -1, 4, self.board_width, self.board_height))\n",
            "        act_probs, value = self.policy_value(current_state)\n",
            "        act_probs = zip(legal_positions, act_probs[0][legal_positions])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_tensorflow.py",
        "line_number": 131,
        "API": ".reshape(",
        "context": [
            "        return act_probs, value\n",
            "\n",
            "    def train_step(self, state_batch, mcts_probs, winner_batch, lr):\n",
            "        \"\"\"perform a training step\"\"\"\n",
            "        winner_batch = np.reshape(winner_batch, (-1, 1))\n",
            "        loss, entropy, _ = self.session.run(\n",
            "                [self.loss, self.entropy, self.optimizer],\n",
            "                feed_dict={self.input_states: state_batch,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\policy_value_net_tensorflow.py",
        "line_number": 141,
        "API": ".save(",
        "context": [
            "                           self.learning_rate: lr})\n",
            "        return loss, entropy\n",
            "\n",
            "    def save_model(self, model_path):\n",
            "        self.saver.save(self.session, model_path)\n",
            "\n",
            "    def restore_model(self, model_path):\n",
            "        self.saver.restore(self.session, model_path)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\train.py",
        "line_number": 70,
        "API": ".array(",
        "context": [
            "        extend_data = []\n",
            "        for state, mcts_porb, winner in play_data:\n",
            "            for i in [1, 2, 3, 4]:\n",
            "                # rotate counterclockwise\n",
            "                equi_state = np.array([np.rot90(s, i) for s in state])\n",
            "                equi_mcts_prob = np.rot90(np.flipud(\n",
            "                    mcts_porb.reshape(self.board_height, self.board_width)), i)\n",
            "                extend_data.append((equi_state,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\train.py",
        "line_number": 77,
        "API": ".array(",
        "context": [
            "                extend_data.append((equi_state,\n",
            "                                    np.flipud(equi_mcts_prob).flatten(),\n",
            "                                    winner))\n",
            "                # flip horizontally\n",
            "                equi_state = np.array([np.fliplr(s) for s in equi_state])\n",
            "                equi_mcts_prob = np.fliplr(equi_mcts_prob)\n",
            "                extend_data.append((equi_state,\n",
            "                                    np.flipud(equi_mcts_prob).flatten(),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\train.py",
        "line_number": 109,
        "API": ".sum(",
        "context": [
            "                    mcts_probs_batch,\n",
            "                    winner_batch,\n",
            "                    self.learn_rate*self.lr_multiplier)\n",
            "            new_probs, new_v = self.policy_value_net.policy_value(state_batch)\n",
            "            kl = np.mean(np.sum(old_probs * (\n",
            "                    np.log(old_probs + 1e-10) - np.log(new_probs + 1e-10)),\n",
            "                    axis=1)\n",
            "            )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\train.py",
        "line_number": 122,
        "API": ".var(",
        "context": [
            "        elif kl < self.kl_targ / 2 and self.lr_multiplier < 10:\n",
            "            self.lr_multiplier *= 1.5\n",
            "\n",
            "        explained_var_old = (1 -\n",
            "                             np.var(np.array(winner_batch) - old_v.flatten()) /\n",
            "                             np.var(np.array(winner_batch)))\n",
            "        explained_var_new = (1 -\n",
            "                             np.var(np.array(winner_batch) - new_v.flatten()) /\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\train.py",
        "line_number": 133,
        "API": ".format(",
        "context": [
            "               \"loss:{},\"\n",
            "               \"entropy:{},\"\n",
            "               \"explained_var_old:{:.3f},\"\n",
            "               \"explained_var_new:{:.3f}\"\n",
            "               ).format(kl,\n",
            "                        self.lr_multiplier,\n",
            "                        loss,\n",
            "                        entropy,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\train.py",
        "line_number": 159,
        "API": ".format(",
        "context": [
            "                                          start_player=i % 2,\n",
            "                                          is_shown=0)\n",
            "            win_cnt[winner] += 1\n",
            "        win_ratio = 1.0*(win_cnt[1] + 0.5*win_cnt[-1]) / n_games\n",
            "        print(\"num_playouts:{}, win: {}, lose: {}, tie:{}\".format(\n",
            "                self.pure_mcts_playout_num,\n",
            "                win_cnt[1], win_cnt[2], win_cnt[-1]))\n",
            "        return win_ratio\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\train.py",
        "line_number": 169,
        "API": ".format(",
        "context": [
            "        \"\"\"run the training pipeline\"\"\"\n",
            "        try:\n",
            "            for i in range(self.game_batch_num):\n",
            "                self.collect_selfplay_data(self.play_batch_size)\n",
            "                print(\"batch i:{}, episode_len:{}\".format(\n",
            "                        i+1, self.episode_len))\n",
            "                if len(self.data_buffer) > self.batch_size:\n",
            "                    loss, entropy = self.policy_update()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\AlphaZero_Gomoku\\train.py",
        "line_number": 176,
        "API": ".format(",
        "context": [
            "                    loss, entropy = self.policy_update()\n",
            "                # check the performance of the current model,\n",
            "                # and save the model params\n",
            "                if (i+1) % self.check_freq == 0:\n",
            "                    print(\"current self-play batch: {}\".format(i+1))\n",
            "                    win_ratio = self.policy_evaluate()\n",
            "                    self.policy_value_net.save_model('./current_policy.model')\n",
            "                    if win_ratio > self.best_win_ratio:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\calcmfcc.py",
        "line_number": 71,
        "API": ".sum(",
        "context": [
            "    feat = numpy.concatenate(([feat[0] for i in range(N)], feat, [feat[-1] for i in range(N)]))\n",
            "    denom = sum([2*i*i for i in range(1,N+1)])\n",
            "    dfeat = []\n",
            "    for j in range(NUMFRAMES):\n",
            "        dfeat.append(numpy.sum([n*feat[N+j+n] for n in range(-1*N,N+1)], axis=0)/denom)\n",
            "    return dfeat\n",
            "\n",
            "def calcMFCC(signal,samplerate=16000,win_length=0.025,win_step=0.01,feature_len=13,filters_num=26,NFFT=512,low_freq=0,high_freq=None,pre_emphasis_coeff=0.97,cep_lifter=22,appendEnergy=True,mode='mfcc'):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\calcmfcc.py",
        "line_number": 101,
        "API": ".log(",
        "context": [
            "        2-D numpy array with shape (NUMFRAMES, features). Each frame containing feature_len of features.\n",
            "    \"\"\"\n",
            "    filters_num = 2*feature_len\n",
            "    feat,energy=fbank(signal,samplerate,win_length,win_step,filters_num,NFFT,low_freq,high_freq,pre_emphasis_coeff)\n",
            "    feat=numpy.log(feat)\n",
            "    # Performing DCT and get first 13 coefficients\n",
            "    if mode == 'mfcc':\n",
            "        feat=dct(feat,type=2,axis=1,norm='ortho')[:,:feature_len]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\calcmfcc.py",
        "line_number": 110,
        "API": ".log(",
        "context": [
            "    elif mode == 'fbank':\n",
            "        feat = feat[:,:feature_len]\n",
            "    if appendEnergy:\n",
            "        # Replace the first coefficient with logE and get 2-13 coefficients.\n",
            "        feat[:,0]=numpy.log(energy)\n",
            "    return feat\n",
            "\n",
            "def fbank(signal,samplerate=16000,win_length=0.025,win_step=0.01,filters_num=26,NFFT=512,low_freq=0,high_freq=None,pre_emphasis_coeff=0.97):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\calcmfcc.py",
        "line_number": 140,
        "API": ".sum(",
        "context": [
            "    # rames: 2-D numpy array with shape (frame_num, frame_length)\n",
            "    frames=audio2frame(signal,win_length*samplerate,win_step*samplerate)\n",
            "    # Caculate energy and modify all zeros to eps.\n",
            "    spec_power=spectrum_power(frames,NFFT)\n",
            "    energy=numpy.sum(spec_power,1)\n",
            "    energy=numpy.where(energy==0,numpy.finfo(float).eps,energy)\n",
            "    # Get Mel filter banks.\n",
            "    fb=get_filter_banks(filters_num,NFFT,samplerate,low_freq,high_freq)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\calcmfcc.py",
        "line_number": 146,
        "API": ".where(",
        "context": [
            "    # Get Mel filter banks.\n",
            "    fb=get_filter_banks(filters_num,NFFT,samplerate,low_freq,high_freq)\n",
            "    # Get MFCC and modify all zeros to eps.\n",
            "    feat=numpy.dot(spec_power,fb.T)\n",
            "    feat=numpy.where(feat==0,numpy.finfo(float).eps,feat)\n",
            "\n",
            "    return feat,energy\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\calcmfcc.py",
        "line_number": 154,
        "API": ".log(",
        "context": [
            "def log_fbank(signal,samplerate=16000,win_length=0.025,win_step=0.01,filters_num=26,NFFT=512,low_freq=0,high_freq=None,pre_emphasis_coeff=0.97):\n",
            "    \"\"\"Calculate log of features.\n",
            "    \"\"\"\n",
            "    feat,energy=fbank(signal,samplerate,win_length,win_step,filters_num,NFFT,low_freq,high_freq,pre_emphasis_coeff)\n",
            "    return numpy.log(feat)\n",
            "\n",
            "def ssc(signal,samplerate=16000,win_length=0.025,win_step=0.01,filters_num=26,NFFT=512,low_freq=0,high_freq=None,pre_emphasis_coeff=0.97):\n",
            "    '''\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\calcmfcc.py",
        "line_number": 164,
        "API": ".where(",
        "context": [
            "    high_freq=high_freq or samplerate/2\n",
            "    signal=pre_emphasis(signal,pre_emphasis_coeff)\n",
            "    frames=audio2frame(signal,win_length*samplerate,win_step*samplerate)\n",
            "    spec_power=spectrum_power(frames,NFFT)\n",
            "    spec_power=numpy.where(spec_power==0,numpy.finfo(float).eps,spec_power) #\u80fd\u91cf\u8c31\n",
            "    fb=get_filter_banks(filters_num,NFFT,samplerate,low_freq,high_freq)\n",
            "    feat=numpy.dot(spec_power,fb.T)  #\u8ba1\u7b97\u80fd\u91cf\n",
            "    R=numpy.tile(numpy.linspace(1,samplerate/2,numpy.size(spec_power,1)),(numpy.size(spec_power,0),1))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\calcmfcc.py",
        "line_number": 201,
        "API": ".linspace(",
        "context": [
            "    # Convert frequency to Mel frequency.\n",
            "    low_mel=hz2mel(low_freq)\n",
            "    high_mel=hz2mel(high_freq)\n",
            "    # Insert filters_num of points between low_mel and high_mel. In total there are filters_num+2 points\n",
            "    mel_points=numpy.linspace(low_mel,high_mel,filters_num+2)\n",
            "    # Convert Mel frequency to frequency and find corresponding position.\n",
            "    hz_points=mel2hz(mel_points)\n",
            "    # Find corresponding position of these hz_points in fft.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\calcmfcc.py",
        "line_number": 207,
        "API": ".zeros(",
        "context": [
            "    hz_points=mel2hz(mel_points)\n",
            "    # Find corresponding position of these hz_points in fft.\n",
            "    bin=numpy.floor((NFFT+1)*hz_points/samplerate)\n",
            "    # Build Mel filters' expression.First and third points of each filter are zeros.\n",
            "    fbank=numpy.zeros([filters_num,NFFT//2+1])\n",
            "    for j in xrange(0,filters_num):\n",
            "        for i in xrange(int(bin[j]),int(bin[j+1])):\n",
            "            fbank[j,i]=(i-bin[j])/(bin[j+1]-bin[j])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\calcmfcc.py",
        "line_number": 222,
        "API": ".shape(",
        "context": [
            "        cepstra: MFCC coefficients.\n",
            "        L: Numbers of lifters. Defaulted to 22.\n",
            "    '''\n",
            "    if L>0:\n",
            "        nframes,ncoeff=numpy.shape(cepstra)\n",
            "        n=numpy.arange(ncoeff)\n",
            "        lift=1+(L/2)*numpy.sin(numpy.pi*n/L)\n",
            "        return lift*cepstra\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\nist2wav.py",
        "line_number": 17,
        "API": ".join(",
        "context": [
            "def nist2wav(src_dir):\n",
            "    count = 0\n",
            "    for subdir, dirs, files in os.walk(src_dir):\n",
            "        for f in files:\n",
            "            fullFilename = os.path.join(subdir, f)\n",
            "            if f.endswith('.wv1') or f.endswith('.wv2'):\n",
            "                count += 1\n",
            "                os.system(\"./sph2pipe_v2.5/sph2pipe \"+fullFilename+\" -f rif \" +fullFilename+\".wav\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\sigprocess.py",
        "line_number": 12,
        "API": ".ones(",
        "context": [
            "\n",
            "import numpy\n",
            "import math\n",
            "\n",
            "def audio2frame(signal,frame_length,frame_step,winfunc=lambda x:numpy.ones((x,))):\n",
            "    \"\"\" Framing audio signal. Uses numbers of samples as unit.\n",
            "\n",
            "    Args:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\sigprocess.py",
        "line_number": 34,
        "API": ".ceil(",
        "context": [
            "    frame_step=int(round(frame_step))\n",
            "    if signal_length<=frame_length:\n",
            "        frames_num=1\n",
            "    else:\n",
            "        frames_num=1+int(math.ceil((1.0*signal_length-frame_length)/frame_step))\n",
            "    pad_length=int((frames_num-1)*frame_step+frame_length)\n",
            "    # Padding zeros at the end of signal if pad_length > signal_length.\n",
            "    zeros=numpy.zeros((pad_length-signal_length,))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\sigprocess.py",
        "line_number": 40,
        "API": ".tile(",
        "context": [
            "    # Padding zeros at the end of signal if pad_length > signal_length.\n",
            "    zeros=numpy.zeros((pad_length-signal_length,))\n",
            "    pad_signal=numpy.concatenate((signal,zeros))\n",
            "    # Calculate the indice of signal for every sample in frames, shape (frams_nums, frams_length)\n",
            "    indices=numpy.tile(numpy.arange(0,frame_length),(frames_num,1))+numpy.tile(\n",
            "        numpy.arange(0,frames_num*frame_step,frame_step),(frame_length,1)).T\n",
            "    indices=numpy.array(indices,dtype=numpy.int32)\n",
            "    # Get signal data according to indices.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\sigprocess.py",
        "line_number": 45,
        "API": ".tile(",
        "context": [
            "        numpy.arange(0,frames_num*frame_step,frame_step),(frame_length,1)).T\n",
            "    indices=numpy.array(indices,dtype=numpy.int32)\n",
            "    # Get signal data according to indices.\n",
            "    frames=pad_signal[indices]\n",
            "    win=numpy.tile(winfunc(frame_length),(frames_num,1))\n",
            "    return frames*win\n",
            "\n",
            "def deframesignal(frames,signal_length,frame_length,frame_step,winfunc=lambda x:numpy.ones((x,))):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\sigprocess.py",
        "line_number": 60,
        "API": ".shape(",
        "context": [
            "    '''\n",
            "    #\u5bf9\u53c2\u6570\u8fdb\u884c\u53d6\u6574\u64cd\u4f5c\n",
            "    signal_length=round(signal_length) #\u4fe1\u53f7\u7684\u957f\u5ea6\n",
            "    frame_length=round(frame_length) #\u5e27\u7684\u957f\u5ea6\n",
            "    frames_num=numpy.shape(frames)[0] #\u5e27\u7684\u603b\u6570\n",
            "    assert numpy.shape(frames)[1]==frame_length,'\"frames\"\u77e9\u9635\u5927\u5c0f\u4e0d\u6b63\u786e\uff0c\u5b83\u7684\u5217\u6570\u5e94\u8be5\u7b49\u4e8e\u4e00\u5e27\u957f\u5ea6'  #\u5224\u65adframes\u7ef4\u5ea6\uff0c\u5904\u7406\u5f02\u5e38\n",
            "    indices=numpy.tile(numpy.arange(0,frame_length),(frames_num,1))+numpy.tile(numpy.arange(0,frames_num*frame_step,frame_step),(frame_length,1)).T  #\u76f8\u5f53\u4e8e\u5bf9\u6240\u6709\u5e27\u7684\u65f6\u95f4\u70b9\u8fdb\u884c\u62bd\u53d6\uff0c\u5f97\u5230frames_num*frame_length\u957f\u5ea6\u7684\u77e9\u9635\n",
            "    indices=numpy.array(indices,dtype=numpy.int32)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\sigprocess.py",
        "line_number": 67,
        "API": ".zeros(",
        "context": [
            "    indices=numpy.array(indices,dtype=numpy.int32)\n",
            "    pad_length=(frames_num-1)*frame_step+frame_length #\u94fa\u5e73\u540e\u7684\u6240\u6709\u4fe1\u53f7\n",
            "    if signal_length<=0:\n",
            "        signal_length=pad_length\n",
            "    recalc_signal=numpy.zeros((pad_length,)) #\u8c03\u6574\u540e\u7684\u4fe1\u53f7\n",
            "    window_correction=numpy.zeros((pad_length,1)) #\u7a97\u5173\u8054\n",
            "    win=winfunc(frame_length)\n",
            "    for i in range(0,frames_num):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\sigprocess.py",
        "line_number": 95,
        "API": ".square(",
        "context": [
            "        NFFT:FFT size\n",
            "    Returns:\n",
            "        Power spectrum: PS = magnitude^2/NFFT\n",
            "    \"\"\"\n",
            "    return 1.0/NFFT * numpy.square(spectrum_magnitude(frames,NFFT))\n",
            "\n",
            "def log_spectrum_power(frames,NFFT,norm=1):\n",
            "    '''Calculate log power spectrum.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\sigprocess.py",
        "line_number": 109,
        "API": ".max(",
        "context": [
            "    # In case of calculating log0, we set 0 in spec_power to 0.\n",
            "    spec_power[spec_power<1e-30]=1e-30\n",
            "    log_spec_power=10*numpy.log10(spec_power)\n",
            "    if norm:\n",
            "        return log_spec_power-numpy.max(log_spec_power)\n",
            "    else:\n",
            "        return log_spec_power\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\spectrogram.py",
        "line_number": 35,
        "API": ".abs(",
        "context": [
            "    hop_length = int(window_stride * samplingRate)\n",
            "    n_fft = win_length\n",
            "    D = librosa.core.stft(samples, n_fft=n_fft,hop_length=hop_length,\n",
            "                      win_length=win_length)\n",
            "    mag = np.abs(D)\n",
            "    log_mag = np.log1p(mag)\n",
            "    # normalization\n",
            "    log_mag = preprocessing.scale(log_mag)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\core\\spectrogram.py",
        "line_number": 44,
        "API": ".shape(",
        "context": [
            "    return log_mag\n",
            "    \n",
            "    \n",
            "if __name__ == '__main__':\n",
            "    print(np.shape(spectrogramPower('test.wav')))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\libri\\libri_preprocess.py",
        "line_number": 29,
        "API": ".join(",
        "context": [
            "    except OSError:\n",
            "        raise OSError(\"\"\"Flac not installed. Install using apt-get install flac\"\"\")\n",
            "    for subdir, dirs, files in os.walk(root_directory):\n",
            "        for f in files:\n",
            "            filename = os.path.join(subdir, f)\n",
            "            if f.endswith('.flac'):\n",
            "                try:\n",
            "                    check_call(['flac', '-d', filename])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\libri\\libri_preprocess.py",
        "line_number": 35,
        "API": ".format(",
        "context": [
            "                try:\n",
            "                    check_call(['flac', '-d', filename])\n",
            "                    os.remove(filename)\n",
            "                except CalledProcessError as e:\n",
            "                    print(\"Failed to convert file {}\".format(filename))\n",
            "            elif f.endswith('.TXT'):\n",
            "                os.remove(filename)\n",
            "            elif f.endswith('.txt'):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\libri\\libri_preprocess.py",
        "line_number": 42,
        "API": ".split(",
        "context": [
            "            elif f.endswith('.txt'):\n",
            "                with open(filename, 'r') as fp:\n",
            "                    lines = fp.readlines()\n",
            "                    for line in lines:\n",
            "                        sub_n = line.split(' ')[0] + '.label'\n",
            "                        subfile = os.path.join(subdir, sub_n)\n",
            "                        sub_c = ' '.join(line.split(' ')[1:])\n",
            "                        sub_c = sub_c.lower()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\libri\\libri_preprocess.py",
        "line_number": 47,
        "API": ".write(",
        "context": [
            "                        subfile = os.path.join(subdir, sub_n)\n",
            "                        sub_c = ' '.join(line.split(' ')[1:])\n",
            "                        sub_c = sub_c.lower()\n",
            "                        with open(subfile, 'w') as sp:\n",
            "                            sp.write(sub_c)\n",
            "            elif f.endswith('.wav'):\n",
            "                if not os.path.isfile(os.path.splitext(filename)[0] +\n",
            "                                      '.label'):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\libri\\libri_preprocess.py",
        "line_number": 60,
        "API": ".join(",
        "context": [
            "def wav2feature(root_directory, save_directory, name, win_len, win_step, mode, feature_len, seq2seq, save):\n",
            "    count = 0\n",
            "    dirid = 0\n",
            "    level = 'cha' if seq2seq is False else 'seq2seq'\n",
            "    data_dir = os.path.join(root_directory, name)\n",
            "    preprocess(data_dir)\n",
            "    for subdir, dirs, files in os.walk(data_dir):\n",
            "        for f in files:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\libri\\libri_preprocess.py",
        "line_number": 79,
        "API": ".transpose(",
        "context": [
            "                    sig = sf.read_frames(nframes)\n",
            "                    rate = sf.samplerate\n",
            "                feat = calcfeat_delta_delta(sig,rate,win_length=win_len,win_step=win_step,mode=mode,feature_len=feature_len)\n",
            "                feat = preprocessing.scale(feat)\n",
            "                feat = np.transpose(feat)\n",
            "                print(feat.shape)\n",
            "                labelFilename = filenameNoSuffix + '.label'\n",
            "                with open(labelFilename,'r') as f:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\libri\\libri_preprocess.py",
        "line_number": 103,
        "API": ".join(",
        "context": [
            "                    if count%4000 == 0:\n",
            "                        dirid += 1\n",
            "                    print('file index:',count)\n",
            "                    print('dir index:',dirid)\n",
            "                    label_dir = os.path.join(save_directory, level, name, str(dirid), 'label')\n",
            "                    feat_dir = os.path.join(save_directory, level, name, str(dirid), 'feature')\n",
            "                    if not os.path.isdir(label_dir):\n",
            "                        os.makedirs(label_dir)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\libri\\libri_preprocess.py",
        "line_number": 108,
        "API": ".makedirs(",
        "context": [
            "                    feat_dir = os.path.join(save_directory, level, name, str(dirid), 'feature')\n",
            "                    if not os.path.isdir(label_dir):\n",
            "                        os.makedirs(label_dir)\n",
            "                    if not os.path.isdir(feat_dir):\n",
            "                        os.makedirs(feat_dir)\n",
            "                    featureFilename = os.path.join(feat_dir, filenameNoSuffix.split('/')[-1] +'.npy')\n",
            "                    np.save(featureFilename,feat)\n",
            "                    t_f = os.path.join(label_dir, filenameNoSuffix.split('/')[-1] +'.npy')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\libri\\libri_preprocess.py",
        "line_number": 113,
        "API": ".save(",
        "context": [
            "                    featureFilename = os.path.join(feat_dir, filenameNoSuffix.split('/')[-1] +'.npy')\n",
            "                    np.save(featureFilename,feat)\n",
            "                    t_f = os.path.join(label_dir, filenameNoSuffix.split('/')[-1] +'.npy')\n",
            "                    print(t_f)\n",
            "                    np.save(t_f,targets)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    parser = argparse.ArgumentParser(prog='libri_preprocess',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\libri\\libri_preprocess.py",
        "line_number": 160,
        "API": ".makedirs(",
        "context": [
            "    if not os.path.isdir(root_directory):\n",
            "        raise ValueError(\"LibriSpeech Directory does not exist!\")\n",
            "\n",
            "    if not os.path.isdir(save_directory):\n",
            "        os.makedirs(save_directory)\n",
            "\n",
            "    wav2feature(root_directory, save_directory, name=name, win_len=win_len, win_step=win_step,\n",
            "                mode=mode, feature_len=feature_len, seq2seq=seq2seq, save=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\madarian\\character2digit.py",
        "line_number": 29,
        "API": ".split(",
        "context": [
            "    '''\n",
            "    \u5206\u5272\u51fa\u4e2a\u4f4d\u6570\u5b57\n",
            "    '''\n",
            "    if u'\u767e\u96f6' in c_str:\n",
            "        return _c2n(c_str.split(u'\u767e\u96f6')[1])\n",
            "    elif u'\u5341' in c_str:\n",
            "        return _c2n(c_str.split(u'\u5341')[1])\n",
            "    elif u'\u5343\u96f6' in c_str:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\madarian\\character2digit.py",
        "line_number": 44,
        "API": ".split(",
        "context": [
            "    '''\n",
            "    if u'\u767e\u96f6' in c_str:\n",
            "        return u'0'\n",
            "    elif u'\u767e' in c_str:\n",
            "        return _c2n(c_str.split(u'\u767e')[1].split(u'\u5341')[0])\n",
            "    elif u'\u5343\u96f6' in c_str and u'\u5341' in c_str:\n",
            "        return _c2n(c_str.split(u'\u5343\u96f6')[1].split(u'\u5341')[0])\n",
            "    elif u'\u5341' in c_str:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\madarian\\character2digit.py",
        "line_number": 50,
        "API": ".split(",
        "context": [
            "        return _c2n(c_str.split(u'\u5343\u96f6')[1].split(u'\u5341')[0])\n",
            "    elif u'\u5341' in c_str:\n",
            "        if c_str.split(u'\u5341')[0]=='':\n",
            "            return u'1'\n",
            "        return _c2n(c_str.split(u'\u5341')[0])\n",
            "    else:\n",
            "        return u'0'\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\madarian\\character2digit.py",
        "line_number": 61,
        "API": ".split(",
        "context": [
            "    '''\n",
            "    if u'\u5343\u96f6' in c_str:\n",
            "        return u'0'\n",
            "    elif u'\u5343' in c_str:\n",
            "        return _c2n(c_str.split(u'\u5343')[1].split(u'\u767e')[0])\n",
            "    elif u'\u767e' in c_str:\n",
            "        return _c2n(c_str.split(u'\u767e')[0])\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\madarian\\character2digit.py",
        "line_number": 74,
        "API": ".split(",
        "context": [
            "    '''\n",
            "    if u'\u4e07\u96f6' in c_str:\n",
            "        return u'0'\n",
            "    elif u'\u4e07' in c_str:\n",
            "        return _c2n(c_str.split(u'\u4e07')[1].split(u'\u5343')[0])\n",
            "    elif u'\u5343' in c_str:\n",
            "        return _c2n(c_str.split(u'\u5343')[0])\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\madarian\\character2digit.py",
        "line_number": 108,
        "API": ".split(",
        "context": [
            "    flag=0\n",
            "    float_part=''\n",
            "    if u'\u70b9' in c_str:\n",
            "        flag1=1\n",
            "        i = c_str.split(u'\u70b9')[1]\n",
            "        c_str = c_str.split(u'\u70b9')[0]\n",
            "        float_part = '.'+_convert_section(i)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\madarian\\character2digit.py",
        "line_number": 114,
        "API": ".split(",
        "context": [
            "        float_part = '.'+_convert_section(i)\n",
            "\n",
            "    if u'\u4ebf' in c_str:\n",
            "        flag=8\n",
            "        i = c_str.split(u'\u4ebf')[0]\n",
            "        c_str = c_str.split(u'\u4ebf')[1]\n",
            "        result += _convert_section(i)\n",
            "        if c_str=='':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\madarian\\character2digit.py",
        "line_number": 122,
        "API": ".split(",
        "context": [
            "            result += '00000000'\n",
            "            return result\n",
            "    if u'\u4e07' in c_str: \n",
            "        flag=4\n",
            "        i = c_str.split(u'\u4e07')[0]\n",
            "        c_str = c_str.split(u'\u4e07')[1]\n",
            "        result += _convert_section(i)\n",
            "        if c_str=='':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\madarian\\character2digit.py",
        "line_number": 134,
        "API": ".sub(",
        "context": [
            "\n",
            "def convertCharacter2Digit(string):\n",
            "    chinese_numbers=re.findall(u'[\u70b9\u96f6\u4e00\u4e8c\u4e09\u56db\u4e94\u516d\u4e03\u516b\u4e5d\u5341\u767e\u5343\u4e07\u4ebf]{1,}', \n",
            "        string, re.S)\n",
            "    sub_str = re.sub(u'[\u70b9\u96f6\u4e00\u4e8c\u4e09\u56db\u4e94\u516d\u4e03\u516b\u4e5d\u5341\u767e\u5343\u4e07\u4ebf]{1,}', '_', string)\n",
            "    for chinese_number in chinese_numbers:\n",
            "        digit = _convert_all(chinese_number)\n",
            "        sub_str = sub_str.replace('_', digit, 1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\madarian\\character2digit.py",
        "line_number": 148,
        "API": ".strip(",
        "context": [
            "if __name__ == '__main__':\n",
            "    with codecs.open('sample.txt','r','utf-8') as f:\n",
            "        content=f.readlines()\n",
            "    for string in content:\n",
            "        convertCharacter2Digit(string.strip())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\madarian\\digit2character.py",
        "line_number": 87,
        "API": ".sub(",
        "context": [
            "    '''\n",
            "    Preprocessing the sentence and splitting the decimal, integer or special\n",
            "    '''\n",
            "    decimal_set = re.findall(r'\\d+\\.\\d+', string)\n",
            "    sub_str = re.sub(r'\\d+\\.\\d+', '_', string)\n",
            "    newStr=_replaceDecimal(decimal_set, sub_str)\n",
            "\n",
            "    integer_set = re.findall(r'\\d+\u5e74', newStr)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\madarian\\digit2character.py",
        "line_number": 95,
        "API": ".sub(",
        "context": [
            "    sub_str = re.sub(r'\\d+\u5e74', '_', newStr)\n",
            "    newStr=_replaceSpecial(integer_set, sub_str)\n",
            "\n",
            "    integer_set = re.findall(r'\\d+', newStr)\n",
            "    sub_str = re.sub(r'\\d+', '_', newStr)\n",
            "    newStr=_replaceInteger(integer_set, sub_str)\n",
            "    print('\u539f\u53e5\u5b50:', string)\n",
            "    print('\u65b0\u53e5\u5b50:', newStr)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\madarian\\digit2character.py",
        "line_number": 159,
        "API": ".split(",
        "context": [
            "    Converting floating number to Chinese expression\n",
            "    '''\n",
            "    charNumSet=['\u96f6', '\u4e00', '\u4e8c', '\u4e09', '\u56db', '\u4e94', '\u516d',\n",
            "             '\u4e03', '\u516b', '\u4e5d']\n",
            "    integer_part, decimal_part=str(number).split('.')\n",
            "    int_result=_integer2Chinese(int(integer_part))\n",
            "    dec_result=''\n",
            "    for c in decimal_part:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\madarian\\digit2character.py",
        "line_number": 173,
        "API": ".strip(",
        "context": [
            "if __name__ == '__main__':\n",
            "    with codecs.open('sample.txt','r','utf-8') as f:\n",
            "        content=f.readlines()\n",
            "    for string in content:\n",
            "        convertDigit2Character(string.strip())\n",
            "\n",
            "    '''\n",
            "    rootdir='/media/pony/DLdigest/data/ASR_zh'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\madarian\\digit2character.py",
        "line_number": 180,
        "API": ".join(",
        "context": [
            "    rootdir='/media/pony/DLdigest/data/ASR_zh'\n",
            "    r1 = re.compile(r'\\d+')\n",
            "    for subdir, dirs, files in os.walk(rootdir):\n",
            "        for file in files:\n",
            "            fullFilename = os.path.join(subdir, file)\n",
            "            filenameNoSuffix =  os.path.splitext(fullFilename)[0]\n",
            "            if file.endswith('.label'):\n",
            "                with open(fullFilename, 'r') as f:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\timit\\timit_preprocess.py",
        "line_number": 38,
        "API": ".join(",
        "context": [
            "## cleaned phonemes\n",
            "#phn = ['sil', 'aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'ax-h', 'ay', 'b', 'ch', 'd', 'dh', 'dx', 'eh', 'el', 'en', 'epi', 'er', 'ey', 'f', 'g', 'hh', 'ih', 'ix', 'iy', 'jh', 'k', 'l', 'm', 'n', 'ng', 'ow', 'oy', 'p', 'q', 'r', 's', 'sh', 't', 'th', 'uh', 'uw', 'v', 'w', 'y', 'z', 'zh']\n",
            "\n",
            "def wav2feature(rootdir, save_directory, mode, feature_len, level, keywords, win_len, win_step,  seq2seq, save):\n",
            "    feat_dir = os.path.join(save_directory, level, keywords, mode)\n",
            "    label_dir = os.path.join(save_directory, level, keywords, 'label')\n",
            "    if not os.path.exists(label_dir):\n",
            "        os.makedirs(label_dir)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\timit\\timit_preprocess.py",
        "line_number": 43,
        "API": ".makedirs(",
        "context": [
            "    label_dir = os.path.join(save_directory, level, keywords, 'label')\n",
            "    if not os.path.exists(label_dir):\n",
            "        os.makedirs(label_dir)\n",
            "    if not os.path.exists(feat_dir):\n",
            "        os.makedirs(feat_dir)\n",
            "    count = 0\n",
            "    for subdir, dirs, files in os.walk(rootdir):\n",
            "        for file in files:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\timit\\timit_preprocess.py",
        "line_number": 53,
        "API": ".transpose(",
        "context": [
            "            if file.endswith('.WAV'):\n",
            "                rate, sig = 16000, np.fromfile(fullFilename, dtype=np.int16)[512:]\n",
            "                feat = calcfeat_delta_delta(sig, rate, win_length=win_len, win_step=win_step, mode=mode, feature_len=feature_len)\n",
            "                feat = preprocessing.scale(feat)\n",
            "                feat = np.transpose(feat)\n",
            "                print(feat.shape)\n",
            "\n",
            "                if level == 'phn':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\timit\\timit_preprocess.py",
        "line_number": 63,
        "API": ".split(",
        "context": [
            "                    with open(labelFilename,'r') as f:\n",
            "                        if seq2seq is True:\n",
            "                            phenome.append(len(phn)) # <start token>\n",
            "                        for line in f.read().splitlines():\n",
            "                            s = line.split(' ')[2]\n",
            "                            p_index = phn.index(s)\n",
            "                            phenome.append(p_index)\n",
            "                        if seq2seq is True:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\timit\\timit_preprocess.py",
        "line_number": 69,
        "API": ".array(",
        "context": [
            "                            phenome.append(p_index)\n",
            "                        if seq2seq is True:\n",
            "                            phenome.append(len(phn)+1) # <end token>\n",
            "                        print(phenome)\n",
            "                    phenome = np.array(phenome)\n",
            "\n",
            "                elif level == 'cha':\n",
            "                    labelFilename = filenameNoSuffix + '.WRD'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\timit\\timit_preprocess.py",
        "line_number": 77,
        "API": ".split(",
        "context": [
            "                    phenome = []\n",
            "                    sentence = ''\n",
            "                    with open(labelFilename,'r') as f:\n",
            "                        for line in f.read().splitlines():\n",
            "                            s=line.split(' ')[2]\n",
            "                            sentence += s+' '\n",
            "                            if seq2seq is True:\n",
            "                                phenome.append(28)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\timit\\timit_preprocess.py",
        "line_number": 97,
        "API": ".split(",
        "context": [
            "\n",
            "                count +=1\n",
            "                print('file index:',count)\n",
            "                if save:\n",
            "                    speaker, sentence_name = filenameNoSuffix.split('/')[-2:]\n",
            "                    feature_filename = \"{}/{}-{}.npy\".format(feat_dir, speaker, sentence_name)\n",
            "                    np.save(feature_filename,feat)\n",
            "                    label_filename = \"{}/{}-{}.npy\".format(label_dir, speaker, sentence_name)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\timit\\timit_preprocess.py",
        "line_number": 102,
        "API": ".save(",
        "context": [
            "                    feature_filename = \"{}/{}-{}.npy\".format(feat_dir, speaker, sentence_name)\n",
            "                    np.save(feature_filename,feat)\n",
            "                    label_filename = \"{}/{}-{}.npy\".format(label_dir, speaker, sentence_name)\n",
            "                    print(label_filename)\n",
            "                    np.save(label_filename,phenome)\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\timit\\timit_preprocess.py",
        "line_number": 143,
        "API": ".join(",
        "context": [
            "    seq2seq = args.seq2seq\n",
            "    win_len = args.winlen\n",
            "    win_step = args.winstep\n",
            "\n",
            "    root_directory = os.path.join(root_directory, name)\n",
            "    if root_directory == \".\":\n",
            "        root_directory = os.getcwd()\n",
            "    if save_directory == \".\":\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\timit\\timit_preprocess.py",
        "line_number": 151,
        "API": ".makedirs(",
        "context": [
            "        save_directory = os.getcwd()\n",
            "    if not os.path.isdir(root_directory):\n",
            "        raise ValueError(\"Root directory does not exist!\")\n",
            "    if not os.path.exists(save_directory):\n",
            "        os.makedirs(save_directory)\n",
            "    wav2feature(root_directory, save_directory, mode=mode, feature_len=feature_len,\n",
            "                level=level, keywords=name, win_len=win_len, win_step=win_step,\n",
            "                seq2seq=seq2seq, save=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\wsj\\extract_wsj.py",
        "line_number": 17,
        "API": ".join(",
        "context": [
            "def extract(rootdir):\n",
            "  for subdir, dirs, files in os.walk(rootdir):\n",
            "    for f in files:\n",
            "      if f.endswith('.zip'):\n",
            "        fullFilename = os.path.join(rootdir, f)\n",
            "        subprocess.call(['atool', '-x', fullFilename])\n",
            "        print(f)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\wsj\\rename_wsj.py",
        "line_number": 17,
        "API": ".split(",
        "context": [
            "  # find new name according to indexing\n",
            "  with open(logfile, 'r') as f:\n",
            "    content = f.readlines()\n",
            "  for line in content:\n",
            "    if int(line.split(' ')[-1][2:]) == int(cd_id[2:]):\n",
            "      if '.' in line.split(' ')[-3]:\n",
            "        newName = line.split(' ')[-3]\n",
            "        return newName\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\wsj\\rename_wsj.py",
        "line_number": 27,
        "API": ".join(",
        "context": [
            "\n",
            "def renameCD(src_dir, mode):\n",
            "  # rename CD directory to new name\n",
            "  logfile = mode+'.links.log'\n",
            "  cd_dir = os.path.join(src_dir, mode)\n",
            "  count = 0\n",
            "  for subdir in os.listdir(cd_dir):\n",
            "    if subdir.startswith('CD') or subdir.startswith('cd'):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\wsj\\rename_wsj.py",
        "line_number": 32,
        "API": ".join(",
        "context": [
            "  count = 0\n",
            "  for subdir in os.listdir(cd_dir):\n",
            "    if subdir.startswith('CD') or subdir.startswith('cd'):\n",
            "      newName = lookup(subdir, os.path.join(src_dir, logfile))\n",
            "      cd_path = os.path.join(src_dir, mode, subdir)\n",
            "      new_cd_path = os.path.join(src_dir, mode, newName)\n",
            "      os.rename(cd_path, new_cd_path)\n",
            "      count += 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\wsj\\split_data_by_s5.py",
        "line_number": 24,
        "API": ".join(",
        "context": [
            "\n",
            "def split_data_by_s5(src_dir, des_dir, keywords=['train_si284', 'test_eval92', 'test_dev93']):\n",
            "  count = 0\n",
            "  for key in keywords:\n",
            "    wav_file_list = os.path.join(src_dir, key+'.flist') \n",
            "    label_file_list = os.path.join(src_dir, key+'.txt') \n",
            "    new_path = check_path_exists(os.path.join(des_dir, key))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\wsj\\split_data_by_s5.py",
        "line_number": 31,
        "API": ".strip(",
        "context": [
            "\n",
            "    with open(wav_file_list, 'r') as wfl:\n",
            "      wfl_contents = wfl.readlines()\n",
            "      for line in wfl_contents:\n",
            "        line = line.strip()\n",
            "        if os.path.isfile(line):\n",
            "          shutil.copyfile(line, os.path.join(des_dir, key, line.split('/')[-1]))\n",
            "          print(line)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\wsj\\split_data_by_s5.py",
        "line_number": 36,
        "API": ".join(",
        "context": [
            "        if os.path.isfile(line):\n",
            "          shutil.copyfile(line, os.path.join(des_dir, key, line.split('/')[-1]))\n",
            "          print(line)\n",
            "        else:\n",
            "          tmp = '/'.join(line.split('/')[:-1]+[line.split('/')[-1].upper()])\n",
            "          shutil.copyfile(tmp, os.path.join(des_dir, key, line.split('/')[-1].replace('WV1', 'wv1')))\n",
            "          print(tmp)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\wsj\\split_data_by_s5.py",
        "line_number": 43,
        "API": ".join(",
        "context": [
            "\n",
            "    with open(label_file_list, 'r') as lfl:\n",
            "      lfl_contents = lfl.readlines()\n",
            "      for line in lfl_contents:\n",
            "        label = ' '.join(line.strip().split(' ')[1:])\n",
            "        with open(os.path.join(des_dir, key, line.strip().split(' ')[0]+'.label'), 'w') as lf:\n",
            "          lf.writelines(label)\n",
            "        print(key, label)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\wsj\\wsj_preprocess.py",
        "line_number": 32,
        "API": ".join(",
        "context": [
            "  dirid = 0\n",
            "  level = 'cha' if seq2seq is False else 'seq2seq'\n",
            "  for subdir, dirs, files in os.walk(root_directory):\n",
            "    for f in files:\n",
            "      fullFilename = os.path.join(subdir, f)\n",
            "      filenameNoSuffix =  os.path.splitext(fullFilename)[0]\n",
            "      if f.endswith('.wv1') or f.endswith('.wav'):\n",
            "        rate = None\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\wsj\\wsj_preprocess.py",
        "line_number": 40,
        "API": ".join(",
        "context": [
            "        sig = None\n",
            "        try:\n",
            "          (rate,sig)= wav.read(fullFilename)\n",
            "        except ValueError as e:\n",
            "          sph2pipe = os.path.join(sph2pipe_dir, 'sph2pipe')\n",
            "          wav_name = fullFilename.replace('wv1', 'wav')\n",
            "          check_call(['./sph2pipe', '-f', 'rif', fullFilename, wav_name])\n",
            "          os.remove(fullFilename)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\wsj\\wsj_preprocess.py",
        "line_number": 50,
        "API": ".transpose(",
        "context": [
            "          os.remove(fullFilename)\n",
            "\n",
            "        feat = calcfeat_delta_delta(sig,rate,win_length=win_len,win_step=win_step,feature_len=feature_len,mode=mode)\n",
            "        feat = preprocessing.scale(feat)\n",
            "        feat = np.transpose(feat)\n",
            "        print(feat.shape)\n",
            "        labelFilename = filenameNoSuffix + '.label'\n",
            "        with open(labelFilename,'r') as f:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\wsj\\wsj_preprocess.py",
        "line_number": 67,
        "API": ".array(",
        "context": [
            "          else:\n",
            "            targets.append(ord(c)-96)\n",
            "        if seq2seq is True:\n",
            "          targets.append(29)\n",
            "        targets = np.array(targets)\n",
            "        print(targets)\n",
            "        if save:\n",
            "          count += 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\wsj\\wsj_preprocess.py",
        "line_number": 75,
        "API": ".join(",
        "context": [
            "          if count%1000 == 0:\n",
            "              dirid += 1\n",
            "          print('file index:',count)\n",
            "          print('dir index:',dirid)\n",
            "          label_dir = os.path.join(save_directory, level, name, str(dirid), 'label')\n",
            "          feat_dir = os.path.join(save_directory, level, name, str(dirid), mode)\n",
            "          if not os.path.isdir(label_dir):\n",
            "              os.makedirs(label_dir)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\wsj\\wsj_preprocess.py",
        "line_number": 80,
        "API": ".makedirs(",
        "context": [
            "          feat_dir = os.path.join(save_directory, level, name, str(dirid), mode)\n",
            "          if not os.path.isdir(label_dir):\n",
            "              os.makedirs(label_dir)\n",
            "          if not os.path.isdir(feat_dir):\n",
            "              os.makedirs(feat_dir)\n",
            "          featureFilename = os.path.join(feat_dir, filenameNoSuffix.split('/')[-1] +'.npy')\n",
            "          np.save(featureFilename,feat)\n",
            "          t_f = os.path.join(label_dir, filenameNoSuffix.split('/')[-1] +'.npy')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\wsj\\wsj_preprocess.py",
        "line_number": 85,
        "API": ".save(",
        "context": [
            "          featureFilename = os.path.join(feat_dir, filenameNoSuffix.split('/')[-1] +'.npy')\n",
            "          np.save(featureFilename,feat)\n",
            "          t_f = os.path.join(label_dir, filenameNoSuffix.split('/')[-1] +'.npy')\n",
            "          print(t_f)\n",
            "          np.save(t_f,targets)\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    parser = argparse.ArgumentParser(prog='wsj_preprocess',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\feature\\wsj\\wsj_preprocess.py",
        "line_number": 131,
        "API": ".makedirs(",
        "context": [
            "    if not os.path.isdir(root_directory):\n",
            "        raise ValueError(\"WSJ Directory does not exist!\")\n",
            "\n",
            "    if not os.path.isdir(save_directory):\n",
            "        os.makedirs(save_directory)\n",
            "\n",
            "    wav2feature(root_directory, save_directory, name=name, win_len=win_len, win_step=win_step,\n",
            "                mode=mode, feature_len=feature_len, seq2seq=seq2seq, save=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\lm\\spellingChecker4CN\\gardener.py",
        "line_number": 31,
        "API": ".join(",
        "context": [
            "    def process_poetry(self, data_dir='/media/pony/DLdigest/data/languageModel/chinese-poetry/json'):\n",
            "        \"\"\"\n",
            "        Process Tang and Song poems dataset\n",
            "        \"\"\"\n",
            "        save_dir = os.path.join(self.save_dir, 'poem')\n",
            "        check_path_exists(save_dir)\n",
            "        count = 0\n",
            "        for entry in os.scandir(data_dir):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\lm\\spellingChecker4CN\\gardener.py",
        "line_number": 37,
        "API": ".load(",
        "context": [
            "        count = 0\n",
            "        for entry in os.scandir(data_dir):\n",
            "            if entry.name.startswith('poet'):\n",
            "                with open(entry.path, 'r') as json_file:\n",
            "                    poems = json.load(json_file)\n",
            "                    for p in poems: \n",
            "                        paras = HanziConv.toSimplified(''.join(p['paragraphs']).replace('\\n', ''))\n",
            "                        paras = filter_punctuation(paras)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\lm\\spellingChecker4CN\\gardener.py",
        "line_number": 42,
        "API": ".strip(",
        "context": [
            "                    for p in poems: \n",
            "                        paras = HanziConv.toSimplified(''.join(p['paragraphs']).replace('\\n', ''))\n",
            "                        paras = filter_punctuation(paras)\n",
            "                        for para in paras.split(' '):\n",
            "                            if len(para.strip())>1:\n",
            "                                pys = ' '.join(np.array(pinyin(para)).flatten())\n",
            "                                with open(os.path.join(save_dir, str(count//400000+1)+'.txt'), 'a') as f:\n",
            "                                    f.write(para+','+pys+'\\n')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\lm\\spellingChecker4CN\\gardener.py",
        "line_number": 52,
        "API": ".join(",
        "context": [
            "    def process_dureader(self, data_dir='/media/pony/DLdigest/data/languageModel/dureader-raw/'): \n",
            "        \"\"\"\n",
            "        Processing Baidu released QA Reader Dataset\n",
            "        \"\"\"\n",
            "        save_dir = os.path.join(self.save_dir, 'dureader')\n",
            "        check_path_exists(save_dir)\n",
            "        count = 0\n",
            "        for entry in os.scandir(data_dir):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\lm\\spellingChecker4CN\\gardener.py",
        "line_number": 63,
        "API": ".join(",
        "context": [
            "                    for line in f:\n",
            "                        contents = json.loads(line)\n",
            "                        con = []\n",
            "                        try:\n",
            "                            answers = ''.join(contents['answers'])\n",
            "                            con.append(answers)\n",
            "                            questions = contents['question']\n",
            "                            con.append(questions)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\lm\\spellingChecker4CN\\gardener.py",
        "line_number": 68,
        "API": ".join(",
        "context": [
            "                            con.append(answers)\n",
            "                            questions = contents['question']\n",
            "                            con.append(questions)\n",
            "                            for doc in contents['documents']:\n",
            "                                paragraphs = ''.join(doc['paragraphs'])\n",
            "                                title = doc['title']\n",
            "                                con.append(paragraphs)\n",
            "                                con.append(title)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\lm\\spellingChecker4CN\\gardener.py",
        "line_number": 74,
        "API": ".split(",
        "context": [
            "                                con.append(paragraphs)\n",
            "                                con.append(title)\n",
            "                            con = HanziConv.toSimplified(''.join(con).replace('\\n', ''))\n",
            "                            cons = filter_punctuation(con)\n",
            "                            for c in cons.split(' '):\n",
            "                                if len(c.strip())>1:\n",
            "                                    pys = ' '.join(np.array(pinyin(c)).flatten())\n",
            "                                    count += 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\lm\\spellingChecker4CN\\gardener.py",
        "line_number": 79,
        "API": ".write(",
        "context": [
            "                                if len(c.strip())>1:\n",
            "                                    pys = ' '.join(np.array(pinyin(c)).flatten())\n",
            "                                    count += 1\n",
            "                                    with open(os.path.join(save_dir, str(count//400000+1)+'.txt'), 'a') as f:\n",
            "                                        f.write(c+','+pys+'\\n')\n",
            "                        except KeyError:\n",
            "                            continue\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\lm\\spellingChecker4CN\\gardener.py",
        "line_number": 87,
        "API": ".join(",
        "context": [
            "    def process_audioLabels(self, data_dir='/media/pony/DLdigest/data/ASR_zh/'): \n",
            "        \"\"\"\n",
            "        Processing label files in collected Chinese audio dataset\n",
            "        \"\"\"\n",
            "        save_dir = os.path.join(self.save_dir, 'audioLabels')\n",
            "        check_path_exists(save_dir)\n",
            "        count = 0\n",
            "        for subdir, dirs, files in os.walk(data_dir):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\lm\\spellingChecker4CN\\gardener.py",
        "line_number": 94,
        "API": ".join(",
        "context": [
            "        for subdir, dirs, files in os.walk(data_dir):\n",
            "            print(subdir)\n",
            "            for f in files:\n",
            "                if f.endswith(\"label\"):\n",
            "                    fullFilename = os.path.join(subdir, f)\n",
            "                    with open(fullFilename, 'r') as f:\n",
            "                        line = f.read()\n",
            "                        con = HanziConv.toSimplified(line)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\lm\\spellingChecker4CN\\gardener.py",
        "line_number": 99,
        "API": ".split(",
        "context": [
            "                    with open(fullFilename, 'r') as f:\n",
            "                        line = f.read()\n",
            "                        con = HanziConv.toSimplified(line)\n",
            "                        con = filter_punctuation(con)\n",
            "                        for c in con.split(' '):\n",
            "                            if len(c.strip())>1:\n",
            "                                pys = ' '.join(np.array(pinyin(c)).flatten())\n",
            "                                count += 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\lm\\spellingChecker4CN\\gardener.py",
        "line_number": 104,
        "API": ".write(",
        "context": [
            "                            if len(c.strip())>1:\n",
            "                                pys = ' '.join(np.array(pinyin(c)).flatten())\n",
            "                                count += 1\n",
            "                                with open(os.path.join(save_dir, str(count//400000+1)+'.txt'), 'a') as f:\n",
            "                                    f.write(c+','+pys+'\\n')\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    cg = CorpusGardener()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\lm\\spellingChecker4CN\\utils.py",
        "line_number": 21,
        "API": ".compile(",
        "context": [
            "    punctuation=string.punctuation + string.ascii_letters + \\\n",
            "                '\uff01\uff1f\uff61\uff02\uff03\uff04\uff05\uff06\uff07\uff08\uff09\uff0a\uff0b\uff0c\uff0d\uff0f\uff1a\uff1b\uff1c\uff1d\uff1e\uff20\uff3b\uff3c\uff3d\uff3e\uff3f\uff40' + \\\n",
            "                '\uff5b\uff5c\uff5d\uff5e\uff5f\uff60\uff62\uff63\uff64\u3001\u3003\u300b\u300c\u300d\u300e\u300f\u3010\u3011\u3014\u3015\u3016\u3017\u3018\u3019\u301a\u301b\u301c\u301d\u301e' + \\\n",
            "                '\u301f\u3030\u303e\u303f\u2013\u2014\u2018\u2019\u201b\u201c\u201d\u201e\u201f\u2026\u2027\ufe4f.\u00b7\u3002\u300a\u300b'\n",
            "    regex = re.compile('[%s]' % re.escape(punctuation))\n",
            "    '''\n",
            "\n",
            "    regex = re.compile(u'[^\\u4E00-\\u9FA5]')#\u975e\u4e2d\u6587\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\lm\\spellingChecker4CN\\utils.py",
        "line_number": 26,
        "API": ".sub(",
        "context": [
            "    '''\n",
            "\n",
            "    regex = re.compile(u'[^\\u4E00-\\u9FA5]')#\u975e\u4e2d\u6587\n",
            "    if remove_duplicate_space:\n",
            "        result = re.sub(' +', ' ', regex.sub(' ', input_str))\n",
            "    else:\n",
            "        result = regex.sub(' ', input_str)\n",
            "    result = re.sub(\"\\d+\", \" \", result)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\lm\\spellingChecker4CN\\utils.py",
        "line_number": 52,
        "API": ".compile(",
        "context": [
            "    if mode == 'integer':\n",
            "    \n",
            "\n",
            "def reviseString(string):\n",
            "    re_int = re.compile('\\d+')\n",
            "    re_float = re.compile('\\d+\\.\\d+')\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\libri_train.py",
        "line_number": 87,
        "API": ".join(",
        "context": [
            "grad_clip = FLAGS.grad_clip\n",
            "datadir = FLAGS.datadir\n",
            "\n",
            "logdir = FLAGS.logdir\n",
            "savedir = os.path.join(logdir, level, 'save')\n",
            "resultdir = os.path.join(logdir, level, 'result')\n",
            "loggingdir = os.path.join(logdir, level, 'logging')\n",
            "check_path_exists([logdir, savedir, resultdir, loggingdir])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\libri_train.py",
        "line_number": 104,
        "API": ".join(",
        "context": [
            "\n",
            "\n",
            "def get_data(datadir, level, train_dataset, dev_dataset, test_dataset, mode):\n",
            "    if mode == 'train':\n",
            "        train_feature_dirs = [os.path.join(os.path.join(datadir, level, train_dataset),\n",
            "            i, 'feature') for i in os.listdir(os.path.join(datadir, level, train_dataset))]\n",
            "\n",
            "        train_label_dirs = [os.path.join(os.path.join(datadir, level, train_dataset),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\libri_train.py",
        "line_number": 112,
        "API": ".join(",
        "context": [
            "            i, 'label') for i in os.listdir(os.path.join(datadir, level, train_dataset))]\n",
            "        return train_feature_dirs, train_label_dirs\n",
            "\n",
            "    if mode == 'dev':\n",
            "        dev_feature_dirs = [os.path.join(os.path.join(datadir, level, dev_dataset),\n",
            "            i, 'feature') for i in os.listdir(os.path.join(datadir, level, dev_dataset))]\n",
            "\n",
            "        dev_label_dirs = [os.path.join(os.path.join(datadir, level, dev_dataset),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\libri_train.py",
        "line_number": 120,
        "API": ".join(",
        "context": [
            "            i, 'label') for i in os.listdir(os.path.join(datadir, level, dev_dataset))]\n",
            "        return dev_feature_dirs, dev_label_dirs\n",
            "\n",
            "    if mode == 'test':\n",
            "        test_feature_dirs = [os.path.join(os.path.join(datadir, level, test_dataset),\n",
            "            i, 'feature') for i in os.listdir(os.path.join(datadir, level, test_dataset))]\n",
            "\n",
            "        test_label_dirs = [os.path.join(os.path.join(datadir, level, test_dataset),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\libri_train.py",
        "line_number": 127,
        "API": ".join(",
        "context": [
            "        test_label_dirs = [os.path.join(os.path.join(datadir, level, test_dataset),\n",
            "            i, 'label') for i in os.listdir(os.path.join(datadir, level, test_dataset))]\n",
            "        return test_feature_dirs, test_label_dirs\n",
            "\n",
            "logfile = os.path.join(loggingdir, str(datetime.datetime.strftime(datetime.datetime.now(),\n",
            "    '%Y-%m-%d %H:%M:%S') + '.txt').replace(' ', '').replace('/', ''))\n",
            "\n",
            "class Runner(object):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\libri_train.py",
        "line_number": 167,
        "API": ".format(",
        "context": [
            "        feature_dirs, label_dirs = zip(*FL_pair)\n",
            "\n",
            "        for feature_dir, label_dir in zip(feature_dirs, label_dirs):\n",
            "            id_dir = feature_dirs.index(feature_dir)\n",
            "            print('dir id:{}'.format(id_dir))\n",
            "            batchedData, maxTimeSteps, totalN = self.load_data(feature_dir, label_dir, mode, level)\n",
            "            model = model_fn(args, maxTimeSteps)\n",
            "            num_params = count_params(model, mode='trainable')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\libri_train.py",
        "line_number": 180,
        "API": ".restore(",
        "context": [
            "                # restore from stored model\n",
            "                if keep == True:\n",
            "                    ckpt = tf.train.get_checkpoint_state(savedir)\n",
            "                    if ckpt and ckpt.model_checkpoint_path:\n",
            "                        model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
            "                        print('Model restored from:' + savedir)\n",
            "                else:\n",
            "                    print('Initializing')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\libri_train.py",
        "line_number": 190,
        "API": ".format(",
        "context": [
            "                for epoch in range(num_epochs):\n",
            "                    ## training\n",
            "                    start = time.time()\n",
            "                    if mode == 'train':\n",
            "                        print('Epoch {} ...'.format(epoch + 1))\n",
            "\n",
            "                    batchErrors = np.zeros(len(batchedData))\n",
            "                    batchRandIxs = np.random.permutation(len(batchedData))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\libri_train.py",
        "line_number": 209,
        "API": ".format(",
        "context": [
            "                                    model.predictions, model.targetY, model.errorRate],\n",
            "                                    feed_dict=feedDict)\n",
            "\n",
            "                                batchErrors[batch] = er\n",
            "                                print('\\n{} mode, total:{},subdir:{}/{},batch:{}/{},epoch:{}/{},train loss={:.3f},mean train CER={:.3f}\\n'.format(\n",
            "                                    level, totalN, id_dir+1, len(feature_dirs), batch+1, len(batchRandIxs), epoch+1, num_epochs, l, er/batch_size))\n",
            "\n",
            "                            elif mode == 'dev':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\libri_train.py",
        "line_number": 216,
        "API": ".format(",
        "context": [
            "                            elif mode == 'dev':\n",
            "                                l, pre, y, er = sess.run([model.loss, model.predictions,\n",
            "                                    model.targetY, model.errorRate], feed_dict=feedDict)\n",
            "                                batchErrors[batch] = er\n",
            "                                print('\\n{} mode, total:{},subdir:{}/{},batch:{}/{},dev loss={:.3f},mean dev CER={:.3f}\\n'.format(\n",
            "                                    level, totalN, id_dir+1, len(feature_dirs), batch+1, len(batchRandIxs), l, er/batch_size))\n",
            "\n",
            "                            elif mode == 'test':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\libri_train.py",
        "line_number": 223,
        "API": ".format(",
        "context": [
            "                            elif mode == 'test':\n",
            "                                l, pre, y, er = sess.run([model.loss, model.predictions,\n",
            "                                    model.targetY, model.errorRate], feed_dict=feedDict)\n",
            "                                batchErrors[batch] = er\n",
            "                                print('\\n{} mode, total:{},subdir:{}/{},batch:{}/{},test loss={:.3f},mean test CER={:.3f}\\n'.format(\n",
            "                                    level, totalN, id_dir+1, len(feature_dirs), batch+1, len(batchRandIxs), l, er/batch_size))\n",
            "                        elif level=='seq2seq':\n",
            "                            raise ValueError('level %s is not supported now'%str(level))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\libri_train.py",
        "line_number": 240,
        "API": ".join(",
        "context": [
            "\n",
            "\n",
            "                        if mode=='train' and ((epoch * len(batchRandIxs) + batch + 1) % 20 == 0 or (\n",
            "                            epoch == num_epochs - 1 and batch == len(batchRandIxs) - 1)):\n",
            "                            checkpoint_path = os.path.join(savedir, 'model.ckpt')\n",
            "                            model.saver.save(sess, checkpoint_path, global_step=epoch)\n",
            "                            print('Model has been saved in {}'.format(savedir))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\libri_train.py",
        "line_number": 250,
        "API": ".join(",
        "context": [
            "                    print('Epoch ' + str(epoch + 1) + ' needs time:' + str(delta_time) + ' s')\n",
            "\n",
            "                    if mode=='train':\n",
            "                        if (epoch + 1) % 1 == 0:\n",
            "                            checkpoint_path = os.path.join(savedir, 'model.ckpt')\n",
            "                            model.saver.save(sess, checkpoint_path, global_step=epoch)\n",
            "                            print('Model has been saved in {}'.format(savedir))\n",
            "                        epochER = batchErrors.sum() / totalN\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\libri_train.py",
        "line_number": 259,
        "API": ".join(",
        "context": [
            "                        logging(model, logfile, epochER, epoch, delta_time, mode='config')\n",
            "                        logging(model, logfile, epochER, epoch, delta_time, mode=mode)\n",
            "\n",
            "                    if mode=='test' or mode=='dev':\n",
            "                        with open(os.path.join(resultdir, level + '_result.txt'), 'a') as result:\n",
            "                            result.write(output_to_sequence(y, type=level) + '\\n')\n",
            "                            result.write(output_to_sequence(pre, type=level) + '\\n')\n",
            "                            result.write('\\n')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\timit_train.py",
        "line_number": 100,
        "API": ".join(",
        "context": [
            "if mode == 'test':\n",
            "  batch_size = 100\n",
            "  num_epochs = 1\n",
            "\n",
            "train_mfcc_dir = os.path.join(datadir, level, 'train', 'mfcc')\n",
            "train_label_dir = os.path.join(datadir, level, 'train', 'label')\n",
            "test_mfcc_dir = os.path.join(datadir, level, 'test', 'mfcc')\n",
            "test_label_dir = os.path.join(datadir, level, 'test', 'label')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\timit_train.py",
        "line_number": 155,
        "API": ".restore(",
        "context": [
            "            # restore from stored model\n",
            "            if keep == True:\n",
            "                ckpt = tf.train.get_checkpoint_state(savedir)\n",
            "                if ckpt and ckpt.model_checkpoint_path:\n",
            "                    model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
            "                    print('Model restored from:' + savedir)\n",
            "            else:\n",
            "                print('Initializing')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\timit_train.py",
        "line_number": 166,
        "API": ".zeros(",
        "context": [
            "                ## training\n",
            "                start = time.time()\n",
            "                if mode == 'train':\n",
            "                    print('Epoch', epoch + 1, '...')\n",
            "                batchErrors = np.zeros(len(batchedData))\n",
            "                batchRandIxs = np.random.permutation(len(batchedData))\n",
            "\n",
            "                for batch, batchOrigI in enumerate(batchRandIxs):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\timit_train.py",
        "line_number": 183,
        "API": ".format(",
        "context": [
            "                                model.predictions, model.targetY, model.errorRate],\n",
            "                                feed_dict=feedDict)\n",
            "\n",
            "                            batchErrors[batch] = er\n",
            "                            print('\\n{} mode, total:{},batch:{}/{},epoch:{}/{},train loss={:.3f},mean train CER={:.3f}\\n'.format(\n",
            "                                level, totalN, batch+1, len(batchRandIxs), epoch+1, num_epochs, l, er/batch_size))\n",
            "\n",
            "                        elif mode == 'test':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\timit_train.py",
        "line_number": 190,
        "API": ".format(",
        "context": [
            "                        elif mode == 'test':\n",
            "                            l, pre, y, er = sess.run([model.loss, model.predictions,\n",
            "                                model.targetY, model.errorRate], feed_dict=feedDict)\n",
            "                            batchErrors[batch] = er\n",
            "                            print('\\n{} mode, total:{},batch:{}/{},test loss={:.3f},mean test CER={:.3f}\\n'.format(\n",
            "                                level, totalN, batch+1, len(batchRandIxs), l, er/batch_size))\n",
            "\n",
            "                    elif level == 'phn':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\timit_train.py",
        "line_number": 200,
        "API": ".format(",
        "context": [
            "                                model.predictions, model.targetY],\n",
            "                                feed_dict=feedDict)\n",
            "\n",
            "                            er = get_edit_distance([pre.values], [y.values], True, level)\n",
            "                            print('\\n{} mode, total:{},batch:{}/{},epoch:{}/{},train loss={:.3f},mean train PER={:.3f}\\n'.format(\n",
            "                                level, totalN, batch+1, len(batchRandIxs), epoch+1, num_epochs, l, er))\n",
            "                            batchErrors[batch] = er * len(batchSeqLengths)\n",
            "                        elif mode == 'test':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\timit_train.py",
        "line_number": 206,
        "API": ".format(",
        "context": [
            "                            batchErrors[batch] = er * len(batchSeqLengths)\n",
            "                        elif mode == 'test':\n",
            "                            l, pre, y = sess.run([model.loss, model.predictions, model.targetY], feed_dict=feedDict)\n",
            "                            er = get_edit_distance([pre.values], [y.values], True, level)\n",
            "                            print('\\n{} mode, total:{},batch:{}/{},test loss={:.3f},mean test PER={:.3f}\\n'.format(\n",
            "                                level, totalN, batch+1, len(batchRandIxs), l, er))\n",
            "                            batchErrors[batch] = er * len(batchSeqLengths)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\timit_train.py",
        "line_number": 221,
        "API": ".join(",
        "context": [
            "\n",
            "\n",
            "                    if mode=='train' and ((epoch * len(batchRandIxs) + batch + 1) % 20 == 0 or (\n",
            "                           epoch == num_epochs - 1 and batch == len(batchRandIxs) - 1)):\n",
            "                        checkpoint_path = os.path.join(savedir, 'model.ckpt')\n",
            "                        model.saver.save(sess, checkpoint_path, global_step=epoch)\n",
            "                        print('Model has been saved in {}'.format(savedir))\n",
            "                end = time.time()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\timit_train.py",
        "line_number": 230,
        "API": ".join(",
        "context": [
            "                print('Epoch ' + str(epoch + 1) + ' needs time:' + str(delta_time) + ' s')\n",
            "\n",
            "                if mode=='train':\n",
            "                    if (epoch + 1) % 1 == 0:\n",
            "                        checkpoint_path = os.path.join(savedir, 'model.ckpt')\n",
            "                        model.saver.save(sess, checkpoint_path, global_step=epoch)\n",
            "                        print('Model has been saved in {}'.format(savedir))\n",
            "                    epochER = batchErrors.sum() / totalN\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\main\\timit_train.py",
        "line_number": 240,
        "API": ".join(",
        "context": [
            "                    logging(model, logfile, epochER, epoch, delta_time, mode=mode)\n",
            "\n",
            "\n",
            "                if mode=='test':\n",
            "                    with open(os.path.join(resultdir, level + '_result.txt'), 'a') as result:\n",
            "                        result.write(output_to_sequence(y, type=level) + '\\n')\n",
            "                        result.write(output_to_sequence(pre, type=level) + '\\n')\n",
            "                        result.write('\\n')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\capsuleNetwork.py",
        "line_number": 28,
        "API": ".reduce_sum(",
        "context": [
            "    size of s: [batch_size, 1, next_channels*next_num_capsules, next_output_vector_len, 1]\n",
            "    size of v: [batch_size, 1, next_channels*next_num_capsules, next_output_vector_len, 1]\n",
            "    \"\"\"\n",
            "    assert s.dtype == tf.float32\n",
            "    squared_s = tf.reduce_sum(tf.square(s), axis=2, keep_dims=True)\n",
            "    normed_s = tf.norm(s, axis=2, keep_dims=True)\n",
            "    v = squared_s / (1+squared_s) / normed_s * s\n",
            "    assert v.get_shape()==s.get_shape()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\capsuleNetwork.py",
        "line_number": 41,
        "API": ".reshape(",
        "context": [
            "    size of w: [batch_size, channels, num_capsules, next_channels, next_num_capsules, vec_len, next_vec_len]\n",
            "    \"\"\"\n",
            "    scope = scope or \"routing\"\n",
            "    shape = u.get_shape()\n",
            "    u = tf.reshape(u, [shape[0], shape[1], shape[2], 1, 1, shape[3], 1])\n",
            "    u_ij = tf.tile(u, [1, 1, 1, next_num_channels, next_num_capsules, 1, 1])\n",
            "    with tf.variable_scope(scope):\n",
            "        w_shape = [1, shape[1], shape[2], next_num_channels, next_num_capsules, shape[3], next_output_vector_len]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\capsuleNetwork.py",
        "line_number": 46,
        "API": ".tile(",
        "context": [
            "    u_ij = tf.tile(u, [1, 1, 1, next_num_channels, next_num_capsules, 1, 1])\n",
            "    with tf.variable_scope(scope):\n",
            "        w_shape = [1, shape[1], shape[2], next_num_channels, next_num_capsules, shape[3], next_output_vector_len]\n",
            "        w = tf.get_variable(\"w\", shape=w_shape, dtype=tf.float32)\n",
            "        w = tf.tile(w, [shape[0], 1, 1, 1, 1, 1, 1])\n",
            "        u_hat = tf.matmul(w, u_ij, transpose_a=True)\n",
            "        # size of u_hat: [batch_size, channels*num_capsules, next_channels*next_num_capsules, next_vec_len, 1]\n",
            "        u_hat = tf.reshape(u_hat, [shape[0], shape[1]*shape[2], -1, next_output_vector_len, 1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\capsuleNetwork.py",
        "line_number": 51,
        "API": ".zeros(",
        "context": [
            "        u_hat = tf.matmul(w, u_ij, transpose_a=True)\n",
            "        # size of u_hat: [batch_size, channels*num_capsules, next_channels*next_num_capsules, next_vec_len, 1]\n",
            "        u_hat = tf.reshape(u_hat, [shape[0], shape[1]*shape[2], -1, next_output_vector_len, 1])\n",
            "        u_hat_without_backprop = tf.stop_gradient(u_hat, \"u_hat_without_backprop\")\n",
            "        b_ij = tf.constant(np.zeros([shape[0], shape[1]*shape[2], next_num_channels*next_num_capsules, 1, 1]), dtype=tf.float32)\n",
            "        c_ij = tf.nn.softmax(b_ij, dim=2)\n",
            "        for r in range(num_iter):\n",
            "            if r != num_iter-1:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\capsuleNetwork.py",
        "line_number": 56,
        "API": ".reduce_sum(",
        "context": [
            "        c_ij = tf.nn.softmax(b_ij, dim=2)\n",
            "        for r in range(num_iter):\n",
            "            if r != num_iter-1:\n",
            "                # size of s_j: [batch_size, 1, next_channels*next_num_capsules, next_output_vector_len, 1]\n",
            "                s_j = tf.reduce_sum(tf.multiply(c_ij, u_hat_without_backprop), axis=1, keep_dims=True)\n",
            "                v_j = squashing(s_j)\n",
            "                v_j =tf.tile(v_j, [1, shape[1]*shape[2], 1, 1, 1])\n",
            "                # b_ij += u_hat * v_j\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\capsuleNetwork.py",
        "line_number": 62,
        "API": ".reduce_sum(",
        "context": [
            "                v_j =tf.tile(v_j, [1, shape[1]*shape[2], 1, 1, 1])\n",
            "                # b_ij += u_hat * v_j\n",
            "                b_ij = b_ij + tf.matmul(u_hat, v_j, transpose_a=True)\n",
            "            else:\n",
            "                s_j = tf.reduce_sum(tf.multiply(c_ij, u_hat), axis=1, keep_dims=True)\n",
            "                v_j = squashing(s_j)\n",
            "    # size of v_j: [batch_size, 1, next_channels*next_num_capsules, next_output_vector_len, 1]\n",
            "    return v_j\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\capsuleNetwork.py",
        "line_number": 94,
        "API": ".conv2d(",
        "context": [
            "            if self._layer_type=='conv':\n",
            "                # shape of conv1:  [batch, height, width, channels]\n",
            "                kernel = tf.get_variable(\"conv_kernel\", shape=[kernel_size[0], kernel_size[1], input_shape[-1],\n",
            "                       self._num_channels*self._num_capsules*self._output_vector_len], dtype=tf.float32)\n",
            "                conv_output = tf.nn.conv2d(inputX, kernel, strides, padding)\n",
            "                shape1 = conv_output.get_shape()\n",
            "                capsule_output = tf.reshape(conv_output, [shape1[0], 1, -1, self._output_vector_len, 1])\n",
            "                if with_routing:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\capsuleNetwork.py",
        "line_number": 103,
        "API": ".reshape(",
        "context": [
            "                    # size of u: [batch_size, channels, num_capsules, output_vector_len]\n",
            "                    capsule_output = routing(capsule_output, self._num_channels, self._num_capsules, self._output_vector_len, num_iter, scope)\n",
            "                capsule_output = squashing(capsule_output)\n",
            "                # size of capsule_output: [batch_size, num_capsules, num_vector_len, output_vector_len]\n",
            "                capsule_output = tf.reshape(capsule_output, [input_shape[0], self._num_capsules, self._output_vector_len, self._num_channels])\n",
            "            elif self._layer_type=='dnn':\n",
            "                # here, we set with_routing to be True defaultly\n",
            "                inputX = tf.reshape(inputX, [input_shape[0], 1, input_shape[1]*input_shape[3], input_shape[2], 1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\capsuleNetwork.py",
        "line_number": 111,
        "API": ".squeeze(",
        "context": [
            "                capsule_output = routing(inputX, self._num_channels, self._num_capsules, self._output_vector_len, num_iter, scope)\n",
            "                # size of s: [batch_size, 1, num_channels*num_capsules, output_vector_len, 1]\n",
            "                capsule_output = squashing(capsule_output)\n",
            "                # size of capsule_output: [batch_size, num_channels*num_capsules, output_vector_len]\n",
            "                capsule_output = tf.squeeze(capsule_output, axis=[1, 4])\n",
            "            else:\n",
            "                capsule_output = None\n",
            "        return capsule_output\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\capsuleNetwork.py",
        "line_number": 124,
        "API": ".placeholder(",
        "context": [
            "        self.build_graph(self.args, self.maxTimeSteps)\n",
            "\n",
            "    def build_graph(self, args, maxTimeSteps):\n",
            "        self.maxTimeSteps = maxTimeSteps\n",
            "        self.inputX = tf.placeholder(tf.float32,shape=[maxTimeSteps, args.batch_size, args.num_feature])\n",
            "\n",
            "        # define tf.SparseTensor for ctc loss\n",
            "        self.targetIxs = tf.placeholder(tf.int64)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\capsuleNetwork.py",
        "line_number": 129,
        "API": ".placeholder(",
        "context": [
            "\n",
            "        # define tf.SparseTensor for ctc loss\n",
            "        self.targetIxs = tf.placeholder(tf.int64)\n",
            "        self.targetVals = tf.placeholder(tf.int32)\n",
            "        self.targetShape = tf.placeholder(tf.int64)\n",
            "        self.targetY = tf.SparseTensor(self.targetIxs, self.targetVals, self.targetShape)\n",
            "        self.seqLengths = tf.placeholder(tf.int32, shape=(args.batch_size))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\capsuleNetwork.py",
        "line_number": 144,
        "API": ".reshape(",
        "context": [
            "                           'keep prob': args.keep_prob,\n",
            "                           'batch size': args.batch_size}\n",
            "\n",
            "\n",
            "        inputX = tf.reshape(self.inputX, [args.batch_size, maxTimeSteps, args.num_feature, 1])\n",
            "        print(inputX.get_shape())\n",
            "        with tf.variable_scope(\"layer_conv1\"):\n",
            "            # shape of kernel: [batch, in_height, in_width, in_channels]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\capsuleNetwork.py",
        "line_number": 150,
        "API": ".conv2d(",
        "context": [
            "        with tf.variable_scope(\"layer_conv1\"):\n",
            "            # shape of kernel: [batch, in_height, in_width, in_channels]\n",
            "            kernel = tf.get_variable(\"kernel\", shape=[3, 3, 1, 16], dtype=tf.float32)\n",
            "            # shape of conv1:  [batch, height, width, channels]\n",
            "            conv1 = tf.nn.conv2d(inputX, kernel, (1,1,1,1), padding='VALID')\n",
            "\n",
            "        print(conv1.get_shape())\n",
            "        output = conv1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\capsuleNetwork.py",
        "line_number": 166,
        "API": ".transpose(",
        "context": [
            "        # last dnn layer for classification\n",
            "        vars_scope = \"capsule_dnn_layer\"\n",
            "        capLayer = CapsuleLayer(8, 16, args.num_classes, layer_type='dnn', vars_scope=vars_scope)\n",
            "        logits3d = capLayer(output, [3, 3], (1,1,1,1), args.num_iter)\n",
            "        logits3d = tf.transpose(logits3d, perm=[1, 0, 2])\n",
            "        self.loss = tf.reduce_mean(tf.nn.ctc_loss(self.targetY, logits3d, self.seqLengths))\n",
            "        self.var_op = tf.global_variables()\n",
            "        self.var_trainable_op = tf.trainable_variables()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\capsuleNetwork.py",
        "line_number": 175,
        "API": ".gradients(",
        "context": [
            "            # not apply gradient clipping\n",
            "            self.optimizer = tf.train.AdamOptimizer(args.learning_rate).minimize(self.loss)\n",
            "        else:\n",
            "            # apply gradient clipping\n",
            "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, self.var_trainable_op), args.grad_clip)\n",
            "            opti = tf.train.AdamOptimizer(args.learning_rate)\n",
            "            self.optimizer = opti.apply_gradients(zip(grads, self.var_trainable_op))\n",
            "        self.predictions = tf.to_int32(tf.nn.ctc_beam_search_decoder(logits3d, self.seqLengths, merge_repeated=False)[0][0])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\capsuleNetwork.py",
        "line_number": 180,
        "API": ".reduce_sum(",
        "context": [
            "            opti = tf.train.AdamOptimizer(args.learning_rate)\n",
            "            self.optimizer = opti.apply_gradients(zip(grads, self.var_trainable_op))\n",
            "        self.predictions = tf.to_int32(tf.nn.ctc_beam_search_decoder(logits3d, self.seqLengths, merge_repeated=False)[0][0])\n",
            "        if args.level == 'cha':\n",
            "            self.errorRate = tf.reduce_sum(tf.edit_distance(self.predictions, self.targetY, normalize=True))\n",
            "        self.initial_op = tf.global_variables_initializer()\n",
            "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=5, keep_checkpoint_every_n_hours=1)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\capsuleNetwork.py",
        "line_number": 188,
        "API": ".constant(",
        "context": [
            "\n",
            "## test code for simplicity\n",
            "if __name__ == '__main__':\n",
            "    sess=tf.InteractiveSession()\n",
            "    conv1 = tf.constant(np.random.rand(2,20,20,2), dtype=tf.float32)\n",
            "    # (self, num_capsules, num_channels, output_vector_len, layer_type='conv', vars_scope=None):\n",
            "    # (self, inputX, kernel_size, strides, routing=True, padding='VALID'):\n",
            "    capLayer1 = CapsuleLayer(2, 3, 10, layer_type='conv', vars_scope=\"testlayer1\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\deepSpeech2.py",
        "line_number": 49,
        "API": ".conv2d(",
        "context": [
            "    layer2_filter = tf.get_variable('layer2_filter', shape=(21, 11, 32, 32), dtype=tf.float32)\n",
            "    layer2_stride = [1, 2, 1, 1]\n",
            "    layer3_filter = tf.get_variable('layer3_filter', shape=(21, 11, 32, 96), dtype=tf.float32)\n",
            "    layer3_stride = [1, 2, 1, 1]\n",
            "    layer1 = tf.nn.conv2d(inputX, layer1_filter, layer1_stride, padding='SAME')\n",
            "    layer1 = tf.layers.batch_normalization(layer1, training=args.is_training)\n",
            "    layer1 = tf.contrib.layers.dropout(layer1, keep_prob=args.keep_prob[0], is_training=args.is_training)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\deepSpeech2.py",
        "line_number": 55,
        "API": ".dropout(",
        "context": [
            "    layer1 = tf.contrib.layers.dropout(layer1, keep_prob=args.keep_prob[0], is_training=args.is_training)\n",
            "\n",
            "    layer2 = tf.nn.conv2d(layer1, layer2_filter, layer2_stride, padding='SAME')\n",
            "    layer2 = tf.layers.batch_normalization(layer2, training=args.isTraining)\n",
            "    layer2 = tf.contrib.layers.dropout(layer2, keep_prob=args.keep_prob[1], is_training=args.is_training)\n",
            "\n",
            "    layer3 = tf.nn.conv2d(layer2, layer3_filter, layer3_stride, padding='SAME')\n",
            "    layer3 = tf.layers.batch_normalization(layer3, training=args.isTraining)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\deepSpeech2.py",
        "line_number": 66,
        "API": ".dropout(",
        "context": [
            "    # inputs must be [max_time, batch_size ,...]\n",
            "    layer4_cell = cell_fn(args.num_hidden, activation=args.activation)\n",
            "    layer4 = tf.nn.dynamic_rnn(layer4_cell, layer3, sequence_length=seqLengths, time_major=True)\n",
            "    layer4 = tf.layers.batch_normalization(layer4, training=args.isTraining)\n",
            "    layer4 = tf.contrib.layers.dropout(layer4, keep_prob=args.keep_prob[3], is_training=args.is_training)\n",
            "\n",
            "    layer5_cell = cell_fn(args.num_hidden, activation=args.activation)\n",
            "    layer5 = tf.nn.dynamic_rnn(layer5_cell, layer4, sequence_length=seqLengths, time_major=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\deepSpeech2.py",
        "line_number": 71,
        "API": ".dropout(",
        "context": [
            "\n",
            "    layer5_cell = cell_fn(args.num_hidden, activation=args.activation)\n",
            "    layer5 = tf.nn.dynamic_rnn(layer5_cell, layer4, sequence_length=seqLengths, time_major=True)\n",
            "    layer5 = tf.layers.batch_normalization(layer5, training=args.isTraining)\n",
            "    layer5 = tf.contrib.layers.dropout(layer5, keep_prob=args.keep_prob[4], is_training=args.is_training)\n",
            "\n",
            "    layer6_cell = cell_fn(args.num_hidden, activation=args.activation)\n",
            "    layer6 = tf.nn.dynamic_rnn(layer6_cell, layer5, sequence_length=seqLengths, time_major=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\deepSpeech2.py",
        "line_number": 76,
        "API": ".dropout(",
        "context": [
            "\n",
            "    layer6_cell = cell_fn(args.num_hidden, activation=args.activation)\n",
            "    layer6 = tf.nn.dynamic_rnn(layer6_cell, layer5, sequence_length=seqLengths, time_major=True)\n",
            "    layer6 = tf.layers.batch_normalization(layer6, training=args.isTraining)\n",
            "    layer6 = tf.contrib.layers.dropout(layer6, keep_prob=args.keep_prob[5], is_training=args.is_training)\n",
            "\n",
            "    layer7_cell = cell_fn(args.num_hidden, activation=args.activation)\n",
            "    layer7 = tf.nn.dynamic_rnn(layer7_cell, layer6, sequence_length=seqLengths, time_major=True)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\deepSpeech2.py",
        "line_number": 81,
        "API": ".dropout(",
        "context": [
            "\n",
            "    layer7_cell = cell_fn(args.num_hidden, activation=args.activation)\n",
            "    layer7 = tf.nn.dynamic_rnn(layer7_cell, layer6, sequence_length=seqLengths, time_major=True)\n",
            "    layer7 = tf.layers.batch_normalization(layer7, training=args.isTraining)\n",
            "    layer7 = tf.contrib.layers.dropout(layer7, keep_prob=args.keep_prob[6], is_training=args.is_training)\n",
            "\n",
            "    # fully-connected layer\n",
            "    layer_fc = tf.layers.dense(layer7, args.num_hidden_fc)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\deepSpeech2.py",
        "line_number": 101,
        "API": ".format(",
        "context": [
            "                self.cell_fn = lnGRUCell\n",
            "            elif args.rnncell == 'lstm':\n",
            "                self.cell_fn = lnBasicLSTMCell\n",
            "            else:\n",
            "                raise Exception(\"rnncell type not supported: {}\".format(args.rnncell))\n",
            "        else:\n",
            "            if args.rnncell == 'rnn':\n",
            "                self.cell_fn = tf.contrib.rnn.BasicRNNCell\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\deepSpeech2.py",
        "line_number": 110,
        "API": ".format(",
        "context": [
            "                self.cell_fn = tf.contrib.rnn.GRUCell\n",
            "            elif args.rnncell == 'lstm':\n",
            "                self.cell_fn = tf.contrib.rnn.BasicLSTMCell\n",
            "            else:\n",
            "                raise Exception(\"rnncell type not supported: {}\".format(args.rnncell))\n",
            "        self.build_graph(args, maxTimeSteps)\n",
            "\n",
            "    @describe\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\deepSpeech2.py",
        "line_number": 119,
        "API": ".placeholder(",
        "context": [
            "        self.graph = tf.Graph()\n",
            "        with self.graph.as_default():\n",
            "            # according to DeepSpeech2 paper, input is the spectrogram power of audio, but if you like,\n",
            "            # you can also use mfcc feature as the input.\n",
            "            self.inputX = tf.placeholder(tf.float32,\n",
            "                                         shape=(maxTimeSteps, args.batch_size, args.num_feature))\n",
            "            inputXrs = tf.reshape(self.inputX, [args.batch_size, args.num_feature, maxTimeSteps, 1])\n",
            "            #self.inputList = tf.split(inputXrs, maxTimeSteps, 0)  # convert inputXrs from [32*maxL,39] to [32,maxL,39]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\deepSpeech2.py",
        "line_number": 124,
        "API": ".placeholder(",
        "context": [
            "                                         shape=(maxTimeSteps, args.batch_size, args.num_feature))\n",
            "            inputXrs = tf.reshape(self.inputX, [args.batch_size, args.num_feature, maxTimeSteps, 1])\n",
            "            #self.inputList = tf.split(inputXrs, maxTimeSteps, 0)  # convert inputXrs from [32*maxL,39] to [32,maxL,39]\n",
            "\n",
            "            self.targetIxs = tf.placeholder(tf.int64)\n",
            "            self.targetVals = tf.placeholder(tf.int32)\n",
            "            self.targetShape = tf.placeholder(tf.int64)\n",
            "            self.targetY = tf.SparseTensor(self.targetIxs, self.targetVals, self.targetShape)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\deepSpeech2.py",
        "line_number": 141,
        "API": ".reduce_mean(",
        "context": [
            "                           'keep prob': args.keep_prob,\n",
            "                           'batch size': args.batch_size}\n",
            "\n",
            "            output_fc = build_deepSpeech2(self.args, maxTimeSteps, self.inputX, self.cell_fn, self.seqLengths)\n",
            "            self.loss = tf.reduce_mean(tf.nn.ctc_loss(self.targetY, output_fc, self.seqLengths))\n",
            "            self.var_op = tf.global_variables()\n",
            "            self.var_trainable_op = tf.trainable_variables()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\deepSpeech2.py",
        "line_number": 150,
        "API": ".gradients(",
        "context": [
            "                # not apply gradient clipping\n",
            "                self.optimizer = tf.train.AdamOptimizer(args.learning_rate).minimize(self.loss)\n",
            "            else:\n",
            "                # apply gradient clipping\n",
            "                grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, self.var_trainable_op), args.grad_clip)\n",
            "                opti = tf.train.AdamOptimizer(args.learning_rate)\n",
            "                self.optimizer = opti.apply_gradients(zip(grads, self.var_trainable_op))\n",
            "            self.predictions = tf.to_int32(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\deepSpeech2.py",
        "line_number": 156,
        "API": ".reduce_sum(",
        "context": [
            "                self.optimizer = opti.apply_gradients(zip(grads, self.var_trainable_op))\n",
            "            self.predictions = tf.to_int32(\n",
            "                tf.nn.ctc_beam_search_decoder(output_fc, self.seqLengths, merge_repeated=False)[0][0])\n",
            "            if args.level == 'cha':\n",
            "                self.errorRate = tf.reduce_sum(tf.edit_distance(self.predictions, self.targetY, normalize=True))\n",
            "            self.initial_op = tf.global_variables_initializer()\n",
            "            self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=5, keep_checkpoint_every_n_hours=1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\dynamic_brnn.py",
        "line_number": 46,
        "API": ".concat(",
        "context": [
            "        # tensor of shape: [max_time, batch_size, input_size]\n",
            "        output_fw, output_bw = outputs\n",
            "        # forward states, backward states\n",
            "        output_state_fw, output_state_bw = output_states\n",
            "        # output_fb = tf.concat(2, [output_fw, output_bw])\n",
            "        output_fb = tf.concat([output_fw, output_bw], 2)\n",
            "        shape = output_fb.get_shape().as_list()\n",
            "        output_fb = tf.reshape(output_fb, [shape[0], shape[1], 2, int(shape[2] / 2)])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\dynamic_brnn.py",
        "line_number": 56,
        "API": ".reshape(",
        "context": [
            "\n",
            "        if i != args.num_layer - 1:\n",
            "            hid_input = hidden\n",
            "        else:\n",
            "            outputXrs = tf.reshape(hidden, [-1, args.num_hidden])\n",
            "            # output_list = tf.split(0, maxTimeSteps, outputXrs)\n",
            "            output_list = tf.split(outputXrs, maxTimeSteps, 0)\n",
            "            fbHrs = [tf.reshape(t, [args.batch_size, args.num_hidden]) for t in output_list]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\dynamic_brnn.py",
        "line_number": 84,
        "API": ".format(",
        "context": [
            "                self.cell_fn = tf.contrib.rnn.GRUCell\n",
            "            elif args.rnncell == 'lstm':\n",
            "                self.cell_fn = tf.contrib.rnn.BasicLSTMCell\n",
            "            else:\n",
            "                raise Exception(\"rnncell type not supported: {}\".format(args.rnncell))\n",
            "\n",
            "        self.build_graph(args, maxTimeSteps)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\dynamic_brnn.py",
        "line_number": 92,
        "API": ".placeholder(",
        "context": [
            "    @describe\n",
            "    def build_graph(self, args, maxTimeSteps):\n",
            "        self.graph = tf.Graph()\n",
            "        with self.graph.as_default():\n",
            "            self.inputX = tf.placeholder(tf.float32,\n",
            "                                         shape=(maxTimeSteps, args.batch_size, args.num_feature))  # [maxL,32,39]\n",
            "            inputXrs = tf.reshape(self.inputX, [-1, args.num_feature])\n",
            "            # self.inputList = tf.split(0, maxTimeSteps, inputXrs) #convert inputXrs from [32*maxL,39] to [32,maxL,39]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\dynamic_brnn.py",
        "line_number": 97,
        "API": ".placeholder(",
        "context": [
            "                                         shape=(maxTimeSteps, args.batch_size, args.num_feature))  # [maxL,32,39]\n",
            "            inputXrs = tf.reshape(self.inputX, [-1, args.num_feature])\n",
            "            # self.inputList = tf.split(0, maxTimeSteps, inputXrs) #convert inputXrs from [32*maxL,39] to [32,maxL,39]\n",
            "            self.inputList = tf.split(inputXrs, maxTimeSteps, 0)  # convert inputXrs from [32*maxL,39] to [32,maxL,39]\n",
            "            self.targetIxs = tf.placeholder(tf.int64)\n",
            "            self.targetVals = tf.placeholder(tf.int32)\n",
            "            self.targetShape = tf.placeholder(tf.int64)\n",
            "            self.targetY = tf.SparseTensor(self.targetIxs, self.targetVals, self.targetShape)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\dynamic_brnn.py",
        "line_number": 118,
        "API": ".zeros(",
        "context": [
            "            with tf.name_scope('fc-layer'):\n",
            "                with tf.variable_scope('fc'):\n",
            "                    weightsClasses = tf.Variable(\n",
            "                        tf.truncated_normal([args.num_hidden, args.num_class], name='weightsClasses'))\n",
            "                    biasesClasses = tf.Variable(tf.zeros([args.num_class]), name='biasesClasses')\n",
            "                    logits = [tf.matmul(t, weightsClasses) + biasesClasses for t in fbHrs]\n",
            "            logits3d = tf.stack(logits)\n",
            "            self.loss = tf.reduce_mean(tf.nn.ctc_loss(self.targetY, logits3d, self.seqLengths))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\dynamic_brnn.py",
        "line_number": 136,
        "API": ".reduce_sum(",
        "context": [
            "                self.optimizer = opti.apply_gradients(zip(grads, self.var_trainable_op))\n",
            "            self.predictions = tf.to_int32(\n",
            "                tf.nn.ctc_beam_search_decoder(logits3d, self.seqLengths, merge_repeated=False)[0][0])\n",
            "            if args.level == 'cha':\n",
            "                self.errorRate = tf.reduce_sum(tf.edit_distance(self.predictions, self.targetY, normalize=True))\n",
            "            self.initial_op = tf.global_variables_initializer()\n",
            "            self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=5, keep_checkpoint_every_n_hours=1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\n-gram\\generate.py",
        "line_number": 14,
        "API": ".load(",
        "context": [
            "import pickle\n",
            "\n",
            "def load_obj(name):\n",
            "    with open(name + '.pkl', 'rb') as f:\n",
            "        return pickle.load(f)\n",
            "\n",
            "def frequence(gram, type=2):\n",
            "    if type == 2:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\n-gram\\ngram.py",
        "line_number": 31,
        "API": ".join(",
        "context": [
            "        bigram['SOS'] = {}\n",
            "        trigram = {}\n",
            "        for subdir, dirs, files in os.walk(self.rootdir):\n",
            "            for f in files:\n",
            "                fullFilename = os.path.join(subdir, f)\n",
            "                filenameNoSuffix =  os.path.splitext(fullFilename)[0]\n",
            "            if f.endswith('.label'):\n",
            "                with open(fullFilename, 'r') as f:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\models\\n-gram\\ngram.py",
        "line_number": 37,
        "API": ".strip(",
        "context": [
            "            if f.endswith('.label'):\n",
            "                with open(fullFilename, 'r') as f:\n",
            "                    line = f.readline()\n",
            "                    corpus.append(line)\n",
            "                    line = line.strip().split(' ')\n",
            "                    len_sent = range(len(line))\n",
            "                    for idx in len_sent:\n",
            "                        word = line[idx]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\pipeline\\big_input.py",
        "line_number": 38,
        "API": ".join(",
        "context": [
            "  def __init__(self, path):\n",
            "    self.path = path\n",
            "\n",
            "  def write(self, content, filename, feature_num=2):\n",
            "    tfrecords_filename = os.path.join(self.path, filename)\n",
            "    writer = tf.python_io.TFRecordWriter(tfrecords_filename)\n",
            "    if feature_num>1:\n",
            "      assert isinstance(content, list), 'content must be a list now'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\pipeline\\big_input.py",
        "line_number": 48,
        "API": ".array(",
        "context": [
            "        feature = content[i]\n",
            "        if isinstance(feature, int):\n",
            "          feature_dict['feature'+str(i+1)]=_int64_feature(feature)\n",
            "        else:\n",
            "          feature_raw = np.array(feature).tostring()\n",
            "          feature_dict['feature'+str(i+1)]=_bytes_feature(feature_raw)\n",
            "      features_to_write = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
            "      writer.write(features_to_write.SerializeToString())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\pipeline\\big_input.py",
        "line_number": 94,
        "API": ".ceil(",
        "context": [
            "  fs = FLAGS.feature_size\n",
            "  num_epochs = FLAGS.num_epochs\n",
            "  batch_size = FLAGS.batch_size\n",
            "  num_classes = FLAGS.num_classes\n",
            "  num_batches = int(math.ceil(1.0*sn/batch_size))\n",
            "\n",
            "  # x:[sn, tl, fs]\n",
            "  with tf.variable_scope('train-samples'):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\pipeline\\big_input.py",
        "line_number": 108,
        "API": ".eye(",
        "context": [
            "  with tf.variable_scope('train-labels'):\n",
            "    y = []\n",
            "    for n in range(sn):\n",
            "      sub_y = np.random.randint(0, num_classes)\n",
            "      y_one_hot = np.eye(num_classes)[sub_y]\n",
            "      y.append(y_one_hot.astype(np.int32))\n",
            "\n",
            "  with tf.variable_scope('TFRecordWriter'):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\pipeline\\big_input.py",
        "line_number": 114,
        "API": ".write(",
        "context": [
            "\n",
            "  with tf.variable_scope('TFRecordWriter'):\n",
            "    record_writer = RecordWriter(logdir)\n",
            "    for n in range(sn):\n",
            "      record_writer.write([x[n], y[n]], 'samples-labels[%s].tfrecords'%str(n))\n",
            "\n",
            "  with tf.variable_scope('FilesProducer'):\n",
            "    filenames = [os.path.join(logdir, 'samples-labels[%s].tfrecords' % str(i)) for i in range(sn)]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\pipeline\\big_input.py",
        "line_number": 123,
        "API": ".reshape(",
        "context": [
            "\n",
            "  with tf.variable_scope('Reader'):\n",
            "    features = read(filenamesQueue, dtypes=[list, list])\n",
            "    # when handling array, must specify its shape, so reshape operation\n",
            "    feature_x = tf.reshape(tf.decode_raw(features['feature1'], tf.float32), [fs])\n",
            "    feature_y = tf.reshape(tf.decode_raw(features['feature2'], tf.int32), [num_classes])\n",
            "\n",
            "  with tf.variable_scope('InputProducer'):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\pipeline\\big_input.py",
        "line_number": 132,
        "API": ".softmax_cross_entropy_with_logits(",
        "context": [
            "    batched_x = tf.layers.dense(batched_x, 2*fs)\n",
            "    batched_x = tf.layers.dense(batched_x, num_classes)\n",
            "\n",
            "  with tf.variable_scope('Loss'):\n",
            "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=batched_y, logits=batched_x))\n",
            "    optimizer = tf.train.AdamOptimizer(0.1)\n",
            "    train_op = optimizer.minimize(loss)\n",
            "    tf.summary.scalar('Loss', loss)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\pipeline\\big_input.py",
        "line_number": 140,
        "API": ".join(",
        "context": [
            "  merged = tf.summary.merge_all()\n",
            "\n",
            "  t1 = time.time()\n",
            "  sess = tf.Session()\n",
            "  checkpoint_path = os.path.join(logdir, scale+'_model')\n",
            "  writer = tf.summary.FileWriter(logdir, sess.graph)\n",
            "  sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\n",
            "  coord = tf.train.Coordinator()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\pipeline\\big_input.py",
        "line_number": 146,
        "API": ".save(",
        "context": [
            "  sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\n",
            "  coord = tf.train.Coordinator()\n",
            "  threads = queue_runner_impl.start_queue_runners(sess=sess)\n",
            "  saver = saver_lib.Saver(write_version=saver_pb2.SaverDef.V2)\n",
            "  saver.save(sess, checkpoint_path)\n",
            "\n",
            "  for i in range(num_batches*num_epochs):\n",
            "    l, _, summary = sess.run([loss, train_op, merged])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\pipeline\\big_input.py",
        "line_number": 154,
        "API": ".join(",
        "context": [
            "    writer.add_summary(summary, i)\n",
            "    print 'batch '+str(i+1)+'/'+str(num_batches*num_epochs)+'\\tLoss:'+str(l)\n",
            "  writer.close()\n",
            "  coord.request_stop()\n",
            "  coord.join(threads)\n",
            "  print 'program takes time:'+str(time.time()-t1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\pipeline\\small_input.py",
        "line_number": 41,
        "API": ".ceil(",
        "context": [
            "  fs = FLAGS.feature_size\n",
            "  num_epochs = FLAGS.num_epochs\n",
            "  batch_size = FLAGS.batch_size\n",
            "  num_classes = FLAGS.num_classes\n",
            "  num_batches = int(math.ceil(1.0*sn/batch_size))\n",
            "\n",
            "  with tf.variable_scope('train-samples'):\n",
            "    x = tf.constant(np.random.rand(sn, fs).astype(np.float32))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\pipeline\\small_input.py",
        "line_number": 48,
        "API": ".one_hot(",
        "context": [
            "    x = tf.constant(np.random.rand(sn, fs).astype(np.float32))\n",
            "\n",
            "  with tf.variable_scope('train-labels'):\n",
            "    indices = np.random.randint(0, num_classes, size=sn).astype(np.int32)\n",
            "    y = tf.one_hot(indices, depth=num_classes,\n",
            "                   on_value=1.0,\n",
            "                   off_value=0.0,\n",
            "                   axis=-1,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\pipeline\\small_input.py",
        "line_number": 60,
        "API": ".batch(",
        "context": [
            "    slice_x, slice_y = tf.train.slice_input_producer([x, y], \n",
            "        num_epochs = num_epochs, seed=22, \n",
            "        capacity=36, shuffle=True)\n",
            "\n",
            "    batched_x, batched_y = tf.train.batch([slice_x, slice_y], \n",
            "        batch_size=batch_size, dynamic_pad=False, \n",
            "        allow_smaller_final_batch=True)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\pipeline\\small_input.py",
        "line_number": 68,
        "API": ".softmax_cross_entropy_with_logits(",
        "context": [
            "    batched_x = tf.layers.dense(batched_x, 2*fs)\n",
            "    batched_x = tf.layers.dense(batched_x, num_classes)\n",
            "  \n",
            "  with tf.variable_scope('Loss'):\n",
            "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
            "        labels=batched_y, logits=batched_x))\n",
            "    optimizer = tf.train.AdamOptimizer(0.1)\n",
            "    train_op = optimizer.minimize(loss)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\pipeline\\small_input.py",
        "line_number": 84,
        "API": ".save(",
        "context": [
            "  sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\n",
            "  coord = tf.train.Coordinator()\n",
            "  threads = queue_runner_impl.start_queue_runners(sess=sess)\n",
            "  saver = saver_lib.Saver(write_version=saver_pb2.SaverDef.V2)\n",
            "  saver.save(sess, checkpoint_path)\n",
            "  for i in range(num_batches*num_epochs):\n",
            "    l, _, summary = sess.run([loss, train_op, merged])\n",
            "    writer.add_summary(summary, i)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\pipeline\\small_input.py",
        "line_number": 91,
        "API": ".join(",
        "context": [
            "    writer.add_summary(summary, i)\n",
            "    print 'batch '+str(i+1)+'/'+str(num_batches*num_epochs)+'\\tLoss:'+str(l)\n",
            "  writer.close()\t\n",
            "  coord.request_stop()\n",
            "  coord.join(threads)\n",
            "  print 'program takes time:'+str(time.time()-t1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\calcPER.py",
        "line_number": 55,
        "API": ".mean(",
        "context": [
            "        if normalize:\n",
            "            dist_i /= float(len(truth_seq_list[i]))\n",
            "        distances.append(dist_i)\n",
            "\n",
            "    return np.mean(distances)\n",
            "\n",
            "\n",
            "def seq_to_single_char_strings(seq):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\calcPER.py",
        "line_number": 61,
        "API": ".join(",
        "context": [
            "\n",
            "def seq_to_single_char_strings(seq):\n",
            "    strings = []\n",
            "    for s in seq:\n",
            "        strings.append(''.join([chr(65 + p) for p in s]))\n",
            "\n",
            "    return strings\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\calcPER.py",
        "line_number": 69,
        "API": ".max(",
        "context": [
            "\n",
            "def sparse_tensor_to_seq_list(sparse_seq, merge_phn=True):\n",
            "    phonemes_list = []\n",
            "    it = 0\n",
            "    num_samples = np.max(sparse_seq.indices, axis=0)[0] + 1\n",
            "    for n in range(num_samples):\n",
            "        cur_sample_indices = sparse_seq.indices[sparse_seq.indices[:, 0] == n, 1]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\calcPER.py",
        "line_number": 76,
        "API": ".max(",
        "context": [
            "\n",
            "        if len(cur_sample_indices) == 0:\n",
            "            seq_length = 0\n",
            "        else:\n",
            "            seq_length = np.max(cur_sample_indices) + 1\n",
            "\n",
            "        seq = sparse_seq.vals[it:it+seq_length]\n",
            "        _seq = [IDX_MAPPING[p] for p in seq] if merge_phn else seq\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\ed.py",
        "line_number": 36,
        "API": ".sort(",
        "context": [
            "    group_phn.append('sil')\n",
            "    for key in mapping.keys():\n",
            "        if key in orig_phn:\n",
            "            group_phn.remove(key)\n",
            "    group_phn.sort()\n",
            "    return group_phn\n",
            "\n",
            "def list_to_sparse_tensor(targetList,mode='train'):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\ed.py",
        "line_number": 59,
        "API": ".max(",
        "context": [
            "                indices.append([tI, seqI])\n",
            "                vals.append(val)\n",
            "            else:\n",
            "                raise ValueError(\"Invalid mode.\",mode)\n",
            "    shape = [len(targetList), np.asarray(indices).max(0)[1]+1] #shape\n",
            "    return (np.array(indices), np.array(vals), np.array(shape))\n",
            "\n",
            "def get_edit_distance(hyp_arr,truth_arr,mode='train'):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\ed.py",
        "line_number": 69,
        "API": ".edit_distance(",
        "context": [
            "    graph = tf.Graph()\n",
            "    with graph.as_default():\n",
            "        truth = tf.sparse_placeholder(tf.int32)\n",
            "        hyp = tf.sparse_placeholder(tf.int32)\n",
            "        editDist = tf.edit_distance(hyp, truth, normalize=True)\n",
            "\n",
            "    with tf.Session(graph=graph) as session:\n",
            "        truthTest = list_to_sparse_tensor(truth_arr, mode)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\taskUtils.py",
        "line_number": 29,
        "API": ".makedirs(",
        "context": [
            "    \"\"\"\n",
            "    if isinstance(path, list):\n",
            "        for p in path:\n",
            "            if not os.path.exists(p):\n",
            "                os.makedirs(p)\n",
            "    else:\n",
            "        if not os.path.exists(path):\n",
            "            os.makedirs(path)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\utils.py",
        "line_number": 77,
        "API": ".join(",
        "context": [
            "            if ind == len(phn):\n",
            "                pass\n",
            "            else:\n",
            "                seq.append(phn[ind])\n",
            "        seq = ' '.join(seq)\n",
            "        return seq\n",
            "\n",
            "    elif type == 'cha':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\utils.py",
        "line_number": 91,
        "API": ".join(",
        "context": [
            "            elif ind == 28:\n",
            "                pass\n",
            "            else:\n",
            "                seq.append(chr(ind+96))\n",
            "        seq = ''.join(seq)\n",
            "        return seq\n",
            "    else:\n",
            "        raise TypeError('mode should be phoneme or character')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\utils.py",
        "line_number": 103,
        "API": ".join(",
        "context": [
            "        if t == len(phn):\n",
            "            pass\n",
            "        else:\n",
            "            seq.append(phn[t])\n",
            "    seq = ' '.join(seq)\n",
            "    return seq\n",
            "\n",
            "@describe\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\utils.py",
        "line_number": 115,
        "API": ".write(",
        "context": [
            "        raise TypeError('mode should be train or test or config.')\n",
            "    logfile = logfile\n",
            "    if mode == 'config':\n",
            "        with open(logfile, \"a\") as myfile:\n",
            "            myfile.write(str(model.config)+'\\n')\n",
            "\n",
            "    elif mode == 'train':\n",
            "        with open(logfile, \"a\") as myfile:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\utils.py",
        "line_number": 120,
        "API": ".write(",
        "context": [
            "\n",
            "    elif mode == 'train':\n",
            "        with open(logfile, \"a\") as myfile:\n",
            "            myfile.write(str(time.strftime('%X %x %Z'))+'\\n')\n",
            "            myfile.write(\"Epoch:\"+str(epoch+1)+' '+\"train error rate:\"+str(errorRate)+'\\n')\n",
            "            myfile.write(\"Epoch:\"+str(epoch+1)+' '+\"train time:\"+str(delta_time)+' s\\n')\n",
            "    elif mode == 'test':\n",
            "        logfile = logfile+'_TEST'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\utils.py",
        "line_number": 125,
        "API": ".write(",
        "context": [
            "            myfile.write(\"Epoch:\"+str(epoch+1)+' '+\"train time:\"+str(delta_time)+' s\\n')\n",
            "    elif mode == 'test':\n",
            "        logfile = logfile+'_TEST'\n",
            "        with open(logfile, \"a\") as myfile:\n",
            "            myfile.write(str(model.config)+'\\n')\n",
            "            myfile.write(str(time.strftime('%X %x %Z'))+'\\n')\n",
            "            myfile.write(\"test error rate:\"+str(errorRate)+'\\n')\n",
            "    elif mode == 'dev':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\utils.py",
        "line_number": 131,
        "API": ".write(",
        "context": [
            "            myfile.write(\"test error rate:\"+str(errorRate)+'\\n')\n",
            "    elif mode == 'dev':\n",
            "        logfile = logfile+'_DEV'\n",
            "        with open(logfile, \"a\") as myfile:\n",
            "            myfile.write(str(model.config)+'\\n')\n",
            "            myfile.write(str(time.strftime('%X %x %Z'))+'\\n')\n",
            "            myfile.write(\"development error rate:\"+str(errorRate)+'\\n')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\utils.py",
        "line_number": 140,
        "API": ".sum(",
        "context": [
            "def count_params(model, mode='trainable'):\n",
            "    ''' count all parameters of a tensorflow graph\n",
            "    '''\n",
            "    if mode == 'all':\n",
            "        num = np.sum([np.product([xi.value for xi in x.get_shape()]) for x in model.var_op])\n",
            "    elif mode == 'trainable':\n",
            "        num = np.sum([np.product([xi.value for xi in x.get_shape()]) for x in model.var_trainable_op])\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\utils.py",
        "line_number": 192,
        "API": ".max(",
        "context": [
            "        for tI, target in enumerate(targetList):\n",
            "            for seqI, val in enumerate(target):\n",
            "                indices.append([tI, seqI])\n",
            "                vals.append(val)\n",
            "        shape = [len(targetList), np.asarray(indices).max(axis=0)[1]+1] #shape\n",
            "        return (np.array(indices), np.array(vals), np.array(shape))\n",
            "\n",
            "    elif level == 'phn':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\utils.py",
        "line_number": 209,
        "API": ".max(",
        "context": [
            "                if val < len(phn) and (phn[val] in mapping.keys()):\n",
            "                    val = group_phn.index(mapping[phn[val]])\n",
            "                indices.append([tI, seqI])\n",
            "                vals.append(val)\n",
            "        shape = [len(targetList), np.asarray(indices).max(0)[1]+1] #shape\n",
            "        return (np.array(indices), np.array(vals), np.array(shape))\n",
            "\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\utils.py",
        "line_number": 225,
        "API": ".reduce_sum(",
        "context": [
            "    graph = tf.Graph()\n",
            "    with graph.as_default():\n",
            "        truth = tf.sparse_placeholder(tf.int32)\n",
            "        hyp = tf.sparse_placeholder(tf.int32)\n",
            "        editDist = tf.reduce_sum(tf.edit_distance(hyp, truth, normalize=normalize))\n",
            "\n",
            "    with tf.Session(graph=graph) as session:\n",
            "        truthTest = list_to_sparse_tensor(truth_arr, level)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\utils.py",
        "line_number": 253,
        "API": ".zeros(",
        "context": [
            "    dataBatches = []\n",
            "\n",
            "    while end <= len(inputList):\n",
            "\t    # batchSeqLengths store the time-length of each sample in a mini-batch\n",
            "        batchSeqLengths = np.zeros(batchSize)\n",
            "\n",
            "  \t    # randIxs is the shuffled index of input list\n",
            "        for batchI, origI in enumerate(randIxs[start:end]):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\utils.py",
        "line_number": 259,
        "API": ".zeros(",
        "context": [
            "  \t    # randIxs is the shuffled index of input list\n",
            "        for batchI, origI in enumerate(randIxs[start:end]):\n",
            "            batchSeqLengths[batchI] = inputList[origI].shape[-1]\n",
            "\n",
            "        batchInputs = np.zeros((maxLength, batchSize, nFeatures))\n",
            "        batchTargetList = []\n",
            "        for batchI, origI in enumerate(randIxs[start:end]):\n",
            "\t        # padSecs is the length of padding\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\utils.py",
        "line_number": 265,
        "API": ".pad(",
        "context": [
            "        for batchI, origI in enumerate(randIxs[start:end]):\n",
            "\t        # padSecs is the length of padding\n",
            "            padSecs = maxLength - inputList[origI].shape[1]\n",
            "\t        # numpy.pad pad the inputList[origI] with zeos at the tail\n",
            "            batchInputs[:,batchI,:] = np.pad(inputList[origI].T, ((0,padSecs),(0,0)), 'constant', constant_values=0)\n",
            "\t        # target label\n",
            "            batchTargetList.append(targetList[origI])\n",
            "        dataBatches.append((batchInputs, list_to_sparse_tensor(batchTargetList, level), batchSeqLengths))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\utils.py",
        "line_number": 276,
        "API": ".join(",
        "context": [
            "\n",
            "def load_batched_data(mfccPath, labelPath, batchSize, mode, level):\n",
            "    '''returns 3-element tuple: batched data (list), maxTimeLength (int), and\n",
            "       total number of samples (int)'''\n",
            "    return data_lists_to_batches([np.load(os.path.join(mfccPath, fn)) for fn in os.listdir(mfccPath)],\n",
            "                                 [np.load(os.path.join(labelPath, fn)) for fn in os.listdir(labelPath)],\n",
            "                                 batchSize, level) + (len(os.listdir(mfccPath)),)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\utils.py",
        "line_number": 296,
        "API": ".moments(",
        "context": [
            "        param_shape = inputs_shape[-1:]\n",
            "\n",
            "        beta = tf.get_variable('beta', param_shape, initializer=tf.constant_initializer(0.))\n",
            "        gamma = tf.get_variable('gamma', param_shape, initializer=tf.constant_initializer(1.))\n",
            "        batch_mean, batch_var = tf.nn.moments(x, axis)\n",
            "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
            "\n",
            "        def mean_var_with_update():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\utils.py",
        "line_number": 301,
        "API": ".control_dependencies(",
        "context": [
            "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
            "\n",
            "        def mean_var_with_update():\n",
            "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
            "            with tf.control_dependencies([ema_apply_op]):\n",
            "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
            "\n",
            "        mean, var = tf.cond(is_training,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\utils.py",
        "line_number": 313,
        "API": ".prod(",
        "context": [
            "\n",
            "def _get_dims(shape):\n",
            "    \"\"\"get shape for initialization\n",
            "    \"\"\"\n",
            "    fan_in = shape[0] if len(shape) == 2 else np.prod(shape[:-1])\n",
            "    fan_out = shape[1] if len(shape) == 2 else shape[-1]\n",
            "    return fan_in, fan_out\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\utils.py",
        "line_number": 320,
        "API": ".dropout(",
        "context": [
            "\n",
            "def dropout(x, keep_prob, is_training):\n",
            "    \"\"\" Apply dropout to a tensor\n",
            "    \"\"\"\n",
            "    return tf.contrib.layers.dropout(x, keep_prob=keep_prob, is_training=is_training)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\Automatic_Speech_Recognition\\speechvalley\\utils\\visualization.py",
        "line_number": 25,
        "API": ".linspace(",
        "context": [
            "    fs = spf.getframerate()\n",
            "    if spf.getnchannels() == 2:\n",
            "        print('Just mono files')\n",
            "        sys.exit(0)\n",
            "    Time=np.linspace(0, len(signal)/fs, num=len(signal))\n",
            "    plt.figure(1)\n",
            "    plt.title('Waveform of an audio')\n",
            "    plt.plot(Time,signal)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\convert.py",
        "line_number": 32,
        "API": ".save(",
        "context": [
            "        if caffemodel_path is not None:\n",
            "            data = transformer.transform_data()\n",
            "            print_stderr('Saving data...')\n",
            "            with open(data_output_path, 'wb') as data_out:\n",
            "                np.save(data_out, data)\n",
            "        if code_output_path:\n",
            "            print_stderr('Saving source...')\n",
            "            with open(code_output_path, 'wb') as src_out:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\convert.py",
        "line_number": 39,
        "API": ".format(",
        "context": [
            "            with open(code_output_path, 'wb') as src_out:\n",
            "                src_out.write(transformer.transform_source())\n",
            "        print_stderr('Done.')\n",
            "    except KaffeError as err:\n",
            "        fatal_error('Error encountered: {}'.format(err))\n",
            "\n",
            "\n",
            "def main():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\classify.py",
        "line_number": 16,
        "API": ".argmax(",
        "context": [
            "    # Get a list of ImageNet class labels\n",
            "    with open('imagenet-classes.txt', 'rb') as infile:\n",
            "        class_labels = map(str.strip, infile.readlines())\n",
            "    # Pick the class with the highest confidence for each image\n",
            "    class_indices = np.argmax(probs, axis=1)\n",
            "    # Display the results\n",
            "    print('\\n{:20} {:30} {}'.format('Image', 'Classified As', 'Confidence'))\n",
            "    print('-' * 70)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\classify.py",
        "line_number": 24,
        "API": ".format(",
        "context": [
            "    for img_idx, image_path in enumerate(image_paths):\n",
            "        img_name = osp.basename(image_path)\n",
            "        class_name = class_labels[class_indices[img_idx]]\n",
            "        confidence = round(probs[img_idx, class_indices[img_idx]] * 100, 2)\n",
            "        print('{:20} {:30} {} %'.format(img_name, class_name, confidence))\n",
            "\n",
            "\n",
            "def classify(model_data_path, image_paths):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\classify.py",
        "line_number": 34,
        "API": ".placeholder(",
        "context": [
            "    # Get the data specifications for the GoogleNet model\n",
            "    spec = models.get_data_spec(model_class=models.GoogleNet)\n",
            "\n",
            "    # Create a placeholder for the input image\n",
            "    input_node = tf.placeholder(tf.float32,\n",
            "                                shape=(None, spec.crop_size, spec.crop_size, spec.channels))\n",
            "\n",
            "    # Construct the network\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\classify.py",
        "line_number": 50,
        "API": ".load(",
        "context": [
            "        threads = image_producer.start(session=sesh, coordinator=coordinator)\n",
            "\n",
            "        # Load the converted parameters\n",
            "        print('Loading the model')\n",
            "        net.load(model_data_path, sesh)\n",
            "\n",
            "        # Load the input image\n",
            "        print('Loading the images')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\classify.py",
        "line_number": 63,
        "API": ".join(",
        "context": [
            "        display_results([image_paths[i] for i in indices], probs)\n",
            "\n",
            "        # Stop the worker threads\n",
            "        coordinator.request_stop()\n",
            "        coordinator.join(threads, stop_grace_period_secs=2)\n",
            "\n",
            "def main():\n",
            "    # Parse arguments\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\dataset.py",
        "line_number": 17,
        "API": ".shape(",
        "context": [
            "    mean  : Subtracted from the image\n",
            "    '''\n",
            "    # Rescale\n",
            "    if isotropic:\n",
            "        img_shape = tf.to_float(tf.shape(img)[:2])\n",
            "        min_length = tf.minimum(img_shape[0], img_shape[1])\n",
            "        new_shape = tf.to_int32((scale / min_length) * img_shape)\n",
            "    else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\dataset.py",
        "line_number": 27,
        "API": ".slice(",
        "context": [
            "    # Center crop\n",
            "    # Use the slice workaround until crop_to_bounding_box supports deferred tensor shapes\n",
            "    # See: https://github.com/tensorflow/tensorflow/issues/521\n",
            "    offset = (new_shape - crop) / 2\n",
            "    img = tf.slice(img, begin=tf.pack([offset[0], offset[1], 0]), size=tf.pack([crop, crop, -1]))\n",
            "    # Mean subtraction\n",
            "    return tf.to_float(img) - mean\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\dataset.py",
        "line_number": 55,
        "API": ".format(",
        "context": [
            "        num_images = len(self.image_paths)\n",
            "        batch_size = min(num_images, batch_size or self.data_spec.batch_size)\n",
            "        if num_images % batch_size != 0:\n",
            "            raise ValueError(\n",
            "                'The total number of images ({}) must be divisible by the batch size ({}).'.format(\n",
            "                    num_images, batch_size))\n",
            "        self.num_batches = num_images / batch_size\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\dataset.py",
        "line_number": 65,
        "API": ".range(",
        "context": [
            "                                       dtypes=[tf.int32, tf.bool, tf.string],\n",
            "                                       name='path_queue')\n",
            "\n",
            "        # Enqueue all image paths, along with their indices\n",
            "        indices = tf.range(num_images)\n",
            "        self.enqueue_paths_op = self.path_queue.enqueue_many([indices, self.extension_mask,\n",
            "                                                              self.image_paths])\n",
            "        # Close the path queue (no more additions)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\dataset.py",
        "line_number": 76,
        "API": ".ceil(",
        "context": [
            "        (idx, processed_image) = self.process()\n",
            "\n",
            "        # Create a queue that will contain the processed images (and their indices)\n",
            "        image_shape = (self.data_spec.crop_size, self.data_spec.crop_size, self.data_spec.channels)\n",
            "        processed_queue = tf.FIFOQueue(capacity=int(np.ceil(num_images / float(num_concurrent))),\n",
            "                                       dtypes=[tf.int32, tf.float32],\n",
            "                                       shapes=[(), image_shape],\n",
            "                                       name='processed_queue')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\dataset.py",
        "line_number": 115,
        "API": ".get(",
        "context": [
            "\n",
            "    def batches(self, session):\n",
            "        '''Yield a batch until no more images are left.'''\n",
            "        for _ in xrange(self.num_batches):\n",
            "            yield self.get(session=session)\n",
            "\n",
            "    def load_image(self, image_path, is_jpeg):\n",
            "        # Read the file\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\dataset.py",
        "line_number": 121,
        "API": ".cond(",
        "context": [
            "    def load_image(self, image_path, is_jpeg):\n",
            "        # Read the file\n",
            "        file_data = tf.read_file(image_path)\n",
            "        # Decode the image data\n",
            "        img = tf.cond(\n",
            "            is_jpeg,\n",
            "            lambda: tf.image.decode_jpeg(file_data, channels=self.data_spec.channels),\n",
            "            lambda: tf.image.decode_png(file_data, channels=self.data_spec.channels))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\dataset.py",
        "line_number": 128,
        "API": ".reverse(",
        "context": [
            "            lambda: tf.image.decode_png(file_data, channels=self.data_spec.channels))\n",
            "        if self.data_spec.expects_bgr:\n",
            "            # Convert from RGB channel ordering to BGR\n",
            "            # This matches, for instance, how OpenCV orders the channels.\n",
            "            img = tf.reverse(img, [False, False, True])\n",
            "        return img\n",
            "\n",
            "    def process(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\dataset.py",
        "line_number": 149,
        "API": ".lower(",
        "context": [
            "    @staticmethod\n",
            "    def create_extension_mask(paths):\n",
            "\n",
            "        def is_jpeg(path):\n",
            "            extension = osp.splitext(path)[-1].lower()\n",
            "            if extension in ('.jpg', '.jpeg'):\n",
            "                return True\n",
            "            if extension != '.png':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\dataset.py",
        "line_number": 168,
        "API": ".split(",
        "context": [
            "    def __init__(self, val_path, data_path, data_spec):\n",
            "        # Read in the ground truth labels for the validation set\n",
            "        # The get_ilsvrc_aux.sh in Caffe's data/ilsvrc12 folder can fetch a copy of val.txt\n",
            "        gt_lines = open(val_path).readlines()\n",
            "        gt_pairs = [line.split() for line in gt_lines]\n",
            "        # Get the full image paths\n",
            "        # You will need a copy of the ImageNet validation set for this.\n",
            "        image_paths = [osp.join(data_path, p[0]) for p in gt_pairs]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\dataset.py",
        "line_number": 173,
        "API": ".array(",
        "context": [
            "        # Get the full image paths\n",
            "        # You will need a copy of the ImageNet validation set for this.\n",
            "        image_paths = [osp.join(data_path, p[0]) for p in gt_pairs]\n",
            "        # The corresponding ground truth labels\n",
            "        labels = np.array([int(p[1]) for p in gt_pairs])\n",
            "        # Initialize base\n",
            "        super(ImageNetProducer, self).__init__(image_paths=image_paths,\n",
            "                                               data_spec=data_spec,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\validate.py",
        "line_number": 23,
        "API": ".format(",
        "context": [
            "    if name not in lut:\n",
            "        print('Invalid model index. Options are:')\n",
            "        # Display a list of valid model names\n",
            "        for model in all_models:\n",
            "            print('\\t* {}'.format(model.__name__))\n",
            "        return None\n",
            "    NetClass = lut[name]\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\validate.py",
        "line_number": 29,
        "API": ".placeholder(",
        "context": [
            "    NetClass = lut[name]\n",
            "\n",
            "    # Create a placeholder for the input image\n",
            "    spec = models.get_data_spec(model_class=NetClass)\n",
            "    data_node = tf.placeholder(tf.float32,\n",
            "                               shape=(None, spec.crop_size, spec.crop_size, spec.channels))\n",
            "\n",
            "    # Construct and return the model\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\validate.py",
        "line_number": 43,
        "API": ".placeholder(",
        "context": [
            "    spec = models.get_data_spec(model_instance=net)\n",
            "    # Get the input node for feeding in the images\n",
            "    input_node = net.inputs['data']\n",
            "    # Create a placeholder for the ground truth labels\n",
            "    label_node = tf.placeholder(tf.int32)\n",
            "    # Get the output of the network (class probabilities)\n",
            "    probs = net.get_output()\n",
            "    # Create a top_k accuracy node\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\validate.py",
        "line_number": 58,
        "API": ".load(",
        "context": [
            "\n",
            "    with tf.Session() as sesh:\n",
            "        coordinator = tf.train.Coordinator()\n",
            "        # Load the converted parameters\n",
            "        net.load(data_path=model_path, session=sesh)\n",
            "        # Start the image processing workers\n",
            "        threads = image_producer.start(session=sesh, coordinator=coordinator)\n",
            "        # Iterate over and classify mini-batches\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\validate.py",
        "line_number": 63,
        "API": ".sum(",
        "context": [
            "        # Start the image processing workers\n",
            "        threads = image_producer.start(session=sesh, coordinator=coordinator)\n",
            "        # Iterate over and classify mini-batches\n",
            "        for (labels, images) in image_producer.batches(sesh):\n",
            "            correct += np.sum(sesh.run(top_k_op,\n",
            "                                       feed_dict={input_node: images,\n",
            "                                                  label_node: labels}))\n",
            "            count += len(labels)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\validate.py",
        "line_number": 68,
        "API": ".format(",
        "context": [
            "                                       feed_dict={input_node: images,\n",
            "                                                  label_node: labels}))\n",
            "            count += len(labels)\n",
            "            cur_accuracy = float(correct) * 100 / count\n",
            "            print('{:>6}/{:<6} {:>6.2f}%'.format(count, total, cur_accuracy))\n",
            "        # Stop the worker threads\n",
            "        coordinator.request_stop()\n",
            "        coordinator.join(threads, stop_grace_period_secs=2)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\alexnet.py",
        "line_number": 18,
        "API": ".softmax(",
        "context": [
            "             .max_pool(3, 3, 2, 2, padding='VALID', name='pool5')\n",
            "             .fc(4096, name='fc6')\n",
            "             .fc(4096, name='fc7')\n",
            "             .fc(1000, relu=False, name='fc8')\n",
            "             .softmax(name='prob'))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\googlenet.py",
        "line_number": 30,
        "API": ".concat(",
        "context": [
            "        (self.feed('inception_3a_1x1',\n",
            "                   'inception_3a_3x3',\n",
            "                   'inception_3a_5x5',\n",
            "                   'inception_3a_pool_proj')\n",
            "             .concat(3, name='inception_3a_output')\n",
            "             .conv(1, 1, 128, 1, 1, name='inception_3b_1x1'))\n",
            "\n",
            "        (self.feed('inception_3a_output')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\googlenet.py",
        "line_number": 49,
        "API": ".concat(",
        "context": [
            "        (self.feed('inception_3b_1x1',\n",
            "                   'inception_3b_3x3',\n",
            "                   'inception_3b_5x5',\n",
            "                   'inception_3b_pool_proj')\n",
            "             .concat(3, name='inception_3b_output')\n",
            "             .max_pool(3, 3, 2, 2, name='pool3_3x3_s2')\n",
            "             .conv(1, 1, 192, 1, 1, name='inception_4a_1x1'))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\googlenet.py",
        "line_number": 69,
        "API": ".concat(",
        "context": [
            "        (self.feed('inception_4a_1x1',\n",
            "                   'inception_4a_3x3',\n",
            "                   'inception_4a_5x5',\n",
            "                   'inception_4a_pool_proj')\n",
            "             .concat(3, name='inception_4a_output')\n",
            "             .conv(1, 1, 160, 1, 1, name='inception_4b_1x1'))\n",
            "\n",
            "        (self.feed('inception_4a_output')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\googlenet.py",
        "line_number": 88,
        "API": ".concat(",
        "context": [
            "        (self.feed('inception_4b_1x1',\n",
            "                   'inception_4b_3x3',\n",
            "                   'inception_4b_5x5',\n",
            "                   'inception_4b_pool_proj')\n",
            "             .concat(3, name='inception_4b_output')\n",
            "             .conv(1, 1, 128, 1, 1, name='inception_4c_1x1'))\n",
            "\n",
            "        (self.feed('inception_4b_output')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\googlenet.py",
        "line_number": 107,
        "API": ".concat(",
        "context": [
            "        (self.feed('inception_4c_1x1',\n",
            "                   'inception_4c_3x3',\n",
            "                   'inception_4c_5x5',\n",
            "                   'inception_4c_pool_proj')\n",
            "             .concat(3, name='inception_4c_output')\n",
            "             .conv(1, 1, 112, 1, 1, name='inception_4d_1x1'))\n",
            "\n",
            "        (self.feed('inception_4c_output')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\googlenet.py",
        "line_number": 126,
        "API": ".concat(",
        "context": [
            "        (self.feed('inception_4d_1x1',\n",
            "                   'inception_4d_3x3',\n",
            "                   'inception_4d_5x5',\n",
            "                   'inception_4d_pool_proj')\n",
            "             .concat(3, name='inception_4d_output')\n",
            "             .conv(1, 1, 256, 1, 1, name='inception_4e_1x1'))\n",
            "\n",
            "        (self.feed('inception_4d_output')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\googlenet.py",
        "line_number": 145,
        "API": ".concat(",
        "context": [
            "        (self.feed('inception_4e_1x1',\n",
            "                   'inception_4e_3x3',\n",
            "                   'inception_4e_5x5',\n",
            "                   'inception_4e_pool_proj')\n",
            "             .concat(3, name='inception_4e_output')\n",
            "             .max_pool(3, 3, 2, 2, name='pool4_3x3_s2')\n",
            "             .conv(1, 1, 256, 1, 1, name='inception_5a_1x1'))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\googlenet.py",
        "line_number": 165,
        "API": ".concat(",
        "context": [
            "        (self.feed('inception_5a_1x1',\n",
            "                   'inception_5a_3x3',\n",
            "                   'inception_5a_5x5',\n",
            "                   'inception_5a_pool_proj')\n",
            "             .concat(3, name='inception_5a_output')\n",
            "             .conv(1, 1, 384, 1, 1, name='inception_5b_1x1'))\n",
            "\n",
            "        (self.feed('inception_5a_output')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\googlenet.py",
        "line_number": 184,
        "API": ".concat(",
        "context": [
            "        (self.feed('inception_5b_1x1',\n",
            "                   'inception_5b_3x3',\n",
            "                   'inception_5b_5x5',\n",
            "                   'inception_5b_pool_proj')\n",
            "             .concat(3, name='inception_5b_output')\n",
            "             .avg_pool(7, 7, 1, 1, padding='VALID', name='pool5_7x7_s1')\n",
            "             .fc(1000, relu=False, name='loss3_classifier')\n",
            "             .softmax(name='prob'))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\helper.py",
        "line_number": 5,
        "API": ".join(",
        "context": [
            "import os.path as osp\n",
            "import numpy as np\n",
            "\n",
            "# Add the kaffe module to the import path\n",
            "sys.path.append(osp.realpath(osp.join(osp.dirname(__file__), '../../../')))\n",
            "\n",
            "from googlenet import GoogleNet\n",
            "from vgg import VGG16\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\helper.py",
        "line_number": 40,
        "API": ".array(",
        "context": [
            "        # The mean to be subtracted from each image. By default, the per-channel ImageNet mean.\n",
            "        # The values below are ordered BGR, as many Caffe models are trained in this order.\n",
            "        # Some of the earlier models (like AlexNet) used a spatial three-channeled mean.\n",
            "        # However, using just the per-channel mean values instead doesn't affect things too much.\n",
            "        self.mean = mean if mean is not None else np.array([104., 117., 124.])\n",
            "        # Whether this model expects images to be in BGR order\n",
            "        self.expects_bgr = True\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\nin.py",
        "line_number": 21,
        "API": ".softmax(",
        "context": [
            "             .conv(3, 3, 1024, 1, 1, name='conv4-1024')\n",
            "             .conv(1, 1, 1024, 1, 1, name='cccp7-1024')\n",
            "             .conv(1, 1, 1000, 1, 1, name='cccp8-1024')\n",
            "             .avg_pool(6, 6, 1, 1, padding='VALID', name='pool4')\n",
            "             .softmax(name='prob'))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 21,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn2a_branch2c'))\n",
            "\n",
            "        (self.feed('bn2a_branch1',\n",
            "                   'bn2a_branch2c')\n",
            "             .add(name='res2a')\n",
            "             .relu(name='res2a_relu')\n",
            "             .conv(1, 1, 64, 1, 1, biased=False, relu=False, name='res2b_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn2b_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 32,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn2b_branch2c'))\n",
            "\n",
            "        (self.feed('res2a_relu',\n",
            "                   'bn2b_branch2c')\n",
            "             .add(name='res2b')\n",
            "             .relu(name='res2b_relu')\n",
            "             .conv(1, 1, 64, 1, 1, biased=False, relu=False, name='res2c_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn2c_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 43,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn2c_branch2c'))\n",
            "\n",
            "        (self.feed('res2b_relu',\n",
            "                   'bn2c_branch2c')\n",
            "             .add(name='res2c')\n",
            "             .relu(name='res2c_relu')\n",
            "             .conv(1, 1, 512, 2, 2, biased=False, relu=False, name='res3a_branch1')\n",
            "             .batch_normalization(name='bn3a_branch1'))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 58,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn3a_branch2c'))\n",
            "\n",
            "        (self.feed('bn3a_branch1',\n",
            "                   'bn3a_branch2c')\n",
            "             .add(name='res3a')\n",
            "             .relu(name='res3a_relu')\n",
            "             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3b_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn3b_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 69,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn3b_branch2c'))\n",
            "\n",
            "        (self.feed('res3a_relu',\n",
            "                   'bn3b_branch2c')\n",
            "             .add(name='res3b')\n",
            "             .relu(name='res3b_relu')\n",
            "             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3c_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn3c_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 80,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn3c_branch2c'))\n",
            "\n",
            "        (self.feed('res3b_relu',\n",
            "                   'bn3c_branch2c')\n",
            "             .add(name='res3c')\n",
            "             .relu(name='res3c_relu')\n",
            "             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3d_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn3d_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 91,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn3d_branch2c'))\n",
            "\n",
            "        (self.feed('res3c_relu',\n",
            "                   'bn3d_branch2c')\n",
            "             .add(name='res3d')\n",
            "             .relu(name='res3d_relu')\n",
            "             .conv(1, 1, 1024, 2, 2, biased=False, relu=False, name='res4a_branch1')\n",
            "             .batch_normalization(name='bn4a_branch1'))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 106,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4a_branch2c'))\n",
            "\n",
            "        (self.feed('bn4a_branch1',\n",
            "                   'bn4a_branch2c')\n",
            "             .add(name='res4a')\n",
            "             .relu(name='res4a_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 117,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b_branch2c'))\n",
            "\n",
            "        (self.feed('res4a_relu',\n",
            "                   'bn4b_branch2c')\n",
            "             .add(name='res4b')\n",
            "             .relu(name='res4b_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4c_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4c_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 128,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4c_branch2c'))\n",
            "\n",
            "        (self.feed('res4b_relu',\n",
            "                   'bn4c_branch2c')\n",
            "             .add(name='res4c')\n",
            "             .relu(name='res4c_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4d_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4d_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 139,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4d_branch2c'))\n",
            "\n",
            "        (self.feed('res4c_relu',\n",
            "                   'bn4d_branch2c')\n",
            "             .add(name='res4d')\n",
            "             .relu(name='res4d_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4e_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4e_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 150,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4e_branch2c'))\n",
            "\n",
            "        (self.feed('res4d_relu',\n",
            "                   'bn4e_branch2c')\n",
            "             .add(name='res4e')\n",
            "             .relu(name='res4e_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4f_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4f_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 161,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4f_branch2c'))\n",
            "\n",
            "        (self.feed('res4e_relu',\n",
            "                   'bn4f_branch2c')\n",
            "             .add(name='res4f')\n",
            "             .relu(name='res4f_relu')\n",
            "             .conv(1, 1, 2048, 2, 2, biased=False, relu=False, name='res5a_branch1')\n",
            "             .batch_normalization(name='bn5a_branch1'))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 176,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn5a_branch2c'))\n",
            "\n",
            "        (self.feed('bn5a_branch1',\n",
            "                   'bn5a_branch2c')\n",
            "             .add(name='res5a')\n",
            "             .relu(name='res5a_relu')\n",
            "             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res5b_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn5b_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 187,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn5b_branch2c'))\n",
            "\n",
            "        (self.feed('res5a_relu',\n",
            "                   'bn5b_branch2c')\n",
            "             .add(name='res5b')\n",
            "             .relu(name='res5b_relu')\n",
            "             .conv(1, 1, 512, 1, 1, biased=False, relu=False, name='res5c_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn5c_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 198,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn5c_branch2c'))\n",
            "\n",
            "        (self.feed('res5b_relu',\n",
            "                   'bn5c_branch2c')\n",
            "             .add(name='res5c')\n",
            "             .relu(name='res5c_relu')\n",
            "             .avg_pool(7, 7, 1, 1, padding='VALID', name='pool5')\n",
            "             .fc(1000, relu=False, name='fc1000')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 261,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn3a_branch2c'))\n",
            "\n",
            "        (self.feed('bn3a_branch1',\n",
            "                   'bn3a_branch2c')\n",
            "             .add(name='res3a')\n",
            "             .relu(name='res3a_relu')\n",
            "             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3b1_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn3b1_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 272,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn3b1_branch2c'))\n",
            "\n",
            "        (self.feed('res3a_relu',\n",
            "                   'bn3b1_branch2c')\n",
            "             .add(name='res3b1')\n",
            "             .relu(name='res3b1_relu')\n",
            "             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3b2_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn3b2_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 283,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn3b2_branch2c'))\n",
            "\n",
            "        (self.feed('res3b1_relu',\n",
            "                   'bn3b2_branch2c')\n",
            "             .add(name='res3b2')\n",
            "             .relu(name='res3b2_relu')\n",
            "             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3b3_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn3b3_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 294,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn3b3_branch2c'))\n",
            "\n",
            "        (self.feed('res3b2_relu',\n",
            "                   'bn3b3_branch2c')\n",
            "             .add(name='res3b3')\n",
            "             .relu(name='res3b3_relu')\n",
            "             .conv(1, 1, 1024, 2, 2, biased=False, relu=False, name='res4a_branch1')\n",
            "             .batch_normalization(name='bn4a_branch1'))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 309,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4a_branch2c'))\n",
            "\n",
            "        (self.feed('bn4a_branch1',\n",
            "                   'bn4a_branch2c')\n",
            "             .add(name='res4a')\n",
            "             .relu(name='res4a_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b1_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b1_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 320,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b1_branch2c'))\n",
            "\n",
            "        (self.feed('res4a_relu',\n",
            "                   'bn4b1_branch2c')\n",
            "             .add(name='res4b1')\n",
            "             .relu(name='res4b1_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b2_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b2_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 331,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b2_branch2c'))\n",
            "\n",
            "        (self.feed('res4b1_relu',\n",
            "                   'bn4b2_branch2c')\n",
            "             .add(name='res4b2')\n",
            "             .relu(name='res4b2_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b3_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b3_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 342,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b3_branch2c'))\n",
            "\n",
            "        (self.feed('res4b2_relu',\n",
            "                   'bn4b3_branch2c')\n",
            "             .add(name='res4b3')\n",
            "             .relu(name='res4b3_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b4_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b4_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 353,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b4_branch2c'))\n",
            "\n",
            "        (self.feed('res4b3_relu',\n",
            "                   'bn4b4_branch2c')\n",
            "             .add(name='res4b4')\n",
            "             .relu(name='res4b4_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b5_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b5_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 364,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b5_branch2c'))\n",
            "\n",
            "        (self.feed('res4b4_relu',\n",
            "                   'bn4b5_branch2c')\n",
            "             .add(name='res4b5')\n",
            "             .relu(name='res4b5_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b6_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b6_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 375,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b6_branch2c'))\n",
            "\n",
            "        (self.feed('res4b5_relu',\n",
            "                   'bn4b6_branch2c')\n",
            "             .add(name='res4b6')\n",
            "             .relu(name='res4b6_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b7_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b7_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 386,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b7_branch2c'))\n",
            "\n",
            "        (self.feed('res4b6_relu',\n",
            "                   'bn4b7_branch2c')\n",
            "             .add(name='res4b7')\n",
            "             .relu(name='res4b7_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b8_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b8_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 397,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b8_branch2c'))\n",
            "\n",
            "        (self.feed('res4b7_relu',\n",
            "                   'bn4b8_branch2c')\n",
            "             .add(name='res4b8')\n",
            "             .relu(name='res4b8_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b9_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b9_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 408,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b9_branch2c'))\n",
            "\n",
            "        (self.feed('res4b8_relu',\n",
            "                   'bn4b9_branch2c')\n",
            "             .add(name='res4b9')\n",
            "             .relu(name='res4b9_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b10_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b10_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 419,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b10_branch2c'))\n",
            "\n",
            "        (self.feed('res4b9_relu',\n",
            "                   'bn4b10_branch2c')\n",
            "             .add(name='res4b10')\n",
            "             .relu(name='res4b10_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b11_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b11_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 430,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b11_branch2c'))\n",
            "\n",
            "        (self.feed('res4b10_relu',\n",
            "                   'bn4b11_branch2c')\n",
            "             .add(name='res4b11')\n",
            "             .relu(name='res4b11_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b12_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b12_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 441,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b12_branch2c'))\n",
            "\n",
            "        (self.feed('res4b11_relu',\n",
            "                   'bn4b12_branch2c')\n",
            "             .add(name='res4b12')\n",
            "             .relu(name='res4b12_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b13_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b13_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 452,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b13_branch2c'))\n",
            "\n",
            "        (self.feed('res4b12_relu',\n",
            "                   'bn4b13_branch2c')\n",
            "             .add(name='res4b13')\n",
            "             .relu(name='res4b13_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b14_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b14_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 463,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b14_branch2c'))\n",
            "\n",
            "        (self.feed('res4b13_relu',\n",
            "                   'bn4b14_branch2c')\n",
            "             .add(name='res4b14')\n",
            "             .relu(name='res4b14_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b15_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b15_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 474,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b15_branch2c'))\n",
            "\n",
            "        (self.feed('res4b14_relu',\n",
            "                   'bn4b15_branch2c')\n",
            "             .add(name='res4b15')\n",
            "             .relu(name='res4b15_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b16_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b16_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 485,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b16_branch2c'))\n",
            "\n",
            "        (self.feed('res4b15_relu',\n",
            "                   'bn4b16_branch2c')\n",
            "             .add(name='res4b16')\n",
            "             .relu(name='res4b16_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b17_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b17_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 496,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b17_branch2c'))\n",
            "\n",
            "        (self.feed('res4b16_relu',\n",
            "                   'bn4b17_branch2c')\n",
            "             .add(name='res4b17')\n",
            "             .relu(name='res4b17_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b18_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b18_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 507,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b18_branch2c'))\n",
            "\n",
            "        (self.feed('res4b17_relu',\n",
            "                   'bn4b18_branch2c')\n",
            "             .add(name='res4b18')\n",
            "             .relu(name='res4b18_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b19_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b19_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 518,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b19_branch2c'))\n",
            "\n",
            "        (self.feed('res4b18_relu',\n",
            "                   'bn4b19_branch2c')\n",
            "             .add(name='res4b19')\n",
            "             .relu(name='res4b19_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b20_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b20_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 529,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b20_branch2c'))\n",
            "\n",
            "        (self.feed('res4b19_relu',\n",
            "                   'bn4b20_branch2c')\n",
            "             .add(name='res4b20')\n",
            "             .relu(name='res4b20_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b21_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b21_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 540,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b21_branch2c'))\n",
            "\n",
            "        (self.feed('res4b20_relu',\n",
            "                   'bn4b21_branch2c')\n",
            "             .add(name='res4b21')\n",
            "             .relu(name='res4b21_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b22_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b22_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 551,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b22_branch2c'))\n",
            "\n",
            "        (self.feed('res4b21_relu',\n",
            "                   'bn4b22_branch2c')\n",
            "             .add(name='res4b22')\n",
            "             .relu(name='res4b22_relu')\n",
            "             .conv(1, 1, 2048, 2, 2, biased=False, relu=False, name='res5a_branch1')\n",
            "             .batch_normalization(name='bn5a_branch1'))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 684,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn3b3_branch2c'))\n",
            "\n",
            "        (self.feed('res3b2_relu',\n",
            "                   'bn3b3_branch2c')\n",
            "             .add(name='res3b3')\n",
            "             .relu(name='res3b3_relu')\n",
            "             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3b4_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn3b4_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 695,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn3b4_branch2c'))\n",
            "\n",
            "        (self.feed('res3b3_relu',\n",
            "                   'bn3b4_branch2c')\n",
            "             .add(name='res3b4')\n",
            "             .relu(name='res3b4_relu')\n",
            "             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3b5_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn3b5_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 706,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn3b5_branch2c'))\n",
            "\n",
            "        (self.feed('res3b4_relu',\n",
            "                   'bn3b5_branch2c')\n",
            "             .add(name='res3b5')\n",
            "             .relu(name='res3b5_relu')\n",
            "             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3b6_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn3b6_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 717,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn3b6_branch2c'))\n",
            "\n",
            "        (self.feed('res3b5_relu',\n",
            "                   'bn3b6_branch2c')\n",
            "             .add(name='res3b6')\n",
            "             .relu(name='res3b6_relu')\n",
            "             .conv(1, 1, 128, 1, 1, biased=False, relu=False, name='res3b7_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn3b7_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 728,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn3b7_branch2c'))\n",
            "\n",
            "        (self.feed('res3b6_relu',\n",
            "                   'bn3b7_branch2c')\n",
            "             .add(name='res3b7')\n",
            "             .relu(name='res3b7_relu')\n",
            "             .conv(1, 1, 1024, 2, 2, biased=False, relu=False, name='res4a_branch1')\n",
            "             .batch_normalization(name='bn4a_branch1'))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 985,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b22_branch2c'))\n",
            "\n",
            "        (self.feed('res4b21_relu',\n",
            "                   'bn4b22_branch2c')\n",
            "             .add(name='res4b22')\n",
            "             .relu(name='res4b22_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b23_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b23_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 996,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b23_branch2c'))\n",
            "\n",
            "        (self.feed('res4b22_relu',\n",
            "                   'bn4b23_branch2c')\n",
            "             .add(name='res4b23')\n",
            "             .relu(name='res4b23_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b24_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b24_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 1007,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b24_branch2c'))\n",
            "\n",
            "        (self.feed('res4b23_relu',\n",
            "                   'bn4b24_branch2c')\n",
            "             .add(name='res4b24')\n",
            "             .relu(name='res4b24_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b25_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b25_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 1018,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b25_branch2c'))\n",
            "\n",
            "        (self.feed('res4b24_relu',\n",
            "                   'bn4b25_branch2c')\n",
            "             .add(name='res4b25')\n",
            "             .relu(name='res4b25_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b26_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b26_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 1029,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b26_branch2c'))\n",
            "\n",
            "        (self.feed('res4b25_relu',\n",
            "                   'bn4b26_branch2c')\n",
            "             .add(name='res4b26')\n",
            "             .relu(name='res4b26_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b27_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b27_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 1040,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b27_branch2c'))\n",
            "\n",
            "        (self.feed('res4b26_relu',\n",
            "                   'bn4b27_branch2c')\n",
            "             .add(name='res4b27')\n",
            "             .relu(name='res4b27_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b28_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b28_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 1051,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b28_branch2c'))\n",
            "\n",
            "        (self.feed('res4b27_relu',\n",
            "                   'bn4b28_branch2c')\n",
            "             .add(name='res4b28')\n",
            "             .relu(name='res4b28_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b29_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b29_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 1062,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b29_branch2c'))\n",
            "\n",
            "        (self.feed('res4b28_relu',\n",
            "                   'bn4b29_branch2c')\n",
            "             .add(name='res4b29')\n",
            "             .relu(name='res4b29_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b30_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b30_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 1073,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b30_branch2c'))\n",
            "\n",
            "        (self.feed('res4b29_relu',\n",
            "                   'bn4b30_branch2c')\n",
            "             .add(name='res4b30')\n",
            "             .relu(name='res4b30_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b31_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b31_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 1084,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b31_branch2c'))\n",
            "\n",
            "        (self.feed('res4b30_relu',\n",
            "                   'bn4b31_branch2c')\n",
            "             .add(name='res4b31')\n",
            "             .relu(name='res4b31_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b32_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b32_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 1095,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b32_branch2c'))\n",
            "\n",
            "        (self.feed('res4b31_relu',\n",
            "                   'bn4b32_branch2c')\n",
            "             .add(name='res4b32')\n",
            "             .relu(name='res4b32_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b33_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b33_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 1106,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b33_branch2c'))\n",
            "\n",
            "        (self.feed('res4b32_relu',\n",
            "                   'bn4b33_branch2c')\n",
            "             .add(name='res4b33')\n",
            "             .relu(name='res4b33_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b34_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b34_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 1117,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b34_branch2c'))\n",
            "\n",
            "        (self.feed('res4b33_relu',\n",
            "                   'bn4b34_branch2c')\n",
            "             .add(name='res4b34')\n",
            "             .relu(name='res4b34_relu')\n",
            "             .conv(1, 1, 256, 1, 1, biased=False, relu=False, name='res4b35_branch2a')\n",
            "             .batch_normalization(relu=True, name='bn4b35_branch2a')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\resnet.py",
        "line_number": 1128,
        "API": ".add(",
        "context": [
            "             .batch_normalization(name='bn4b35_branch2c'))\n",
            "\n",
            "        (self.feed('res4b34_relu',\n",
            "                   'bn4b35_branch2c')\n",
            "             .add(name='res4b35')\n",
            "             .relu(name='res4b35_relu')\n",
            "             .conv(1, 1, 2048, 2, 2, biased=False, relu=False, name='res5a_branch1')\n",
            "             .batch_normalization(name='bn5a_branch1'))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\imagenet\\models\\vgg.py",
        "line_number": 26,
        "API": ".softmax(",
        "context": [
            "             .max_pool(2, 2, 2, 2, name='pool5')\n",
            "             .fc(4096, name='fc6')\n",
            "             .fc(4096, name='fc7')\n",
            "             .fc(1000, relu=False, name='fc8')\n",
            "             .softmax(name='prob'))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\mnist\\finetune_mnist.py",
        "line_number": 16,
        "API": ".reshape(",
        "context": [
            "    while True:\n",
            "        indices = range(len(source.images))\n",
            "        random.shuffle(indices)\n",
            "        for i in indices:\n",
            "            image = np.reshape(source.images[i], (28, 28, 1))\n",
            "            label = source.labels[i]\n",
            "            yield image, label\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\mnist\\finetune_mnist.py",
        "line_number": 29,
        "API": ".array(",
        "context": [
            "        for _ in range(batch_size):\n",
            "            image, label = next(data_gen)\n",
            "            image_batch.append(image)\n",
            "            label_batch.append(label)\n",
            "        yield np.array(image_batch), np.array(label_batch)\n",
            "\n",
            "\n",
            "images = tf.placeholder(tf.float32, [batch_size, 28, 28, 1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\mnist\\finetune_mnist.py",
        "line_number": 37,
        "API": ".softmax(",
        "context": [
            "labels = tf.placeholder(tf.float32, [batch_size, 10])\n",
            "net = MyNet({'data': images})\n",
            "\n",
            "ip2 = net.layers['ip2']\n",
            "pred = tf.nn.softmax(ip2)\n",
            "\n",
            "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(ip2, labels), 0)\n",
            "opt = tf.train.RMSPropOptimizer(0.001)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\examples\\mnist\\finetune_mnist.py",
        "line_number": 46,
        "API": ".load(",
        "context": [
            "\n",
            "with tf.Session() as sess:\n",
            "    # Load the data\n",
            "    sess.run(tf.initialize_all_variables())\n",
            "    net.load('mynet.npy', sess)\n",
            "\n",
            "    data_gen = gen_data_batch(mnist.train)\n",
            "    for i in range(1000):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\errors.py",
        "line_number": 6,
        "API": ".write(",
        "context": [
            "class KaffeError(Exception):\n",
            "    pass\n",
            "\n",
            "def print_stderr(msg):\n",
            "    sys.stderr.write('%s\\n' % msg)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\graph.py",
        "line_number": 84,
        "API": ".add(",
        "context": [
            "            if node in temp_marked:\n",
            "                raise KaffeError('Graph is not a DAG.')\n",
            "            if node in perm_marked:\n",
            "                return\n",
            "            temp_marked.add(node)\n",
            "            for child in node.children:\n",
            "                visit(child)\n",
            "            perm_marked.add(node)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\graph.py",
        "line_number": 108,
        "API": ".format(",
        "context": [
            "        graph = self\n",
            "        for transformer in transformers:\n",
            "            graph = transformer(graph)\n",
            "            if graph is None:\n",
            "                raise KaffeError('Transformer failed: {}'.format(transformer))\n",
            "            assert isinstance(graph, Graph)\n",
            "        return graph\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\graph.py",
        "line_number": 116,
        "API": ".format(",
        "context": [
            "    def __contains__(self, key):\n",
            "        return key in self.node_lut\n",
            "\n",
            "    def __str__(self):\n",
            "        hdr = '{:<20} {:<30} {:>20} {:>20}'.format('Type', 'Name', 'Param', 'Output')\n",
            "        s = [hdr, '-' * 94]\n",
            "        for node in self.topologically_sorted():\n",
            "            # If the node has learned parameters, display the first one's shape.\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\graph.py",
        "line_number": 123,
        "API": ".format(",
        "context": [
            "            # If the node has learned parameters, display the first one's shape.\n",
            "            # In case of convolutions, this corresponds to the weights.\n",
            "            data_shape = node.data[0].shape if node.data else '--'\n",
            "            out_shape = node.output_shape or '--'\n",
            "            s.append('{:<20} {:<30} {:>20} {:>20}'.format(node.kind, node.name, data_shape,\n",
            "                                                          tuple(out_shape)))\n",
            "        return '\\n'.join(s)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\graph.py",
        "line_number": 139,
        "API": ".load(",
        "context": [
            "        phase: Either 'test' or 'train'. Used for filtering phase-specific nodes.\n",
            "        '''\n",
            "        self.def_path = def_path\n",
            "        self.phase = phase\n",
            "        self.load()\n",
            "\n",
            "    def load(self):\n",
            "        '''Load the layer definitions from the prototxt.'''\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\graph.py",
        "line_number": 168,
        "API": ".add(",
        "context": [
            "            if not exclude:\n",
            "                filtered_layers.append(layer)\n",
            "                # Guard against dupes.\n",
            "                assert layer.name not in filtered_layer_names\n",
            "                filtered_layer_names.add(layer.name)\n",
            "        return filtered_layers\n",
            "\n",
            "    def make_node(self, layer):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\graph.py",
        "line_number": 230,
        "API": ".get(",
        "context": [
            "        for layer in layers:\n",
            "            node = graph.get_node(layer.name)\n",
            "            for input_name in layer.bottom:\n",
            "                assert input_name != layer.name\n",
            "                parent_node = node_outputs.get(input_name)\n",
            "                if (parent_node is None) or (parent_node == node):\n",
            "                    parent_node = graph.get_node(input_name)\n",
            "                node.add_parent(parent_node)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\layers.py",
        "line_number": 85,
        "API": ".lower(",
        "context": [
            "    @staticmethod\n",
            "    def get_handler_name(node_kind):\n",
            "        if len(node_kind) <= 4:\n",
            "            # A catch-all for things like ReLU and tanh\n",
            "            return node_kind.lower()\n",
            "        # Convert from CamelCase to under_scored\n",
            "        name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', node_kind)\n",
            "        return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name).lower()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\layers.py",
        "line_number": 92,
        "API": ".join(",
        "context": [
            "        return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name).lower()\n",
            "\n",
            "    def get_handler(self, node_kind, prefix):\n",
            "        name = self.get_handler_name(node_kind)\n",
            "        name = '_'.join((prefix, name))\n",
            "        try:\n",
            "            return getattr(self, name)\n",
            "        except AttributeError:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\layers.py",
        "line_number": 109,
        "API": ".join(",
        "context": [
            "\n",
            "    @property\n",
            "    def parameters(self):\n",
            "        name = NodeDispatch.get_handler_name(self.kind)\n",
            "        name = '_'.join((name, 'param'))\n",
            "        try:\n",
            "            return getattr(self.layer, name)\n",
            "        except AttributeError:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\transformers.py",
        "line_number": 28,
        "API": ".load(",
        "context": [
            "        self.did_use_pb = False\n",
            "        # A list containing (layer name, parameters) tuples\n",
            "        self.params = None\n",
            "        # Load the parameters\n",
            "        self.load()\n",
            "\n",
            "    def load(self):\n",
            "        if has_pycaffe():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\transformers.py",
        "line_number": 61,
        "API": ".reshape(",
        "context": [
            "                c_o = blob.num\n",
            "                c_i = blob.channels\n",
            "                h = blob.height\n",
            "                w = blob.width\n",
            "            data = np.array(blob.data, dtype=np.float32).reshape(c_o, c_i, h, w)\n",
            "            transformed.append(data)\n",
            "        return transformed\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\transformers.py",
        "line_number": 78,
        "API": ".squeeze(",
        "context": [
            "        squeeze_indices = [1]  # Squeeze biases.\n",
            "        if node.kind == NodeKind.InnerProduct:\n",
            "            squeeze_indices.append(0)  # Squeeze FC.\n",
            "        for idx in squeeze_indices:\n",
            "            data[idx] = np.squeeze(data[idx])\n",
            "        return data\n",
            "\n",
            "    def __call__(self, graph):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\transformers.py",
        "line_number": 114,
        "API": ".format(",
        "context": [
            "    def map(self, node_kind):\n",
            "        try:\n",
            "            return self.mapping[node_kind]\n",
            "        except KeyError:\n",
            "            raise KaffeError('Ordering not found for node kind: {}'.format(node_kind))\n",
            "\n",
            "    def __call__(self, graph):\n",
            "        for node in graph.nodes:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\transformers.py",
        "line_number": 123,
        "API": ".format(",
        "context": [
            "                continue\n",
            "            if node.kind not in self.reshaped_node_types:\n",
            "                # Check for 2+ dimensional data\n",
            "                if any(len(tensor.shape) > 1 for tensor in node.data):\n",
            "                    print_stderr('Warning: parmaters not reshaped for node: {}'.format(node))\n",
            "                continue\n",
            "            transpose_order = self.map(node.kind)\n",
            "            weights = node.data[0]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\transformers.py",
        "line_number": 133,
        "API": ".reshape(",
        "context": [
            "                # re-wired to match the new spatial ordering.\n",
            "                in_shape = node.get_only_parent().output_shape\n",
            "                fc_shape = weights.shape\n",
            "                output_channels = fc_shape[0]\n",
            "                weights = weights.reshape((output_channels, in_shape.channels, in_shape.height,\n",
            "                                           in_shape.width))\n",
            "                weights = weights.transpose(self.map(NodeKind.Convolution))\n",
            "                node.reshaped_data = weights.reshape(fc_shape[transpose_order[0]],\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\transformers.py",
        "line_number": 139,
        "API": ".transpose(",
        "context": [
            "                weights = weights.transpose(self.map(NodeKind.Convolution))\n",
            "                node.reshaped_data = weights.reshape(fc_shape[transpose_order[0]],\n",
            "                                                     fc_shape[transpose_order[1]])\n",
            "            else:\n",
            "                node.reshaped_data = weights.transpose(transpose_order)\n",
            "\n",
            "        if self.replace:\n",
            "            for node in graph.nodes:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\transformers.py",
        "line_number": 177,
        "API": ".merge(",
        "context": [
            "            # Disconnect the fused node from the graph.\n",
            "            parent.children.remove(node)\n",
            "            fused_nodes.append(node)\n",
            "            # Let the sub-class merge the fused node in any arbitrary way.\n",
            "            self.merge(parent, node)\n",
            "        transformed_nodes = [node for node in nodes if node not in fused_nodes]\n",
            "        return graph.replaced(transformed_nodes)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\transformers.py",
        "line_number": 285,
        "API": ".format(",
        "context": [
            "                names = ('mean', 'variance')\n",
            "                if len(node.data) == 4:\n",
            "                    names += ('scale', 'offset')\n",
            "            else:\n",
            "                print_stderr('WARNING: Unhandled parameters: {}'.format(node.kind))\n",
            "                continue\n",
            "            assert len(names) == len(node.data)\n",
            "            node.data = dict(zip(names, node.data))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\caffe\\resolver.py",
        "line_number": 47,
        "API": ".write(",
        "context": [
            "    * This backend is UNTESTED!\n",
            "------------------------------------------------------------\n",
            "\n",
            "'''\n",
            "    sys.stderr.write(msg)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\network.py",
        "line_number": 43,
        "API": ".constant(",
        "context": [
            "        self.layers = dict(inputs)\n",
            "        # If true, the resulting variables are set as trainable\n",
            "        self.trainable = trainable\n",
            "        # Switch variable for dropout\n",
            "        self.use_dropout = tf.placeholder_with_default(tf.constant(1.0),\n",
            "                                                       shape=[],\n",
            "                                                       name='use_dropout')\n",
            "        self.setup()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\network.py",
        "line_number": 58,
        "API": ".load(",
        "context": [
            "        data_path: The path to the numpy-serialized network weights\n",
            "        session: The current TensorFlow session\n",
            "        ignore_missing: If true, serialized weights for missing layers are ignored.\n",
            "        '''\n",
            "        data_dict = np.load(data_path).item()\n",
            "        for op_name in data_dict:\n",
            "            with tf.variable_scope(op_name, reuse=True):\n",
            "                for param_name, data in data_dict[op_name].iteritems():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\network.py",
        "line_number": 64,
        "API": ".assign(",
        "context": [
            "            with tf.variable_scope(op_name, reuse=True):\n",
            "                for param_name, data in data_dict[op_name].iteritems():\n",
            "                    try:\n",
            "                        var = tf.get_variable(param_name)\n",
            "                        session.run(var.assign(data))\n",
            "                    except ValueError:\n",
            "                        if not ignore_missing:\n",
            "                            raise\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\network.py",
        "line_number": 124,
        "API": ".conv2d(",
        "context": [
            "        # Verify that the grouping parameter is valid\n",
            "        assert c_i % group == 0\n",
            "        assert c_o % group == 0\n",
            "        # Convolution for a given input and kernel\n",
            "        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n",
            "        with tf.variable_scope(name) as scope:\n",
            "            kernel = self.make_var('weights', shape=[k_h, k_w, c_i / group, c_o])\n",
            "            if group == 1:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\network.py",
        "line_number": 132,
        "API": ".split(",
        "context": [
            "                # This is the common-case. Convolve the input without any further complications.\n",
            "                output = convolve(input, kernel)\n",
            "            else:\n",
            "                # Split the input into groups and then convolve each of them independently\n",
            "                input_groups = tf.split(3, group, input)\n",
            "                kernel_groups = tf.split(3, group, kernel)\n",
            "                output_groups = [convolve(i, k) for i, k in zip(input_groups, kernel_groups)]\n",
            "                # Concatenate the groups\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\network.py",
        "line_number": 143,
        "API": ".relu(",
        "context": [
            "                biases = self.make_var('biases', [c_o])\n",
            "                output = tf.nn.bias_add(output, biases)\n",
            "            if relu:\n",
            "                # ReLU non-linearity\n",
            "                output = tf.nn.relu(output, name=scope.name)\n",
            "            return output\n",
            "\n",
            "    @layer\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\network.py",
        "line_number": 148,
        "API": ".relu(",
        "context": [
            "            return output\n",
            "\n",
            "    @layer\n",
            "    def relu(self, input, name):\n",
            "        return tf.nn.relu(input, name=name)\n",
            "\n",
            "    @layer\n",
            "    def max_pool(self, input, k_h, k_w, s_h, s_w, name, padding=DEFAULT_PADDING):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\network.py",
        "line_number": 179,
        "API": ".concat(",
        "context": [
            "                                                  name=name)\n",
            "\n",
            "    @layer\n",
            "    def concat(self, inputs, axis, name):\n",
            "        return tf.concat(concat_dim=axis, values=inputs, name=name)\n",
            "\n",
            "    @layer\n",
            "    def add(self, inputs, name):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\network.py",
        "line_number": 194,
        "API": ".reshape(",
        "context": [
            "                # The input is spatial. Vectorize it first.\n",
            "                dim = 1\n",
            "                for d in input_shape[1:].as_list():\n",
            "                    dim *= d\n",
            "                feed_in = tf.reshape(input, [-1, dim])\n",
            "            else:\n",
            "                feed_in, dim = (input, input_shape[-1].value)\n",
            "            weights = self.make_var('weights', shape=[dim, num_out])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\network.py",
        "line_number": 211,
        "API": ".squeeze(",
        "context": [
            "            # For certain models (like NiN), the singleton spatial dimensions\n",
            "            # need to be explicitly squeezed, since they're not broadcast-able\n",
            "            # in TensorFlow's NHWC ordering (unlike Caffe's NCHW).\n",
            "            if input_shape[1] == 1 and input_shape[2] == 1:\n",
            "                input = tf.squeeze(input, squeeze_dims=[1, 2])\n",
            "            else:\n",
            "                raise ValueError('Rank 2 tensor input expected for softmax!')\n",
            "        return tf.nn.softmax(input, name=name)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\network.py",
        "line_number": 237,
        "API": ".relu(",
        "context": [
            "                # Get the actual eps from parameters\n",
            "                variance_epsilon=1e-5,\n",
            "                name=name)\n",
            "            if relu:\n",
            "                output = tf.nn.relu(output)\n",
            "            return output\n",
            "\n",
            "    @layer\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\network.py",
        "line_number": 243,
        "API": ".dropout(",
        "context": [
            "\n",
            "    @layer\n",
            "    def dropout(self, input, keep_prob, name):\n",
            "        keep = 1 - self.use_dropout + (self.use_dropout * keep_prob)\n",
            "        return tf.nn.dropout(input, keep, name=name)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\transformer.py",
        "line_number": 20,
        "API": ".ceil(",
        "context": [
            "    how the padding edge-cases are handled. These are described here:\n",
            "    https://github.com/Yangqing/caffe2/blob/master/caffe2/proto/caffe2_legacy.proto\n",
            "    '''\n",
            "    k_h, k_w, s_h, s_w, p_h, p_w = kernel_params\n",
            "    s_o_h = np.ceil(input_shape.height / float(s_h))\n",
            "    s_o_w = np.ceil(input_shape.width / float(s_w))\n",
            "    if (output_shape.height == s_o_h) and (output_shape.width == s_o_w):\n",
            "        return 'SAME'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\transformer.py",
        "line_number": 25,
        "API": ".ceil(",
        "context": [
            "    s_o_w = np.ceil(input_shape.width / float(s_w))\n",
            "    if (output_shape.height == s_o_h) and (output_shape.width == s_o_w):\n",
            "        return 'SAME'\n",
            "    v_o_h = np.ceil((input_shape.height - k_h + 1.0) / float(s_h))\n",
            "    v_o_w = np.ceil((input_shape.width - k_w + 1.0) / float(s_w))\n",
            "    if (output_shape.height == v_o_h) and (output_shape.width == v_o_w):\n",
            "        return 'VALID'\n",
            "    return None\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\transformer.py",
        "line_number": 50,
        "API": ".format(",
        "context": [
            "        return \"'%s'\" % arg if isinstance(arg, basestring) else str(arg)\n",
            "\n",
            "    def pair(self, key, value):\n",
            "        '''Returns key=formatted(value).'''\n",
            "        return '%s=%s' % (key, self.format(value))\n",
            "\n",
            "    def emit(self):\n",
            "        '''Emits the Python source for this node.'''\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\transformer.py",
        "line_number": 61,
        "API": ".join(",
        "context": [
            "        if self.kwargs:\n",
            "            args += [self.pair(k, v) for k, v in self.kwargs]\n",
            "        # Set the node name\n",
            "        args.append(self.pair('name', self.node.name))\n",
            "        args = ', '.join(args)\n",
            "        return '%s(%s)' % (self.op, args)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\transformer.py",
        "line_number": 69,
        "API": ".get(",
        "context": [
            "class MaybeActivated(object):\n",
            "\n",
            "    def __init__(self, node, default=True):\n",
            "        self.inject_kwargs = {}\n",
            "        if node.metadata.get('relu', False) != default:\n",
            "            self.inject_kwargs['relu'] = not default\n",
            "\n",
            "    def __call__(self, *args, **kwargs):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\transformer.py",
        "line_number": 158,
        "API": ".format(",
        "context": [
            "        op_code = node.parameters.operation\n",
            "        try:\n",
            "            return TensorFlowNode(operations[op_code])\n",
            "        except KeyError:\n",
            "            raise KaffeError('Unknown elementwise operation: {}'.format(op_code))\n",
            "\n",
            "    def commit(self, chains):\n",
            "        return chains\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\transformer.py",
        "line_number": 192,
        "API": ".join(",
        "context": [
            "    def emit_parents(self, chain):\n",
            "        assert len(chain)\n",
            "        s = '(self.feed('\n",
            "        sep = ', \\n' + self.prefix + (' ' * len(s))\n",
            "        s += sep.join([\"'%s'\" % parent.name for parent in chain[0].node.parents])\n",
            "        return self.statement(s + ')')\n",
            "\n",
            "    def emit_node(self, node):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\transformer.py",
        "line_number": 211,
        "API": ".join(",
        "context": [
            "            b += self.emit_parents(chain)\n",
            "            for node in chain:\n",
            "                b += self.emit_node(node)\n",
            "            blocks.append(b[:-1] + ')')\n",
            "        s = s + '\\n\\n'.join(blocks)\n",
            "        return s\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\caffe-tensorflow\\kaffe\\tensorflow\\transformer.py",
        "line_number": 220,
        "API": ".load(",
        "context": [
            "\n",
            "    def __init__(self, def_path, data_path, verbose=True, phase='test'):\n",
            "        self.verbose = verbose\n",
            "        self.phase = phase\n",
            "        self.load(def_path, data_path, phase)\n",
            "        self.params = None\n",
            "        self.source = None\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 51,
        "API": ".makedirs(",
        "context": [
            "        self.parent.iconbitmap(Wizard.resource_path(\"resource/icon.ico\"))\n",
            "        self.current_project: str = \"\"\n",
            "        self.project_root_path = \"./projects\"\n",
            "        if not os.path.exists(self.project_root_path):\n",
            "            os.makedirs(self.project_root_path)\n",
            "        self.parent.title('Eve-DL Trainer v1({})'.format(get_version()))\n",
            "        self.parent.resizable(width=False, height=False)\n",
            "        self.window_width = 815\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 788,
        "API": ".compile(",
        "context": [
            "            tiny_space=True\n",
            "        )\n",
            "\n",
            "        # \u7f16\u8bd1\u6a21\u578b - \u6309\u94ae\n",
            "        self.btn_compile = ttk.Button(self.parent, text='Compile', command=lambda: self.compile())\n",
            "        self.layout_utils.before_widget(\n",
            "            src=self.btn_compile,\n",
            "            target=self.btn_stop,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 863,
        "API": ".pack(",
        "context": [
            "    def listbox_scrollbar(listbox: tk.Listbox):\n",
            "        y_scrollbar = tk.Scrollbar(\n",
            "            listbox, command=listbox.yview\n",
            "        )\n",
            "        y_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
            "        listbox.config(yscrollcommand=y_scrollbar.set)\n",
            "\n",
            "    def blank_click(self, event):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 871,
        "API": ".format(",
        "context": [
            "        if self.current_project != self.comb_project_name.get():\n",
            "            self.project_name_fill_callback(event)\n",
            "\n",
            "    def project_name_fill_callback(self, event):\n",
            "        suffix = '-{}-{}-H{}-{}-C{}'.format(\n",
            "            self.comb_neu_cnn.get(),\n",
            "            self.comb_recurrent.get(),\n",
            "            self.units_num_spin.get(),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 876,
        "API": ".get(",
        "context": [
            "            self.comb_neu_cnn.get(),\n",
            "            self.comb_recurrent.get(),\n",
            "            self.units_num_spin.get(),\n",
            "            self.comb_loss.get(),\n",
            "            self.comb_channel.get(),\n",
            "        )\n",
            "        current_project_name = self.comb_project_name.get()\n",
            "        if len(current_project_name) > 0 and current_project_name not in self.project_names:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 887,
        "API": ".get(",
        "context": [
            "            self.sample_map[DatasetType.Directory][RunMode.Validation].delete(0, tk.END)\n",
            "            self.category_val.set(\"\")\n",
            "            if not current_project_name.endswith(suffix):\n",
            "                self.comb_project_name.insert(tk.END, suffix)\n",
            "            self.current_project = self.comb_project_name.get()\n",
            "            self.update_dataset_files_path(mode=RunMode.Trains)\n",
            "            self.update_dataset_files_path(mode=RunMode.Validation)\n",
            "            self.data_augmentation_entity = DataAugmentationEntity()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 897,
        "API": ".format(",
        "context": [
            "    @property\n",
            "    def project_path(self):\n",
            "        if not self.current_project:\n",
            "            return None\n",
            "        project_path = \"{}/{}\".format(self.project_root_path, self.current_project)\n",
            "        if not os.path.exists(project_path):\n",
            "            os.makedirs(project_path)\n",
            "        return project_path\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 903,
        "API": ".format(",
        "context": [
            "            os.makedirs(project_path)\n",
            "        return project_path\n",
            "\n",
            "    def update_dataset_files_path(self, mode: RunMode):\n",
            "        dataset_name = \"dataset/{}.0.tfrecords\".format(mode.value)\n",
            "        dataset_path = os.path.join(self.project_path, dataset_name)\n",
            "        dataset_path = dataset_path.replace(\"\\\\\", '/')\n",
            "        self.sample_map[DatasetType.TFRecords][mode].delete(0, tk.END)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 935,
        "API": ".format(",
        "context": [
            "        self.button_state(self.btn_attach_dataset, tk.DISABLED)\n",
            "\n",
            "        for mode in [RunMode.Trains, RunMode.Validation]:\n",
            "            attached_dataset_name = model_conf.dataset_increasing_name(mode)\n",
            "            attached_dataset_name = \"dataset/{}\".format(attached_dataset_name)\n",
            "            attached_dataset_path = os.path.join(self.project_path, attached_dataset_name)\n",
            "            attached_dataset_path = attached_dataset_path.replace(\"\\\\\", '/')\n",
            "            if mode == RunMode.Validation and self.validation_num_val.get() == 0:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 968,
        "API": ".format(",
        "context": [
            "            messagebox.showerror(\n",
            "                \"Error!\", \"Please terminate the current training first or wait for the training to end.\"\n",
            "            )\n",
            "            return\n",
            "        project_path = \"./projects/{}\".format(self.current_project)\n",
            "        try:\n",
            "            shutil.rmtree(project_path)\n",
            "        except Exception as e:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 991,
        "API": ".format(",
        "context": [
            "            messagebox.showerror(\n",
            "                \"Error!\", \"Please terminate the current training first or wait for the training to end.\"\n",
            "            )\n",
            "            return\n",
            "        project_history_path = \"./projects/{}/model\".format(self.current_project)\n",
            "        try:\n",
            "            shutil.rmtree(project_history_path)\n",
            "        except Exception as e:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1021,
        "API": ".format(",
        "context": [
            "            messagebox.showerror(\n",
            "                \"Error!\", \"Please terminate the current training first or wait for the training to end.\"\n",
            "            )\n",
            "            return\n",
            "        project_history_path = \"./projects/{}/dataset\".format(self.current_project)\n",
            "        try:\n",
            "            shutil.rmtree(project_history_path)\n",
            "            self.dataset_train_listbox.delete(1, tk.END)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1036,
        "API": ".format(",
        "context": [
            "        )\n",
            "\n",
            "    @staticmethod\n",
            "    def popup_about():\n",
            "        messagebox.showinfo(\"About\", \"Eve-DL Trainer CORE_VERSION({})\\n\\nAuthor's mailbox: kerlomz@gmail.com\\n\\nQQ Group: 857149419\".format(get_version()))\n",
            "\n",
            "    def auto_loss(self, event):\n",
            "        if self.comb_recurrent.get() == 'NoRecurrent':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1044,
        "API": ".get(",
        "context": [
            "            self.comb_loss.set(\"CrossEntropy\")\n",
            "\n",
            "    @staticmethod\n",
            "    def get_param(src: dict, key, default=None):\n",
            "        result = src.get(key)\n",
            "        return result if result else default\n",
            "\n",
            "    def read_conf(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1049,
        "API": ".get(",
        "context": [
            "        return result if result else default\n",
            "\n",
            "    def read_conf(self):\n",
            "        print('Reading configuration...')\n",
            "        selected = self.comb_project_name.get()\n",
            "        self.current_project = selected\n",
            "        model_conf = ModelConfig(selected)\n",
            "        self.edit_var.set(model_conf.memory_usage)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1131,
        "API": ".size(",
        "context": [
            "        return self.model_conf\n",
            "\n",
            "    @property\n",
            "    def validation_batch_size(self):\n",
            "        # if self.dataset_validation_listbox.size() > 1:\n",
            "        return self.validation_batch_size_val.get()\n",
            "        # else:\n",
            "        #     return min(self.validation_batch_size_val.get(), self.validation_num_val.get())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1138,
        "API": ".get(",
        "context": [
            "        #     return min(self.validation_batch_size_val.get(), self.validation_num_val.get())\n",
            "\n",
            "    @property\n",
            "    def device_usage(self):\n",
            "        return self.edit_var.get()\n",
            "\n",
            "    def save_conf(self):\n",
            "        if not self.current_project:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1151,
        "API": ".get(",
        "context": [
            "            project_name=self.current_project,\n",
            "            MemoryUsage=self.device_usage,\n",
            "            CNNNetwork=self.neu_cnn,\n",
            "            RecurrentNetwork=self.neu_recurrent,\n",
            "            UnitsNum=self.units_num_spin.get(),\n",
            "            Optimizer=self.optimizer,\n",
            "            LossFunction=self.loss_func,\n",
            "            Decoder=self.comb_loss.get(),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1160,
        "API": ".get(",
        "context": [
            "            ModelField=ModelField.Image.value,\n",
            "            ModelScene=ModelScene.Classification.value,\n",
            "            Category=self.category,\n",
            "            Resize=self.resize,\n",
            "            ImageChannel=self.comb_channel.get(),\n",
            "            ImageWidth=self.image_width,\n",
            "            ImageHeight=self.image_height,\n",
            "            MaxLabelNum=self.label_num_spin.get(),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1168,
        "API": ".get(",
        "context": [
            "            AutoPadding=True,\n",
            "            ReplaceTransparent=False,\n",
            "            HorizontalStitching=False,\n",
            "            OutputSplit='',\n",
            "            LabelFrom=self.label_from_var.get(),\n",
            "            ExtractRegex=self.extract_regex,\n",
            "            LabelSplit=self.label_split,\n",
            "            DatasetTrainsPath=self.dataset_value(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1183,
        "API": ".get(",
        "context": [
            "            ),\n",
            "            SourceValidationPath=self.dataset_value(\n",
            "                dataset_type=DatasetType.Directory, mode=RunMode.Validation\n",
            "            ),\n",
            "            ValidationSetNum=self.validation_num_val.get(),\n",
            "            SavedSteps=100,\n",
            "            ValidationSteps=500,\n",
            "            EndAcc=self.end_acc_val.get(),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1188,
        "API": ".get(",
        "context": [
            "            SavedSteps=100,\n",
            "            ValidationSteps=500,\n",
            "            EndAcc=self.end_acc_val.get(),\n",
            "            EndCost=self.end_cost_val.get(),\n",
            "            EndEpochs=self.end_epochs_spin.get(),\n",
            "            BatchSize=self.batch_size_val.get(),\n",
            "            ValidationBatchSize=self.validation_batch_size,\n",
            "            LearningRate=self.learning_rate_spin.get(),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1236,
        "API": ".format(",
        "context": [
            "        train_path = self.dataset_value(DatasetType.Directory, RunMode.Trains)\n",
            "        validation_path = self.dataset_value(DatasetType.Directory, RunMode.Validation)\n",
            "        if len(train_path) < 1:\n",
            "            messagebox.showerror(\n",
            "                \"Error!\", \"{} Sample set has not been added.\".format(RunMode.Trains.value)\n",
            "            )\n",
            "            self.button_state(self.btn_make_dataset, tk.NORMAL)\n",
            "            return\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1252,
        "API": ".get(",
        "context": [
            "        )\n",
            "\n",
            "    @property\n",
            "    def size(self):\n",
            "        return self.json_filter(self.size_val.get(), int)\n",
            "\n",
            "    @property\n",
            "    def image_height(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1264,
        "API": ".get(",
        "context": [
            "        return self.size[0]\n",
            "\n",
            "    @property\n",
            "    def resize(self):\n",
            "        return self.json_filter(self.resize_val.get(), int)\n",
            "\n",
            "    @property\n",
            "    def neu_cnn(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1272,
        "API": ".get(",
        "context": [
            "        return self.comb_neu_cnn.get()\n",
            "\n",
            "    @property\n",
            "    def neu_recurrent(self):\n",
            "        return self.comb_recurrent.get()\n",
            "\n",
            "    @property\n",
            "    def loss_func(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1280,
        "API": ".get(",
        "context": [
            "        return self.comb_loss.get()\n",
            "\n",
            "    @property\n",
            "    def optimizer(self):\n",
            "        return self.comb_optimizer.get()\n",
            "\n",
            "    @staticmethod\n",
            "    def json_filter(content, item_type):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1301,
        "API": ".get(",
        "context": [
            "        return content\n",
            "\n",
            "    @property\n",
            "    def category(self):\n",
            "        comb_selected = self.comb_category.get()\n",
            "        if not comb_selected:\n",
            "            messagebox.showerror(\n",
            "                \"Error!\", \"Please select built-in category or custom category first\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1308,
        "API": ".get(",
        "context": [
            "                \"Error!\", \"Please select built-in category or custom category first\"\n",
            "            )\n",
            "            return None\n",
            "        if comb_selected == 'CUSTOMIZED':\n",
            "            category_value = self.category_entry.get()\n",
            "            if category_value == NOT_EDITABLE_MSG:\n",
            "                return self.model_conf.category_param_text\n",
            "            category_value = category_value.replace(\"'\", '\"') if \"'\" in category_value else category_value\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1319,
        "API": ".size(",
        "context": [
            "        return category_value\n",
            "\n",
            "    def dataset_value(self, dataset_type: DatasetType, mode: RunMode):\n",
            "        listbox = self.sample_map[dataset_type][mode]\n",
            "        value = list(listbox.get(0, listbox.size() - 1))\n",
            "        return value\n",
            "\n",
            "    def compile_task(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1441,
        "API": ".size(",
        "context": [
            "        is_sub = False\n",
            "        for i, item in enumerate(os.scandir(filename)):\n",
            "            if item.is_dir():\n",
            "                path = item.path.replace(\"\\\\\", \"/\")\n",
            "                if self.sample_map[dataset_type][mode].size() == 0:\n",
            "                    self.fetch_sample([path])\n",
            "                self.sample_map[dataset_type][mode].insert(tk.END, path)\n",
            "                if i > 0:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1451,
        "API": ".size(",
        "context": [
            "            else:\n",
            "                break\n",
            "        if not is_sub:\n",
            "            filename = filename.replace(\"\\\\\", \"/\")\n",
            "            if self.sample_map[dataset_type][mode].size() == 0:\n",
            "                self.fetch_sample([filename])\n",
            "            self.sample_map[dataset_type][mode].insert(tk.END, filename)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1472,
        "API": ".get(",
        "context": [
            "                return k\n",
            "\n",
            "    def fetch_category(self):\n",
            "        self.model_conf = self.save_conf()\n",
            "        if self.model_conf.label_from == LabelFrom.TXT or self.label_from_var.get() == LabelFrom.TXT.value:\n",
            "            messagebox.showerror(\n",
            "                \"Error!\", \"The Label From is currently not supported.\"\n",
            "            )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1497,
        "API": ".split(",
        "context": [
            "        len_label = -1\n",
            "\n",
            "        for file_name in file_names:\n",
            "            if \"_\" in file_name:\n",
            "                label = file_name.split(\"_\")[0]\n",
            "                label = [i for i in label]\n",
            "                len_label = len(label)\n",
            "                category.extend(label)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1502,
        "API": ".join(",
        "context": [
            "                label = [i for i in label]\n",
            "                len_label = len(label)\n",
            "                category.extend(label)\n",
            "\n",
            "        size = PilImage.open(os.path.join(dataset_path[0], file_names[0])).size\n",
            "        self.size_val.set(json.dumps(size))\n",
            "        self.resize_val.set(json.dumps(size))\n",
            "        self.label_num_spin.set(len_label)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1521,
        "API": ".get(",
        "context": [
            "        except IndexError as e:\n",
            "            print(e)\n",
            "\n",
            "    def comb_category_callback(self, event):\n",
            "        comb_selected = self.comb_category.get()\n",
            "        if comb_selected == 'CUSTOMIZED':\n",
            "            self.category_entry['state'] = tk.NORMAL\n",
            "        else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1532,
        "API": ".ceil(",
        "context": [
            "    def check_resize(self):\n",
            "        if self.loss_func == 'CTC':\n",
            "            return True\n",
            "        param = OUTPUT_SHAPE1_MAP[NETWORK_MAP[self.neu_cnn]]\n",
            "        shape1w = math.ceil(1.0*self.resize[0]/param[0])\n",
            "        shape1h = math.ceil(1.0*self.resize[1]/param[0])\n",
            "        input_s1 = shape1w * shape1h * param[1]\n",
            "        label_num = int(self.label_num_spin.get())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1538,
        "API": ".format(",
        "context": [
            "        input_s1 = shape1w * shape1h * param[1]\n",
            "        label_num = int(self.label_num_spin.get())\n",
            "        if input_s1 % label_num != 0:\n",
            "            messagebox.showerror(\n",
            "                \"Error!\", \"Shape[1] = {} must divide the label_num = {}.\".format(input_s1, label_num)\n",
            "            )\n",
            "            return False\n",
            "        return True\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app.py",
        "line_number": 1550,
        "API": ".join(",
        "context": [
            "            # PyInstaller creates a temp folder and stores path in _MEIPASS\n",
            "            base_path = sys._MEIPASS\n",
            "        except AttributeError:\n",
            "            base_path = os.path.abspath(\".\")\n",
            "        return os.path.join(base_path, relative_path)\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app_cn.py",
        "line_number": 51,
        "API": ".makedirs(",
        "context": [
            "        self.parent.iconbitmap(Wizard.resource_path(\"resource/icon.ico\"))\n",
            "        self.current_project: str = \"\"\n",
            "        self.project_root_path = \"./projects\"\n",
            "        if not os.path.exists(self.project_root_path):\n",
            "            os.makedirs(self.project_root_path)\n",
            "        self.parent.title('Eve-\u6df1\u5ea6\u8bad\u7ec3\u6846\u67b6 v1({})'.format(get_version()))\n",
            "        self.parent.resizable(width=False, height=False)\n",
            "        self.window_width = 815\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app_cn.py",
        "line_number": 794,
        "API": ".compile(",
        "context": [
            "            tiny_space=True\n",
            "        )\n",
            "\n",
            "        # \u7f16\u8bd1\u6a21\u578b - \u6309\u94ae\n",
            "        self.btn_compile = ttk.Button(self.parent, style='my.TButton', text='\u7f16\u8bd1', command=lambda: self.compile())\n",
            "        self.layout_utils.before_widget(\n",
            "            src=self.btn_compile,\n",
            "            target=self.btn_stop,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app_cn.py",
        "line_number": 975,
        "API": ".format(",
        "context": [
            "            messagebox.showerror(\n",
            "                \"Error!\", \"\u8bf7\u5148\u7ed3\u675f\u5f53\u524d\u8bad\u7ec3\u6216\u8005\u7b49\u5f85\u8bad\u7ec3\u5b8c\u6210.\"\n",
            "            )\n",
            "            return\n",
            "        project_path = \"./projects/{}\".format(self.current_project)\n",
            "        try:\n",
            "            shutil.rmtree(project_path)\n",
            "        except Exception as e:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app_cn.py",
        "line_number": 998,
        "API": ".format(",
        "context": [
            "            messagebox.showerror(\n",
            "                \"Error!\", \"\u8bf7\u5148\u7ed3\u675f\u5f53\u524d\u8bad\u7ec3\u6216\u8005\u7b49\u5f85\u8bad\u7ec3\u5b8c\u6210.\"\n",
            "            )\n",
            "            return\n",
            "        project_history_path = \"./projects/{}/model\".format(self.current_project)\n",
            "        try:\n",
            "            shutil.rmtree(project_history_path)\n",
            "        except Exception as e:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app_cn.py",
        "line_number": 1028,
        "API": ".format(",
        "context": [
            "            messagebox.showerror(\n",
            "                \"Error!\", \"\u8bf7\u5148\u7ed3\u675f\u5f53\u524d\u8bad\u7ec3\u6216\u8005\u7b49\u5f85\u8bad\u7ec3\u5b8c\u6210.\"\n",
            "            )\n",
            "            return\n",
            "        project_history_path = \"./projects/{}/dataset\".format(self.current_project)\n",
            "        try:\n",
            "            shutil.rmtree(project_history_path)\n",
            "            self.dataset_train_listbox.delete(1, tk.END)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app_cn.py",
        "line_number": 1044,
        "API": ".format(",
        "context": [
            "\n",
            "    @staticmethod\n",
            "    def popup_about():\n",
            "        messagebox.showinfo(\"\u5173\u4e8e\",\n",
            "                            \"Eve-\u6df1\u5ea6\u8bad\u7ec3 \u6838\u5fc3\u7248\u672c({})\\n\\n\u4f5c\u8005\u90ae\u7bb1: kerlomz@gmail.com\\n\\nQQ \u7fa4: 857149419\".format(\n",
            "                                get_version()))\n",
            "\n",
            "    def auto_loss(self, event):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app_cn.py",
        "line_number": 1245,
        "API": ".format(",
        "context": [
            "        train_path = self.dataset_value(DatasetType.Directory, RunMode.Trains)\n",
            "        validation_path = self.dataset_value(DatasetType.Directory, RunMode.Validation)\n",
            "        if len(train_path) < 1:\n",
            "            messagebox.showerror(\n",
            "                \"\u9519\u8bef!\", \"{} \u6837\u672c\u5c1a\u672a\u88ab\u6dfb\u52a0.\".format(RunMode.Trains.value)\n",
            "            )\n",
            "            self.button_state(self.btn_make_dataset, tk.NORMAL)\n",
            "            return\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app_cn.py",
        "line_number": 1310,
        "API": ".get(",
        "context": [
            "        return content\n",
            "\n",
            "    @property\n",
            "    def category(self):\n",
            "        comb_selected = self.comb_category.get()\n",
            "        if not comb_selected:\n",
            "            messagebox.showerror(\n",
            "                \"Error!\", \"\u8bf7\u9009\u62e9\u5185\u7f6e\u5206\u7c7b\u6216\u81ea\u5b9a\u4e49\u5206\u7c7b\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app_cn.py",
        "line_number": 1317,
        "API": ".get(",
        "context": [
            "                \"Error!\", \"\u8bf7\u9009\u62e9\u5185\u7f6e\u5206\u7c7b\u6216\u81ea\u5b9a\u4e49\u5206\u7c7b\"\n",
            "            )\n",
            "            return None\n",
            "        if comb_selected == 'CUSTOMIZED':\n",
            "            category_value = self.category_entry.get()\n",
            "            if category_value == NOT_EDITABLE_MSG:\n",
            "                return self.model_conf.category_param_text\n",
            "            category_value = category_value.replace(\"'\", '\"') if \"'\" in category_value else category_value\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app_cn.py",
        "line_number": 1480,
        "API": ".get(",
        "context": [
            "            if v == min_index:\n",
            "                return k\n",
            "\n",
            "    def fetch_category(self):\n",
            "        if self.model_conf.label_from == LabelFrom.TXT or self.label_from_var.get() == LabelFrom.TXT.value:\n",
            "            messagebox.showerror(\n",
            "                \"Error!\", \"\u5f53\u524d\u6807\u7b7e\u6e90\u4e0d\u652f\u6301.\"\n",
            "            )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app_cn.py",
        "line_number": 1541,
        "API": ".ceil(",
        "context": [
            "    def check_resize(self):\n",
            "        if self.loss_func == 'CTC':\n",
            "            return True\n",
            "        param = OUTPUT_SHAPE1_MAP[NETWORK_MAP[self.neu_cnn]]\n",
            "        shape1w = math.ceil(1.0 * self.resize[0] / param[0])\n",
            "        shape1h = math.ceil(1.0 * self.resize[1] / param[0])\n",
            "        input_s1 = shape1w * shape1h * param[1]\n",
            "        label_num = int(self.label_num_spin.get())\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\app_cn.py",
        "line_number": 1547,
        "API": ".format(",
        "context": [
            "        input_s1 = shape1w * shape1h * param[1]\n",
            "        label_num = int(self.label_num_spin.get())\n",
            "        if input_s1 % label_num != 0:\n",
            "            messagebox.showerror(\n",
            "                \"Error!\", \"Shape[1] = {} \u5fc5\u987b\u88ab label_num = {} \u6574\u9664.\".format(input_s1, label_num)\n",
            "            )\n",
            "            return False\n",
            "        return True\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\category.py",
        "line_number": 533,
        "API": ".get(",
        "context": [
            "    if isinstance(param, SimpleCharset):\n",
            "        param = param.value\n",
            "    if isinstance(param, str):\n",
            "        if param in SIMPLE_CATEGORY_MODEL.keys():\n",
            "            return SIMPLE_CATEGORY_MODEL.get(param)\n",
            "        exception(\n",
            "            \"Category set configuration error, customized category set should be list type\",\n",
            "            ConfigException.CATEGORY_INCORRECT\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 18,
        "API": ".system(",
        "context": [
            "# Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n",
            "# If you have a GPU, you shouldn't care about AVX support.\n",
            "# Just disables the warning, doesn't enable AVX/FMA\n",
            "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
            "PLATFORM = platform.system()\n",
            "# PATH_SPLIT = \"\\\\\" if PLATFORM == \"Windows\" else \"/\"\n",
            "PATH_SPLIT = \"/\"\n",
            "MODEL_CONFIG_NAME = \"model.yaml\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 31,
        "API": ".join(",
        "context": [
            "        # PyInstaller creates a temp folder and stores path in _MEIPASS\n",
            "        base_path = sys._MEIPASS\n",
            "    except AttributeError:\n",
            "        base_path = os.path.abspath(\".\")\n",
            "    return os.path.join(base_path, relative_path)\n",
            "\n",
            "\n",
            "def get_version():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 41,
        "API": ".join(",
        "context": [
            "    if not os.path.exists(version_file_path):\n",
            "        return \"NULL\"\n",
            "\n",
            "    with open(version_file_path, \"r\", encoding=\"utf8\") as f:\n",
            "        return \"\".join(f.readlines()).strip()\n",
            "\n",
            "\n",
            "NETWORK_MAP = {\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 258,
        "API": ".format(",
        "context": [
            "    compile_model_path: str\n",
            "\n",
            "    def __init__(self, project_name, project_path=None, is_dev=True, **argv):\n",
            "        self.is_dev = is_dev\n",
            "        self.project_path = project_path if project_path else \"./projects/{}\".format(project_name)\n",
            "        self.output_path = os.path.join(self.project_path, 'out')\n",
            "        self.compile_conf_path = os.path.join(self.output_path, 'model')\n",
            "        self.compile_conf_path = os.path.join(self.compile_conf_path, \"{}_model.yaml\".format(project_name))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 263,
        "API": ".join(",
        "context": [
            "        self.output_path = os.path.join(self.project_path, 'out')\n",
            "        self.compile_conf_path = os.path.join(self.output_path, 'model')\n",
            "        self.compile_conf_path = os.path.join(self.compile_conf_path, \"{}_model.yaml\".format(project_name))\n",
            "        self.model_root_path = os.path.join(self.project_path, 'model')\n",
            "        self.model_conf_path = os.path.join(self.project_path, MODEL_CONFIG_NAME)\n",
            "        self.dataset_root_path = os.path.join(self.project_path, 'dataset')\n",
            "        self.checkpoint_tag = 'checkpoint'\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 268,
        "API": ".makedirs(",
        "context": [
            "        self.dataset_root_path = os.path.join(self.project_path, 'dataset')\n",
            "        self.checkpoint_tag = 'checkpoint'\n",
            "\n",
            "        if not os.path.exists(self.project_path):\n",
            "            os.makedirs(self.project_path)\n",
            "\n",
            "        if not os.path.exists(self.model_root_path):\n",
            "            os.makedirs(self.model_root_path)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 274,
        "API": ".makedirs(",
        "context": [
            "        if not os.path.exists(self.model_root_path):\n",
            "            os.makedirs(self.model_root_path)\n",
            "\n",
            "        if not os.path.exists(self.output_path):\n",
            "            os.makedirs(self.output_path)\n",
            "\n",
            "        if not os.path.exists(self.dataset_root_path):\n",
            "            os.makedirs(self.dataset_root_path)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 287,
        "API": ".get(",
        "context": [
            "\n",
            "    def read_conf(self):\n",
            "        \"\"\"MODEL\"\"\"\n",
            "        self.model_root = self.conf['Model']\n",
            "        self.model_name = self.model_root.get('ModelName')\n",
            "        self.model_tag = '{model_name}.model'.format(model_name=self.model_name)\n",
            "\n",
            "        self.model_field_param = self.model_root.get('ModelField')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 295,
        "API": ".get(",
        "context": [
            "        self.model_scene_param = self.model_root.get('ModelScene')\n",
            "\n",
            "        \"\"\"SYSTEM\"\"\"\n",
            "        self.system_root = self.conf['System']\n",
            "        self.memory_usage = self.system_root.get('MemoryUsage')\n",
            "        self.model_version = self.system_root.get(\"Version\")\n",
            "        self.save_model = os.path.join(self.model_root_path, self.model_tag)\n",
            "        self.save_checkpoint = os.path.join(self.model_root_path, self.checkpoint_tag)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 302,
        "API": ".get(",
        "context": [
            "        self.save_checkpoint = os.path.join(self.model_root_path, self.checkpoint_tag)\n",
            "\n",
            "        \"\"\"FIELD PARAM - IMAGE\"\"\"\n",
            "        self.field_root = self.conf['FieldParam']\n",
            "        self.category_param = self.field_root.get('Category')\n",
            "        if isinstance(self.category_param, list):\n",
            "            self.category_param_text = json.dumps(self.category_param, ensure_ascii=False)\n",
            "        elif isinstance(self.category_param, str):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 307,
        "API": ".get(",
        "context": [
            "        if isinstance(self.category_param, list):\n",
            "            self.category_param_text = json.dumps(self.category_param, ensure_ascii=False)\n",
            "        elif isinstance(self.category_param, str):\n",
            "            self.category_param_text = self.category_param\n",
            "        self.image_channel = self.field_root.get('ImageChannel')\n",
            "        self.image_width = self.field_root.get('ImageWidth')\n",
            "        self.image_height = self.field_root.get('ImageHeight')\n",
            "        self.resize = self.field_root.get('Resize')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 312,
        "API": ".get(",
        "context": [
            "        self.image_width = self.field_root.get('ImageWidth')\n",
            "        self.image_height = self.field_root.get('ImageHeight')\n",
            "        self.resize = self.field_root.get('Resize')\n",
            "        self.max_label_num = self.field_root.get('MaxLabelNum')\n",
            "        self.auto_padding = self.field_root.get('AutoPadding')\n",
            "        self.output_split = self.field_root.get('OutputSplit')\n",
            "\n",
            "        \"\"\"NEURAL NETWORK\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 317,
        "API": ".get(",
        "context": [
            "        self.output_split = self.field_root.get('OutputSplit')\n",
            "\n",
            "        \"\"\"NEURAL NETWORK\"\"\"\n",
            "        self.neu_network_root = self.conf['NeuralNet']\n",
            "        self.neu_cnn_param = self.neu_network_root.get('CNNNetwork')\n",
            "\n",
            "        self.neu_recurrent_param = self.neu_network_root.get('RecurrentNetwork')\n",
            "        self.neu_recurrent_param = self.neu_recurrent_param if self.neu_recurrent_param else 'NoRecurrent'\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 322,
        "API": ".get(",
        "context": [
            "\n",
            "        self.neu_recurrent_param = self.neu_network_root.get('RecurrentNetwork')\n",
            "        self.neu_recurrent_param = self.neu_recurrent_param if self.neu_recurrent_param else 'NoRecurrent'\n",
            "\n",
            "        self.units_num = self.neu_network_root.get('UnitsNum')\n",
            "        self.neu_optimizer_param = self.neu_network_root.get('Optimizer')\n",
            "        self.neu_optimizer_param = self.neu_optimizer_param if self.neu_optimizer_param else 'RAdam'\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 327,
        "API": ".get(",
        "context": [
            "        self.neu_optimizer_param = self.neu_network_root.get('Optimizer')\n",
            "        self.neu_optimizer_param = self.neu_optimizer_param if self.neu_optimizer_param else 'RAdam'\n",
            "\n",
            "        self.output_layer = self.neu_network_root.get('OutputLayer')\n",
            "        self.loss_func_param = self.output_layer.get('LossFunction')\n",
            "\n",
            "        self.decoder = self.output_layer.get('Decoder')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 332,
        "API": ".get(",
        "context": [
            "\n",
            "        self.decoder = self.output_layer.get('Decoder')\n",
            "\n",
            "        \"\"\"LABEL\"\"\"\n",
            "        self.label_root = self.conf.get('Label')\n",
            "        self.label_from_param = self.label_root.get('LabelFrom')\n",
            "        self.extract_regex = self.label_root.get('ExtractRegex')\n",
            "        self.extract_regex = self.extract_regex if self.extract_regex else \".*?(?=_)\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 341,
        "API": ".get(",
        "context": [
            "\n",
            "        \"\"\"PATH\"\"\"\n",
            "        self.trains_root = self.conf['Trains']\n",
            "\n",
            "        self.dataset_path_root = self.trains_root.get('DatasetPath')\n",
            "        self.trains_path[DatasetType.TFRecords]: list = self.dataset_path_root.get('Training')\n",
            "        self.validation_path[DatasetType.TFRecords]: list = self.dataset_path_root.get('Validation')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 346,
        "API": ".get(",
        "context": [
            "        self.trains_path[DatasetType.TFRecords]: list = self.dataset_path_root.get('Training')\n",
            "        self.validation_path[DatasetType.TFRecords]: list = self.dataset_path_root.get('Validation')\n",
            "\n",
            "        self.source_path_root = self.trains_root.get('SourcePath')\n",
            "        self.trains_path[DatasetType.Directory]: list = self.source_path_root.get('Training')\n",
            "        self.validation_path[DatasetType.Directory]: list = self.source_path_root.get('Validation')\n",
            "\n",
            "        self.validation_set_num: int = self.trains_root.get('ValidationSetNum')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 353,
        "API": ".get(",
        "context": [
            "        self.validation_set_num: int = self.trains_root.get('ValidationSetNum')\n",
            "        # self.validation_set_num = self.validation_set_num if self.validation_set_num else 500\n",
            "\n",
            "        \"\"\"TRAINS\"\"\"\n",
            "        self.trains_save_steps = self.trains_root.get('SavedSteps')\n",
            "        self.trains_validation_steps = self.trains_root.get('ValidationSteps')\n",
            "        self.trains_end_acc = self.trains_root.get('EndAcc')\n",
            "        self.trains_end_cost = self.trains_root.get('EndCost')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 358,
        "API": ".get(",
        "context": [
            "        self.trains_validation_steps = self.trains_root.get('ValidationSteps')\n",
            "        self.trains_end_acc = self.trains_root.get('EndAcc')\n",
            "        self.trains_end_cost = self.trains_root.get('EndCost')\n",
            "        self.trains_end_cost = self.trains_end_cost if self.trains_end_cost else 1\n",
            "        self.trains_end_epochs = self.trains_root.get('EndEpochs')\n",
            "        self.trains_end_epochs = self.trains_end_epochs if self.trains_end_epochs else 2\n",
            "        self.trains_learning_rate: float = float(self.trains_root.get('LearningRate'))\n",
            "        self.batch_size = self.trains_root.get('BatchSize')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 363,
        "API": ".get(",
        "context": [
            "        self.trains_end_epochs = self.trains_end_epochs if self.trains_end_epochs else 2\n",
            "        self.trains_learning_rate: float = float(self.trains_root.get('LearningRate'))\n",
            "        self.batch_size = self.trains_root.get('BatchSize')\n",
            "        self.batch_size = self.batch_size if self.batch_size else 64\n",
            "        self.validation_batch_size = self.trains_root.get('ValidationBatchSize')\n",
            "        self.validation_batch_size = self.validation_batch_size if self.validation_batch_size else 300\n",
            "\n",
            "        \"\"\"DATA AUGMENTATION\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 368,
        "API": ".get(",
        "context": [
            "        self.validation_batch_size = self.validation_batch_size if self.validation_batch_size else 300\n",
            "\n",
            "        \"\"\"DATA AUGMENTATION\"\"\"\n",
            "        self.data_augmentation_root = self.conf['DataAugmentation']\n",
            "        self.da_binaryzation = self.data_augmentation_root.get('Binaryzation')\n",
            "        self.da_median_blur = self.data_augmentation_root.get('MedianBlur')\n",
            "        self.da_gaussian_blur = self.data_augmentation_root.get('GaussianBlur')\n",
            "        self.da_equalize_hist = self.data_augmentation_root.get('EqualizeHist')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 373,
        "API": ".get(",
        "context": [
            "        self.da_median_blur = self.data_augmentation_root.get('MedianBlur')\n",
            "        self.da_gaussian_blur = self.data_augmentation_root.get('GaussianBlur')\n",
            "        self.da_equalize_hist = self.data_augmentation_root.get('EqualizeHist')\n",
            "        self.da_laplace = self.data_augmentation_root.get('Laplace')\n",
            "        self.da_rotate = self.data_augmentation_root.get('Rotate')\n",
            "        self.da_warp_perspective = self.data_augmentation_root.get('WarpPerspective')\n",
            "        self.da_sp_noise = self.data_augmentation_root.get('PepperNoise')\n",
            "        self.da_brightness = self.data_augmentation_root.get('Brightness')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 378,
        "API": ".get(",
        "context": [
            "        self.da_warp_perspective = self.data_augmentation_root.get('WarpPerspective')\n",
            "        self.da_sp_noise = self.data_augmentation_root.get('PepperNoise')\n",
            "        self.da_brightness = self.data_augmentation_root.get('Brightness')\n",
            "        self.da_saturation = self.data_augmentation_root.get('Saturation')\n",
            "        self.da_hue = self.data_augmentation_root.get('Hue')\n",
            "        self.da_gamma = self.data_augmentation_root.get('Gamma')\n",
            "        self.da_channel_swap = self.data_augmentation_root.get('ChannelSwap')\n",
            "        self.da_random_blank = self.data_augmentation_root.get('RandomBlank')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 383,
        "API": ".get(",
        "context": [
            "        self.da_gamma = self.data_augmentation_root.get('Gamma')\n",
            "        self.da_channel_swap = self.data_augmentation_root.get('ChannelSwap')\n",
            "        self.da_random_blank = self.data_augmentation_root.get('RandomBlank')\n",
            "        self.da_random_transition = self.data_augmentation_root.get('RandomTransition')\n",
            "        self.da_random_captcha = self.data_augmentation_root.get('RandomCaptcha')\n",
            "        if not self.da_random_captcha:\n",
            "            self.da_random_captcha = {\"Enable\": False, \"FontPath\": \"\"}\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 389,
        "API": ".get(",
        "context": [
            "            self.da_random_captcha = {\"Enable\": False, \"FontPath\": \"\"}\n",
            "\n",
            "        \"\"\"PRETREATMENT\"\"\"\n",
            "        self.pretreatment_root = self.conf['Pretreatment']\n",
            "        self.pre_binaryzation = self.pretreatment_root.get('Binaryzation')\n",
            "        self.pre_replace_transparent = self.pretreatment_root.get(\"ReplaceTransparent\")\n",
            "        self.pre_horizontal_stitching = self.pretreatment_root.get(\"HorizontalStitching\")\n",
            "        self.pre_concat_frames = self.pretreatment_root.get('ConcatFrames')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 394,
        "API": ".get(",
        "context": [
            "        self.pre_replace_transparent = self.pretreatment_root.get(\"ReplaceTransparent\")\n",
            "        self.pre_horizontal_stitching = self.pretreatment_root.get(\"HorizontalStitching\")\n",
            "        self.pre_concat_frames = self.pretreatment_root.get('ConcatFrames')\n",
            "        self.pre_blend_frames = self.pretreatment_root.get('BlendFrames')\n",
            "        self.pre_exec_map = self.pretreatment_root.get('ExecuteMap')\n",
            "        self.pre_exec_map = self.pre_exec_map if self.pre_exec_map else {}\n",
            "\n",
            "        \"\"\"COMPILE_MODEL\"\"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 407,
        "API": ".format(",
        "context": [
            "    def model_field(self) -> ModelField:\n",
            "        return ModelConfig.param_convert(\n",
            "            source=self.model_field_param,\n",
            "            param_map=MODEL_FIELD_MAP,\n",
            "            text=\"Current model field ({model_field}) is not supported\".format(model_field=self.model_field_param),\n",
            "            code=ConfigException.MODEL_FIELD_NOT_SUPPORTED\n",
            "        )\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 416,
        "API": ".format(",
        "context": [
            "    def model_scene(self) -> ModelScene:\n",
            "        return ModelConfig.param_convert(\n",
            "            source=self.model_scene_param,\n",
            "            param_map=MODEL_SCENE_MAP,\n",
            "            text=\"Current model scene ({model_scene}) is not supported\".format(model_scene=self.model_scene_param),\n",
            "            code=ConfigException.MODEL_SCENE_NOT_SUPPORTED\n",
            "        )\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 425,
        "API": ".format(",
        "context": [
            "    def neu_cnn(self) -> CNNNetwork:\n",
            "        return ModelConfig.param_convert(\n",
            "            source=self.neu_cnn_param,\n",
            "            param_map=NETWORK_MAP,\n",
            "            text=\"This cnn layer ({param}) is not supported at this time.\".format(param=self.neu_cnn_param),\n",
            "            code=ConfigException.NETWORK_NOT_SUPPORTED\n",
            "        )\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 434,
        "API": ".format(",
        "context": [
            "    def neu_recurrent(self) -> RecurrentNetwork:\n",
            "        return ModelConfig.param_convert(\n",
            "            source=self.neu_recurrent_param,\n",
            "            param_map=NETWORK_MAP,\n",
            "            text=\"Current recurrent layer ({recurrent}) is not supported\".format(recurrent=self.neu_recurrent_param),\n",
            "            code=ConfigException.NETWORK_NOT_SUPPORTED\n",
            "        )\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 443,
        "API": ".format(",
        "context": [
            "    def neu_optimizer(self) -> Optimizer:\n",
            "        return ModelConfig.param_convert(\n",
            "            source=self.neu_optimizer_param,\n",
            "            param_map=OPTIMIZER_MAP,\n",
            "            text=\"This optimizer ({param}) is not supported at this time.\".format(param=self.neu_optimizer_param),\n",
            "            code=ConfigException.NETWORK_NOT_SUPPORTED\n",
            "        )\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 452,
        "API": ".format(",
        "context": [
            "    def loss_func(self) -> LossFunction:\n",
            "        return ModelConfig.param_convert(\n",
            "            source=self.loss_func_param,\n",
            "            param_map=LOSS_FUNC_MAP,\n",
            "            text=\"This type of loss function ({loss}) is not supported at this time.\".format(loss=self.loss_func_param),\n",
            "            code=ConfigException.LOSS_FUNC_NOT_SUPPORTED,\n",
            "        )\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 461,
        "API": ".format(",
        "context": [
            "    def label_from(self) -> LabelFrom:\n",
            "        return ModelConfig.param_convert(\n",
            "            source=self.label_from_param,\n",
            "            param_map=LABEL_FROM_MAP,\n",
            "            text=\"This type of label from ({lf}) is not supported at this time.\".format(lf=self.label_from_param),\n",
            "            code=ConfigException.ERROR_LABEL_FROM,\n",
            "        )\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 487,
        "API": ".format(",
        "context": [
            "\n",
            "        if not os.path.exists(self.model_conf_path):\n",
            "            exception(\n",
            "                'Configuration File \"{}\" No Found. '\n",
            "                'If it is used for the first time, please copy one according to model.template as {}'.format(\n",
            "                    MODEL_CONFIG_NAME,\n",
            "                    MODEL_CONFIG_NAME\n",
            "                ), ConfigException.MODEL_CONFIG_PATH_NOT_EXIST\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 493,
        "API": ".makedirs(",
        "context": [
            "                    MODEL_CONFIG_NAME\n",
            "                ), ConfigException.MODEL_CONFIG_PATH_NOT_EXIST\n",
            "            )\n",
            "        if not os.path.exists(self.model_root_path):\n",
            "            os.makedirs(self.model_root_path)\n",
            "\n",
            "        model_file = ModelConfig.checkpoint(self.model_name, self.model_root_path)\n",
            "        checkpoint = 'model_checkpoint_path: {}\\nall_model_checkpoint_paths: {}'.format(model_file, model_file)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 498,
        "API": ".write(",
        "context": [
            "\n",
            "        model_file = ModelConfig.checkpoint(self.model_name, self.model_root_path)\n",
            "        checkpoint = 'model_checkpoint_path: {}\\nall_model_checkpoint_paths: {}'.format(model_file, model_file)\n",
            "        with open(self.save_checkpoint, 'w') as f:\n",
            "            f.write(checkpoint)\n",
            "\n",
            "    @staticmethod\n",
            "    def checkpoint(_name, _path):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 504,
        "API": ".split(",
        "context": [
            "    @staticmethod\n",
            "    def checkpoint(_name, _path):\n",
            "        file_list = os.listdir(_path)\n",
            "        checkpoint_group = [\n",
            "            '\"{}\"'.format(i.split(\".meta\")[0]) for i in file_list if\n",
            "            _name + \".model\" in i and i.endswith('.meta')\n",
            "        ]\n",
            "        if not checkpoint_group:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 509,
        "API": ".group(",
        "context": [
            "            _name + \".model\" in i and i.endswith('.meta')\n",
            "        ]\n",
            "        if not checkpoint_group:\n",
            "            return None\n",
            "        checkpoint_step = [int(re.search('(?<=model-).*?(?=\")', i).group()) for i in checkpoint_group]\n",
            "        return checkpoint_group[checkpoint_step.index(max(checkpoint_step))]\n",
            "\n",
            "    @property\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 516,
        "API": ".load(",
        "context": [
            "    @property\n",
            "    def conf(self) -> dict:\n",
            "        with open(self.model_conf_path if self.is_dev else self.compile_conf_path, 'r', encoding=\"utf-8\") as sys_fp:\n",
            "            sys_stream = sys_fp.read()\n",
            "            return yaml.load(sys_stream, Loader=yaml.SafeLoader)\n",
            "\n",
            "    @staticmethod\n",
            "    def list_param(params, intent=6):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 524,
        "API": ".join(",
        "context": [
            "        if params is None:\n",
            "            params = []\n",
            "        if isinstance(params, str):\n",
            "            params = [params]\n",
            "        result = \"\".join([\"\\n{}- \".format(' ' * intent) + i for i in params])\n",
            "        return result\n",
            "\n",
            "    @staticmethod\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 531,
        "API": ".join(",
        "context": [
            "    @staticmethod\n",
            "    def dict_param(params: dict, intent=6):\n",
            "        if params is None:\n",
            "            params = {}\n",
            "        result = \"\".join([\"\\n{} \".format(' ' * intent) + \"{}: {}\".format(k, v) for k, v in params.items()])\n",
            "        return result\n",
            "\n",
            "    @staticmethod\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 537,
        "API": ".format(",
        "context": [
            "\n",
            "    @staticmethod\n",
            "    def val_filter(val):\n",
            "        if isinstance(val, str) and len(val) == 1:\n",
            "            val = \"'{}'\".format(val)\n",
            "        elif val is None:\n",
            "            val = 'null'\n",
            "        return val\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 544,
        "API": ".join(",
        "context": [
            "        return val\n",
            "\n",
            "    def update(self, model_conf_path=None, model_name=None):\n",
            "        with open(\"model.template\", encoding=\"utf8\") as f:\n",
            "            base_config = \"\".join(f.readlines())\n",
            "            model = base_config.format(\n",
            "                MemoryUsage=self.memory_usage,\n",
            "                CNNNetwork=self.neu_cnn.value,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 604,
        "API": ".write(",
        "context": [
            "                Pre_BlendFrames=self.pre_blend_frames,\n",
            "                Pre_ExecuteMap=self.pre_exec_map\n",
            "            )\n",
            "        with open(model_conf_path if model_conf_path else self.model_conf_path, \"w\", encoding=\"utf8\") as f:\n",
            "            f.write(model)\n",
            "\n",
            "    def output_config(self, target_model_name=None):\n",
            "        compiled_config_dir_path = os.path.join(self.output_path, \"model\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 609,
        "API": ".makedirs(",
        "context": [
            "\n",
            "    def output_config(self, target_model_name=None):\n",
            "        compiled_config_dir_path = os.path.join(self.output_path, \"model\")\n",
            "        if not os.path.exists(compiled_config_dir_path):\n",
            "            os.makedirs(compiled_config_dir_path)\n",
            "        compiled_config_path = os.path.join(compiled_config_dir_path, \"{}_model.yaml\".format(self.model_name))\n",
            "        self.update(model_conf_path=compiled_config_path, model_name=target_model_name)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 617,
        "API": ".split(",
        "context": [
            "    def dataset_increasing_name(self, mode: RunMode):\n",
            "        dataset_group = os.listdir(self.dataset_root_path)\n",
            "        if len([i for i in dataset_group if i.startswith(mode.value)]) < 1:\n",
            "            return \"Trains.0.tfrecords\" if mode == RunMode.Trains else \"Validation.0.tfrecords\"\n",
            "        name_split = [i.split(\".\") for i in dataset_group if mode.value in i]\n",
            "        if not name_split:\n",
            "            name_split = [mode.value, \"0\", \".tfrecords\"]\n",
            "        try:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 629,
        "API": ".format(",
        "context": [
            "\n",
            "        current_index = last_index + 1\n",
            "        name_prefix = name_split[0][0]\n",
            "        name_suffix = name_split[0][2]\n",
            "        return \"{}.{}.{}\".format(name_prefix, current_index, name_suffix)\n",
            "\n",
            "    def new(self, **argv):\n",
            "        self.memory_usage = argv.get('MemoryUsage')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 634,
        "API": ".get(",
        "context": [
            "\n",
            "    def new(self, **argv):\n",
            "        self.memory_usage = argv.get('MemoryUsage')\n",
            "        self.neu_cnn_param = argv.get('CNNNetwork')\n",
            "        self.neu_recurrent_param = argv.get('RecurrentNetwork')\n",
            "        self.units_num = argv.get('UnitsNum')\n",
            "        self.neu_optimizer_param = argv.get('Optimizer')\n",
            "        self.loss_func_param = argv.get('LossFunction')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 639,
        "API": ".get(",
        "context": [
            "        self.units_num = argv.get('UnitsNum')\n",
            "        self.neu_optimizer_param = argv.get('Optimizer')\n",
            "        self.loss_func_param = argv.get('LossFunction')\n",
            "        self.decoder = argv.get('Decoder')\n",
            "        self.model_name = argv.get('ModelName')\n",
            "        self.model_field_param = argv.get('ModelField')\n",
            "        self.model_scene_param = argv.get('ModelScene')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 644,
        "API": ".get(",
        "context": [
            "        self.model_field_param = argv.get('ModelField')\n",
            "        self.model_scene_param = argv.get('ModelScene')\n",
            "\n",
            "        if isinstance(argv.get('Category'), list):\n",
            "            self.category_param = json.dumps(argv.get('Category'), ensure_ascii=False)\n",
            "        else:\n",
            "            self.category_param = argv.get('Category')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 653,
        "API": ".get(",
        "context": [
            "            self.category_param_text = json.dumps(self.category_param, ensure_ascii=False)\n",
            "        elif isinstance(self.category_param, str):\n",
            "            self.category_param_text = self.category_param\n",
            "\n",
            "        self.resize = argv.get('Resize')\n",
            "        self.image_channel = argv.get('ImageChannel')\n",
            "        self.image_width = argv.get('ImageWidth')\n",
            "        self.image_height = argv.get('ImageHeight')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 658,
        "API": ".get(",
        "context": [
            "        self.image_channel = argv.get('ImageChannel')\n",
            "        self.image_width = argv.get('ImageWidth')\n",
            "        self.image_height = argv.get('ImageHeight')\n",
            "        self.max_label_num = argv.get('MaxLabelNum')\n",
            "        self.auto_padding = argv.get('AutoPadding')\n",
            "        self.output_split = argv.get('OutputSplit')\n",
            "        self.label_from_param = argv.get('LabelFrom')\n",
            "        self.extract_regex = argv.get('ExtractRegex')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 663,
        "API": ".get(",
        "context": [
            "        self.output_split = argv.get('OutputSplit')\n",
            "        self.label_from_param = argv.get('LabelFrom')\n",
            "        self.extract_regex = argv.get('ExtractRegex')\n",
            "        self.label_split = argv.get('LabelSplit')\n",
            "        self.trains_path[DatasetType.TFRecords] = argv.get('DatasetTrainsPath')\n",
            "        self.validation_path[DatasetType.TFRecords] = argv.get('DatasetValidationPath')\n",
            "        self.trains_path[DatasetType.Directory] = argv.get('SourceTrainPath')\n",
            "        self.validation_path[DatasetType.Directory] = argv.get('SourceValidationPath')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 668,
        "API": ".get(",
        "context": [
            "        self.validation_path[DatasetType.TFRecords] = argv.get('DatasetValidationPath')\n",
            "        self.trains_path[DatasetType.Directory] = argv.get('SourceTrainPath')\n",
            "        self.validation_path[DatasetType.Directory] = argv.get('SourceValidationPath')\n",
            "        self.validation_set_num = argv.get('ValidationSetNum')\n",
            "        self.trains_save_steps = argv.get('SavedSteps')\n",
            "        self.trains_validation_steps = argv.get('ValidationSteps')\n",
            "        self.trains_end_acc = argv.get('EndAcc')\n",
            "        self.trains_end_cost = argv.get('EndCost')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 673,
        "API": ".get(",
        "context": [
            "        self.trains_validation_steps = argv.get('ValidationSteps')\n",
            "        self.trains_end_acc = argv.get('EndAcc')\n",
            "        self.trains_end_cost = argv.get('EndCost')\n",
            "        self.trains_end_epochs = argv.get('EndEpochs')\n",
            "        self.batch_size = argv.get('BatchSize')\n",
            "        self.validation_batch_size = argv.get('ValidationBatchSize')\n",
            "        self.trains_learning_rate = argv.get('LearningRate')\n",
            "        self.da_binaryzation = argv.get('DA_Binaryzation')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 678,
        "API": ".get(",
        "context": [
            "        self.validation_batch_size = argv.get('ValidationBatchSize')\n",
            "        self.trains_learning_rate = argv.get('LearningRate')\n",
            "        self.da_binaryzation = argv.get('DA_Binaryzation')\n",
            "        self.da_median_blur = argv.get('DA_MedianBlur')\n",
            "        self.da_gaussian_blur = argv.get('DA_GaussianBlur')\n",
            "        self.da_equalize_hist = argv.get('DA_EqualizeHist')\n",
            "        self.da_laplace = argv.get('DA_Laplace')\n",
            "        self.da_warp_perspective = argv.get('DA_WarpPerspective')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 683,
        "API": ".get(",
        "context": [
            "        self.da_equalize_hist = argv.get('DA_EqualizeHist')\n",
            "        self.da_laplace = argv.get('DA_Laplace')\n",
            "        self.da_warp_perspective = argv.get('DA_WarpPerspective')\n",
            "        self.da_rotate = argv.get('DA_Rotate')\n",
            "        self.da_sp_noise = argv.get('DA_PepperNoise')\n",
            "        self.da_brightness = argv.get('DA_Brightness')\n",
            "        self.da_saturation = argv.get('DA_Saturation')\n",
            "        self.da_hue = argv.get('DA_Hue')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 688,
        "API": ".get(",
        "context": [
            "        self.da_brightness = argv.get('DA_Brightness')\n",
            "        self.da_saturation = argv.get('DA_Saturation')\n",
            "        self.da_hue = argv.get('DA_Hue')\n",
            "        self.da_gamma = argv.get('DA_Gamma')\n",
            "        self.da_channel_swap = argv.get('DA_ChannelSwap')\n",
            "        self.da_random_blank = argv.get('DA_RandomBlank')\n",
            "        self.da_random_transition = argv.get('DA_RandomTransition')\n",
            "        self.da_random_captcha = argv.get('DA_RandomCaptcha')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 693,
        "API": ".get(",
        "context": [
            "        self.da_random_blank = argv.get('DA_RandomBlank')\n",
            "        self.da_random_transition = argv.get('DA_RandomTransition')\n",
            "        self.da_random_captcha = argv.get('DA_RandomCaptcha')\n",
            "        self.pre_binaryzation = argv.get('Pre_Binaryzation')\n",
            "        self.pre_replace_transparent = argv.get('Pre_ReplaceTransparent')\n",
            "        self.pre_horizontal_stitching = argv.get('Pre_HorizontalStitching')\n",
            "        self.pre_concat_frames = argv.get('Pre_ConcatFrames')\n",
            "        self.pre_blend_frames = argv.get('Pre_BlendFrames')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\config.py",
        "line_number": 706,
        "API": ".format(",
        "context": [
            "        print(\"PROJECT_PATH\", self.project_path)\n",
            "        print('MODEL_PATH:', self.save_model)\n",
            "        print('COMPILE_MODEL_PATH:', self.compile_model_path)\n",
            "        print('CATEGORY_NUM:', self.category_num)\n",
            "        print('IMAGE_WIDTH: {}, IMAGE_HEIGHT: {}'.format(\n",
            "            self.image_width, self.image_height)\n",
            "        )\n",
            "        print('NEURAL NETWORK: {}'.format(self.neu_network_root))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\core.py",
        "line_number": 97,
        "API": ".info(",
        "context": [
            "\n",
            "        \"\"\"\u9009\u62e9\u91c7\u7528\u54ea\u79cd\u5faa\u73af\u7f51\u7edc\"\"\"\n",
            "\n",
            "        # time_major = True: [max_time_step, batch_size, num_classes]\n",
            "        tf.compat.v1.logging.info(\"CNN Output: {}\".format(x.get_shape()))\n",
            "\n",
            "        self.seq_len = tf.compat.v1.fill([tf.shape(x)[0]], tf.shape(x)[1], name=\"seq_len\")\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\core.py",
        "line_number": 159,
        "API": ".reduce_mean(",
        "context": [
            "                labels=self.labels,\n",
            "                logits=self.outputs\n",
            "            )\n",
            "\n",
            "        self.cost = tf.reduce_mean(self.loss)\n",
            "\n",
            "        tf.compat.v1.summary.scalar('cost', self.cost)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\core.py",
        "line_number": 171,
        "API": ".scalar(",
        "context": [
            "            staircase=True,\n",
            "            decay_steps=self.decay_steps,\n",
            "            decay_rate=0.98,\n",
            "        )\n",
            "        tf.compat.v1.summary.scalar('learning_rate', self.lrn_rate)\n",
            "\n",
            "        if self.model_conf.neu_optimizer == Optimizer.AdaBound:\n",
            "            self.optimizer = AdaBoundOptimizer(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\core.py",
        "line_number": 214,
        "API": ".control_dependencies(",
        "context": [
            "        # BN \u64cd\u4f5c\u7b26\u66f4\u65b0(moving_mean, moving_variance)\n",
            "        update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
            "\n",
            "        # \u5c06 train_op \u548c update_ops \u878d\u5408\n",
            "        with tf.control_dependencies(update_ops):\n",
            "            self.train_op = self.optimizer.minimize(\n",
            "                    loss=self.cost,\n",
            "                    global_step=self.global_step,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\decoder.py",
        "line_number": 18,
        "API": ".to_dense(",
        "context": [
            "\n",
            "    def ctc(self, inputs, sequence_length):\n",
            "        \"\"\"\u9488\u5bf9CTC Loss\u7684\u89e3\u7801\"\"\"\n",
            "        ctc_decode, _ = tf.compat.v1.nn.ctc_beam_search_decoder_v2(inputs, sequence_length, beam_width=1)\n",
            "        decoded_sequences = tf.sparse.to_dense(ctc_decode[0], default_value=self.category_num, name='dense_decoded')\n",
            "        return decoded_sequences\n",
            "\n",
            "    @staticmethod\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\decoder.py",
        "line_number": 24,
        "API": ".argmax(",
        "context": [
            "\n",
            "    @staticmethod\n",
            "    def cross_entropy(inputs):\n",
            "        \"\"\"\u9488\u5bf9CrossEntropy Loss\u7684\u89e3\u7801\"\"\"\n",
            "        return tf.argmax(inputs, 2, name='dense_decoded')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\encoder.py",
        "line_number": 56,
        "API": ".format(",
        "context": [
            "        # if im is None:\n",
            "\n",
            "        path_or_stream = io.BytesIO(path_or_bytes) if isinstance(path_or_bytes, bytes) else path_or_bytes\n",
            "        if not path_or_stream:\n",
            "            return \"Picture is corrupted: {}\".format(path_or_bytes)\n",
            "        try:\n",
            "            pil_image = PIL.Image.open(path_or_stream)\n",
            "        except OSError as e:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\encoder.py",
        "line_number": 67,
        "API": ".convert(",
        "context": [
            "\n",
            "        gif_handle = self.model_conf.pre_concat_frames != -1 or self.model_conf.pre_blend_frames != -1\n",
            "\n",
            "        if pil_image.mode == 'P' and not gif_handle:\n",
            "            pil_image = pil_image.convert('RGB')\n",
            "\n",
            "        rgb = pil_image.split()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\encoder.py",
        "line_number": 74,
        "API": ".save(",
        "context": [
            "\n",
            "        # if self.mode == RunMode.Trains and use_compress:\n",
            "        #     img_compress = io.BytesIO()\n",
            "        #\n",
            "        #     pil_image.convert('RGB').save(img_compress, format='JPEG', quality=random.randint(75, 100))\n",
            "        #     img_compress_bytes = img_compress.getvalue()\n",
            "        #     img_compress.close()\n",
            "        #     path_or_stream = io.BytesIO(img_compress_bytes)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\encoder.py",
        "line_number": 81,
        "API": ".format(",
        "context": [
            "        #     path_or_stream = io.BytesIO(img_compress_bytes)\n",
            "        #     pil_image = PIL.Image.open(path_or_stream)\n",
            "\n",
            "        if len(rgb) == 1 and self.model_conf.image_channel == 3:\n",
            "            return \"The number of image channels {} is inconsistent with the number of configured channels {}.\".format(\n",
            "                len(rgb), self.model_conf.image_channel\n",
            "            )\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\encoder.py",
        "line_number": 104,
        "API": ".convert(",
        "context": [
            "        #             size[1] + random_offset_h\n",
            "        #         ),\n",
            "        #         None\n",
            "        #     )\n",
            "        #     background.convert('RGB')\n",
            "        #     pil_image = background\n",
            "\n",
            "        if len(rgb) > 3 and self.model_conf.pre_replace_transparent and not gif_handle and not use_compress:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\encoder.py",
        "line_number": 111,
        "API": ".convert(",
        "context": [
            "        if len(rgb) > 3 and self.model_conf.pre_replace_transparent and not gif_handle and not use_compress:\n",
            "            background = PIL.Image.new('RGBA', pil_image.size, (255, 255, 255))\n",
            "            try:\n",
            "                background.paste(pil_image, (0, 0, size[0], size[1]), pil_image)\n",
            "                background.convert('RGB')\n",
            "                pil_image = background\n",
            "            except:\n",
            "                pil_image = pil_image.convert('RGB')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\encoder.py",
        "line_number": 116,
        "API": ".split(",
        "context": [
            "                pil_image = background\n",
            "            except:\n",
            "                pil_image = pil_image.convert('RGB')\n",
            "\n",
            "        if len(pil_image.split()) > 3 and self.model_conf.image_channel == 3:\n",
            "            pil_image = pil_image.convert('RGB')\n",
            "\n",
            "        if self.model_conf.pre_concat_frames != -1:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\encoder.py",
        "line_number": 124,
        "API": ".array(",
        "context": [
            "            im = concat_frames(pil_image, need_frame=self.model_conf.pre_concat_frames)\n",
            "        elif self.model_conf.pre_blend_frames != -1:\n",
            "            im = blend_frame(pil_image, need_frame=self.model_conf.pre_blend_frames)\n",
            "        else:\n",
            "            im = np.array(pil_image)\n",
            "\n",
            "        if isinstance(im, list):\n",
            "            return None\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\encoder.py",
        "line_number": 184,
        "API": ".array(",
        "context": [
            "            im = cv2.resize(im, (self.model_conf.resize[0], self.model_conf.resize[1]))\n",
            "        im = im.swapaxes(0, 1)\n",
            "\n",
            "        if self.model_conf.image_channel == 1:\n",
            "            return np.array((im[:, :, np.newaxis]) / 255.)\n",
            "        else:\n",
            "            return np.array(im[:, :]) / 255.\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\encoder.py",
        "line_number": 196,
        "API": ".lower(",
        "context": [
            "\n",
            "        found = content\n",
            "        # \u5982\u679c\u5339\u914d\u5185\u7f6e\u7684\u5927\u5c0f\u5199\u89c4\u8303\uff0c\u89e6\u53d1\u81ea\u52a8\u8f6c\u6362\n",
            "        if isinstance(self.category_param, str) and '_LOWER' in self.category_param:\n",
            "            found = found.lower()\n",
            "        if isinstance(self.category_param, str) and '_UPPER' in self.category_param:\n",
            "            found = found.upper()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\encoder.py",
        "line_number": 205,
        "API": ".split(",
        "context": [
            "            found = found.replace(\"x\", \"\u00d7\").replace('\uff1f', \"?\")\n",
            "\n",
            "        # \u6807\u7b7e\u662f\u5426\u5305\u542b\u5206\u9694\u7b26\n",
            "        if self.model_conf.label_split:\n",
            "            labels = found.split(self.model_conf.label_split)\n",
            "        elif '&' in found:\n",
            "            labels = found.split('&')\n",
            "        elif self.model_conf.max_label_num == 1:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\encoder.py",
        "line_number": 230,
        "API": ".format(",
        "context": [
            "\n",
            "        except KeyError as e:\n",
            "            return dict(e=e, label=content, char=e.args[0])\n",
            "            # exception(\n",
            "            #     'The sample label {} contains invalid charset: {}.'.format(\n",
            "            #         content, e.args[0]\n",
            "            #     ), ConfigException.SAMPLE_LABEL_ERROR\n",
            "            # )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\encoder.py",
        "line_number": 256,
        "API": ".get(",
        "context": [
            "        return content\n",
            "\n",
            "    @staticmethod\n",
            "    def filter_full_angle(content):\n",
            "        return [FULL_ANGLE_MAP.get(i) if i in FULL_ANGLE_MAP.keys() else i for i in content if i != ' ']\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\fuse_model.py",
        "line_number": 25,
        "API": ".split(",
        "context": [
            "    model_bytes_list = []\n",
            "    graph_bytes_list = []\n",
            "    slice_index = source_bytes.index(key[0])\n",
            "    split_tag_len = len(split_tag)\n",
            "    slice_0 = source_bytes[0: slice_index].split(split_tag)\n",
            "    model_slice_len = len(slice_0[1])\n",
            "    graph_slice_len = len(slice_0[0])\n",
            "    slice_len = split_tag_len + model_slice_len + graph_slice_len\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\fuse_model.py",
        "line_number": 33,
        "API": ".split(",
        "context": [
            "\n",
            "    for i in range(key_len_int-1):\n",
            "        slice_index = source_bytes.index(key[i])\n",
            "        print(slice_index, slice_index - slice_len)\n",
            "        slices = source_bytes[slice_index - slice_len: slice_index].split(split_tag)\n",
            "        model_bytes_list.append(slices[1])\n",
            "        graph_bytes_list.append(slices[0])\n",
            "    slices = source_bytes.split(key[-2])[1][:-len(key[-1])].split(split_tag)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\fuse_model.py",
        "line_number": 40,
        "API": ".join(",
        "context": [
            "    slices = source_bytes.split(key[-2])[1][:-len(key[-1])].split(split_tag)\n",
            "\n",
            "    model_bytes_list.append(slices[1])\n",
            "    graph_bytes_list.append(slices[0])\n",
            "    model_bytes = b\"\".join(model_bytes_list)\n",
            "    model_conf: ModelConfig = pickle.loads(model_bytes)\n",
            "    graph_bytes: bytes = b\"\".join(graph_bytes_list)\n",
            "    return model_conf, graph_bytes\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\fuse_model.py",
        "line_number": 63,
        "API": ".join(",
        "context": [
            "    for i in range(key_len_int):\n",
            "        new_model.append(graph_slice[i] + b'-#||#-')\n",
            "        new_model.append(model_slice[i])\n",
            "        new_model.append(key[i])\n",
            "    new_model = b\"\".join(new_model)\n",
            "    with open(output_path, \"wb\") as f:\n",
            "        f.write(new_model)\n",
            "    print(\"Successfully write to model {}\".format(output_path))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\fuse_model.py",
        "line_number": 75,
        "API": ".join(",
        "context": [
            "\n",
            "    graph_parent_path = model_conf.compile_model_path\n",
            "    model_suffix = COMPILE_MODEL_MAP[model_type]\n",
            "    model_bytes = pickle.dumps(model_conf.conf)\n",
            "    graph_path = os.path.join(graph_parent_path, \"{}{}\".format(model_conf.model_name, model_suffix))\n",
            "\n",
            "    with open(graph_path, \"rb\") as f:\n",
            "        graph_bytes = f.read()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\loss.py",
        "line_number": 15,
        "API": ".sparse_softmax_cross_entropy_with_logits(",
        "context": [
            "    def cross_entropy(labels, logits):\n",
            "        \"\"\"\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\"\"\"\n",
            "\n",
            "        # return tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels)\n",
            "        # return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
            "        # return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
            "        # return tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)\n",
            "        target = tf.sparse.to_dense(labels)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\loss.py",
        "line_number": 22,
        "API": ".shape(",
        "context": [
            "        target = tf.sparse.to_dense(labels)\n",
            "        # target = labels\n",
            "        print('logits', logits.shape)\n",
            "        print('target', target.shape)\n",
            "        # logits = tf.reshape(tensor=logits, shape=[tf.shape(labels)[0], None])\n",
            "        return tf.compat.v1.keras.backend.sparse_categorical_crossentropy(\n",
            "            target=target,\n",
            "            output=logits,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\make_dataset.py",
        "line_number": 20,
        "API": ".makedirs(",
        "context": [
            "    def __init__(self, model: ModelConfig):\n",
            "        self.ignore_list = [\"Thumbs.db\", \".DS_Store\"]\n",
            "        self.model: ModelConfig = model\n",
            "        if not os.path.exists(self.model.dataset_root_path):\n",
            "            os.makedirs(self.model.dataset_root_path)\n",
            "\n",
            "    @staticmethod\n",
            "    def read_image(path):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\make_dataset.py",
        "line_number": 54,
        "API": ".join(",
        "context": [
            "        if is_add:\n",
            "            output_filename = self.model.dataset_increasing_name(mode)\n",
            "            if not output_filename:\n",
            "                raise FileNotFoundError('Basic data set missing, please check.')\n",
            "            output_filename = os.path.join(self.model.dataset_root_path, output_filename)\n",
            "        with tf.io.TFRecordWriter(output_filename) as writer:\n",
            "            pbar = tqdm(file_list)\n",
            "            for i, file_name in enumerate(pbar):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\make_dataset.py",
        "line_number": 59,
        "API": ".split(",
        "context": [
            "        with tf.io.TFRecordWriter(output_filename) as writer:\n",
            "            pbar = tqdm(file_list)\n",
            "            for i, file_name in enumerate(pbar):\n",
            "                try:\n",
            "                    if file_name.split(\"/\")[-1] in self.ignore_list:\n",
            "                        continue\n",
            "                    image_data = self.read_image(file_name)\n",
            "                    try:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\make_dataset.py",
        "line_number": 68,
        "API": ".group(",
        "context": [
            "                    except re.error as e:\n",
            "                        print('error:', e)\n",
            "                        return\n",
            "                    if labels:\n",
            "                        labels = labels.group()\n",
            "                    else:\n",
            "                        tf.compat.v1.logging.warning('invalid filename {}, ignored.'.format(file_name))\n",
            "                        continue\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\make_dataset.py",
        "line_number": 76,
        "API": ".write(",
        "context": [
            "                        # raise NameError('invalid filename {}'.format(file_name))\n",
            "                    labels = labels.encode('utf-8')\n",
            "\n",
            "                    example = self.input_to_tfrecords(image_data, labels)\n",
            "                    writer.write(example.SerializeToString())\n",
            "                    pbar.set_description('[Processing dataset %s] [filename: %s]' % (mode, file_name))\n",
            "\n",
            "                except IOError as e:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\make_dataset.py",
        "line_number": 89,
        "API": ".join(",
        "context": [
            "        if is_add:\n",
            "            output_filename = self.model.dataset_increasing_name(mode)\n",
            "            if not output_filename:\n",
            "                raise FileNotFoundError('Basic data set missing, please check.')\n",
            "            output_filename = os.path.join(self.model.dataset_root_path, output_filename)\n",
            "        file_list, label_list = [], []\n",
            "        for line in label_lines:\n",
            "            filename, label = line.split(\" \", 1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\make_dataset.py",
        "line_number": 95,
        "API": ".join(",
        "context": [
            "        for line in label_lines:\n",
            "            filename, label = line.split(\" \", 1)\n",
            "            label = label.replace(\"\\n\", \"\")\n",
            "            label_list.append(label.encode('utf-8'))\n",
            "            path = os.path.join(file_path, filename)\n",
            "            file_list.append(path)\n",
            "\n",
            "        if os.path.exists(output_filename):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\make_dataset.py",
        "line_number": 109,
        "API": ".write(",
        "context": [
            "                try:\n",
            "                    image_data = self.read_image(file_name)\n",
            "                    labels = label_list[i]\n",
            "                    example = self.input_to_tfrecords(image_data, labels)\n",
            "                    writer.write(example.SerializeToString())\n",
            "                    pbar.set_description('[Processing dataset %s] [filename: %s]' % (mode, file_name))\n",
            "                except IOError as e:\n",
            "                    print('could not read:', file_list[1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\make_dataset.py",
        "line_number": 122,
        "API": ".join(",
        "context": [
            "        if isinstance(source, list):\n",
            "            origin_dataset = []\n",
            "            for trains_path in source:\n",
            "                origin_dataset += [\n",
            "                    os.path.join(trains_path, trains).replace(\"\\\\\", \"/\") for trains in os.listdir(trains_path)\n",
            "                ]\n",
            "        elif isinstance(source, str):\n",
            "            origin_dataset = [os.path.join(source, trains) for trains in os.listdir(source)]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\make_dataset.py",
        "line_number": 128,
        "API": ".seed(",
        "context": [
            "        elif isinstance(source, str):\n",
            "            origin_dataset = [os.path.join(source, trains) for trains in os.listdir(source)]\n",
            "        else:\n",
            "            return\n",
            "        random.seed(0)\n",
            "        random.shuffle(origin_dataset)\n",
            "        return origin_dataset\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\make_dataset.py",
        "line_number": 173,
        "API": ".join(",
        "context": [
            "                    is_add=is_add,\n",
            "                )\n",
            "            elif self.model.label_from == LabelFrom.TXT:\n",
            "\n",
            "                train_label_file = os.path.join(os.path.dirname(trains_path[0]), \"train.txt\")\n",
            "                val_label_file = os.path.join(os.path.dirname(validation_path[0]), \"val.txt\")\n",
            "\n",
            "                if not os.path.exists(train_label_file) or not os.path.exists(val_label_file):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\make_dataset.py",
        "line_number": 230,
        "API": ".join(",
        "context": [
            "                    is_add=is_add\n",
            "                )\n",
            "            elif self.model.label_from == LabelFrom.TXT:\n",
            "\n",
            "                train_label_file = os.path.join(os.path.dirname(trains_path[0]), \"train.txt\")\n",
            "\n",
            "                if not os.path.exists(train_label_file):\n",
            "                    msg(\"Train label file not found!\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\pretreatment.py",
        "line_number": 117,
        "API": ".zeros(",
        "context": [
            "    #     return dst\n",
            "\n",
            "    def sp_noise(self, prob, modify=False):\n",
            "        size = self.origin.shape\n",
            "        output = np.zeros(self.origin.shape, np.uint8)\n",
            "        thres = 1 - prob\n",
            "        for i in range(size[0]):\n",
            "            for j in range(size[1]):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\pretreatment.py",
        "line_number": 133,
        "API": ".uniform(",
        "context": [
            "            self.origin = output\n",
            "        return output\n",
            "\n",
            "    def random_brightness(self, modify=False):\n",
            "        beta = np.random.uniform(-84, 84)\n",
            "        output = np.uint8(np.clip((self.origin + beta), 0, 255))\n",
            "        if modify:\n",
            "            self.origin = output\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\pretreatment.py",
        "line_number": 142,
        "API": ".uniform(",
        "context": [
            "\n",
            "    def random_saturation(self, modify=False):\n",
            "        if len(self.origin.shape) < 3:\n",
            "            return self.origin\n",
            "        factor = np.random.uniform(0.3, 2.0)\n",
            "        output = self.origin\n",
            "        output[:, :, 1] = np.clip(output[:, :, 1] * factor, 0, 255)\n",
            "        if modify:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\pretreatment.py",
        "line_number": 152,
        "API": ".uniform(",
        "context": [
            "\n",
            "    def random_hue(self, max_delta=18, modify=False):\n",
            "        if len(self.origin.shape) < 3:\n",
            "            return self.origin\n",
            "        delta = np.random.uniform(-max_delta, max_delta)\n",
            "        output = self.origin\n",
            "        output[:, :, 0] = (output[:, :, 0] + delta) % 180.0\n",
            "        if modify:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\pretreatment.py",
        "line_number": 162,
        "API": ".uniform(",
        "context": [
            "\n",
            "    def random_gamma(self, modify=False):\n",
            "        if len(self.origin.shape) < 3:\n",
            "            return self.origin\n",
            "        gamma = np.random.uniform(0.25, 2.0)\n",
            "        gamma_inv = 1.0 / gamma\n",
            "        table = np.array([((i / 255.0) ** gamma_inv) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
            "        output = cv2.LUT(self.origin, table)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\pretreatment.py",
        "line_number": 189,
        "API": ".ones(",
        "context": [
            "            return self.origin\n",
            "\n",
            "        new_shape = list(self.origin.shape)\n",
            "        new_shape[0] = random.randint(1, 15)\n",
            "        blank_down = np.ones(new_shape, dtype=np.uint8) * 255\n",
            "        output = np.concatenate([self.origin, blank_down], axis=0)\n",
            "        new_shape[0] = random.randint(1, 15)\n",
            "        blank_up = np.ones(new_shape, dtype=np.uint8) * 255\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\pretreatment.py",
        "line_number": 196,
        "API": ".ones(",
        "context": [
            "        blank_up = np.ones(new_shape, dtype=np.uint8) * 255\n",
            "        output = np.concatenate([blank_up, output], axis=0)\n",
            "        new_shape = list(output.shape)\n",
            "        new_shape[1] = random.randint(1, 15)\n",
            "        blank_left = np.ones(new_shape, dtype=np.uint8) * 255\n",
            "        output = np.concatenate([blank_left, output], axis=1)\n",
            "        new_shape[1] = random.randint(1, 15)\n",
            "        blank_right = np.ones(new_shape, dtype=np.uint8) * 255\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\pretreatment.py",
        "line_number": 388,
        "API": ".get(",
        "context": [
            "    if random_saturation and (bool(random.getrandbits(1)) or not is_random):\n",
            "        pretreatment.random_saturation(True)\n",
            "    if random_hue and (bool(random.getrandbits(1)) or not is_random):\n",
            "        pretreatment.random_hue(18, True)\n",
            "    return pretreatment.get()\n",
            "\n",
            "\n",
            "def preprocessing_by_func(exec_map: dict, src_arr, key=None):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\pretreatment.py",
        "line_number": 397,
        "API": ".get(",
        "context": [
            "        return src_arr\n",
            "    target_arr = cv2.cvtColor(src_arr, cv2.COLOR_RGB2BGR)\n",
            "    if not key:\n",
            "        key = random.choice(list(exec_map.keys()))\n",
            "    for sentence in exec_map.get(key):\n",
            "        if sentence.startswith(\"@@\"):\n",
            "            target_arr = eval(sentence[2:])\n",
            "        elif sentence.startswith(\"$$\"):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\pretreatment.py",
        "line_number": 416,
        "API": ".makedirs(",
        "context": [
            "\n",
            "    root_dir = r\"H:\\TrainSet\\\u751f\u62101\\\u5355\u5b57\\333\\default\"\n",
            "    target_dir = r\"H:\\TrainSet\\\u751f\u62101\\\u5355\u5b57\\333\\default-555\"\n",
            "    if not os.path.exists(target_dir):\n",
            "        os.makedirs(target_dir)\n",
            "    # name = random.choice(os.listdir(root_dir))\n",
            "    # name = \"3956_b8cee4da-3530-11ea-9778-c2f9192435fa.png\"\n",
            "    for i, name in enumerate(os.listdir(root_dir)):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\pretreatment.py",
        "line_number": 422,
        "API": ".split(",
        "context": [
            "    # name = \"3956_b8cee4da-3530-11ea-9778-c2f9192435fa.png\"\n",
            "    for i, name in enumerate(os.listdir(root_dir)):\n",
            "        path = os.path.join(root_dir, name)\n",
            "        if name.count('_') > 1:\n",
            "            label = name.split(\"_\")[0:-1]\n",
            "            label = \"_\".join(label)\n",
            "        else:\n",
            "            label = name.split(\"_\")[0]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\pretreatment.py",
        "line_number": 444,
        "API": ".array(",
        "context": [
            "        #     continue\n",
            "        # offset = random.randint(5, 9)\n",
            "        # pil_image = pil_image.crop([offset, offset, size[0]-offset, size[1]-offset])\n",
            "        # im = concat_frames(pil_image, [16, 47])\n",
            "        im = np.array(pil_image)\n",
            "        print(im.shape)\n",
            "        # im = preprocessing_by_func(exec_map={\n",
            "        #     \"black\": [\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\pretreatment.py",
        "line_number": 475,
        "API": ".convert(",
        "context": [
            "\n",
            "        # background = PIL.Image.new('RGBA', pil_image.size, (255, 255, 255))\n",
            "        # try:\n",
            "        #     background.paste(pil_image, (0, 0, size[0], size[1]), pil_image)\n",
            "        #     background.convert('RGB')\n",
            "        #     pil_image = background\n",
            "        # except:\n",
            "        #     pil_image = pil_image.convert('RGB')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\pretreatment.py",
        "line_number": 507,
        "API": ".format(",
        "context": [
            "        im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)\n",
            "        cv_img = cv2.imencode('.png', im)[1]\n",
            "        img_bytes = bytes(bytearray(cv_img))\n",
            "        tag = hashlib.md5(img_bytes).hexdigest()\n",
            "        new_name = \"{}_{}.png\".format(label, tag)\n",
            "        with open(os.path.join(target_dir, new_name), \"wb\") as f:\n",
            "            f.write(img_bytes)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\tf_graph_util.py",
        "line_number": 105,
        "API": ".device(",
        "context": [
            "\n",
            "\n",
            "################################################################################\n",
            "#\n",
            "# device functions for use in with g.device(...)\n",
            "#\n",
            "################################################################################\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\tf_graph_util.py",
        "line_number": 114,
        "API": ".split(",
        "context": [
            "def _node_name(n):\n",
            "    if n.startswith(\"^\"):\n",
            "        return n[1:]\n",
            "    else:\n",
            "        return n.split(\":\")[0]\n",
            "\n",
            "\n",
            "def _extract_graph_summary(graph_def):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\tf_graph_util.py",
        "line_number": 159,
        "API": ".add(",
        "context": [
            "        del next_to_visit[0]\n",
            "        if node in nodes_to_keep:\n",
            "            # Already visited this node.\n",
            "            continue\n",
            "        nodes_to_keep.add(node)\n",
            "        if node in name_to_input_name:\n",
            "            next_to_visit += name_to_input_name[node]\n",
            "    return nodes_to_keep\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\tf_graph_util.py",
        "line_number": 256,
        "API": ".add(",
        "context": [
            "        control_ops = [\"Enter\", \"Exit\", \"NextIteration\", \"Switch\"]\n",
            "\n",
            "        current_name = origin_name\n",
            "        while name_to_nodes[current_name].op != \"VarHandleOp\":\n",
            "            nodes_in_path.add(current_name)\n",
            "            current_node = name_to_nodes[current_name]\n",
            "            op_name = current_node.op\n",
            "            if op_name in control_ops or op_name == \"Identity\":\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\tf_graph_util.py",
        "line_number": 323,
        "API": ".info(",
        "context": [
            "        returned_variables = sess.run(variable_names)\n",
            "    else:\n",
            "        returned_variables = []\n",
            "    variables_data_map = dict(zip(variable_dict_names, returned_variables))\n",
            "    logging.info(\"Froze %d variables.\", len(returned_variables))\n",
            "\n",
            "    # Reconstruct the graph with constants in place of variables.\n",
            "    output_graph_def = graph_pb2.GraphDef()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\tf_graph_util.py",
        "line_number": 382,
        "API": ".info(",
        "context": [
            "            output_node.CopyFrom(input_node)\n",
            "        output_graph_def.node.extend([output_node])\n",
            "\n",
            "    output_graph_def.library.CopyFrom(inference_graph.library)\n",
            "    logging.info(\"Converted %d variables to const ops.\", how_many_converted)\n",
            "    return output_graph_def\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\tf_graph_util.py",
        "line_number": 426,
        "API": ".sub(",
        "context": [
            "        new_node.CopyFrom(node)\n",
            "        input_before_removal = node.input\n",
            "        del new_node.input[:]\n",
            "        for full_input_name in input_before_removal:\n",
            "            input_name = re.sub(r\"^\\^\", \"\", full_input_name)\n",
            "            if input_name in names_to_remove:\n",
            "                continue\n",
            "            new_node.input.append(full_input_name)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\tf_graph_util.py",
        "line_number": 438,
        "API": ".add(",
        "context": [
            "    node_names_with_control_input = set()\n",
            "    for node in nodes_after_removal:\n",
            "        for node_input in node.input:\n",
            "            if \"^\" in node_input:\n",
            "                control_input_names.add(node_input.replace(\"^\", \"\"))\n",
            "                node_names_with_control_input.add(node.name)\n",
            "\n",
            "    names_to_splice = {}\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\tf_graph_util.py",
        "line_number": 463,
        "API": ".sub(",
        "context": [
            "        new_node.CopyFrom(node)\n",
            "        input_before_removal = node.input\n",
            "        del new_node.input[:]\n",
            "        for full_input_name in input_before_removal:\n",
            "            input_name = re.sub(r\"^\\^\", \"\", full_input_name)\n",
            "            while input_name in names_to_splice:\n",
            "                full_input_name = names_to_splice[input_name]\n",
            "                input_name = re.sub(r\"^\\^\", \"\", full_input_name)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\tf_onnx_util2.py",
        "line_number": 46,
        "API": ".split(",
        "context": [
            "\n",
            "\n",
            "def freeze_session(sess, keep_var_names=None, output_names=None, clear_devices=True):\n",
            "    \"\"\"Freezes the state of a session into a pruned computation graph.\"\"\"\n",
            "    output_names = [i.split(':')[:-1][0] for i in output_names]\n",
            "    graph = sess.graph\n",
            "    with graph.as_default():\n",
            "        freeze_var_names = list(set(v.op.name for v in tf.compat.v1.global_variables()).difference(keep_var_names or []))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\tf_onnx_util2.py",
        "line_number": 70,
        "API": ".join(",
        "context": [
            "            if utils.node_name(inp) == n.name:\n",
            "                frozen_inputs.append(inp)\n",
            "    deleted_inputs = list(set(input_names) - set(frozen_inputs))\n",
            "    if deleted_inputs:\n",
            "        logger.warning(\"inputs [%s] is not in frozen graph, delete them\", \",\".join(deleted_inputs))\n",
            "    return frozen_inputs\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\tf_onnx_util2.py",
        "line_number": 93,
        "API": ".split(",
        "context": [
            "\n",
            "    if inputs_op:\n",
            "        inputs_op, shape_override = utils.split_nodename_and_shape(inputs_op)\n",
            "    if outputs_op:\n",
            "        outputs_op = outputs_op.split(\",\")\n",
            "\n",
            "    # logging.basicConfig(level=logging.get_verbosity_level(True))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\tf_onnx_util2.py",
        "line_number": 109,
        "API": ".join(",
        "context": [
            "        tf.compat.v1.import_graph_def(graph_def, name='')\n",
            "    with tf.compat.v1.Session(graph=tf_graph):\n",
            "        g = process_tf_graph(tf_graph,\n",
            "                             continue_on_error=True,\n",
            "                             target=\",\".join(constants.DEFAULT_TARGET),\n",
            "                             opset=9,\n",
            "                             custom_op_handlers=None,\n",
            "                             extra_opset=None,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\tf_onnx_util2.py",
        "line_number": 119,
        "API": ".format(",
        "context": [
            "                             output_names=outputs_op,\n",
            "                             inputs_as_nchw=None)\n",
            "\n",
            "    onnx_graph = optimizer.optimize_graph(g)\n",
            "    model_proto = onnx_graph.make_model(\"converted from {}\".format(model_path))\n",
            "\n",
            "    # write onnx graph\n",
            "    logger.info(\"\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\tf_onnx_util2.py",
        "line_number": 127,
        "API": ".info(",
        "context": [
            "    logger.info(\"Successfully converted TensorFlow model %s to ONNX\", model_path)\n",
            "    # if args.output:\n",
            "    output_path = input_path.replace(\".pb\", \".onnx\")\n",
            "    utils.save_protobuf(output_path, model_proto)\n",
            "    logger.info(\"ONNX model is saved at %s\", output_path)\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\trains.py",
        "line_number": 7,
        "API": ".list_physical_devices(",
        "context": [
            "import tensorflow as tf\n",
            "tf.compat.v1.disable_v2_behavior()\n",
            "tf.compat.v1.disable_eager_execution()\n",
            "try:\n",
            "    gpus = tf.config.list_physical_devices('GPU')\n",
            "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
            "\n",
            "except Exception as e:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\trains.py",
        "line_number": 23,
        "API": ".set_verbosity(",
        "context": [
            "from PIL import ImageFile\n",
            "# if build_info['cuda_version'] == '64_110':\n",
            "\n",
            "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
            "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
            "\n",
            "\n",
            "class Trains:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\trains.py",
        "line_number": 61,
        "API": ".info(",
        "context": [
            "            model.build_graph()\n",
            "            model.build_train_op()\n",
            "            input_graph_def = predict_sess.graph.as_graph_def()\n",
            "            saver = tf.compat.v1.train.Saver(var_list=tf.compat.v1.global_variables())\n",
            "            tf.compat.v1.logging.info(tf.train.latest_checkpoint(self.model_conf.model_root_path))\n",
            "            saver.restore(predict_sess, tf.train.latest_checkpoint(self.model_conf.model_root_path))\n",
            "\n",
            "            output_graph_def = convert_variables_to_constants(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\trains.py",
        "line_number": 71,
        "API": ".makedirs(",
        "context": [
            "                output_node_names=['dense_decoded']\n",
            "            )\n",
            "\n",
            "        if not os.path.exists(self.model_conf.compile_model_path):\n",
            "            os.makedirs(self.model_conf.compile_model_path)\n",
            "\n",
            "        last_compile_model_path = (\n",
            "            os.path.join(self.model_conf.compile_model_path, \"{}.pb\".format(self.model_conf.model_name))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\trains.py",
        "line_number": 77,
        "API": ".format(",
        "context": [
            "        last_compile_model_path = (\n",
            "            os.path.join(self.model_conf.compile_model_path, \"{}.pb\".format(self.model_conf.model_name))\n",
            "        ).replace('.pb', '_{}.pb'.format(int(acc * 10000)))\n",
            "\n",
            "        self.model_conf.output_config(target_model_name=\"{}_{}\".format(self.model_conf.model_name, int(acc * 10000)))\n",
            "        with tf.io.gfile.GFile(last_compile_model_path, mode='wb') as gf:\n",
            "            gf.write(output_graph_def.SerializeToString())\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\trains.py",
        "line_number": 96,
        "API": ".join(",
        "context": [
            "        path = self.model_conf.da_random_captcha['FontPath']\n",
            "        if not os.path.exists(path):\n",
            "            exception(\"Font path does not exist.\", code=-6754)\n",
            "        items = os.listdir(path)\n",
            "        fonts = [os.path.join(path, item) for item in items]\n",
            "        ran_captcha.sample = NUMBER + ALPHA_UPPER + ALPHA_LOWER\n",
            "        ran_captcha.fonts_list = fonts\n",
            "        ran_captcha.check_font()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\trains.py",
        "line_number": 121,
        "API": ".info(",
        "context": [
            "            recurrent=self.model_conf.neu_recurrent\n",
            "        )\n",
            "        model.build_graph()\n",
            "\n",
            "        tf.compat.v1.logging.info('Loading Trains DataSet...')\n",
            "        train_feeder = utils.data.DataIterator(\n",
            "            model_conf=self.model_conf, mode=RunMode.Trains\n",
            "        )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\trains.py",
        "line_number": 127,
        "API": ".info(",
        "context": [
            "            model_conf=self.model_conf, mode=RunMode.Trains\n",
            "        )\n",
            "        train_feeder.read_sample_from_tfrecords(self.model_conf.trains_path[DatasetType.TFRecords])\n",
            "\n",
            "        tf.compat.v1.logging.info('Loading Validation DataSet...')\n",
            "        validation_feeder = utils.data.DataIterator(\n",
            "            model_conf=self.model_conf, mode=RunMode.Validation\n",
            "        )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\trains.py",
        "line_number": 133,
        "API": ".info(",
        "context": [
            "            model_conf=self.model_conf, mode=RunMode.Validation\n",
            "        )\n",
            "        validation_feeder.read_sample_from_tfrecords(self.model_conf.validation_path[DatasetType.TFRecords])\n",
            "\n",
            "        tf.compat.v1.logging.info('Total {} Trains DataSets'.format(train_feeder.size))\n",
            "        tf.compat.v1.logging.info('Total {} Validation DataSets'.format(validation_feeder.size))\n",
            "        if validation_feeder.size >= train_feeder.size:\n",
            "            exception(\"The number of training sets cannot be less than the validation set.\", )\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\trains.py",
        "line_number": 145,
        "API": ".warn(",
        "context": [
            "        num_validation_samples = validation_feeder.size\n",
            "\n",
            "        if num_validation_samples < self.model_conf.validation_batch_size:\n",
            "            self.model_conf.validation_batch_size = num_validation_samples\n",
            "            tf.compat.v1.logging.warn(\n",
            "                'The number of validation sets is less than the validation batch size, '\n",
            "                'will use validation set size as validation batch size.'.format(validation_feeder.size))\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\trains.py",
        "line_number": 184,
        "API": ".restore(",
        "context": [
            "        # try:\n",
            "        checkpoint_state = tf.train.get_checkpoint_state(self.model_conf.model_root_path)\n",
            "        if checkpoint_state and checkpoint_state.model_checkpoint_path:\n",
            "            # \u52a0\u8f7d\u88ab\u4e2d\u65ad\u7684\u8bad\u7ec3\u4efb\u52a1\n",
            "            saver.restore(sess, checkpoint_state.model_checkpoint_path)\n",
            "\n",
            "        tf.compat.v1.logging.info('Start training...')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\trains.py",
        "line_number": 218,
        "API": ".info(",
        "context": [
            "                )\n",
            "                train_writer.add_summary(summary_str, step)\n",
            "\n",
            "                if step % save_step == 0 and step != 0:\n",
            "                    tf.compat.v1.logging.info(\n",
            "                        'Step: {} Time: {:.3f} sec/batch, Cost = {:.8f}, BatchSize: {}, Shape[1]: {}'.format(\n",
            "                            step,\n",
            "                            time.time() - batch_time,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\trains.py",
        "line_number": 230,
        "API": ".save(",
        "context": [
            "                    )\n",
            "\n",
            "                # \u8fbe\u5230\u4fdd\u5b58\u6b65\u6570\u5bf9\u6a21\u578b\u8fc7\u7a0b\u8fdb\u884c\u5b58\u50a8\n",
            "                if step % save_step == 0 and step != 0:\n",
            "                    saver.save(sess, self.model_conf.save_model, global_step=step)\n",
            "\n",
            "                # \u8fdb\u5165\u9a8c\u8bc1\u96c6\u9a8c\u8bc1\u73af\u8282\n",
            "                if step % trains_validation_steps == 0 and step != 0:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\trains.py",
        "line_number": 255,
        "API": ".info(",
        "context": [
            "                        dense_decoded,\n",
            "                    )\n",
            "                    log = \"Epoch: {}, Step: {}, Accuracy = {:.4f}, Cost = {:.5f}, \" \\\n",
            "                          \"Time = {:.3f} sec/batch, LearningRate: {}\"\n",
            "                    tf.compat.v1.logging.info(log.format(\n",
            "                        epoch_count,\n",
            "                        step,\n",
            "                        accuracy,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\trains.py",
        "line_number": 276,
        "API": ".info(",
        "context": [
            "                # sess.close()\n",
            "                tf.compat.v1.keras.backend.clear_session()\n",
            "                sess.close()\n",
            "                self.compile_graph(accuracy)\n",
            "                tf.compat.v1.logging.info('Total Time: {} sec.'.format(time.time() - start_time))\n",
            "\n",
            "                break\n",
            "            epoch_count += 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\trains.py",
        "line_number": 287,
        "API": ".info(",
        "context": [
            "def main(argv):\n",
            "    project_name = argv[-1]\n",
            "    model_conf = ModelConfig(project_name=project_name)\n",
            "    Trains(model_conf).train_process()\n",
            "    tf.compat.v1.logging.info('Training completed.')\n",
            "    pass\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\trains.py",
        "line_number": 292,
        "API": ".set_verbosity(",
        "context": [
            "    pass\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    # tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
            "    tf.compat.v1.app.run()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\validation.py",
        "line_number": 34,
        "API": ".error(",
        "context": [
            "        original_seq_len = len(original_seq)\n",
            "        decoded_seq_len = len(decoded_seq)\n",
            "\n",
            "        if original_seq_len != decoded_seq_len:\n",
            "            tf.compat.v1.logging.error(original_seq)\n",
            "            tf.compat.v1.logging.error(decoded_seq)\n",
            "            tf.compat.v1.logging.error('original lengths {} is different from the decoded_seq {}, please check again'.format(\n",
            "                original_seq_len,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\validation.py",
        "line_number": 54,
        "API": ".info(",
        "context": [
            "            processed_decoded_label = [j for j in decoded_label if j not in ignore_value]\n",
            "            processed_origin_label = [j for j in origin_label if j not in ignore_value]\n",
            "\n",
            "            if i < 5:\n",
            "                tf.compat.v1.logging.info(\n",
            "                    \"{} {} {} {} {} --> {} {}\".format(\n",
            "                        i,\n",
            "                        len(processed_origin_label),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\validation.py",
        "line_number": 71,
        "API": ".join(",
        "context": [
            "            # Training is not useful for decoding\n",
            "            # Here is for debugging, positioning error source use\n",
            "            if processed_origin_label != processed_decoded_label and len(error_sample) < 5:\n",
            "                error_sample.append({\n",
            "                    \"origin\": \"\".join([self.category[_] if _ != self.category_num else '-' for _ in origin_label if _ != -1]),\n",
            "                    \"decode\": \"\".join([self.category[_] if _ != self.category_num else '-' for _ in decoded_label if _ != -1])\n",
            "                })\n",
            "        tf.compat.v1.logging.error(json.dumps(error_sample, ensure_ascii=False))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\compat\\upgrade.py",
        "line_number": 39,
        "API": ".get(",
        "context": [
            "        self.assignment()\n",
            "\n",
            "    def assignment(self):\n",
            "\n",
            "        system = self.cf_model.get('System')\n",
            "        self.device = system.get('Device') if system else None\n",
            "        self.device = self.device if self.device else \"cpu:0\"\n",
            "        self.device_usage = system.get('DeviceUsage') if system else None\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\compat\\upgrade.py",
        "line_number": 44,
        "API": ".get(",
        "context": [
            "        self.device = system.get('Device') if system else None\n",
            "        self.device = self.device if self.device else \"cpu:0\"\n",
            "        self.device_usage = system.get('DeviceUsage') if system else None\n",
            "        self.device_usage = self.device_usage if self.device_usage else 0.02\n",
            "        self.charset = self.cf_model['Model'].get('CharSet')\n",
            "        self.char_exclude = self.cf_model['Model'].get('CharExclude')\n",
            "        self.model_name = self.cf_model['Model'].get('ModelName')\n",
            "        self.model_type = self.cf_model['Model'].get('ModelType')\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\compat\\upgrade.py",
        "line_number": 50,
        "API": ".get(",
        "context": [
            "        self.model_name = self.cf_model['Model'].get('ModelName')\n",
            "        self.model_type = self.cf_model['Model'].get('ModelType')\n",
            "        self.model_site = self.cf_model['Model'].get('Sites')\n",
            "        self.model_site = self.model_site if self.model_site else []\n",
            "        self.version = self.cf_model['Model'].get('Version')\n",
            "        self.version = self.version if self.version else 1.0\n",
            "        self.split_char = self.cf_model['Model'].get('SplitChar')\n",
            "        self.split_char = '' if not self.split_char else self.split_char\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\compat\\upgrade.py",
        "line_number": 55,
        "API": ".get(",
        "context": [
            "        self.version = self.version if self.version else 1.0\n",
            "        self.split_char = self.cf_model['Model'].get('SplitChar')\n",
            "        self.split_char = '' if not self.split_char else self.split_char\n",
            "\n",
            "        self.image_height = self.cf_model['Model'].get('ImageHeight')\n",
            "        self.image_width = self.cf_model['Model'].get('ImageWidth')\n",
            "        self.image_channel = self.cf_model['Model'].get('ImageChannel')\n",
            "        self.image_channel = self.image_channel if self.image_channel else 1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\compat\\upgrade.py",
        "line_number": 60,
        "API": ".get(",
        "context": [
            "        self.image_width = self.cf_model['Model'].get('ImageWidth')\n",
            "        self.image_channel = self.cf_model['Model'].get('ImageChannel')\n",
            "        self.image_channel = self.image_channel if self.image_channel else 1\n",
            "        self.binaryzation = self.cf_model['Pretreatment'].get('Binaryzation')\n",
            "        self.resize = self.cf_model['Pretreatment'].get('Resize')\n",
            "        self.resize = self.resize if self.resize else [self.image_width, self.image_height]\n",
            "        self.replace_transparent = self.cf_model['Pretreatment'].get('ReplaceTransparent')\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\compat\\upgrade.py",
        "line_number": 68,
        "API": ".load(",
        "context": [
            "    @property\n",
            "    def read_conf(self):\n",
            "        with open(self.model_conf, 'r', encoding=\"utf-8\") as sys_fp:\n",
            "            sys_stream = sys_fp.read()\n",
            "            return yaml.load(sys_stream, Loader=yaml.SafeLoader)\n",
            "\n",
            "    def convert(self):\n",
            "        with open(\"../model.template\", encoding=\"utf8\") as f:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\compat\\upgrade.py",
        "line_number": 73,
        "API": ".join(",
        "context": [
            "\n",
            "    def convert(self):\n",
            "        with open(\"../model.template\", encoding=\"utf8\") as f:\n",
            "            lines = f.readlines()\n",
            "            bc = \"\".join(lines)\n",
            "            model = bc.format(\n",
            "                MemoryUsage=0.7,\n",
            "                CNNNetwork='CNNX',\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\compat\\upgrade.py",
        "line_number": 132,
        "API": ".write(",
        "context": [
            "                Pre_BlendFrames=\"-1\",\n",
            "                DA_RandomCaptcha=\"\",\n",
            "                Pre_ExecuteMap=\"\",\n",
            "            )\n",
            "        open(self.model_conf.replace(\".yaml\", \"_2.0.yaml\"), \"w\", encoding=\"utf8\").write(model)\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\fc\\cnn.py",
        "line_number": 34,
        "API": ".reshape(",
        "context": [
            "            units=self.category_num,\n",
            "        )(inputs=outputs)\n",
            "\n",
            "        print(\"output to reshape ----------- \", self.outputs.shape)\n",
            "        self.outputs = tf.reshape(self.outputs, [-1, self.max_label_num, self.category_num], name=\"predict\")\n",
            "\n",
            "    def build(self):\n",
            "        return self.outputs\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\fc\\rnn.py",
        "line_number": 24,
        "API": ".transpose(",
        "context": [
            "            bias_initializer='zeros',\n",
            "        )\n",
            "\n",
            "        self.outputs = self.dense(outputs)\n",
            "        self.predict = tf.compat.v1.transpose(self.outputs, perm=(1, 0, 2), name=\"predict\")\n",
            "        # self.predict = tf.keras.backend.permute_dimensions(self.outputs, pattern=(1, 0, 2))\n",
            "\n",
            "    def build(self):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\gui\\data_augmentation.py",
        "line_number": 364,
        "API": ".get(",
        "context": [
            "        if self.da_random_captcha['Enable']:\n",
            "            self.random_captcha_font_val.set(self.da_random_captcha['FontPath'])\n",
            "\n",
            "    def save_conf(self):\n",
            "        self.data_augmentation_entity.binaryzation = json.loads(self.binaryzation_val.get()) if self.binaryzation_val else []\n",
            "        self.data_augmentation_entity.median_blur = self.median_blur_val.get()\n",
            "        self.data_augmentation_entity.gaussian_blur = self.gaussian_blur_val.get()\n",
            "        self.data_augmentation_entity.rotate = self.rotate_val.get()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\gui\\data_augmentation.py",
        "line_number": 369,
        "API": ".get(",
        "context": [
            "        self.data_augmentation_entity.median_blur = self.median_blur_val.get()\n",
            "        self.data_augmentation_entity.gaussian_blur = self.gaussian_blur_val.get()\n",
            "        self.data_augmentation_entity.rotate = self.rotate_val.get()\n",
            "        self.data_augmentation_entity.sp_noise = self.sp_noise_val.get()\n",
            "        self.data_augmentation_entity.random_blank = self.random_blank_val.get()\n",
            "        self.data_augmentation_entity.random_transition = self.random_transition_val.get()\n",
            "\n",
            "        if self.random_captcha_font_val.get():\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\gui\\data_augmentation.py",
        "line_number": 374,
        "API": ".get(",
        "context": [
            "        self.data_augmentation_entity.random_transition = self.random_transition_val.get()\n",
            "\n",
            "        if self.random_captcha_font_val.get():\n",
            "            self.data_augmentation_entity.random_captcha['Enable'] = True\n",
            "            self.data_augmentation_entity.random_captcha['FontPath'] = self.random_captcha_font_val.get()\n",
            "        else:\n",
            "            self.data_augmentation_entity.random_captcha['Enable'] = False\n",
            "            self.data_augmentation_entity.random_captcha['FontPath'] = \"\"\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\gui\\data_augmentation.py",
        "line_number": 379,
        "API": ".get(",
        "context": [
            "        else:\n",
            "            self.data_augmentation_entity.random_captcha['Enable'] = False\n",
            "            self.data_augmentation_entity.random_captcha['FontPath'] = \"\"\n",
            "\n",
            "        self.data_augmentation_entity.equalize_hist = True if self.equalize_hist_val.get() == 1 else False\n",
            "        self.data_augmentation_entity.laplace = True if self.laplace_val.get() == 1 else False\n",
            "        self.data_augmentation_entity.warp_perspective = True if self.warp_perspective_val.get() == 1 else False\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\gui\\data_augmentation.py",
        "line_number": 384,
        "API": ".get(",
        "context": [
            "        self.data_augmentation_entity.laplace = True if self.laplace_val.get() == 1 else False\n",
            "        self.data_augmentation_entity.warp_perspective = True if self.warp_perspective_val.get() == 1 else False\n",
            "\n",
            "        self.data_augmentation_entity.brightness = True if self.brightness_val.get() == 1 else False\n",
            "        self.data_augmentation_entity.saturation = True if self.saturation_val.get() == 1 else False\n",
            "        self.data_augmentation_entity.hue = True if self.hue_val.get() == 1 else False\n",
            "        self.data_augmentation_entity.gamma = True if self.gamma_val.get() == 1 else False\n",
            "        self.data_augmentation_entity.channel_swap = True if self.channel_swap_val.get() == 1 else False\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\gui\\pretreatment.py",
        "line_number": 176,
        "API": ".get(",
        "context": [
            "        )\n",
            "\n",
            "    @staticmethod\n",
            "    def check_btn_event(src: tk.IntVar, entry: tk.Entry):\n",
            "        if src.get() == 1:\n",
            "            entry['state'] = tk.NORMAL\n",
            "        else:\n",
            "            entry['state'] = tk.DISABLED\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\gui\\pretreatment.py",
        "line_number": 217,
        "API": ".get(",
        "context": [
            "\n",
            "    def save_conf(self):\n",
            "        try:\n",
            "\n",
            "            if self.concat_frames_check_val.get() == 1:\n",
            "                self.pretreatment_entity.concat_frames = json.loads(self.concat_frames_val.get())\n",
            "            else:\n",
            "                self.pretreatment_entity.concat_frames = -1\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\gui\\pretreatment.py",
        "line_number": 222,
        "API": ".get(",
        "context": [
            "                self.pretreatment_entity.concat_frames = json.loads(self.concat_frames_val.get())\n",
            "            else:\n",
            "                self.pretreatment_entity.concat_frames = -1\n",
            "            if self.blend_frames_check_val.get() == 1:\n",
            "                self.pretreatment_entity.blend_frames = json.loads(self.blend_frames_val.get())\n",
            "            else:\n",
            "                self.pretreatment_entity.blend_frames = -1\n",
            "            self.pretreatment_entity.horizontal_stitching = True if self.horizontal_stitching_check_val.get() == 1 else False\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\gui\\pretreatment.py",
        "line_number": 227,
        "API": ".get(",
        "context": [
            "            else:\n",
            "                self.pretreatment_entity.blend_frames = -1\n",
            "            self.pretreatment_entity.horizontal_stitching = True if self.horizontal_stitching_check_val.get() == 1 else False\n",
            "            self.pretreatment_entity.replace_transparent = True if self.replace_transparent_check_val.get() == 1 else False\n",
            "            self.pretreatment_entity.binaryzation = self.binaryzation_val.get()\n",
            "        except Exception as e:\n",
            "            messagebox.showerror(\n",
            "                e.__class__.__name__, json.dumps(e.args)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\middleware\\random_captcha.py",
        "line_number": 178,
        "API": ".format(",
        "context": [
            "                    if codepoint in uni_map.keys():\n",
            "                        continue\n",
            "                    else:\n",
            "                        font.close()\n",
            "                        raise Exception(\"{} not found!\".format(item))\n",
            "            except Exception as e:\n",
            "                try:\n",
            "                    os.remove(font_type)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\middleware\\random_captcha.py",
        "line_number": 264,
        "API": ".save(",
        "context": [
            "                img = ImageDraw.Draw(__image)\n",
            "                labels, font_type = self.set_content(img, img_width, img_height)\n",
            "                if mode == \"bytes\":\n",
            "                    img_byte_arr = io.BytesIO()\n",
            "                    __image.save(img_byte_arr, format=img_format)\n",
            "                    return img_byte_arr.getvalue(), labels, font_type\n",
            "                elif mode == \"numpy\":\n",
            "                    return np.array(__image), labels, font_type\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\middleware\\random_captcha.py",
        "line_number": 270,
        "API": ".save(",
        "context": [
            "                elif mode == \"numpy\":\n",
            "                    return np.array(__image), labels, font_type\n",
            "                elif mode == \"base64\":\n",
            "                    img_byte_arr = io.BytesIO()\n",
            "                    __image.save(img_byte_arr, format=img_format)\n",
            "                    _bytes = img_byte_arr.getvalue()\n",
            "                    return base64.b64encode(_bytes).decode(), labels, font_type\n",
            "                else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\network\\utils.py",
        "line_number": 15,
        "API": ".control_dependencies(",
        "context": [
            "    \"\"\"\n",
            "    \u7f51\u7edc\u7ec4\u5408\u5757 - \u7ec6\u8282\u5b9e\u73b0\n",
            "    \u8bf4\u660e: \u672c\u7c7b\u4e2d\u6240\u6709\u7684BN\u5b9e\u73b0\u90fd\u91c7\u7528: tf.layers.batch_normalization\n",
            "    \u4e3a\u4ec0\u4e48\u4e0d\u7528 \u3010tf.keras.layers.BatchNormalization/tf.layers.BatchNormalization]\n",
            "    \u524d\u8005: `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)`\n",
            "    should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "    \u5c1d\u8bd5\u8fc7\u4ee5\u4e0b\u6539\u8fdb\u65e0\u679c:\n",
            "    --------------------------------------------------------------------------------------\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\network\\utils.py",
        "line_number": 39,
        "API": ".placeholder(",
        "context": [
            "        self.is_training = self._is_training()\n",
            "\n",
            "    def _is_training(self):\n",
            "        \"\"\" \u53d6\u6d88 is_training \u5360\u4f4d\u7b26\u4f5c\u4e3a[Predict]\u6a21\u5f0f\u7684\u8f93\u5165\u4f9d\u8d56 \"\"\"\n",
            "        return False if self.mode == RunMode.Predict else tf.keras.backend.placeholder(dtype=tf.bool)\n",
            "\n",
            "    @staticmethod\n",
            "    def hard_swish(x, name='hard_swish'):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\network\\utils.py",
        "line_number": 56,
        "API": ".sqrt(",
        "context": [
            "        kl -- kernel size\n",
            "        dl -- filter numbers\n",
            "        \"\"\"\n",
            "\n",
            "        stddev = math.sqrt(2. / (kl ** 2 * dl))\n",
            "        return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
            "\n",
            "    def reshape_layer(self, input_tensor, loss_func, shape_list):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\network\\utils.py",
        "line_number": 73,
        "API": ".format(",
        "context": [
            "\n",
            "    def cnn_layer(self, index, inputs, filters, kernel_size, strides):\n",
            "        \"\"\"\u5377\u79ef-BN-\u6fc0\u6d3b\u51fd\u6570-\u6c60\u5316\u7ed3\u6784\u5757\"\"\"\n",
            "\n",
            "        with tf.keras.backend.name_scope('unit-{}'.format(index + 1)):\n",
            "            x = tf.keras.layers.Conv2D(\n",
            "                filters=filters,\n",
            "                kernel_size=kernel_size,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\network\\utils.py",
        "line_number": 81,
        "API": ".format(",
        "context": [
            "                strides=strides[0],\n",
            "                kernel_regularizer=l1(0.01),\n",
            "                kernel_initializer=self.msra_initializer(kernel_size, filters),\n",
            "                padding='same',\n",
            "                name='cnn-{}'.format(index + 1),\n",
            "            )(inputs)\n",
            "            x = tf.compat.v1.layers.batch_normalization(\n",
            "                x,\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\network\\utils.py",
        "line_number": 93,
        "API": ".format(",
        "context": [
            "                    'dmax': 5\n",
            "                } if index == 0 else None,\n",
            "                reuse=False,\n",
            "                momentum=0.9,\n",
            "                name='bn{}'.format(index + 1),\n",
            "                training=self.is_training\n",
            "            )\n",
            "            x = tf.keras.layers.LeakyReLU(0.01)(x)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\network\\utils.py",
        "line_number": 283,
        "API": ".add(",
        "context": [
            "            training=self.is_training,\n",
            "            name=bn_name_base + '1'\n",
            "        )\n",
            "\n",
            "        x = tf.keras.layers.add([x, shortcut])\n",
            "        x = tf.keras.layers.LeakyReLU(0.01)(x)\n",
            "        return x\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\network\\utils.py",
        "line_number": 353,
        "API": ".add(",
        "context": [
            "            momentum=0.9,\n",
            "            training=self.is_training,\n",
            "            name=bn_name_base + '2c',\n",
            "        )\n",
            "        x = tf.keras.layers.add([x, input_tensor])\n",
            "        x = tf.keras.layers.LeakyReLU(0.01)(x)\n",
            "        return x\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\network\\utils.py",
        "line_number": 373,
        "API": ".format(",
        "context": [
            "\n",
            "        in_channels = tf.keras.backend.int_shape(input_tensor)[channel_axis]\n",
            "        pointwise_filters = int(filters)\n",
            "        x = input_tensor\n",
            "        prefix = 'block_{}_'.format(block_id)\n",
            "\n",
            "        if block_id:\n",
            "            # Expand\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 56,
        "API": ".executing_eagerly(",
        "context": [
            "\n",
            "    def _create_slots(self, var_list):\n",
            "        first_var = min(var_list, key=lambda x: x.name)\n",
            "        if StrictVersion(tf.__version__) >= StrictVersion('1.10.0'):\n",
            "            graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
            "        else:\n",
            "            graph = ops.get_default_graph()\n",
            "        create_new = self._get_non_slot_variable(\"beta1_power\", graph) is None\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 80,
        "API": ".convert_to_tensor(",
        "context": [
            "            self._zeros_slot(v, \"v\", self._name)\n",
            "            self._zeros_slot(v, \"vhat\", self._name)\n",
            "\n",
            "    def _prepare(self):\n",
            "        self._lr_t = ops.convert_to_tensor(self._lr)\n",
            "        self._base_lr_t = ops.convert_to_tensor(self._lr)\n",
            "        self._beta1_t = ops.convert_to_tensor(self._beta1)\n",
            "        self._beta2_t = ops.convert_to_tensor(self._beta2)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 85,
        "API": ".convert_to_tensor(",
        "context": [
            "        self._base_lr_t = ops.convert_to_tensor(self._lr)\n",
            "        self._beta1_t = ops.convert_to_tensor(self._beta1)\n",
            "        self._beta2_t = ops.convert_to_tensor(self._beta2)\n",
            "        self._epsilon_t = ops.convert_to_tensor(self._epsilon)\n",
            "        self._gamma_t = ops.convert_to_tensor(self._gamma)\n",
            "\n",
            "    def _apply_dense(self, grad, var):\n",
            "        if StrictVersion(tf.__version__) >= StrictVersion('1.10.0'):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 92,
        "API": ".cast(",
        "context": [
            "        if StrictVersion(tf.__version__) >= StrictVersion('1.10.0'):\n",
            "            graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
            "        else:\n",
            "            graph = ops.get_default_graph()\n",
            "        beta1_power = math_ops.cast(self._get_non_slot_variable(\"beta1_power\", graph=graph), var.dtype.base_dtype)\n",
            "        beta2_power = math_ops.cast(self._get_non_slot_variable(\"beta2_power\", graph=graph), var.dtype.base_dtype)\n",
            "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
            "        base_lr_t = math_ops.cast(self._base_lr_t, var.dtype.base_dtype)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 97,
        "API": ".cast(",
        "context": [
            "        beta2_power = math_ops.cast(self._get_non_slot_variable(\"beta2_power\", graph=graph), var.dtype.base_dtype)\n",
            "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
            "        base_lr_t = math_ops.cast(self._base_lr_t, var.dtype.base_dtype)\n",
            "        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n",
            "        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n",
            "        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n",
            "        gamma_multi = math_ops.cast(self._get_non_slot_variable(\"gamma_multi\", graph=graph), var.dtype.base_dtype)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 109,
        "API": ".assign(",
        "context": [
            "\n",
            "        # m_t = beta1 * m + (1 - beta1) * g_t\n",
            "        m = self.get_slot(var, \"m\")\n",
            "        m_scaled_g_values = grad * (1 - beta1_t)\n",
            "        m_t = state_ops.assign(m, beta1_t * m + m_scaled_g_values, use_locking=self._use_locking)\n",
            "\n",
            "        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n",
            "        v = self.get_slot(var, \"v\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 114,
        "API": ".assign(",
        "context": [
            "\n",
            "        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n",
            "        v = self.get_slot(var, \"v\")\n",
            "        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n",
            "        v_t = state_ops.assign(v, beta2_t * v + v_scaled_g_values, use_locking=self._use_locking)\n",
            "\n",
            "        # amsgrad\n",
            "        vhat = self.get_slot(var, \"vhat\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 119,
        "API": ".maximum(",
        "context": [
            "\n",
            "        # amsgrad\n",
            "        vhat = self.get_slot(var, \"vhat\")\n",
            "        if self._amsbound :\n",
            "            vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n",
            "            v_sqrt = math_ops.sqrt(vhat_t)\n",
            "        else:\n",
            "            vhat_t = state_ops.assign(vhat, vhat)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 130,
        "API": ".group(",
        "context": [
            "        step_size_bound = step_size / (v_sqrt + epsilon_t)\n",
            "        bounded_lr = m_t * clip_by_value(step_size_bound, lower_bound, upper_bound)\n",
            "\n",
            "        var_update = state_ops.assign_sub(var, bounded_lr, use_locking=self._use_locking)\n",
            "        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n",
            "\n",
            "    def _resource_apply_dense(self, grad, var):\n",
            "        if StrictVersion(tf.__version__) >= StrictVersion('1.10.0'):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 137,
        "API": ".cast(",
        "context": [
            "        if StrictVersion(tf.__version__) >= StrictVersion('1.10.0'):\n",
            "            graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
            "        else:\n",
            "            graph = ops.get_default_graph()\n",
            "        beta1_power = math_ops.cast(self._get_non_slot_variable(\"beta1_power\", graph=graph), grad.dtype.base_dtype)\n",
            "        beta2_power = math_ops.cast(self._get_non_slot_variable(\"beta2_power\", graph=graph), grad.dtype.base_dtype)\n",
            "        lr_t = math_ops.cast(self._lr_t, grad.dtype.base_dtype)\n",
            "        base_lr_t = math_ops.cast(self._base_lr_t, var.dtype.base_dtype)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 142,
        "API": ".cast(",
        "context": [
            "        beta2_power = math_ops.cast(self._get_non_slot_variable(\"beta2_power\", graph=graph), grad.dtype.base_dtype)\n",
            "        lr_t = math_ops.cast(self._lr_t, grad.dtype.base_dtype)\n",
            "        base_lr_t = math_ops.cast(self._base_lr_t, var.dtype.base_dtype)\n",
            "        beta1_t = math_ops.cast(self._beta1_t, grad.dtype.base_dtype)\n",
            "        beta2_t = math_ops.cast(self._beta2_t, grad.dtype.base_dtype)\n",
            "        epsilon_t = math_ops.cast(self._epsilon_t, grad.dtype.base_dtype)\n",
            "        gamma_multi = math_ops.cast(self._get_non_slot_variable(\"gamma_multi\", graph=graph), var.dtype.base_dtype)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 164,
        "API": ".maximum(",
        "context": [
            "\n",
            "        # amsgrad\n",
            "        vhat = self.get_slot(var, \"vhat\")\n",
            "        if self._amsbound:\n",
            "            vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n",
            "            v_sqrt = math_ops.sqrt(vhat_t)\n",
            "        else:\n",
            "            vhat_t = state_ops.assign(vhat, vhat)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 176,
        "API": ".group(",
        "context": [
            "        bounded_lr = m_t * clip_by_value(step_size_bound, lower_bound, upper_bound)\n",
            "\n",
            "        var_update = state_ops.assign_sub(var, bounded_lr, use_locking=self._use_locking)\n",
            "\n",
            "        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n",
            "\n",
            "    def _apply_sparse_shared(self, grad, var, indices, scatter_add):\n",
            "        if StrictVersion(tf.__version__) >= StrictVersion('1.10.0'):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 188,
        "API": ".cast(",
        "context": [
            "        beta2_power = math_ops.cast(self._get_non_slot_variable(\"beta2_power\", graph=graph), var.dtype.base_dtype)\n",
            "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
            "        base_lr_t = math_ops.cast(self._base_lr_t, var.dtype.base_dtype)\n",
            "        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n",
            "        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n",
            "        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n",
            "        gamma_t = math_ops.cast(self._gamma_t, var.dtype.base_dtype)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 200,
        "API": ".assign(",
        "context": [
            "\n",
            "        # m_t = beta1 * m + (1 - beta1) * g_t\n",
            "        m = self.get_slot(var, \"m\")\n",
            "        m_scaled_g_values = grad * (1 - beta1_t)\n",
            "        m_t = state_ops.assign(m, m * beta1_t, use_locking=self._use_locking)\n",
            "        with ops.control_dependencies([m_t]):\n",
            "            m_t = scatter_add(m, indices, m_scaled_g_values)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 207,
        "API": ".assign(",
        "context": [
            "\n",
            "        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n",
            "        v = self.get_slot(var, \"v\")\n",
            "        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n",
            "        v_t = state_ops.assign(v, v * beta2_t, use_locking=self._use_locking)\n",
            "        with ops.control_dependencies([v_t]):\n",
            "            v_t = scatter_add(v, indices, v_scaled_g_values)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 226,
        "API": ".group(",
        "context": [
            "        bounded_lr = m_t * clip_by_value(step_size_bound, lower_bound, upper_bound)\n",
            "\n",
            "        var_update = state_ops.assign_sub(var, bounded_lr, use_locking=self._use_locking)\n",
            "\n",
            "        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n",
            "\n",
            "    def _apply_sparse(self, grad, var):\n",
            "        return self._apply_sparse_shared(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 235,
        "API": ".control_dependencies(",
        "context": [
            "            lambda x, i, v: state_ops.scatter_add(  # pylint: disable=g-long-lambda\n",
            "                x, i, v, use_locking=self._use_locking))\n",
            "\n",
            "    def _resource_scatter_add(self, x, i, v):\n",
            "        with ops.control_dependencies(\n",
            "                [resource_variable_ops.resource_scatter_add(x, i, v)]):\n",
            "            return x.value()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 245,
        "API": ".control_dependencies(",
        "context": [
            "            grad, var, indices, self._resource_scatter_add)\n",
            "\n",
            "    def _finish(self, update_ops, name_scope):\n",
            "        # Update the power accumulators.\n",
            "        with ops.control_dependencies(update_ops):\n",
            "            if StrictVersion(tf.__version__) >= StrictVersion('1.10.0'):\n",
            "                graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
            "            else:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 254,
        "API": ".assign(",
        "context": [
            "            beta1_power = self._get_non_slot_variable(\"beta1_power\", graph=graph)\n",
            "            beta2_power = self._get_non_slot_variable(\"beta2_power\", graph=graph)\n",
            "            gamma_multi = self._get_non_slot_variable(\"gamma_multi\", graph=graph)\n",
            "            with ops.colocate_with(beta1_power):\n",
            "                update_beta1 = beta1_power.assign(\n",
            "                    beta1_power * self._beta1_t,\n",
            "                    use_locking=self._use_locking)\n",
            "                update_beta2 = beta2_power.assign(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\AdaBound.py",
        "line_number": 260,
        "API": ".assign(",
        "context": [
            "                    use_locking=self._use_locking)\n",
            "                update_beta2 = beta2_power.assign(\n",
            "                    beta2_power * self._beta2_t,\n",
            "                    use_locking=self._use_locking)\n",
            "                update_gamma = gamma_multi.assign(\n",
            "                    gamma_multi + self._gamma_t,\n",
            "                    use_locking=self._use_locking)\n",
            "        return control_flow_ops.group(*update_ops + [update_beta1, update_beta2, update_gamma], name=name_scope)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 87,
        "API": ".executing_eagerly(",
        "context": [
            "        self._min_lr_t = None\n",
            "\n",
            "    def _get_beta_accumulators(self):\n",
            "        with ops.init_scope():\n",
            "            if context.executing_eagerly():\n",
            "                graph = None\n",
            "            else:\n",
            "                graph = ops.get_default_graph()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 116,
        "API": ".convert_to_tensor(",
        "context": [
            "        total_steps = self._call_if_callable(self._total_steps)\n",
            "        warmup_proportion = self._call_if_callable(self._warmup_proportion)\n",
            "        min_lr = self._call_if_callable(self._min_lr)\n",
            "\n",
            "        self._lr_t = ops.convert_to_tensor(lr, name=\"learning_rate\")\n",
            "        self._beta1_t = ops.convert_to_tensor(beta1, name=\"beta1\")\n",
            "        self._beta2_t = ops.convert_to_tensor(beta2, name=\"beta2\")\n",
            "        self._epsilon_t = ops.convert_to_tensor(epsilon, name=\"epsilon\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 121,
        "API": ".convert_to_tensor(",
        "context": [
            "        self._beta1_t = ops.convert_to_tensor(beta1, name=\"beta1\")\n",
            "        self._beta2_t = ops.convert_to_tensor(beta2, name=\"beta2\")\n",
            "        self._epsilon_t = ops.convert_to_tensor(epsilon, name=\"epsilon\")\n",
            "        self._weight_decay_t = ops.convert_to_tensor(weight_decay, name=\"weight_decay\")\n",
            "        self._total_steps_t = ops.convert_to_tensor(total_steps, name=\"total_steps\")\n",
            "        self._warmup_proportion_t = ops.convert_to_tensor(warmup_proportion, name=\"warmup_proportion\")\n",
            "        self._min_lr_t = ops.convert_to_tensor(min_lr, name=\"min_lr\")\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 130,
        "API": ".cast(",
        "context": [
            "        return self._resource_apply_dense(grad, var)\n",
            "\n",
            "    def _resource_apply_dense(self, grad, var):\n",
            "        step, beta1_power, beta2_power = self._get_beta_accumulators()\n",
            "        beta1_power = math_ops.cast(beta1_power, var.dtype.base_dtype)\n",
            "        beta2_power = math_ops.cast(beta2_power, var.dtype.base_dtype)\n",
            "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 135,
        "API": ".cast(",
        "context": [
            "        beta2_power = math_ops.cast(beta2_power, var.dtype.base_dtype)\n",
            "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
            "\n",
            "        if self._initial_total_steps > 0:\n",
            "            total_steps = math_ops.cast(self._total_steps_t, var.dtype.base_dtype)\n",
            "            warmup_proportion = math_ops.cast(self._warmup_proportion_t, var.dtype.base_dtype)\n",
            "            min_lr = math_ops.cast(self._min_lr_t, var.dtype.base_dtype)\n",
            "            warmup_steps = total_steps * warmup_proportion\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 141,
        "API": ".where(",
        "context": [
            "            min_lr = math_ops.cast(self._min_lr_t, var.dtype.base_dtype)\n",
            "            warmup_steps = total_steps * warmup_proportion\n",
            "            decay_steps = math_ops.maximum(total_steps - warmup_steps, 1)\n",
            "            decay_rate = (min_lr - lr_t) / decay_steps\n",
            "            lr_t = tf.where(\n",
            "                step <= warmup_steps,\n",
            "                lr_t * (step / warmup_steps),\n",
            "                lr_t + decay_rate * math_ops.minimum(step - warmup_steps, decay_steps),\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 147,
        "API": ".cast(",
        "context": [
            "                lr_t * (step / warmup_steps),\n",
            "                lr_t + decay_rate * math_ops.minimum(step - warmup_steps, decay_steps),\n",
            "            )\n",
            "\n",
            "        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n",
            "        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n",
            "        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 155,
        "API": ".assign(",
        "context": [
            "        sma_inf = 2.0 / (1.0 - beta2_t) - 1.0\n",
            "        sma_t = sma_inf - 2.0 * step * beta2_power / (1.0 - beta2_power)\n",
            "\n",
            "        m = self.get_slot(var, \"m\")\n",
            "        m_t = state_ops.assign(m, beta1_t * m + (1.0 - beta1_t) * grad, use_locking=self._use_locking)\n",
            "        m_corr_t = m_t / (1.0 - beta1_power)\n",
            "\n",
            "        v = self.get_slot(var, \"v\")\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 162,
        "API": ".maximum(",
        "context": [
            "        v = self.get_slot(var, \"v\")\n",
            "        v_t = state_ops.assign(v, beta2_t * v + (1.0 - beta2_t) * math_ops.square(grad), use_locking=self._use_locking)\n",
            "        if self._amsgrad:\n",
            "            vhat = self.get_slot(var, 'vhat')\n",
            "            vhat_t = state_ops.assign(vhat, math_ops.maximum(vhat, v_t), use_locking=self._use_locking)\n",
            "            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta2_power))\n",
            "        else:\n",
            "            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta2_power))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 167,
        "API": ".sqrt(",
        "context": [
            "            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta2_power))\n",
            "        else:\n",
            "            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta2_power))\n",
            "\n",
            "        r_t = math_ops.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n",
            "                            (sma_t - 2.0) / (sma_inf - 2.0) *\n",
            "                            sma_inf / sma_t)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 174,
        "API": ".cast(",
        "context": [
            "\n",
            "        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t)\n",
            "\n",
            "        if self._initial_weight_decay > 0.0:\n",
            "            var_t += math_ops.cast(self._weight_decay_t, var.dtype.base_dtype) * var\n",
            "\n",
            "        var_update = state_ops.assign_sub(var, lr_t * var_t, use_locking=self._use_locking)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 181,
        "API": ".group(",
        "context": [
            "\n",
            "        updates = [var_update, m_t, v_t]\n",
            "        if self._amsgrad:\n",
            "            updates.append(vhat_t)\n",
            "        return control_flow_ops.group(*updates)\n",
            "\n",
            "    def _apply_sparse_shared(self, grad, var, indices, scatter_add):\n",
            "        step, beta1_power, beta2_power = self._get_beta_accumulators()\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 186,
        "API": ".cast(",
        "context": [
            "\n",
            "    def _apply_sparse_shared(self, grad, var, indices, scatter_add):\n",
            "        step, beta1_power, beta2_power = self._get_beta_accumulators()\n",
            "        beta1_power = math_ops.cast(beta1_power, var.dtype.base_dtype)\n",
            "        beta2_power = math_ops.cast(beta2_power, var.dtype.base_dtype)\n",
            "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
            "\n",
            "        if self._initial_total_steps > 0:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 191,
        "API": ".cast(",
        "context": [
            "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
            "\n",
            "        if self._initial_total_steps > 0:\n",
            "            total_steps = math_ops.cast(self._total_steps_t, var.dtype.base_dtype)\n",
            "            warmup_proportion = math_ops.cast(self._warmup_proportion_t, var.dtype.base_dtype)\n",
            "            min_lr = math_ops.cast(self._min_lr_t, var.dtype.base_dtype)\n",
            "            warmup_steps = total_steps * warmup_proportion\n",
            "            decay_steps = math_ops.maximum(total_steps - warmup_steps, 1)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 211,
        "API": ".assign(",
        "context": [
            "        sma_t = sma_inf - 2.0 * step * beta2_power / (1.0 - beta2_power)\n",
            "\n",
            "        m = self.get_slot(var, \"m\")\n",
            "        m_scaled_g_values = grad * (1 - beta1_t)\n",
            "        m_t = state_ops.assign(m, m * beta1_t, use_locking=self._use_locking)\n",
            "        with ops.control_dependencies([m_t]):\n",
            "            m_t = scatter_add(m, indices, m_scaled_g_values)\n",
            "        m_corr_t = m_t / (1.0 - beta1_power)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 218,
        "API": ".assign(",
        "context": [
            "        m_corr_t = m_t / (1.0 - beta1_power)\n",
            "\n",
            "        v = self.get_slot(var, \"v\")\n",
            "        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n",
            "        v_t = state_ops.assign(v, v * beta2_t, use_locking=self._use_locking)\n",
            "        with ops.control_dependencies([v_t]):\n",
            "            v_t = scatter_add(v, indices, v_scaled_g_values)\n",
            "        if self._amsgrad:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 223,
        "API": ".maximum(",
        "context": [
            "        with ops.control_dependencies([v_t]):\n",
            "            v_t = scatter_add(v, indices, v_scaled_g_values)\n",
            "        if self._amsgrad:\n",
            "            vhat = self.get_slot(var, 'vhat')\n",
            "            vhat_t = state_ops.assign(vhat, math_ops.maximum(vhat, v_t), use_locking=self._use_locking)\n",
            "            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta2_power))\n",
            "        else:\n",
            "            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta2_power))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 235,
        "API": ".cast(",
        "context": [
            "\n",
            "        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t)\n",
            "\n",
            "        if self._initial_weight_decay > 0.0:\n",
            "            var_t += math_ops.cast(self._weight_decay_t, var.dtype.base_dtype) * var\n",
            "\n",
            "        var_t = lr_t * var_t\n",
            "        var_update = state_ops.scatter_sub(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 241,
        "API": ".gather(",
        "context": [
            "        var_t = lr_t * var_t\n",
            "        var_update = state_ops.scatter_sub(\n",
            "                    var,\n",
            "                    indices,\n",
            "                    array_ops.gather(var_t, indices),\n",
            "                    use_locking=self._use_locking)\n",
            "\n",
            "        updates = [var_update, m_t, v_t]\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 247,
        "API": ".group(",
        "context": [
            "\n",
            "        updates = [var_update, m_t, v_t]\n",
            "        if self._amsgrad:\n",
            "            updates.append(vhat_t)\n",
            "        return control_flow_ops.group(*updates)\n",
            "\n",
            "    def _apply_sparse(self, grad, var):\n",
            "        return self._apply_sparse_shared(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 257,
        "API": ".control_dependencies(",
        "context": [
            "            grad.indices,\n",
            "            lambda x, i, v: state_ops.scatter_add(x, i, v, use_locking=self._use_locking))\n",
            "\n",
            "    def _resource_scatter_add(self, x, i, v):\n",
            "        with ops.control_dependencies([resource_variable_ops.resource_scatter_add(x.handle, i, v)]):\n",
            "            return x.value()\n",
            "\n",
            "    def _resource_apply_sparse(self, grad, var, indices):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 264,
        "API": ".control_dependencies(",
        "context": [
            "    def _resource_apply_sparse(self, grad, var, indices):\n",
            "        return self._apply_sparse_shared(grad, var, indices, self._resource_scatter_add)\n",
            "\n",
            "    def _finish(self, update_ops, name_scope):\n",
            "        with ops.control_dependencies(update_ops):\n",
            "            step, beta1_power, beta2_power = self._get_beta_accumulators()\n",
            "            with ops.colocate_with(beta1_power):\n",
            "                update_step = step.assign(step + 1.0, use_locking=self._use_locking)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\optimizer\\RAdam.py",
        "line_number": 269,
        "API": ".assign(",
        "context": [
            "            step, beta1_power, beta2_power = self._get_beta_accumulators()\n",
            "            with ops.colocate_with(beta1_power):\n",
            "                update_step = step.assign(step + 1.0, use_locking=self._use_locking)\n",
            "                update_beta1 = beta1_power.assign(beta1_power * self._beta1_t, use_locking=self._use_locking)\n",
            "                update_beta2 = beta2_power.assign(beta2_power * self._beta2_t, use_locking=self._use_locking)\n",
            "        return control_flow_ops.group(*update_ops + [update_step, update_beta1, update_beta2], name=name_scope)"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\tools\\delete_repeat_img.py",
        "line_number": 11,
        "API": ".split(",
        "context": [
            "from skimage.measure import compare_ssim\n",
            "\n",
            "EXT = ['.jpg', '.jpeg']\n",
            "path = r'C:\\Users\\sml2h\\Desktop\\sb'\n",
            "codes = [item.split(\"_\")[0].lower() for item in os.listdir(path)]\n",
            "codes = list(set(codes))\n",
            "codes_dict = {}\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\tools\\delete_repeat_img.py",
        "line_number": 18,
        "API": ".split(",
        "context": [
            "\n",
            "for code in codes:\n",
            "    codes_dict[code] = []\n",
            "for item in os.listdir(path):\n",
            "    codes_dict[item.split(\"_\")[0].lower()].append(item)\n",
            "\n",
            "def delete(imgs_n):\n",
            "    # return\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\tools\\delete_repeat_img.py",
        "line_number": 28,
        "API": ".join(",
        "context": [
            "\n",
            "#\n",
            "def find_sim_images(code_lists):\n",
            "    imgs_n = []\n",
            "    img_files = [os.path.join(path, code) for code in code_lists]\n",
            "    for currIndex, filename in enumerate(img_files):\n",
            "        if filename in imgs_n:\n",
            "            continue\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\tools\\package.py",
        "line_number": 10,
        "API": ".write(",
        "context": [
            "\n",
            "\n",
            "with open(\"../resource/VERSION\", \"w\", encoding=\"utf8\") as f:\n",
            "    today = time.strftime(\"%Y%m%d\", time.localtime(time.time()))\n",
            "    f.write(today)\n",
            "\n",
            "\n",
            "def package(prefix):\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\tools\\package.py",
        "line_number": 15,
        "API": ".format(",
        "context": [
            "\n",
            "\n",
            "def package(prefix):\n",
            "    \"\"\"\u57fa\u4e8ePyInstaller\u6253\u5305\u7f16\u8bd1\u4e3a\u5355\u53ef\u6267\u884c\u6587\u4ef6\"\"\"\n",
            "    opts = ['{}app.spec'.format(prefix), '--distpath={}dist'.format(prefix), '--workpath={}build'.format(prefix)]\n",
            "    run(opts)\n",
            "\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\utils\\category_frequency_statistics.py",
        "line_number": 13,
        "API": ".split(",
        "context": [
            "PATH_SPLIT = \"/\"\n",
            "\n",
            "\n",
            "def extract_labels_from_filename(filename: str, extract_regex):\n",
            "    if filename.split(\"/\")[-1] in ignore_list:\n",
            "        return None\n",
            "    try:\n",
            "        labels = re.search(extract_regex, filename.split(PATH_SPLIT)[-1])\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\utils\\category_frequency_statistics.py",
        "line_number": 21,
        "API": ".group(",
        "context": [
            "    except re.error as e:\n",
            "        print('error:', e)\n",
            "        return None\n",
            "    if labels:\n",
            "        labels = labels.group()\n",
            "    else:\n",
            "        print('invalid filename {}, ignored.'.format(filename))\n",
            "        return None\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\utils\\category_frequency_statistics.py",
        "line_number": 58,
        "API": ".add(",
        "context": [
            "                labels = extract_labels_from_filename(filename, model.extract_regex)\n",
            "                if not labels:\n",
            "                    continue\n",
            "                if int(model.max_label_num) == 1:\n",
            "                    category_set.add(labels)\n",
            "                elif '&' in labels:\n",
            "                    for label_item in labels.split('&'):\n",
            "                        category_set.add(label_item)\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\utils\\category_frequency_statistics.py",
        "line_number": 64,
        "API": ".add(",
        "context": [
            "                    for label_item in labels.split('&'):\n",
            "                        category_set.add(label_item)\n",
            "                else:\n",
            "                    for label_item in labels:\n",
            "                        category_set.add(label_item)\n",
            "        category_list = list(category_set)\n",
            "        category_list.sort()\n",
            "        if is_json:\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\utils\\category_frequency_statistics.py",
        "line_number": 76,
        "API": ".sort(",
        "context": [
            "if __name__ == '__main__':\n",
            "    model_conf = ModelConfig(\"test-CNNX-GRU-H64-CTC-C1\")\n",
            "    # labels_dict = fetch_category_freq(model_conf)\n",
            "    # label_list = [k for k, v in labels_dict if v < 5000]\n",
            "    # label_list.sort()\n",
            "    # high_freq = \"\".join(label_list)\n",
            "    # print(high_freq)\n",
            "    # print(len(high_freq))\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\utils\\data.py",
        "line_number": 53,
        "API": ".cast(",
        "context": [
            "                'label': tf.io.FixedLenFeature([], tf.string),\n",
            "                'input': tf.io.FixedLenFeature([], tf.string),\n",
            "            }\n",
            "        )\n",
            "        _input = tf.cast(features['input'], tf.string)\n",
            "        _label = tf.cast(features['label'], tf.string)\n",
            "\n",
            "        return _input, _label\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\utils\\data.py",
        "line_number": 89,
        "API": ".repeat(",
        "context": [
            "        ).map(self.parse_example)\n",
            "        dataset_train = dataset_train.shuffle(\n",
            "            min_after_dequeue,\n",
            "            reshuffle_each_iteration=True\n",
            "        ).prefetch(128).batch(batch, drop_remainder=True).repeat()\n",
            "        iterator = tf.compat.v1.data.make_one_shot_iterator(dataset_train)\n",
            "        self.next_element = iterator.get_next()\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\utils\\data.py",
        "line_number": 117,
        "API": ".join(",
        "context": [
            "        for i in range(num):\n",
            "            try:\n",
            "                image, labels, font_type = self.ran_captcha.create()\n",
            "                _images.append(image)\n",
            "                _labels.append(''.join(labels).encode())\n",
            "            except Exception as e:\n",
            "                print(e)\n",
            "                pass\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\utils\\data.py",
        "line_number": 141,
        "API": ".image(",
        "context": [
            "        for index, (i1, i2) in enumerate(zip(_input, _label)):\n",
            "            try:\n",
            "                label_array = self.encoder.text(i2)\n",
            "                if self.model_conf.model_field == ModelField.Image:\n",
            "                    input_array = self.encoder.image(i1)\n",
            "                else:\n",
            "                    input_array = self.encoder.text(i1)\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\utils\\data.py",
        "line_number": 146,
        "API": ".warn(",
        "context": [
            "                else:\n",
            "                    input_array = self.encoder.text(i1)\n",
            "\n",
            "                if input_array is None:\n",
            "                    # tf.compat.v1.logging.warn(\n",
            "                    #     \"{}, Cannot identify image file labeled: {}, ignored.\".format(input_array, label_array))\n",
            "                    continue\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\utils\\data.py",
        "line_number": 151,
        "API": ".warn(",
        "context": [
            "                    #     \"{}, Cannot identify image file labeled: {}, ignored.\".format(input_array, label_array))\n",
            "                    continue\n",
            "\n",
            "                if isinstance(input_array, str):\n",
            "                    # tf.compat.v1.logging.warn(\"{}, \\nInput errors labeled: {} [{}], ignored.\".format(input_array, i1, label_array))\n",
            "                    continue\n",
            "                if isinstance(label_array, dict):\n",
            "                    # tf.logging.warn(\"The sample label {} contains invalid charset: {}.\".format(\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\utils\\data.py",
        "line_number": 161,
        "API": ".warn(",
        "context": [
            "                    continue\n",
            "\n",
            "                if input_array.shape[-1] != self.model_conf.image_channel:\n",
            "                    # pass\n",
            "                    tf.compat.v1.logging.warn(\"{}, \\nInput shape: {}, ignored.\".format(\n",
            "                        self.model_conf.image_channel, input_array.shape[-1])\n",
            "                    )\n",
            "                    continue\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\utils\\data.py",
        "line_number": 169,
        "API": ".warn(",
        "context": [
            "\n",
            "                label_len_correct = len(label_array) != self.model_conf.max_label_num\n",
            "                using_cross_entropy = self.model_conf.loss_func == LossFunction.CrossEntropy\n",
            "                if label_len_correct and using_cross_entropy and not self.model_conf.auto_padding:\n",
            "                    tf.compat.v1.logging.warn(\"The number of labels must be fixed when using cross entropy, label: {}, \"\n",
            "                                    \"the number of tags is incorrect, ignored.\".format(i2))\n",
            "                    continue\n",
            "\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\utils\\data.py",
        "line_number": 174,
        "API": ".warn(",
        "context": [
            "                                    \"the number of tags is incorrect, ignored.\".format(i2))\n",
            "                    continue\n",
            "\n",
            "                if len(label_array) > self.model_conf.max_label_num and using_cross_entropy:\n",
            "                    tf.compat.v1.logging.warn(\n",
            "                        \"The number of label[{}] exceeds the maximum number of labels, ignored.{}\".format(i2,\n",
            "                                                                                                          label_array))\n",
            "                    continue\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\utils\\data.py",
        "line_number": 184,
        "API": ".format(",
        "context": [
            "                label_batch.append(label_array)\n",
            "            except OSError:\n",
            "                random_suffix = hashlib.md5(i1).hexdigest()\n",
            "                file_format = EXCEPT_FORMAT_MAP[self.model_conf.model_field]\n",
            "                with open(file=\"oserror_{}.{}\".format(random_suffix, file_format), mode=\"wb\") as f:\n",
            "                    f.write(i1)\n",
            "                tf.compat.v1.logging.warn(\"OSError [{}]\".format(i2))\n",
            "                continue\n"
        ]
    },
    {
        "file_path": "C:\\@code\\APIMISUSE\\data\\repo_tensorflow_test\\captcha_trainer\\utils\\sparse.py",
        "line_number": 19,
        "API": ".max(",
        "context": [
            "    try:\n",
            "        values = np.asarray(values, dtype=dtype)\n",
            "    except Exception as e:\n",
            "        print(e, values)\n",
            "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1] + 1], dtype=np.int64)\n",
            "    return indices, values, shape\n"
        ]
    }
]