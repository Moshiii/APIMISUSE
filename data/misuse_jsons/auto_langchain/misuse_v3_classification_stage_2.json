[
    {
        "number": 0,
        "comments": "remove API version fix skip test",
        "commit_message": "Previously, many unit test files started with `enable_v2_behavior`, which would have caused them to run in V2 mode when executing with a V1 test flag. The correct behavior would in fact be to skip such tests when executing with a V1 test flag.\n\nThis fix significantly reduces the total V1 + V2 test load by eliminating redundancy.\n\nPiperOrigin-RevId: 424734850\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class IndexLookupDistributionTest(",
            "",
            "",
            "if __name__ == \"__main__\":",
            "-  tf.compat.v1.enable_v2_behavior()",
            "tf.__internal__.distribute.multi_process_runner.test_main()"
        ]
    },
    {
        "number": 5,
        "comments": "add condition check for version fix",
        "commit_message": "Fix torch version comparisons (#18460)\n\nComparisons like\nversion.parse(torch.__version__) > version.parse(\"1.6\")\nare True for torch==1.6.0+cu101 or torch==1.6.0+cpu\n\nversion.parse(version.parse(torch.__version__).base_version) are preferred (and available in pytorch_utils.py\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class AlbertEmbeddings(nn.Module):",
            "# position_ids (1, len position emb) is contiguous in memory and exported when serialized",
            "self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))",
            "self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")",
            "-        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):",
            "+        if is_torch_greater_than_1_6:",
            "self.register_buffer(",
            "\"token_type_ids\",",
            "torch.zeros(self.position_ids.size(), dtype=torch.long),"
        ]
    },
    {
        "number": 6,
        "comments": "no API used",
        "commit_message": "Encoder abstractions (#518)\n\n* Allow stacked RNNs in PytorchSeq2SeqWrapper\n\n* Modify the zero sequence length logic in PytorchSeq2SeqWrapper to work with stateful RNNs\n\n* WIP: stateful RNNs\n\n* Stateful RNNs\n\n* pylint\n\n* more pylint\n\n* fix the docs\n\n* Address Joel's comments\n\n*  stateful=True that works with GRU\n\n* Deal with sorting in stateful RNNs\n\n* pylint\n\n* mypy\n\n* pylint\n\n* Remove max_batch_size\n\n* Add tests for correctness\n\n* pylint\n\n* initial pass at adding Seq2StackEncoder and adding an EncoderBase\n\n* remove print statements\n\n* docs work\n\n* make sort function return the indices it sorted by\n\n* more work on getting statefulness working correctly\n\n* tweaks to comments, make it clear that wrapping the state in a list is only for statefulness\n\n* finish state update logic and improve docs\n\n* add TODO for review\n\n* fix current tests\n\n* remove abstract Seq2Stack encoder\n\n* use Callable type for base\n\n* add tests for update states\n\n* lint, fix mypy using elipsis in tuple\n\n* fix up ELMo with docs and test\n\n* don't require docs for encoder_base\n\n* add comment for ignored docs\n\n* remove out of date docstring\n\n* remove num_valid from encoder_base, superficial fixes\n\n* use random state in tests, use correct num_valid size\n\n* rename a few things, don't zero out actual tensor in test\n\n* don't split elmo output, fix docs\n\n* actually fix docs\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def sort_batch_by_length(tensor: torch.autograd.Variable, sequence_lengths: torc",
            "index_range = Variable(index_range.long())",
            "_, reverse_mapping = permutation_index.sort(0, descending=False)",
            "restoration_indices = index_range.index_select(0, reverse_mapping)",
            "-    return sorted_tensor, sorted_sequence_lengths, restoration_indices",
            "+    return sorted_tensor, sorted_sequence_lengths, restoration_indices, permutation_index",
            "",
            "",
            "def get_dropout_mask(dropout_probability: float, tensor_for_masking: torch.autograd.Variable):"
        ]
    },
    {
        "number": 7,
        "comments": "change API call for math fix",
        "commit_message": "Fix unit test failures with torch==1.1.0 (#1840)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_quantile():",
            "",
            "",
            "def test_pi():",
            "-    x = torch.empty(1000).log_normal_(0, 1)",
            "+    x = torch.randn(1000).exp()",
            "assert_equal(pi(x, prob=0.8), quantile(x, probs=[0.1, 0.9]))"
        ]
    },
    {
        "number": 8,
        "comments": "add condition check for state fix",
        "commit_message": "[bugfix] TPU + all_gather + SingleTPU shouldn't call xm.all_gather (#6296)\n\n* resolve an issue with TPU\n\n* update\n\n* add changelog\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TPUAccelerator(Accelerator):",
            "Return:",
            "A tensor of shape (world_size, batch, ...)",
            "\"\"\"",
            "-        return xm.all_gather(tensor, group=group, sync_grads=sync_grads)",
            "+        # todo: Add support for backward with all_gather",
            "+        if torch.distributed.is_initialized():",
            "+            return xm.all_gather(tensor, group=group, sync_grads=sync_grads)",
            "+        return tensor"
        ]
    },
    {
        "number": 10,
        "comments": "add condition check for resource fix",
        "commit_message": "Validate --task speed CPU fix (#10244)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def main(opt):",
            "",
            "else:",
            "weights = opt.weights if isinstance(opt.weights, list) else [opt.weights]",
            "-        opt.half = True  # FP16 for fastest results",
            "+        opt.half = torch.cuda.is_available() and opt.device != 'cpu'  # FP16 for fastest results",
            "if opt.task == 'speed':  # speed benchmarks",
            "# python val.py --task speed --data coco.yaml --batch 1 --weights yolov5n.pt yolov5s.pt...",
            "opt.conf_thres, opt.iou_thres, opt.save_json = 0.25, 0.45, False"
        ]
    },
    {
        "number": 11,
        "comments": "revert the fix",
        "commit_message": "Revert fix on module name for hooked functionc\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class TorchHook:",
            "if type(native_func) in [types.FunctionType, types.BuiltinFunctionType]:",
            "# 3. Build the hooked function",
            "new_func = self.get_hooked_func(native_func)",
            "-                # 4. Move the native function to its original module",
            "-                # /!\\ Can be different from the torch_module!",
            "-                # Ex: in torch.py `torch.argmax = torch.functional.argmax`",
            "-                # ... So torch.argmax.__module__ is 'torch.functional' != 'torch'",
            "-                setattr(eval(native_func.__module__), f\"native_{func}\", native_func)",
            "+                # 4. Move the native function",
            "+                setattr(torch_module, f\"native_{func}\", native_func)",
            "# 5. Put instead the hooked one",
            "setattr(torch_module, func, new_func)"
        ]
    },
    {
        "number": 12,
        "comments": "add param for type fix",
        "commit_message": "[Enhance] Added dtype flag to geometry module (#820)\n\n* Added docs and tests\n\n* Fixed bug for batch prob generator\n\n* Fixed typo\n\n* Fixed mypy\n\n* Fixed mypy\n\n* Added warning\n\n* Updated tests for geometry\n\n* Added dtype for conversions\n\n* Completed tests\n\n* Fixed homography\n\n* Fixed pinhole camera\n\n* Fixed tests\n\n* Fix geometry testing bugs\n\n* Fixed augmentation tests\n\n* code updated\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def get_rotation_matrix2d(",
            "",
            "# create output tensor",
            "batch_size: int = center.shape[0]",
            "-    one = torch.tensor(1.).to(center.device)",
            "+    one = torch.tensor(1., device=center.device, dtype=center.dtype)",
            "M: torch.Tensor = torch.zeros(",
            "batch_size, 2, 3, device=center.device, dtype=center.dtype)",
            "M[..., 0:2, 0:2] = scaled_rotation"
        ]
    },
    {
        "number": 15,
        "comments": "add condition check for state fix",
        "commit_message": "save/load model only for rank==0 (#212)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/212\n\nSave/load can break for distributed training with multiple processes writing to the same file.  Fix by only doing it for rank 0.\n\nReviewed By: hikushalhere, seayoung1112\n\nDifferential Revision: D13704156\n\nfbshipit-source-id: cb468f76ccda93e29735ab7badb130fedf946df9\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Trainer(TrainerBase):",
            "break",
            "sys.stdout.flush()",
            "",
            "-        model.load_state_dict(torch.load(best_model_path))",
            "+        if rank == 0:",
            "+            model.load_state_dict(torch.load(best_model_path))",
            "return model, best_metric",
            "",
            "def _run_epoch("
        ]
    },
    {
        "number": 17,
        "comments": "change class name",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "Returns:",
            "\"\"\"",
            "",
            "",
            "-class BERTScore(nlp.Metric):",
            "+class BERTScore(datasets.Metric):",
            "def _info(self):",
            "-        return nlp.MetricInfo(",
            "+        return datasets.MetricInfo(",
            "description=_DESCRIPTION,",
            "citation=_CITATION,",
            "homepage=\"https://github.com/Tiiiger/bert_score\",",
            "inputs_description=_KWARGS_DESCRIPTION,",
            "-            features=nlp.Features(",
            "+            features=datasets.Features(",
            "{",
            "-                    \"predictions\": nlp.Value(\"string\", id=\"sequence\"),",
            "-                    \"references\": nlp.Sequence(nlp.Value(\"string\", id=\"sequence\"), id=\"references\"),",
            "+                    \"predictions\": datasets.Value(\"string\", id=\"sequence\"),",
            "+                    \"references\": datasets.Sequence(datasets.Value(\"string\", id=\"sequence\"), id=\"references\"),",
            "}",
            "),",
            "codebase_urls=[\"https://github.com/Tiiiger/bert_score\"],"
        ]
    },
    {
        "number": 19,
        "comments": "style fix",
        "commit_message": "Rename config and environment variable for in memory max size (#2454)\n\n* Rename config and env variable IN_MEMORY_MAX_SIZE\n\n* Rename also in documentation\n\n* Fix style\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def test_load_from_disk_with_default_in_memory(",
            "current_dataset_size = 512  # arrow file size = 512, in-memory dataset size = 148",
            "if max_in_memory_dataset_size == \"default\":",
            "# default = 250 * 2 ** 20",
            "-        max_in_memory_dataset_size = datasets.config.HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES",
            "+        max_in_memory_dataset_size = datasets.config.IN_MEMORY_MAX_SIZE",
            "else:",
            "-        monkeypatch.setattr(datasets.config, \"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", max_in_memory_dataset_size)",
            "+        monkeypatch.setattr(datasets.config, \"IN_MEMORY_MAX_SIZE\", max_in_memory_dataset_size)",
            "if max_in_memory_dataset_size:",
            "expected_in_memory = current_dataset_size < max_in_memory_dataset_size",
            "else:"
        ]
    },
    {
        "number": 23,
        "comments": "add param for version fix",
        "commit_message": "fix unittests for the latest updates\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TacotronTrainTest(unittest.TestCase):",
            "optimizer = optim.Adam(model.parameters(), lr=c.lr)",
            "for i in range(5):",
            "mel_out, linear_out, align, stop_tokens = model.forward(",
            "-                input, input_lengths, mel_spec)",
            "+                input, input_lengths, mel_spec, speaker_ids)",
            "optimizer.zero_grad()",
            "loss = criterion(mel_out, mel_spec, mel_lengths)",
            "stop_loss = criterion_st(stop_tokens, stop_targets)"
        ]
    },
    {
        "number": 24,
        "comments": "add condition check for type fix",
        "commit_message": "Fix bug in examples: double wrap into DataParallel during eval\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def evaluate(args, model, tokenizer, prefix=\"\", test=False):",
            "eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)",
            "",
            "# multi-gpu evaluate",
            "-        if args.n_gpu > 1:",
            "+        if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):",
            "model = torch.nn.DataParallel(model)",
            "",
            "# Eval!"
        ]
    },
    {
        "number": 25,
        "comments": "add API call for resource fix",
        "commit_message": "fix cuda test issues\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TestMotionBlur:",
            ") -> torch.Tensor:",
            "return kornia.filters.motion_blur(input, ksize, angle, direction)",
            "",
            "-        img = torch.rand(2, 3, 4, 5)",
            "+        img = torch.rand(2, 3, 4, 5).to(device)",
            "ksize = 5",
            "angle = 65.",
            "direction = .1"
        ]
    },
    {
        "number": 30,
        "comments": "add condition check for math fix",
        "commit_message": "Misc fixes (#88)\n\n* make log_dir if it doesn't exist\n\n* use tqdm for all dataset readers\n\n* save vocab if log_dir exists\n\n* fix logging error\n\n* get around passing batch_first to custom lstms\n\n* fix srl default params\n\n* tentative fix for tensor creation\n\n* use tdqm at correct abstraction in srl reader\n\n* don't do boolean checks on tensors\n\n* use an actual tensor in get_dropout_mask to preserve type\n\n* ensure tensors are on cpu for logging in training loop\n\n* raise if Datasets from readers are empty\n\n* use os.path.join fr logging\n\n* update config\n\n* use incorrect american spelling of labeller\n\n* use more sensible order for bulding things in train\n\n* try different lstm in model\n\n* unpack validation tensors into forward\n\n* fix byteTensor overflow bug in masking\n\n* pylint\n\n* fix merge\n\n* fixes for Matt\n\n* fix pylint\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class _Seq2VecWrapper:",
            "def from_params(self, params: Params) -> PytorchSeq2VecWrapper:",
            "if not params.pop('batch_first', True):",
            "raise ConfigurationError(\"Our encoder semantics assumes batch is always first!\")",
            "-        params['batch_first'] = True",
            "+        if self._module_class in self.PYTORCH_MODELS:",
            "+            params['batch_first'] = True",
            "module = self._module_class(**params.as_dict())",
            "return PytorchSeq2VecWrapper(module)"
        ]
    },
    {
        "number": 31,
        "comments": "add param for argument fix",
        "commit_message": "Fixed non-square highres fix generation\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class StableDiffusionProcessingTxt2Img(StableDiffusionProcessing):",
            "return samples",
            "",
            "x = create_random_tensors([opt_C, self.firstphase_height // opt_f, self.firstphase_width // opt_f], seeds=seeds, subseeds=subseeds, subseed_strength=self.subseed_strength, seed_resize_from_h=self.seed_resize_from_h, seed_resize_from_w=self.seed_resize_from_w, p=self)",
            "-        samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.create_dummy_mask(x))",
            "+        samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.create_dummy_mask(x, first_phase=True))",
            "",
            "samples = samples[:, :, self.truncate_y//2:samples.shape[2]-self.truncate_y//2, self.truncate_x//2:samples.shape[3]-self.truncate_x//2]"
        ]
    },
    {
        "number": 35,
        "comments": "no API",
        "commit_message": "chamfer for empty pointclouds #1174\n\nSummary: Fix divide by zero for empty pointcloud in chamfer. Also for empty batches. In process, needed to regularize num_points_per_cloud for empty batches.\n\nReviewed By: kjchalup\n\nDifferential Revision: D36311330\n\nfbshipit-source-id: 3378ab738bee77ecc286f2110a5c8dc445960340\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class Pointclouds:",
            "self._compute_packed()",
            "return self._cloud_to_packed_first_idx",
            "",
            "-    def num_points_per_cloud(self):",
            "+    def num_points_per_cloud(self) -> torch.Tensor:",
            "\"\"\"",
            "Return a 1D tensor x with length equal to the number of clouds giving",
            "the number of points in each cloud."
        ]
    },
    {
        "number": 40,
        "comments": "update param for refactor fix",
        "commit_message": "fix isinstance() validation in pytorch\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def makenp(x, modality=None):",
            "",
            "def pytorch_np(x, modality):",
            "import torch",
            "-    if isinstance(x, torch.autograd.variable.Variable):",
            "+    if isinstance(x, torch.autograd.Variable):",
            "x = x.data",
            "x = x.cpu().numpy()",
            "if modality == 'IMG':"
        ]
    },
    {
        "number": 43,
        "comments": "change param for resource fix",
        "commit_message": "Removed unnecessary `_move_optimizer_state` method overrides (#10849)\n\n* Update tpu tp share same logic with ttp\n\n* run test\n\n* Update tpu_spawn.py\n\n* debug\n\n* Add changelog\n\n* Apply suggestions from code review\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* Update training_type_plugin.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update training_type_plugin.py\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TrainingTypePlugin(ABC):",
            "self.lr_schedulers = schedulers",
            "",
            "def _move_optimizer_state(self, device: Optional[torch.device] = None) -> None:",
            "-        \"\"\"Moves the state of the optimizers to the GPU if needed.\"\"\"",
            "-        device = device or self.root_device",
            "+        \"\"\"Moves the state of the optimizers to the appropriate device if needed.\"\"\"",
            "for opt in self.optimizers:",
            "for p, v in opt.state.items():",
            "-                opt.state[p] = apply_to_collection(v, torch.Tensor, move_data_to_device, device)",
            "+                # `self.root_device` would raise error if called outside the spawn process",
            "+                # while training on 8 and more cores.",
            "+                opt.state[p] = apply_to_collection(v, torch.Tensor, move_data_to_device, device or self.root_device)",
            "",
            "def optimizer_state(self, optimizer: Optimizer) -> Dict[str, Tensor]:",
            "\"\"\"Returns state of an optimizer."
        ]
    },
    {
        "number": 49,
        "comments": "remove comments",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class LayerNorm(torch.nn.Module):",
            "self.beta = torch.nn.Parameter(torch.zeros(dimension))",
            "self.eps = eps",
            "",
            "-    def forward(self, tensor: torch.Tensor):  # pylint: disable=arguments-differ",
            "+    def forward(self, tensor: torch.Tensor):",
            "mean = tensor.mean(-1, keepdim=True)",
            "std = tensor.std(-1, unbiased=False, keepdim=True)",
            "return self.gamma * (tensor - mean) / (std + self.eps) + self.beta"
        ]
    },
    {
        "number": 50,
        "comments": "add condition check for shape fix",
        "commit_message": "Fix: `to_hetero` with `GCN` on single node types (#4279)\n\n* fix\n\n* typo\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class GraphConv(MessagePassing):",
            "self.lin.reset_parameters()",
            "",
            "def forward(self, x, edge_index):",
            "+        if isinstance(x, Tensor):",
            "+            x = (x, x)",
            "return self.propagate(edge_index, x=(self.lin(x[0]), x[1]))"
        ]
    },
    {
        "number": 53,
        "comments": "update API call for version fix",
        "commit_message": "update smddp api to v1.4.0 (#16371)\n\n* update smddp api to v1.4.0\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* address comments\n\n* fix style\n\n* remove unused import\n\n* fix indent\n\n* disable style check for import\n\n* fix space\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Trainer:",
            ").to(self.args.device)",
            "",
            "elif is_sagemaker_dp_enabled():",
            "-            model = DDP(model, device_ids=[dist.get_local_rank()], broadcast_buffers=False)",
            "+            model = nn.parallel.DistributedDataParallel(",
            "+                model, device_ids=[int(os.getenv(\"SMDATAPARALLEL_LOCAL_RANK\"))]",
            "+            )",
            "elif self.args.local_rank != -1:",
            "kwargs = {}",
            "if self.args.ddp_find_unused_parameters is not None:"
        ]
    },
    {
        "number": 55,
        "comments": "add condition check for state fix",
        "commit_message": "fix error for rnn encoders flatten_parameters\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class RNN(torch.nn.Module):",
            "if not isinstance(ilens, torch.Tensor):",
            "ilens = torch.tensor(ilens)",
            "xs_pack = pack_padded_sequence(xs_pad, ilens.cpu(), batch_first=True)",
            "-        self.nbrnn.flatten_parameters()",
            "+        if self.training:",
            "+            self.nbrnn.flatten_parameters()",
            "if prev_state is not None and self.nbrnn.bidirectional:",
            "# We assume that when previous state is passed,",
            "# it means that we're streaming the input"
        ]
    },
    {
        "number": 56,
        "comments": "add API call for type fix",
        "commit_message": "XLA train step fixes (#17973)\n\n* Copy inputs to train and test step before modifying them, as this breaks things\n\n* Add XLA tests, fix our loss functions to be XLA-compatible\n\n* make fixup\n\n* Update loss computation test to expect vector of per-sample losses\n\n* Patch loss for TFLED\n\n* Patch loss for TFAlbert\n\n* Add a tf_legacy_loss config flag that enables old loss functions\n\n* Stop using config.get() because it's not a dict\n\n* Skip loss computation test for RAG because its loss is very strange and I'm afraid to rewrite it\n\n* make fixup\n\n* Add XLA-compatible RAG loss\n\n* Fix dtype of loss mask for TFAlbert\n\n* Fix test for XLNet too because it overrides the default one\n\n* make fixup\n\n* Fix config test\n\n* No more depending on GPU NaN behaviour\n\n* Add test, avoid potential zero division\n\n* Fix test item assignment\n\n* Fix loss computation masking test\n\n* make fixup\n\n* Fix dtype bugs\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFXLNetModelTest(TFModelTesterMixin, unittest.TestCase):",
            "# Send to model",
            "loss = model(tuple_input[:-1])[0]",
            "",
            "-                self.assertEqual(loss.shape, [loss_size])",
            "+                self.assertEqual(loss.shape.as_list(), expected_loss_size)",
            "",
            "",
            "@require_tf"
        ]
    },
    {
        "number": 58,
        "comments": "change param for shape fix",
        "commit_message": "Add OED estimators and accompanying tests (#1925)\n\n* Update EIG estimators and the accompanying tests\n\n* add back some line spacing\n\n* no need to add pre-release methods\n\n* delete old examples\n\n* remove deleted examples from test list\n\n* stray newline\n\n* remove other irrelevant code\n\n* flake8\n\n* do not use CensoredSigmoidNormal\n\n* remove unnecessary expands\n\n* another flake8\n\n* improve docstrings in glmm\n\n* use reshape in place of contiguous().view\n\n* add docstrings to oed/util.py\n\n* add reason= and deprecation warning\n\n* add a lot of docstrings\n\n* double escape\n\n* try writing warning on one line\n\n* rewrite broadcast_cat to use broadcast_tensors\n\n* rename loss functions\n\n* make use of nn.ParameterDict\n\n* always return an EIG by default\n\n* do not update eig_estimation_benchmarking on this PR\n\n* fix issue with glmm\n\n* change from martin\n\n* add citations\n\n* remove N_seq\n\n* rename mean_field_guide_entropy as mean_field_entropy, allow user to avoid mean-field assumption\n\n* delete eig_estimation_benchmarking\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def sigmoid_example(design):",
            "torch.tensor([[-1.5, 0.5], [1.5, 0.]])",
            "),",
            "(",
            "-        known_covariance_linear_model(torch.tensor([1., -1.]), torch.tensor(10.), torch.tensor(1.)),",
            "+        known_covariance_linear_model(torch.tensor([1., -1.]), torch.tensor([10., 10.]), torch.tensor(1.)),",
            "nz_lm_2p_10_10_1,",
            "torch.tensor([[-1., 0.5], [2.5, -2.]])",
            "),"
        ]
    },
    {
        "number": 59,
        "comments": "add API call for math fix",
        "commit_message": "prepare for \"__floordiv__ is deprecated  and its behavior will change in a future version of pytorch\" (#20211)\n\n* rounding_mode = \"floor\"  instead of // to prevent behavioral change\n\n* add other TODO\n\n* use `torch_int_div` from pytrch_utils\n\n* same for tests\n\n* fix copies\n\n* style\n\n* use relative imports when needed\n\n* Co-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DetaModel(DetaPreTrainedModel):",
            "scale = 2 * math.pi",
            "",
            "dim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=proposals.device)",
            "-        dim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats)",
            "+        dim_t = temperature ** (2 * torch.div(dim_t, 2) / num_pos_feats)",
            "# batch_size, num_queries, 4",
            "proposals = proposals.sigmoid() * scale",
            "# batch_size, num_queries, 4, 128"
        ]
    },
    {
        "number": 61,
        "comments": "update API call for refactor fix",
        "commit_message": "use functional interface for softmax in attention (#14198)\n\n* use functional interface instead of instantiating module and immediately calling it\n\n* fix torch.nn.functional to nn.functional. Thank you Stas!\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class LxmertAttention(nn.Module):",
            "attention_scores = attention_scores + attention_mask",
            "",
            "# Normalize the attention scores to probabilities.",
            "-        attention_probs = nn.Softmax(dim=-1)(attention_scores)",
            "+        attention_probs = nn.functional.softmax(attention_scores, dim=-1)",
            "",
            "# This is actually dropping out entire tokens to attend to, which might",
            "# seem a bit unusual, but is taken from the original Transformer paper."
        ]
    },
    {
        "number": 66,
        "comments": "format",
        "commit_message": "[MOD] fix typo, add tests for ldconv\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class DynamicConvolution2D(nn.Module):",
            "weight = self.linear_weight(x)  # B x T x kH",
            "weight = F.dropout(weight, self.dropout_rate, training=self.training)",
            "weight = weight.view(B, T, H, k).transpose(1, 2).contiguous()  # B x H x T x k",
            "-        weight_new = torch.zeros(B * H * T * (T + k - 1)).view(B, H, T, T + k - 1).fill_(float('-inf'))",
            "+        weight_new = torch.zeros(B * H * T * (T + k - 1), dtype=weight.dtype)",
            "+        weight_new = weight_new.view(B, H, T, T + k - 1).fill_(float('-inf'))",
            "weight_new = weight_new.to(x.device)  # B x H x T x T+k-1",
            "weight_new.as_strided((B, H, T, k), ((T + k - 1) * T * H, (T + k - 1) * T, T + k, 1)).copy_(weight)",
            "weight_new = weight_new.narrow(-1, int((k - 1) / 2), T)  # B x H x T x T(k)"
        ]
    },
    {
        "number": 67,
        "comments": "refactor fix",
        "commit_message": "Fixes to import\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def test_ddp_sharded_plugin_correctness_multi_gpu():",
            "run_sharded_correctness(gpus=2, accelerator='ddp_spawn')",
            "",
            "",
            "-@pytest.mark.skipif(",
            "-    LooseVersion(torch.__version__) < LooseVersion(\"1.6.0\"),",
            "-    reason=\"Minimal PT version is set to 1.6\")",
            "+@pytest.mark.skipif(not NATIVE_AMP_AVALAIBLE, reason=\"Requires native AMP\")",
            "@pytest.mark.skipif(platform.system() == \"Windows\",",
            "reason=\"Distributed training is not supported on Windows\")",
            "@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=\"test requires multi-GPU machine\")"
        ]
    },
    {
        "number": 68,
        "comments": "add param for argument fix",
        "commit_message": "fixed trpo\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Gru(TransformationBase):",
            "",
            "def tf_apply(self, x, sequence_length=None):",
            "x, state = tf.nn.dynamic_rnn(",
            "-            cell=self.cell, inputs=x, sequence_length=sequence_length, dtype=tf.float32,",
            "+            cell=self.cell, inputs=x, sequence_length=sequence_length, initial_state=None,",
            "+            dtype=util.tf_dtype(dtype='float'),",
            "# Weird TensorFlow behavior? (https://github.com/tensorflow/tensorflow/issues/15874)",
            "parallel_iterations=(self.input_spec['shape'][0] + 1)",
            ")"
        ]
    },
    {
        "number": 71,
        "comments": "remove condition check for resource fix",
        "commit_message": "Fix quantized-inference & Add generic support of checkpoint loading (#2547)\n\n* fix checkpoint loading when it is a dictionary\n\n* fix some issues with saving ckpt & int8 inference\n\n* fix quantized-inference & add generic support of checkpoint loading\n\n* remove int8 hard-coded flag\n\n* fix mlp return tensors\n\n* fix several issue to load checkpoints of GPT-J, GPT-NEOX, and OPT with different TP-size\n\n* add more comments & description for checkpoint-loading module\n\nCo-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DeepSpeedSelfAttention(nn.Module):",
            "data_type_fp = torch.half if config.fp16 else torch.float",
            "self.config.layer_id = DeepSpeedSelfAttention.num_layers",
            "DeepSpeedSelfAttention.num_layers = DeepSpeedSelfAttention.num_layers + 1",
            "-        device = torch.cuda.current_device() if config.bigscience_bloom else 'cpu'",
            "+        device = torch.cuda.current_device()  #if config.bigscience_bloom else 'cpu'",
            "qkv_size_per_partition = (self.config.hidden_size // self.config.mp_size) * 3",
            "self.attn_qkvw = nn.Parameter(torch.empty(self.config.hidden_size,",
            "qkv_size_per_partition,"
        ]
    },
    {
        "number": 74,
        "comments": "version fix",
        "commit_message": "[RLlib] `rllib train` crashes when using torch PPO/PG/A2C. (#7508)\n\n* Fix.\n\n* Rollback.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n\n* TEST.\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class TorchCategorical(TorchDistributionWrapper):",
            "@override(ActionDistribution)",
            "def __init__(self, inputs, model=None, temperature=1.0):",
            "assert temperature > 0.0, \"Categorical `temperature` must be > 0.0!\"",
            "-        super().__init__(inputs / temperature, model)",
            "+        inputs /= temperature",
            "+        super().__init__(inputs, model)",
            "self.dist = torch.distributions.categorical.Categorical(",
            "logits=self.inputs)"
        ]
    },
    {
        "number": 76,
        "comments": "remove API call for version fix",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def main(args):",
            "bob_decision = Marginal(Search(bob))",
            "",
            "# Here Alice and Bob slightly prefer one location over the other a priori",
            "-    shared_preference = Variable(torch.Tensor([args.preference]))",
            "+    shared_preference = torch.tensor([args.preference])",
            "",
            "bob_depth = args.depth",
            "num_samples = args.num_samples"
        ]
    },
    {
        "number": 77,
        "comments": "format",
        "commit_message": "Fixing missing var\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "if torch.backends.cudnn.version() >= 7603:",
            "#",
            "# Channels Last support not limited by existing models, as any model can be converted to Channels Last and propagate format through the graph as soon as input formatted correctly.",
            "#",
            "-input = input.to(memory_format=torch.channels_last)",
            "-model = model.to(memory_format=torch.channels_last)",
            "+",
            "+# Need to be done once, after model initialization (or load)",
            "+model = model.to(memory_format=torch.channels_last) # Replace with your model",
            "+",
            "+# Need to be done for every input",
            "+input = input.to(memory_format=torch.channels_last) # Replace with your input",
            "output = model(input)",
            "",
            "#######################################################################"
        ]
    },
    {
        "number": 81,
        "comments": "add API call for type fix",
        "commit_message": "fixed mnist superpixels\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Planetoid(Dataset):",
            "# Create unweighted sparse adjacency matrix.",
            "weight = torch.ones(index.size(1))",
            "n = input.size(0)",
            "-        adj = torch.sparse.FloatTensor(index, weight, torch.Size([n, n]))",
            "+        adj = SparseTensor(index, weight, torch.Size([n, n]))",
            "",
            "# Bundle graph to data object.",
            "-        self.data = Data(input, adj, position=None, target=target)",
            "+        self.data = Data(input, adj, position=None, target=target.long())",
            "",
            "def __getitem__(self, index):",
            "data = self.data"
        ]
    },
    {
        "number": 84,
        "comments": "add API call for state fix",
        "commit_message": "fixed multiple super\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Tacotron2(TTSInterface, torch.nn.Module):",
            "",
            "def __init__(self, idim, odim, args):",
            "super(Tacotron2, self).__init__()",
            "+        torch.nn.Module.__init__(self)",
            "# store hyperparameters",
            "self.idim = idim",
            "self.odim = odim"
        ]
    },
    {
        "number": 85,
        "comments": "add condition check for resource fix",
        "commit_message": "Fix universal sentence encoder colab\n\nPiperOrigin-RevId: 387544699\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "\"import sys\\n\",",
            "\"sys.path.append(f'{os.getcwd()}/SentEval')\\n\",",
            "\"\\n\",",
            "-        \"import tensorflow_hub as hub\\n\",",
            "\"import tensorflow as tf\\n\",",
            "+        \"\\n\",",
            "+        \"# Prevent TF from claiming all GPU memory so there is some left for pytorch.\\n\",",
            "+        \"gpus = tf.config.list_physical_devices('GPU')\\n\",",
            "+        \"if gpus:\\n\",",
            "+        \"  # Memory growth needs to be the same across GPUs.\\n\",",
            "+        \"  for gpu in gpus:\\n\",",
            "+        \"    tf.config.experimental.set_memory_growth(gpu, True)\\n\",",
            "+        \"\\n\",",
            "+        \"import tensorflow_hub as hub\\n\",",
            "\"import tensorflow_text\\n\",",
            "\"import senteval\\n\",",
            "\"import time\\n\","
        ]
    },
    {
        "number": 86,
        "comments": "add param for argument fix",
        "commit_message": "Fix conflict\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Encoder(torch.nn.Module):",
            "self.embed = Conv2dSubsampling(idim, attention_dim, dropout_rate)",
            "elif input_layer == \"embed\":",
            "self.embed = torch.nn.Sequential(",
            "-                torch.nn.Embedding(idim, attention_dim),",
            "+                torch.nn.Embedding(idim, attention_dim, padding_idx=padding_idx),",
            "pos_enc_class(attention_dim, positional_dropout_rate)",
            ")",
            "elif isinstance(input_layer, torch.nn.Module):"
        ]
    },
    {
        "number": 88,
        "comments": "add param for type fix",
        "commit_message": "[Enhance] Added dtype flag to geometry module (#820)\n\n* Added docs and tests\n\n* Fixed bug for batch prob generator\n\n* Fixed typo\n\n* Fixed mypy\n\n* Fixed mypy\n\n* Added warning\n\n* Updated tests for geometry\n\n* Added dtype for conversions\n\n* Completed tests\n\n* Fixed homography\n\n* Fixed pinhole camera\n\n* Fixed tests\n\n* Fix geometry testing bugs\n\n* Fixed augmentation tests\n\n* code updated\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def create_checkerboard(h, w, nw):",
            "",
            "",
            "# TODO: Isn't this function duplicated with eye_like?",
            "-def create_eye_batch(batch_size, eye_size):",
            "+def create_eye_batch(batch_size, eye_size, device=None, dtype=None):",
            "\"\"\"Creates a batch of identity matrices of shape Bx3x3",
            "\"\"\"",
            "-    return torch.eye(eye_size).view(",
            "+    return torch.eye(eye_size, device=device, dtype=dtype).view(",
            "1, eye_size, eye_size).expand(batch_size, -1, -1)"
        ]
    },
    {
        "number": 91,
        "comments": "change condition check for version fix",
        "commit_message": "Move complex tensor related functions to espnet2/enh/layers/complex_utils.py; Refactor complex support code; fix unit tests\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TransformerSeparator(AbsSeparator):",
            "",
            "# if complex spectrum,",
            "if isinstance(input, ComplexTensor) or (",
            "-            is_torch_1_8_plus and torch.is_complex(input)",
            "+            is_torch_1_9_plus and torch.is_complex(input)",
            "):",
            "feature = abs(input)",
            "else:"
        ]
    },
    {
        "number": 94,
        "comments": "API update",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class T5Attention(nn.Module):",
            "position_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)",
            "",
            "scores += position_bias",
            "-        attn_weights = F.softmax(scores.float(), dim=-1).type_as(",
            "+        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(",
            "scores",
            ")  # (batch_size, n_heads, seq_length, key_length)",
            "-        attn_weights = F.dropout(",
            "+        attn_weights = nn.functional.dropout(",
            "attn_weights, p=self.dropout, training=self.training",
            ")  # (batch_size, n_heads, seq_length, key_length)"
        ]
    },
    {
        "number": 97,
        "comments": "add API call for type fix",
        "commit_message": "Fix bug with multiple models running in parallels (#43)\n\n* add possibility to select the maximum number of threads per model to be used\n\n* change version\n\nCo-authored-by: morgoth95 <diego.fiori@epfl.ch>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def _get_ort_session_options() -> ort.SessionOptions:",
            "if not torch.cuda.is_available():",
            "sess_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL",
            "sess_options.inter_op_num_threads = 1",
            "-        sess_options.intra_op_num_threads = max(torch.get_num_threads(), 1)",
            "+        sess_options.intra_op_num_threads = max(",
            "+            int(",
            "+                os.environ.get(\"NEBULLVM_THREADS_PER_MODEL\")",
            "+                or torch.get_num_threads()",
            "+            ),",
            "+            1,",
            "+        )",
            "return sess_options"
        ]
    },
    {
        "number": 99,
        "comments": "add condition check for type fix",
        "commit_message": "fix quaternion_exp_to_log test errors\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def quaternion_exp_to_log(quaternion: torch.Tensor,",
            ">>> kornia.quaternion_exp_to_log(quaternion)",
            "tensor([0., 0., 0.])",
            "\"\"\"",
            "-    if not torch.is_tensor(quaternion):",
            "+    if not isinstance(quaternion, torch.Tensor):",
            "raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(",
            "type(quaternion)))"
        ]
    },
    {
        "number": 100,
        "comments": "add API call for type fix",
        "commit_message": "Replace strided slice with tf.expand_dims (#10078)\n\n* Replace tf.newaxis -> tf.expand_dims\n\n* Fix tests\n\n* Fix tests\n\n* Use reshape when a tensors needs a double expand\n\n* Fix GPT2\n\n* Fix GPT2\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFFlaubertMainLayer(tf.keras.layers.Layer):",
            "tensor_normalized = self.layer_norm2[i](tensor)",
            "tensor = tensor + self.ffns[i](tensor_normalized)",
            "",
            "-            tensor = tensor * mask[..., tf.newaxis]",
            "+            tensor = tensor * tf.expand_dims(mask, axis=-1)",
            "",
            "# Add last hidden state",
            "if inputs[\"output_hidden_states\"]:"
        ]
    },
    {
        "number": 101,
        "comments": "change API call for type fix",
        "commit_message": "fix bug in 552c2b3b51c83fc27bdf6df691f1d707e40865f9\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def _replace_global_by_local(kwargs):",
            "if 'collections' in kwargs:",
            "collections = kwargs['collections']",
            "if not collections:",
            "-        collections = set(tf.GraphKeys.GLOBAL_VARIABLES)",
            "+        collections = {tf.GraphKeys.GLOBAL_VARIABLES}",
            "else:",
            "collections = set(collections.copy())",
            "collections.remove(tf.GraphKeys.GLOBAL_VARIABLES)"
        ]
    },
    {
        "number": 103,
        "comments": "change condition check for null fix",
        "commit_message": "fix for pytorch < 1.6 (#6300)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ReformerLayer(nn.Module):",
            "\"\"\"",
            "# randomize seeds",
            "# use cuda generator if available",
            "-        if len(torch.cuda.default_generators) > 0:",
            "+        if hasattr(torch.cuda, \"default_generators\") and len(torch.cuda.default_generators) > 0:",
            "# GPU",
            "device_idx = torch.cuda.current_device()",
            "self.feed_forward_seed = torch.cuda.default_generators[device_idx].seed()"
        ]
    },
    {
        "number": 104,
        "comments": "add API call for shape fix",
        "commit_message": "Math fixes for 3d affine transformation & doc updates\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def get_projective_transform(center: torch.Tensor, angles: torch.Tensor) -> torc",
            "",
            "# create rotation matrix",
            "angle_axis_rad: torch.Tensor = K.deg2rad(angles)",
            "-    rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad)  # Bx3x3",
            "+    rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad) * scales.view(-1, 1, 1)  # Bx3x3",
            "",
            "# define matrix to move forth and back to origin",
            "from_origin_mat = torch.eye(4)[None].repeat(rmat.shape[0], 1, 1).type_as(center)  # Bx4x4"
        ]
    },
    {
        "number": 105,
        "comments": "add param for math fix",
        "commit_message": "[RLlib]: Off-Policy Evaluation fixes. (#25899)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class FQETorchModel:",
            "q_values, _ = self.q_model({\"obs\": obs}, [], None)",
            "if actions is not None:",
            "actions = torch.tensor(actions, device=self.device, dtype=int)",
            "-            q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze()",
            "+            q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze(-1)",
            "return q_values.detach()",
            "",
            "def estimate_v("
        ]
    },
    {
        "number": 107,
        "comments": "add condition check for refactor fix",
        "commit_message": "[RLlib] Fixed import tensorflow when module not available (#16171)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class ModelCatalog:",
            "model_name (str): Name to register the model under.",
            "model_class (type): Python class of the model.",
            "\"\"\"",
            "-        if issubclass(model_class, tf.keras.Model):",
            "-            deprecation_warning(old=\"register_custom_model\", error=False)",
            "+        if tf is not None:",
            "+            if issubclass(model_class, tf.keras.Model):",
            "+                deprecation_warning(old=\"register_custom_model\", error=False)",
            "_global_registry.register(RLLIB_MODEL, model_name, model_class)",
            "",
            "@staticmethod"
        ]
    },
    {
        "number": 109,
        "comments": "remove condition check for shape fix",
        "commit_message": "Refactoring, cleanup, improved test coverage.\n* Add eca_nfnet_l2 weights, 84.7 @ 384x384\n* All 'non-std' (ie transformer / mlp) models have classifier / default_cfg test added\n* Fix #694 reset_classifer / num_features / forward_features / num_classes=0 consistency for transformer / mlp models\n* Add direct loading of npz to vision transformer (pure transformer so far, hybrid to come)\n* Rename vit_deit* to deit_*\n* Remove some deprecated vit hybrid model defs\n* Clean up classifier flatten for conv classifiers and unusual cases (mobilenetv3/ghostnet)\n* Remove explicit model fns for levit conv, just pass in arg\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DLA(nn.Module):",
            "if self.drop_rate > 0.:",
            "x = F.dropout(x, p=self.drop_rate, training=self.training)",
            "x = self.fc(x)",
            "-        if not self.global_pool.is_identity():",
            "-            x = x.flatten(1)  # conv classifier, flatten if pooling isn't pass-through (disabled)",
            "+        x = self.flatten(x)",
            "return x"
        ]
    },
    {
        "number": 110,
        "comments": "add API call for type fix",
        "commit_message": "[RLlib] Bandit tf2 fix (+ add tf2 to test cases). (#24908)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class OnlineLinearRegression(tf.Module if tf else object):",
            "x = tf.squeeze(x, axis=0)",
            "y = y[0]",
            "self.time += 1",
            "-        self.delta_f += y * x",
            "+        self.delta_f += tf.cast(y, tf.float32) * x",
            "self.delta_b += tf.tensordot(x, x, axes=0)",
            "# Can follow an update schedule if not doing sherman morison updates",
            "if self.time % self.update_schedule == 0:"
        ]
    },
    {
        "number": 112,
        "comments": "add API call for math fix",
        "commit_message": "Fix disabled grads after call to predict (#6657)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Trainer(",
            "",
            "results = self.predict_loop.on_predict_epoch_end()",
            "self.predict_loop.on_predict_end()",
            "+",
            "+        # re-enable grads",
            "+        torch.set_grad_enabled(True)",
            "+",
            "return results",
            "",
            "def run_sanity_check(self, ref_model):"
        ]
    },
    {
        "number": 113,
        "comments": "add API call for type fix",
        "commit_message": "Augmentation Base Refactor (#2117)\n\n* refactor\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Updated augmentation base.\n\n* Removed label & start apply/inverse\n\n* Finished augmentaion base refactor\n\n* container refactoring\n\n* Added missing files\n\n* Added ops\n\n* Update sequential ops\n\n* Almost there\n\n* Fixed computation matrix computation\n\n* Fixed randomcrop\n\n* Fixed erasing\n\n* almost almost\n\n* finished\n\n* Added missing file\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Bug and typing fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Added list typing\n\n* fixed test base\n\n* Fixed typing\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* bug fix\n\n* Update kornia/augmentation/_2d/geometric/crop.py\n\nCo-authored-by: Jo\u00e3o Gustavo A. Amorim <joaogustavoamorim@gmail.com>\n\n* Fixed build-docs\n\n* Fixed bfloat16 issue on torch1.13.1\n\n* Revert the last commit\n\n* Fixed typing\n\n* Fixed typos\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Typo fix\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Jo\u00e3o Gustavo A. Amorim <joaogustavoamorim@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def filter2d(",
            "input = input.view(-1, tmp_kernel.size(0), input.size(-2), input.size(-1))",
            "",
            "# convolve the tensor with the kernel.",
            "-    output = F.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1)",
            "+    # NOTE: type(...) to fix getting `torch.bfloat16` type.",
            "+    # TODO: @johnnv1, fix it through the Augmentation Base.",
            "+    output = F.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1).type(input.dtype)",
            "",
            "if padding == 'same':",
            "out = output.view(b, c, h, w)"
        ]
    },
    {
        "number": 114,
        "comments": "add API call for type fix",
        "commit_message": "small fixes to remainder dtype handling when modulus=False.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def remainder(",
            "res_floored = torch.where(res >= 0, torch.floor(res), torch.ceil(res))",
            "diff = res - res_floored",
            "diff, x2 = ivy.promote_types_of_inputs(diff, x2)",
            "-        return torch.mul(diff, x2, out=out)",
            "+        return torch.mul(diff, x2, out=out).to(x1.dtype)",
            "return torch.remainder(x1, x2, out=out)"
        ]
    },
    {
        "number": 115,
        "comments": "change param for math fix",
        "commit_message": "fix: action mask parameter\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class GoalOrientedBotNetwork(TFModel):",
            "name='features')",
            "self._action = tf.placeholder(tf.int32, [1, None],",
            "name='ground_truth_action')",
            "-        self._action_mask = tf.placeholder(tf.float32, [1, None, self.n_actions],",
            "+        self._action_mask = tf.placeholder(tf.float32, [None, None, self.n_actions],",
            "name='action_mask')",
            "",
            "def _build_body(self):"
        ]
    },
    {
        "number": 116,
        "comments": "add API call for resource fix",
        "commit_message": "[RLlib] SAC Torch (incl. Atari learning) (#7984)\n\n* Policy-classes cleanup and torch/tf unification.\n- Make Policy abstract.\n- Add `action_dist` to call to `extra_action_out_fn` (necessary for PPO torch).\n- Move some methods and vars to base Policy\n  (from TFPolicy): num_state_tensors, ACTION_PROB, ACTION_LOGP and some more.\n\n* Fix `clip_action` import from Policy (should probably be moved into utils altogether).\n\n* - Move `is_recurrent()` and `num_state_tensors()` into TFPolicy (from DynamicTFPolicy).\n- Add config to all Policy c'tor calls (as 3rd arg after obs and action spaces).\n\n* Add `config` to c'tor call to TFPolicy.\n\n* Add missing `config` to c'tor call to TFPolicy in marvil_policy.py.\n\n* Fix test_rollout_worker.py::MockPolicy and BadPolicy classes (Policy base class is now abstract).\n\n* Fix LINT errors in Policy classes.\n\n* Implement StatefulPolicy abstract methods in test cases: test_multi_agent_env.py.\n\n* policy.py LINT errors.\n\n* Create a simple TestPolicy to sub-class from when testing Policies (reduces code in some test cases).\n\n* policy.py\n- Remove abstractmethod from `apply_gradients` and `compute_gradients` (these are not required iff `learn_on_batch` implemented).\n- Fix docstring of `num_state_tensors`.\n\n* Make QMIX torch Policy a child of TorchPolicy (instead of Policy).\n\n* QMixPolicy add empty implementations of abstract Policy methods.\n\n* Store Policy's config in self.config in base Policy c'tor.\n\n* - Make only compute_actions in base Policy's an abstractmethod and provide pass\nimplementation to all other methods if not defined.\n- Fix state_batches=None (most Policies don't have internal states).\n\n* Cartpole tf learning.\n\n* Cartpole tf AND torch learning (in ~ same ts).\n\n* Cartpole tf AND torch learning (in ~ same ts). 2\n\n* Cartpole tf (torch syntax-broken) learning (in ~ same ts). 3\n\n* Cartpole tf AND torch learning (in ~ same ts). 4\n\n* Cartpole tf AND torch learning (in ~ same ts). 5\n\n* Cartpole tf AND torch learning (in ~ same ts). 6\n\n* Cartpole tf AND torch learning (in ~ same ts). Pendulum tf learning.\n\n* WIP.\n\n* WIP.\n\n* SAC torch learning Pendulum.\n\n* WIP.\n\n* SAC torch and tf learning Pendulum and Cartpole after cleanup.\n\n* WIP.\n\n* LINT.\n\n* LINT.\n\n* SAC: Move policy.target_model to policy.device as well.\n\n* Fixes and cleanup.\n\n* Fix data-format of tf keras Conv2d layers (broken for some tf-versions which have data_format=\"channels_first\" as default).\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fix and LINT.\n\n* WIP.\n\n* Test fixes and LINT.\n\n* Fixes and LINT.\n\nCo-authored-by: Sven Mika <sven@Svens-MacBook-Pro.local>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class EpsilonGreedy(Exploration):",
            "torch.multinomial(random_valid_action_logits, 1), axis=1)",
            "# Pick either random or greedy.",
            "action = torch.where(",
            "-                torch.empty((batch_size, )).uniform_() < epsilon,",
            "+                torch.empty(",
            "+                    (batch_size, )).uniform_().to(self.device) < epsilon,",
            "random_actions, exploit_action)",
            "",
            "return action, action_logp"
        ]
    },
    {
        "number": 118,
        "comments": "add API call for type fix",
        "commit_message": "Fix bug in dist.Delta.expand() type (#1236)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Delta(TorchDistribution):",
            "",
            "def expand(self, batch_shape):",
            "validate_args = self.__dict__.get('_validate_args')",
            "+        batch_shape = torch.Size(batch_shape)",
            "v = self.v.expand(batch_shape + self.event_shape)",
            "log_density = self.log_density.expand(batch_shape)",
            "return Delta(v, log_density, self.event_dim, validate_args=validate_args)"
        ]
    },
    {
        "number": 119,
        "comments": "add API call for resource fix",
        "commit_message": "fix buffer transfer bug (#2045)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def main():",
            "",
            "pruner = AGP_Pruner(model, configure_list)",
            "model = pruner.compress()",
            "-",
            "+    model = model.to(device)",
            "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)",
            "for epoch in range(10):",
            "pruner.update_epoch(epoch)",
            "print('# Epoch {} #'.format(epoch))",
            "train(model, device, train_loader, optimizer)",
            "test(model, device, test_loader)",
            "-    pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28])",
            "+    pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28], device)",
            "",
            "",
            "if __name__ == '__main__':"
        ]
    },
    {
        "number": 120,
        "comments": "change API call for type fix",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TestBidirectionalLanguageModelTransformer(AllenNlpTestCase):",
            "input_dim=32, hidden_dim=64, num_layers=2",
            ")",
            "",
            "-        mask = torch.ones(3, 6).int()",
            "-        mask[0, 3:] = 0",
            "-        mask[1, 5:] = 0",
            "+        mask = torch.ones(3, 6).bool()",
            "+        mask[0, 3:] = False",
            "+        mask[1, 5:] = False",
            "",
            "forward_mask, backward_mask = transformer_encoder.get_attention_masks(mask)"
        ]
    },
    {
        "number": 121,
        "comments": "add API call for state fix",
        "commit_message": "[release][train] Add `FileLock` to `tensorflow_mnist_example`. (#32712)\n\n* [release][train] Add `FileLock` to `tensorflow_mnist_example`.\n\nSigned-off-by: xwjiang2010 <xwjiang2010@gmail.com>\n\n* fix\n\nSigned-off-by: xwjiang2010 <xwjiang2010@gmail.com>\n\n---------\n\nSigned-off-by: xwjiang2010 <xwjiang2010@gmail.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "from ray.air.config import ScalingConfig",
            "",
            "",
            "def mnist_dataset(batch_size: int) -> tf.data.Dataset:",
            "-    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()",
            "+    with FileLock(os.path.expanduser(\"~/.mnist_lock\")):",
            "+        (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()",
            "# The `x` arrays are in uint8 and have values in the [0, 255] range.",
            "# You need to convert them to float32 with values in the [0, 1] range.",
            "x_train = x_train / np.float32(255)"
        ]
    },
    {
        "number": 122,
        "comments": "change param for math fix",
        "commit_message": "Fix examples given new tensorflow rnn interface. Update bidirectional rnn to return final state.\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def rnn_model(X, y):",
            "# Given encoding of RNN, take encoding of last step (e.g hidden size of the",
            "# neural network of last step) and pass it as features for logistic",
            "# regression over output classes.",
            "-    return skflow.models.logistic_regression(encoding[-1], y)",
            "+    return skflow.models.logistic_regression(encoding, y)",
            "",
            "classifier = skflow.TensorFlowEstimator(model_fn=rnn_model, n_classes=15,",
            "steps=1000, optimizer='Adam', learning_rate=0.01, continue_training=True)"
        ]
    },
    {
        "number": 126,
        "comments": "update API call for version fix",
        "commit_message": "fix deprecation warnings; remove pydoop dependency; update README\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def main_fun(argv, ctx):",
            "grads = average_gradients(tower_grads)",
            "",
            "# Add a summary to track the learning rate.",
            "-      summaries.append(tf.scalar_summary('learning_rate', lr))",
            "+      summaries.append(tf.summary.scalar('learning_rate', lr))",
            "",
            "# Add histograms for gradients.",
            "for grad, var in grads:",
            "if grad is not None:",
            "summaries.append(",
            "-              tf.histogram_summary(var.op.name + '/gradients', grad))",
            "+              tf.summary.histogram(var.op.name + '/gradients', grad))",
            "",
            "# Apply the gradients to adjust the shared variables.",
            "apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)",
            "",
            "# Add histograms for trainable variables.",
            "for var in tf.trainable_variables():",
            "-        summaries.append(tf.histogram_summary(var.op.name, var))",
            "+        summaries.append(tf.summary.histogram(var.op.name, var))",
            "",
            "# Track the moving averages of all trainable variables.",
            "variable_averages = tf.train.ExponentialMovingAverage("
        ]
    },
    {
        "number": 127,
        "comments": "add condition check for resource fix",
        "commit_message": "Fix PyText GPU Test (#378)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/378\n\nA patch to Fix GPU Testing.\n\nReviewed By: hikushalhere\n\nDifferential Revision: D14400231\n\nfbshipit-source-id: 0cf058bf887fc11d2a88e17dc0809157f3f9eb68\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Trainer(TrainerBase):",
            "",
            "@timing.time(\"Trainer.test\")",
            "def test(self, test_iter, model, metric_reporter: MetricReporter):",
            "+        if cuda.CUDA_ENABLED:",
            "+            model = model.cuda()",
            "+",
            "model.eval()",
            "with torch.no_grad():",
            "test_metric = self._run_epoch("
        ]
    },
    {
        "number": 128,
        "comments": "add condition check for resource fix",
        "commit_message": "fix _setup_devices in case where there is no torch.distributed package in build (#16821)\n\n* fix _setup_devices in case where there is not torch.distributed\n\n* in training_args_sm.py as well\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TrainingArguments:",
            "@torch_required",
            "def _setup_devices(self) -> \"torch.device\":",
            "logger.info(\"PyTorch: setting up devices\")",
            "-        if torch.distributed.is_initialized() and self.local_rank == -1:",
            "+        if torch.distributed.is_available() and torch.distributed.is_initialized() and self.local_rank == -1:",
            "logger.warning(",
            "\"torch.distributed process group is initialized, but local_rank == -1. \"",
            "\"In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\""
        ]
    },
    {
        "number": 131,
        "comments": "update API call for version fix",
        "commit_message": "Fix deprecated functions warnings\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "with tf.Graph().as_default():",
            "checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")",
            "if not os.path.exists(checkpoint_dir):",
            "os.makedirs(checkpoint_dir)",
            "-        saver = tf.train.Saver(tf.all_variables())",
            "+        saver = tf.train.Saver(tf.global_variables())",
            "",
            "# Write vocabulary",
            "vocab_processor.save(os.path.join(out_dir, \"vocab\"))",
            "",
            "# Initialize all variables",
            "-        sess.run(tf.initialize_all_variables())",
            "+        sess.run(tf.global_variables_initializer())",
            "",
            "def train_step(x_batch, y_batch):",
            "\"\"\""
        ]
    },
    {
        "number": 132,
        "comments": "add param for type fix",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class CLIPTextTransformer(nn.Module):",
            "attentions=encoder_outputs.attentions,",
            ")",
            "",
            "-    def _build_causal_attention_mask(self, bsz, seq_len):",
            "+    def _build_causal_attention_mask(self, bsz, seq_len, dtype):",
            "# lazily create causal attention mask, with full attention between the vision tokens",
            "# pytorch uses additive attention mask; fill with -inf",
            "-        mask = torch.empty(bsz, seq_len, seq_len)",
            "-        mask.fill_(torch.tensor(float(\"-inf\")))",
            "+        mask = torch.empty(bsz, seq_len, seq_len, dtype=dtype)",
            "+        mask.fill_(torch.tensor(torch.finfo(dtype).min))",
            "mask.triu_(1)  # zero out the lower diagonal",
            "mask = mask.unsqueeze(1)  # expand mask",
            "return mask"
        ]
    },
    {
        "number": 135,
        "comments": "update API call for version fix",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def get_mini_batch(mini_batch_indices, sequences, seq_lengths, cuda=False):",
            "# get mask for mini-batch",
            "mini_batch_mask = get_mini_batch_mask(mini_batch, sorted_seq_lengths)",
            "",
            "-    # wrap in PyTorch Variables",
            "-    mini_batch = Variable(torch.Tensor(mini_batch))",
            "-    mini_batch_reversed = Variable(torch.Tensor(mini_batch_reversed))",
            "-    mini_batch_mask = Variable(torch.Tensor(mini_batch_mask))",
            "+    # wrap in PyTorch Tensors",
            "+    mini_batch = torch.tensor(mini_batch)",
            "+    mini_batch_reversed = torch.tensor(mini_batch_reversed)",
            "+    mini_batch_mask = torch.tensor(mini_batch_mask)",
            "",
            "# cuda() here because need to cuda() before packing",
            "if cuda:"
        ]
    },
    {
        "number": 137,
        "comments": "change API call for version fix",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class FlopsProfiler(object):",
            "start_time_hook)",
            "",
            "def end_time_hook(module, input, output):",
            "-                torch.cuda.synchronize()",
            "+                get_accelerator().synchronize()",
            "module.__duration__ += time.time() - module.__start_time__",
            "",
            "if not hasattr(module, \"__end_time_hook_handle__\"):"
        ]
    },
    {
        "number": 140,
        "comments": "change API call for math fix",
        "commit_message": "fix seed generator\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class PaintByExample(DiffusionInpaintModel):",
            "mask: [H, W, 1] 255 means area to repaint",
            "return: BGR IMAGE",
            "\"\"\"",
            "-        set_seed(config.paint_by_example_seed)",
            "-",
            "output = self.model(",
            "image=PIL.Image.fromarray(image),",
            "mask_image=PIL.Image.fromarray(mask[:, :, -1], mode=\"L\"),",
            "example_image=config.paint_by_example_example_image,",
            "num_inference_steps=config.paint_by_example_steps,",
            "output_type='np.array',",
            "+            generator=torch.manual_seed(config.paint_by_example_seed)",
            ").images[0]",
            "",
            "output = (output * 255).round().astype(\"uint8\")"
        ]
    },
    {
        "number": 151,
        "comments": "add API call for type fix",
        "commit_message": "Remove duplicated functional of gp (#1607)\n\n* clean gp\n\n* rearange sgpr\n\n* kernel add -> sum in test_benchmark\n\n* nit\n\n* fix error\n\n* fix error during clean\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def main(args):",
            "# CNN will transform a high dimension image into a low dimension 2D tensors for RBF kernel.",
            "# This kernel accepts inputs are inputs of CNN and gives outputs are covariance matrix of RBF on",
            "# outputs of CNN.",
            "-    kernel = gp.kernels.RBF(input_dim=10, lengthscale=torch.ones(10)).warp(iwarping_fn=cnn_fn)",
            "+    kernel = gp.kernels.Warp(gp.kernels.RBF(input_dim=10, lengthscale=torch.ones(10)),",
            "+                             iwarping_fn=cnn_fn)",
            "",
            "# init inducing points (taken randomly from dataset)",
            "Xu = next(iter(train_loader))[0][:args.num_inducing]"
        ]
    },
    {
        "number": 153,
        "comments": "add param for type fix",
        "commit_message": "fixing torch backend matrix_rank helper to pass array-api test\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def linspace_helper(start, stop, num, axis=None, *, device):",
            "else:",
            "res = [linspace_method(start, stp, num, device=device) for stp in stop]",
            "else:",
            "-        return linspace_method(start, stop, num, device=device)",
            "+        return linspace_method(start, stop, num, dtype=torch.float64, device=device)",
            "res = torch.cat(res, -1).reshape(sos_shape + [num])",
            "if axis is not None:",
            "res = torch.transpose(res, axis, -1)"
        ]
    },
    {
        "number": 154,
        "comments": "add custom function",
        "commit_message": "Allow saving / loading checkpoints from cloud paths (#683)\n\n* Allow saving / loading checkpoints from cloud paths\n\nAllows saving and loading checkpoints directly from cloud paths like\nAmazon S3 (s3://) and Google Cloud Storage (gs://) by using fsspec.\n\nNote: The user will have to install the relevant dependency for each\nprotocol. Otherwise fsspec will fail and specify which dependency is\nmissing.\n\n* Append suffix _fsspec to save/load function names\n\n* Add a lower bound to the fsspec dependency\n\nSkips the 0 major version.\n\n* Add missing changes from refactor\n\n* Use fsspec for remaining artifacts\n\n* Add test case with path requiring fsspec\n\n* Avoid writing logs to file unless output_path is local\n\n* Document the possibility of using paths supported by fsspec\n\n* Fix style and lint\n\n* Add missing lint fixes\n\n* Add type annotations to new functions\n\n* Use Coqpit method for converting config to dict\n\n* Fix type annotation in semi-new function\n\n* Add return type for load_fsspec\n\n* Fix bug where fs not always created\n\n* Restore the experiment removal functionality\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def save_best_model(model, optimizer, criterion, model_loss, best_loss, out_path",
            "bestmodel_path = \"best_model.pth.tar\"",
            "bestmodel_path = os.path.join(out_path, bestmodel_path)",
            "print(\"\\n > BEST MODEL ({0:.5f}) : {1:}\".format(model_loss, bestmodel_path))",
            "-        torch.save(state, bestmodel_path)",
            "+        save_fsspec(state, bestmodel_path)",
            "return best_loss"
        ]
    },
    {
        "number": 156,
        "comments": "add API call for state fix",
        "commit_message": "bugfix for unittests\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class UnittestBase(object):",
            "datetime.now().strftime('%H:%M:%S'), self.__class__.__name__[4:], name",
            "))",
            "sys.stdout.flush()",
            "+        tf.compat.v1.reset_default_graph()",
            "",
            "def finished_test(self, assertion=None):",
            "\"\"\""
        ]
    },
    {
        "number": 158,
        "comments": "change param for math fix",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class GPTNeoXModel(GPTNeoXPreTrainedModel):",
            "# Since we are adding it to the raw scores before the softmax, this is",
            "# effectively the same as removing these entirely.",
            "attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility",
            "-            attention_mask = (1.0 - attention_mask) * -10000.0",
            "+            attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min",
            "",
            "# Prepare head mask if needed",
            "# 1.0 in head_mask indicate we keep the head"
        ]
    },
    {
        "number": 159,
        "comments": "no API used",
        "commit_message": "batch and test fixed\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class PatchAffineShapeEstimator(nn.Module):",
            "\"input shape should be must be [Bx1x{}x{}]. \"",
            "\"Got {}\".format(self.patch_size, self.patch_size, patch.size()))",
            "self.weighting = self.weighting.to(patch.dtype).to(patch.device)",
            "-        grads: torch.Tensor = self.gradient(patch)",
            "+        grads: torch.Tensor = self.gradient(patch) * self.weighting",
            "# unpack the edges",
            "gx: torch.Tensor = grads[:, :, 0]",
            "gy: torch.Tensor = grads[:, :, 1]"
        ]
    },
    {
        "number": 160,
        "comments": "add param for argument fix",
        "commit_message": "fix minor bug\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def compute_tf_latency(",
            "with tf.device(device):",
            "for _ in range(steps):",
            "starting_time = time.time()",
            "-            _ = model(x)",
            "+            _ = model(*xs)",
            "latencies.append(time.time() - starting_time)",
            "latency = sum(latencies) / steps",
            "return latency, latencies"
        ]
    },
    {
        "number": 163,
        "comments": "add param for resource fix",
        "commit_message": "Hub device mismatch bug fix (#1619)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def non_max_suppression(prediction, conf_thres=0.1, iou_thres=0.6, classes=None,",
            "merge = False  # use merge-NMS",
            "",
            "t = time.time()",
            "-    output = [torch.zeros(0, 6)] * prediction.shape[0]",
            "+    output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]",
            "for xi, x in enumerate(prediction):  # image index, image inference",
            "# Apply constraints",
            "# x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height"
        ]
    },
    {
        "number": 166,
        "comments": "add param for argument fix",
        "commit_message": "Fix DARTS 2nd order (#4385)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DartsTrainer(BaseOneShotTrainer):",
            "p += e * d",
            "",
            "_, loss = self._logits_and_loss(trn_X, trn_y)",
            "-            dalphas.append(torch.autograd.grad(loss, [c.alpha for c in self.nas_modules]))",
            "+            dalphas.append(torch.autograd.grad(loss, [c.alpha for _, c in self.nas_modules]))",
            "",
            "dalpha_pos, dalpha_neg = dalphas  # dalpha { L_trn(w+) }, # dalpha { L_trn(w-) }",
            "hessian = [(p - n) / (2. * eps) for p, n in zip(dalpha_pos, dalpha_neg)]"
        ]
    },
    {
        "number": 167,
        "comments": "update API call for refactor fix",
        "commit_message": "Fixed subract fn (#4068)\n\nCo-authored-by: abdrahmandiab <abdrahmandiab99@gmail.com>\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def subtract(",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "x1, x2 = ivy.promote_types_of_inputs(x1, x2)",
            "-    return tf.subtract(x1, x2)",
            "+    return tf.experimental.numpy.subtract(x1, x2)",
            "",
            "",
            "def tan("
        ]
    },
    {
        "number": 169,
        "comments": "update API call for version fix",
        "commit_message": "Fix compatibility with 1.12 (#17925)\n\n* Fix compatibility with 1.12\n\n* Remove pin from examples requirements\n\n* Update torch scatter version\n\n* Fix compatibility with 1.12\n\n* Remove pin from examples requirements\n\n* Update torch scatter version\n\n* fix torch.onnx.symbolic_opset12 import\n\n* Reject bad version\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class XDropout(torch.autograd.Function):",
            "# Once https://github.com/pytorch/pytorch/issues/78391 is fixed, do something like:",
            "# if opset_version < 12:",
            "#   return torch.onnx.symbolic_opset9.dropout(g, input, dropout_p, train)",
            "-        return torch.onnx.symbolic_opset12.dropout(g, input, dropout_p, train)",
            "+        return symbolic_opset12.dropout(g, input, dropout_p, train)",
            "",
            "",
            "# Copied from transformers.models.deberta.modeling_deberta.StableDropout"
        ]
    },
    {
        "number": 170,
        "comments": "change condition check for version fix",
        "commit_message": "Use packaging to handle versions (#2777)\n\n* Get Python version from platform module\n\n* Set PY_VERSION as version class\n\n* Set PYARROW_VERSION as version class\n\n* Set TORCH_VERSION as version class\n\n* Set TF_VERSION as version class\n\n* Set JAX_VERSION as version class\n\n* Set BEAM_VERSION as version class\n\n* Set RARFILE_VERSION as version class\n\n* Use version class to validate PyArrow version at import\n\n* Use version class in SCRIPTS_VERSION at import\n\n* Use config.PYARROW_VERSION for parquet submodules\n\n* Fix style\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Parquet(datasets.ArrowBasedBuilder):",
            "BUILDER_CONFIG_CLASS = ParquetConfig",
            "",
            "def _info(self):",
            "-        if version.parse(pa.__version__) < version.parse(\"3.0.0\"):",
            "+        if datasets.config.PYARROW_VERSION.major < 3:",
            "raise ImportError(",
            "\"PyArrow >= 3.0.0 is required to used the Parquet dataset builder: pip install --upgrade pyarrow\"",
            ")"
        ]
    },
    {
        "number": 171,
        "comments": "add API call for state fix",
        "commit_message": "[bug-fix] Trainer.test points to latest best_model_path (#5161)\n\n* resolve bug\n\n* update code\n\n* add set -e\n\n* Update pytorch_lightning/callbacks/model_checkpoint.py\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* update test\n\n* Update tests/checkpointing/test_trainer_checkpoint.py\n\nCo-authored-by: Sean Naren <sean.narenthiran@gmail.com>\n\n* Update tests/checkpointing/test_trainer_checkpoint.py\n\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n\n* update on comments\n\n* resolve test\n\n* convert to set\n\n* update\n\n* add error triggering\n\n* update\n\n* update on comments\n\n* update\n\n* resolve import\n\n* update\n\n* update\n\n* Update pytorch_lightning/plugins/rpc_plugin.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* update\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\nCo-authored-by: Sean Naren <sean.narenthiran@gmail.com>\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-62-109.ec2.internal>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n(cherry picked from commit d5b367871fa3924090ec74bf903bd172bd3e2343)\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class RPCPlugin(DDPPlugin):",
            "world_size: int) -> None:",
            "os.environ['MASTER_PORT'] = os.getenv('RPC_MASTER_PORT', '15000')",
            "rpc.init_rpc(f\"worker{global_rank}\", rank=global_rank, world_size=world_size)",
            "+        rpc._set_rpc_timeout(self.rpc_timeout_sec)",
            "self.rpc_initialized = True",
            "",
            "def rpc_save_model(self,"
        ]
    },
    {
        "number": 172,
        "comments": "add param for type fix",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class SimpleSeq2SeqTest(ModelTestCase):",
            "state = self.model._init_decoder_state(state)",
            "batch_size = state[\"source_mask\"].size()[0]",
            "start_predictions = state[\"source_mask\"].new_full(",
            "-            (batch_size,), fill_value=self.model._start_index",
            "+            (batch_size,), fill_value=self.model._start_index, dtype=torch.long",
            ")",
            "all_top_k_predictions, _ = beam_search.search(",
            "start_predictions, state, self.model.take_step"
        ]
    },
    {
        "number": 174,
        "comments": "remove condition check for resource fix",
        "commit_message": "`enable_model_cpu_offload` (#2285)\n\n* enable_model_offload PoC\n\nIt's surprisingly more involved than expected, see comments in the PR.\n\n* Rename final_offload_hook\n\n* Invoke the vae forward hook manually.\n\n* Completely remove decoder.\n\n* Style\n\n* apply_forward_hook decorator\n\n* Rename method.\n\n* Style\n\n* Copy enable_model_cpu_offload\n\n* Fix copies.\n\n* Remove comment.\n\n* Fix copies\n\n* Missing import\n\n* Fix doc-builder style.\n\n* Merge main and fix again.\n\n* Add docs\n\n* Fix docs.\n\n* Add a couple of tests.\n\n* style\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class StableDiffusionDepth2ImgPipeline(DiffusionPipeline):",
            "`pipeline.enable_sequential_cpu_offload()` the execution device can only be inferred from Accelerate's module",
            "hooks.",
            "\"\"\"",
            "-        if self.device != torch.device(\"meta\") or not hasattr(self.unet, \"_hf_hook\"):",
            "+        if not hasattr(self.unet, \"_hf_hook\"):",
            "return self.device",
            "for module in self.unet.modules():",
            "if ("
        ]
    },
    {
        "number": 177,
        "comments": "update param for refactor fix",
        "commit_message": "small fix to Haiku converter method.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class IvyModule(ivy.Module):",
            "if ivy.array_mode():",
            "a, kw = ivy.args_to_native(*a, **kw)",
            "# noinspection PyUnresolvedReferences",
            "-        params_hk = self._native_module.init(ivy.functional.core.random.RNG, *a, **kw)",
            "+        params_hk = self._native_module.init(ivy.random.RNG, *a, **kw)",
            "params_dict = _hk_flat_map_to_dict(params_hk)",
            "self._hk_params = ivy.Container(params_dict)",
            "param_iterator = self._hk_params.to_iterator()"
        ]
    },
    {
        "number": 179,
        "comments": "remove API call for resource fix",
        "commit_message": "use `f`-strings (#3557)\n\n* fix f-strings\n\n* update\n\n* update\n\n* update\n\n* fix test\n\n* update\n\n* fix tests\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DataParallel(torch.nn.DataParallel):",
            "Batch.from_data_list(data_list[split[i]:split[i + 1]],",
            "follow_batch=self.follow_batch,",
            "exclude_keys=self.exclude_keys).to(",
            "-                                     torch.device('cuda:{}'.format(",
            "-                                         device_ids[i])))",
            "+                                     torch.device(f'cuda:{device_ids[i]}'))",
            "for i in range(len(split) - 1)",
            "]"
        ]
    },
    {
        "number": 180,
        "comments": "change API call for type fix",
        "commit_message": "fix #827\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class ProjectedAdaptiveLogSoftmax(nn.Module):",
            "d_emb_i = d_embed // (div_val ** i)",
            "",
            "self.out_projs.append(",
            "-                    nn.Parameter(torch.Tensor(d_proj, d_emb_i))",
            "+                    nn.Parameter(torch.FloatTensor(d_proj, d_emb_i))",
            ")",
            "",
            "self.out_layers.append(nn.Linear(d_emb_i, r_idx-l_idx))"
        ]
    },
    {
        "number": 181,
        "comments": "update API call for refactor fix",
        "commit_message": "fix dropout management in TensorFlowEstimator._predict - get correct list of dropout variables, pass keep_prob = 1.0\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TensorFlowEstimator(BaseEstimator):",
            "raise NotFittedError()",
            "predict_data_feeder = setup_predict_data_feeder(X)",
            "preds = []",
            "-        dropouts = tf.get_collection(DROPOUTS)",
            "-        feed_dict = {prob: 0.0 for prob in dropouts}",
            "+        dropouts = self._graph.get_collection(DROPOUTS)",
            "+        feed_dict = {prob: 1.0 for prob in dropouts}",
            "for data in predict_data_feeder:",
            "feed_dict[self._inp] = data",
            "preds.append(self._session.run("
        ]
    },
    {
        "number": 187,
        "comments": "add API call for type fix",
        "commit_message": "fix small issues\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Pix2PixModel(BaseModel):",
            "def backward_D(self):",
            "# Fake",
            "# stop backprop to the generator by detaching fake_B",
            "-        fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1))",
            "+        fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1).data)",
            "pred_fake = self.netD.forward(fake_AB.detach())",
            "self.loss_D_fake = self.criterionGAN(pred_fake, False)",
            "",
            "# Real",
            "real_AB = torch.cat((self.real_A, self.real_B), 1)",
            "pred_real = self.netD.forward(real_AB)",
            "-        self.loss_D_real = self.criterionGAN(self.pred_real, True)",
            "+        self.loss_D_real = self.criterionGAN(pred_real, True)",
            "",
            "# Combined loss",
            "self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5"
        ]
    },
    {
        "number": 189,
        "comments": "add condition check for null fix",
        "commit_message": "fix FSDP ShardedGradScaler (#18358)\n\nrenaming it\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Trainer:",
            "transformer_cls_to_wrap = get_module_class_from_name(",
            "model, self.args.fsdp_transformer_layer_cls_to_wrap",
            ")",
            "+                    if transformer_cls_to_wrap is None:",
            "+                        raise Exception(\"Could not find the transformer layer class to wrap in the model.\")",
            "auto_wrap_policy = functools.partial(",
            "transformer_auto_wrap_policy,",
            "# Transformer layer class to wrap"
        ]
    },
    {
        "number": 190,
        "comments": "add custom method to get var",
        "commit_message": "agents and models base classes moved, various fixes\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class NAFModel(Model):",
            "# Naf directly outputs V(s)",
            "target_value[action] = target_value_output",
            "",
            "-            target_output_vars = get_variables('target_outputs')",
            "+            target_output_vars = tf.contrib.framework.get_variables('target_outputs')",
            "",
            "with tf.name_scope(\"update\"):",
            "for action in self.action:"
        ]
    },
    {
        "number": 193,
        "comments": "add API call for shape fix",
        "commit_message": "fix tensorflow pre-trained model last layer outputshape problem.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class tensorflow_extractor(base_extractor):",
            "writer.close()",
            "sess.run(init)",
            "saver = tf.train.Saver()",
            "+            tf.train.export_meta_graph(\"kit.meta\", as_text=True)",
            "saver.restore(sess, path + cls.architecture_map[architecture]['filename'])",
            "save_path = saver.save(sess, path + \"imagenet_{}.ckpt\".format(architecture))",
            "print(\"Model saved in file: %s\" % save_path)"
        ]
    },
    {
        "number": 194,
        "comments": "change API call for math fix",
        "commit_message": "Stabilize autoguide scale parameters via SoftplusTransform (#2767)\n\n* Update to PyTorch nightly\n\n* Update README.md\n\n* Commit to PyTorch nightly on dev branch\n\n* Fix constraint bugs\n\n* Relax torchvision version\n\n* Fix CorrCholesky constraint and test data\n\n* Fix funsor bugs\n\n* Try harder to generate positive data\n\n* xfail; switch to torch_test channel\n\n* Fix versions\n\n* Pin to fixed nightly version\n\n* lint\n\n* xfail some funsor tests\n\n* Remove accidental file\n\n* Use softplus transforms for autoguide scales\n\n* Add transform tests\n\n* Rename stable_positive -> softplus_positive\n\n* Make autoguide constraints configurable\n\n* Address review comments\n\n* lint\n\n* Tweak parameters in inference test\n\n* Regster transforms in docs\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def test_auto_diagonal_gaussians(auto_class, Elbo):",
            "guide = auto_class(model, rank=1)",
            "else:",
            "guide = auto_class(model)",
            "-    adam = optim.Adam({\"lr\": .001, \"betas\": (0.95, 0.999)})",
            "+    adam = optim.ClippedAdam({\"lr\": .01, \"betas\": (0.95, 0.999),",
            "+                              \"lrd\": 0.1 ** (1 / n_steps)})",
            "svi = SVI(model, guide, adam, loss=Elbo())",
            "",
            "for k in range(n_steps):"
        ]
    },
    {
        "number": 195,
        "comments": "add param for math fix",
        "commit_message": "fix random seed for stable test\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_frontend_backward_multi_channel(train, use_wpe, use_beamformer):",
            "frontend.train()",
            "else:",
            "frontend.eval()",
            "+    torch.random.manual_seed(14)",
            "x = torch.randn(2, 1000, 2, requires_grad=True)",
            "x_lengths = torch.LongTensor([1000, 980])",
            "y, y_lengths = frontend(x, x_lengths)"
        ]
    },
    {
        "number": 197,
        "comments": "test fix",
        "commit_message": "Fix hub (#2687)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2687\n\nReviewed By: alexeib\n\nDifferential Revision: D24095130\n\nPulled By: myleott\n\nfbshipit-source-id: 7d371bccb550ec68b2b9b39dfa4c0718356508d6\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class TestGradientScaling(unittest.TestCase):",
            "optimizer = FP16Optimizer.build_optimizer(self.namespace_dls, params)",
            "",
            "self.run_iter(model, params, optimizer)",
            "-        self.assertTrue(torch.all(optimizer.fp32_params.eq(torch.tensor([3.1000, 5.1000], device='cuda:0', requires_grad=True))))",
            "+        self.assertTrue(all(",
            "+            torch.all(fp32_params.eq(torch.tensor([3.1000, 5.1000], device='cuda:0', requires_grad=True)))",
            "+            for fp32_params in optimizer.fp32_params.values()",
            "+        ))",
            "",
            "def test_memory_efficient(self):",
            "model = copy.deepcopy(self.model)"
        ]
    },
    {
        "number": 198,
        "comments": "test api",
        "commit_message": "[Feat] Add tpu-testing in circleci (#787)\n\n* add needed files for tpu-testing in circleci\n\n* adapt script for pytorch nghtlies\n\n* fix shape test\n\n* fix color tests\n\n* add pytorch version to the dockerfile\n\n* fix xla precision and disable torch-xla[1.6,1.7]\n\n* adjust rgb2luv precision\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class TestLuvToRgb(BaseTester):",
            "[0.06325728, 0.78878325, 0.74280596, 0.99514300, 0.47176042]]",
            "]], device=device, dtype=dtype)",
            "",
            "-        assert_allclose(kornia.color.luv_to_rgb(data), expected)",
            "+        assert_allclose(kornia.color.luv_to_rgb(data), expected, rtol=1e-4, atol=1e-4)",
            "",
            "def test_forth_and_back(self, device, dtype):",
            "data = torch.rand(3, 4, 5, device=device, dtype=dtype)"
        ]
    },
    {
        "number": 199,
        "comments": "add param for type fix",
        "commit_message": "Fix HALTON sequence utilization in Euler Sampling. Disable Sobol random_type in Euler sampling when time_step is non-constant till TensorFlow 2.2 is released.\n\nPiperOrigin-RevId: 291143718\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class EulerSamplingTest(tf.test.TestCase, parameterized.TestCase):",
            "times=times,",
            "num_samples=num_samples,",
            "initial_state=x0,",
            "-            random_type=tff.math.random.RandomType.SOBOL,",
            "+            random_type=tff.math.random.RandomType.HALTON,",
            "time_step=0.01,",
            "-            seed=12134))",
            "+            seed=12134,",
            "+            skip=100,",
            "+            dtype=tf.float32))",
            "",
            "-    self.assertAllClose(paths.shape, (num_samples, 5, 2), atol=0)",
            "+    self.assertAllClose(paths.shape, (num_samples, 3, 2), atol=0)",
            "means = np.mean(paths, axis=0)",
            "times = np.reshape(times, [-1, 1])",
            "expected_means = x0 + (2.0 / 3.0) * mu * np.power(times, 1.5)"
        ]
    },
    {
        "number": 202,
        "comments": "use custom method to fix the bug",
        "commit_message": "Generalize TorchHook to FrameworkHook (#2561)\n\n* create FrameworkHook and move generic hooking functionality over\n\n* PEP-ify import statements all around\n\n* PR comment - fix docstring\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class TorchTensor(AbstractTensor):",
            ")",
            "# This handles case 3: it redirects the command to the appropriate class depending",
            "# of the syft type of the arguments and returns",
            "-            if args_type not in (torch.Tensor, torch.nn.Parameter):",
            "+            if args_type not in FrameworkTensor:",
            "return args_type.handle_func_command(command)",
            "",
            "# build the new command"
        ]
    },
    {
        "number": 203,
        "comments": "add API call for state fix",
        "commit_message": "Fix BatchNorm TransformModule (#2459)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class BatchNorm(TransformModule):",
            "if self.training:",
            "mean, var = y.mean(0), y.var(0)",
            "",
            "-            # NOTE: The momentum variable agrees with the definition in e.g. `torch.nn.BatchNorm1d`",
            "-            self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum)",
            "-            self.moving_variance.mul_(1 - self.momentum).add_(var * self.momentum)",
            "+            with torch.no_grad():",
            "+                # NOTE: The momentum variable agrees with the definition in e.g. `torch.nn.BatchNorm1d`",
            "+                self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum)",
            "+                self.moving_variance.mul_(1 - self.momentum).add_(var * self.momentum)",
            "",
            "# During test time, use smoothed averages rather than the sample ones",
            "else:"
        ]
    },
    {
        "number": 205,
        "comments": "remove API call for math fix",
        "commit_message": "fixed a bug about dropout\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class RNNLM(nn.Module):",
            "",
            "def forward(self, state, x):",
            "h0 = self.embed(x)",
            "-        h1, c1 = self.l1(F.dropout(h0), (state['h1'], state['c1']))",
            "-        h2, c2 = self.l2(F.dropout(h1), (state['h2'], state['c2']))",
            "-        y = self.lo(F.dropout(h2))",
            "+        h1, c1 = self.l1(self.d0(h0), (state['h1'], state['c1']))",
            "+        h2, c2 = self.l2(self.d1(h1), (state['h2'], state['c2']))",
            "+        y = self.lo(self.d2(h2))",
            "state = {'c1': c1, 'h1': h1, 'c2': c2, 'h2': h2}",
            "return state, y"
        ]
    },
    {
        "number": 208,
        "comments": "add condition check for resource fix",
        "commit_message": "translate bug fix\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def reportScore(name, scoreTotal, wordsTotal):",
            "def main():",
            "opt = parser.parse_args()",
            "opt.cuda = opt.gpu > -1",
            "-    torch.cuda.set_device(opt.gpu)",
            "+    if opt.cuda:",
            "+        torch.cuda.set_device(opt.gpu)",
            "",
            "translator = onmt.Translator(opt)"
        ]
    },
    {
        "number": 209,
        "comments": "change param for math fix",
        "commit_message": "Fix static padding calculation\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Conv2dStaticSamePadding(nn.Conv2d):",
            "pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)",
            "pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)",
            "if pad_h > 0 or pad_w > 0:",
            "-            self.static_padding = nn.ZeroPad2d((pad_w - pad_w // 2, pad_w - pad_w // 2,",
            "-                                                pad_h - pad_h // 2, pad_h - pad_h // 2))",
            "+            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2,",
            "+                                                pad_h // 2, pad_h - pad_h // 2))",
            "else:",
            "self.static_padding = nn.Identity()"
        ]
    },
    {
        "number": 210,
        "comments": "format",
        "commit_message": "lazy datasets (#675)\n\n* lazy datasets\n\n* add end-to-end lazy dataset test + fix bugs\n\n* fix notebook tests\n\n* remove accidentally committed ipynb checkpoints\n\n* remove duplicate code in tests\n\n* Iterator -> Iterable\n\n* address PR comments\n\n* fix api docs\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def train_model(params: Params, serialization_dir: str) -> Model:",
            "",
            "logger.info(\"Creating a vocabulary using %s data.\", \", \".join(datasets_for_vocab_creation))",
            "vocab = Vocabulary.from_params(params.pop(\"vocabulary\", {}),",
            "-                                   Dataset([instance for key, dataset in all_datasets.items()",
            "-                                            for instance in dataset.instances",
            "-                                            if key in datasets_for_vocab_creation]))",
            "+                                   (instance for key, dataset in all_datasets.items()",
            "+                                    for instance in dataset",
            "+                                    if key in datasets_for_vocab_creation))",
            "vocab.save_to_files(os.path.join(serialization_dir, \"vocabulary\"))",
            "",
            "model = Model.from_params(vocab, params.pop('model'))"
        ]
    },
    {
        "number": 213,
        "comments": "add API call for state fix",
        "commit_message": "Fix multi-speaker init of Tacotron models & tests\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class SpeedySpeech(BaseTTS):",
            "outputs = {\"model_outputs\": o_de.transpose(1, 2), \"durations_log\": o_dr_log.squeeze(1), \"alignments\": attn}",
            "return outputs",
            "",
            "+    @torch.no_grad()",
            "def inference(self, x, aux_input={\"d_vectors\": None, \"speaker_ids\": None}):  # pylint: disable=unused-argument",
            "\"\"\"",
            "Shapes:"
        ]
    },
    {
        "number": 214,
        "comments": "add API call for math fix",
        "commit_message": "Fix einsum for keras implementation\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class KerasBackend(AbstractBackend):",
            "return keras",
            "",
            "def einsum(self, pattern, *x):",
            "-        return self.tf.einsum(pattern, *x)",
            "+        return self.tf.vectorized_map(",
            "+            functools.partial(self.tf.einsum, pattern),",
            "+            *x",
            "+        )",
            "",
            "",
            "class OneFlowBackend(AbstractBackend):"
        ]
    },
    {
        "number": 215,
        "comments": "add API call for math fix",
        "commit_message": "additional fix for difference model merging\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def run_modelmerger(primary_model_name, secondary_model_name, teritary_model_nam",
            "t2 = theta_2.get(key, torch.zeros_like(theta_1[key]))",
            "theta_1[key] = theta_func1(theta_1[key], t2)",
            "else:",
            "-                    theta_1[key] = 0",
            "+                    theta_1[key] = torch.zeros_like(theta_1[key])",
            "del theta_2, teritary_model",
            "",
            "for key in tqdm.tqdm(theta_0.keys()):"
        ]
    },
    {
        "number": 216,
        "comments": "add param for resource fix",
        "commit_message": "Fix loss computation for empty tensors\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DefaultClassifier(Classifier):",
            "",
            "def _calculate_loss(self, scores, labels):",
            "",
            "-        if len(labels) == 0: return torch.tensor(0., requires_grad=True), 1",
            "+        if len(labels) == 0: return torch.tensor(0., requires_grad=True, device=flair.device), 1",
            "",
            "if self.multi_label:",
            "labels = torch.tensor([[1 if l in all_labels_for_point else 0 for l in self.label_dictionary.get_items()]"
        ]
    },
    {
        "number": 217,
        "comments": "change API call for resource fix",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class EmbeddingLayer(nn.Module):",
            "torch.empty(weight_shape[0],",
            "weight_shape[1],",
            "dtype=dtype,",
            "-                        device=torch.cuda.current_device()))",
            "+                        device=get_accelerator().current_device_name()))",
            "",
            "def forward(self, input):",
            "return F.embedding(input, self.weight)"
        ]
    },
    {
        "number": 218,
        "comments": "change API call for version fix",
        "commit_message": "[rllib] tuple space shouldn't assume elements are all the same size (#2637)\n\n* fix\n\n* lint\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class MultiActionDistribution(ActionDistribution):",
            "",
            "def logp(self, x):",
            "\"\"\"The log-likelihood of the action distribution.\"\"\"",
            "-        split_list = self.reshaper.split_tensor(x)",
            "+        split_list = tf.split(x, len(self.input_lens), axis=1)",
            "for i, distribution in enumerate(self.child_distributions):",
            "# Remove extra categorical dimension",
            "if isinstance(distribution, Categorical):"
        ]
    },
    {
        "number": 219,
        "comments": "add API call for math fix",
        "commit_message": "fixed stochastic policy shapes, naf under construction\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class CategoricalOneHotPolicy(StochasticPolicy):",
            "def __init__(self, network, session, state, random, action_count=1, scope='policy'):",
            "with tf.variable_scope(scope):",
            "action_layer = linear(layer_input=network.output, config={'num_outputs': action_count}, scope='outputs')",
            "+            action_layer = tf.reshape(action_layer, [-1, action_count])",
            "+",
            "distribution = tf.nn.softmax(action_layer)",
            "sample = tf.multinomial(distribution, 1)"
        ]
    },
    {
        "number": 221,
        "comments": "version fix",
        "commit_message": "Support FP16 training (#520)\n\n* add fp16 support\n\n* fpn donot need bn normalize\n\n* refactor wrapped bn\n\n* fix bug of retinanet\n\n* add fp16 ssd300 voc, cascade r50, cascade mask r50\n\n* fix bug in cascade rcnn testing\n\n* add support to fix bn training\n\n* add fix bn cfg\n\n* delete fixbn cfg, mv fixbn fp16 to a new branch\n\n* fix cascade mask fp16 bug in test\n\n* fix bug in cascade mask rcnn fp16 test\n\n* add more fp16 cfgs\n\n* add fp16 fast-r50 and faster-dconv-r50\n\n* add fp16 test, minor fix\n\n* clean code\n\n* fix config work_dir name\n\n* add patch func, refactor code\n\n* fix format\n\n* clean code\n\n* move convert rois to single_level_extractor\n\n* fix bug for cascade mask, the seg mask is ndarray\n\n* refactor code, add two decorator force_fp32 and auto_fp16\n\n* add fp16_enable attribute\n\n* add more comment, fix format and test assertion\n\n* fix pep8 format error\n\n* format commont and api\n\n* rename distribute to distributed, fix dict copy\n\n* rename function name\n\n* move function, add comment\n\n* remove unused parameter\n\n* mv decorators into decorators.py, hook related functions to hook\n\n* add auto_fp16 to forward of semantic head\n\n* add auto_fp16 to all heads and fpn\n\n* add docstrings and minor bug fix\n\n* simple refactoring\n\n* bug fix for patching forward method\n\n* roi extractor in fp32 mode\n\n* fix flake8 error\n\n* fix ci error\n\n* add fp16 support to ga head\n\n* remove parallel test assert\n\n* minor fix\n\n* add comment in build_optimizer\n\n* fix typo in comment\n\n* fix typo enable --> enabling\n\n* udpate README\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class SingleRoIExtractor(nn.Module):",
            "out_size = self.roi_layers[0].out_size",
            "num_levels = len(feats)",
            "target_lvls = self.map_roi_levels(rois, num_levels)",
            "-        roi_feats = torch.cuda.FloatTensor(rois.size()[0], self.out_channels,",
            "-                                           out_size, out_size).fill_(0)",
            "+        roi_feats = feats[0].new_zeros(rois.size()[0], self.out_channels,",
            "+                                       out_size, out_size)",
            "for i in range(num_levels):",
            "inds = target_lvls == i",
            "if inds.any():"
        ]
    },
    {
        "number": 222,
        "comments": "add condition check for resource fix",
        "commit_message": "train with multi-gpu half test bug fix #99\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def test(data,",
            "else:  # called by train.py",
            "training = True",
            "device = next(model.parameters()).device  # get model device",
            "-        half = device.type != 'cpu'  # half precision only supported on CUDA",
            "+        half = device.type != 'cpu' and torch.cuda.device_count() == 1  # half precision only supported on single-GPU",
            "if half:",
            "model.half()  # to FP16"
        ]
    },
    {
        "number": 223,
        "comments": "update API call for version fix",
        "commit_message": "Fix issue with torchvision 0.11.0\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class MobileNetV3LargeEncoder(MobileNetV3):",
            ")",
            "",
            "if pretrained:",
            "-            self.load_state_dict(load_state_dict_from_url(",
            "+            self.load_state_dict(torch.hub.load_state_dict_from_url(",
            "'https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth'))",
            "",
            "del self.avgpool"
        ]
    },
    {
        "number": 224,
        "comments": "add param for type fix",
        "commit_message": "fixed zeros matrix definition\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def make_non_pad_mask(lengths):",
            "\"\"\"",
            "bs = int(len(lengths))",
            "maxlen = int(max(lengths))",
            "-    mask = torch.zeros(bs, maxlen).byte()",
            "+    mask = torch.zeros(bs, maxlen, dtype=torch.uint8)",
            "for i, l in enumerate(lengths):",
            "mask[i, :l] = 1"
        ]
    },
    {
        "number": 226,
        "comments": "remove constraint",
        "commit_message": "fix mt task collect_stats at stage 9\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class Embedding(AbsFrontend):",
            "assert check_argument_types()",
            "super().__init__()",
            "self.embed_dim = embed_dim",
            "-        self.padding = padding",
            "self.embed_scale = 1.0 if no_embed_scale else math.sqrt(embed_dim)",
            "-        self.embed = torch.nn.Embedding(input_size, embed_dim, padding_idx=padding)",
            "+        self.embed = torch.nn.Embedding(input_size, embed_dim)",
            "",
            "def forward(",
            "self, input: torch.Tensor, input_lengths: torch.Tensor"
        ]
    },
    {
        "number": 229,
        "comments": "change param for type fix",
        "commit_message": "Update serving signatures and make sure we actually use them (#19034)\n\n* Override save() to use the serving signature as the default\n\n* Replace int32 with int64 in all our serving signatures\n\n* Remember one very important line so as not to break every test at once\n\n* Dtype fix for TFLED\n\n* dtype fix for shift_tokens_right in general\n\n* Dtype fixes in mBART and RAG\n\n* Fix dtypes for test_unpack_inputs\n\n* More dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Add a check that the model actually has a serving method\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TFXGLMPreTrainedModel(TFPreTrainedModel):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "-                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "+                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "}",
            "]",
            ")"
        ]
    },
    {
        "number": 233,
        "comments": "add param for argument fix",
        "commit_message": "bug fix for tacotron and tests update\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TacotronTrainTest(unittest.TestCase):",
            "optimizer = optim.Adam(model.parameters(), lr=c.lr)",
            "for i in range(5):",
            "mel_out, linear_out, align, stop_tokens = model.forward(",
            "-                input, mel_spec)",
            "+                input, input_lengths, mel_spec)",
            "assert stop_tokens.data.max() <= 1.0",
            "assert stop_tokens.data.min() >= 0.0",
            "optimizer.zero_grad()"
        ]
    },
    {
        "number": 234,
        "comments": "remove condition check for resource fix",
        "commit_message": "Fix slow tests (#1210)\n\n* fix tests\n\n* Fix more\n\n* more\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class EulerAncestralDiscreteScheduler(SchedulerMixin, ConfigMixin):",
            "",
            "prev_sample = sample + derivative * dt",
            "",
            "-        device = model_output.device if torch.is_tensor(model_output) else torch.device(\"cpu\")",
            "+        device = model_output.device",
            "if device.type == \"mps\":",
            "# randn does not work reproducibly on mps",
            "noise = torch.randn(model_output.shape, dtype=model_output.dtype, device=\"cpu\", generator=generator).to("
        ]
    },
    {
        "number": 236,
        "comments": "update API call for version fix",
        "commit_message": "Disable use of timm nn.Linear wrapper since AMP autocast + torchscript use appears fixed\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def _create_fc(num_features, num_classes, use_conv=False):",
            "elif use_conv:",
            "fc = nn.Conv2d(num_features, num_classes, 1, bias=True)",
            "else:",
            "-        # NOTE: using my Linear wrapper that fixes AMP + torchscript casting issue",
            "-        fc = Linear(num_features, num_classes, bias=True)",
            "+        fc = nn.Linear(num_features, num_classes, bias=True)",
            "return fc"
        ]
    },
    {
        "number": 242,
        "comments": "remove debug",
        "commit_message": "Fixup `utils` for the trainer\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def get_commit_hash():",
            "return commit",
            "",
            "",
            "-def create_experiment_folder(root_path, model_name, debug):",
            "+def create_experiment_folder(root_path, model_name):",
            "\"\"\"Create a folder with the current date and time\"\"\"",
            "date_str = datetime.datetime.now().strftime(\"%B-%d-%Y_%I+%M%p\")",
            "-    if debug:",
            "-        commit_hash = \"debug\"",
            "-    else:",
            "-        commit_hash = get_commit_hash()",
            "+    commit_hash = get_commit_hash()",
            "output_folder = os.path.join(root_path, model_name + \"-\" + date_str + \"-\" + commit_hash)",
            "os.makedirs(output_folder, exist_ok=True)",
            "print(\" > Experiment folder: {}\".format(output_folder))"
        ]
    },
    {
        "number": 244,
        "comments": "update API call for version fix",
        "commit_message": "fix slow sd test\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_runway_sd_1_5_negative_prompt(sd_device, strategy, sampler):",
            "def test_cv2(strategy, cv2_flag, cv2_radius):",
            "model = ModelManager(",
            "name=\"cv2\",",
            "-        device=device,",
            "+        device=torch.device(device),",
            ")",
            "cfg = get_config(strategy, cv2_flag=cv2_flag, cv2_radius=cv2_radius)",
            "assert_equal("
        ]
    },
    {
        "number": 245,
        "comments": "parameterize the parameter",
        "commit_message": "Upgrade shufflenet; fix paramsetter for restore\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class Model(ModelDesc):",
            "summary.add_moving_summary(self.cost)",
            "",
            "def _get_optimizer(self):",
            "-        lr = tf.get_variable('learning_rate', initializer=1e-3, trainable=False)",
            "+        lr = tf.get_variable('learning_rate', initializer=self.learning_rate, trainable=False)",
            "opt = tf.train.AdamOptimizer(lr, epsilon=1e-3)",
            "return optimizer.apply_grad_processors(",
            "opt, [gradproc.GlobalNormClip(10), gradproc.SummaryGradient()])"
        ]
    },
    {
        "number": 246,
        "comments": "change name",
        "commit_message": "Fixed naming of the fully connected layer\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class Network(object):",
            "weights = self.make_var('weights', shape=[dim, num_out])",
            "biases = self.make_var('biases', [num_out])",
            "op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b",
            "-            fc = op(feed_in, weights, biases, name=scope.name)",
            "+            #fc = op(feed_in, weights, biases, name=scope.name)",
            "+            fc = op(feed_in, weights, biases, name=name)",
            "return fc"
        ]
    },
    {
        "number": 250,
        "comments": "update API call for version fix",
        "commit_message": "Updated code to mesh with get_weights returning a dict and new tf code (#187)\n\n* Updated code to mesh with get_weights returning a dict and new tf code\n\n* Added tf.global_variables_initalizer to hyperopt example as well\n\n* Small fix.\n\n* Small name change.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def train_cnn_and_compute_accuracy(params, steps, train_images, train_labels, va",
            "# Do the training and evaluation.",
            "with tf.Session() as sess:",
            "# Initialize the network weights.",
            "-    sess.run(tf.initialize_all_variables())",
            "+    sess.run(tf.global_variables_initializer())",
            "for i in range(1, steps + 1):",
            "# Fetch the next batch of data.",
            "image_batch = get_batch(train_images, i, batch_size)"
        ]
    },
    {
        "number": 251,
        "comments": "change API call for math fix",
        "commit_message": "[RLlib] Fix PyTorch A3C / A2C loss function using mixed reduced sum / mean (#11449)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def actor_critic_loss(policy, model, dist_class, train_batch):",
            "values = model.value_function()",
            "dist = dist_class(logits, model)",
            "log_probs = dist.logp(train_batch[SampleBatch.ACTIONS])",
            "-    policy.entropy = dist.entropy().mean()",
            "+    policy.entropy = dist.entropy().sum()",
            "policy.pi_err = -train_batch[Postprocessing.ADVANTAGES].dot(",
            "log_probs.reshape(-1))",
            "-    policy.value_err = nn.functional.mse_loss(",
            "-        values.reshape(-1), train_batch[Postprocessing.VALUE_TARGETS])",
            "+    policy.value_err = torch.sum(",
            "+        torch.pow(",
            "+            values.reshape(-1) - train_batch[Postprocessing.VALUE_TARGETS],",
            "+            2.0))",
            "overall_err = sum([",
            "policy.pi_err,",
            "policy.config[\"vf_loss_coeff\"] * policy.value_err,"
        ]
    },
    {
        "number": 252,
        "comments": "update API call for version fix",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DeepSpeedDataLoader(object):",
            "else:",
            "if data_sampler is None:",
            "data_sampler = RandomSampler(dataset)",
            "-                device_count = torch.cuda.device_count()",
            "+                device_count = get_accelerator().device_count()",
            "batch_size *= device_count",
            "",
            "if num_local_io_workers is None:"
        ]
    },
    {
        "number": 254,
        "comments": "use customized API",
        "commit_message": "Refactor library namespaces [pre-release][0.6-rc1] (#1412)\n\n* flake fixes\n\n* initial flake8 fixeS\n\n* remove top level from kornia.color\n\n* kornia filters\n\n* kornia losses\n\n* kornia features\n\n* geomtry and all ok\n\n* removed jit module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* apply formatting and few fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add keep block for isort\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* skip init\n\n* fix the docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove old code\n\n* simplify ci workflow\n\n* fix circular dependency\n\n* few format fixes\n\n* fix code format test\n\n* remove kornia.jit imports\n\n* final fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* ci fixes\n\n* add versioneer\n\n* fix pnp import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* exclude version files from pre-commit\n\n* exclude files precommit\n\n* update to pytorch 1.10 and add fixes\n\n* Update tests_cpu.yml\n\n* Update setup_dev_env.sh\n\n* Update tests_cpu.yml\n\n* undo versioneer\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix no_grad\n\n* Apply suggestions from code review\n\n* fix skip windows tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update test_integrated.py\n\n* Apply suggestions from code review\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def transform_bbox(trans_mat: torch.Tensor, boxes: torch.Tensor, mode: str = \"xy",
            "boxes[..., -2] = boxes[..., 0] + boxes[..., -2]  # x + w",
            "boxes[..., -1] = boxes[..., 1] + boxes[..., -1]  # y + h",
            "",
            "-    transformed_boxes: torch.Tensor = kornia.transform_points(trans_mat, boxes.view(boxes.shape[0], -1, 2))",
            "+    transformed_boxes: torch.Tensor = transform_points(trans_mat, boxes.view(boxes.shape[0], -1, 2))",
            "transformed_boxes = transformed_boxes.view_as(boxes)",
            "",
            "if mode == 'xywh':"
        ]
    },
    {
        "number": 257,
        "comments": "format",
        "commit_message": "fix tensorflow asinh failure\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def asin(x: Union[tf.Tensor, tf.Variable]) -> Union[tf.Tensor, tf.Variable]:",
            "return tf.asin(x)",
            "",
            "",
            "-def asinh(",
            "-        x: Union[tf.Tensor, tf.Variable]",
            "-) -> Union[tf.Tensor, tf.Variable]:",
            "-    x = tf.cast(x, tf.float32)",
            "+def asinh(x: Union[tf.Tensor, tf.Variable]) -> Union[tf.Tensor, tf.Variable]:",
            "return tf.asinh(x)"
        ]
    },
    {
        "number": 258,
        "comments": "add API call for type fix",
        "commit_message": "Fix typing issue in meters when resuming FP16 training (#1132)\n\nSummary:\nWhen we save checkpoints, we move all CUDA tensors to CPU. This includes meter values (e.g., grad norm). Upon reloading the checkpoint, these meter values remain on the CPU, but subsequent meter values are likely to be on GPU, thus raising an exception (PyTorch doesn't support operations between CPU and CUDA tensors). In the case of FP16 training, you get a slightly different exception due to trying to add float16 tensors on CPU, but it's the same underlying cause.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1132\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D20850925\n\nPulled By: myleott\n\nfbshipit-source-id: df12b051f2eae3566a1f4cd1b621ed1c8fdf0050\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class StopwatchMeter(Meter):",
            "if self.start_time is not None:",
            "delta = time.perf_counter() - self.start_time",
            "self.sum = self.sum + delta",
            "-            self.n = self.n + n",
            "+            self.n = type_as(self.n, n) + n",
            "",
            "def reset(self):",
            "self.sum = 0  # cumulative time during which stopwatch was active"
        ]
    },
    {
        "number": 259,
        "comments": "update API call for version fix",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class RagTokenForGeneration(RagPreTrainedModel):",
            "n_docs = n_docs if n_docs is not None else self.config.n_docs",
            "",
            "# RAG-token marginalization",
            "-        seq_logprobs = torch.nn.functional.log_softmax(seq_logits, dim=-1).view(",
            "+        seq_logprobs = nn.functional.log_softmax(seq_logits, dim=-1).view(",
            "seq_logits.shape[0] // n_docs, n_docs, -1, seq_logits.size(-1)",
            ")",
            "doc_logprobs = torch.log_softmax(doc_scores, dim=1)"
        ]
    },
    {
        "number": 265,
        "comments": "add param for resource fix",
        "commit_message": "[train][docs] update docstrings/quickstarts to work when `use_gpu=True` (#31692)\n\nFixes Trainer docstrings and quickstarts to work when use_gpu=True.\n\nSigned-off-by: Matthew Deng <matt@anyscale.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class HorovodTrainer(DataParallelTrainer):",
            "),",
            ")",
            "train_dataset = ray.data.from_items([{\"x\": x, \"y\": x + 1} for x in range(32)])",
            "-        scaling_config = ScalingConfig(num_workers=3)",
            "-        # If using GPUs, use the below scaling config instead.",
            "-        # scaling_config = ScalingConfig(num_workers=3, use_gpu=True)",
            "+        scaling_config = ScalingConfig(num_workers=3, use_gpu=use_gpu)",
            "trainer = HorovodTrainer(",
            "train_loop_per_worker=train_loop_per_worker,",
            "scaling_config=scaling_config,"
        ]
    },
    {
        "number": 266,
        "comments": "remove API call for version fix",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TestStackedSelfAttention(AllenNlpTestCase):",
            "feedforward_hidden_dim=5,",
            "num_layers=3,",
            "num_attention_heads=3)",
            "-        inputs = Variable(torch.randn([3, 5, 9]))",
            "+        inputs = torch.randn([3, 5, 9])",
            "encoder_output = encoder(inputs, None)",
            "assert list(encoder_output.size()) == [3, 5, 12]"
        ]
    },
    {
        "number": 267,
        "comments": "change condition check for version fix",
        "commit_message": "PyTorch 1.4 compat\n\nSummary: Restore compatibility with PyTorch 1.4 and 1.5, and a few lint fixes.\n\nReviewed By: patricklabatut\n\nDifferential Revision: D30048115\n\nfbshipit-source-id: ee05efa7c625f6079fb06a3cc23be93e48df9433\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def qr(A: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:  # pragma: no cove",
            "\"\"\"",
            "Like torch.linalg.qr.",
            "\"\"\"",
            "-    if hasattr(torch.linalg, \"qr\"):",
            "+    if hasattr(torch, \"linalg\") and hasattr(torch.linalg, \"qr\"):",
            "# PyTorch version >= 1.9",
            "return torch.linalg.qr(A)",
            "return torch.qr(A)"
        ]
    },
    {
        "number": 268,
        "comments": "add param for resource fix",
        "commit_message": "fix test (#9669)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def prepare_bart_inputs_dict(",
            "if decoder_attention_mask is None:",
            "decoder_attention_mask = decoder_input_ids.ne(config.pad_token_id)",
            "if head_mask is None:",
            "-        head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads)",
            "+        head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads, device=torch_device)",
            "if decoder_head_mask is None:",
            "-        decoder_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads)",
            "+        decoder_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads, device=torch_device)",
            "return {",
            "\"input_ids\": input_ids,",
            "\"decoder_input_ids\": decoder_input_ids,"
        ]
    },
    {
        "number": 270,
        "comments": "add param for state fix",
        "commit_message": "[RLlib] Dreamer fixes and reinstate Dreamer test. (#17821)\n\nCo-authored-by: sven1977 <svenmika1977@gmail.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DreamerModel(TorchModelV2, nn.Module):",
            "and policy to obtain action.",
            "\"\"\"",
            "if state is None:",
            "-            self.initial_state()",
            "+            self.state = self.get_initial_state(batch_size=obs.shape[0])",
            "else:",
            "self.state = state",
            "post = self.state[:4]"
        ]
    },
    {
        "number": 271,
        "comments": "add condition check for null fix",
        "commit_message": "multiple bug fixes\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Plan(Serializable):",
            "# prevent circular dependency",
            "# syft relative",
            "from ...core.node.vm.vm import VirtualMachine  # noqa: F401",
            "+        if self.local_executor is not None:",
            "+            # this is necessary for syfts nn.module, because the plan contains state from the module",
            "+            # in order to use this state, we first need to send the model, and then execute te plan",
            "+            return self.local_executor(**kwargs)",
            "",
            "alice = VirtualMachine(name=\"plan_executor\")",
            "alice_client: client.Client = alice.get_client()"
        ]
    },
    {
        "number": 272,
        "comments": "change API call for math fix",
        "commit_message": "more fixes\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class GradTTS(DiffusionPipeline):",
            "mu_y = mu_y.transpose(1, 2)",
            "",
            "# Sample latent representation from terminal distribution N(mu_y, I)",
            "-        z = mu_y + torch.randn_like(mu_y, device=mu_y.device) / temperature",
            "+        z = mu_y + torch.randn(mu_y.shape, device=mu_y.device, generator=generator) / temperature",
            "",
            "xt = z * y_mask",
            "h = 1.0 / num_inference_steps"
        ]
    },
    {
        "number": 273,
        "comments": "add API call for type fix",
        "commit_message": "Data2vec prelim (#2929)\n\nSummary:\nPreliminaries for data2vec release, include some minor improvements and bug fixes\n\nMost important change is that we now default to raising an exception when fields in config do not have a corresponding field in the model dataclass\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2929\n\nReviewed By: wnhsu\n\nDifferential Revision: D33649708\n\nPulled By: alexeib\n\nfbshipit-source-id: 629bdb4c361550740b451c570c2005bb956c6fcb\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class NanDetector:",
            "gradients = {}",
            "for name, param in self.named_parameters:",
            "if param.grad is not None:",
            "-                grad_norm = torch.norm(param.grad.data, p=2, dtype=torch.float32)",
            "+                grad_norm = torch.norm(param.grad.data.float(), p=2)",
            "norm[name] = grad_norm.item()",
            "if torch.isnan(grad_norm).any() or torch.isinf(grad_norm).any():",
            "gradients[name] = param.grad.data"
        ]
    },
    {
        "number": 274,
        "comments": "add condition check for null fix",
        "commit_message": "Mixup and prefetcher improvements\n* Do mixup in custom collate fn if prefetcher enabled, reduces performance impact\n* Move mixup code to own file\n* Add arg to disable prefetcher\n* Fix no cuda transfer when prefetcher off\n* Random erasing when prefetcher off wasn't changed to match new args, fixed\n* Default random erasing to off (prob = 0.) for train\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def create_loader(",
            "# of samples per-process, will slightly alter validation results",
            "sampler = OrderedDistributedSampler(dataset)",
            "",
            "+    if collate_fn is None:",
            "+        collate_fn = fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate",
            "+",
            "loader = torch.utils.data.DataLoader(",
            "dataset,",
            "batch_size=batch_size,",
            "shuffle=sampler is None and is_training,",
            "num_workers=num_workers,",
            "sampler=sampler,",
            "-        collate_fn=fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate,",
            "+        collate_fn=collate_fn,",
            "drop_last=is_training,",
            ")",
            "if use_prefetcher:"
        ]
    },
    {
        "number": 277,
        "comments": "log update",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def fpn_map_rois_to_levels(boxes):",
            "Be careful that the returned tensor could be empty.",
            "\"\"\"",
            "sqrtarea = tf.sqrt(tf_area(boxes))",
            "-    level = tf.to_int32(tf.floor(",
            "-        4 + tf.log(sqrtarea * (1. / 224) + 1e-6) * (1.0 / np.log(2))))",
            "+    level = tf.cast(tf.floor(",
            "+        4 + tf.log(sqrtarea * (1. / 224) + 1e-6) * (1.0 / np.log(2))), tf.int32)",
            "",
            "# RoI levels range from 2~5 (not 6)",
            "level_ids = ["
        ]
    },
    {
        "number": 278,
        "comments": "change param for math fix",
        "commit_message": "bugfix in cotcurv laplacian loss. closes #551 (#553)\n\nSummary: Pull Request resolved: https://github.com/facebookresearch/pytorch3d/pull/553\n\nReviewed By: theschnitz\n\nDifferential Revision: D26257591\n\nPulled By: gkioxari\n\nfbshipit-source-id: 899a3f733a77361e8572b0900a34b55764ff08f2\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def mesh_laplacian_smoothing(meshes, method: str = \"uniform\"):",
            "elif method == \"cot\":",
            "loss = L.mm(verts_packed) * norm_w - verts_packed",
            "elif method == \"cotcurv\":",
            "-        loss = (L.mm(verts_packed) - verts_packed) * norm_w",
            "+        loss = (L.mm(verts_packed) - L_sum * verts_packed) * norm_w",
            "loss = loss.norm(dim=1)",
            "",
            "loss = loss * weights"
        ]
    },
    {
        "number": 280,
        "comments": "add condition check for resource fix",
        "commit_message": "bug fix\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class AsyncMultiGPUTrainer(MultiGPUTrainer,",
            "",
            "self._setup_predictor_factory(predict_tower)",
            "self._average_gradient = average_gradient",
            "+        assert tf.test.is_gpu_available()",
            "",
            "def _setup(self):",
            "super(AsyncMultiGPUTrainer, self)._setup()"
        ]
    },
    {
        "number": 282,
        "comments": "add param for argument fix",
        "commit_message": "fixed self-loop bug in gcn\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class GCNConv(MessagePassing):",
            "x = torch.matmul(x, self.weight)",
            "",
            "if not self.cached or self.cached_result is None:",
            "-            edge_index, norm = GCNConv.norm(edge_index,",
            "-                                            x.size(0), edge_weight,",
            "+            edge_index, norm = GCNConv.norm(edge_index, x.size(0), edge_weight,",
            "self.improved, x.dtype)",
            "self.cached_result = edge_index, norm"
        ]
    },
    {
        "number": 283,
        "comments": "add API call for shape fix",
        "commit_message": "fix for asr_mix\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class CTC(torch.nn.Module):",
            "if self.ctc_type == \"builtin\":",
            "olens = to_device(ys_hat, torch.LongTensor([len(s) for s in ys]))",
            "hlens = hlens.long()",
            "+            ys_pad = torch.cat(ys)  # without this the code breaks for asr_mix",
            "self.loss = self.loss_fn(ys_hat, ys_pad, hlens, olens)",
            "else:",
            "self.loss = None"
        ]
    },
    {
        "number": 284,
        "comments": "def",
        "commit_message": "Fixes calibration and adds example scripts (#2431)\n\n* Adds calibration to binary and category output feature schema.\n\n* Adds type annotations for create_calibration_module.\n\n* Fixes initialization of calibration module for category features.\n\n* First pass at forest cover and mushroom edibility.\n\n* Fixed brier plot.\n\n* Adds forest cover visualizations.\n\n* Reduce epochs to 1 and default transformer params.\n\n* Adds calibration as an output feature key which should not be nested inside decoder.\n\n* Moved output_features below input_features.\n\nCo-authored-by: Daniel Treiman <daniel@predibase.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class CategoryOutputFeature(CategoryFeatureMixin, OutputFeature):",
            "# hidden: shape [batch_size, size of final fully connected layer]",
            "return {LOGITS: self.decoder_obj(hidden), PROJECTION_INPUT: hidden}",
            "",
            "-    def create_calibration_module(self, feature) -> torch.nn.Module:",
            "+    def create_calibration_module(self, feature: CategoryOutputFeatureConfig) -> torch.nn.Module:",
            "\"\"\"Creates the appropriate calibration module based on the feature config.",
            "",
            "Today, only one type of calibration (\"temperature_scaling\") is available, but more options may be supported in",
            "the future.",
            "\"\"\"",
            "-        if feature.get(\"calibration\"):",
            "+        if feature.calibration:",
            "calibration_cls = calibration.get_calibration_cls(CATEGORY, \"temperature_scaling\")",
            "return calibration_cls(num_classes=self.num_classes)",
            "return None"
        ]
    },
    {
        "number": 286,
        "comments": "test fix",
        "commit_message": "fix tests on ubuntu\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def test_gcn_conv():",
            "assert out2.size() == (4, 32)",
            "assert torch.allclose(conv(x, adj2.t()), out2, atol=1e-6)",
            "",
            "-    torch.jit.script(conv.jittable())",
            "-",
            "t = '(Tensor, Tensor, OptTensor) -> Tensor'",
            "jit = torch.jit.script(conv.jittable(t))",
            "assert jit(x, edge_index).tolist() == out1.tolist()"
        ]
    },
    {
        "number": 287,
        "comments": "no API",
        "commit_message": "[Flax SD finetune] Fix dtype (#1038)\n\nfix jnp dtype\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def main():",
            "train_dataset, shuffle=True, collate_fn=collate_fn, batch_size=total_train_batch_size, drop_last=True",
            ")",
            "",
            "-    weight_dtype = torch.float32",
            "+    weight_dtype = jnp.float32",
            "if args.mixed_precision == \"fp16\":",
            "-        weight_dtype = torch.float16",
            "+        weight_dtype = jnp.float16",
            "elif args.mixed_precision == \"bf16\":",
            "-        weight_dtype = torch.bfloat16",
            "+        weight_dtype = jnp.bfloat16",
            "",
            "# Load models and create wrapper for stable diffusion",
            "tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")"
        ]
    },
    {
        "number": 294,
        "comments": "format",
        "commit_message": "Quantisation (#5706)\n\n* empty\n\n* sq\n\n* obs\n\n\n* int\n\n* ts\n\n* helpers\n\n* chlog\n\n* yapf\n\n* avg\n\n* dupl\n\n* Apply suggestions from code review\n\nCo-authored-by: Nicki Skafte <skaftenicki@gmail.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n\n* fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n\n* fixes\n\n* note\n\n* warn\n\n* 45\n\n* link\n\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n\n* yapf\n\n* flake8\n\n* Apply suggestions from code review\n\n* Apply suggestions from code review\n\nCo-authored-by: Nicki Skafte <skaftenicki@gmail.com>\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def test_result_reduce_ddp(result_cls):",
            "pytest.param(5, False, 0, id='nested_list_predictions'),",
            "pytest.param(6, False, 0, id='dict_list_predictions'),",
            "pytest.param(7, True, 0, id='write_dict_predictions'),",
            "-        pytest.param(",
            "-            0,",
            "-            True,",
            "-            1,",
            "-            id='full_loop_single_gpu',",
            "-            marks=pytest.mark.skipif(torch.cuda.device_count() < 1, reason=\"test requires single-GPU machine\")",
            "-        )",
            "+        pytest.param(0, True, 1, id='full_loop_single_gpu', marks=pytest.mark.skipif(**_SKIPIF_ARGS_NO_GPU))",
            "]",
            ")",
            "def test_result_obj_predictions(tmpdir, test_option, do_train, gpus):"
        ]
    },
    {
        "number": 296,
        "comments": "add API call for resource fix",
        "commit_message": "Fix torch device issues (#20304)\n\n* fix device issue\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DeformableDetrModelIntegrationTests(unittest.TestCase):",
            "results = feature_extractor.post_process_object_detection(",
            "outputs, threshold=0.3, target_sizes=[image.size[::-1]]",
            ")[0]",
            "-        expected_scores = torch.tensor([0.7999, 0.7894, 0.6331, 0.4720, 0.4382])",
            "+        expected_scores = torch.tensor([0.7999, 0.7894, 0.6331, 0.4720, 0.4382]).to(torch_device)",
            "expected_labels = [17, 17, 75, 75, 63]",
            "-        expected_slice_boxes = torch.tensor([16.5028, 52.8390, 318.2544, 470.7841])",
            "+        expected_slice_boxes = torch.tensor([16.5028, 52.8390, 318.2544, 470.7841]).to(torch_device)",
            "",
            "self.assertEqual(len(results[\"scores\"]), 5)",
            "self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-4))"
        ]
    },
    {
        "number": 297,
        "comments": "change param for math fix",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Critic(object):",
            "n = InputLayer(self.s, name='in')",
            "n = DenseLayer(n, n_units=30, act=tf.nn.relu6, W_init=tf.random_uniform_initializer(0, 0.01), name='hidden')",
            "# n = DenseLayer(n, n_units=5, act=tf.nn.relu, W_init=tf.random_uniform_initializer(0, 0.01), name='hidden2')",
            "-            n = DenseLayer(n, n_units=1, act=tf.identity, name='V')",
            "+            n = DenseLayer(n, n_units=1, act=None, name='V')",
            "self.v = n.outputs",
            "",
            "with tf.variable_scope('squared_TD_error'):"
        ]
    },
    {
        "number": 298,
        "comments": "update API call for refactor fix",
        "commit_message": "Fix wrong replacement\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TensorflowBackendInferenceLearner(TensorflowBaseInferenceLearner):",
            "metadata = LearnerMetadata.read(path)",
            "network_parameters = ModelParams(**metadata.network_parameters)",
            "input_tfms = metadata.input_tfms",
            "-        model = nebullvm.operations.inference_learners.utils.load_model(",
            "+        model = tf.keras.models.load_model(",
            "path / TENSORFLOW_BACKEND_FILENAMES[\"tf_model\"]",
            ")",
            "device = Device(metadata.device)"
        ]
    },
    {
        "number": 300,
        "comments": "add API call for state fix",
        "commit_message": "[RLlib] Curiosity minor fixes, do-overs, and testing. (#10143)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "FileType = Any",
            "# Represents the result dict returned by Trainer.train().",
            "ResultDict = dict",
            "",
            "+# A tf or torch local optimizer object.",
            "+LocalOptimizer = Union[\"tf.keras.optimizers.Optimizer\",",
            "+                       \"torch.optim.Optimizer\"]",
            "+",
            "# Dict of tensors returned by compute gradients on the policy, e.g.,",
            "# {\"td_error\": [...], \"learner_stats\": {\"vf_loss\": ..., ...}}, for multi-agent,",
            "# {\"policy1\": {\"learner_stats\": ..., }, \"policy2\": ...}."
        ]
    },
    {
        "number": 301,
        "comments": "method define",
        "commit_message": "Refactor triton buffer to use CLBuffer of cuda runtime (#524)\n\n* Refactor triton buffer to use CLBuffer of runtime\n\n* Fix opencl GT0\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class CLImage:",
            "",
            "class CLBuffer:",
            "def __init__(self, size): self.cl = cuda.mem_alloc(size)",
            "-  def copyin(self, b:np.ndarray): cuda.memcpy_htod_async(self.cl, b)",
            "+  def copyin(self, b:np.ndarray, stream:Optional[cuda.Stream]=None): cuda.memcpy_htod_async(self.cl, b, stream)",
            "def copyout(self, a:np.ndarray): cuda.memcpy_dtoh(a, self.cl)",
            "",
            "class CLProgram:"
        ]
    },
    {
        "number": 302,
        "comments": "value change",
        "commit_message": "Fix RMSLE metric (#3188)\n\n* fix rmsle\n\n* Updated test to match rmsle fix\n\n* Updated RMSLE example result to match functional\n\n* chlog\n\n* add randomized test\n\n* fix pep8\n\nCo-authored-by: Jirka Borovec <jirka@pytorchlightning.ai>\nCo-authored-by: Nicki Skafte <skaftenicki@gmail.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def rmsle(",
            ">>> x = torch.tensor([0., 1, 2, 3])",
            ">>> y = torch.tensor([0., 1, 2, 2])",
            ">>> rmsle(x, y)",
            "-        tensor(0.0207)",
            "+        tensor(0.1438)",
            "",
            "\"\"\"",
            "-    rmsle = mse(torch.log(pred + 1), torch.log(target + 1), reduction=reduction)",
            "+    rmsle = rmse(torch.log(pred + 1), torch.log(target + 1), reduction=reduction)",
            "return rmsle"
        ]
    },
    {
        "number": 303,
        "comments": "test fix",
        "commit_message": "More unit test fixes\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def add_dataset_args(parser, train=False, gen=False):",
            "",
            "def add_distributed_training_args(parser):",
            "group = parser.add_argument_group('Distributed training')",
            "-    group.add_argument('--distributed-world-size', default=1, type=int, metavar='N',",
            "-                       help='total number of GPUs across all nodes, default: 1 GPU')",
            "+    group.add_argument('--distributed-world-size', type=int, metavar='N',",
            "+                       default=torch.cuda.device_count(),",
            "+                       help='total number of GPUs across all nodes (default: all visible GPUs)')",
            "group.add_argument('--distributed-rank', default=0, type=int,",
            "help='rank of the current worker')",
            "group.add_argument('--distributed-backend', default='nccl', type=str,"
        ]
    },
    {
        "number": 305,
        "comments": "add param for type fix",
        "commit_message": "Added dtype parameter to zeros_like and ones_like (#5062)\n\n* Fixed checking input masks in Layer.compute_mask\n\n* Added dtype parameter to zeros_like and ones_like\n\n* Fix existing docstring for ones_like and zeros_like\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def ones_like(x, name=None):",
            "[ 1.,  1.,  1.]], dtype=float32)",
            "```",
            "\"\"\"",
            "-    return tf.ones_like(x, name=name)",
            "+    return tf.ones_like(x, dtype=dtype, name=name)",
            "",
            "",
            "def random_uniform_variable(shape, low, high, dtype=None,"
        ]
    },
    {
        "number": 307,
        "comments": "remove API call for version fix",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "from allennlp.common.params import Params",
            "",
            "class TestStackedBidirectionalLstm(AllenNlpTestCase):",
            "def test_stacked_bidirectional_lstm_completes_forward_pass(self):",
            "-        input_tensor = torch.autograd.Variable(torch.rand(4, 5, 3))",
            "+        input_tensor = torch.rand(4, 5, 3)",
            "input_tensor[1, 4:, :] = 0.",
            "input_tensor[2, 2:, :] = 0.",
            "input_tensor[3, 1:, :] = 0."
        ]
    },
    {
        "number": 309,
        "comments": "add param for argument fix",
        "commit_message": "fixing CTRL tests and OpenAI GPT tests\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFCTRLMainLayer(tf.keras.layers.Layer):",
            "token_type_embeds = 0",
            "position_ids = tf.reshape(position_ids, [-1, shape_list(position_ids)[-1]])",
            "",
            "-        inputs_embeds = self.w(input_ids)",
            "+        inputs_embeds = self.w(input_ids, mode='embedding')",
            "# x = embedded.unsqueeze(0) if len(input_ids.shape)<2 else embedded",
            "seq_len = input_shape[-1]",
            "mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)"
        ]
    },
    {
        "number": 310,
        "comments": "format",
        "commit_message": "Fix issue with non-canonical TF version name format.\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def _preprocess_conv3d_input(x, data_format):",
            "A tensor.",
            "\"\"\"",
            "# tensorflow doesn't support float64 for conv layer before 1.8.0",
            "-    if (dtype(x) == 'float64'",
            "-            and StrictVersion(tf.__version__) < StrictVersion('1.8.0')):",
            "+    if (dtype(x) == 'float64' and",
            "+            StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):",
            "x = tf.cast(x, 'float32')",
            "tf_data_format = 'NDHWC'",
            "if data_format == 'channels_first':"
        ]
    },
    {
        "number": 312,
        "comments": "remove constraint for version fix",
        "commit_message": "add fix for torch 1.0 on RTD (#1591)\n\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "from __future__ import absolute_import, division, print_function",
            "",
            "+import os",
            "+",
            "import torch",
            "",
            "-assert torch.__version__.startswith('1.')",
            "+if 'READTHEDOCS' not in os.environ:",
            "+    # RTD is running 0.4.1 due to a memory issue with pytorch 1.0",
            "+    assert torch.__version__.startswith('1.')",
            "",
            "",
            "def patch_dependency(target, root_module=torch):"
        ]
    },
    {
        "number": 314,
        "comments": "functional change",
        "commit_message": "Fix bug in get_or_create_layer migration utility that produced regularization losses of the wrong rank, causing failures on model fit.\n\nPiperOrigin-RevId: 414066868\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class _EagerVariableStore(tf.Module):",
            "layer = create_layer_method()",
            "self._layers[name] = layer",
            "if isinstance(layer, base_layer.Layer):",
            "-        self._regularizers[name] = lambda: layer.losses",
            "+        self._regularizers[name] = lambda: tf.math.reduce_sum(layer.losses)",
            "return self._layers[name]",
            "",
            "def add_regularizer(self, var, regularizer):"
        ]
    },
    {
        "number": 315,
        "comments": "add param for argument fix",
        "commit_message": "Fix Padded Batch Error 12282 (#12487)\n\nThis fixes the padded batch [issue](https://github.com/huggingface/transformers/issues/12282). The error was generated due to the maximum sequence length of the attention mask not matching the padded sequence length of the hidden_states. `np.allclose` now passes with a 1e-2 absolute tolerance.\n\nThis change fixes\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TFWav2Vec2MainLayer(tf.keras.layers.Layer):",
            "if inputs[\"attention_mask\"] is not None:",
            "# compute real output lengths according to convolution formula",
            "output_lengths = self._get_feat_extract_output_lengths(tf.reduce_sum(inputs[\"attention_mask\"], -1))",
            "-            attention_mask = tf.sequence_mask(output_lengths, dtype=hidden_states.dtype)",
            "+",
            "+            attention_mask = tf.sequence_mask(",
            "+                output_lengths, maxlen=shape_list(hidden_states)[1], dtype=hidden_states.dtype",
            "+            )",
            "",
            "hidden_states = self.feature_projection(hidden_states, training=inputs[\"training\"])"
        ]
    },
    {
        "number": 320,
        "comments": "remove constraint",
        "commit_message": "Refactoring, cleanup, improved test coverage.\n* Add eca_nfnet_l2 weights, 84.7 @ 384x384\n* All 'non-std' (ie transformer / mlp) models have classifier / default_cfg test added\n* Fix #694 reset_classifer / num_features / forward_features / num_classes=0 consistency for transformer / mlp models\n* Add direct loading of npz to vision transformer (pure transformer so far, hybrid to come)\n* Rename vit_deit* to deit_*\n* Remove some deprecated vit hybrid model defs\n* Clean up classifier flatten for conv classifiers and unusual cases (mobilenetv3/ghostnet)\n* Remove explicit model fns for levit conv, just pass in arg\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class MobileNetV3(nn.Module):",
            "",
            "def forward(self, x):",
            "x = self.forward_features(x)",
            "-        if not self.global_pool.is_identity():",
            "-            x = x.flatten(1)",
            "+        x = self.flatten(x)",
            "if self.drop_rate > 0.:",
            "x = F.dropout(x, p=self.drop_rate, training=self.training)",
            "return self.classifier(x)"
        ]
    },
    {
        "number": 322,
        "comments": "value change",
        "commit_message": "Fix docstrings for TF BLIP (#22618)\n\n* Fix docstrings for TFBLIP\n\n* Fix missing line in TF port!\n\n* Use values from torch tests now other bugs fixed\n\n* Use values from torch tests now other bugs fixed\n\n* Fix doctest string\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class TFBlipModelIntegrationTest(unittest.TestCase):",
            "out_itm = model(**inputs)",
            "out = model(**inputs, use_itm_head=False, training=False)",
            "",
            "-        expected_scores = tf.convert_to_tensor([[0.9798, 0.0202]])",
            "+        expected_scores = tf.convert_to_tensor([[0.0029, 0.9971]])",
            "self.assertTrue(np.allclose(tf.nn.softmax(out_itm[0]).numpy(), expected_scores, rtol=1e-3, atol=1e-3))",
            "-        self.assertTrue(np.allclose(out[0], tf.convert_to_tensor([[0.5053]]), rtol=1e-3, atol=1e-3))",
            "+        self.assertTrue(np.allclose(out[0], tf.convert_to_tensor([[0.5162]]), rtol=1e-3, atol=1e-3))"
        ]
    },
    {
        "number": 323,
        "comments": "update API call for version fix",
        "commit_message": "fix small issues in the code refactoring\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class BaseModel():",
            "save_filename = '%s_net_%s.pth' % (which_epoch, name)",
            "save_path = os.path.join(self.save_dir, save_filename)",
            "net = getattr(self, 'net' + name)",
            "-                net.load_state_dict(torch.load(save_path))",
            "+                net.module.load_state_dict(torch.load(save_path))",
            "",
            "# print network information",
            "def print_networks(self, verbose):"
        ]
    },
    {
        "number": 324,
        "comments": "change param for type fix",
        "commit_message": "Fix dynamic info extraction for stable diffusion\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class SpeedsterRootOp(Operation):",
            ") -> List[BaseInferenceLearner]:",
            "if self.orig_latency_measure_op.get_result() is not None:",
            "model_outputs = self.orig_latency_measure_op.get_result()[0]",
            "-            if isinstance(model, Module):",
            "+            if isinstance(model, torch.nn.Module):",
            "optimization_op = self.torch_optimization_op",
            "elif isinstance(model, tf.Module) and model is not None:",
            "optimization_op = self.tensorflow_optimization_op"
        ]
    },
    {
        "number": 325,
        "comments": "rename",
        "commit_message": "YOLOv5 Apple Metal Performance Shader (MPS) support (#7878)\n\n* Apple Metal Performance Shader (MPS) device support\n\nFollowing https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/\n\nShould work with Apple M1 devices with PyTorch nightly installed with command `--device mps`. Usage examples:\n```bash\npython train.py --device mps\npython detect.py --device mps\npython val.py --device mps\n```\n\n* Update device strategy to fix MPS issue\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def run(",
            "):",
            "# PyTorch model",
            "im = torch.zeros((batch_size, 3, *imgsz))  # BCHW image",
            "-    model = attempt_load(weights, map_location=torch.device('cpu'), inplace=True, fuse=False)",
            "+    model = attempt_load(weights, device=torch.device('cpu'), inplace=True, fuse=False)",
            "_ = model(im)  # inference",
            "model.info()"
        ]
    },
    {
        "number": 327,
        "comments": "rename",
        "commit_message": "Fix AnchorHead in_channels (#1506)\n\n* test that all configs can be loaded\n\n* Use in_channels correctly in anchor_head and guided_anchor_head\n\n* Fix lint errors. Only tests a subset of configs\n\n* remove local config\n\n* fix yapf\n\n* Remove slower tests\n\n* Remove debug code\n\n* trigger travis\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class GuidedAnchorHead(AnchorHead):",
            "",
            "def _init_layers(self):",
            "self.relu = nn.ReLU(inplace=True)",
            "-        self.conv_loc = nn.Conv2d(self.feat_channels, 1, 1)",
            "-        self.conv_shape = nn.Conv2d(self.feat_channels, self.num_anchors * 2,",
            "-                                    1)",
            "+        self.conv_loc = nn.Conv2d(self.in_channels, 1, 1)",
            "+        self.conv_shape = nn.Conv2d(self.in_channels, self.num_anchors * 2, 1)",
            "self.feature_adaption = FeatureAdaption(",
            "-            self.feat_channels,",
            "+            self.in_channels,",
            "self.feat_channels,",
            "kernel_size=3,",
            "deformable_groups=self.deformable_groups)"
        ]
    },
    {
        "number": 328,
        "comments": "update API call for version fix",
        "commit_message": "Inference support for `mps` device (#355)\n\n* Initial support for mps in Stable Diffusion pipeline.\n\n* Initial \"warmup\" implementation when using mps.\n\n* Make some deterministic tests pass with mps.\n\n* Disable training tests when using mps.\n\n* SD: generate latents in CPU then move to device.\n\nThis is especially important when using the mps device, because\ngenerators are not supported there. See for example\nhttps://github.com/pytorch/pytorch/issues/84288.\n\nIn addition, the other pipelines seem to use the same approach: generate\nthe random samples then move to the appropriate device.\n\nAfter this change, generating an image in MPS produces the same result\nas when using the CPU, if the same seed is used.\n\n* Remove prints.\n\n* Pass AutoencoderKL test_output_pretrained with mps.\n\nSampling from `posterior` must be done in CPU.\n\n* Style\n\n* Do not use torch.long for log op in mps device.\n\n* Perform incompatible padding ops in CPU.\n\nUNet tests now pass.\nSee https://github.com/pytorch/pytorch/issues/84535\n\n* Style: fix import order.\n\n* Remove unused symbols.\n\n* Remove MPSWarmupMixin, do not apply automatically.\n\nWe do apply warmup in the tests, but not during normal use.\nThis adopts some PR suggestions by @patrickvonplaten.\n\n* Add comment for mps fallback to CPU step.\n\n* Add README_mps.md for mps installation and use.\n\n* Apply `black` to modified files.\n\n* Restrict README_mps to SD, show measures in table.\n\n* Make PNDM indexing compatible with mps.\n\nAddresses #239.\n\n* Do not use float64 when using LDMScheduler.\n\nFixes #358.\n\n* Fix typo identified by @patil-suraj\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* Adapt example to new output style.\n\n* Restore 1:1 results reproducibility with CompVis.\n\nHowever, mps latents need to be generated in CPU because generators\ndon't work in the mps device.\n\n* Move PyTorch nightly to requirements.\n\n* Adapt `test_scheduler_outputs_equivalence` ton MPS.\n\n* mps: skip training tests instead of ignoring silently.\n\n* Make VQModel tests pass on mps.\n\n* mps ddim tests: warmup, increase tolerance.\n\n* ScoreSdeVeScheduler indexing made mps compatible.\n\n* Make ldm pipeline tests pass using warmup.\n\n* Style\n\n* Simplify casting as suggested in PR.\n\n* Add Known Issues to readme.\n\n* `isort` import order.\n\n* Remove _mps_warmup helpers from ModelMixin.\n\nAnd just make changes to the tests.\n\n* Skip tests using unittest decorator for consistency.\n\n* Remove temporary var.\n\n* Remove spurious blank space.\n\n* Remove unused symbol.\n\n* Remove README_mps.\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com> \n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class UNet2DConditionModel(ModelMixin, ConfigMixin):",
            "if not torch.is_tensor(timesteps):",
            "timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)",
            "elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:",
            "-            timesteps = timesteps[None].to(sample.device)",
            "+            timesteps = timesteps.to(dtype=torch.float32)",
            "+            timesteps = timesteps[None].to(device=sample.device)",
            "",
            "# broadcast to batch dimension in a way that's compatible with ONNX/Core ML",
            "timesteps = timesteps.expand(sample.shape[0])"
        ]
    },
    {
        "number": 332,
        "comments": "value change",
        "commit_message": "keep the '-summary' suffix\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def add_moving_summary(*args, **kwargs):",
            "ema_ops.append(ema_op)",
            "with tf.name_scope(None):",
            "# cannot add it into colocate group -- will force everything to cpus",
            "-            tf.summary.scalar(name, ema_op)    # write the EMA value as a summary",
            "+            tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary",
            "if coll is not None:",
            "for op in ema_ops:",
            "# TODO a new collection to summary every step?"
        ]
    },
    {
        "number": 333,
        "comments": "update API call for version fix",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class BilinearSimilarity(SimilarityFunction):",
            "self.reset_parameters()",
            "",
            "def reset_parameters(self):",
            "-        torch.nn.init.xavier_uniform(self._weight_matrix)",
            "+        torch.nn.init.xavier_uniform_(self._weight_matrix)",
            "self._bias.data.fill_(0)",
            "",
            "@overrides"
        ]
    },
    {
        "number": 335,
        "comments": "update API call for refactor fix",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class AdaptiveEmbedding(nn.Module):",
            "",
            "inp_i = inp_flat.index_select(0, indices_i) - l_idx",
            "emb_i = self.emb_layers[i](inp_i)",
            "-                emb_i = F.linear(emb_i, self.emb_projs[i])",
            "+                emb_i = nn.functional.linear(emb_i, self.emb_projs[i])",
            "",
            "emb_flat.index_copy_(0, indices_i, emb_i)"
        ]
    },
    {
        "number": 337,
        "comments": "change param for math fix",
        "commit_message": "fix and use corr2d without tf cast\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Residual(tf.keras.Model):  #@save",
            "if self.conv3 is not None:",
            "X = self.conv3(X)",
            "Y += X",
            "-        return tf.keras.activations.relu(Y + X)",
            "+        return tf.keras.activations.relu(Y)"
        ]
    },
    {
        "number": 341,
        "comments": "add API call for state fix",
        "commit_message": "Fix Mobilenet V3 model name for sotabench. Minor res2net cleanup.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "for m in model_list:",
            "data_root=os.environ.get('IMAGENET_DIR', './imagenet')",
            ")",
            "",
            "+    torch.cuda.empty_cache()",
            "+"
        ]
    },
    {
        "number": 342,
        "comments": "add API call for type fix",
        "commit_message": "Refactoring the TF activations functions (#7150)\n\n* Refactoring the activations functions into a common file\n\n* Apply style\n\n* remove unused import\n\n* fix tests\n\n* Fix tests.\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFMLP(tf.keras.layers.Layer):",
            "nx = config.n_embd",
            "self.c_fc = TFConv1D(n_state, nx, initializer_range=config.initializer_range, name=\"c_fc\")",
            "self.c_proj = TFConv1D(nx, n_state, initializer_range=config.initializer_range, name=\"c_proj\")",
            "-        self.act = gelu",
            "+        self.act = get_tf_activation(\"gelu\")",
            "self.dropout = tf.keras.layers.Dropout(config.resid_pdrop)",
            "",
            "def call(self, x, training=False):"
        ]
    },
    {
        "number": 347,
        "comments": "remove param for math fix",
        "commit_message": "fix mozilla/TTS#685\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TacotronAbstract(ABC, nn.Module):",
            "def _backward_pass(self, mel_specs, encoder_outputs, mask):",
            "\"\"\" Run backwards decoder \"\"\"",
            "decoder_outputs_b, alignments_b, _ = self.decoder_backward(",
            "-            encoder_outputs, torch.flip(mel_specs, dims=(1,)), mask,",
            "-            self.speaker_embeddings_projected)",
            "+            encoder_outputs, torch.flip(mel_specs, dims=(1,)), mask)",
            "decoder_outputs_b = decoder_outputs_b.transpose(1, 2).contiguous()",
            "return decoder_outputs_b, alignments_b"
        ]
    },
    {
        "number": 348,
        "comments": "add param for resource fix",
        "commit_message": "fix test ddp initialize (#2100)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def main():",
            "model = MMDataParallel(model, device_ids=[0])",
            "outputs = single_gpu_test(model, data_loader, args.show)",
            "else:",
            "-        model = MMDistributedDataParallel(model.cuda())",
            "+        model = MMDistributedDataParallel(",
            "+            model.cuda(),",
            "+            device_ids=[torch.cuda.current_device()],",
            "+            broadcast_buffers=False)",
            "outputs = multi_gpu_test(model, data_loader, args.tmpdir,",
            "args.gpu_collect)"
        ]
    },
    {
        "number": 350,
        "comments": "rename",
        "commit_message": "Allow directly passing text embeddings to Stable Diffusion Pipeline for prompt weighting (#2071)\n\n* add text embeds to sd\n\n* add text embeds to sd\n\n* finish tests\n\n* finish\n\n* finish\n\n* make style\n\n* fix tests\n\n* make style\n\n* make style\n\n* up\n\n* better docs\n\n* fix\n\n* fix\n\n* new try\n\n* up\n\n* up\n\n* finish\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class StableDiffusionImageVariationPipeline(DiffusionPipeline):",
            "image_embeddings = image_embeddings.view(bs_embed * num_images_per_prompt, seq_len, -1)",
            "",
            "if do_classifier_free_guidance:",
            "-            uncond_embeddings = torch.zeros_like(image_embeddings)",
            "+            negative_prompt_embeds = torch.zeros_like(image_embeddings)",
            "",
            "# For classifier free guidance, we need to do two forward passes.",
            "# Here we concatenate the unconditional and text embeddings into a single batch",
            "# to avoid doing two forward passes",
            "-            image_embeddings = torch.cat([uncond_embeddings, image_embeddings])",
            "+            image_embeddings = torch.cat([negative_prompt_embeds, image_embeddings])",
            "",
            "return image_embeddings"
        ]
    },
    {
        "number": 357,
        "comments": "remove param for shape fix",
        "commit_message": "fix input shape in get_rotation_matrix2d\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Tester(unittest.TestCase):",
            "# generate input data",
            "batch_size = 1",
            "center = torch.zeros(batch_size, 2)",
            "-        angle = torch.ones(batch_size, 1)",
            "-        scale = torch.ones(batch_size, 1)",
            "+        angle = torch.ones(batch_size)",
            "+        scale = torch.ones(batch_size)",
            "",
            "center = utils.tensor_to_gradcheck_var(center)  # to var",
            "angle = utils.tensor_to_gradcheck_var(angle)  # to var"
        ]
    },
    {
        "number": 360,
        "comments": "change condition check for state fix",
        "commit_message": "[deepspeed] check whether model is NLP one instead of counting on input type (#21800)\n\n* trying to figure out whether model is NLP\n\n* drop my changes and apply easier fix\n\n* trying to handle all int input types\n\n* fix logic\n\n---------\n\nCo-authored-by: Stas Bekman <stas@stason.org>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Trainer:",
            "return type(data)(self._prepare_input(v) for v in data)",
            "elif isinstance(data, torch.Tensor):",
            "kwargs = {\"device\": self.args.device}",
            "-            if self.deepspeed and data.dtype != torch.int64:",
            "-                # NLP models inputs are int64 and those get adjusted to the right dtype of the",
            "+            if self.deepspeed and (torch.is_floating_point(data) or torch.is_complex(data)):",
            "+                # NLP models inputs are int/uint and those get adjusted to the right dtype of the",
            "# embedding. Other models such as wav2vec2's inputs are already float and thus",
            "# may need special handling to match the dtypes of the model",
            "kwargs.update({\"dtype\": self.args.hf_deepspeed_config.dtype()})"
        ]
    },
    {
        "number": 362,
        "comments": "add API call for resource fix",
        "commit_message": "Fix torch device issues (#20584)\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DeformableDetrImageProcessor(BaseImageProcessor):",
            "img_w = torch.Tensor([i[1] for i in target_sizes])",
            "else:",
            "img_h, img_w = target_sizes.unbind(1)",
            "-        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)",
            "+        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)",
            "boxes = boxes * scale_fct[:, None, :]",
            "",
            "results = []"
        ]
    },
    {
        "number": 363,
        "comments": "change API call for type fix",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class SpanBasedF1Test(AllenNlpTestCase):",
            "gold_indices = [self.vocab.get_token_index(x, \"tags\") for x in bio_tags]",
            "gold_tensor = torch.tensor([gold_indices], device=device)",
            "prediction_tensor = torch.rand([1, 6, self.vocab.get_vocab_size(\"tags\")], device=device)",
            "-        mask = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device=device)",
            "+        mask = torch.BoolTensor(",
            "+            [[True, True, True, True, True, True, True, True, True]], device=device",
            "+        )",
            "",
            "# Make prediction so that it is exactly correct.",
            "for i, tag_index in enumerate(gold_indices):"
        ]
    },
    {
        "number": 364,
        "comments": "add condition check for null fix",
        "commit_message": "Remove prefix for first tower in replicated mode. Support inference now.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TowerContext(object):",
            "self._ctxs = []",
            "if len(self._name):",
            "if self.has_own_variables:",
            "-                # open new variable scopes",
            "-                self._ctxs.append(tf.variable_scope(self._name))",
            "+                if self.vs_name:",
            "+                    self._ctxs.append(tf.variable_scope(self.vs_name))",
            "else:",
            "# use existing variable scope",
            "reuse = self.index > 0 or (not self.is_training)"
        ]
    },
    {
        "number": 367,
        "comments": "add param for shape fix",
        "commit_message": "Fixes shape issue in `_BinaryPostprocessing` (#2094)\n\n* Fixes shape issue in\n\n* adds regression test\n\nCo-authored-by: Geoffrey Angus <geoffrey@predibase.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class _BinaryPostprocessing(torch.nn.Module):",
            "predictions = [self.bool2str.get(pred, self.bool2str[0]) for pred in predictions]",
            "",
            "probs = preds[self.probabilities_key]",
            "-        probs = torch.dstack(1 - probs, probs)",
            "+        probs = torch.stack([1 - probs, probs], dim=-1)",
            "",
            "return {",
            "self.predictions_key: predictions,"
        ]
    },
    {
        "number": 368,
        "comments": "change API call for resource fix",
        "commit_message": "revert \"chore(core): update launch backend to subprocess (#158)\" (#176)\n\nThis reverts commit 9ac889 to fix training bug\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "if __name__ == \"__main__\":",
            "exp = get_exp(args.exp_file, args.name)",
            "exp.merge(args.opts)",
            "",
            "-    num_gpu = get_num_devices() if args.devices is None else args.devices",
            "-    assert num_gpu <= get_num_devices()",
            "+    num_gpu = torch.cuda.device_count() if args.devices is None else args.devices",
            "+    assert num_gpu <= torch.cuda.device_count()",
            "",
            "dist_url = \"auto\" if args.dist_url is None else args.dist_url",
            "launch("
        ]
    },
    {
        "number": 370,
        "comments": "change API call for type fix",
        "commit_message": "Fix tests failing on CUDA (#1834)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def test_log_prob_eta1(d):",
            "assert (lps_less_ladj - lps_less_ladj.min()).abs().sum() < 1e-4",
            "",
            "",
            "-@pytest.mark.parametrize(\"eta\", [.1, .5, 1, 2, 5])",
            "+@pytest.mark.parametrize(\"eta\", [.1, .5, 1., 2., 5.])",
            "def test_log_prob_d2(eta):",
            "-    dist = LKJCorrCholesky(2, torch.DoubleTensor([eta]))",
            "+    dist = LKJCorrCholesky(2, torch.tensor([eta]))",
            "test_dist = TransformedDistribution(Beta(eta, eta), AffineTransform(loc=-1., scale=2.0))",
            "",
            "samples = dist.sample(torch.Size([100]))"
        ]
    },
    {
        "number": 372,
        "comments": "rename the variable",
        "commit_message": "fix a bug in DDPG.py.\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class Critic(object):",
            "self.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss)",
            "",
            "with tf.variable_scope('a_grad'):",
            "-            self.a_grads = tf.gradients(self.q, a)[0]   # tensor of gradients of each sample (None, a_dim)",
            "+            self.a_grads = tf.gradients(self.q, self.a)[0]   # tensor of gradients of each sample (None, a_dim)",
            "",
            "if self.replacement['name'] == 'hard':",
            "self.t_replace_counter = 0"
        ]
    },
    {
        "number": 375,
        "comments": "add API call for state fix",
        "commit_message": "[PyTorch] Set net.eval and net.train explicitly (#1110)\n\n* [PyTorch] Set net.eval and net.train explicitly\n\n* [PyTorch] Set net.eval if isinstance nn.Module\n\n* [PyTorch] Set net.train if isinstance nn.Module\n\n* [Fix] Custom Dropout, Closes #1093\n\n* 2 spaces before inline comments\n\n* 2 spaces before inline comments\n\n* 2 spaces before inline comments\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def corr2d(X, K):  #@save",
            "",
            "# Defined in file: ./chapter_convolutional-neural-networks/lenet.md",
            "def evaluate_accuracy_gpu(net, data_iter, device=None): #@save",
            "+    net.eval()  # Set the model to evaluation mode",
            "if not device:",
            "device = next(iter(net.parameters())).device",
            "metric = d2l.Accumulator(2)  # num_corrected_examples, num_examples"
        ]
    },
    {
        "number": 376,
        "comments": "add param for argument fix",
        "commit_message": "[feat] updated patch augmentation (#1095)\n\n* updated patch augmentation\n\n* Fixed lint\n\n* Renamed forward_batchwise and forward_patchwise\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class VideoSequential(ImageSequential):",
            "# Size of T",
            "frame_num = input.size(self._temporal_channel)",
            "# Got param generation shape to (B, C, H, W). Ignoring T.",
            "-        batch_shape = self.__infer_channel_exclusive_batch_shape__(input)",
            "+        batch_shape = self.__infer_channel_exclusive_batch_shape__(input, self._temporal_channel)",
            "input = self._input_shape_convert_in(input)",
            "input = input.reshape(-1, *batch_shape[1:])",
            "if not self.same_on_frame:"
        ]
    },
    {
        "number": 377,
        "comments": "add param for resource fix",
        "commit_message": "Fix2 `select_device()` for Multi-GPU (#6461)\n\n* Fix2 select_device() for Multi-GPU\n\n* Cleanup\n\n* Cleanup\n\n* Simplify error message\n\n* Improve assert\n\n* Update torch_utils.py\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def create_dataloader(path, imgsz, batch_size, stride, single_cls=False, hyp=Non",
            "prefix=prefix)",
            "",
            "batch_size = min(batch_size, len(dataset))",
            "-    nw = min([os.cpu_count() // DEVICE_COUNT, batch_size if batch_size > 1 else 0, workers])  # number of workers",
            "+    nd = torch.cuda.device_count()  # number of CUDA devices",
            "+    nw = min([os.cpu_count() // max(nd, 1), batch_size if batch_size > 1 else 0, workers])  # number of workers",
            "sampler = None if rank == -1 else distributed.DistributedSampler(dataset, shuffle=shuffle)",
            "loader = DataLoader if image_weights else InfiniteDataLoader  # only DataLoader allows for attribute updates",
            "return loader(dataset,"
        ]
    },
    {
        "number": 381,
        "comments": "add module",
        "commit_message": "POC crypten.nn.module in plans (#3531)\n\n* parent db61f12afe51b3a216c53db80813b8d8ac378055\nauthor George Muraru <murarugeorgec@gmail.com> 1589458543 +0300\ncommitter George Muraru <murarugeorgec@gmail.com> 1592429175 +0300\n\nInitial POC for models in plans\n\n* Format linter\n\n* Flake\n\n* Add tests for OnnxModel\n\n* Add decrypt as method to overload\n\n* Add tests compared to jail\n\n* Review suggestions\n\n* Remove protobuf for Onnx and linter check\n\n* Remove TODO indent\n\n* Put wrap back\n\n* Remove OnnxModel from tests\n\n* Remove duplicated test\n\n* Put comment back\n\n* Flake8 fix\n\n* Fix test serde\n\n* Add tests to increase cov\n\n* Changes from crypten - prevent enc of shapes\n\n* Add no coverage\n\n* Remove Onnx Model\n\n* Fix syft-proto\n\n* Fix review\n\n* Fix review some more\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "if dependency_check.crypten_available:",
            "",
            "framework_packages[\"crypten\"] = crypten",
            "framework_tensors.append(crypten.mpc.MPCTensor)",
            "+    framework_tensors.append(crypten.nn.Module)",
            "+",
            "",
            "framework_tensors = tuple(framework_tensors)",
            "FrameworkTensorType = Union[framework_tensors]"
        ]
    },
    {
        "number": 384,
        "comments": "update API call for type fix",
        "commit_message": "fixing some imports etc.\n\nsome leftover from commit 44af78a09c306d764aec5985de4cf0e054316f28\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def array(object_in, dtype: Optional[str] = None, dev: Optional[str] = None):",
            "dev = default_device(dev)",
            "dtype = dtype_from_str(default_dtype(dtype, object_in))",
            "if isinstance(object_in, np.ndarray):",
            "-        return _torch.Tensor(object_in).to(dev_from_str(dev))",
            "+        return torch.Tensor(object_in).to(dev_from_str(dev))",
            "if dtype is not None:",
            "-        return _torch.tensor(object_in, dtype=dtype, device=dev_from_str(dev))",
            "-    elif isinstance(object_in, _torch.Tensor):",
            "+        return torch.tensor(object_in, dtype=dtype, device=dev_from_str(dev))",
            "+    elif isinstance(object_in, torch.Tensor):",
            "return object_in.to(dev_from_str(dev))",
            "else:",
            "-        return _torch.tensor(object_in, device=dev_from_str(dev))",
            "+        return torch.tensor(object_in, device=dev_from_str(dev))",
            "",
            "asarray = array"
        ]
    },
    {
        "number": 387,
        "comments": "add API call for resource fix",
        "commit_message": "Shubhamagarwal92 master (#1349)\n\n* SA: for #958: set torch cuda device when finding root\n\n* SA: for #958: removing root gpu hack in trainer/evaluation_loop\n\n* SA: setting torch cuda device\n\n* comment line too long\n\n* check if root gpu exists or available\n\n* Incorporating suggestions on #1094\n\n* since root gpu returns none instead of -1 for cpu\n\n* undo changes\n\n* fixed dp memory thing\n\nCo-authored-by: Shubham Agarwal <shubhamagarwal92@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Trainer(",
            "self.gpus = gpus",
            "self.data_parallel_device_ids = parse_gpu_ids(self.gpus)",
            "self.root_gpu = determine_root_gpu_device(self.data_parallel_device_ids)",
            "+        self.root_device = torch.device(\"cpu\")",
            "",
            "# tpu state flags",
            "self.use_tpu = False"
        ]
    },
    {
        "number": 390,
        "comments": "update API call for refactor fix",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def _calculate_expected_result(",
            "aggregation_op_only_probs = gumbel_dist.sample()",
            "else:",
            "# <float32>[batch_size, num_aggregation_labels - 1]",
            "-        aggregation_op_only_probs = torch.nn.functional.softmax(",
            "+        aggregation_op_only_probs = nn.functional.softmax(",
            "logits_aggregation[:, 1:] / config.aggregation_temperature, dim=-1",
            ")"
        ]
    },
    {
        "number": 394,
        "comments": "update API call for resource fix",
        "commit_message": "Fix CI issues related to cupy install (#2483)\n\n* remove any cupy install when setting up environments\n\n* revert previous changes to run on cu111 runners\n\n* fix for when no cupy is installed\n\n* remove cupy uninstall for workflows not using latest torch version\n\n* update to cu116 for inference tests\n\n* fix pip uninstall line\n\n* move python environment list to after DS install\n\n* remove cupy uninstall\n\n* re-add --forked\n\n* fix how we get cupy version (should be based on nvcc version)\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "if torch_available and torch.cuda.is_available():",
            "if rocm_major <= 4:",
            "cupy = f\"cupy-rocm-{rocm_major}-{rocm_minor}\"",
            "else:",
            "-        cupy = f\"cupy-cuda{torch.version.cuda.replace('.','')[:3]}\"",
            "+        cupy = f\"cupy-cuda{''.join(map(str,installed_cuda_version()))}\"",
            "if cupy:",
            "extras_require['1bit'].append(cupy)",
            "extras_require['1bit_mpi'].append(cupy)"
        ]
    },
    {
        "number": 396,
        "comments": "doc update",
        "commit_message": "Refactor/framework context to context (#2115)\n\n* refactor(model): rename framework_context to context\n\n* style: reformat\n\n* refactor: context format changes\n\n* test: fix tensorflow_hub test\n\n* fix: tensorflow_hub related tests\n\n* Apply suggestions from code review\n\n* fix: move detectron test sample image to local\n\n* fix: fix onnx rebase\n\n* fix: fix style\n\nCo-authored-by: Chaoyu <paranoyang@gmail.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def save(",
            "",
            "Examples::",
            "\"\"\"  # noqa",
            "-    context: t.Dict[str, t.Any] = {\"statsmodels\": statsmodels.__version__}",
            "+    context: t.Dict[str, t.Any] = {",
            "+        \"framework_name\": \"statsmodels\",",
            "+        \"pip_dependencies\": [f\"statsmodels=={_statsmodels_version}\"],",
            "+    }",
            "_model = Model.create(",
            "name,",
            "module=__name__,",
            "metadata=metadata,",
            "-        framework_context=context,",
            "+        context=context,",
            ")",
            "",
            "model.save(_model.path_of(f\"{SAVE_NAMESPACE}{PKL_EXT}\"))"
        ]
    },
    {
        "number": 397,
        "comments": "update API call for refactor fix",
        "commit_message": "Fixed bug after contrib merging\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def spatial_soft_argmax2d(",
            ">>> coords = kornia.spatial_soft_argmax2d(input, False)",
            "tensor([[[1.0000, 1.0000]]])",
            "\"\"\"",
            "-    input_soft: torch.Tensor = dsnt.spatial_softmax_2d(input, temperature)",
            "-    output: torch.Tensor = dsnt.spatial_softargmax_2d(input_soft,",
            "-                                                      normalized_coordinates)",
            "+    input_soft: torch.Tensor = spatial_softmax_2d(input, temperature)",
            "+    output: torch.Tensor = spatial_softargmax_2d(input_soft,",
            "+                                                 normalized_coordinates)",
            "return output"
        ]
    },
    {
        "number": 400,
        "comments": "test fix",
        "commit_message": "small fixes to test_frontend_function.\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def test_frontend_function(",
            "ivy.set_backend(frontend)",
            "",
            "# check for unsupported dtypes in frontend framework",
            "-    function = getattr(ivy, fn_name)",
            "+    function = getattr(ivy.functional.frontends.__dict__[frontend], fn_name)",
            "for d in input_dtypes:",
            "if d in ivy.function_unsupported_dtypes(function, None):",
            "return"
        ]
    },
    {
        "number": 401,
        "comments": "add param for resource fix",
        "commit_message": "Fixed torch.Tensor.to to support device input as a positional argument\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_torch_instance_to(",
            "frontend,",
            "):",
            "input_dtype, x, method_num_positional_args, method_all_as_kwargs_np = args_kwargs",
            "+    method_flags.num_positional_args = method_num_positional_args",
            "helpers.test_frontend_method(",
            "init_input_dtypes=input_dtype,",
            "init_all_as_kwargs_np={"
        ]
    },
    {
        "number": 407,
        "comments": "update param for type fix",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class UnigramRecall(Metric):",
            "A tensor of predictions of shape (batch_size, k, sequence_length).",
            "gold_labels : `torch.Tensor`, required.",
            "A tensor of integer class label of shape (batch_size, sequence_length).",
            "-        mask : `torch.Tensor`, optional (default = None).",
            "+        mask : `torch.BoolTensor`, optional (default = None).",
            "A masking tensor the same size as `gold_labels`.",
            "\"\"\"",
            "predictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)"
        ]
    },
    {
        "number": 408,
        "comments": "update API call for refactor fix",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "\"        # compute the gating function and one minus the gating function\\n\",",
            "\"        gate_intermediate = self.relu(self.lin_gate_z_to_hidden(z_t_1))\\n\",",
            "\"        gate = self.sigmoid(self.lin_gate_hidden_to_z(gate_intermediate))\\n\",",
            "-    \"        one_minus_gate = ng_ones(gate.size()).type_as(gate) - gate\\n\",",
            "+    \"        one_minus_gate = torch.ones(gate.size()).type_as(gate) - gate\\n\",",
            "\"        # compute the 'proposed mean'\\n\",",
            "\"        proposed_mean_intermediate = self.relu(self.lin_proposed_mean_z_to_hidden(z_t_1))\\n\",",
            "\"        proposed_mean = self.lin_proposed_mean_hidden_to_z(proposed_mean_intermediate)\\n\","
        ]
    },
    {
        "number": 410,
        "comments": "change param for math fix",
        "commit_message": "fix formulation issue in rotation_matrix_to_quaternion\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def rotation_matrix_to_quaternion(",
            "return torch.cat([qx, qy, qz, qw], dim=-1)",
            "",
            "def cond_3():",
            "-        sq = torch.sqrt(trace + 1.0 + m00 - m11 - m22) * 2.  # sq = 4 * qw.",
            "+        sq = torch.sqrt(1.0 + m00 - m11 - m22 + eps) * 2.  # sq = 4 * qw.",
            "qw = safe_zero_division(m10 - m01, sq)",
            "qx = safe_zero_division(m02 - m20, sq)",
            "qy = safe_zero_division(m12 - m21, sq)"
        ]
    },
    {
        "number": 412,
        "comments": "change API",
        "commit_message": "Speed-up warp_affine and fix bugs in RandomAffine [WIP: to add tests] (#474)\n\n* speed-up warp_affine, rotate, random_crop\n\n* added basic speed test for warp_affine\n\n* fixed centerization for random shear and bug (radians instead of degrees)\n\n* add test versus torchvision\n\n* added convert_affinematrix_to_homography function\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def crop_by_boxes(tensor, src_box, dst_box,",
            "dst_trans_src = dst_trans_src.expand(tensor.shape[0], -1, -1)",
            "",
            "bbox = _infer_bounding_box(dst_box)",
            "-    patches: torch.Tensor = warp_perspective(",
            "-        tensor, dst_trans_src, (int(bbox[0].int().data.item()), int(bbox[1].int().data.item())))",
            "+    patches: torch.Tensor = warp_affine(",
            "+        tensor, dst_trans_src[:, :2, :], (int(bbox[0].int().data.item()), int(bbox[1].int().data.item())))",
            "",
            "# return in the original shape",
            "if is_unbatched:"
        ]
    },
    {
        "number": 417,
        "comments": "update API call for version fix",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "if get_current_tower_context().is_training:",
            "wd_w = tf.train.exponential_decay(2e-4, get_global_step_var(),",
            "80000, 0.7, True)",
            "-            wd_cost = tf.mul(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')",
            "+            wd_cost = tf.multiply(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')",
            "costs.append(wd_cost)",
            "",
            "add_param_summary(('.*/W', ['histogram']))   # monitor W"
        ]
    },
    {
        "number": 418,
        "comments": "remove API call for resource fix",
        "commit_message": "More speed improvements, doc for the model, fixed improper cycling in random_partial_utterances, new color scale for UMAP\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "if __name__ == '__main__':",
            "loss_values.clear()",
            "accuracies.clear()",
            "if step % 100 == 0:",
            "-            vis.draw_projections(embeds.detach().cpu().numpy(), utterances_per_speaker, step)",
            "+            vis.draw_projections(embeds.detach().numpy(), utterances_per_speaker, step)"
        ]
    },
    {
        "number": 419,
        "comments": "change API call for type fix",
        "commit_message": "Improve reproduceability 2/3 (#1906)\n\n* [Repro] Correct reproducability\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* need better image\n\n* allow conversion from no state dict checkpoints\n\n* up\n\n* up\n\n* up\n\n* up\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* next try\n\n* up\n\n* up\n\n* better name\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\n* correct more\n\n* up\n\n* replace all torch randn\n\n* fix\n\n* correct\n\n* correct\n\n* finish\n\n* fix more\n\n* up\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class ScoreSdeVpScheduler(SchedulerMixin, ConfigMixin):",
            "x_mean = x + drift * dt",
            "",
            "# add noise",
            "-        noise = torch.randn(x.shape, layout=x.layout, generator=generator).to(x.device)",
            "+        noise = randn_tensor(x.shape, layout=x.layout, generator=generator, device=x.device, dtype=x.dtype)",
            "x = x_mean + diffusion * math.sqrt(-dt) * noise",
            "",
            "return x, x_mean"
        ]
    },
    {
        "number": 422,
        "comments": "update param for math fix",
        "commit_message": "fix #1182\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def get_keras_model():",
            "M.add(KL.Conv2D(32, 3, padding='same', activation='relu'))",
            "M.add(KL.Flatten())",
            "M.add(KL.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-5)))",
            "-        M.add(KL.Dropout(0.5))",
            "+        M.add(KL.Dropout(rate=0.5))",
            "M.add(KL.Dense(10, activation=None, kernel_regularizer=keras.regularizers.l2(1e-5)))",
            "return M"
        ]
    },
    {
        "number": 424,
        "comments": "change API call for resource fix",
        "commit_message": "adding eval lm changes for model parallel (#1113)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1113\n\nReviewed By: myleott\n\nDifferential Revision: D20670665\n\nfbshipit-source-id: 8e2846637195b7200f1f60a8421d2fe5ffab789b\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def main(parsed_args):",
            "",
            "def cli_main():",
            "parser = options.get_eval_lm_parser()",
            "+    add_distributed_training_args(parser)",
            "args = options.parse_args_and_arch(parser)",
            "-    main(args)",
            "+    distributed_utils.call_main(args, main)",
            "",
            "",
            "if __name__ == '__main__':"
        ]
    },
    {
        "number": 425,
        "comments": "add param for resource fix",
        "commit_message": "degree device bugfix\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def degree(index, num_nodes=None, dtype=None, device=None):",
            "tensor([3., 1., 1.])",
            "\"\"\"",
            "num_nodes = maybe_num_nodes(index, num_nodes)",
            "-    out = torch.zeros((num_nodes), dtype=dtype, device=device)",
            "+    out = torch.zeros((num_nodes), dtype=dtype, device=index.device)",
            "return out.scatter_add_(0, index, out.new_ones((index.size(0))))"
        ]
    },
    {
        "number": 426,
        "comments": "change condition check for version fix",
        "commit_message": "fix ctc init condition\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TransducerTasks(torch.nn.Module):",
            "if ctc_loss:",
            "self.ctc_lin = torch.nn.Linear(encoder_dim, output_dim)",
            "",
            "-            if LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\"):",
            "+            if LooseVersion(torch.__version__) > LooseVersion(\"1.0.1\"):",
            "self.ctc_loss = torch.nn.CTCLoss(",
            "blank=blank_id,",
            "reduction=\"sum\","
        ]
    },
    {
        "number": 429,
        "comments": "update API call for refactor fix",
        "commit_message": "[Feat] Initiate AutoAugment modules (#2181)\n\n* init\n\n* Added autoaugment\n\n* Added RandAugment\n\n* Added trivial augment\n\n* Added missing files\n\n* Updated docs\n\n* refactoring\n\n* Added missing files\n\n* Added shear and translate functions\n\n* update\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fixed translate bug\n\n* Added color\n\n* Added autocontrast\n\n* Fix\n\n* Fix doctest\n\n* Fixed typing\n\n* Removed legacy generators\n\n* fix\n\n* fixed cutmix float64 bug\n\n* include more test\n\n* Sequential support first commit\n\n* Added missing files\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* first refactor\n\n* Make it compatible with AugmentationSequential\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Added more tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Make ops module lazy loading\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Make policy module lazy loading\n\n* Lazy loading auto module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Make augmentation module lazy loading\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Bug fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\n* fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\n* final fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Apply suggestions from code review\n\nCo-authored-by: Jo\u00e3o Gustavo A. Amorim <joaogustavoamorim@gmail.com>\n\n* Revert review commnets\n\n* Update\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\nCo-authored-by: Jo\u00e3o Gustavo A. Amorim <joaogustavoamorim@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class RandomThinPlateSpline(AugmentationBase2D):",
            "",
            "def generate_parameters(self, shape: torch.Size) -> Dict[str, Tensor]:",
            "B, _, _, _ = shape",
            "-        src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2",
            "+        src = tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2",
            "dst = src + self.dist.rsample(src.shape)",
            "return dict(src=src, dst=dst)"
        ]
    },
    {
        "number": 432,
        "comments": "version fix",
        "commit_message": "[Pytorch] pytorch only timesteps (#724)\n\n* pytorch timesteps\n\n* style\n\n* get rid of if-else\n\n* fix test\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class PNDMScheduler(SchedulerMixin, ConfigMixin):",
            "::-1",
            "].copy()  # we copy to avoid having negative strides which are not supported by torch.from_numpy",
            "",
            "-        self.timesteps = np.concatenate([self.prk_timesteps, self.plms_timesteps]).astype(np.int64)",
            "+        timesteps = np.concatenate([self.prk_timesteps, self.plms_timesteps]).astype(np.int64)",
            "+        self.timesteps = torch.from_numpy(timesteps).to(device)",
            "",
            "self.ets = []",
            "self.counter = 0"
        ]
    },
    {
        "number": 433,
        "comments": "update API call for refactor fix",
        "commit_message": "fix homography regression example (#349)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def HomographyRegressionApp():",
            "[-1, 1],  # top-right",
            "]]).to(dst_homo_src.device)",
            "# transform points",
            "-            pts_dst = dgm.transform_points(dgm.inverse(dst_homo_src), pts_src)",
            "+            pts_dst = dgm.transform_points(torch.inverse(dst_homo_src), pts_src)",
            "",
            "def compute_factor(size):",
            "return 1.0 * size / 2"
        ]
    },
    {
        "number": 436,
        "comments": "update API call for refactor fix",
        "commit_message": "upgrade to pytorch 0.4.1 + make work with python 3.7 (but still 3.6 also) (#1543)\n\n* changes for pytorch 0.4.1\n\n* increase tolerance for srl test\n\n* update versions in setup.py\n\n* add script to check requirements.txt vs setup.py + fix setup.py\n\n* loosen bounds on pytorch version\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Highway(torch.nn.Module):",
            "# above, too.",
            "nonlinear_part, gate = projected_input.chunk(2, dim=-1)",
            "nonlinear_part = self._activation(nonlinear_part)",
            "-            gate = torch.nn.functional.sigmoid(gate)",
            "+            gate = torch.sigmoid(gate)",
            "current_input = gate * linear_part + (1 - gate) * nonlinear_part",
            "return current_input"
        ]
    },
    {
        "number": 437,
        "comments": "change param for type fix",
        "commit_message": "fix is_tensor\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Model(object):",
            "\"It should be either Tensor or a list of Tensor.\"",
            ")",
            "for idx in range(len(check_argu)):",
            "-                        if not isinstance(check_argu[idx], tf_ops._TensorLike) or not tf_ops.is_dense_tensor_like(",
            "+                        if not isinstance(check_argu[idx], [tf.Tensor, tf.SparseTensor, tf.Variable]) or not tf_ops.is_dense_tensor_like(",
            "check_argu[idx]):",
            "raise TypeError(",
            "\"The argument `%s` should be either Tensor or a list of Tensor \" % (check_order[co]) +"
        ]
    },
    {
        "number": 440,
        "comments": "format",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class Metric(Registrable):",
            "return cls.by_name(metric_type)(**params.as_dict())  # type: ignore",
            "",
            "@staticmethod",
            "-    def unwrap_to_tensors(*tensors):",
            "+    def unwrap_to_tensors(*tensors: torch.Tensor):",
            "\"\"\"",
            "-        If you actually passed in Variables to a Metric instead of Tensors, there will be",
            "+        If you actually passed gradient-tracking Tensors to a Metric, there will be",
            "a huge memory leak, because it will prevent garbage collection for the computation",
            "graph. This method ensures that you're using tensors directly and that they are on",
            "the CPU.",
            "\"\"\"",
            "-        return (x.data.cpu() if isinstance(x, torch.autograd.Variable) else x for x in tensors)",
            "+        return (x.detach().cpu() if isinstance(x, torch.Tensor) else x for x in tensors)"
        ]
    },
    {
        "number": 444,
        "comments": "format",
        "commit_message": "bugfix\n\nSummary: It seemed that even though the chamfer diff was rebased on top of the knn autograd diff, some of the final updates did not get applied. I'm really surprised that the sandcastle tests did not fail and prevent the diff from landing.\n\nReviewed By: gkioxari\n\nDifferential Revision: D21066156\n\nfbshipit-source-id: 5216efe95180c1b6082d0bac404fa1920cfb7b02\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def chamfer_distance(",
            "",
            "if return_normals:",
            "# Gather the normals using the indices and keep only value for k=0",
            "-        x_normals_near = knn_gather(y_normals, x_idx, y_lengths)[..., 0, :]",
            "-        y_normals_near = knn_gather(x_normals, y_idx, x_lengths)[..., 0, :]",
            "+        x_normals_near = knn_gather(y_normals, x_nn.idx, y_lengths)[..., 0, :]",
            "+        y_normals_near = knn_gather(x_normals, y_nn.idx, x_lengths)[..., 0, :]",
            "",
            "cham_norm_x = 1 - torch.abs(",
            "F.cosine_similarity(x_normals, x_normals_near, dim=2, eps=1e-6)"
        ]
    },
    {
        "number": 446,
        "comments": "add API call for type fix",
        "commit_message": "Unpack `dl_manager.iter_files` to allow parallization (#4625)\n\n* Unpack `dl_manager.iter_files` to allow parallization\n\n* Fix _generate_tables\n\n* Fix remaining tests\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Csv(datasets.ArrowBasedBuilder):",
            "if schema is not None",
            "else None",
            ")",
            "-        for file_idx, file in enumerate(files):",
            "+        for file_idx, file in enumerate(itertools.chain.from_iterable(files)):",
            "csv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)",
            "try:",
            "for batch_idx, df in enumerate(csv_file_reader):"
        ]
    },
    {
        "number": 448,
        "comments": "add condition check for null fix",
        "commit_message": "fix arma conv\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class ARMAConv(MessagePassing):",
            "if self.bias is not None:",
            "out += self.bias[0 if self.shared_weights else t]",
            "",
            "-            if t < self.num_layers - 1:",
            "+            if self.act is not None and t < self.num_layers - 1:",
            "out = self.act(out)",
            "",
            "return out.mean(dim=-3)"
        ]
    },
    {
        "number": 449,
        "comments": "add API call for type fix",
        "commit_message": "fix typing in dependency parser model\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DependencyParser(flair.nn.Model):",
            "sentence_tensor = self.word_dropout(sentence_tensor)",
            "",
            "if self.use_rnn:",
            "-            sentence_tensor = pack_padded_sequence(sentence_tensor, lengths, True, False)",
            "+            sentence_sequence = pack_padded_sequence(sentence_tensor, torch.IntTensor(lengths), True, False)",
            "",
            "-            sentence_tensor, _ = self.lstm(sentence_tensor)",
            "-            sentence_tensor, _ = pad_packed_sequence(sentence_tensor, True, total_length=seq_len)",
            "+            sentence_sequence, _ = self.lstm(sentence_sequence)",
            "+            sentence_tensor, _ = pad_packed_sequence(sentence_sequence, True, total_length=seq_len)",
            "",
            "# apply MLPs for arc and relations to the BiLSTM output states",
            "arc_h = self.mlp_arc_h(sentence_tensor)"
        ]
    },
    {
        "number": 451,
        "comments": "add API call for type fix",
        "commit_message": "Fix linspace error\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def testtanh():",
            "",
            "Ptensor = PolynomialTensor()",
            "",
            "-    x = torch.linspace(-3, 3, steps=10)",
            "+    x = torch.tensor(np.linspace(-3, 3, 10))",
            "expected = torch.tensor(",
            "[",
            "-3.3883e02,"
        ]
    },
    {
        "number": 452,
        "comments": "add param for resource fix",
        "commit_message": "[ci] fix 3 remaining slow GPU failures (#4584)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class BartTranslationTests(unittest.TestCase):",
            "with torch.no_grad():",
            "logits, *other_stuff = model(**self.net_input)",
            "",
            "-        expected_slice = torch.tensor([9.0078, 10.1113, 14.4787])",
            "+        expected_slice = torch.tensor([9.0078, 10.1113, 14.4787], device=torch_device)",
            "result_slice = logits[0][0][:3]",
            "self.assertTrue(torch.allclose(expected_slice, result_slice, atol=TOLERANCE))"
        ]
    },
    {
        "number": 453,
        "comments": "update API call for version fix",
        "commit_message": "Fix unit test errors in test_dc_crn_separator.py\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def test_dc_crn_separator_invalid_type():",
            "def test_dc_crn_separator_output():",
            "real = torch.rand(2, 10, 17)",
            "imag = torch.rand(2, 10, 17)",
            "-    x = ComplexTensor(real, imag) if is_torch_1_9_plus else torch.complex(real, imag)",
            "+    x = torch.complex(real, imag) if is_torch_1_9_plus else ComplexTensor(real, imag)",
            "x_lens = torch.tensor([10, 8], dtype=torch.long)",
            "",
            "for num_spk in range(1, 3):"
        ]
    },
    {
        "number": 454,
        "comments": "rename",
        "commit_message": "Fix Pylint issues\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class ModelSavingTests(unittest.TestCase):",
            "model = T.nn.DataParallel(layer)",
            "",
            "# save the model",
            "-        best_loss = save_best_model(model, None, 0, 100, OUT_PATH, 10, 1)",
            "+        save_best_model(model, None, 0, 100, OUT_PATH, 10, 1)",
            "",
            "# load the model to CPU",
            "-        model_dict = torch.load(",
            "+        model_dict = T.load(",
            "MODEL_PATH, map_location=lambda storage, loc: storage)",
            "model.load_state_dict(model_dict['model'])"
        ]
    },
    {
        "number": 457,
        "comments": "update API call for refactor fix",
        "commit_message": "upgrade detection / some v2 compat fix\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def multilevel_roi_align(features, rcnn_boxes, resolution):",
            "all_rois = tf.concat(all_rois, axis=0)  # NCHW",
            "# Unshuffle to the original order, to match the original samples",
            "level_id_perm = tf.concat(level_ids, axis=0)  # A permutation of 1~N",
            "-    level_id_invert_perm = tf.invert_permutation(level_id_perm)",
            "+    level_id_invert_perm = tf.math.invert_permutation(level_id_perm)",
            "all_rois = tf.gather(all_rois, level_id_invert_perm, name=\"output\")",
            "return all_rois"
        ]
    },
    {
        "number": 458,
        "comments": "change param for math fix",
        "commit_message": "fix bug in temperature & async\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def SoftMax(x, use_temperature=False, temperature_init=1.0):",
            ":param x: a 2D tensor",
            "\"\"\"",
            "if use_temperature:",
            "-        t = tf.get_variable('temp', [1],",
            "+        t = tf.get_variable('invtemp', [],",
            "initializer=tf.constant_initializer(1.0 / float(temperature_init)))",
            "x = x * t",
            "return tf.nn.softmax(x, name='output')"
        ]
    },
    {
        "number": 459,
        "comments": "add API call for type fix",
        "commit_message": "Potential deconv model saving fix? (#4999)\n\nAdding this cast to a tuple seems to fix this issue.\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def _preprocess_deconv_output_shape(x, shape, dim_ordering):",
            "shape = (shape[0], shape[2], shape[3], shape[1])",
            "",
            "if shape[0] is None:",
            "-        shape = (tf.shape(x)[0], ) + shape[1:]",
            "+        shape = (tf.shape(x)[0], ) + tuple(shape[1:])",
            "return shape"
        ]
    },
    {
        "number": 460,
        "comments": "test fix",
        "commit_message": "Fix doctests for `DeiT` and `TFGroupViT` (#19466)\n\n* Fix some doctests\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class TFGroupViTModel(TFGroupViTPreTrainedModel):",
            "",
            ">>> outputs = model(**inputs)",
            ">>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score",
            "-        >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities",
            "+        >>> probs = tf.math.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities",
            "```\"\"\"",
            "",
            "outputs = self.groupvit("
        ]
    },
    {
        "number": 461,
        "comments": "custom API",
        "commit_message": "Option to provide seed to random generators to ensure reproducibility (#1572)\n\n* Option to provide seed to random generators to ensure reproducibility\n\nI added small function in utilities which imports torch, numpy, python\nrandom and sets seed for all of the libraries to ensure reproducibility\nof results.\n\n* Apply recommendations from core contributors on seeding\n\n1. Moved the seeding code to another file\n2. Make deterministic as a parameter for trainer class\n3. Add assertions for seeding numpy\n4. Added warnings\n5. torch.manual_seed should be enough for seeding torch\n\n* Revert \"Apply recommendations from core contributors on seeding\"\n\nThis reverts commit a213c8e6882eec8a9e7408b9418926d2db7c5461.\n\n* Revert \"Revert \"Apply recommendations from core contributors on seeding\"\"\n\nThis reverts commit 59b2da53c62878de7aab0aa3feb3115e105eea06.\n\n* Change in test, for correct seeding\n\n* Allow seed equal to 0\n\n* Allow seed to be uint32.max\n\n* Added deterministic to benchmarks\n\n* Cuda manual seed as in benchmark seeding\n\n* Seeding should be done before model initialization\n\n* cuda manual_seed is not necessary\n\n* Fixing seed test_cpu_lbfgs\n\nOn some seeds seems like lbfgs doesn't converge.\nSo I fixed the seed during testing.\n\n* rebasing issue with old reproducibility.py\n\n* Improved documentation and ability to seed before initializing Train\nclass\n\n* Change in docs\n\n* Removed seed from trainer, update for documentation\n\n* Typo in the docs\n\n* Added seed_everything to _all_\n\n* Fixing old changes\n\n* Model initialization should be earlier then Trainer\n\n* Update pytorch_lightning/trainer/__init__.py\n\nFrom Example to testcode\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* Fixing according to the contributors suggestions\n\n* Moving horovod deterministic to Trainer class\n\n* deterministic flag affects horovod docs update\n\n* Improved static typing\n\n* Added deterministic to test runners of horovod\n\nIt is failing on some versions, not very predictable\n\n* static seeds for horovod tests\n\n* Change for reset_seed function in tests\n\n* Seeding horovod using reset_seed from tutils\n\n* Update pytorch_lightning/trainer/__init__.py\n\n* chlog\n\n* Update trainer.py\n\n* change \"testcode\" to \"Example\" in trainer init documentation\n\n* Update pytorch_lightning/trainer/seed.py, first line in comment\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Jirka <jirka.borovec@seznam.cz>\nCo-authored-by: William Falcon <waf2107@columbia.edu>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def lightning_loop(MODEL, num_runs=10, num_epochs=10):",
            "",
            "# set seed",
            "seed = i",
            "-        _set_seed(seed)",
            "+        seed_everything(seed)",
            "",
            "-        # init model parts",
            "model = MODEL()",
            "+        # init model parts",
            "trainer = Trainer(",
            "max_epochs=num_epochs,",
            "progress_bar_refresh_rate=0,",
            "weights_summary=None,",
            "gpus=1,",
            "early_stop_callback=False,",
            "-            checkpoint_callback=False",
            "+            checkpoint_callback=False,",
            "+            deterministic=True,",
            ")",
            "trainer.fit(model)"
        ]
    },
    {
        "number": 462,
        "comments": "change condition check for null fix",
        "commit_message": "rgcn bugfix\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class RGCNConv(MessagePassing):",
            "return out if edge_norm is None else out * edge_norm.view(-1, 1)",
            "",
            "def update(self, aggr_out, x):",
            "-        if x.dtype == torch.long:",
            "+        if x is None:",
            "out = aggr_out + self.root",
            "else:",
            "out = aggr_out + torch.matmul(x, self.root)"
        ]
    },
    {
        "number": 470,
        "comments": "change API call for math fix",
        "commit_message": "Merge PyG master (#48)\n\n* avoid the 'inf'\n\n* Create GATv2Conv\n\n* Update gatv2_conv.py\n\n* Update gatv2_conv.py\n\n* Update gatv2_conv.py\n\n* More doc\n\n* Update gatv2_conv.py\n\n* Update gatv2_conv.py\n\n* Update README.md\n\n* Update README.md\n\n* Create test_gatv2_conv.py\n\n* Update test_gatv2_conv.py\n\n* fixed gatv2 test\n\n* Fixed types\n\n* Update gatv2_conv.py\n\n* fix types\n\n* remove script folder\n\n* update test CI\n\n* fixed comments\n\n* lint + type\n\n* lint\n\n* Update test_gatv2_conv.py\n\n* Update test_gatv2_conv.py\n\n* fixed gatv2 test+ types\n\n* Update gatv2_conv.py\n\n* Update gatv2_conv.py\n\n* pytorch 1.9.0 support\n\n* typo\n\n* The dataset was introduced in the MUSAE paper\n\nDear Matthias,\n\nThese datasets were introduced in the Multi-scale Attributed Node Embedding paper.\n\nhttps://arxiv.org/abs/1909.13021\n\nBenedek\n\n* Github Dataset\n\n* Github Dataset\n\n* Github Dataset\n\n* Revert \"Merge branch 'master' into master\"\n\nThis reverts commit ef38f142465f736692c7c251a315ada287d7f104, reversing\nchanges made to d86de00a98173653a6158fc40238d34d0fb57cc1.\n\n* clean up\n\n* fix doc\n\n* fix gnn explainer\n\n* remove OGB-LSC\n\nCo-authored-by: Ethanzjp <13810907+Ethanzjp@users.noreply.github.com>\nCo-authored-by: shakedbr <shakedbr@campus.technion.ac.il>\nCo-authored-by: Uri Alon <urialon1@gmail.com>\nCo-authored-by: Shaked Brody <shakedbr@gmail.com>\nCo-authored-by: Benedek Rozemberczki <benedek.rozemberczki@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DenseGCNConv(torch.nn.Module):",
            "idx = torch.arange(N, dtype=torch.long, device=adj.device)",
            "adj[:, idx, idx] = 1 if not self.improved else 2",
            "",
            "-        out = self.lin(x)",
            "+        out = torch.matmul(x, self.weight)",
            "deg_inv_sqrt = adj.sum(dim=-1).clamp(min=1).pow(-0.5)",
            "",
            "adj = deg_inv_sqrt.unsqueeze(-1) * adj * deg_inv_sqrt.unsqueeze(-2)"
        ]
    },
    {
        "number": 472,
        "comments": "add condition check for state fix",
        "commit_message": "Fix half of the tests that are failing in v1.\n\nMost of them are failing since the actual code are expected to run only in v2 (eg need eager/resource variable, or certain fix we added is only applied to the v2 code path).\n\nPiperOrigin-RevId: 394765626\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class GradientsTest(tf.test.TestCase):",
            "",
            "",
            "if __name__ == \"__main__\":",
            "-  tf.test.main()",
            "+  if tf.__internal__.tf2.enabled():",
            "+    tf.test.main()"
        ]
    },
    {
        "number": 473,
        "comments": "update API call for version fix",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
            "cost = tf.reduce_mean(cost, name='cross_entropy_loss')",
            "",
            "-        acc = tf.to_float(tf.nn.in_top_k(logits, label, 1))",
            "+        acc = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32)",
            "",
            "acc = tf.reduce_mean(acc, name='accuracy')",
            "summary.add_moving_summary(acc)"
        ]
    },
    {
        "number": 475,
        "comments": "format",
        "commit_message": "fix moe override defaults\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def block(params, scope, layer_num, bias, sequence_dim, memory_length_dim, varia",
            "if use_moe:",
            "moe_params = mtf.transformer.moe.HParams()",
            "mtf.transformer.moe.set_default_moe_hparams(moe_params)",
            "+",
            "+                # override defaults",
            "for k, v in params[\"moe_params\"].items():",
            "moe_params.add_hparam(k, v)",
            "-                mtf.transformer.moe.set_default_moe_hparams(moe_params)",
            "+",
            "moe_train = params[\"mode\"] == \"train\"",
            "",
            "m, aux_loss = mtf.transformer.moe.transformer_moe_layer_v1(res_x, x.shape[-1], moe_params,"
        ]
    },
    {
        "number": 476,
        "comments": "update API call for version fix",
        "commit_message": "Fix tests of mixed precision now that experimental is deprecated (#17300)\n\n* Fix tests of mixed precision now that experimental is deprecated\n\n* Fix mixed precision in training_args_tf.py too\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFCoreModelTesterMixin:",
            "",
            "self.assertIsNotNone(outputs)",
            "",
            "-        tf.keras.mixed_precision.experimental.set_policy(\"float32\")",
            "+        tf.keras.mixed_precision.set_global_policy(\"float32\")",
            "",
            "@slow",
            "def test_train_pipeline_custom_model(self):"
        ]
    },
    {
        "number": 479,
        "comments": "add API call for type fix",
        "commit_message": "Add test to ensure models can take int64 inputs (#17210)\n\n* Add test to ensure models can take int64 inputs\n\n* is_integer is an attribute, not a method\n\n* Fix test when some inputs aren't tensors\n\n* Add casts to blenderbot and blenderbot-small\n\n* Add casts to the other failing models\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TFMBartForConditionalGeneration(TFMBartPreTrainedModel, TFCausalLanguageMo",
            "if labels is not None:",
            "labels = tf.where(",
            "labels == self.config.pad_token_id,",
            "-                tf.fill(shape_list(labels), -100),",
            "+                tf.cast(tf.fill(shape_list(labels), -100), labels.dtype),",
            "labels,",
            ")",
            "use_cache = False"
        ]
    },
    {
        "number": 480,
        "comments": "no API",
        "commit_message": "[WIP] Test TF Flaubert + Add {XLM, Flaubert}{TokenClassification, MultipleC\u2026 (#5614)\n\n* Test TF Flaubert + Add {XLM, Flaubert}{TokenClassification, MultipleChoice} models and tests\n\n* AutoModels\n\n\nTiny tweaks\n\n* Style\n\n* Final changes before merge\n\n* Re-order for simpler review\n\n* Final fixes\n\n* Addressing @sgugger's comments\n\n* Test MultipleChoice\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class TFModelTesterMixin:",
            "def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):",
            "if model_class in TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING.values():",
            "inputs_dict = {",
            "-                k: tf.tile(tf.expand_dims(v, 1), (1, self.model_tester.num_choices, 1))",
            "-                if isinstance(v, tf.Tensor) and v.ndim != 0",
            "+                k: tf.tile(tf.expand_dims(v, 1), (1, self.model_tester.num_choices) + (1,) * (v.ndim - 1))",
            "+                if isinstance(v, tf.Tensor) and v.ndim > 0",
            "else v",
            "for k, v in inputs_dict.items()",
            "}"
        ]
    },
    {
        "number": 481,
        "comments": "add condition check for resource fix",
        "commit_message": "fix n_gpu count when no_cuda flag is activated (#3077)\n\n* fix n_gpu count when no_cuda flag is activated\n\n* someone was left behind\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def main():",
            "# Setup CUDA, GPU & distributed training",
            "if args.local_rank == -1 or args.no_cuda:",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")",
            "-        args.n_gpu = torch.cuda.device_count()",
            "+        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()",
            "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs",
            "torch.cuda.set_device(args.local_rank)",
            "device = torch.device(\"cuda\", args.local_rank)"
        ]
    },
    {
        "number": 482,
        "comments": "add API call for type fix",
        "commit_message": "Unpack `dl_manager.iter_files` to allow parallization (#4625)\n\n* Unpack `dl_manager.iter_files` to allow parallization\n\n* Fix _generate_tables\n\n* Fix remaining tests\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Pandas(datasets.ArrowBasedBuilder):",
            "return pa_table",
            "",
            "def _generate_tables(self, files):",
            "-        for i, file in enumerate(files):",
            "+        for i, file in enumerate(itertools.chain.from_iterable(files)):",
            "with open(file, \"rb\") as f:",
            "pa_table = pa.Table.from_pandas(pd.read_pickle(f))",
            "yield i, self._cast_table(pa_table)"
        ]
    },
    {
        "number": 483,
        "comments": "add API call for type fix",
        "commit_message": "fix mypy errors\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DiceLoss(nn.Module):",
            "cardinality = torch.sum(input_soft + target_one_hot, dims)",
            "",
            "dice_score = 2. * intersection / (cardinality + self.eps)",
            "-        return torch.mean(1. - dice_score)",
            "+        return torch.mean(torch.tensor(1.) - dice_score)",
            "",
            "",
            "######################"
        ]
    },
    {
        "number": 488,
        "comments": "add param for state fix",
        "commit_message": "Adds timeout argument to training_args to avoid socket timeouts in DDP (#18562)\n\n* chore(training_args): Adds support for timeout argument.\n\n* fix(training_args): Passes make style through changes.\n\n* fix(training_args): Removes wrong docstring sentence.\n\n* fix(training_args): Fixes timeout not being JSON serializable.\n\n* fix(training_args_sm): Also updates timeout to timeout_delta.\n\n* fix(training_args): Fixes PR according to suggestions.\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class SageMakerTrainingArguments(TrainingArguments):",
            "# Here, we'll use torch.distributed.",
            "# Initializes the distributed backend which will take care of synchronizing nodes/GPUs",
            "if not torch.distributed.is_initialized():",
            "-                torch.distributed.init_process_group(backend=\"nccl\")",
            "+                torch.distributed.init_process_group(backend=\"nccl\", timeout=self.ddp_timeout_delta)",
            "device = torch.device(\"cuda\", self.local_rank)",
            "self._n_gpu = 1"
        ]
    },
    {
        "number": 489,
        "comments": "change param for null fix",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def main(_):",
            "# net = tl.layers.ReshapeLayer(net,",
            "#       shape=[-1, int(net.outputs._shape[-1])], name='reshape')",
            "net = tl.layers.DropoutLayer(net, keep=keep_prob, is_fix=True, is_train=is_training, name='drop3')",
            "-            net = tl.layers.DenseLayer(net, vocab_size, W_init=init, b_init=init, act=tf.identity, name='output')",
            "+            net = tl.layers.DenseLayer(net, vocab_size, W_init=init, b_init=init, act=None, name='output')",
            "return net, lstm1, lstm2",
            "",
            "# Inference for Training"
        ]
    },
    {
        "number": 494,
        "comments": "update param for type fix",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DecoderLayer(nn.Module):",
            "self.sublayer = nn_util.clone(SublayerConnection(size, dropout), 3)",
            "",
            "def forward(",
            "-        self, x: torch.Tensor, memory: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor",
            "+        self,",
            "+        x: torch.Tensor,",
            "+        memory: torch.Tensor,",
            "+        src_mask: torch.BoolTensor,",
            "+        tgt_mask: torch.BoolTensor,",
            ") -> torch.Tensor:",
            "# Follow Figure 1 (right) for connections.",
            "x = self.sublayer[0](x, lambda y: self.self_attn(y, y, y, tgt_mask))"
        ]
    },
    {
        "number": 497,
        "comments": "add condition check for resource fix",
        "commit_message": "fix Average metric (#4624)\n\n* fix Average metric\n\n* try spawn\n\n* try again\n\n* oops\n\n* clean up, fix evalb too\n\n* use different start method for GPU vs CPU tests\n\n* add comment\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class EvalbBracketingScorer(Metric):",
            "shutil.rmtree(tempdir)",
            "",
            "if is_distributed():",
            "-            # Setting the device to CPU since this metric is not expected to run on GPUs.",
            "-            device = torch.device(\"cpu\")",
            "+            device = torch.device(\"cuda\" if dist.get_backend() == \"nccl\" else \"cpu\")",
            "correct_predicted_brackets = torch.tensor(_correct_predicted_brackets).to(device)",
            "predicted_brackets = torch.tensor(_predicted_brackets).to(device)",
            "gold_brackets = torch.tensor(_gold_brackets).to(device)"
        ]
    },
    {
        "number": 501,
        "comments": "add condition check for null fix",
        "commit_message": "Bug fix for ctc mode:   espnet/nets/pytorch_backend/e2e_asr.py\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class E2E(torch.nn.Module):",
            "# Neither CPUTensor nor float/int value can be used",
            "# because NCCL communicates between GPU devices.",
            "device = next(self.parameters()).device",
            "-        acc = torch.tensor([acc], device=device)",
            "+",
            "+        acc = torch.tensor([acc], device=device) if acc is not None else None",
            "cer = torch.tensor([cer], device=device)",
            "wer = torch.tensor([wer], device=device)",
            "return self.loss, loss_ctc, loss_att, acc, cer, wer"
        ]
    },
    {
        "number": 502,
        "comments": "remove API call for type fix",
        "commit_message": "bug fixes, under construction\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class DeepQNetwork(ValueFunction):",
            "\"\"\"",
            "",
            "# Compute estimated future value",
            "-        float_terminals = tf.to_float(batch['terminals'])",
            "+        float_terminals = batch['terminals'].astype(float)",
            "q_targets = batch['rewards'] + (1. - float_terminals) \\",
            "* self.gamma * self.get_target_values(batch['next_states'])"
        ]
    },
    {
        "number": 508,
        "comments": "remove condition check for resource fix",
        "commit_message": "Bug fix\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DistributedFusedAdam(torch.optim.Optimizer):",
            "grp = torch.distributed.new_group(ranks=ranks)",
            "if torch.distributed.get_rank() in ranks:",
            "self._rs_pg.append(grp)",
            "-            if self._compute_L2_grad_norm and torch.distributed.get_rank() in ranks:",
            "-                self._l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)",
            "-                torch.distributed.all_reduce(self._overflow_buf,group=self._l2_grad_norm_pg)",
            "+            if self._compute_L2_grad_norm:",
            "+                l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)",
            "+                if torch.distributed.get_rank() in ranks:",
            "+                    self._l2_grad_norm_pg = l2_grad_norm_pg",
            "+                    torch.distributed.all_reduce(self._overflow_buf,group=self._l2_grad_norm_pg)",
            "self._rs_st = [torch.cuda.Stream() for _ in range(self._num_rs_pg)]",
            "for rs_pg in self._rs_pg:",
            "torch.distributed.all_reduce(self._overflow_buf,group=rs_pg)"
        ]
    },
    {
        "number": 510,
        "comments": "add condition check for type fix",
        "commit_message": "Remove assert statement from non-test files (#1206)\n\n* Remove assert statement from non-test files\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Autofix issues in 4 files\n\nResolved issues in the following files via DeepSource Autofix:\n1. kornia/augmentation/container/image.py\n2. kornia/augmentation/utils/helpers.py\n3. kornia/augmentation/utils/param_validation.py\n4. kornia/enhance/adjust.py\n\n* fixl linting issues\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Autofix issues in 1 file\n\nResolved issues in kornia/augmentation/container/augment.py via DeepSource Autofix\n\nCo-authored-by: deepsource-autofix[bot] <62050782+deepsource-autofix[bot]@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def cartesian_product_of_parameters(**possible_parameters):",
            "",
            "",
            "def default_with_one_parameter_changed(*, default={}, **possible_parameters):",
            "-    assert isinstance(default, dict), f\"default should be a dict not a {type(default)}\"",
            "+    if not isinstance(default, dict):",
            "+        raise AssertionError(f\"default should be a dict not a {type(default)}\")",
            "",
            "for parameter_name, possible_values in possible_parameters.items():",
            "for v in possible_values:"
        ]
    },
    {
        "number": 511,
        "comments": "rename var",
        "commit_message": "center transform, py 2.7 fixes\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def read_ply_data(path):",
            "pos = ([torch.tensor(data['vertex'][axis]) for axis in ['x', 'y', 'z']])",
            "pos = torch.stack(pos, dim=-1)",
            "",
            "-    face = data['face']['vertex_indices']",
            "-    face = [torch.tensor(f, dtype=torch.long) for f in face]",
            "-    face = torch.stack(face, dim=-1)",
            "+    faces = data['face']['vertex_indices']",
            "+    faces = [torch.tensor(face, dtype=torch.long) for face in faces]",
            "+    face = torch.stack(faces, dim=-1)",
            "",
            "edge_index = face_to_edge_index(face, num_nodes=pos.size(0))"
        ]
    },
    {
        "number": 516,
        "comments": "add API call for type fix",
        "commit_message": "Fix `get_embedding` dtype at init. time (#19473)\n\n* cast positions dtype in XGLMModel\n\n* Get the correct dtype at init time\n\n* Get the correct dtype at init time\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Speech2Text2SinusoidalPositionalEmbedding(nn.Module):",
            "emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)",
            "if padding_idx is not None:",
            "emb[padding_idx, :] = 0",
            "-        return emb",
            "+        return emb.to(torch.get_default_dtype())",
            "",
            "@torch.no_grad()",
            "def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0):"
        ]
    },
    {
        "number": 518,
        "comments": "add API call for resource fix",
        "commit_message": "MPS schedulers: don't use float64 (#1169)\n\n* Schedulers: don't use float64 on mps\n\n* Test set_timesteps() on device (float schedulers).\n\n* SD pipeline: use device in set_timesteps.\n\n* SD in-painting pipeline: use device in set_timesteps.\n\n* Tests: fix mps crashes.\n\n* Skip test_load_pipeline_from_git on mps.\n\nNot compatible with float16.\n\n* Use device.type instead of str in Euler schedulers.\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class EulerAncestralDiscreteScheduler(SchedulerMixin, ConfigMixin):",
            "",
            "prev_sample = sample + derivative * dt",
            "",
            "-        device = model_output.device if torch.is_tensor(model_output) else \"cpu\"",
            "-        if str(device) == \"mps\":",
            "+        device = model_output.device if torch.is_tensor(model_output) else torch.device(\"cpu\")",
            "+        if device.type == \"mps\":",
            "# randn does not work reproducibly on mps",
            "noise = torch.randn(model_output.shape, dtype=model_output.dtype, device=\"cpu\", generator=generator).to(",
            "device"
        ]
    },
    {
        "number": 519,
        "comments": "add API call for resource fix",
        "commit_message": "Fix AutoRegNN with default CUDA tensor (#1308)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class AutoRegressiveNN(nn.Module):",
            "",
            "if permutation is None:",
            "# By default set a random permutation of variables, which is important for performance with multiple steps",
            "-            self.permutation = torch.randperm(input_dim)",
            "+            self.permutation = torch.randperm(input_dim, device='cpu').to(torch.Tensor().device)",
            "else:",
            "# The permutation is chosen by the user",
            "self.permutation = permutation.type(dtype=torch.int64)"
        ]
    },
    {
        "number": 522,
        "comments": "change param for resource fix",
        "commit_message": "make fix copies\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class StableDiffusionInpaintPipeline(DiffusionPipeline):",
            "else:",
            "raise ImportError(\"Please install accelerate via `pip install accelerate`\")",
            "",
            "-        device = torch.device(\"cuda\")",
            "+        device = torch.device(f\"cuda:{gpu_id}\")",
            "",
            "for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.safety_checker]:",
            "if cpu_offloaded_model is not None:"
        ]
    },
    {
        "number": 524,
        "comments": "change API call for resource fix",
        "commit_message": "Add onnx convert script (#2291)\n\n* add onnx convert script\n\n* upload pytorch2onnx.py\n\n* restore config\n\n* set use_torchvion on-the-fly, update doc\n\n* update doc\n\n* support pass input\n\n* fixed passes\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TwoStageDetector(BaseDetector, RPNTestMixin, BBoxTestMixin,",
            "if self.with_rpn:",
            "rpn_outs = self.rpn_head(x)",
            "outs = outs + (rpn_outs, )",
            "-        proposals = torch.randn(1000, 4).cuda()",
            "+        proposals = torch.randn(1000, 4).to(device=img.device)",
            "# bbox head",
            "rois = bbox2roi([proposals])",
            "if self.with_bbox:"
        ]
    },
    {
        "number": 526,
        "comments": "add API call for shape fix",
        "commit_message": "fixed shape errors, now numerical instability in categorical trpo update\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class PGModel(Model):",
            "actions = np.concatenate([path['actions'] for path in batch])",
            "batch_advantage = np.concatenate([path[\"advantage\"] for path in batch])",
            "batch_advantage = zero_mean_unit_variance(batch_advantage)",
            "+        batch_advantage = np.expand_dims(batch_advantage, axis=1)",
            "states = np.concatenate([path['states'] for path in batch])",
            "",
            "return action_log_stds, action_means, actions, batch_advantage, states"
        ]
    },
    {
        "number": 529,
        "comments": "change API call for math fix",
        "commit_message": "Add Cuda tests for pyro.distributions; fix errors (#297)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Categorical(Distribution):",
            "elif one_hot:",
            "boolean_mask = x",
            "else:",
            "-                boolean_mask = torch.zeros(ps.size()).scatter_(-1, x.data.long(), 1)",
            "+                boolean_mask = torch_zeros_like(ps.data).scatter_(-1, x.data.long(), 1)",
            "# apply log function to masked probability tensor",
            "return torch.log(ps.masked_select(boolean_mask.byte()).contiguous().view(*batch_pdf_size))"
        ]
    },
    {
        "number": 530,
        "comments": "add API call for resource fixfor resource fix",
        "commit_message": "Fix CI: test_inference_for_pretraining in ViTMAEModelTest (#16591)\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class ViTMAEModelIntegrationTest(unittest.TestCase):",
            "",
            "# forward pass",
            "with torch.no_grad():",
            "-            outputs = model(**inputs, noise=torch.from_numpy(noise))",
            "+            outputs = model(**inputs, noise=torch.from_numpy(noise).to(device=torch_device))",
            "",
            "# verify the logits",
            "expected_shape = torch.Size((1, 196, 768))"
        ]
    },
    {
        "number": 532,
        "comments": "add API call for type fix",
        "commit_message": "Update nlp.py\n\nFix some bugs like https://github.com/tensorflow/tensorflow/issues/5118.\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def initialize_vocabulary(vocabulary_path):",
            "rev_vocab = []",
            "with gfile.GFile(vocabulary_path, mode=\"rb\") as f:",
            "rev_vocab.extend(f.readlines())",
            "-    rev_vocab = [line.strip() for line in rev_vocab]",
            "+    rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]",
            "vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])",
            "return vocab, rev_vocab",
            "else:"
        ]
    },
    {
        "number": 533,
        "comments": "def",
        "commit_message": "enable `disallow_incomplete_defs` on mypy (#2094)\n\n* enable `disallow_incomplete_defs` on mypy\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix `blur_pool2d` doc\n\n* finish v1: works on torch 1.13.1\n\n- Remove JIT support for Boxes3D\n\n* rip off the np typing\n\n* replace `Size` with `Tuple[int, ...]` on augs\n\n* add `Dtype` to kornia.filters.kernels\n\n* minor fix after rebase\n\n* Remove old torch from typing CI\n\n---------\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class CenterCrop(GeometricAugmentationBase2D):",
            "padding_mode=\"zeros\",",
            ")",
            "",
            "-    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, Tensor]:",
            "+    def generate_parameters(self, batch_shape: Tuple[int, ...]) -> Dict[str, Tensor]:",
            "return rg.center_crop_generator(batch_shape[0], batch_shape[-2], batch_shape[-1], self.size, self.device)",
            "",
            "def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:"
        ]
    },
    {
        "number": 534,
        "comments": "add condition check for version fix",
        "commit_message": "Fix complex support\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class ConformerSeparator(AbsSeparator):",
            "\"\"\"",
            "",
            "# if complex spectrum,",
            "-        if isinstance(input, ComplexTensor):",
            "+        if isinstance(input, ComplexTensor) or (",
            "+            is_torch_1_8_plus and torch.is_complex(input)",
            "+        ):",
            "feature = abs(input)",
            "else:",
            "feature = input"
        ]
    },
    {
        "number": 537,
        "comments": "add API call for type fix",
        "commit_message": "bug fix : cnn tensorflow backend error (#3558)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def batch_flatten(x):",
            "'''Turn a n-D tensor into a 2D tensor where",
            "the first dimension is conserved.",
            "'''",
            "-    x = tf.reshape(x, [-1, prod(shape(x)[1:])])",
            "+    x = tf.reshape(x, tf.pack([-1, prod(shape(x)[1:])]))",
            "return x"
        ]
    },
    {
        "number": 538,
        "comments": "add condition check for resource fix",
        "commit_message": "[App] Fix multi-node pytorch example CI (#15753)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def distributed_train(local_rank: int, main_address: str, main_port: int, num_no",
            "# 2. PREPARE DISTRIBUTED MODEL",
            "model = torch.nn.Linear(32, 2)",
            "device = torch.device(f\"cuda:{local_rank}\") if torch.cuda.is_available() else torch.device(\"cpu\")",
            "-    model = DistributedDataParallel(model, device_ids=[local_rank]).to(device)",
            "+    model = DistributedDataParallel(model, device_ids=[local_rank] if torch.cuda.is_available() else None).to(device)",
            "",
            "# 3. SETUP LOSS AND OPTIMIZER",
            "criterion = torch.nn.MSELoss()"
        ]
    },
    {
        "number": 543,
        "comments": "test fix",
        "commit_message": "Refac module factory + avoid etag requests for hub datasets (#2986)\n\n* refac module factory + avoid etag requests for hub datasets\n\n* fix tests\n\n* typing\n\n* fixes\n\n* prepare timeout\n\n* fix offline simulator with hugginggace_hub\n\n* add module factory tests (1/N)\n\n* add module factory test (2/N)\n\n* add data files tests (1/N)\n\n* add data fiels tests (2/N)\n\n* add data files tests (3/N)\n\n* style\n\n* docstrings\n\n* don't update counts when running tests\n\n* nump huggingface_hub\n\n* add timeouts for offline mode\n\n* minor\n\n* minor bis\n\n* install ruamel-yaml properly in the CI for windows\n\n* fix windows test\n\n* style\n\n* fix comet intensive calls patcher\n\n* warning message when loading from the master branch\n\n* style\n\n* albert's comments\n\n* remove unnecessary check\n\n* don't use master if HF_SCRIPTS_VERSION is specified\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class COMET(datasets.Metric):",
            "",
            "def _download_and_prepare(self, dl_manager):",
            "if self.config_name == \"default\":",
            "-            self.scorer = download_model(\"wmt-large-da-estimator-1719\")",
            "+            self.scorer = comet.models.download_model(\"wmt-large-da-estimator-1719\")",
            "else:",
            "-            self.scorer = download_model(self.config_name)",
            "+            self.scorer = comet.models.download_model(self.config_name)",
            "",
            "def _compute(self, sources, predictions, references, cuda=True, show_progress=False):",
            "data = {\"src\": sources, \"mt\": predictions, \"ref\": references}"
        ]
    },
    {
        "number": 545,
        "comments": "change param for shape fix",
        "commit_message": "fix a size mismatch bug in faster_rcnn\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "if __name__ == '__main__':",
            "",
            "# dataset = roiLoader(roidb, imdb.num_classes)",
            "dataset = roibatchLoader(roidb, imdb.num_classes)",
            "-  dataloader = torch.utils.data.DataLoader(dataset, batch_size=cfg.TRAIN.IMS_PER_BATCH,",
            "+  dataloader = torch.utils.data.DataLoader(dataset, batch_size=8,",
            "shuffle=False, num_workers=5)",
            "",
            "# initilize the tensor holder here."
        ]
    },
    {
        "number": 548,
        "comments": "change condition check for null fix",
        "commit_message": "Update benchmark script to add precision arg. Fix some downstream (DeiT) compat issues with latest changes. Bump version to 0.4.7\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class VisionTransformer(nn.Module):",
            "",
            "def forward(self, x):",
            "x = self.forward_features(x)",
            "-        if isinstance(x, tuple):",
            "-            x, x_dist = self.head(x[0]), self.head_dist(x[1])",
            "+        if self.head_dist is not None:",
            "+            x, x_dist = self.head(x[0]), self.head_dist(x[1])  # x must be a tuple",
            "if self.training and not torch.jit.is_scripting():",
            "# during inference, return the average of both classifier predictions",
            "return x, x_dist"
        ]
    },
    {
        "number": 549,
        "comments": "add condition check for type fix",
        "commit_message": "Wrap dataset generation function to disable autograph to fix issues with invalid tensor shapes (#2069)\n\nSigned-off-by: Travis Addair <taddair@uber.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TFKerasUtil(object):",
            "",
            "dataset = dataset.batch(batch_size).map(prep_data_tf_keras)",
            "return dataset",
            "-        return fn",
            "+        return tf.autograph.experimental.do_not_convert(fn) if _HAS_AUTOGRAPH else fn",
            "",
            "@staticmethod",
            "def get_horovod():"
        ]
    },
    {
        "number": 554,
        "comments": "change param for null fix",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "with tf.device('/cpu:0'):",
            "net = FlattenLayer(net, name='flatten')",
            "net = DenseLayer(net, 384, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d1relu')",
            "net = DenseLayer(net, 192, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d2relu')",
            "-            net = DenseLayer(net, n_units=10, act=tf.identity, W_init=W_init2, name='output')",
            "+            net = DenseLayer(net, n_units=10, act=None, W_init=W_init2, name='output')",
            "y = net.outputs",
            "",
            "ce = tl.cost.cross_entropy(y, y_, name='cost')"
        ]
    },
    {
        "number": 557,
        "comments": "change param for math fix",
        "commit_message": "Fixed bad use of ConvTranspose2D\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class up(nn.Module):",
            "if bilinear:",
            "self.up = nn.UpsamplingBilinear2d(scale_factor=2)",
            "else:",
            "-            self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)",
            "+            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)",
            "",
            "self.conv = double_conv(in_ch, out_ch)"
        ]
    },
    {
        "number": 559,
        "comments": "add API call for state fix",
        "commit_message": "[Metrics] Detach bugfix (#4313)\n\n* detach on buffer\n\n* doc update\n\n* remove file\n\n* changelog\n\n* suggestions\n\n* Update docs/source/metrics.rst\n\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\n\n* fix for 4266\n\n* Update docs/source/metrics.rst\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* Update CHANGELOG.md\n\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\nCo-authored-by: chaton <thomas@grid.ai>\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\nCo-authored-by: Ananya Harsh Jha <ananya@pytorchlightning.ai>\nCo-authored-by: Roger Shieh <sh.rog@protonmail.ch>\nCo-authored-by: Sean Naren <sean.narenthiran@gmail.com>\nCo-authored-by: Rohit Gupta <rohitgr1998@gmail.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Metric(nn.Module, ABC):",
            "Automatically calls ``update()``. Returns the metric value over inputs if ``compute_on_step`` is True.",
            "\"\"\"",
            "# add current step",
            "-        self.update(*args, **kwargs)",
            "+        with torch.no_grad():",
            "+            self.update(*args, **kwargs)",
            "self._forward_cache = None",
            "",
            "if self.compute_on_step:"
        ]
    },
    {
        "number": 560,
        "comments": "change API call for resource fix",
        "commit_message": "fixing inference to use volatile variables\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "temperature = max(args.temperature, 1e-3)",
            "with open(args.outf, 'w') as outf:",
            "for i in range(args.nwords):",
            "",
            "-        output, hidden = model(Variable(input, requires_grad=False), hidden)",
            "-        gen = torch.multinomial(output[0].data.cpu().div(temperature).exp(), 1)[0][0] # FIXME: no multinomial on GPU?",
            "+        output, hidden = model(Variable(input, volatile=True), hidden)",
            "+        gen = torch.multinomial(output[0].data.div(temperature).exp().cpu(), 1)[0][0] # FIXME: multinomial is only for CPU",
            "input.fill_(gen)",
            "word = corpus.dic.idx2word[gen]",
            "outf.write(word)"
        ]
    },
    {
        "number": 561,
        "comments": "change param for type fix",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFFunnelForMultipleChoice(TFFunnelPreTrainedModel, TFMultipleChoiceLoss):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "\"attention_mask\": tf.TensorSpec((None, None), tf.float32, name=\"attention_mask\"),",
            "-                \"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),",
            "+                \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),",
            "}",
            "]",
            ")"
        ]
    },
    {
        "number": 563,
        "comments": "not clear",
        "commit_message": "Prosody-aware Generative Spoken Language Modelling (#3063)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/3063\n\nReviewed By: eugene-kharitonov\n\nDifferential Revision: D34323605\n\nPulled By: wnhsu\n\nfbshipit-source-id: 9dc779a6c399cda710863596e0880b9277ff2919\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class CodeGenerator(Generator):",
            "x = torch.cat([x, spkr], dim=1)",
            "",
            "for k, feat in kwargs.items():",
            "-            if k in [\"spkr\", \"code\", \"dur_prediction\"]:",
            "+            if k in [\"spkr\", \"code\", \"f0\", \"dur_prediction\"]:",
            "continue",
            "",
            "feat = self._upsample(feat, x.shape[-1])"
        ]
    },
    {
        "number": 566,
        "comments": "change condition check for null fix",
        "commit_message": "Fix horovod training; import pyarrow without torch\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class ModelSaver(Callback):",
            "self.var_collections = var_collections",
            "if checkpoint_dir is None:",
            "checkpoint_dir = logger.get_logger_dir()",
            "-        assert checkpoint_dir is not None",
            "-        if not tf.gfile.IsDirectory(checkpoint_dir):",
            "-            tf.gfile.MakeDirs(checkpoint_dir)",
            "+        if checkpoint_dir is not None:",
            "+            if not tf.gfile.IsDirectory(checkpoint_dir):",
            "+                tf.gfile.MakeDirs(checkpoint_dir)",
            "self.checkpoint_dir = checkpoint_dir",
            "",
            "def _setup_graph(self):",
            "+        assert self.checkpoint_dir is not None, \\",
            "+            \"ModelSaver() doesn't have a valid checkpoint directory.\"",
            "vars = []",
            "for key in self.var_collections:",
            "vars.extend(tf.get_collection(key))"
        ]
    },
    {
        "number": 569,
        "comments": "add condition check for resource fix",
        "commit_message": "PyTorch 1.7 Stable support (#3821)\n\n* prepare for 1.7 support [ci skip]\n\n* tpu [ci skip]\n\n* test run 1.7\n\n* all 1.7, needs to fix tests\n\n* couple with torchvision\n\n* windows try\n\n* remove windows\n\n* 1.7 is here\n\n* on purpose fail [ci skip]\n\n* return [ci skip]\n\n* 1.7 docker\n\n* back to normal [ci skip]\n\n* change to some_val [ci skip]\n\n* add seed [ci skip]\n\n* 4 places [ci skip]\n\n* fail on purpose [ci skip]\n\n* verbose=True [ci skip]\n\n* use filename to track\n\n* use filename to track\n\n* monitor epoch + changelog\n\n* Update tests/checkpointing/test_model_checkpoint.py\n\nCo-authored-by: Rohit Gupta <rohitgr1998@gmail.com>\n\nCo-authored-by: Sean Naren <sean.narenthiran@gmail.com>\nCo-authored-by: Rohit Gupta <rohitgr1998@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def setup_ddp(rank, world_size):",
            "os.environ[\"MASTER_ADDR\"] = 'localhost'",
            "os.environ['MASTER_PORT'] = '8088'",
            "",
            "-    if torch.distributed.is_available():",
            "+    if torch.distributed.is_available() and sys.platform not in ['win32', 'cygwin']:",
            "torch.distributed.init_process_group(\"gloo\", rank=rank, world_size=world_size)"
        ]
    },
    {
        "number": 571,
        "comments": "change condition check for type fix",
        "commit_message": "[Feat] Cleanup ModelCheckpoint / EarlyStopping by moving logic to LoggerConnector (#5218)\n\n* [bug-fix] Metric reduction with Logging (#5150)\n\n* add test\n\n* resolve bug\n\n* udpate test\n\n* wrongly copy / paste\n\n* update test\n\n* resolve a second bug\n\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-62-109.ec2.internal>\n\n* iupdate\n\n* resolve bugs\n\n* add test back\n\n* correct flake8\n\n* resolve flake8\n\n* update on comments\n\n* update tests\n\n* add a test\n\n* add test\n\n* update to Callable\n\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-62-109.ec2.internal>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class ModelCheckpoint(Callback):",
            "self.best_k_models.pop(del_filepath)",
            "",
            "# do not save nan, replace with +/- inf",
            "-        if torch.isnan(current):",
            "+        if isinstance(current, torch.Tensor) and torch.isnan(current):",
            "current = torch.tensor(float('inf' if self.mode == \"min\" else '-inf'))",
            "",
            "filepath = self._get_metric_interpolated_filepath_name(ckpt_name_metrics, epoch, step, del_filepath)"
        ]
    },
    {
        "number": 574,
        "comments": "remove API call for resource fix",
        "commit_message": "bug fix for distributed strategy (#1285)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Graph(kerastuner.HyperModel, serializable.Serializable):",
            "",
            "def build(self, hp):",
            "\"\"\"Build the HyperModel into a Keras Model.\"\"\"",
            "-        tf.keras.backend.clear_session()",
            "self._register_hps(hp)",
            "self.compile()",
            "real_nodes = {}"
        ]
    },
    {
        "number": 575,
        "comments": "change param for shape fix",
        "commit_message": "Add support for HuggingFace's TensorFlow models (#127)\n\n* added support for for HuggingFace's TensorFlow models\n\n* added notebook for HuggingFace's tensorflow bert model\n\n* change nebullvm name in logs\n\n* Add optimized model details + warning if static shape is used for HF models (#1)\n\n* add optimized model type info\n\n* fix tvm issue\n\n* edit dockerfile and add image auto building\n\n* add docker installation on azure pipeline\n\n* fix bug in neural compressor output shape\n\n* add support for openvino with python 3.10\n\n* add build docker image to azure pipelines\n\n* revert docker build from az pipelines and edit format of the optimization results\n\n* revert docker build from az pipelines\n\n* added tabulate to setup.py and general fixes\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TensorflowONNXTensorRTInferenceLearner(",
            "else None",
            ")",
            "out_arrays = self._predict_array(cuda_input_arrays, input_shapes)",
            "-        return tuple(tf.convert_to_tensor(array) for array in out_arrays)",
            "+        return tuple(tf.convert_to_tensor(array[0]) for array in out_arrays)",
            "",
            "",
            "class NumpyONNXTensorRTInferenceLearner("
        ]
    },
    {
        "number": 578,
        "comments": "add param for math fix",
        "commit_message": "Cleanup the usage of `layer_norm_eps` in some models (#21336)\n\n* fix\n\n* fix\n\n* make style\n\n* For CLIP\n\n* For OwlViT\n\n* For XCLIP\n\n* For CLIPSeg\n\n* For GroupViT\n\n* fix docstrings\n\n* fix docstrings\n\n* For AltCLIP\n\n* For ChineseCLIP\n\n* For Blip\n\n* For GiT\n\n* make style\n\n* update\n\n* update\n\n* update\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class GroupViTVisionTransformer(nn.Module):",
            "",
            "self.embeddings = GroupViTVisionEmbeddings(config)",
            "self.encoder = GroupViTVisionEncoder(config)",
            "-        self.layernorm = nn.LayerNorm(embed_dim)",
            "+        self.layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)",
            "",
            "@add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)",
            "@replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTVisionConfig)"
        ]
    },
    {
        "number": 585,
        "comments": "change param for type fix",
        "commit_message": "Fix hooks test\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TestTrainSampleHook(tf.test.TestCase):",
            "pred_dict = {}",
            "pred_dict[\"predicted_tokens\"] = tf.constant([[\"Hello\", \"World\", \"\u7b11w\"]])",
            "pred_dict[\"labels.target_tokens\"] = tf.constant([[\"Hello\", \"World\", \"\u7b11w\"]])",
            "-    pred_dict[\"labels.target_len\"] = tf.constant([2]),",
            "+    pred_dict[\"labels.target_len\"] = tf.constant(2),",
            "graph_utils.add_dict_to_collection(pred_dict, \"predictions\")",
            "",
            "def tearDown(self):"
        ]
    },
    {
        "number": 586,
        "comments": "format",
        "commit_message": "fixing float_power promotion issue in backends and numpy frontend (#10332)\n\nFixed the promotion of the function but as the torch backend only is directed to return float64 and complex128 will add dtype option to the ivy function to govern the type of return we want from the function this will permit to produce proper results in the frontends\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def xlogy(",
            "return torch.xlogy(x, y, out=out)",
            "",
            "",
            "-def real(",
            "-    x: Union[torch.Tensor], /, *, out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:",
            "+def real(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "return torch.real(x)"
        ]
    },
    {
        "number": 587,
        "comments": "change API call for type fix",
        "commit_message": "Fixes to enable dev branch to move to pytorch-1.2 (#2001)\n\n* Fixes to enable dev branch to move to pytorch-1.2\n\n* change name for published cpu wheels\n\n* fix failing unit tests\n\n* change docs conf\n\n* change tracking example\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class VonMises(TorchDistribution):",
            "\"\"\"",
            "shape = self._extended_shape(sample_shape)",
            "x = torch.empty(shape, dtype=self.loc.dtype, device=self.loc.device)",
            "-        done = torch.zeros(shape, dtype=self.loc.dtype, device=self.loc.device).byte()",
            "+        done = torch.zeros(shape, dtype=self.loc.dtype, device=self.loc.device).bool()",
            "while not done.all():",
            "u = torch.rand((3,) + shape, dtype=self.loc.dtype, device=self.loc.device)",
            "u1, u2, u3 = u.unbind()"
        ]
    },
    {
        "number": 588,
        "comments": "change API",
        "commit_message": "bugfix\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class GridTest(TestCase):",
            "assert_equal(adj.to_dense().numpy(), expected_adj)",
            "",
            "def test_grid_with_connectivity_8(self):",
            "-        adj = grid(torch.Size([3, 2]), connectivity=8)",
            "+        adj = grid_3x3(torch.Size([3, 2]), connectivity=8)",
            "",
            "expected_adj = [",
            "[0, 1, 1, 2, 0, 0],"
        ]
    },
    {
        "number": 591,
        "comments": "update API call for version fix",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Model(ModelDesc):",
            "self.last_state = tf.identity(last_state, 'last_state')",
            "",
            "# seqlen x (Bxrnnsize)",
            "-        output = tf.reshape(tf.concat(1, outputs), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize",
            "+        output = tf.reshape(tf.concat_v2(outputs, 1), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize",
            "logits = FullyConnected('fc', output, param.vocab_size, nl=tf.identity)",
            "self.prob = tf.nn.softmax(logits / param.softmax_temprature)"
        ]
    },
    {
        "number": 595,
        "comments": "change API call for math fix",
        "commit_message": "RANSAC improvements (#1435)\n\n- Added LU solver for RANSAC-homography, it speeds-up the minimal-solver part x10\n- Now find_fundamental weights is optional, as it was for homography\n- Fix test precision and made one xFail test to be compulsory\n- Fixed tests for fundamental matrix, as one have to have >= 8 points (also added to docstring)\n- updates deprecated torch.svd to torch.linalg.svd\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def fit_line(points: Tensor, weights: Optional[Tensor] = None) -> ParametrizedLi",
            "A = A.transpose(-2, -1) @ A",
            "",
            "# NOTE: not optimal for 2d points, but for now works for other dimensions",
            "-    _, _, V = torch.linalg.svd(A)",
            "+    _, _, V = _torch_svd_cast(A)",
            "+    V = V.transpose(-2, -1)",
            "",
            "# the first left eigenvector is the direction on the fited line",
            "direction = V[..., 0, :]  # BxD"
        ]
    },
    {
        "number": 596,
        "comments": "change API call for shape fix",
        "commit_message": "Bug fix when target is a SparseTensor. (#4200)\n\n* Bug fix when target is a SparseTensor.\nCheck for sparsity when creating target placeholder.\nRemove shape argument when creating sparse placeholder.\n\n* Fixed ndim behavior for sparse tensor\n\n* Fix sparse variable instantiation.\n\n* Bug fix\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def ndim(x):",
            "'''Returns the number of axes in a tensor, as an integer.",
            "'''",
            "if is_sparse(x):",
            "-        return int(x.shape.get_shape()[0])",
            "+        return x._dims",
            "",
            "dims = x.get_shape()._dims",
            "if dims is not None:"
        ]
    },
    {
        "number": 599,
        "comments": "add condition check for null fix",
        "commit_message": "[Ray Dataset] fix the type infer of `pd.dataframe` (when dtype is `object`.)  (#25563)\n\nthis is a temp fix of #25556. When the dtype from the pandas dataframe gives object, we set the dtype to be None and make use of the auto-inferring of the type in the conversion.\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def convert_pandas_to_tf_tensor(",
            "# them. If the columns contain different types (for example, `float32`s",
            "# and `int32`s), then `tf.concat` raises an error.",
            "dtype: np.dtype = np.find_common_type(df.dtypes, [])",
            "+",
            "+            # if the columns are `ray.data.extensions.tensor_extension.TensorArray`,",
            "+            # the dtype will be `object`. In this case, we need to set the dtype to",
            "+            # none, and use the automatic type casting of `tf.convert_to_tensor`.",
            "+            if isinstance(dtype, object):",
            "+                dtype = None",
            "+",
            "except TypeError:",
            "# `find_common_type` fails if a series has `TensorDtype`. In this case,",
            "# don't cast any of the series and continue."
        ]
    },
    {
        "number": 600,
        "comments": "add param for argument fix",
        "commit_message": "fix: add missing symbolink of data_prep and reset GroupNorm initialization\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def initialize(model: torch.nn.Module, init: str):",
            "",
            "# reset some modules with default init",
            "for m in model.modules():",
            "-            if isinstance(m, (torch.nn.Embedding, torch.nn.LayerNorm)):",
            "+            if isinstance(m, (torch.nn.Embedding, torch.nn.LayerNorm, torch.nn.GroupNorm)):",
            "m.reset_parameters()",
            "if hasattr(m, \"espnet_initialization_fn\"):",
            "m.espnet_initialization_fn()"
        ]
    },
    {
        "number": 601,
        "comments": "update API call for refactor fix",
        "commit_message": "Change rnn-cell to fix #103 (#104)\n\n* Change rnn-cell to fix #103\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "input_list = tf.unstack(input_feature, axis=1)  # seqlen x (Bxrnnsize)",
            "",
            "# seqlen is 1 in inference. don't need loop_function",
            "-        outputs, last_state = tf.nn.rnn(cell, input_list, initial, scope='rnnlm')",
            "+        outputs, last_state = tf.contrib.rnn.static_rnn(cell, input_list, initial, scope='rnnlm')",
            "self.last_state = tf.identity(last_state, 'last_state')",
            "",
            "# seqlen x (Bxrnnsize)"
        ]
    },
    {
        "number": 606,
        "comments": "add param for argument fix",
        "commit_message": "bugfix VirtualWorker\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def tests_worker_convenience_methods():",
            "\"\"\"",
            "",
            "me = sy.torch.hook.local_worker",
            "-    bob = VirtualWorker()",
            "-    alice = VirtualWorker()",
            "+    bob = VirtualWorker(sy.torch.hook)",
            "+    alice = VirtualWorker(sy.torch.hook)",
            "obj = torch.Tensor([100, 100])",
            "",
            "# Send data to alice"
        ]
    },
    {
        "number": 609,
        "comments": "refactor",
        "commit_message": "fix multi-singer duration predictor\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class DurationPredictor(torch.nn.Module):",
            "self.norm_2 = LayerNorm(filter_channels, dim=1)",
            "self.proj = torch.nn.Conv1d(filter_channels, 1, 1)",
            "",
            "-        if gin_channels != 0:",
            "-            self.cond = torch.nn.Conv1d(gin_channels, channels, 1)",
            "+        if global_channels > 0:",
            "+            self.cond = torch.nn.Conv1d(global_channels, channels, 1)",
            "",
            "def forward(self, x, x_mask, beat_lab, g=None):",
            "x = torch.detach(x)"
        ]
    },
    {
        "number": 610,
        "comments": "add API call for type fix",
        "commit_message": "fix dtype, device in `sum`, `prod`, `to_dev` (#1358)\n\n* fix dtype, device in `sum`, `prod`, `to_dev`\n\n* make `device` have `None` as default\n\n* add `dtype = ivy.as_native_dtype(dtype)` and make `copy` positional\n\n* `to_dev` conform to array API\n\n* `astype` fixes to signature\n\n* black\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def prod(",
            "elif x.dtype == torch.bfloat16:",
            "dtype = torch.float16",
            "",
            "+    dtype = ivy.as_native_dtype(dtype)",
            "+",
            "if axis is None:",
            "axis = x.dim() - 1",
            "elif type(axis) == tuple:"
        ]
    },
    {
        "number": 611,
        "comments": "remove API call for math fix",
        "commit_message": "change the bitwise for masking and small fixes\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Attention(nn.Module):",
            "query, processed_inputs)",
            "# apply masking",
            "if mask is not None:",
            "-            attention.data.masked_fill_(torch.bitwise_not(mask), self._mask_value)",
            "+            attention.data.masked_fill_(~mask, self._mask_value)",
            "# apply windowing - only in eval mode",
            "if not self.training and self.windowing:",
            "attention = self.apply_windowing(attention, inputs)"
        ]
    },
    {
        "number": 613,
        "comments": "add API call for resource fix",
        "commit_message": "Fix(Early Stopping): move best score to device (#7959)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class EarlyStopping(Callback):",
            "f\" {self.monitor} = {current} {self.order_dict[self.mode]} {self.divergence_threshold}.\"",
            "\" Signaling Trainer to stop.\"",
            ")",
            "-        elif self.monitor_op(current - self.min_delta, self.best_score):",
            "+        elif self.monitor_op(current - self.min_delta, self.best_score.to(trainer.lightning_module.device)):",
            "should_stop = False",
            "reason = self._improvement_message(current)",
            "self.best_score = current"
        ]
    },
    {
        "number": 614,
        "comments": "update API call for refactor fix",
        "commit_message": "Replaced discontinued rnn_cell.BasicLSTMCell with rnn_cell.LSTMCell (#4703)\n\n* Fixed bug in Dirichlet (#4440)\n\n* Replaced deprecated rnn_cell.BasicLSTMCell with rnn_cell.LSTMCell\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class LSTM(Model):",
            "last_layer = add_time_dimension(features, self.seq_lens)",
            "",
            "# Setup the LSTM cell",
            "-        lstm = rnn.BasicLSTMCell(cell_size, state_is_tuple=True)",
            "+        lstm = tf.nn.rnn_cell.LSTMCell(cell_size, state_is_tuple=True)",
            "self.state_init = [",
            "np.zeros(lstm.state_size.c, np.float32),",
            "np.zeros(lstm.state_size.h, np.float32)"
        ]
    },
    {
        "number": 615,
        "comments": "add API call for type fix",
        "commit_message": "Fx support for multiple model architectures (#17393)\n\n* Support for Bart and LayoutLM, and partial support for XLNet\n\n* Support for mbart\n\n* A lot of new models supported\n\n* Support for other models\n\n* LayoutLM fix\n\n* Use strings instead of classes\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class XGLMModel(XGLMPreTrainedModel):",
            "",
            "hidden_states = inputs_embeds + positions",
            "",
            "-        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)",
            "+        hidden_states = nn.functional.dropout(hidden_states, p=float(self.dropout), training=self.training)",
            "",
            "# decoder layers",
            "all_hidden_states = () if output_hidden_states else None"
        ]
    },
    {
        "number": 619,
        "comments": "add condition check for math fix",
        "commit_message": "fix unconditional after simplification of text mask\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Imagen(nn.Module):",
            "text_embeds, text_masks = t5_encode_text(texts, name = self.text_encoder_name, return_attn_mask = True)",
            "text_embeds, text_masks = map(lambda t: t.to(images.device), (text_embeds, text_masks))",
            "",
            "-        text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))",
            "+        if not self.unconditional:",
            "+            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))",
            "",
            "assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into decoder if specified'",
            "assert not (not self.condition_on_text and exists(text_embeds)), 'decoder specified not to be conditioned on text, yet it is presented'"
        ]
    },
    {
        "number": 621,
        "comments": "change API call for refactor fix",
        "commit_message": "Update to PyTorch 1.11.0 (#3045)\n\n* Run black\n\n* Fix ProvenanceTensor\n\n* Replace torch.triangular_solve -> torch.linalg.solve_triangular\n\n* More fixes to torch.linalg.solve_triangular\n\n* Bump torch version\n\n* Bump Python version 3.6 -> 3.7\n\n* Fix some tests\n\n* Decrease tolerance on stable tests\n\n* Remove obsolete xfail\n\n* Resolve #3032\n\n* Fix solve_triangular in ops.gaussian, revert test weakening\n\n* Fix catching of singular matrices in hmc\n\n* xfail funsor tests\n\n* Work around pandas 1.3 bug\n\n* Allow mypy to install missing stubs\n\n* Clarify triangular_solve test\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def conditional(",
            "if f_scale_tril is not None:",
            "pack = torch.cat((pack, f_scale_tril_2D), dim=1)",
            "",
            "-        Lffinv_pack = pack.triangular_solve(Lff, upper=False)[0]",
            "+        Lffinv_pack = torch.linalg.solve_triangular(Lff, pack, upper=False)",
            "# unpack",
            "v_2D = Lffinv_pack[:, : f_loc_2D.size(1)]",
            "W = Lffinv_pack[:, f_loc_2D.size(1) : f_loc_2D.size(1) + M].t()"
        ]
    },
    {
        "number": 623,
        "comments": "update API call for version fix",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Model(ModelDesc):",
            "wrong = prediction_incorrect(logits, label, 5, name='wrong-top5')",
            "add_moving_summary(tf.reduce_mean(wrong, name='train-error-top5'))",
            "",
            "-        wd_cost = tf.mul(1e-4, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss')",
            "+        wd_cost = regularize_cost('.*/W', l2_regularizer(1e-4), name='l2_regularize_loss')",
            "add_moving_summary(loss, wd_cost)",
            "self.cost = tf.add_n([loss, wd_cost], name='cost')"
        ]
    },
    {
        "number": 627,
        "comments": "update API call for version fix",
        "commit_message": "Fix nccl package location on newer TF versions.\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Optimizer:",
            "g = [dev_grads[dev][var_idx][0] for dev in devices]",
            "",
            "if np.prod(grad_shape):  # nccl does not support zero-sized tensors",
            "-                            g = tf.contrib.nccl.all_sum(g)",
            "+                            g = nccl_ops.all_sum(g)",
            "",
            "for dev, gg in zip(devices, g):",
            "dev_grads[dev][var_idx] = (gg, dev_grads[dev][var_idx][1])"
        ]
    },
    {
        "number": 633,
        "comments": "change param for type fix",
        "commit_message": "fix\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Decoder(torch.nn.Module, ScorerInterface):",
            "",
            "if self.labeldist is not None:",
            "if self.vlabeldist is None:",
            "-                self.vlabeldist = to_device(hs_pad, torch.from_numpy(self.labeldist))",
            "+                self.vlabeldist = to_device(hs_pad[0], torch.from_numpy(self.labeldist))",
            "loss_reg = -torch.sum(",
            "(F.log_softmax(y_all, dim=1) * self.vlabeldist).view(-1), dim=0",
            ") / len(ys_in)"
        ]
    },
    {
        "number": 635,
        "comments": "change param for math fix",
        "commit_message": "Fix Transformer init_weights\n\nWeights of decoders are initialized twice. I think we need to call the zeros_() to initialize bias.\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TransformerModel(nn.Module):",
            "def init_weights(self):",
            "initrange = 0.1",
            "nn.init.uniform_(self.encoder.weight, -initrange, initrange)",
            "-        nn.init.zeros_(self.decoder.weight)",
            "+        nn.init.zeros_(self.decoder.bias)",
            "nn.init.uniform_(self.decoder.weight, -initrange, initrange)",
            "",
            "def forward(self, src, has_mask=True):"
        ]
    },
    {
        "number": 637,
        "comments": "change param for type fix",
        "commit_message": "CrypTen Message Handler (#3676)\n\n* Fix plans framework\n\n* CrypTen Message Handler\n\n* Test fixup\n\n* Black fixup\n\n* Remove unused/undefined functions\n\n* Move message registration to worker level\n\n* Lint\n\n* Use global msgpack\n\n* factorize worker_id to rank translation\n\n* fix sequential model with last version of crypten\n\nCo-authored-by: youben11 <ayouben9@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "import syft",
            "def model():",
            "l_in, l_h, l_out = 32, 16, 2",
            "model = crypten.nn.Sequential(",
            "-        [crypten.nn.Linear(l_in, l_h), crypten.nn.ReLU(), crypten.nn.Linear(l_h, l_out)]",
            "+        crypten.nn.Linear(l_in, l_h), crypten.nn.ReLU(), crypten.nn.Linear(l_h, l_out)",
            ")",
            "return model"
        ]
    },
    {
        "number": 639,
        "comments": "add API call for state fix",
        "commit_message": "Added functionality for retrieving variables from control dependencies (#220)\n\n* Added test for retriving variables from an optimizer\n\n* Added comments to test\n\n* Addressed comments\n\n* Fixed travis bug\n\n* Added fix to circular controls\n\n* Added set for explored operations and duplicate prefix stripping\n\n* Removed embeded ipython\n\n* Removed prefix, use seperate graph for each network\n\n* Removed redundant imports\n\n* Addressed comments and added separate graph to initializer\n\n* fix typos\n\n* get rid of prefix in documentation\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class LinearModel(object):",
            "return self.sess.run(self.cross_entropy_grads, feed_dict={self.x: xs, self.y_: ys})",
            "",
            "def net_initialization():",
            "-  return LinearModel([784,10])",
            "+  with tf.Graph().as_default():",
            "+    return LinearModel([784,10])",
            "",
            "# By default, when an environment variable is used by a remote function, the",
            "# initialization code will be rerun at the end of the remote task to ensure"
        ]
    },
    {
        "number": 640,
        "comments": "refacotr",
        "commit_message": "Fix for incorrect usage of detach(), cpu(), to() (#6216)\n\n* Fix for incorrect detach/cpu calls (#6214)\n\n* Fix incorrect use of detach(), to(), and cpu(), #6214\n\n* Fix incorrect use of detach() and cpu(), #6214\n\n* update pr\n\n* add typing\n\n* chlog\n\n* more...\n\n* revert on module\n\n* update on comments\n\n* revert changes on model\n\nCo-authored-by: tchaton <thomas@grid.ai>\nCo-authored-by: Jirka Borovec <jirka.borovec@seznam.cz>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class EpochResultStore:",
            "# attach capture batch_size",
            "Result.attach_batch_size(self._batch_size, hook_result)",
            "",
            "-            hook_result.detach()",
            "+            hook_result = hook_result.detach()",
            "if self.trainer.move_metrics_to_cpu:",
            "-                hook_result.cpu()",
            "+                hook_result = hook_result.cpu()",
            "elif self.trainer._distrib_type == DistributedType.DP:",
            "-                hook_result.to(torch.device(\"cuda\", self.trainer.root_gpu))",
            "+                hook_result = hook_result.to(torch.device(\"cuda\", self.trainer.root_gpu))",
            "",
            "self._internals[fx_name].append(hook_result, info)"
        ]
    },
    {
        "number": 641,
        "comments": "add param for argument fix",
        "commit_message": "Fix issue #2026\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class LinearRegression(d2l.Module):",
            "def __init__(self, lr):",
            "super().__init__()",
            "self.save_hyperparameters()",
            "-        self.net = tf.keras.layers.Dense(1)",
            "+        initializer = tf.initializers.RandomNormal(stddev=0.01)",
            "+        self.net = tf.keras.layers.Dense(1, kernel_initializer=initializer)",
            "",
            "def forward(self, X):",
            "\"\"\"The linear regression model."
        ]
    },
    {
        "number": 642,
        "comments": "add param for version fix",
        "commit_message": "[RLlib] SAC add discrete action support. (#7320)\n\n* Exploration API (+EpsilonGreedy sub-class).\n\n* Exploration API (+EpsilonGreedy sub-class).\n\n* Cleanup/LINT.\n\n* Add `deterministic` to generic Trainer config (NOTE: this is still ignored by most Agents).\n\n* Add `error` option to deprecation_warning().\n\n* WIP.\n\n* Bug fix: Get exploration-info for tf framework.\nBug fix: Properly deprecate some DQN config keys.\n\n* WIP.\n\n* LINT.\n\n* WIP.\n\n* Split PerWorkerEpsilonGreedy out of EpsilonGreedy.\nDocstrings.\n\n* Fix bug in sampler.py in case Policy has self.exploration = None\n\n* Update rllib/agents/dqn/dqn.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* Update rllib/agents/trainer.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* Change requests.\n\n* LINT\n\n* In tune/utils/util.py::deep_update() Only keep deep_updat'ing if both original and value are dicts. If value is not a dict, set\n\n* Completely obsolete syn_replay_optimizer.py's parameters schedule_max_timesteps AND beta_annealing_fraction (replaced with prioritized_replay_beta_annealing_timesteps).\n\n* Update rllib/evaluation/worker_set.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Review fixes.\n\n* Fix default value for DQN's exploration spec.\n\n* LINT\n\n* Fix recursion bug (wrong parent c'tor).\n\n* Do not pass timestep to get_exploration_info.\n\n* Update tf_policy.py\n\n* Fix some remaining issues with test cases and remove more deprecated DQN/APEX exploration configs.\n\n* Bug fix tf-action-dist\n\n* DDPG incompatibility bug fix with new DQN exploration handling (which is imported by DDPG).\n\n* Switch off exploration when getting action probs from off-policy-estimator's policy.\n\n* LINT\n\n* Fix test_checkpoint_restore.py.\n\n* Deprecate all SAC exploration (unused) configs.\n\n* Properly use `model.last_output()` everywhere. Instead of `model._last_output`.\n\n* WIP.\n\n* Take out set_epsilon from multi-agent-env test (not needed, decays anyway).\n\n* WIP.\n\n* Trigger re-test (flaky checkpoint-restore test).\n\n* WIP.\n\n* WIP.\n\n* Add test case for deterministic action sampling in PPO.\n\n* bug fix.\n\n* Added deterministic test cases for different Agents.\n\n* Fix problem with TupleActions in dynamic-tf-policy.\n\n* Separate supported_spaces tests so they can be run separately for easier debugging.\n\n* LINT.\n\n* Fix autoregressive_action_dist.py test case.\n\n* Re-test.\n\n* Fix.\n\n* Remove duplicate py_test rule from bazel.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* SAC fix.\n\n* SAC fix.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* FIX 2 examples tests.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* Renamed test file.\n\n* WIP.\n\n* Add unittest.main.\n\n* Make action_dist_class mandatory.\n\n* fix\n\n* FIX.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix explorations test case (contextlib cannot find its own nullcontext??).\n\n* Force torch to be installed for QMIX.\n\n* LINT.\n\n* Fix determine_tests_to_run.py.\n\n* Fix determine_tests_to_run.py.\n\n* WIP\n\n* Add Random exploration component to tests (fixed issue with \"static-graph randomness\" via py_function).\n\n* Add Random exploration component to tests (fixed issue with \"static-graph randomness\" via py_function).\n\n* Rename some stuff.\n\n* Rename some stuff.\n\n* WIP.\n\n* update.\n\n* WIP.\n\n* Gumbel Softmax Dist.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP\n\n* WIP.\n\n* WIP.\n\n* Hypertune.\n\n* Hypertune.\n\n* Hypertune.\n\n* Lock-in.\n\n* Cleanup.\n\n* LINT.\n\n* Fix.\n\n* Update rllib/policy/eager_tf_policy.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Update rllib/agents/sac/sac_policy.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Update rllib/agents/sac/sac_policy.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Update rllib/models/tf/tf_action_dist.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Update rllib/models/tf/tf_action_dist.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Fix items from review comments.\n\n* Add dm_tree to RLlib dependencies.\n\n* Add dm_tree to RLlib dependencies.\n\n* Fix DQN test cases ((Torch)Categorical).\n\n* Fix wrong pip install.\n\nCo-authored-by: Eric Liang <ekhliang@gmail.com>\nCo-authored-by: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TorchCategorical(TorchDistributionWrapper):",
            "\"\"\"Wrapper class for PyTorch Categorical distribution.\"\"\"",
            "",
            "@override(ActionDistribution)",
            "-    def __init__(self, inputs, model):",
            "-        super().__init__(inputs, model)",
            "-        self.dist = torch.distributions.categorical.Categorical(logits=inputs)",
            "+    def __init__(self, inputs, model=None, temperature=1.0):",
            "+        assert temperature > 0.0, \"Categorical `temperature` must be > 0.0!\"",
            "+        super().__init__(inputs / temperature, model)",
            "+        self.dist = torch.distributions.categorical.Categorical(",
            "+            logits=self.inputs)",
            "",
            "@override(ActionDistribution)",
            "def deterministic_sample(self):"
        ]
    },
    {
        "number": 643,
        "comments": "add API call for type fix",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TestPretrainedTransformerEmbedder(AllenNlpTestCase):",
            "def test_xlnet_token_type_ids(self):",
            "token_embedder = PretrainedTransformerEmbedder(\"xlnet-base-cased\")",
            "token_ids = torch.LongTensor([[1, 2, 3], [2, 3, 4]])",
            "-        mask = torch.ones_like(token_ids)",
            "+        mask = torch.ones_like(token_ids).bool()",
            "type_ids = torch.zeros_like(token_ids)",
            "type_ids[1, 1] = 1",
            "token_embedder(token_ids, mask, type_ids)"
        ]
    },
    {
        "number": 645,
        "comments": "change param for resource fix",
        "commit_message": "Fix bug in block layer\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Block(Layer):",
            "layer_counter[layer_type] += 1",
            "",
            "# layer_name = self.name + '-' + layer_name",
            "-            self.layers[n] = self.submodule(",
            "+            layer = self.submodule(",
            "name=layer_name, module=layer_spec, modules=tensorforce.core.layer_modules,",
            "input_spec=self._input_spec",
            ")",
            "-            self._input_spec = self.layers[n].output_spec()",
            "-",
            "+            self.layers.append(layer)",
            "+            self._input_spec = layer.output_spec()",
            "",
            "return self.layers[0].input_spec.copy()"
        ]
    },
    {
        "number": 648,
        "comments": "add param for resource fix",
        "commit_message": "Fix Approx NN on devices other than CUDA\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def model():",
            "",
            "if sd_vae_approx_model is None:",
            "sd_vae_approx_model = VAEApprox()",
            "-        sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\")))",
            "+        sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\"), map_location='cpu' if devices.device.type != 'cuda' else None))",
            "sd_vae_approx_model.eval()",
            "sd_vae_approx_model.to(devices.device, devices.dtype)"
        ]
    },
    {
        "number": 650,
        "comments": "customized API",
        "commit_message": "[Metrics] Confusion matrix class interface (#4348)\n\n* docs + precision + recall + f_beta + refactor\n\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\n\n* rebase\n\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\n\n* fixes\n\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\n\n* added missing file\n\n* docs\n\n* docs\n\n* extra import\n\n* add confusion matrix\n\n* add to docs\n\n* add test\n\n* pep8 + isort\n\n* update tests\n\n* move util function\n\n* unify functional and class\n\n* add to init\n\n* remove old implementation\n\n* update tests\n\n* pep8\n\n* add duplicate\n\n* fix doctest\n\n* Update pytorch_lightning/metrics/classification/confusion_matrix.py\n\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\n\n* changelog\n\n* bullet point args\n\n* bullet docs\n\n* bullet docs\n\nCo-authored-by: ananyahjha93 <ananya@pytorchlightning.ai>\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\nCo-authored-by: chaton <thomas@grid.ai>\nCo-authored-by: Roger Shieh <55400948+s-rog@users.noreply.github.com>\nCo-authored-by: Rohit Gupta <rohitgr1998@gmail.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class Accuracy(Metric):",
            "preds: Predictions from model",
            "target: Ground truth values",
            "\"\"\"",
            "-        preds, target = self._input_format(preds, target)",
            "+        preds, target = _input_format_classification(preds, target, self.threshold)",
            "assert preds.shape == target.shape",
            "",
            "self.correct += torch.sum(preds == target)"
        ]
    },
    {
        "number": 654,
        "comments": "remove API call for resource fix",
        "commit_message": "Bug fix for norm calculation in absence of model parallel group (#551)\n\nIn the absence of a model parallel group, model_parallel_allreduce should not do any reduction. This commit fixes the bug which was doing a model parallel allreduce across world group when model parallel group is None\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class FP16_DeepSpeedZeroOptimizer(object):",
            "\"\"\" Perform all reduce within model parallel group, if any.",
            "\"\"\"",
            "if self.model_parallel_group is None:",
            "-            torch.distributed.all_reduce(tensor=tensor, op=op)",
            "+            pass",
            "else:",
            "torch.distributed.all_reduce(tensor=tensor,",
            "op=op,"
        ]
    },
    {
        "number": 655,
        "comments": "change condition check for state fix",
        "commit_message": "[T5] Fix speed degradation bug t5 (#10496)\n\n* fix speed degradation bug t5\n\n* fix for all models\n\n* fix code quality\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class {{cookiecutter.camelcase_modelname}}EncoderLayer(nn.Module):",
            "hidden_states = residual + hidden_states",
            "hidden_states = self.final_layer_norm(hidden_states)",
            "",
            "-        if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():",
            "+        if hidden_states.dtype == torch.float16 and (torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()):",
            "clamp_value = torch.finfo(hidden_states.dtype).max - 1000",
            "hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)"
        ]
    },
    {
        "number": 656,
        "comments": "change API call for refactor fix",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class LabelSmoother:",
            "",
            "def __call__(self, model_output, labels):",
            "logits = model_output[\"logits\"] if isinstance(model_output, dict) else model_output[0]",
            "-        log_probs = -torch.nn.functional.log_softmax(logits, dim=-1)",
            "+        log_probs = -nn.functional.log_softmax(logits, dim=-1)",
            "if labels.dim() == log_probs.dim() - 1:",
            "labels = labels.unsqueeze(-1)"
        ]
    },
    {
        "number": 657,
        "comments": "add API call for type fix",
        "commit_message": "fix mypy errors\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class NonMaximaSuppression2d(nn.Module):",
            "def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore",
            "assert len(x.shape) == 4, x.shape",
            "# find local maximum values",
            "-        x_max: torch.Tensor = self.max_pool2d(x)",
            "+        x_max: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]] = \\",
            "+            self.max_pool2d(x)",
            "",
            "# create mask for maximums in the original map",
            "x_mask: torch.Tensor = torch.where("
        ]
    },
    {
        "number": 659,
        "comments": "add param for math fix",
        "commit_message": "Adding normalization bias verification (#4990)\n\n* adding batchnorm verification\n\n* Adding trainer callback\n\n* updating changelog\n\n* renaming class\n\n* detailed message for sanity check\n\n* run sanity checks by default\n\n* fix normalization bias issue in image embeddings\n\n* update docstring\n\n* fix test\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class ImageFeatureEmbeddings(Embeddings):",
            "",
            "def __init__(self, feature_size: int, embedding_size: int, dropout: float = 0.0):",
            "image_embeddings = torch.nn.Linear(feature_size, embedding_size)",
            "-        location_embeddings = torch.nn.Linear(4, embedding_size)",
            "+        location_embeddings = torch.nn.Linear(4, embedding_size, bias=False)",
            "embeddings = torch.nn.ModuleDict(",
            "{\"image_embeddings\": image_embeddings, \"location_embeddings\": location_embeddings}",
            ")"
        ]
    },
    {
        "number": 661,
        "comments": "update API call for version fix",
        "commit_message": "fix deprecation about dropout; fix Keras compatibility in tf1.13\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            ".apply(fg)",
            ".BatchNorm('bn5').apply(activate)",
            "# 5",
            "-                      .tf.nn.dropout(0.5 if is_training else 1.0)",
            "+                      .Dropout(rate=0.5 if is_training else 0.0)",
            ".Conv2D('conv6', 512, 5, padding='VALID')",
            ".apply(fg).BatchNorm('bn6')",
            ".apply(nonlin)"
        ]
    },
    {
        "number": 665,
        "comments": "change param for type fix",
        "commit_message": "upgrade to pytorch 1.2 (#3182)\n\n* first attempt at pytorch 1.2\n\n* explicit is better than implicit\n\n* more explicit\n\n* attempt to fix flaky tests\n\n* pylint\n\n* no disable dropout\n\n* disable dropout by default\n\n* restore dropout, don't deepcopy\n\n* change batch size for biaffine_dependency_parser_multilang_test, maybe that will make it pass? :(\n\n* try batch size 10\n\n* ignore bad gradient parameter\n\n* cleanup\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class BLEU(Metric):",
            "return math.exp(1.0 - self._reference_lengths / self._prediction_lengths)",
            "",
            "def _get_valid_tokens_mask(self, tensor: torch.LongTensor) -> torch.ByteTensor:",
            "-        valid_tokens_mask = torch.ones(tensor.size(), dtype=torch.uint8)",
            "+        valid_tokens_mask = torch.ones(tensor.size(), dtype=torch.bool)",
            "for index in self._exclude_indices:",
            "valid_tokens_mask = valid_tokens_mask & (tensor != index)",
            "return valid_tokens_mask"
        ]
    },
    {
        "number": 672,
        "comments": "asset check doc update",
        "commit_message": "Add proper error messages in `__check_input__()` (#5042)\n\n* Add proper error messages in `__check_input__()`\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Format update to pass linting error\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class MessagePassing(torch.nn.Module):",
            "the_size: List[Optional[int]] = [None, None]",
            "",
            "if isinstance(edge_index, Tensor):",
            "-            assert edge_index.dtype == torch.long",
            "-            assert edge_index.dim() == 2",
            "-            assert edge_index.size(0) == 2",
            "+            assert edge_index.dtype == torch.long, \\",
            "+                \"edge_index.dtype is not of torch.long\"",
            "+            assert edge_index.dim() == 2, \\",
            "+                \"edge_index.dim() is not equal to 2\"",
            "+            assert edge_index.size(0) == 2, \\",
            "+                \"edge_index.size(0) is not equal to 2\"",
            "if size is not None:",
            "the_size[0] = size[0]",
            "the_size[1] = size[1]"
        ]
    },
    {
        "number": 674,
        "comments": "add param for shape fix",
        "commit_message": "fixed\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DecoderLayer(nn.Module):",
            "if self.normalize_before:",
            "x = self.norm2(x)",
            "if self.concate_after:",
            "-            x_concat = torch.cat(x, self.src_attn(x, memory, memory, memory_mask))",
            "+            x_concat = torch.cat((x, self.src_attn(x, memory, memory, memory_mask)), dim=-1)",
            "x = residual + self.concate_linear2(x_concat)",
            "else:",
            "x = residual + self.dropout(self.src_attn(x, memory, memory, memory_mask))"
        ]
    },
    {
        "number": 676,
        "comments": "change param for math fix",
        "commit_message": "Explicitly set max sequence length for the roberta encoder, fix output shape computation, and add unit test. (#2861)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class RoBERTaEncoder(Encoder):",
            "@property",
            "def output_shape(self) -> torch.Size:",
            "if self.reduce_output is None:",
            "-            return torch.Size([self.max_sequence_length, self.transformer.module.config.hidden_size])",
            "+            return torch.Size([self.max_sequence_length - 2, self.transformer.module.config.hidden_size])",
            "return torch.Size([self.transformer.module.config.hidden_size])",
            "",
            "@property"
        ]
    },
    {
        "number": 677,
        "comments": "change API call for type fix",
        "commit_message": "fix type issue of `torch.less` in `clip` (#3852)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def clip(",
            "*,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    assert torch.all(torch.less(x_min, x_max)), \"Min value must be less than max.\"",
            "+    assert torch.all(",
            "+        torch.less(torch.tensor(x_min), x_max)",
            "+    ), \"Min value must be less than max.\"",
            "if hasattr(x_min, \"dtype\"):",
            "promoted_type = torch.promote_types(x_min.dtype, x_max.dtype)",
            "promoted_type = torch.promote_types(promoted_type, x.dtype)"
        ]
    },
    {
        "number": 678,
        "comments": "remove API call for type fix",
        "commit_message": "Fix the TF Trainer gradient accumulation and the TF NER example (#6713)\n\n* Align TF NER example over the PT one\n\n* Fix Dataset call\n\n* Fix gradient accumulation training\n\n* Apply style\n\n* Address Sylvain's comments\n\n* Address Sylvain's comments\n\n* Apply style\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFTokenClassificationLoss:",
            ")",
            "# make sure only labels that are not equal to -100",
            "# are taken into account as loss",
            "-        if tf.math.reduce_any(labels == -1).numpy() is True:",
            "+        if tf.math.reduce_any(labels == -1):",
            "warnings.warn(\"Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\")",
            "active_loss = tf.reshape(labels, (-1,)) != -1",
            "else:"
        ]
    },
    {
        "number": 681,
        "comments": "add param for type fix",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class LayoutLMv3Model(LayoutLMv3PreTrainedModel):",
            "position_ids = position_ids.expand_as(input_ids)",
            "final_position_ids = position_ids",
            "",
            "-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, None, device)",
            "+        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(",
            "+            attention_mask, None, device, dtype=embedding_output.dtype",
            "+        )",
            "",
            "# Prepare head mask if needed",
            "# 1.0 in head_mask indicate we keep the head"
        ]
    },
    {
        "number": 683,
        "comments": "change condition check for shape fix",
        "commit_message": "Make sure all pipelines can run with batched input (#1669)\n\n* [SD] Make sure batched input works correctly\n\n* uP\n\n* uP\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* fix mask stuff\n\n* up\n\n* uP\n\n* more up\n\n* up\n\n* uP\n\n* up\n\n* finish\n\n* Apply suggestions from code review\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class StableDiffusionKDiffusionPipeline(DiffusionPipeline):",
            "return_tensors=\"pt\",",
            ")",
            "text_input_ids = text_inputs.input_ids",
            "-        untruncated_ids = self.tokenizer(prompt, padding=\"max_length\", return_tensors=\"pt\").input_ids",
            "+        untruncated_ids = self.tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids",
            "",
            "-        if not torch.equal(text_input_ids, untruncated_ids):",
            "+        if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(text_input_ids, untruncated_ids):",
            "removed_text = self.tokenizer.batch_decode(untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1])",
            "logger.warning(",
            "\"The following part of your input was truncated because CLIP can only handle sequences up to\""
        ]
    },
    {
        "number": 685,
        "comments": "change API call for refactor fix",
        "commit_message": "[tune] Fix TB Memory Leak (#5629)\n\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def to_tf_values(result, path):",
            "",
            "class TFLogger(Logger):",
            "def _init(self):",
            "-        logger.info(",
            "-            \"Initializing TFLogger instead of TF2Logger. We recommend \"",
            "-            \"migrating to TF2.0. This class will be removed in the future.\")",
            "-        self._file_writer = tf.summary.FileWriter(self.logdir)",
            "+        logger.info(\"Initializing TFLogger instead of TF2Logger.\")",
            "+        self._file_writer = tf.compat.v1.summary.FileWriter(self.logdir)",
            "",
            "def on_result(self, result):",
            "tmp = result.copy()"
        ]
    },
    {
        "number": 686,
        "comments": "format",
        "commit_message": "Add tensorboard support (pytorch, tf2+) (#124)\n\n* Add base of tensorboard support (pytorch, tf2+)\n\n* fix formatting\n\n* send protobuf message\n\n* add internal tbwatcher stub\n\n* Add tbdir watcher threads\n\n* Save files that are out of wandb files dir\n\n* add consumer, need logger\n\n* Connected up history, still debuggin tho\n\n* move tensorflow to framework dir\n\n* fix missing metrics with keras\n\n* consolidate summary in internal process\n\n* comment out keras summary test that is no longer valid\n\n* add directory watcher shutdown delay and flush\n\n* fixes from PR comments\n\n* fix indents\n\n* more PR feedback fixes and circleci timeout bump\n\n* fix histogram logging (on some versions of tf/tb)\n\n* chicken out and dont fail users who we cant support yet\n\n* disable console on windows for now\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def test_hook():",
            "tf_summary.scalar(\"c1\", c1)",
            "summary_op = tf_summary.merge_all()",
            "",
            "-        hook = wandb_tensorflow.WandbHook(summary_op, history=history, steps_per_log=1)",
            "+        hook = wandb.tensorflow.WandbHook(summary_op, history=history, steps_per_log=1)",
            "with MonitoredTrainingSession(hooks=[hook]) as sess:",
            "summary, acc = sess.run([summary_op, c1])",
            "history.add({})  # Flush the previous row.",
            "",
            "-    assert wandb_tensorflow.tf_summary_to_dict(summary) == {\"c1\": 42.0}",
            "+    assert wandb.tensorboard.tf_summary_to_dict(summary) == {\"c1\": 42.0}",
            "assert summaries_logged[0][\"c1\"] == 42.0"
        ]
    },
    {
        "number": 690,
        "comments": "no API",
        "commit_message": "Replace Distribution+TorchDistribution with a thin Distribution mixin (#769)\n\n* Sketch Distribution class as mixin\n\n* Remove TorchDistribution class\n\n* Simplify TransformedDistribution\n\n* Update torch wrappers for most distributions\n\n* Fix docs\n\n* Use dist.Reshape() to set extra_event_dims\n\n* Fix bugs in Reshape distribution\n\n* Fix rejector tests\n\n* Update _Subsample distribution\n\n* Use .reshape() method for extra_event_dims and sample_shape\n\n* Refactor Distribution -> TorchDistribution class hierarchy\n\n* Update docs\n\n* Fix json error in air.ipynb\n\n* Fix bugs in air.ipynb and abstract_infer.py\n\n* Fix distributions docs\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class _Subsample(Distribution):",
            "self.subsample_size = subsample_size",
            "self.use_cuda = torch.Tensor.is_cuda if use_cuda is None else use_cuda",
            "",
            "-    def sample(self, sample_shape=None):",
            "+    def sample(self, sample_shape=torch.Size()):",
            "\"\"\"",
            ":returns: a random subsample of `range(size)`",
            ":rtype: torch.autograd.Variable of torch.LongTensor",
            "\"\"\"",
            "+        if sample_shape:",
            "+            raise NotImplementedError",
            "subsample_size = self.subsample_size",
            "if subsample_size is None or subsample_size > self.size:",
            "subsample_size = self.size"
        ]
    },
    {
        "number": 693,
        "comments": "change param for resource fix",
        "commit_message": "experimental.py Apple MPS device fix (#8121)\n\n* experimental.py Apple MPS fix\n\nMay resolve https://github.com/ultralytics/yolov5/issues/8102\n\n* Update experimental.py\n\n* Update experimental.py\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Ensemble(nn.ModuleList):",
            "",
            "",
            "def attempt_load(weights, device=None, inplace=True, fuse=True):",
            "+    # Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a",
            "from models.yolo import Detect, Model",
            "",
            "-    # Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a",
            "model = Ensemble()",
            "for w in weights if isinstance(weights, list) else [weights]:",
            "-        ckpt = torch.load(attempt_download(w), map_location=device)",
            "-        ckpt = (ckpt.get('ema') or ckpt['model']).float()  # FP32 model",
            "+        ckpt = torch.load(attempt_download(w), map_location='cpu')  # load",
            "+        ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model",
            "model.append(ckpt.fuse().eval() if fuse else ckpt.eval())  # fused or un-fused model in eval mode",
            "",
            "# Compatibility updates"
        ]
    },
    {
        "number": 694,
        "comments": "change param for math fix",
        "commit_message": "fix the calcluation of the output dim after 6 subsampmling\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Conv2dSubsampling6(torch.nn.Module):",
            "torch.nn.ReLU(),",
            ")",
            "self.out = torch.nn.Sequential(",
            "-            torch.nn.Linear(odim * (((idim - 1) // 2 - 1) // 3), odim),",
            "+            torch.nn.Linear(odim * (((idim - 1) // 2 - 2) // 3), odim),",
            "PositionalEncoding(odim, dropout_rate),",
            ")"
        ]
    },
    {
        "number": 705,
        "comments": "def",
        "commit_message": "Fixed positional and keyword arguments and added missing out argument to backend implementations (#9660)\n\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def broadcast_arrays(*arrays: torch.Tensor) -> List[torch.Tensor]:",
            "{\"1.11.0 and below\": (\"uint8\", \"uint16\", \"uint32\", \"uint64\")}, backend_version",
            ")",
            "def broadcast_to(",
            "-    x: torch.Tensor, shape: Union[ivy.NativeShape, Sequence[int]]",
            "+    x: torch.Tensor,",
            "+    /,",
            "+    shape: Union[ivy.NativeShape, Sequence[int]],",
            "+    *,",
            "+    out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "if x.ndim > len(shape):",
            "return torch.broadcast_to(x.reshape(-1), shape)"
        ]
    },
    {
        "number": 707,
        "comments": "add API call for type fix",
        "commit_message": "Add support for HuggingFace's TensorFlow models (#127)\n\n* added support for for HuggingFace's TensorFlow models\n\n* added notebook for HuggingFace's tensorflow bert model\n\n* change nebullvm name in logs\n\n* Add optimized model details + warning if static shape is used for HF models (#1)\n\n* add optimized model type info\n\n* fix tvm issue\n\n* edit dockerfile and add image auto building\n\n* add docker installation on azure pipeline\n\n* fix bug in neural compressor output shape\n\n* add support for openvino with python 3.10\n\n* add build docker image to azure pipelines\n\n* revert docker build from az pipelines and edit format of the optimization results\n\n* revert docker build from az pipelines\n\n* added tabulate to setup.py and general fixes\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TFLiteBackendInferenceLearner(TensorflowBaseInferenceLearner):",
            "self.interpreter.set_tensor(i, input_tensor)",
            "self.interpreter.invoke()",
            "return tuple(",
            "-            self.interpreter.get_tensor(output_detail[\"index\"])",
            "+            tf.convert_to_tensor(",
            "+                self.interpreter.get_tensor(output_detail[\"index\"])",
            "+            )",
            "for output_detail in output_details",
            ")"
        ]
    },
    {
        "number": 710,
        "comments": "update API call for refactor fix",
        "commit_message": "fix deprecation about dropout; fix Keras compatibility in tf1.13\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Model(ModelDesc):",
            ".Conv2D('conv3.1', filters=128, padding='VALID') \\",
            ".Conv2D('conv3.2', filters=128, padding='VALID') \\",
            ".FullyConnected('fc0', 1024 + 512, activation=tf.nn.relu) \\",
            "-                .tf.nn.dropout(keep_prob) \\",
            "+                .Dropout(rate=drop_rate) \\",
            ".FullyConnected('fc1', 512, activation=tf.nn.relu) \\",
            ".FullyConnected('linear', out_dim=self.cifar_classnum)()"
        ]
    },
    {
        "number": 716,
        "comments": "rename",
        "commit_message": "Private Tensors (#2709)\n\n* ADD Private Tensor into syft lib\n\n* Allow to serialize Private Tensor\n\n* Change native allow_to_get parameters\n\n* - Overload allow_to_get method\n - Add user parameters at get method (native.py, pointer_tensor.py, object_pointer.py)\n - Modify allow_to_get method at native.py\n - Implement allow_to_get method at PrivateTensor\n\n* Fix allowed_to_get method\n\n* Update virtual_worker test\n\n* Update fit method\n\n* ADD Private Tensor experimental notebook\n\n* Register Private Tensor at hook\n\n* - Update docstrings\n- Fix allow_to_get  method\n- Fix private_tensor method\n\n* ADD get_class_attributes method\n\n* Overload torch module methods\n\n* ADD Private Tensor unit tests\n\n* Fix flake8 issues\n\n* Fix code style\n\n* Fix unit tests coverage\n\n* Change proto reference\n\n* Update PrivateTensor simplify method\n\n* Overload Private Tensor methods\n- Create custom _get_hooked_method (_get_hooked_private_method).\n- Handle PrivateTensors with _get_hooked_private_method.\n- Add parents/command attributes at private tensor.\n\n* ADD new Exception -> SendNotPermittedError\n\n* - Verify permissions during send()/get() methods.\n   We need to verify user credentials during send()/get() tensor commands to keep it safe.\n- REFACTORY/RENAME allowed_to_get(user) -> allow(user)\n   Now, we're using this method to verify permissions in an generic context, not only for get() commands.\n\n* Fix CI tests\n\n* Fix coverage\n\n* Fix detail method to enable remote operations.\n  Change data structure used to store allowed users (list -> tuple).\n\n* Fix proto requirement reference\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def test_obj_not_found(workers):",
            "",
            "def test_get_not_permitted(workers):",
            "bob = workers[\"bob\"]",
            "-    with patch.object(torch.Tensor, \"allowed_to_get\") as mock_allowed_to_get:",
            "+    x = torch.tensor([1, 2, 3, 4, 5]).send(bob)",
            "+    with patch.object(torch.Tensor, \"allow\") as mock_allowed_to_get:",
            "mock_allowed_to_get.return_value = False",
            "-        x = torch.tensor([1, 2, 3, 4, 5]).send(bob)",
            "with pytest.raises(GetNotPermittedError):",
            "x.get()",
            "mock_allowed_to_get.assert_called_once()"
        ]
    },
    {
        "number": 720,
        "comments": "change param for type fix",
        "commit_message": "Fix for ST\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class CustomConverter(object):",
            "xs_pad = pad_list([torch.from_numpy(x).float() for x in xs], 0).to(device, dtype=self.dtype)",
            "",
            "ilens = torch.from_numpy(ilens).to(device)",
            "-        # NOTE: this is for multi-task learning (e.g., speech translation)",
            "-        ys_pad = pad_list([torch.from_numpy(np.array(y[0]) if isinstance(y, tuple) else y).long()",
            "+        # NOTE: this is for multi-output (e.g., speech translation)",
            "+        ys_pad = pad_list([torch.from_numpy(np.array(y[0][:]) if isinstance(y, tuple) else y).long()",
            "for y in ys], self.ignore_id).to(device)",
            "",
            "return xs_pad, ilens, ys_pad"
        ]
    },
    {
        "number": 721,
        "comments": "use custom API",
        "commit_message": "Scale Factor Fix (#2039)\n\n* fix (scale_factor): use new_tensor(scale_factor) in case it is numpy array\n\n* reformat (models): reformat with flake8, yapf, and isort to pass CI\n\n* fix (bbox_mapping): fix scale_factor bug in bbox_mapping\n\n* fix img_meta bug\n\nCo-authored-by: beansi <zhangwenwei@sensetime.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class BBoxHead(nn.Module):",
            "if isinstance(scale_factor, float):",
            "bboxes /= scale_factor",
            "else:",
            "-                scale_factor = torch.from_numpy(scale_factor).to(bboxes.device)",
            "+                scale_factor = bboxes.new_tensor(scale_factor)",
            "bboxes = (bboxes.view(bboxes.size(0), -1, 4) /",
            "scale_factor).view(bboxes.size()[0], -1)"
        ]
    },
    {
        "number": 722,
        "comments": "test",
        "commit_message": "Fixed tests and docs (#654)\n\n* Fixed tests\n\n* Fixed doc error and warnings\n\n* Fixed lint\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class TestAffine2d:",
            "",
            "def test_affine_scale(self, device):",
            "torch.manual_seed(0)",
            "-        scale_factor = torch.rand(1, device=device) * 2.0",
            "+        _scale_factor = torch.rand(1, device=device) * 2.0",
            "+        scale_factor = torch.stack([_scale_factor, _scale_factor], dim=1)",
            "input = torch.rand(1, 2, 3, 4, device=device)",
            "",
            "transform = kornia.Affine(scale_factor=scale_factor).to(device)"
        ]
    },
    {
        "number": 725,
        "comments": "change API call for version fix",
        "commit_message": "Fix example about synthetic benchmark elastic (#2265)\n\nSigned-off-by: jiaqianjing <jiaqianjing@gmail.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def run_benchmark(state):",
            "",
            "",
            "def on_state_reset():",
            "-    tf.keras.backend.set_value(model.optimizer.lr, lr * hvd.size())",
            "+    opt.lr.assign(lr * hvd.size())",
            "",
            "",
            "state = hvd.elastic.TensorFlowKerasState(model, opt, img_secs=[], iter=0, batch=0, warm=False)"
        ]
    },
    {
        "number": 727,
        "comments": "change param for math fix",
        "commit_message": "fix ndc/screen problem in blender/llff (#39)\n\nSummary:\nX-link: https://github.com/fairinternal/pytorch3d/pull/39\n\nBlender and LLFF cameras were sending screen space focal length and principal point to a camera init function expecting NDC\n\nReviewed By: shapovalov\n\nDifferential Revision: D37788686\n\nfbshipit-source-id: 2ddf7436248bc0d174eceb04c288b93858138582\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def _interpret_blender_cameras(",
            "",
            "Rpt3, Tpt3 = mtx[:, :3].split([3, 1], dim=0)",
            "",
            "-        focal_length_pt3 = torch.FloatTensor([[-focal, focal]])",
            "-        principal_point_pt3 = torch.FloatTensor([[W / 2, H / 2]])",
            "+        focal_length_pt3 = torch.FloatTensor([[focal, focal]])",
            "+        principal_point_pt3 = torch.FloatTensor([[0.0, 0.0]])",
            "",
            "cameras = PerspectiveCameras(",
            "focal_length=focal_length_pt3,"
        ]
    },
    {
        "number": 728,
        "comments": "add param for type fix",
        "commit_message": "Type promotion fixes (#2516)\n\n* casting fixes\n\n* lint fixes\n\n* changes\n\n* more changes\n\n* lint fixes\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def vector_to_skew_symmetric_matrix(",
            "a2s = vector_expanded[..., 1:2, :]",
            "a3s = vector_expanded[..., 2:3, :]",
            "# BS x 1 x 1",
            "-    zs = torch.zeros(batch_shape + [1, 1], device=vector.device)",
            "+    zs = torch.zeros(batch_shape + [1, 1], device=vector.device, dtype=vector.dtype)",
            "# BS x 1 x 3",
            "row1 = torch.cat((zs, -a3s, a2s), -1)",
            "row2 = torch.cat((a3s, zs, -a1s), -1)"
        ]
    },
    {
        "number": 730,
        "comments": "add param for resource fix",
        "commit_message": "Fix CUDA tests on master (#1542)\n\n* Fix CUDA tests on master\n\n* address comments\n\n* change to range(11)\n\n* remove unused import\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "from pyro.ops.einsum import contract",
            "def _finfo(tensor):",
            "# This can be replaced with torch.finfo once it is available",
            "# https://github.com/pytorch/pytorch/issues/10742",
            "-    return np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype).numpy().dtype)",
            "+    return np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype, device=\"cpu\").numpy().dtype)",
            "",
            "",
            "def _check_batch_dims_are_sensible(output_dims, nonoutput_ordinal):"
        ]
    },
    {
        "number": 733,
        "comments": "add condition check for state fix",
        "commit_message": "Fix Tracing mode TS export for LayerNorm layer\n\nSummary: When under TorchScript Tracing (instead of only doing this for Scripting) we set `export=True` for `LayerNorm` as `FusedLayerNorm `doesn't work with JIT yet (see `torch.jit.unused decorator`).\n\nReviewed By: cndn\n\nDifferential Revision: D33103054\n\nfbshipit-source-id: f8c24a4a30a89dd4c70b19362fd60c51fcb9a1f0\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "try:",
            "with torch.cuda.device(x.device):",
            "return super().forward(x)",
            "",
            "+",
            "except ImportError:",
            "has_fused_layernorm = False",
            "",
            "",
            "def LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True, export=False):",
            "-    if torch.jit.is_scripting():",
            "+    if torch.jit.is_scripting() or torch.jit.is_tracing():",
            "export = True",
            "if not export and torch.cuda.is_available() and has_fused_layernorm:",
            "return FusedLayerNorm(normalized_shape, eps, elementwise_affine)"
        ]
    },
    {
        "number": 735,
        "comments": "add API call for state fix",
        "commit_message": "fixed missing super update call in TRPO\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TRPOModel(PolicyGradientModel):",
            ":param batch:",
            ":return:",
            "\"\"\"",
            "+        super(TRPOModel, self).update(batch)",
            "+",
            "self.feed_dict = {state: batch['states'][name] for name, state in self.state.items()}",
            "self.feed_dict.update({action: batch['actions'][name] for name, action in self.action.items()})",
            "self.feed_dict[self.reward] = batch['rewards']"
        ]
    },
    {
        "number": 740,
        "comments": "change param for math fix",
        "commit_message": "fixed weight indices for root node in 3d case\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def spline_gcn(",
            "row = row.view(-1, 1).expand(row.size(0), output.size(1))",
            "output = zero.scatter_add_(0, row, output)",
            "",
            "-    # Weighten root node features by multiplying with the meaned weights at the",
            "-    # origin.",
            "-    index = torch.arange(0, kernel_size[-1]).long()",
            "+    # Weighten root node features by multiplying with the meaned weights from",
            "+    # the origin.",
            "+    index = torch.arange(0, reduce(lambda x, y: x * y, kernel_size[1:])).long()",
            "root_weight = weight[index].mean(0)",
            "output += torch.mm(features, root_weight)"
        ]
    },
    {
        "number": 742,
        "comments": "update API call for refactor fix",
        "commit_message": "[RLlib] Minor cleanup in preparation to tf2.x support. (#9130)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* Fixes.\n\n* Fixes and LINT.\n\n* WIP.\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def stats(policy, train_batch):",
            "\"cur_lr\": tf.cast(policy.cur_lr, tf.float64),",
            "\"policy_loss\": policy.loss.pi_loss,",
            "\"entropy\": policy.loss.entropy,",
            "-        \"var_gnorm\": tf.global_norm(policy.model.trainable_variables()),",
            "+        \"var_gnorm\": tf.linalg.global_norm(policy.model.trainable_variables()),",
            "\"vf_loss\": policy.loss.vf_loss,",
            "\"vf_explained_var\": explained_variance(",
            "tf.reshape(policy.loss.value_targets, [-1]),"
        ]
    },
    {
        "number": 743,
        "comments": "add API call for type fix",
        "commit_message": "Fix issues with tests\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_tensorrt_torch(",
            "res_orig = tuple(model(*inputs_example))",
            "assert all(",
            "[",
            "-                    torch.allclose(res_tensor, res_orig_tensor, rtol=1e-01)",
            "+                    torch.allclose(",
            "+                        res_tensor.float(), res_orig_tensor, rtol=1e-01",
            "+                    )",
            "for (res_tensor, res_orig_tensor) in zip(res, res_orig)",
            "]",
            ")"
        ]
    },
    {
        "number": 744,
        "comments": "update API call for refactor fix",
        "commit_message": "use functional interface for softmax in attention (#14198)\n\n* use functional interface instead of instantiating module and immediately calling it\n\n* fix torch.nn.functional to nn.functional. Thank you Stas!\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Attention(nn.Module):",
            "# Apply the attention mask",
            "w = w + attention_mask",
            "",
            "-        w = nn.Softmax(dim=-1)(w)",
            "+        w = nn.functional.softmax(w, dim=-1)",
            "w = self.attn_dropout(w)",
            "",
            "# Mask heads if we want to"
        ]
    },
    {
        "number": 752,
        "comments": "add API call for type fix",
        "commit_message": "Fix parrots compatibility issues (#4143)\n\n* fix parrots compatibility\n\n* add comments\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DistributedGroupSampler(Sampler):",
            "if size > 0:",
            "indice = np.where(self.flag == i)[0]",
            "assert len(indice) == size",
            "-                indice = indice[list(torch.randperm(int(size),",
            "-                                                    generator=g))].tolist()",
            "+                # add .numpy() to avoid bug when selecting indice in parrots.",
            "+                # TODO: check whether torch.randperm() can be replaced by",
            "+                # numpy.random.permutation().",
            "+                indice = indice[list(",
            "+                    torch.randperm(int(size), generator=g).numpy())].tolist()",
            "extra = int(",
            "math.ceil(",
            "size * 1.0 / self.samples_per_gpu / self.num_replicas)"
        ]
    },
    {
        "number": 759,
        "comments": "add param for math fix",
        "commit_message": "fix pairnorm test\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_pair_norm(scale_individually):",
            "assert out1.size() == (100, 16)",
            "",
            "out2 = norm(torch.cat([x, x], dim=0), torch.cat([batch, batch + 1], dim=0))",
            "-    assert torch.allclose(out1, out2[:100])",
            "-    assert torch.allclose(out1, out2[100:])",
            "+    assert torch.allclose(out1, out2[:100], atol=1e-6)",
            "+    assert torch.allclose(out1, out2[100:], atol=1e-6)"
        ]
    },
    {
        "number": 764,
        "comments": "add condition check for type fix",
        "commit_message": "[T5] Fix speed degradation bug t5 (#10496)\n\n* fix speed degradation bug t5\n\n* fix for all models\n\n* fix code quality\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class BlenderbotSmallEncoderLayer(nn.Module):",
            "hidden_states = residual + hidden_states",
            "hidden_states = self.final_layer_norm(hidden_states)",
            "",
            "-        if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():",
            "+        if hidden_states.dtype == torch.float16 and (",
            "+            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()",
            "+        ):",
            "clamp_value = torch.finfo(hidden_states.dtype).max - 1000",
            "hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)"
        ]
    },
    {
        "number": 774,
        "comments": "update API call for refactor fix",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class QuantLinear(nn.Module):",
            "x_int = x / prev_act_scaling_factor",
            "",
            "return (",
            "-            F.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor,",
            "+            nn.functional.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor,",
            "bias_scaling_factor,",
            ")"
        ]
    },
    {
        "number": 779,
        "comments": "update API call for refactor fix",
        "commit_message": "Fix various docs formatting (#1688)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Importance(TracePosterior):",
            "\"\"\"",
            "if self.log_weights:",
            "log_w_norm = self.get_normalized_weights(log_scale=True)",
            "-            ess = torch.exp(-logsumexp(2*log_w_norm, 0))",
            "+            ess = torch.exp(-torch.logsumexp(2*log_w_norm, 0))",
            "else:",
            "warnings.warn(\"The log_weights list is empty, effective sample size is zero.\")",
            "ess = 0"
        ]
    },
    {
        "number": 780,
        "comments": "update API call for version fix",
        "commit_message": "fix for API changes\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "for epoch in range(opt.niter):",
            "vutils.save_image(fake.data, 'fake_samples.png')",
            "",
            "# do checkpointing",
            "-    torch.save(netG.parameter_dict(), 'netG_epoch_%d.pth' % epoch)",
            "-    torch.save(netD.parameter_dict(), 'netD_epoch_%d.pth' % epoch)",
            "+    torch.save(netG.state_dict(), 'netG_epoch_%d.pth' % epoch)",
            "+    torch.save(netD.state_dict(), 'netD_epoch_%d.pth' % epoch)"
        ]
    },
    {
        "number": 781,
        "comments": "test",
        "commit_message": "Fix onnx unitest (#6369)\n\nFix all unitests\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def distance2bbox(points, distance, max_shape=None):",
            "bboxes = torch.stack([x1, y1, x2, y2], -1)",
            "",
            "if max_shape is not None:",
            "-        if points.dim() == 2 and not torch.onnx.is_in_onnx_export():",
            "+        if bboxes.dim() == 2 and not torch.onnx.is_in_onnx_export():",
            "# speed up",
            "bboxes[:, 0::2].clamp_(min=0, max=max_shape[1])",
            "bboxes[:, 1::2].clamp_(min=0, max=max_shape[0])"
        ]
    },
    {
        "number": 782,
        "comments": "update API call for version fix",
        "commit_message": "Fixed a broken test for v0.6 with importing linear function instead of module\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def dnn(tensor_in, hidden_units, activation=tf.nn.relu, keep_prob=None):",
            "with tf.variable_scope('dnn'):",
            "for i, n_units in enumerate(hidden_units):",
            "with tf.variable_scope('layer%d' % i):",
            "-                tensor_in = linear.linear(tensor_in, n_units, True)",
            "+                tensor_in = linear(tensor_in, n_units, True)",
            "tensor_in = activation(tensor_in)",
            "if keep_prob:",
            "tensor_in = tf.nn.dropout(tensor_in, keep_prob)"
        ]
    },
    {
        "number": 783,
        "comments": "update API call for refactor fix",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DSClipEncoder(torch.nn.Module):",
            "seq_len,",
            "seq_len,",
            "dtype=dtype,",
            "-                           device=torch.cuda.current_device())",
            "+                           device=get_accelerator().current_device_name())",
            "mask.fill_(torch.tensor(torch.finfo(dtype).min))",
            "mask.triu_(1)",
            "mask = mask.unsqueeze(1)"
        ]
    },
    {
        "number": 785,
        "comments": "add condition check for resource fix",
        "commit_message": "fix some compatibility problems with PyTorch 1.3.0 in ESPnet\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class CTC(torch.nn.Module):",
            "if self.ctc_type == \"warpctc\":",
            "# warpctc only supports float32",
            "ys_hat = ys_hat.to(dtype=torch.float32)",
            "+        else:",
            "+            # use GPU when using the cuDNN implementation",
            "+            ys_true = to_device(self, ys_true)",
            "self.loss = to_device(self, self.loss_fn(ys_hat, ys_true, hlens, olens)).to(dtype=dtype)",
            "if self.reduce:",
            "# NOTE: sum() is needed to keep consistency since warpctc return as tensor w/ shape (1,)"
        ]
    },
    {
        "number": 789,
        "comments": "change API call for version fix",
        "commit_message": "Data Input Refactoring\n\n- Data input pipeline can now deal properly with missing target data.\nFixes #101\n- Moved some of the featurizer functionality into the data reader and\nmade featurizer implicit part of the model.\n- Misc: Replace concat with concat_v2\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class AttentionDecoder(DecoderBase):",
            "])",
            "else:",
            "attention_context = output.attention_context",
            "-    return tf.concat(1, [next_input, attention_context])",
            "+    return tf.concat_v2([next_input, attention_context], 1)",
            "",
            "def _pad_att_scores(self, scores):",
            "\"\"\"Pads attention scores to fixed length. This is a hack because raw_rnn"
        ]
    },
    {
        "number": 791,
        "comments": "change param for math fix",
        "commit_message": "fix test\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def test_graph_saint():",
            "assert sample.node_norm.numel() == sample.num_nodes",
            "assert sample.edge_norm.numel() == sample.num_edges",
            "",
            "+    torch.manual_seed(12345)",
            "loader = GraphSAINTRandomWalkSampler(data, batch_size=2, walk_length=1,",
            "-                                         num_steps=4, log=False)",
            "+                                         num_steps=4, sample_coverage=10,",
            "+                                         log=False)",
            "",
            "for sample in loader:",
            "assert len(sample) == 4"
        ]
    },
    {
        "number": 792,
        "comments": "remove API call for type fix",
        "commit_message": "[torch] remove deprecated uint8 in favor of bool (#21384)\n\n* uint8 -> bool\n\n* fix copies\n\n* style\n\n* update test modeling commen when checking attention buffers\n\n* style\n\n* use logical not on random mask instead of subtraction with 1\n\n* remove torch uint8\n\n* quality\n\n* remove modified modeling utils\n\n* Update based on review\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n\n---------\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class GPTJAttention(nn.Module):",
            "):",
            "# compute causal mask from causal mask buffer",
            "query_length, key_length = query.size(-2), key.size(-2)",
            "-        causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].to(torch.bool)",
            "+        causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]",
            "",
            "# Keep the attention weights computation in fp32 to avoid overflow issues",
            "query = query.to(torch.float32)"
        ]
    },
    {
        "number": 796,
        "comments": "no API check version fix",
        "commit_message": "Fix parsing Keras version identifiers like `2.2.4-tf` (#3794)\n\nSigned-off-by: Max H. Gerlach <git@maxgerlach.de>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "from horovod.tensorflow.keras import callbacks, elastic",
            "try:",
            "# In later versions of TensorFlow, optimizers are spread across multiple modules. This set is used to distinguish",
            "# stock optimizers that come with tf.keras from custom optimizers that may need to be wrapped specially.",
            "-    if version.parse(keras.__version__) < version.parse(\"2.11\"):",
            "+    if version.parse(keras.__version__.replace(\"-tf\", \"+tf\")) < version.parse(\"2.11\"):",
            "optimizer_type = tf.keras.optimizers.Optimizer",
            "else:",
            "optimizer_type = keras.optimizers.legacy.Optimizer"
        ]
    },
    {
        "number": 800,
        "comments": "update API call for refactor fix",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class GeneralizedChannelPermute(ConditionedGeneralizedChannelPermute, TransformM",
            "self.__delattr__('permutation')",
            "",
            "# Sample a random orthogonal matrix",
            "-        W, _ = torch.qr(torch.randn(channels, channels))",
            "+        W, _ = torch.linalg.qr(torch.randn(channels, channels))",
            "",
            "# Construct the partially pivoted LU-form and the pivots",
            "LU, pivots = W.lu()"
        ]
    },
    {
        "number": 802,
        "comments": "change param for shape fix",
        "commit_message": "Fixed nightly errors (#886)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TestConfusionMatrix:",
            "conf_mat = kornia.utils.metrics.confusion_matrix(",
            "predicted, actual, num_classes)",
            "conf_mat_real = torch.tensor(",
            "-            [[[3, 1],",
            "-              [0, 4]]], dtype=torch.float32)",
            "+            [",
            "+                [[3, 1], [0, 4]],",
            "+                [[3, 1], [0, 4]]",
            "+            ], dtype=torch.float32)",
            "assert_allclose(conf_mat, conf_mat_real)",
            "",
            "def test_three_classes(self):"
        ]
    },
    {
        "number": 803,
        "comments": "update param for type fix",
        "commit_message": "remove graph api (#818)\n\n* remove graph api\n\n* changelog\n\n* timeout test\n\n* codacy\n\n* remove graph in tl.models\n\n* increase timeout time\n\n* remove tl in tests\n\n* Additional Cleaning\n\n* Timeout time added\n\n* tests directory refactored\n\n* Tests Fix Update python, 2.6\n\n* YAPF Cleaning\n\n* get_env fix\n\n* Python 2 fix applied\n\n* Python 2 Error fixes\n\n* TL 1.10.1rc0 released\n\n* Doc and YAPF Fix\n\n* Test YAPF Fix\n\n* RTD Lazy Import Fix\n\n* Travis config restored\n\n* Revert \"Travis config restored\"\n\nThis reverts commit 03fe83eb568c91765461012a84bde309b8d15e69.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def model(x, is_train, reuse):",
            "# nt = Conv2d(nin, 16, (3, 3), (2, 2), act=tf.nn.relu, padding='SAME', name='tc1')",
            "# nt = Conv2d(nt, 8, (3, 3), (2, 2), act=tf.nn.relu, padding='SAME', name='tc2')",
            "## 2. Spatial transformer module (sampler)",
            "-        n = tl.layers.SpatialTransformer2dAffineLayer(nin, theta_layer=nt, out_size=[40, 40], name='spatial')",
            "+        n = tl.layers.SpatialTransformer2dAffineLayer(nin, theta_layer=nt, out_size=(40, 40), name='spatial')",
            "s = n",
            "## 3. Classifier",
            "n = tl.layers.Conv2d("
        ]
    },
    {
        "number": 804,
        "comments": "change API call for shape fix",
        "commit_message": "\ud83d\udea8 \ud83d\udea8 \ud83d\udea8 Fix ViT parameter initialization (#19341)\n\nThis PR aims to rectify the discrepancy between the training performances of HF and Timm ViT implementations.\n\n- Initializes torch and flax ViT dense layer weights with trunc_normal instead of normal (consistent with the TF implementation.\n- Initializes cls_token and positional_embeddings with trunc_normal\n- Updates DeiT copy to reflect the changes\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DeiTPreTrainedModel(PreTrainedModel):",
            "def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:",
            "\"\"\"Initialize the weights\"\"\"",
            "if isinstance(module, (nn.Linear, nn.Conv2d)):",
            "-            # Slightly different from the TF version which uses truncated_normal for initialization",
            "-            # cf https://github.com/pytorch/pytorch/pull/5617",
            "-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)",
            "+            module.weight.data = nn.init.trunc_normal_(module.weight.data, mean=0.0, std=self.config.initializer_range)",
            "if module.bias is not None:",
            "module.bias.data.zero_()",
            "elif isinstance(module, nn.LayerNorm):"
        ]
    },
    {
        "number": 806,
        "comments": "change condition check for type fix",
        "commit_message": "Refactoring, cleanup, improved test coverage.\n* Add eca_nfnet_l2 weights, 84.7 @ 384x384\n* All 'non-std' (ie transformer / mlp) models have classifier / default_cfg test added\n* Fix #694 reset_classifer / num_features / forward_features / num_classes=0 consistency for transformer / mlp models\n* Add direct loading of npz to vision transformer (pure transformer so far, hybrid to come)\n* Rename vit_deit* to deit_*\n* Remove some deprecated vit hybrid model defs\n* Clean up classifier flatten for conv classifiers and unusual cases (mobilenetv3/ghostnet)\n* Remove explicit model fns for levit conv, just pass in arg\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class SelectAdaptivePool2d(nn.Module):",
            "assert False, 'Invalid pool type: %s' % pool_type",
            "",
            "def is_identity(self):",
            "-        return self.pool_type == ''",
            "+        return not self.pool_type",
            "",
            "def forward(self, x):",
            "x = self.pool(x)",
            "-        if self.flatten:",
            "-            x = x.flatten(1)",
            "+        x = self.flatten(x)",
            "return x",
            "",
            "def feat_mult(self):"
        ]
    },
    {
        "number": 808,
        "comments": "add API call for type fix",
        "commit_message": "Fix matmul inputs dtype (#18585)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DisentangledSelfAttention(nn.Module):",
            "dim=-1,",
            "index=p2c_pos.squeeze(0).expand([query_layer.size(0), key_layer.size(-2), key_layer.size(-2)]),",
            ").transpose(-1, -2)",
            "-            score += p2c_att / scale",
            "+            score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)",
            "",
            "return score"
        ]
    },
    {
        "number": 809,
        "comments": "config replace rename",
        "commit_message": "Fix Tutorial 9  (#734)\n\n* Add package download\n\n* Change dev to train file\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "\"\\n\",",
            "\"retriever.train(\\n\",",
            "\"    data_dir=doc_dir,\\n\",",
            "-    \"    train_filename=dev_filename,\\n\",",
            "+    \"    train_filename=train_filename,\\n\",",
            "\"    dev_filename=dev_filename,\\n\",",
            "\"    test_filename=dev_filename,\\n\",",
            "\"    n_epochs=1,\\n\","
        ]
    },
    {
        "number": 812,
        "comments": "add condition check for resource fix",
        "commit_message": "fix some code\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def evaluate(model, data_loader, device):",
            "image = list(img.to(device) for img in image)",
            "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]",
            "",
            "-        torch.cuda.synchronize(device)",
            "+        # \u5f53\u4f7f\u7528CPU\u65f6\uff0c\u8df3\u8fc7GPU\u76f8\u5173\u6307\u4ee4",
            "+        if device != torch.device(\"cpu\"):",
            "+            torch.cuda.synchronize(device)",
            "+",
            "model_time = time.time()",
            "outputs = model(image)"
        ]
    },
    {
        "number": 813,
        "comments": "remove param for argument fix",
        "commit_message": "minor fixe lambda test\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Layer_Lambda_Test(CustomTestCase):",
            "self.dense1 = tl.layers.Dense(in_channels=1, n_units=5)",
            "self.dense2 = tl.layers.Dense(in_channels=1, n_units=5)",
            "self.dense3 = tl.layers.Dense(in_channels=1, n_units=5)",
            "-                self.lambdalayer = tl.layers.ElementwiseLambda(customize_func, fn_weights=[], fn_args={'foo': 1024})",
            "+                self.lambdalayer = tl.layers.ElementwiseLambda(customize_func, fn_args={'foo': 1024})",
            "",
            "def forward(self, x, bar=None):",
            "noise = self.dense1(x)"
        ]
    },
    {
        "number": 814,
        "comments": "rename",
        "commit_message": "mass linter fix\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class TacotronTrainTest(unittest.TestCase):",
            "optimizer = optim.Adam(model.parameters(), lr=c.lr)",
            "for i in range(5):",
            "mel_out, mel_postnet_out, align, stop_tokens = model.forward(",
            "-                input, input_lengths, mel_spec, mel_lengths, speaker_ids)",
            "+                input_dummy, input_lengths, mel_spec, mel_lengths, speaker_ids)",
            "assert torch.sigmoid(stop_tokens).data.max() <= 1.0",
            "assert torch.sigmoid(stop_tokens).data.min() >= 0.0",
            "optimizer.zero_grad()"
        ]
    },
    {
        "number": 816,
        "comments": "format",
        "commit_message": "Fixes #79: Added custom dropout op, that adds probability tensor to the collection DROPOUTS and zeros it on prediction\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def dnn(tensor_in, hidden_units, activation=tf.nn.relu, keep_prob=None):",
            "for i, n_units in enumerate(hidden_units):",
            "with tf.variable_scope('layer%d' % i):",
            "tensor_in = linear(tensor_in, n_units, True)",
            "-            tensor_in = activation(tensor_in)",
            "-            if keep_prob:",
            "-                tensor_in = tf.nn.dropout(tensor_in, keep_prob)",
            "+                tensor_in = activation(tensor_in)",
            "+                if keep_prob:",
            "+                    tensor_in = skflow.ops.dropout(tensor_in, keep_prob)",
            "return tensor_in"
        ]
    },
    {
        "number": 821,
        "comments": "add condition check for resource fix",
        "commit_message": "Misc fixes (#2342)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2342\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D22601110\n\nPulled By: myleott\n\nfbshipit-source-id: 7a704c07d507692f274c31ec74b090134fa9dee3\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def clip_grad_norm_(params, max_norm, aggregate_norm_fn=None) -> torch.Tensor:",
            "if multi_tensor_l2norm_available:",
            "total_norm = multi_tensor_total_norm(grads)",
            "else:",
            "-            warnings.warn(",
            "-                \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"",
            "-                \"you may get better performance by installing NVIDIA's apex library\"",
            "-            )",
            "+            if torch.cuda.is_available():",
            "+                warnings.warn(",
            "+                    \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"",
            "+                    \"you may get better performance by installing NVIDIA's apex library\"",
            "+                )",
            "total_norm = torch.norm(",
            "torch.stack([torch.norm(g, p=2, dtype=torch.float32) for g in grads])",
            ")"
        ]
    },
    {
        "number": 823,
        "comments": "add condition check for type fix",
        "commit_message": "fix data device type bug (#3856)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ModelSpeedupTensorRT(BaseModelSpeedup):",
            "Model input tensor",
            "\"\"\"",
            "# convert pytorch tensor to numpy darray",
            "+        if test_data.device != torch.device(\"cpu\"):",
            "+            test_data = test_data.to(\"cpu\")",
            "test_data = test_data.numpy()",
            "# Numpy dtype should be float32",
            "assert test_data.dtype == np.float32"
        ]
    },
    {
        "number": 824,
        "comments": "no API",
        "commit_message": "fixed the case when r > 1\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class Dio(AbsFeatsExtract):",
            "",
            "return f0",
            "",
            "-    @staticmethod",
            "-    def _average_by_duration(x: torch.Tensor, d: torch.Tensor) -> torch.Tensor:",
            "-        assert d.sum() == len(x)",
            "+    def _average_by_duration(self, x: torch.Tensor, d: torch.Tensor) -> torch.Tensor:",
            "+        assert len(x) - d.sum() < self.reduction_factor",
            "d_cumsum = F.pad(d.cumsum(dim=0), (1, 0))",
            "x_avg = [",
            "x[start:end].masked_select(x[start:end].gt(0.0)).mean(dim=0)"
        ]
    },
    {
        "number": 828,
        "comments": "add condition check for type fix",
        "commit_message": "Fix #1712 broken support for AMP w/ PyTorch < 1.10. Disable loss scaler for bfloat16\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def main():",
            "if utils.is_primary(args):",
            "_logger.info('Using NVIDIA APEX AMP. Training in mixed precision.')",
            "elif use_amp == 'native':",
            "-        amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)",
            "-        if device.type == 'cuda':",
            "+        try:",
            "+            amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)",
            "+        except (AttributeError, TypeError):",
            "+            # fallback to CUDA only AMP for PyTorch < 1.10",
            "+            assert device.type == 'cuda'",
            "+            amp_autocast = torch.cuda.amp.autocast",
            "+        if device.type == 'cuda' and amp_dtype == torch.float16:",
            "+            # loss scaler only used for float16 (half) dtype, bfloat16 does not need it",
            "loss_scaler = NativeScaler()",
            "if utils.is_primary(args):",
            "_logger.info('Using native Torch AMP. Training in mixed precision.')"
        ]
    },
    {
        "number": 829,
        "comments": "remove API call for version fix",
        "commit_message": "Previously, many unit test files started with `enable_v2_behavior`, which would have caused them to run in V2 mode when executing with a V1 test flag. The correct behavior would in fact be to skip such tests when executing with a V1 test flag.\n\nThis fix significantly reduces the total V1 + V2 test load by eliminating redundancy.\n\nPiperOrigin-RevId: 424734850\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class AutoOutsideCompilationWithKerasTest(tf.test.TestCase):",
            "",
            "",
            "if __name__ == '__main__':",
            "-  tf.compat.v1.enable_eager_execution()",
            "tf.test.main()"
        ]
    },
    {
        "number": 837,
        "comments": "add API call for type fix",
        "commit_message": "fixed to keep compatibility\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Encoder(torch.nn.Module):",
            "pos_enc_class(attention_dim, positional_dropout_rate),",
            ")",
            "elif input_layer is None:",
            "-            self.embed = pos_enc_class(attention_dim, positional_dropout_rate)",
            "+            self.embed = torch.nn.Sequential(",
            "+                pos_enc_class(attention_dim, positional_dropout_rate)",
            "+            )",
            "else:",
            "raise ValueError(\"unknown input_layer: \" + input_layer)",
            "self.normalize_before = normalize_before"
        ]
    },
    {
        "number": 838,
        "comments": "add param for math fix",
        "commit_message": "Cleanup the usage of `layer_norm_eps` in some models (#21336)\n\n* fix\n\n* fix\n\n* make style\n\n* For CLIP\n\n* For OwlViT\n\n* For XCLIP\n\n* For CLIPSeg\n\n* For GroupViT\n\n* fix docstrings\n\n* fix docstrings\n\n* For AltCLIP\n\n* For ChineseCLIP\n\n* For Blip\n\n* For GiT\n\n* make style\n\n* update\n\n* update\n\n* update\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class ChineseCLIPVisionTransformer(nn.Module):",
            "embed_dim = config.hidden_size",
            "",
            "self.embeddings = ChineseCLIPVisionEmbeddings(config)",
            "-        self.pre_layrnorm = nn.LayerNorm(embed_dim)",
            "+        self.pre_layrnorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)",
            "self.encoder = ChineseCLIPVisionEncoder(config)",
            "-        self.post_layernorm = nn.LayerNorm(embed_dim)",
            "+        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)",
            "",
            "@add_start_docstrings_to_model_forward(CHINESE_CLIP_VISION_INPUTS_DOCSTRING)",
            "@replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=ChineseCLIPVisionConfig)"
        ]
    },
    {
        "number": 840,
        "comments": "add API call for math fix",
        "commit_message": "fix failing tests related to `solve_cast` on torch 1.9 (#2066)\n\n* update torch seed\n\n* rerun CI\n\n* update to use rescale\n\n* rerun ci\n\n* update tests cases to have hardcoded input\n\ndeleted:    test/utilities.py - not used anywhere\n\n* remove fail fast at ci\n\n* manual seed 0 TestImageStitcher::test_smoke\n\n* manual seed 0 TestHomographyTracker::test_real\n\n* manual seed 6 TestHomographyTracker::test_real\n\n* manual seed 245 TestImageStitcher::test_smoke and hardcoded case\n\n* manual seed 1 TestImageStitcher::test_smoke\n\n* manual seed 8 TestHomographyTracker::test_real\n\n- Comment the second frame test\n\n* rerun CI\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TestSolveCast:",
            "",
            "class TestSolveWithMask:",
            "def test_smoke(self, device, dtype):",
            "+        torch.manual_seed(0)  # issue kornia#2027",
            "A = torch.randn(2, 3, 1, 4, 4, device=device, dtype=dtype)",
            "B = torch.randn(2, 3, 1, 4, 6, device=device, dtype=dtype)"
        ]
    },
    {
        "number": 842,
        "comments": "change param for type fix",
        "commit_message": "Update serving signatures and make sure we actually use them (#19034)\n\n* Override save() to use the serving signature as the default\n\n* Replace int32 with int64 in all our serving signatures\n\n* Remember one very important line so as not to break every test at once\n\n* Dtype fix for TFLED\n\n* dtype fix for shift_tokens_right in general\n\n* Dtype fixes in mBART and RAG\n\n* Fix dtypes for test_unpack_inputs\n\n* More dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Add a check that the model actually has a serving method\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TFHubertPreTrainedModel(TFPreTrainedModel):",
            "input_signature=[",
            "{",
            "\"input_values\": tf.TensorSpec((None, None), tf.float32, name=\"input_values\"),",
            "-                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "-                \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),",
            "+                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "+                \"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),",
            "}",
            "]",
            ")"
        ]
    },
    {
        "number": 845,
        "comments": "change param for math fix",
        "commit_message": "fix a bug in test\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_maskctc(encoder_arch, interctc_layer_idx, interctc_use_conditioning):",
            "inputs = dict(",
            "speech=torch.randn(2, 10, 20, requires_grad=True),",
            "speech_lengths=torch.tensor([10, 8], dtype=torch.long),",
            "-        text=torch.randint(0, vocab_size + 1, [2, 4], dtype=torch.long),",
            "+        text=torch.randint(2, 4, [2, 4], dtype=torch.long),",
            "text_lengths=torch.tensor([4, 3], dtype=torch.long),",
            ")",
            "loss, *_ = model(**inputs)"
        ]
    },
    {
        "number": 847,
        "comments": "update API call for version fix",
        "commit_message": "fix deprecation warnings; remove pydoop dependency; update README\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def _add_gradients_summaries(grads_and_vars):",
            "grad_values = grad.values",
            "else:",
            "grad_values = grad",
            "-      summaries.append(tf.histogram_summary(var.op.name + ':gradient',",
            "+      summaries.append(tf.summary.histogram(var.op.name + ':gradient',",
            "grad_values))",
            "-      summaries.append(tf.histogram_summary(var.op.name + ':gradient_norm',",
            "+      summaries.append(tf.summary.histogram(var.op.name + ':gradient_norm',",
            "tf.global_norm([grad_values])))",
            "else:",
            "tf.logging.info('Var %s has no gradient', var.op.name)"
        ]
    },
    {
        "number": 849,
        "comments": "add param for type fix",
        "commit_message": "fix warning\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class QM9(InMemoryDataset):",
            "edge_type += 2 * [self.bonds[bond.GetBondType()]]",
            "",
            "edge_index = torch.tensor([row, col], dtype=torch.long)",
            "-            edge_type = torch.tensor(edge_type)",
            "-            edge_attr = F.one_hot(torch.tensor(edge_type),",
            "+            edge_type = torch.tensor(edge_type, dtype=torch.long)",
            "+            edge_attr = F.one_hot(edge_type,",
            "num_classes=len(self.bonds)).to(torch.float)",
            "",
            "perm = (edge_index[0] * N + edge_index[1]).argsort()"
        ]
    },
    {
        "number": 850,
        "comments": "doc update",
        "commit_message": "fix mymy issues\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def connect(",
            "self.client_type = client_type",
            "",
            "if credentials:",
            "-                metadata, _user_key = self.conn.login(credentials=credentials)",
            "+                metadata, _user_key = self.conn.login(credentials=credentials)  # type: ignore",
            "_user_key = SigningKey(_user_key.encode(\"utf-8\"), encoder=HexEncoder)",
            "else:",
            "-                metadata = self.conn._get_metadata()",
            "+                metadata = self.conn._get_metadata()  # type: ignore",
            "if not user_key:",
            "_user_key = SigningKey.generate()",
            "else:"
        ]
    },
    {
        "number": 854,
        "comments": "update API call for version fix",
        "commit_message": "Replace some fs operations by tf.gfile for other fs support. (fix #416)\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class MinSaver(Callback):",
            "newname = os.path.join(logger.LOG_DIR,",
            "self.filename or",
            "('max-' + self.monitor_stat if self.reverse else 'min-' + self.monitor_stat))",
            "-        files_to_copy = glob.glob(path + '*')",
            "+        files_to_copy = tf.gfile.Glob(path + '*')",
            "for file_to_copy in files_to_copy:",
            "-            shutil.copy(file_to_copy, file_to_copy.replace(path, newname))",
            "+            tf.gfile.Copy(file_to_copy, file_to_copy.replace(path, newname), overwrite=True)",
            "logger.info(\"Model with {} '{}' saved.\".format(",
            "'maximum' if self.reverse else 'minimum', self.monitor_stat))"
        ]
    },
    {
        "number": 855,
        "comments": "add API call for state fix",
        "commit_message": "fix prediction (apply pre-processing)\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Evaluator(object):",
            "The mean average result per tensor over the entire dataset.",
            "",
            "\"\"\"",
            "+        tflearn.is_training(False, self.session)",
            "coord = tf.train.Coordinator()",
            "inputs = tf.get_collection(tf.GraphKeys.INPUTS)",
            "# Data Preprocessing"
        ]
    },
    {
        "number": 857,
        "comments": "add API call for type fix",
        "commit_message": "One more scalar -> tensor fix for lamb optimizer\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Lamb(Optimizer):",
            "global_grad_norm.add_(grad.pow(2).sum())",
            "",
            "global_grad_norm = torch.sqrt(global_grad_norm)",
            "-        max_grad_norm = self.defaults['max_grad_norm']",
            "+        # FIXME it'd be nice to remove explicit tensor conversion of scalars when torch.where promotes",
            "+        # scalar types properly https://github.com/pytorch/pytorch/issues/9190",
            "+        max_grad_norm = torch.tensor(self.defaults['max_grad_norm'], device=device)",
            "clip_global_grad_norm = torch.where(",
            "global_grad_norm > max_grad_norm,",
            "global_grad_norm / max_grad_norm,"
        ]
    },
    {
        "number": 860,
        "comments": "add API call for math fix",
        "commit_message": "[rllib] General RNN support (#2299)\n\n* wip\n\n* cls\n\n* re\n\n* wip\n\n* wip\n\n* a3c working\n\n* torch support\n\n* pg works\n\n* lint\n\n* rm v2\n\n* consumer id\n\n* clean up pg\n\n* clean up more\n\n* fix python 2.7\n\n* tf session management\n\n* docs\n\n* dqn wip\n\n* fix compile\n\n* dqn\n\n* apex runs\n\n* up\n\n* impotrs\n\n* ddpg\n\n* quotes\n\n* fix tests\n\n* fix last r\n\n* fix tests\n\n* lint\n\n* pass checkpoint restore\n\n* kwar\n\n* nits\n\n* policy graph\n\n* fix yapf\n\n* com\n\n* class\n\n* pyt\n\n* vectorization\n\n* update\n\n* test cpe\n\n* unit test\n\n* fix ddpg2\n\n* changes\n\n* wip\n\n* args\n\n* faster test\n\n* common\n\n* fix\n\n* add alg option\n\n* batch mode and policy serving\n\n* multi serving test\n\n* todo\n\n* wip\n\n* serving test\n\n* doc async env\n\n* num envs\n\n* comments\n\n* thread\n\n* remove init hook\n\n* update\n\n* fix ppo\n\n* comments1\n\n* fix\n\n* updates\n\n* add jenkins tests\n\n* fix\n\n* fix pytorch\n\n* fix\n\n* fixes\n\n* fix a3c policy\n\n* fix squeeze\n\n* fix trunc on apex\n\n* fix squeezing for real\n\n* update\n\n* remove horizon test for now\n\n* multiagent wip\n\n* update\n\n* fix race condition\n\n* fix ma\n\n* t\n\n* doc\n\n* st\n\n* wip\n\n* example\n\n* wip\n\n* working\n\n* cartpole\n\n* wip\n\n* batch wip\n\n* fix bug\n\n* make other_batches None default\n\n* working\n\n* debug\n\n* nit\n\n* warn\n\n* comments\n\n* fix ppo\n\n* fix obs filter\n\n* update\n\n* wip\n\n* tf\n\n* update\n\n* fix\n\n* cleanup\n\n* cleanup\n\n* spacing\n\n* model\n\n* fix\n\n* dqn\n\n* fix ddpg\n\n* doc\n\n* keep names\n\n* update\n\n* fix\n\n* com\n\n* docs\n\n* clarify model outputs\n\n* Update torch_policy_graph.py\n\n* fix obs filter\n\n* pass thru worker index\n\n* fix\n\n* rename\n\n* vlad torch comments\n\n* fix log action\n\n* debug name\n\n* fix lstm\n\n* remove unused ddpg net\n\n* remove conv net\n\n* revert lstm\n\n* wip\n\n* wip\n\n* cast\n\n* wip\n\n* works\n\n* fix a3c\n\n* works\n\n* lstm util test\n\n* doc\n\n* clean up\n\n* update\n\n* fix lstm check\n\n* move to end\n\n* fix sphinx\n\n* fix cmd\n\n* remove bad doc\n\n* clarify\n\n* copy\n\n* async sa\n\n* fix\n\n* comments\n\n* fix a3c conf\n\n* tune lstm\n\n* fix reshape\n\n* fix\n\n* back to 16\n\n* tuned a3c update\n\n* update\n\n* tuned\n\n* optional\n\n* fix catalog\n\n* remove prep\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class ModelCatalogTest(unittest.TestCase):",
            "def testCustomModel(self):",
            "ray.init()",
            "ModelCatalog.register_custom_model(\"foo\", CustomModel)",
            "-        p1 = ModelCatalog.get_model(1, 5, {\"custom_model\": \"foo\"})",
            "+        p1 = ModelCatalog.get_model(",
            "+            tf.constant([1, 2, 3]), 5, {\"custom_model\": \"foo\"})",
            "self.assertEqual(str(type(p1)), str(CustomModel))"
        ]
    },
    {
        "number": 862,
        "comments": "change condition check for resource fix",
        "commit_message": "fix a bug in DCGAN (#121)\n\nUsing single GPU in DCGAN caused an error because  'gpu_ids'  in forward function will be None .\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class _netD(nn.Module):",
            "",
            "def forward(self, input):",
            "gpu_ids = None",
            "-        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:",
            "+        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > =1:",
            "gpu_ids = range(self.ngpu)",
            "output = nn.parallel.data_parallel(self.main, input, gpu_ids)",
            "return output.view(-1, 1)"
        ]
    },
    {
        "number": 864,
        "comments": "add condition check for type fix",
        "commit_message": "Add Dorckerfile and implement TFLite and Torchscript as model-backends (#64)\n\n* add tf and torch backeneds\n\n* add torch backend\n\n* add tqdm to requirements\n\n* avoid installation of compilers when NO_COMPILER_INSTALLATION is set\n\n* fix error with onnx installation\n\n* fix error with onnx installation\n\n* fix error with onnx installation\n\n* fix error with onnx installation\n\n* Added Dockerfile & bugfix (#63)\n\n* add dockerfile\n\n* fix tvm configs issue in the tvm installer\n\n* fix tvm issue\n\n* fix tvm\n\n* fix dockerfile & created build script for the docker images\n\n* removed redundant spaces\n\nCo-authored-by: Valerio Sofi <v.sofi@nebuly.ai>\n\n* add tflite to tf api\n\n* fix logging\n\n* fix error with half precision in torch\n\n* fix minor bugs\n\n* fix bugs\n\n* fix import\n\n* fix bug with tf\n\n* fix error with DeviceArrays in polygraphy\n\n* fix another bug\n\n* upgrade version\n\nCo-authored-by: morgoth95 <d.fiori@nebuly.ai>\nCo-authored-by: Valerio Sofi <v.sofi@nebuly.ai>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "from nebullvm.transformations.base import BaseTransformation",
            "",
            "",
            "class VerifyContiguity(BaseTransformation):",
            "-    def _transform(self, _input: torch.Tensor, **kwargs) -> Any:",
            "+    def _transform(self, _input: Any, **kwargs) -> Any:",
            "+        if not isinstance(_input, torch.Tensor):",
            "+            return _input",
            "if not _input.is_contiguous():",
            "_input = _input.contiguous()",
            "return _input"
        ]
    },
    {
        "number": 865,
        "comments": "add param for resource fix",
        "commit_message": "distributed strategy bug fix\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "from tests import utils",
            "def test_image_classifier(tmp_path):",
            "train_x = utils.generate_data(num_instances=320, shape=(32, 32, 3))",
            "train_y = utils.generate_one_hot_labels(num_instances=320, num_classes=10)",
            "-    clf = ak.ImageClassifier(directory=tmp_path, max_trials=2, seed=utils.SEED)",
            "+    clf = ak.ImageClassifier(",
            "+        directory=tmp_path,",
            "+        max_trials=2,",
            "+        seed=utils.SEED,",
            "+        distribution_strategy=tf.distribute.MirroredStrategy(),",
            "+    )",
            "clf.fit(train_x, train_y, epochs=1, validation_split=0.2)",
            "keras_model = clf.export_model()",
            "clf.evaluate(train_x, train_y)"
        ]
    },
    {
        "number": 867,
        "comments": "remove API call for type fix",
        "commit_message": "Fix conv2d_transpose\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),",
            "output_shape[3],",
            "output_shape[1])",
            "if output_shape[0] is None:",
            "-        output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:])",
            "-        output_shape = tf.stack(list(output_shape))",
            "+        output_shape = (shape(x)[0],) + tuple(output_shape[1:])",
            "+",
            "+    output_shape = tf.stack(list(output_shape))",
            "",
            "padding = _preprocess_padding(padding)",
            "if tf_data_format == 'NHWC':"
        ]
    },
    {
        "number": 868,
        "comments": "add API call for math fix",
        "commit_message": "lint fixes\n\nSummary:\nRan the linter.\nTODO: need to update the linter as per D21353065.\n\nReviewed By: bottler\n\nDifferential Revision: D21362270\n\nfbshipit-source-id: ad0e781de0a29f565ad25c43bc94a19b1828c020\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TestRasterizeMeshes(TestCaseMixin, unittest.TestCase):",
            "], dtype=torch.int64, device=device)",
            "# fmt: on",
            "",
            "-        pix_to_face_padded = -torch.ones_like(pix_to_face_frontface)",
            "+        pix_to_face_padded = -(torch.ones_like(pix_to_face_frontface))",
            "# Run with and without culling",
            "# Without culling, for k=0, the front face (i.e. face 2) is",
            "# rasterized and for k=1, the back face (i.e. face 3) is"
        ]
    },
    {
        "number": 871,
        "comments": "add API call for state fix",
        "commit_message": "add compat.is_gpu_available(); fix test failure\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class PipelineTest(test.SparkTest):",
            "import tensorflow as tf",
            "from tensorflowonspark import TFNode",
            "",
            "+      tf.compat.v1.disable_eager_execution()",
            "tf.compat.v1.reset_default_graph()",
            "strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()"
        ]
    },
    {
        "number": 875,
        "comments": "change condition check for version fix",
        "commit_message": "Fix parsing Keras version identifiers like `2.2.4-tf` (#3794)\n\nSigned-off-by: Max H. Gerlach <git@maxgerlach.de>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class SparkKerasTests(tf.test.TestCase):",
            "",
            "def test_fit_model_multiclass(self):",
            "model = create_mnist_model()",
            "-        if version.parse(tf.keras.__version__) < version.parse(\"2.11\"):",
            "+        if version.parse(tf.keras.__version__.replace(\"-tf\", \"+tf\")) < version.parse(\"2.11\"):",
            "optimizer = tf.keras.optimizers.Adadelta(1.0)",
            "else:",
            "optimizer = tf.keras.optimizers.legacy.Adadelta(1.0)"
        ]
    },
    {
        "number": 878,
        "comments": "change param for resource fix",
        "commit_message": "fix amp+multi gpu in asr\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def train(args):",
            "dtype = torch.float32",
            "model = model_class(args.n_vocab, args).to(dtype=dtype)",
            "if args.ngpu > 0:",
            "-        model.to(\"cuda:0\")",
            "+        model.to(\"cuda\")",
            "gpu_id = list(range(args.ngpu))",
            "else:",
            "gpu_id = [-1]"
        ]
    },
    {
        "number": 880,
        "comments": "change param for shape fix",
        "commit_message": "fix create_meshgrid indexing and refactor tensor_to_image\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def create_meshgrid(height, width, normalized_coordinates=True):",
            "else:",
            "xs = torch.linspace(0, width - 1, width)",
            "ys = torch.linspace(0, height - 1, height)",
            "-    return torch.stack(torch.meshgrid([ys, xs])).view(1, 2, -1)",
            "+    return torch.stack(torch.meshgrid([ys, xs])).view(1, 2, -1)[:, (1, 0), :]",
            "",
            "",
            "class HomographyWarper(nn.Module):"
        ]
    },
    {
        "number": 882,
        "comments": "add condition check for resource fix",
        "commit_message": "fix device\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def main():",
            "# download url: https://download.pytorch.org/models/resnet34-333f7ec4.pth",
            "model_weight_path = \"./resnet34-pre.pth\"",
            "assert os.path.exists(model_weight_path), \"file {} does not exist.\".format(model_weight_path)",
            "-    net.load_state_dict(torch.load(model_weight_path, map_location=device))",
            "+    net.load_state_dict(torch.load(model_weight_path, map_location='cpu'))",
            "# for param in net.parameters():",
            "#     param.requires_grad = False"
        ]
    },
    {
        "number": 883,
        "comments": "update API call for refactor fix",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def triangular_solve(x, y, upper=False, transpose=False):",
            "",
            "",
            "def precision_to_scale_tril(P):",
            "-    Lf = torch.cholesky(torch.flip(P, (-2, -1)))",
            "+    Lf = torch.linalg.cholesky(torch.flip(P, (-2, -1)))",
            "L_inv = torch.transpose(torch.flip(Lf, (-2, -1)), -2, -1)",
            "L = torch.triangular_solve(torch.eye(P.shape[-1], dtype=P.dtype, device=P.device),",
            "L_inv, upper=False)[0]"
        ]
    },
    {
        "number": 884,
        "comments": "remove API call for type fixfor distributed bug",
        "commit_message": "Summaries completely changed, distributed mode incompletely changed, various fixes\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class NaturalGradient(Optimizer):",
            "applied = self.apply_step(variables=variables, diffs=estimated_diffs)",
            "",
            "with tf.control_dependencies(control_inputs=(applied,)):",
            "-                return [tf.identity(input=estimated_diff) for estimated_diff in estimated_diffs]",
            "+                return [estimated_diff + 0.0 for estimated_diff in estimated_diffs]",
            "",
            "def false_fn():",
            "return [tf.zeros_like(tensor=diff) for diff in diffs]"
        ]
    },
    {
        "number": 885,
        "comments": "refactor fix error",
        "commit_message": "\ud83d\udc1b  fix path, entrypoint and logging\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def configure_logger(verbose: bool) -> None:",
            "verbose (bool):",
            "`True` to use verbose logger, `False` otherwise.",
            "\"\"\"",
            "-    tf_logger = tf.get_logger()",
            "+    from tensorflow import get_logger",
            "+    from tensorflow.compat.v1 import logging as tf_logging",
            "+    tf_logger = get_logger()",
            "tf_logger.handlers = [handler]",
            "if verbose:",
            "tf_logging.set_verbosity(tf_logging.INFO)"
        ]
    },
    {
        "number": 886,
        "comments": "add API call for type fix",
        "commit_message": "Fix argsort test for tensorflow (#2389)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def argsort(",
            "ret = tf.argsort(",
            "tf.convert_to_tensor(x), axis=axis, direction=\"ASCENDING\", stable=stable",
            ")",
            "-    return ret",
            "+    return tf.cast(ret, dtype=tf.int64)",
            "",
            "",
            "def sort("
        ]
    },
    {
        "number": 887,
        "comments": "update API call for version fix",
        "commit_message": "remove redundant recursive calls in _to_native and fix _to_ivy_array\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def _to_ivy(x: Any) -> Any:",
            "",
            "",
            "def _to_ivy_array(x: Any) -> ivy.Array:",
            "-    if isinstance(x, (torch.Tensor, tf.Tensor, jnp.numpy.DeviceArray, numpy.ndarray)):",
            "+    if isinstance(x, (torch.Tensor, tf.Tensor, jnp.DeviceArray, numpy.ndarray)):",
            "return ivy.array(numpy.array(x))",
            "return x"
        ]
    },
    {
        "number": 888,
        "comments": "add condition check for type fix",
        "commit_message": "Another fix to vecdot (#4639)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def vecdot(",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "dtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))",
            "-    x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)",
            "+    if dtype != \"float64\":",
            "+        x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)",
            "ret = tf.cast(tf.tensordot(x1, x2, axes=(axis, axis)), dtype)",
            "return ret"
        ]
    },
    {
        "number": 895,
        "comments": "add API call for type fix",
        "commit_message": "Fix buffers in sinusoidal positional embeddings\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class SinusoidalPositionalEmbedding(nn.Module):",
            "self.embedding_dim,",
            "self.padding_idx,",
            ").type_as(self.weights)",
            "+        self.weights = self.weights.type_as(self._float_tensor)",
            "weights = Variable(self.weights)",
            "",
            "if incremental_state is not None:"
        ]
    },
    {
        "number": 901,
        "comments": "add API call for type fix",
        "commit_message": "fix generate_share range and reduce field to 62\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class AdditiveSharingTensor(AbstractTensor):",
            "random_shares = [random_type(secret.shape) for _ in range(n_workers - 1)]",
            "",
            "for share in random_shares:",
            "-            share.random_(-field, field)",
            "+            share.random_(int(-field/2), int(field/2)-1)",
            "",
            "shares = []",
            "for i in range(n_workers):"
        ]
    },
    {
        "number": 902,
        "comments": "update API call for version fix",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class PipelineEngine(DeepSpeedEngine):",
            "mem_cached = new_cached",
            "mem_alloced = new_alloced",
            "",
            "-        max_alloced = torch.cuda.max_memory_allocated()",
            "-        max_cached = torch.cuda.max_memory_cached()",
            "+        max_alloced = get_accelerator().max_memory_allocated()",
            "+        max_cached = get_accelerator().max_memory_cached()",
            "",
            "# convert to GB for printing",
            "new_alloced /= 1024**3"
        ]
    },
    {
        "number": 903,
        "comments": "add param for resource fix",
        "commit_message": "Fix gpu placement bug in enum_discrete (#672)\n\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def torch_multinomial(input, num_samples, replacement=False):",
            "Does not support keyword argument `out`.",
            "\"\"\"",
            "if input.is_cuda:",
            "-        return torch_multinomial(input.cpu(), num_samples, replacement).cuda()",
            "+        return torch.multinomial(input.cpu(), num_samples, replacement).cuda(input.get_device())",
            "else:",
            "return torch.multinomial(input, num_samples, replacement)"
        ]
    },
    {
        "number": 904,
        "comments": "add param for argument fix",
        "commit_message": "add redis test marker to conftest.py\nadd redis database tests to tox and .github flows\nuse print instead of std.write\nfix datasets tests to work with the new skip_checks flag\nadd skip_checks flag on dataset purge\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def test_delete_entire_dataset(domain_owner, cleanup_storage):",
            "assert domain_owner.datasets[0].name == \"Dataset_1\"",
            "assert domain_owner.datasets[1].name == \"Dataset_2\"",
            "",
            "-    domain_owner.datasets.delete(dataset_id=domain_owner.datasets[0].id)",
            "+    domain_owner.datasets.delete(",
            "+        dataset_id=domain_owner.datasets[0].id, skip_checks=True",
            "+    )",
            "",
            "# Check if the number of available datasets has been decreased",
            "assert len(domain_owner.datasets) == 1"
        ]
    },
    {
        "number": 905,
        "comments": "update API call for refactor fix",
        "commit_message": "modified logspace to also include the dtype argument, fixing the failing unit test.\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def logspace(",
            "base=10.0,",
            "axis=None,",
            "*,",
            "+    dtype: torch.dtype,",
            "device: torch.device,",
            "out: Optional[torch.Tensor] = None,",
            "):",
            "-    power_seq = linspace(",
            "-        start, stop, num, axis, dtype=None, device=default_device(device)",
            "+    power_seq = ivy.linspace(",
            "+        start, stop, num, axis, dtype=dtype, device=ivy.default_device(device)",
            ")",
            "return base**power_seq"
        ]
    },
    {
        "number": 908,
        "comments": "add API call for type fix",
        "commit_message": "Add pytorch=1.10.0 to CI configuration (#3749)\n\n* Add pytorch=1.10.0 to CI configuration\n\n* fix:   test/espnet2/bin/test_k2_asr_inference.py\n\n* fix:   espnet2/main_funcs/pack_funcs.py\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def pack(",
            "try:",
            "import torch",
            "",
            "-        meta_objs.update(torch=torch.__version__)",
            "+        meta_objs.update(torch=str(torch.__version__))",
            "except ImportError:",
            "pass",
            "try:"
        ]
    },
    {
        "number": 909,
        "comments": "update API call for refactor fix",
        "commit_message": "Should be fixed finally\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def main():",
            "# recog",
            "logging.info('backend = ' + args.backend)",
            "if args.backend == \"chainer\":",
            "-        from espnet.lmchainer.asr_chainer import recog",
            "+        from espnet.asr.chainer.asr_chainer import recog",
            "recog(args)",
            "elif args.backend == \"pytorch\":",
            "-        from espnet.lmpytorch.asr_pytorch import recog",
            "+        from espnet.asr.pytorch.asr_pytorch import recog",
            "recog(args)",
            "else:",
            "raise ValueError(\"Only chainer and pytorch are supported.\")"
        ]
    },
    {
        "number": 910,
        "comments": "change API call for version fix",
        "commit_message": "[RLlib] DDPG refactor and Exploration API action noise classes. (#7314)\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix\n\n* WIP.\n\n* Add TD3 quick Pendulum regresison.\n\n* Cleanup.\n\n* Fix.\n\n* LINT.\n\n* Fix.\n\n* Sort quick_learning test cases, add TD3.\n\n* Sort quick_learning test cases, add TD3.\n\n* Revert test_checkpoint_restore.py (debugging) changes.\n\n* Fix old soft_q settings in documentation and test configs.\n\n* More doc fixes.\n\n* Fix test case.\n\n* Fix test case.\n\n* Lower test load.\n\n* WIP.\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Schedule(metaclass=ABCMeta):",
            "raise NotImplementedError",
            "",
            "def value(self, t):",
            "-        if self.framework == \"tf\" and tf.executing_eagerly() is False:",
            "+        if self.framework == \"tf\":",
            "return tf.cast(",
            "-                tf.py_func(self._value, [t], tf.float64),",
            "+                tf.py_function(self._value, [t], tf.float64),",
            "tf.float32,",
            "-                name=\"schedule-value\")",
            "+                name=\"schedule_value\")",
            "return self._value(t)",
            "",
            "def __call__(self, t):"
        ]
    },
    {
        "number": 911,
        "comments": "conditional warning",
        "commit_message": "misc small changes and fix #688\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def Dropout(x, *args, **kwargs):",
            "if 'is_training' in kwargs:",
            "kwargs['training'] = kwargs.pop('is_training')",
            "if len(args) > 0:",
            "-        logger.warn(",
            "-            \"The first positional argument to tensorpack.Dropout is the probability to keep rather than to drop. \"",
            "-            \"This is different from the rate argument in tf.layers.Dropout due to historical reasons. \"",
            "-            \"To mimic tf.layers.Dropout, explicitly use keyword argument 'rate' instead\")",
            "+        if args[0] != 0.5:",
            "+            logger.warn(",
            "+                \"The first positional argument to tensorpack.Dropout is the probability to keep, rather than to drop. \"",
            "+                \"This is different from the rate argument in tf.layers.Dropout due to historical reasons. \"",
            "+                \"To mimic tf.layers.Dropout, explicitly use keyword argument 'rate' instead\")",
            "rate = 1 - args[0]",
            "elif 'keep_prob' in kwargs:",
            "assert 'rate' not in kwargs, \"Cannot set both keep_prob and rate!\""
        ]
    },
    {
        "number": 912,
        "comments": "update API call for version fix",
        "commit_message": "fixed a bug of overwriting the variable\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class FeedForwardTransformer(TTSInterface, torch.nn.Module):",
            "",
            "# concat speaker embedding",
            "if self.spk_embed_dim is not None:",
            "-            spembs = torch.nn.functional.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)",
            "-            hs = self.projection(torch.cat([hs, spembs], dim=-1))",
            "+            spembs_ = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)",
            "+            hs = self.projection(torch.cat([hs, spembs_], dim=-1))",
            "",
            "# forward duration predictor and length regulator",
            "d_masks = make_pad_mask(ilens).to(xs.device)"
        ]
    },
    {
        "number": 916,
        "comments": "value change",
        "commit_message": " Upgrade to modern Python syntax (#1213)\n\n* Upgrade to modern Python syntax\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Use f-strings\n\n* Placate DeepSource Python\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def symmetrical_epipolar_distance(",
            "",
            "\"\"\"",
            "if not isinstance(Fm, torch.Tensor):",
            "-        raise TypeError(\"Fm type is not a torch.Tensor. Got {}\".format(type(Fm)))",
            "+        raise TypeError(f\"Fm type is not a torch.Tensor. Got {type(Fm)}\")",
            "",
            "if (len(Fm.shape) != 3) or not Fm.shape[-2:] == (3, 3):",
            "-        raise ValueError(\"Fm must be a (*, 3, 3) tensor. Got {}\".format(Fm.shape))",
            "+        raise ValueError(f\"Fm must be a (*, 3, 3) tensor. Got {Fm.shape}\")",
            "",
            "if pts1.size(-1) == 2:",
            "pts1 = kornia.convert_points_to_homogeneous(pts1)"
        ]
    },
    {
        "number": 918,
        "comments": "no API",
        "commit_message": "Superset Behaviour - linalg submodule (#4768)\n\n* added superset behaviour to cross, eigh, eighvalsh\n\n* fixed tests to support superset behaviour\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "eigh.unsupported_dtypes = (",
            "eigh.support_native_out = True",
            "",
            "",
            "-def eigvalsh(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "-    return torch.linalg.eigvalsh(x, out=out)",
            "+def eigvalsh(",
            "+    x: torch.Tensor, /, *, UPLO: Optional[str] = \"L\", out: Optional[torch.Tensor] = None",
            "+) -> torch.Tensor:",
            "+    return torch.linalg.eigvalsh(x, UPLO=UPLO, out=out)",
            "",
            "",
            "eigvalsh.unsupported_dtypes = ("
        ]
    },
    {
        "number": 919,
        "comments": "update param for version fix",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def image_histogram2d(",
            "hist = hist.squeeze()",
            "elif image.dim() == 3:",
            "hist = hist.squeeze(0)",
            "-    return hist, torch.zeros_like(hist, dtype=hist.dtype, device=device)",
            "+    return hist, torch.zeros_like(hist, dtype=hist.dtype, device=hist.device)"
        ]
    },
    {
        "number": 920,
        "comments": "remove API call for type fix",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class FeedForwardEncoder(Seq2SeqEncoder):",
            "return self._feedforward(inputs)",
            "else:",
            "outputs = self._feedforward(inputs)",
            "-            return outputs * mask.unsqueeze(dim=-1).float()",
            "+            return outputs * mask.unsqueeze(dim=-1)"
        ]
    },
    {
        "number": 921,
        "comments": "change API call for type fix",
        "commit_message": "Small refactor on `filters` module: Dropping JIT support (#2187)\n\n* add suport to Tensor for sigmas\n\n- Removed the functions `gaussian_blur2d_t`, `get_gaussian_kernel1d_t`, `get_gaussian_kernel2d_t`, and\n`get_gaussian_kernel3d_t` in favor of support sigmas as `Tensors`, floats, or tuple of floats on the functions without\nthe suffix `_t`\n\n* Remove crashing JIT tests related to filters\n\n* fix typing\n\n* add support to kernel size as tuple or integer\n\n* update laplacian test\n\n* update DTYPES atol and rtol\n\n- added bfloat16\n- use same values for BaseTester class and the function\n\n* add device and dtype for default gaussian\n\n* add device and dtype for discrete gaussian by erf\n\n* add device, dtype, batched for discrete gaussian\n\n- add support to compute batched kernerl for discrete gaussian by bessel functions\n- add support to pass device and dtype when sigma is a float\n\n* add device and dtype for laplacian kernels\n\n* add device and dtype for binary and box kernels\n\n* add new typping annotations\n\n- by adding `from __future__ import annotations` we can use the pyupgrade\nto update the typing annotations to be like on python 3.10\n\n* add device and dtype for static kernels (sobel)\n\n* add dtype and device for canny\n\n- fix gaussian_blur2d sigma shape\n\n* add KORNIA_CHECK API to filters\n\n* fix typing and docs\n\n* fix laplacian module kernel_size typing\n\n* remove unsqueeze in favor of pythonic slicing\n\n* update kernel size canny module\n\n* add depreciation for `*_t` functions\n\n* update blur tests\n\n* add noncontiguous tests to blur\n\n* update canny tests\n\n- add BaseTester\n- add Dynamo test\n\n* update gaussian tests\n\n- Add basetester\n- add dynamo tests\n\n* update hanning tests\n\n* update laplacian tests\n\n- Add BaseTester\n- Add dynamo test\n\n* update median tests\n\n- add BaseTester\n- add dynamo test\n\n* remove border parameter\n\n- this should be tested just on `filter2d`, `filter3d`, etc\n\n* update motion tests\n\n- Add BaseTester\n- Add dynamo\n- Add tests for 3D\n\n* update sobel tests\n\n- Add basetester\n- add dynamo tests\n\n* update unsharp mask test\n\n- add basetester\n- add dynamo test\n\n* update filter 2d and 3d tests\n\n- add basetester\n- add dynamo\n\n* fix typing\n\n* remove Pyr down and up jit tests\n\n* update atol for fp64 based on old _DTYPE_PRECISIONS values\n\n* add TODO note about dtype precision for fp64\n\n* skip filter3d with reflect border for < 1.9 torch\n\n* small fix\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def elastic_transform2d(",
            "sigma_t = sigma.to(device=device, dtype=dtype)",
            "",
            "# Get Gaussian kernel for 'y' and 'x' displacement",
            "-    kernel_x: torch.Tensor = get_gaussian_kernel2d_t(kernel_size, sigma_t[0].expand(2).unsqueeze(0))",
            "-    kernel_y: torch.Tensor = get_gaussian_kernel2d_t(kernel_size, sigma_t[1].expand(2).unsqueeze(0))",
            "+    kernel_x: torch.Tensor = get_gaussian_kernel2d(kernel_size, sigma_t[0].expand(2).unsqueeze(0))",
            "+    kernel_y: torch.Tensor = get_gaussian_kernel2d(kernel_size, sigma_t[1].expand(2).unsqueeze(0))",
            "",
            "# Convolve over a random displacement matrix and scale them with 'alpha'",
            "disp_x: torch.Tensor = noise[:, :1]"
        ]
    },
    {
        "number": 925,
        "comments": "customized API",
        "commit_message": "Barber-Agakov, and OED refactoring (#1361)\n\n* Merge changes from oed-master excluding sequential sigmoid example\n\n* Some flake8 and Python 2 changes\n\n* Indentation and noqa for lambdas\n\n* Blank __init__ file\n\n* Better docstrings, fix some tests\n\n* Revert Makefile\n\n* Some cosmetic tune-ups\n\n* Correct ewma formula in doc\n\n* A better docstring\n\n* Add from __future__ import... to all files\n\n* Better treatment of intial values\n\n* Rename variables\n\n* Rename shuffled properly\n\n* Better docstring\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def main(N, M):",
            "item_thetas = torch.tensor([[0., 0.], [0., .5], [0., 1.]])",
            "design_tensor = build_design_tensor(item_thetas, individual_assignment)",
            "print(\"Design tensor\", design_tensor)",
            "-    y = naive_rainforth(model, design_tensor, target_labels=[\"w_global\", \"w_local\"], N=N, M=M)",
            "+    y = naive_rainforth_eig(model, design_tensor, observation_labels=\"y\",",
            "+                            target_labels=[\"w\", \"u\", \"G_u\"], N=N, M=M)",
            "print(\"EIG\", y)"
        ]
    },
    {
        "number": 929,
        "comments": "add API call for type fix",
        "commit_message": "[BugFix] fix compression bugs (#5140)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def rand_like_with_shape(shape, ori_t):",
            "higher_bound = torch.max(ori_t)",
            "",
            "if dtype in [torch.uint8, torch.int16, torch.short, torch.int16, torch.long, torch.bool]:",
            "-        return torch.randint(lower_bound, higher_bound+1, shape, dtype=dtype, device=device)",
            "+        return torch.randint(lower_bound.long(), higher_bound.long() + 1, shape, dtype=dtype, device=device)",
            "else:",
            "return torch.rand(shape, dtype=dtype, device=device, requires_grad=require_grad)"
        ]
    },
    {
        "number": 930,
        "comments": "add condition check for resource fix",
        "commit_message": "More cuda fixes (#379)\n\nDefault use_cuda=torch.Tensor.is_cuda in irange and friends\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def map_data_vector_model(subsample_size):",
            "pyro.sample(\"x\", dist.normal, mu[batch], sigma[batch])",
            "return batch",
            "",
            "-    ind = Variable(torch.LongTensor(range(20)))",
            "+    LongTensor = torch.cuda.LongTensor if torch.Tensor.is_cuda else torch.LongTensor",
            "+    ind = Variable(LongTensor(range(20)))",
            "batch = pyro.map_data('mapdata', ind, local_model, batch_size=subsample_size)",
            "return list(batch.data)"
        ]
    },
    {
        "number": 933,
        "comments": "change param for math fix",
        "commit_message": "fix dropout scaling from p to 1/(1-p) (#816)\n\nCo-authored-by: Sukru Eryilmaz <seryilmaz@computelab-dgx1v-32.nvidia.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class SelfAttnFunc(torch.autograd.Function):",
            "values_grads   = torch.bmm(dropout_results.transpose(1,2), output_lin_grads, out=values_grads.transpose(0,1))",
            "",
            "# Mask and Scaling for Dropout (not a publically documented op)",
            "-        dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, dropout_prob_t[0])",
            "+        dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, 1.0/(1.0-dropout_prob_t[0]))",
            "",
            "# Softmax Grad (not a publically documented op)",
            "softmax_grads = torch._softmax_backward_data(dropout_grads, softmax_results, -1, softmax_results)"
        ]
    },
    {
        "number": 934,
        "comments": "add condition check for null fix",
        "commit_message": "refactor trainer checks (#1651)\n\n* refactor trainer checks\n\n* opt\n\n* none\n\n* Apply suggestions from code review\n\n* imports\n\n* fix tensors\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ValidationEpochEndVariations(ABC):",
            "",
            "val_acc_mean += val_acc",
            "",
            "-        val_loss_mean /= len(outputs)",
            "-        val_acc_mean /= len(outputs)",
            "+        if outputs:  # skip zero divisions",
            "+            val_loss_mean /= len(outputs)",
            "+            val_acc_mean /= len(outputs)",
            "",
            "metrics_dict = {'val_loss': val_loss_mean.item(), 'val_acc': val_acc_mean.item()}",
            "results = {'progress_bar': metrics_dict, 'log': metrics_dict}"
        ]
    },
    {
        "number": 935,
        "comments": "add param for resource fix",
        "commit_message": "TensorRT PyTorch Hub inference fix (#7560)\n\nSolution proposed in https://github.com/ultralytics/yolov5/issues/7128 to TRT PyTorch Hub CUDA illegal memory errors.\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class AutoShape(nn.Module):",
            "#   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images",
            "",
            "t = [time_sync()]",
            "-        p = next(self.model.parameters()) if self.pt else torch.zeros(1)  # for device and type",
            "+        p = next(self.model.parameters()) if self.pt else torch.zeros(1, device=self.model.device)  # for device, type",
            "autocast = self.amp and (p.device.type != 'cpu')  # Automatic Mixed Precision (AMP) inference",
            "if isinstance(imgs, torch.Tensor):  # torch",
            "with amp.autocast(autocast):"
        ]
    },
    {
        "number": 936,
        "comments": "add condition check for null fix",
        "commit_message": "Fix all stable diffusion (#1415)\n\n* up\n\n* uP\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class CycleDiffusionPipeline(DiffusionPipeline):",
            "",
            "device = torch.device(f\"cuda:{gpu_id}\")",
            "",
            "-        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.safety_checker]:",
            "+        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:",
            "if cpu_offloaded_model is not None:",
            "cpu_offload(cpu_offloaded_model, device)",
            "",
            "+        if self.safety_checker is not None:",
            "+            # TODO(Patrick) - there is currently a bug with cpu offload of nn.Parameter in accelerate",
            "+            # fix by only offloading self.safety_checker for now",
            "+            cpu_offload(self.safety_checker.vision_model)",
            "+",
            "@property",
            "# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline._execution_device",
            "def _execution_device(self):"
        ]
    },
    {
        "number": 937,
        "comments": "add param for type fix",
        "commit_message": "TF Seq2Seq int dtype fix (#13496)\n\nFixes problems with passing int64 input to TF Seq2Seq models.\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def shift_tokens_right(input_ids: tf.Tensor, pad_token_id: int, decoder_start_to",
            "",
            "if tf.executing_eagerly():",
            "# \"Verify that `labels` has only positive values and -100\"",
            "-        assert_gte0 = tf.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0))",
            "+        assert_gte0 = tf.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0, dtype=input_ids.dtype))",
            "",
            "# Make sure the assertion op is called by wrapping the result in an identity no-op",
            "with tf.control_dependencies([assert_gte0]):"
        ]
    },
    {
        "number": 940,
        "comments": "change API call for math fix",
        "commit_message": "fixes to arange\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def arange(start, stop=None, step=1, dtype=None, dev=None):",
            "if dtype in [torch.int8, torch.uint8, torch.int16]:",
            "return torch.arange(start, stop, step=step, dtype=torch.int64, device=dev).to(dtype)",
            "else:",
            "-            return torch.range(start, stop, step=step, dtype=dtype, device=dev)",
            "+            return torch.arange(start, stop, step=step, dtype=dtype, device=dev)"
        ]
    },
    {
        "number": 942,
        "comments": "doc string print error log update",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario \u0160a\u0161ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class TimesOfIndiaNewsHeadlines(datasets.GeneratorBasedBuilder):",
            "path_to_manual_file = os.path.join(os.path.abspath(os.path.expanduser(dl_manager.manual_dir)), _FILENAME)",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('times_of_india_news_headlines', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format(",
            "-                    path_to_manual_file, _FILENAME, self.manual_download_instructions",
            "-                )",
            "+                f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('times_of_india_news_headlines', data_dir=...)` that includes a file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions})\"",
            ")",
            "return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"path\": path_to_manual_file})]"
        ]
    },
    {
        "number": 944,
        "comments": "change API call for shape fix",
        "commit_message": "Speed-up warp_affine and fix bugs in RandomAffine [WIP: to add tests] (#474)\n\n* speed-up warp_affine, rotate, random_crop\n\n* added basic speed test for warp_affine\n\n* fixed centerization for random shear and bug (radians instead of degrees)\n\n* add test versus torchvision\n\n* added convert_affinematrix_to_homography function\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def _apply_affine(input: torch.Tensor,",
            "",
            "height, width = x_data.shape[-2:]",
            "transform: torch.Tensor = params['transform'].to(device, dtype)",
            "-",
            "-    out_data: torch.Tensor = warp_perspective(x_data, transform, (height, width))",
            "+    out_data: torch.Tensor = warp_affine(x_data, transform[:, :2, :], (height, width))",
            "",
            "if return_transform:",
            "return out_data.view_as(input), transform"
        ]
    },
    {
        "number": 946,
        "comments": "update API call for refactor fix",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class GPTNeoAttentionMixin:",
            "else:",
            "raise ValueError(f\"Input tensor rank should be one of [2, 3], but is: {len(tensor.shape)}\")",
            "",
            "-        padded_tensor = F.pad(tensor, padding_side, value=pad_value)",
            "+        padded_tensor = nn.functional.pad(tensor, padding_side, value=pad_value)",
            "padded_tensor = padded_tensor.unfold(dimension=1, size=window_size + block_length, step=block_length)",
            "",
            "if is_key_value:"
        ]
    },
    {
        "number": 949,
        "comments": "change param for type fix",
        "commit_message": "Update serving signatures and make sure we actually use them (#19034)\n\n* Override save() to use the serving signature as the default\n\n* Replace int32 with int64 in all our serving signatures\n\n* Remember one very important line so as not to break every test at once\n\n* Dtype fix for TFLED\n\n* dtype fix for shift_tokens_right in general\n\n* Dtype fixes in mBART and RAG\n\n* Fix dtypes for test_unpack_inputs\n\n* More dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Add a check that the model actually has a serving method\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFTransfoXLPreTrainedModel(TFPreTrainedModel):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "}",
            "]",
            ")"
        ]
    },
    {
        "number": 951,
        "comments": "change param for argument fix",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class BiattentiveClassificationNetwork(Model):",
            "# Create ELMo embeddings if applicable",
            "if self._elmo:",
            "if elmo_tokens is not None:",
            "-                elmo_representations = self._elmo(elmo_tokens)[\"elmo_representations\"]",
            "+                elmo_representations = self._elmo(elmo_tokens[\"tokens\"])[\"elmo_representations\"]",
            "# Pop from the end is more performant with list",
            "if self._use_integrator_output_elmo:",
            "integrator_output_elmo = elmo_representations.pop()"
        ]
    },
    {
        "number": 953,
        "comments": "test fix",
        "commit_message": "fix some unit tests\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def test_tagged_corpus_downsample():",
            "",
            "assert 10 == len(corpus.train)",
            "",
            "-    corpus.downsample(percentage=0.3, only_downsample_train=True)",
            "+    corpus.downsample(percentage=0.3, downsample_dev=False, downsample_test=False)",
            "",
            "assert 3 == len(corpus.train)"
        ]
    },
    {
        "number": 957,
        "comments": "change param for refactor fix",
        "commit_message": "Fix LiL sparse matrix on Tensorflow (#4173)\n\nLiL sparse matrices would not work correctly due to dtype being\ndifferent. Using the sparse_coo data fixes it.\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Function(object):",
            "if is_sparse(tensor):",
            "sparse_coo = value.tocoo()",
            "indices = np.concatenate((np.expand_dims(sparse_coo.row, 1), np.expand_dims(sparse_coo.col, 1)), 1)",
            "-                value = (indices, value.data, value.shape)",
            "+                value = (indices, sparse_coo.data, sparse_coo.shape)",
            "feed_dict[tensor] = value",
            "session = get_session()",
            "updated = session.run(self.outputs + [self.updates_op], feed_dict=feed_dict)"
        ]
    },
    {
        "number": 958,
        "comments": "version fix",
        "commit_message": "Adds model configs to ludwig.datasets (#2540)\n\n* Adds README and stub for reading dataset configs.\n\n* Adds __init__.py for configs, moves circular import into function scope in ludwig/datasets/__init__.py\n\n* Print config files in datasets folder.\n\n* First pass at automatic archive extraction.\n\n* Implemented downloading and extract.\n\n* Refactor DatasetConfig into its own file.\n\n* Fixed bugs downloading kaggle dataset.\n\n* Makes registry store dataset instances, not classes. Also comments out import_submodules for testing.\n\n* Typo fix.\n\n* Only pass data files on to load_unprocessed_dataframe, symlink directories.\n\n* Downloading dataset files into existing directory if exists.\n\n* Refactor: make datasets fully config-first, lazy load dataset loaders.\n\n* Implemented agnews custom loader.\n\n* Implements train/validation/test split by files, and globbing support\n\n* Adds _glob_multiple\n\n* Adds adult_census_income, agnews, allstate_claims_severity.\n\n* Implements sha256 verification, adds more datasets up to creditcard_fraud.\n\n* Adds checksums, dbpedia, electricity\n\n* Fixes gzip file name returned as string not list, adds up to forest_cover dataset.\n\n* Adds datasets up to reuters_r8\n\n* Adds all datasets which don't require a custom class.\n\n* Restore dataset import behavior by implementing module __getattr__\n\n* Adds KDD datasets.\n\n* Adds ieee_fraud.\n\n* Adds imbalanced_insurance, insurance_lite.\n\n* Adds mnist.\n\n* Completes implementation of all of the built-in datasets.\n\n* Made cache_dir optional, read from environment variable if set.\n\n* Upgrades datasets tests.\n\n* Adds test for new dataset config API.  Also adds scripts for dataset link checking.\n\n* Fixes loading allstate claims severity dataset.\n\n* Use @lru_cache(1), @cache not supported in python < 3.9\n\n* Deletes dataset registry, updates automl test utils\n\n* Fix imports of datasets API.\n\n* Adds more detail to sha256: docstring and basic README\n\n* Copy-paste link oops.\n\n* Fixes handling of nested archive types like .tar.bz  Also adds a LUDWIG_CACHE and export to the README\n\n* Adds link for twitter bots.\n\n* Fix order of splits in README.md\n\n* typo\n\n* Adds verify as a phase in doc string.\n\n* Support .pqt, .pq extensions for parquet.\n\n* Handle nested archives with longer file extensions like .csv.zip\n\n* Handle nested .gz types properly too.  Check all extensions with .endswith\n\n* Handle all archive types with .endswith\n\n* Update ludwig/datasets/loaders/split_loaders.py\n\nCo-authored-by: Joppe Geluykens <joppe@rvrie.com>\n\n* Adds explanation for export, fixes preserve_paths (should be relative to processed_dataset_dir)\n\n* Resolve preserved paths relative to raw dataset dir before move.\n\n* Catch runtime exception from extracting sub-archives.\n\n* Started adding info to README about dataset model configs.\n\n* Adds method to get model configs for datasets.\n\n* Adds mnist, titanic examples as default configs.\n\n* Export dataset before training.\n\n* Adds multiprocessing version of train_all_model_configs, and adds a few configs mosttly from automl experiments.\n\n* Adds a few more configs, removes AG news, training is too slow.\n\n* Default to only 4 processes due to memory constraints.\n\n* Visualize learning curves.\n\n* Started documenting API functions in readme.\n\n* Adds test for model configs API, updates tests to mock protected _load_dataset_config\n\n* Clear dataset cache after testing with mock datasets.\n\n* Adds best configs, improved README\n\n* higgs_best consistent formatting.\n\n* Update ludwig/datasets/README.md\n\nCo-authored-by: abidwael <103003638+abidwael@users.noreply.github.com>\n\n* Adds model commit hash to results.\n\n* Adds MSE, MAE to metric list.\n\n* Don't printout ludwig commit.\n\n* Increase display width to show more columns in print output.\n\n* Fix error in get_commit_hash\n\nCo-authored-by: Daniel Treiman <daniel@predibase.com>\nCo-authored-by: Joppe Geluykens <joppe@rvrie.com>\nCo-authored-by: abidwael <103003638+abidwael@users.noreply.github.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def test_download_mnist_dataset(tmpdir):",
            ")",
            "",
            "ludwig.datasets._get_dataset_configs.cache_clear()",
            "-    with mock.patch(\"ludwig.datasets.load_dataset_config\", return_value=config):",
            "+    with mock.patch(\"ludwig.datasets._load_dataset_config\", return_value=config):",
            "dataset = ludwig.datasets.get_dataset(\"mnist\", cache_dir=tmpdir)",
            "assert not dataset.state == DatasetState.DOWNLOADED",
            "assert not dataset.state == DatasetState.TRANSFORMED",
            "dataset.download()",
            "",
            "assert dataset.state == DatasetState.DOWNLOADED",
            "+    ludwig.datasets._get_dataset_configs.cache_clear()"
        ]
    },
    {
        "number": 960,
        "comments": "add param for argument fix",
        "commit_message": "minor fix of li52\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_transformer_trainable_and_decodable(model_dict):",
            "attn_dict = model.calculate_all_attentions(",
            "x[0:1], ilens[0:1], y_tgt[0:1], y_src[0:1]",
            ")",
            "-    plot.plot_multi_head_attention(data, attn_dict, \"\", savefn=_savefn)",
            "+    plot.plot_multi_head_attention(data, uttid_list, attn_dict, \"\", savefn=_savefn)",
            "",
            "# test CTC plot",
            "ctc_probs = model.calculate_all_ctc_probs("
        ]
    },
    {
        "number": 961,
        "comments": "change param for type fix",
        "commit_message": "Update serving signatures and make sure we actually use them (#19034)\n\n* Override save() to use the serving signature as the default\n\n* Replace int32 with int64 in all our serving signatures\n\n* Remember one very important line so as not to break every test at once\n\n* Dtype fix for TFLED\n\n* dtype fix for shift_tokens_right in general\n\n* Dtype fixes in mBART and RAG\n\n* Fix dtypes for test_unpack_inputs\n\n* More dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Add a check that the model actually has a serving method\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TFLayoutLMv3PreTrainedModel(TFPreTrainedModel):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "-                \"bbox\": tf.TensorSpec((None, None, 4), tf.int32, name=\"bbox\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "+                \"bbox\": tf.TensorSpec((None, None, 4), tf.int64, name=\"bbox\"),",
            "\"pixel_values\": tf.TensorSpec((None, None, None, None), tf.float32, name=\"pixel_values\"),",
            "-                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "+                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "}",
            "]",
            ")"
        ]
    },
    {
        "number": 962,
        "comments": "add API call for resource fix",
        "commit_message": "Fix gpt2 fp16 training when tracing is enabled (#20656)\n\n* ONNX tracing fix\n\n* Remove conditional\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DecisionTransformerGPT2Attention(nn.Module):",
            "# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.",
            "# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`",
            "mask_value = torch.full([], mask_value, dtype=attn_weights.dtype).to(attn_weights.device)",
            "-            attn_weights = torch.where(causal_mask, attn_weights, mask_value)",
            "+            attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)",
            "",
            "if attention_mask is not None:",
            "# Apply the attention mask"
        ]
    },
    {
        "number": 963,
        "comments": "update API call for refactor fix",
        "commit_message": "Fix and enable some tfcoreml converter convent tests\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def load_tf_graph(graph_file):",
            "\"\"\"",
            "# We load the protobuf file from the disk and parse it to retrieve the",
            "# unserialized graph_def",
            "-    with tf.gfile.GFile(graph_file, \"rb\") as f:",
            "-        graph_def = tf.GraphDef()",
            "+    with tf.io.gfile.GFile(graph_file, \"rb\") as f:",
            "+        graph_def = tf.compat.v1.GraphDef()",
            "graph_def.ParseFromString(f.read())",
            "",
            "# Then, we import the graph_def into a new Graph and returns it"
        ]
    },
    {
        "number": 967,
        "comments": "add API call for type fix",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TestScalarMix(AllenNlpTestCase):",
            "tensors = [torch.randn([3, 4, 5]) for _ in range(3)]",
            "numpy_mask = numpy.ones((3, 4), dtype=\"int32\")",
            "numpy_mask[1, 2:] = 0",
            "-        mask = torch.from_numpy(numpy_mask)",
            "+        mask = torch.from_numpy(numpy_mask).bool()",
            "",
            "weights = [0.1, 0.2, 0.3]",
            "for k in range(3):"
        ]
    },
    {
        "number": 968,
        "comments": "change param for type fix",
        "commit_message": "fix bn and rewrite saverrestore with var.load\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def get_global_step_var():",
            "with tf.variable_scope(scope, reuse=False), \\",
            "tf.name_scope(None):",
            "var = tf.get_variable(GLOBAL_STEP_OP_NAME,",
            "-                                  initializer=0,",
            "-                                  trainable=False, dtype=tf.int32)",
            "+                                  initializer=tf.constant(0, dtype=tf.int64),",
            "+                                  trainable=False, dtype=tf.int64)",
            "return var"
        ]
    },
    {
        "number": 969,
        "comments": "add API call for type fix",
        "commit_message": "Speech2TextTransformer (#10175)\n\n* s2t\n\n* fix config\n\n* conversion script\n\n* fix import\n\n* add tokenizer\n\n* fix tok init\n\n* fix tokenizer\n\n* first version working\n\n* fix embeds\n\n* fix lm head\n\n* remove extra heads\n\n* fix convert script\n\n* handle encoder attn mask\n\n* style\n\n* better enc attn mask\n\n* override _prepare_attention_mask_for_generation\n\n* handle attn_maks in encoder and decoder\n\n* input_ids => input_features\n\n* enable use_cache\n\n* remove old code\n\n* expand embeddings if needed\n\n* remove logits bias\n\n* masked_lm_loss => loss\n\n* hack tokenizer to support feature processing\n\n* fix model_input_names\n\n* style\n\n* fix error message\n\n* doc\n\n* remove inputs_embeds\n\n* remove input_embeds\n\n* remove unnecessary docstring\n\n* quality\n\n* SpeechToText => Speech2Text\n\n* style\n\n* remove shared_embeds\n\n* subsample => conv\n\n* remove Speech2TextTransformerDecoderWrapper\n\n* update output_lengths formula\n\n* fix table\n\n* remove max_position_embeddings\n\n* update conversion scripts\n\n* add possibility to do upper case for now\n\n* add FeatureExtractor and Processor\n\n* add tests for extractor\n\n* require_torch_audio => require_torchaudio\n\n* add processor test\n\n* update import\n\n* remove classification head\n\n* attention mask is now 1D\n\n* update docstrings\n\n* attention mask should be of type long\n\n* handle attention mask from generate\n\n* alwyas return attention_mask\n\n* fix test\n\n* style\n\n* doc\n\n* Speech2TextTransformer => Speech2Text\n\n* Speech2TextTransformerConfig => Speech2TextConfig\n\n* remove dummy_inputs\n\n* nit\n\n* style\n\n* multilinguial tok\n\n* fix tokenizer\n\n* add tgt_lang setter\n\n* save lang_codes\n\n* fix tokenizer\n\n* add forced_bos_token_id to tokenizer\n\n* apply review suggestions\n\n* add torchaudio to extra deps\n\n* add speech deps to CI\n\n* fix dep\n\n* add libsndfile to ci\n\n* libsndfile1\n\n* add speech to extras all\n\n* libsndfile1 -> libsndfile1\n\n* libsndfile\n\n* libsndfile1-dev\n\n* apt update\n\n* add sudo to install\n\n* update deps table\n\n* install libsndfile1-dev on CI\n\n* tuple to list\n\n* init conv layer\n\n* add model tests\n\n* quality\n\n* add integration tests\n\n* skip_special_tokens\n\n* add speech_to_text_transformer in toctree\n\n* fix tokenizer\n\n* fix fp16 tests\n\n* add tokenizer tests\n\n* fix copyright\n\n* input_values => input_features\n\n* doc\n\n* add model in readme\n\n* doc\n\n* change checkpoint names\n\n* fix copyright\n\n* fix code example\n\n* add max_model_input_sizes in tokenizer\n\n* fix integration tests\n\n* add do_lower_case to tokenizer\n\n* remove clamp trick\n\n* fix \"Add modeling imports here\"\n\n* fix copyrights\n\n* fix tests\n\n* SpeechToTextTransformer => SpeechToText\n\n* fix naming\n\n* fix table formatting\n\n* fix typo\n\n* style\n\n* fix typos\n\n* remove speech dep from extras[testing]\n\n* fix copies\n\n* rename doc file,\n\n* put imports under is_torch_available\n\n* run feat extract tests when torch is available\n\n* dummy objects for processor and extractor\n\n* fix imports in tests\n\n* fix import in modeling test\n\n* fxi imports\n\n* fix torch import\n\n* fix imports again\n\n* fix positional embeddings\n\n* fix typo in import\n\n* adapt new extractor refactor\n\n* style\n\n* fix torchscript test\n\n* doc\n\n* doc\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix docs, copied from, style\n\n* fix docstring\n\n* handle imports\n\n* remove speech from all extra deps\n\n* remove s2t from seq2seq lm mapping\n\n* better names\n\n* skip training tests\n\n* add install instructions\n\n* List => Tuple\n\n* doc\n\n* fix conversion script\n\n* fix urls\n\n* add instruction for libsndfile\n\n* fix fp16 test\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class SequenceFeatureExtractionTestMixin(FeatureExtractionSavingTestMixin):",
            "input_np = feat_extract.pad(processed_features, padding=\"longest\", return_tensors=\"np\")[input_name]",
            "input_tf = feat_extract.pad(processed_features, padding=\"longest\", return_tensors=\"tf\")[input_name]",
            "",
            "-        self.assertTrue(abs(input_np.sum() - input_tf.numpy().sum()) < 1e-2)",
            "+        self.assertTrue(abs(input_np.astype(np.float32).sum() - input_tf.numpy().sum()) < 1e-2)",
            "",
            "def test_attention_mask(self):",
            "feat_dict = self.feat_extract_dict"
        ]
    },
    {
        "number": 971,
        "comments": "add param for resource fix",
        "commit_message": "[Pytorch] pytorch only timesteps (#724)\n\n* pytorch timesteps\n\n* style\n\n* get rid of if-else\n\n* fix test\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ScoreSdeVeScheduler(SchedulerMixin, ConfigMixin):",
            "\"\"\"",
            "sampling_eps = sampling_eps if sampling_eps is not None else self.config.sampling_eps",
            "",
            "-        self.timesteps = torch.linspace(1, sampling_eps, num_inference_steps)",
            "+        self.timesteps = torch.linspace(1, sampling_eps, num_inference_steps, device=device)",
            "",
            "def set_sigmas(",
            "self, num_inference_steps: int, sigma_min: float = None, sigma_max: float = None, sampling_eps: float = None"
        ]
    },
    {
        "number": 972,
        "comments": "add API call for type fix",
        "commit_message": "fix mypy errors\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class SSIM(nn.Module):",
            "ssim_map = ((2 * mu1_mu2 + self.C1) * (2 * sigma12 + self.C2)) / \\",
            "((mu1_sq + mu2_sq + self.C1) * (sigma1_sq + sigma2_sq + self.C2))",
            "",
            "-        loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2.",
            "+        loss = torch.clamp(torch.tensor(1.) - ssim_map, min=0, max=1) / 2.",
            "",
            "if self.reduction == 'mean':",
            "loss = torch.mean(loss)"
        ]
    },
    {
        "number": 974,
        "comments": "add API call for math fix",
        "commit_message": "bugfix\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def rpn_losses(anchor_labels, anchor_boxes, label_logits, box_logits):",
            "placeholder = 1.",
            "label_loss = tf.nn.sigmoid_cross_entropy_with_logits(",
            "labels=tf.to_float(valid_anchor_labels), logits=valid_label_logits)",
            "-    label_loss = label_loss * (1. / config.RPN_BATCH_PER_IM)",
            "+    label_loss = tf.reduce_sum(label_loss) * (1. / config.RPN_BATCH_PER_IM)",
            "label_loss = tf.where(tf.equal(nr_valid, 0), placeholder, label_loss, name='label_loss')",
            "",
            "pos_anchor_boxes = tf.boolean_mask(anchor_boxes, pos_mask)"
        ]
    },
    {
        "number": 975,
        "comments": "add API call for type fix",
        "commit_message": "bug fix about calculation precision\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def test_beamformer_net_wpe_output(ch, num_spk, use_dnn_mask_for_wpe):",
            "def test_beamformer_net_bf_output(num_spk):",
            "ch = 3",
            "inputs = torch.randn(2, 16, ch)",
            "+    inputs = inputs.float()",
            "ilens = torch.LongTensor([16, 12])",
            "model = BeamformerNet(",
            "n_fft=8,"
        ]
    },
    {
        "number": 976,
        "comments": "remove API call for version fix",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "from allennlp.common.testing import AllenNlpTestCase",
            "",
            "class TestElmoLstmCell(AllenNlpTestCase):",
            "def test_elmo_lstm(self):",
            "-        input_tensor = Variable(torch.rand(4, 5, 3))",
            "+        input_tensor = torch.rand(4, 5, 3)",
            "input_tensor[1, 4:, :] = 0.",
            "input_tensor[2, 2:, :] = 0.",
            "input_tensor[3, 1:, :] = 0.",
            "-        mask = Variable(torch.ones([4, 5]))",
            "+        mask = torch.ones([4, 5])",
            "mask[1, 4:] = 0.",
            "mask[2, 2:] = 0.",
            "mask[3, 1:] = 0."
        ]
    },
    {
        "number": 977,
        "comments": "update API call for version fix",
        "commit_message": "Remove object metadata when saving SavedModel.\n\nThis change also fixes a few bugs when loading the metadata file, and fixes Keras tests so that they use model.save instead of tf.saved_model.save\n\nPiperOrigin-RevId: 378963258\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TpuStrategyTest(tf.test.TestCase):",
            "serving_fn = create_serving_signature(model)",
            "",
            "saved_model_dir = tempfile.mkdtemp(dir=self.get_temp_dir())",
            "-      tf.saved_model.save(",
            "-          model, saved_model_dir, signatures={\"serving_default\": serving_fn})",
            "+      model.save(saved_model_dir, save_format=\"tf\",",
            "+                 signatures={\"serving_default\": serving_fn})",
            "",
            "# Test the saved_model.",
            "loaded_serving_fn = tf.keras.models.load_model("
        ]
    },
    {
        "number": 984,
        "comments": "add condition check for version fix",
        "commit_message": "Convolutional layer supports float64 dtype after tensorflow 1.8.0 (#10977)\n\n* Support float64 dtype after tensorflow 1.8.0\n\n* Fix explanation message.\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def _preprocess_conv3d_input(x, data_format):",
            "# Returns",
            "A tensor.",
            "\"\"\"",
            "-    if dtype(x) == 'float64':",
            "+    # tensorflow doesn't support float64 for conv layer before 1.8.0",
            "+    if (dtype(x) == 'float64'",
            "+            and StrictVersion(tf.__version__) < StrictVersion('1.8.0')):",
            "x = tf.cast(x, 'float32')",
            "tf_data_format = 'NDHWC'",
            "if data_format == 'channels_first':"
        ]
    },
    {
        "number": 987,
        "comments": "add condition check for version fix",
        "commit_message": "add pat change (#3414)\n\n* add pat change\n\n* fix grid roi head\n\n* fix comments\n\n* clean\n\n* revert change\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class SingleRoIExtractor(BaseRoIExtractor):",
            "num_levels = len(feats)",
            "roi_feats = feats[0].new_zeros(",
            "rois.size(0), self.out_channels, *out_size)",
            "+        # TODO: remove this when parrots supports",
            "+        if torch.__version__ == 'parrots':",
            "+            roi_feats.requires_grad = True",
            "",
            "if num_levels == 1:",
            "if len(rois) == 0:"
        ]
    },
    {
        "number": 988,
        "comments": "add param for resource fix",
        "commit_message": "Dreambooth: reduce VRAM usage (#2039)\n\n* Dreambooth: use `optimizer.zero_grad(set_to_none=True)` to reduce VRAM usage\n\n* Allow the user to control `optimizer.zero_grad(set_to_none=True)` with --set_grads_to_none\n\n* Update Dreambooth readme\n\n* Fix link in readme\n\n* Fix header size in readme\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def main(args):",
            "accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)",
            "optimizer.step()",
            "lr_scheduler.step()",
            "-                optimizer.zero_grad()",
            "+                optimizer.zero_grad(set_to_none=args.set_grads_to_none)",
            "",
            "# Checks if the accelerator has performed an optimization step behind the scenes",
            "if accelerator.sync_gradients:"
        ]
    },
    {
        "number": 989,
        "comments": "update API call for refactor fix",
        "commit_message": "Fix dropout by temporarily replacing with nn.Dropout\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Tagger(nn.Module):",
            "# criterion",
            "self.crit = nn.CrossEntropyLoss(ignore_index=0) # ignore padding",
            "",
            "-        self.drop = Dropout(args['dropout'])",
            "+        self.drop = nn.Dropout(args['dropout'])",
            "self.worddrop = WordDropout(args['word_dropout'])",
            "",
            "def forward(self, word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens):"
        ]
    },
    {
        "number": 990,
        "comments": "feature change",
        "commit_message": "fixed scatter_sum call\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)",
            "def train():",
            "model.train()",
            "optimizer.zero_grad()",
            "-    pos_z, neg_z, summary = model(data.x, data.edge_index, data.edge_attr)",
            "-    loss = model.loss(pos_z, neg_z, summary)",
            "+    y = model(data.x, data.edge_index, data.edge_attr)",
            "+    loss = torch.sum(y) #TODO: actual loss function",
            "loss.backward()",
            "optimizer.step()",
            "return loss.item()"
        ]
    },
    {
        "number": 991,
        "comments": "update API call for refactor fix",
        "commit_message": "Fix torch meshgrid warnings (#20475)\n\n* fix torch meshgrid warnings\n\n* support lower torch versions\n\n* don't edit examples\n\n* dont edit examples\n\n* fix ci\n\n* fix style\n\n* rebase cleanup\n\n* fix ci again\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def build_linear_positions(index_dims, output_range=(-1.0, 1.0)):",
            "return torch.linspace(start=output_range[0], end=output_range[1], steps=n_xels_per_dim, dtype=torch.float32)",
            "",
            "dim_ranges = [_linspace(n_xels_per_dim) for n_xels_per_dim in index_dims]",
            "-    array_index_grid = torch.meshgrid(*dim_ranges)",
            "+    array_index_grid = meshgrid(*dim_ranges, indexing=\"ij\")",
            "",
            "return torch.stack(array_index_grid, dim=-1)"
        ]
    },
    {
        "number": 1007,
        "comments": "update API call for refactor fix",
        "commit_message": "use functional interface for softmax in attention (#14198)\n\n* use functional interface instead of instantiating module and immediately calling it\n\n* fix torch.nn.functional to nn.functional. Thank you Stas!\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class GPT2Attention(nn.Module):",
            "# Apply the attention mask",
            "attn_weights = attn_weights + attention_mask",
            "",
            "-        attn_weights = nn.Softmax(dim=-1)(attn_weights)",
            "+        attn_weights = nn.functional.softmax(attn_weights, dim=-1)",
            "",
            "# Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op if otherwise",
            "if attn_weights.dtype != torch.float32:"
        ]
    },
    {
        "number": 1008,
        "comments": "add param for type fix",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class BidirectionalEndpointSpanExtractor(SpanExtractor):",
            "sequence_lengths = util.get_lengths_from_binary_sequence_mask(sequence_mask)",
            "else:",
            "# shape (batch_size), filled with the sequence length size of the sequence_tensor.",
            "-            sequence_lengths = util.ones_like(sequence_tensor[:, 0, 0]).long() * sequence_tensor.size(1)",
            "+            sequence_lengths = (torch.ones_like(sequence_tensor[:, 0, 0], dtype=torch.long) *",
            "+                                sequence_tensor.size(1))",
            "",
            "# shape (batch_size, num_spans, 1)",
            "end_sentinel_mask = (exclusive_span_ends == sequence_lengths.unsqueeze(-1)).long().unsqueeze(-1)"
        ]
    },
    {
        "number": 1010,
        "comments": "doc",
        "commit_message": "Fix layer combination bug\n\nThe raw outputs of each layer need to be combined at the end, not the\nskipped inputs for the next layer.\nThanks to @keskival for pointing this out!\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class WaveNet(object):",
            "tf.histogram_summary('postprocess2_weights', w2)",
            "",
            "# We skip connections from the outputs of each layer, adding them all up here",
            "-            # We perform pairwise addition instead of using tf.add_n, so TensorFlow can free",
            "-            # the memory of previous layers",
            "total = outputs[0]",
            "for out in outputs[1:]:",
            "total += out"
        ]
    },
    {
        "number": 1013,
        "comments": "update API call for refactor fix",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ARSTFPolicy:",
            "self.num_params = sum(",
            "np.prod(variable.shape.as_list())",
            "for _, variable in self.variables.variables.items())",
            "-        self.sess.run(tf.global_variables_initializer())",
            "+        self.sess.run(tf1.global_variables_initializer())",
            "",
            "def compute_actions(self,",
            "observation,"
        ]
    },
    {
        "number": 1016,
        "comments": "change API call for resource fix",
        "commit_message": "GH-464: fix text generation on cuda:1\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class LanguageModel(nn.Module):",
            "",
            "for i in range(number_of_characters):",
            "",
            "-                if torch.cuda.is_available():",
            "-                    input = input.cuda()",
            "+                input = input.to(flair.device)",
            "",
            "# get predicted weights",
            "prediction, _, hidden = self.forward(input, hidden)"
        ]
    },
    {
        "number": 1022,
        "comments": "custom API",
        "commit_message": "[CI] Fix ci  (#21940)\n\n* fix `get_proposal_pos_embed`\n\n* fix order\n\n* style\n\n* zero shot simplify test\n\n* add approximate values for zero shot audio classification\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class DeformableDetrModel(DeformableDetrPreTrainedModel):",
            "scale = 2 * math.pi",
            "",
            "dim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=proposals.device)",
            "-        dim_t = temperature ** (2 * torch.div(dim_t, 2) / num_pos_feats)",
            "+        dim_t = temperature ** (2 * torch_int_div(dim_t, 2) / num_pos_feats)",
            "# batch_size, num_queries, 4",
            "proposals = proposals.sigmoid() * scale",
            "# batch_size, num_queries, 4, 128"
        ]
    },
    {
        "number": 1024,
        "comments": "add API call for shape fix",
        "commit_message": "[RLlib] Fix KL method of MultiCategorial tf distribution (issue #7009). (#7119)\n\n* Fix KL method of MultiCategorial tf distribution.\n\n* Fix KL method of MultiCategorial tf distribution.\n\n* Merge AsyncReplayOptimizer fixes into this branch.\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class MultiCategorical(TFActionDistribution):",
            "",
            "@override(ActionDistribution)",
            "def multi_kl(self, other):",
            "-        return [cat.kl(oth_cat) for cat, oth_cat in zip(self.cats, other.cats)]",
            "+        return tf.stack(",
            "+            [cat.kl(oth_cat) for cat, oth_cat in zip(self.cats, other.cats)],",
            "+            axis=1)",
            "",
            "@override(ActionDistribution)",
            "def kl(self, other):"
        ]
    },
    {
        "number": 1027,
        "comments": "change condition check for type fix",
        "commit_message": "DataParallel fixes (#5733)\n\n* DataParallel fixes:\n\n1. switched to a more precise check\n-        if self.args.n_gpu > 1:\n+        if isinstance(model, nn.DataParallel):\n\n2. fix tests - require the same fixup under DataParallel as the training module\n\n* another fix\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Trainer:",
            "if self.args.past_index >= 0:",
            "inputs[\"mems\"] = past",
            "# Our model outputs do not work with DataParallel, so forcing return tuple.",
            "-            if self.args.n_gpu > 1:",
            "+            if isinstance(model, nn.DataParallel):",
            "inputs[\"return_tuple\"] = True",
            "",
            "with torch.no_grad():"
        ]
    },
    {
        "number": 1028,
        "comments": "change param for math fix",
        "commit_message": "Dev pylint (#1697)\n\nFix pylint errors\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class PolicyWithValue:",
            "def sample(logits, mask_npinf):",
            "new_logits = tf.math.add(logits, mask_npinf)",
            "u = tf.random_uniform(tf.shape(new_logits), dtype=logits.dtype)",
            "-            return tf.argmax(new_logits - tf.log(-tf.log(u)), axis=-1)",
            "+            return tf.argmax(new_logits - tf.log(-1*tf.log(u)), axis=-1)",
            "",
            "def neglogp(logits, x):",
            "# return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=x)"
        ]
    },
    {
        "number": 1030,
        "comments": "add param for resource fix",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def Conv2DTranspose(",
            "if get_tf_version_tuple() <= (1, 12):",
            "kernel_initializer = tf.contrib.layers.variance_scaling_initializer(2.0),",
            "else:",
            "-            kernel_initializer = tf.keras.initializers.VarianceScaling(2.0)",
            "+            kernel_initializer = tf.keras.initializers.VarianceScaling(2.0, distribution='untruncated_normal')",
            "",
            "with rename_get_variable({'kernel': 'W', 'bias': 'b'}):",
            "layer = tf.layers.Conv2DTranspose("
        ]
    },
    {
        "number": 1033,
        "comments": "remove API call for type fix",
        "commit_message": "data augmentation pipeline\n\nSummary:\nSeq2Seq Data augmentation pipeline based on prefix with the following workflow\n\n{F207337357}\n\nThe pipeline takes about 5 hours to process music domain, the music domain output is about 100k augmented data.\n\nExample:\nf136633622\n\nfix 2 bugs:\n\n* sequence_generator max_len assertion\n* lengths in interactive.py should be python list instead of tensor\n\nReviewed By: myleott\n\nDifferential Revision: D17138089\n\nfbshipit-source-id: eaeeadd5ba81e02930a45f8873069137469925b6\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def make_batches(lines, args, task, max_positions, encode_fn):",
            ").long()",
            "for src_str in lines",
            "]",
            "-    lengths = torch.LongTensor([t.numel() for t in tokens])",
            "+    lengths = [t.numel() for t in tokens]",
            "itr = task.get_batch_iterator(",
            "dataset=task.build_dataset_for_inference(tokens, lengths),",
            "max_tokens=args.max_tokens,"
        ]
    },
    {
        "number": 1034,
        "comments": "add param for math fix",
        "commit_message": "bugfix/3185 transpose (#3252)\n\n* change t() to transpose() as xla devices do not support .t() on 1-dim tensor\n\n* detach tensor before copying\n\n* Revert \"detach tensor before copying\"\n\nThis reverts commit 37cc7bbe\n\n* changed dims\n\n* added test_result_obj_on_tpu\n\n* detach before copying\n\n* detach before copying\n\n* detach before copying\n\n* replace torch.cat with sum\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class EarlyStopping(Callback):",
            "",
            "if trainer.use_tpu:",
            "stop = torch.tensor(int(trainer.should_stop), device=pl_module.device, dtype=torch.int32)",
            "-            stop = xm.mesh_reduce(\"stop_signal\", stop, torch.cat)",
            "+            stop = xm.mesh_reduce(\"stop_signal\", stop, sum)",
            "torch_xla.core.xla_model.rendezvous(\"pl.EarlyStoppingCallback.stop_distributed_training_check\")",
            "trainer.should_stop = int(stop.item()) == trainer.world_size"
        ]
    },
    {
        "number": 1035,
        "comments": "change API call for resource fix",
        "commit_message": "Make most metrics work on GPU (#3851)\n\n* Make most metrics work on GPU\n\n* Make metric tests work on both GPU and CPU\n\n* Add a test for the test utility\n\n* mypy\n\n* Update allennlp/common/testing/test_case.py\n\nCo-Authored-By: Mark Neumann <markn@allenai.org>\n\n* Fix a PR comment\n\n* flake8\n\nCo-authored-by: Mark Neumann <markn@allenai.org>\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Entropy(Metric):",
            "mask : `torch.Tensor`, optional (default = None).",
            "A masking tensor of shape (batch_size, ...).",
            "\"\"\"",
            "-        logits, mask = self.unwrap_to_tensors(logits, mask)",
            "+        logits, mask = self.detach_tensors(logits, mask)",
            "",
            "if mask is None:",
            "-            mask = torch.ones(logits.size()[:-1])",
            "+            mask = torch.ones(logits.size()[:-1], device=logits.device)",
            "",
            "log_probs = torch.nn.functional.log_softmax(logits, dim=-1)",
            "probabilities = torch.exp(log_probs) * mask.unsqueeze(-1)"
        ]
    },
    {
        "number": 1036,
        "comments": "doc fix",
        "commit_message": "Fixed run resuming summary, deprecated warn -> warning, better keras error message\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "from itertools import chain",
            "if \"keras\" in sys.modules:",
            "if \"tensorflow.python.keras\" in sys.modules:",
            "wandb.termlog(",
            "-            \"WARNING: found both keras and tensorflow.python.keras. Use `from tensorflow import keras` and remove `import keras` to use the latest W&B features.\")",
            "+            \"Found keras and tensorflow.keras. WandbCallback will be configured for keras not tensorflow.keras.\")",
            "import keras",
            "import keras.backend as K",
            "elif \"tensorflow.python.keras\" in sys.modules:"
        ]
    },
    {
        "number": 1039,
        "comments": "add API call for type fix",
        "commit_message": "double down on dual patch norm, fix MAE and Simmim to be compatible with dual patchnorm\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class ImageEmbedder(nn.Module):",
            "",
            "self.to_patch_embedding = nn.Sequential(",
            "Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),",
            "+            nn.LayerNorm(patch_dim),",
            "nn.Linear(patch_dim, dim),",
            "+            nn.LayerNorm(dim)",
            ")",
            "",
            "self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))"
        ]
    },
    {
        "number": 1043,
        "comments": "change param for refactor fix",
        "commit_message": "Fixed Style Inconsistency (#3976)\n\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class BertForSequenceClassification(BertPreTrainedModel):",
            "",
            "self.bert = BertModel(config)",
            "self.dropout = nn.Dropout(config.hidden_dropout_prob)",
            "-        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)",
            "+        self.classifier = nn.Linear(config.hidden_size, config.num_labels)",
            "",
            "self.init_weights()"
        ]
    },
    {
        "number": 1044,
        "comments": "add param for type fix",
        "commit_message": "fix tensorflow's fmod\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def fmod(",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "result = tf.math.floormod(x1, x2, name=None)",
            "-    temp = (result, x1)",
            "-    return tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp)",
            "+    temp = [result, x1]",
            "+    return tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp, fn_output_signature=result.dtype)",
            "",
            "",
            "def fmax("
        ]
    },
    {
        "number": 1046,
        "comments": "change condition check for type fix",
        "commit_message": "[`T5`] Fix torchquant issue (#21843)\n\n* fix torchquant issue\n\n* add tests\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class MT5DenseGatedActDense(nn.Module):",
            "# To make 8bit quantization work for google/flan-t5-xxl, self.wo is kept in float32.",
            "# See https://github.com/huggingface/transformers/issues/20287",
            "# we also make sure the weights are not in `int8` in case users will force `_keep_in_fp32_modules` to be `None``",
            "-        if hidden_states.dtype != self.wo.weight.dtype and self.wo.weight.dtype != torch.int8:",
            "+        if (",
            "+            isinstance(self.wo.weight, torch.Tensor)",
            "+            and hidden_states.dtype != self.wo.weight.dtype",
            "+            and self.wo.weight.dtype != torch.int8",
            "+        ):",
            "hidden_states = hidden_states.to(self.wo.weight.dtype)",
            "",
            "hidden_states = self.wo(hidden_states)"
        ]
    },
    {
        "number": 1047,
        "comments": "use custom api",
        "commit_message": "Add fp16 support (#963)\n\n* Update imgwarp.py\n\n* update fix\n\n* move _torch_inverse_cast under utils\n\n* fix some tests in pyramid\n\n* fix issues\n\n* implement other autocasting functions\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def _scale_channel(im: torch.Tensor) -> torch.Tensor:",
            "",
            "im = im * 255",
            "# Compute the histogram of the image channel.",
            "-    histo = torch.histc(im, bins=256, min=0, max=255)",
            "+    histo = _torch_histc_cast(im, bins=256, min=0, max=255)",
            "# For the purposes of computing the step, filter out the nonzeros.",
            "nonzero_histo = torch.reshape(histo[histo != 0], [-1])",
            "step = (torch.sum(nonzero_histo) - nonzero_histo[-1]) // 255"
        ]
    },
    {
        "number": 1048,
        "comments": "add param for resource fix",
        "commit_message": "small fixes regarding device placement.\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def vector_to_skew_symmetric_matrix(vector):",
            "a2s = vector_expanded[..., 1:2, :]",
            "a3s = vector_expanded[..., 2:3, :]",
            "# BS x 1 x 1",
            "-    zs = torch.zeros(batch_shape + [1, 1])",
            "+    zs = torch.zeros(batch_shape + [1, 1], device=vector.device)",
            "# BS x 1 x 3",
            "row1 = torch.cat((zs, -a3s, a2s), -1)",
            "row2 = torch.cat((a3s, zs, -a1s), -1)"
        ]
    },
    {
        "number": 1049,
        "comments": "add comment",
        "commit_message": "Fix misstyping\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class YOLOLayer(nn.Module):",
            "w = prediction[..., 2]  # Width",
            "h = prediction[..., 3]  # Height",
            "pred_conf = torch.sigmoid(prediction[..., 4])  # Conf",
            "-        pred_cls = torch.sigmoid(prediction[..., 5:]        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor)  # Cls pred.",
            "+        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.",
            "",
            "# If grid size does not match current we compute new offsets",
            "if grid_size != self.grid_size:"
        ]
    },
    {
        "number": 1050,
        "comments": "add param for type fix",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class PointAssigner(BaseAssigner):",
            "",
            "if gt_labels is not None:",
            "assigned_labels = assigned_gt_inds.new_full((num_points, ), -1)",
            "-            pos_inds = torch.nonzero(assigned_gt_inds > 0).squeeze()",
            "+            pos_inds = torch.nonzero(",
            "+                assigned_gt_inds > 0, as_tuple=False).squeeze()",
            "if pos_inds.numel() > 0:",
            "assigned_labels[pos_inds] = gt_labels[",
            "assigned_gt_inds[pos_inds] - 1]"
        ]
    },
    {
        "number": 1052,
        "comments": "change param for shape fix",
        "commit_message": "fixed att_to_numpy() function for AttCov, AttCovLoc\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def att_to_numpy(att_ws, att):",
            "att_ws = torch.stack([aw[:, -1] for aw in att_ws], dim=1).cpu().numpy()",
            "elif isinstance(att, (AttCov, AttCovLoc)):",
            "# att_ws => list of list of previous attentions",
            "-        att_ws = torch.stack([aw[-1] for aw in att_ws], dim=1).cpu().numpy()",
            "+        att_ws = torch.stack([aw[idx] for idx, aw in enumerate(att_ws)], dim=1).cpu().numpy()",
            "elif isinstance(att, AttLocRec):",
            "# att_ws => list of tuple of attention and hidden states",
            "att_ws = torch.stack([aw[0] for aw in att_ws], dim=1).cpu().numpy()"
        ]
    },
    {
        "number": 1054,
        "comments": "change param for type fix",
        "commit_message": "Fix default dtype in HJM, HullWhite, and Heston model, as well as in PiecewiseConstant class.\n\nPiperOrigin-RevId: 387639335\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class HestonModel(generic_ito_process.GenericItoProcess):",
            "drift = tf.stack([log_spot_drift, var_drift], -1)",
            "return drift",
            "",
            "-    super(HestonModel, self).__init__(2, _drift_fn, _vol_fn, dtype, name)",
            "+    super(HestonModel, self).__init__(2, _drift_fn, _vol_fn, self._dtype, name)",
            "",
            "def sample_paths(self,",
            "times: types.RealTensor,"
        ]
    },
    {
        "number": 1056,
        "comments": "add API call for type fix",
        "commit_message": "Fix the evaluate() method in the SimilarityLearner class\n\nThe evaluate method does not have a similar function signature as other models, which is causing errors when training it.\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class SimilarityLearner(flair.nn.Model):",
            "epoch_results_str,",
            "detailed_results,",
            "),",
            "-            0,",
            "+            torch.tensor(0),",
            ")",
            "",
            "def _get_state_dict(self):"
        ]
    },
    {
        "number": 1059,
        "comments": "add condition check for null fix",
        "commit_message": "Add final_layer_norm to OPT model (#17785)\n\n* Add final_layer_norm to OPT model\n\n* Add JAX and TF version\n\n* Fix Keras name\n\n* Woops\n\n* Allow for non breaking change\n\n* Apply suggestions from code review\n\n* add tests\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class TFOPTDecoder(tf.keras.layers.Layer):",
            "if output_attentions:",
            "all_self_attns += (layer_self_attn,)",
            "",
            "+        if self.final_layer_norm is not None:",
            "+            hidden_states = self.final_layer_norm(hidden_states)",
            "+",
            "if self.project_out is not None:",
            "hidden_states = self.project_out(hidden_states)"
        ]
    },
    {
        "number": 1060,
        "comments": "change API call for type fix",
        "commit_message": "Fixing python2 logging for extract_features.py\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def convert_examples_to_features(examples, seq_length, tokenizer):",
            "if ex_index < 5:",
            "tf.logging.info(\"*** Example ***\")",
            "tf.logging.info(\"unique_id: %s\" % (example.unique_id))",
            "-      tf.logging.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))",
            "+      tf.logging.info(\"tokens: %s\" % \" \".join(",
            "+          [tokenization.printable_text(x) for x in tokens]))",
            "tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))",
            "tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))",
            "tf.logging.info("
        ]
    },
    {
        "number": 1064,
        "comments": "add param for argument fix",
        "commit_message": "Fixed module resolution for tf.keras optimizers and added unit tests (#1935)\n\nSigned-off-by: Travis Addair <taddair@uber.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def load_model(filepath, custom_optimizers=None, custom_objects=None, compressio",
            "\"\"\"",
            "def wrap_optimizer(cls):",
            "return lambda **kwargs: DistributedOptimizer(cls(**kwargs), compression=compression)",
            "-    return _impl.load_model(keras, wrap_optimizer, filepath, custom_optimizers, custom_objects)",
            "+    optimizer_modules = {keras.optimizers.Optimizer.__module__}",
            "+    return _impl.load_model(keras, wrap_optimizer, optimizer_modules, filepath, custom_optimizers, custom_objects)"
        ]
    },
    {
        "number": 1065,
        "comments": "add API call for shape fix",
        "commit_message": "1. Add time dependent boundary condition to the PDE solver\n2. Remove numpy dependency of the PDE solver implementation.\n2. Fix a bug in 1-d PDE solver for the default boundary to work with a batch of PDEs.\n\nPiperOrigin-RevId: 371112748\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class ParabolicEquationStepperTest(tf.test.TestCase, parameterized.TestCase):",
            "second_order_coeff_fn=second_order_coeff_fn,",
            "inner_first_order_coeff_fn=inner_first_order_coeff_fn)[0]",
            "",
            "-    true_values = tf.math.exp(final_t + grid[0])",
            "+    true_values = tf.expand_dims(tf.math.exp(final_t + grid[0]), axis=0)",
            "self.assertAllClose(",
            "est_values, true_values, atol=1e-2, rtol=1e-2)"
        ]
    },
    {
        "number": 1066,
        "comments": "change API call for math fix",
        "commit_message": "[rllib] Fix atari reward calculations, add LR annealing, explained var stat for A2C / impala (#2700)\n\nChanges needed to reproduce Atari plots in IMPALA / A2C: https://github.com/ray-project/rl-experiments\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class LocalMultiGPUOptimizer(PolicyOptimizer):",
            "else:",
            "rnn_inputs = []",
            "self.par_opt = LocalSyncParallelOptimizer(",
            "-                        tf.train.AdamOptimizer(",
            "-                            self.sgd_stepsize), self.devices,",
            "+                        self.policy.optimizer(), self.devices,",
            "[v for _, v in self.policy.loss_inputs()], rnn_inputs,",
            "self.per_device_batch_size, self.policy.copy,",
            "os.getcwd())"
        ]
    },
    {
        "number": 1067,
        "comments": "update API call for version fix",
        "commit_message": "fix multigpu training\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DistributedReplicatedBuilder(DataParallelBuilder):",
            "return grads",
            "",
            "# Ngpu * Nvar * 2",
            "-        grad_list = self.build_on_multi_tower(",
            "-            get_grads,",
            "+        grad_list = DataParallelBuilder.build_on_towers(",
            "+            self.towers, get_grads,",
            "devices=self.raw_devices,",
            "use_vs=[True] * len(self.towers))  # open vs at each tower",
            "DataParallelBuilder._check_grad_list(grad_list)"
        ]
    },
    {
        "number": 1069,
        "comments": "add API call for resource fix",
        "commit_message": "Fix CIs for PyTorch 1.13 (#20686)\n\n* fix 1\n\n* fix 2\n\n* fix 3\n\n* fix 4\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class OPTForSequenceClassification(OPTPreTrainedModel):",
            "sequence_lengths = -1",
            "else:",
            "if input_ids is not None:",
            "-                sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1",
            "+                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)",
            "else:",
            "sequence_lengths = -1",
            "logger.warning("
        ]
    },
    {
        "number": 1072,
        "comments": "change API call for math fix",
        "commit_message": "Rename .reshape(s,n) -> .expand_by(s).independent(n) (#1016)\n\n* Start to rename .reshape()\n\n* Remove the .reshape() method entirely\n\n* Add .reshape() with informative error message\n\n* Fix test\n\n* Fix failing test\n\n* Fix failing test\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def test_discrete_parallel(continuous_class):",
            "",
            "def model(data):",
            "weights = pyro.sample('weights', dist.Dirichlet(0.5 * torch.ones(K)))",
            "-        locs = pyro.sample('locs', dist.Normal(0, 10).reshape([K], extra_event_dims=1))",
            "+        locs = pyro.sample('locs', dist.Normal(0, 10).expand_by([K]).independent(1))",
            "scale = pyro.sample('scale', dist.LogNormal(0, 1))",
            "",
            "with pyro.iarange('data', len(data)):"
        ]
    },
    {
        "number": 1073,
        "comments": "refactor",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class FBetaMeasure(Metric):",
            "self._total_sum = torch.zeros(num_classes, device=predictions.device)",
            "",
            "if mask is None:",
            "-            mask = torch.ones_like(gold_labels)",
            "-        mask = mask.to(dtype=torch.bool)",
            "+            mask = torch.ones_like(gold_labels).bool()",
            "gold_labels = gold_labels.float()",
            "",
            "argmax_predictions = predictions.max(dim=-1)[1].float()",
            "-        true_positives = (gold_labels == argmax_predictions) * mask",
            "+        true_positives = (gold_labels == argmax_predictions) & mask",
            "true_positives_bins = gold_labels[true_positives]",
            "",
            "# Watch it:"
        ]
    },
    {
        "number": 1077,
        "comments": "add API call for type fix",
        "commit_message": "Fix compatibility with PyTorch 1.0.x (Fixes #906)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/910\n\nDifferential Revision: D16536532\n\nPulled By: myleott\n\nfbshipit-source-id: 56bb5570e70b5670ad87c64d9dd20c64c1fa9f5c\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class MaskTokensDataset(BaseWrapperDataset):",
            "if self.mask_whole_words is not None:",
            "mask = np.repeat(mask, word_lens)",
            "new_item = np.full(len(mask), self.pad_idx)",
            "-                new_item[mask] = item[torch.from_numpy(mask)]",
            "+                new_item[mask] = item[torch.from_numpy(mask.astype(np.uint8))]",
            "return torch.from_numpy(new_item)",
            "",
            "# decide unmasking and random replacement"
        ]
    },
    {
        "number": 1079,
        "comments": "change API call for refactor fix",
        "commit_message": "Add a OneTwoMatching distribution (#2697)\n\n* Implement OneTwoMatching.enumerate_support()\n\n* Clarify docs\n\n* Sketch BP computation of Bethe free energy\n\n* Add large-scale smoke test\n\n* Fix some bugs\n\n* Fix sign error\n\n* Simplify\n\n* Add float32 tests\n\n* Clarify; add debug statements\n\n* Relax test, clamp logits\n\n* Optimize clamping logic\n\n* Simplify Z2 computation\n\n* Avoid splitting logits in half\n\n* Add naive sampling algorithm\n\n* Add a more accurate mean field algorithm\n\n* Fix tests\n\n* Simplify\n\n* Change temperature, tighten tests\n\n* Remove unused algorithms\n\n* Update docs and comments\n\n* Numerically stabilize\n\n* Improve accuraccy\n\n* Remove heuristic\n\n* Reduce memory footprint\n\n* Improve accuracy of free energy formula\n\n* Fuse ops\n\n* Implement OneTwoMatching.mode()\n\n* Simplify\n\n* Simplify test\n\n* Simplify and add a test\n\n* Fix docs\n\n* Add Sinkhorn preconditioner\n\n* Add a .mode() method using lap\n\n* Fix in-place op\n\n* Implement .sample() via perturb-and-map\n\n* Remove bad .sample() implementation\n\n* Add link to docs\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def _sample_coalescent_times(leaf_times):",
            "coal_times.append(t)",
            "coal_times.reverse()",
            "",
            "-    return torch.tensor(coal_times)",
            "+    return proto.new_tensor(coal_times)"
        ]
    },
    {
        "number": 1080,
        "comments": "def",
        "commit_message": "python2 fixes\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def transform(point, center, scale, resolution, invert=False):",
            "return new_point.int()",
            "",
            "",
            "-def crop(image, center, scale, resolution=256):",
            "+def crop(image, center, scale, resolution=256.0):",
            "# Crop around the center point",
            "\"\"\" Crops the image around the center. Input is expected to be an np.ndarray \"\"\"",
            "ul = transform([1, 1], center, scale, resolution, True)",
            "br = transform([resolution, resolution], center, scale, resolution, True)",
            "-    pad = math.ceil(torch.norm((ul - br).float()) / 2 - (br[0] - ul[0]) / 2)",
            "+    pad = math.ceil(torch.norm((ul - br).float()) / 2.0 - (br[0] - ul[0]) / 2.0)",
            "if image.ndim > 2:",
            "newDim = np.array([br[1] - ul[1], br[0] - ul[0],",
            "image.shape[2]], dtype=np.int32)"
        ]
    },
    {
        "number": 1083,
        "comments": "add param for resource fix",
        "commit_message": "Make most metrics work on GPU (#3851)\n\n* Make most metrics work on GPU\n\n* Make metric tests work on both GPU and CPU\n\n* Add a test for the test utility\n\n* mypy\n\n* Update allennlp/common/testing/test_case.py\n\nCo-Authored-By: Mark Neumann <markn@allenai.org>\n\n* Fix a PR comment\n\n* flake8\n\nCo-authored-by: Mark Neumann <markn@allenai.org>\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class CategoricalAccuracy(Metric):",
            "# ith entry in gold_labels points to index (0-num_classes) for ith row in max_predictions",
            "# For each row check if index pointed by gold_label is was 1 or not (among max scored classes)",
            "correct = max_predictions_mask[",
            "-                torch.arange(gold_labels.numel()).long(), gold_labels",
            "+                torch.arange(gold_labels.numel(), device=gold_labels.device).long(), gold_labels",
            "].float()",
            "tie_counts = max_predictions_mask.sum(-1)",
            "correct /= tie_counts.float()"
        ]
    },
    {
        "number": 1087,
        "comments": "rename",
        "commit_message": "Fix tensorflow implementation\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def proposal_layer_tf(rpn_cls_prob, rpn_bbox_pred, im_info, cfg_key, _feat_strid",
            "proposals = bbox_transform_inv_tf(anchors, rpn_bbox_pred)",
            "proposals = clip_boxes_tf(proposals, im_info[:2])",
            "",
            "-  indices = tf.image.non_max_suppression(rpn_bbox_pred, scores, max_output_size=post_nms_topN, iou_threshold=nms_thresh)",
            "+  indices = tf.image.non_max_suppression(proposals, scores, max_output_size=post_nms_topN, iou_threshold=nms_thresh)",
            "",
            "-  boxes = tf.gather(rpn_bbox_pred, indices)",
            "+  boxes = tf.gather(proposals, indices)",
            "boxes = tf.to_float(boxes)",
            "scores = tf.gather(scores, indices)",
            "scores = tf.reshape(scores, shape=(-1, 1))"
        ]
    },
    {
        "number": 1091,
        "comments": "add param for argument fix",
        "commit_message": "re-organize predict/; fix TF incompatibile change of sparse_softmax_cross_entropy_loss\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Model(ModelDesc):",
            ".GlobalAvgPooling('gap')",
            ".FullyConnected('linear', 1000, nl=tf.identity)())",
            "",
            "-        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)",
            "+        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
            "loss = tf.reduce_mean(loss, name='xentropy-loss')",
            "",
            "wrong = prediction_incorrect(logits, label, 1, name='wrong-top1')"
        ]
    },
    {
        "number": 1094,
        "comments": "change API call for version fix",
        "commit_message": "Fix the //third_party/py/keras/distribute:minimize_loss_test that fails on local.\n\nThe compat.v1.layer is populated from keras/legacy_tf_layers, which need to be imported differently.\n\nPiperOrigin-RevId: 399749315\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "def batchnorm_example(optimizer_fn,",
            "for z in range(batch_per_epoch)]).repeat()",
            "",
            "optimizer = optimizer_fn()",
            "-  batchnorm = tf.compat.v1.layers.BatchNormalization(",
            "+  batchnorm = normalization.BatchNormalization(",
            "renorm=renorm, momentum=momentum, fused=False)",
            "-  layer = tf.compat.v1.layers.Dense(1, use_bias=False)",
            "+  layer = core.Dense(1, use_bias=False)",
            "",
            "def model_fn(x):",
            "\"\"\"A model that uses batchnorm.\"\"\""
        ]
    },
    {
        "number": 1100,
        "comments": "add condition check for state fix",
        "commit_message": "Various fix including:\n- Forbid several Torchhook\n- Fix or alter tests\n- Fix create_pointer()\nTODO: There is an issue with the .get() function (just run unittest to see...)\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TorchHook(object):",
            "",
            "self._hook_torch_module()",
            "",
            "+        if torch.torch_hooked > 0:",
            "+            raise Exception('Torch was already hooked')",
            "+",
            "def _hook_native_tensors_and_variables(self, tensor_type):",
            "\"\"\"Overloading a given tensor_type\"\"\"",
            "# Overload 'special' methods here"
        ]
    },
    {
        "number": 1101,
        "comments": "doc update",
        "commit_message": "[Pipeline] fix failing bloom `pipeline` test (#20778)\n\nfix failing `pipeline` test\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class TextGenerationPipelineTests(unittest.TestCase, metaclass=PipelineTestCaseM",
            "],",
            ")",
            "",
            "-        # torch_dtype not necessary",
            "+        # torch_dtype will be automatically set to float32 if not provided - check: https://github.com/huggingface/transformers/pull/20602",
            "pipe = pipeline(model=\"hf-internal-testing/tiny-random-bloom\", device_map=\"auto\")",
            "self.assertEqual(pipe.model.device, torch.device(0))",
            "-        self.assertEqual(pipe.model.lm_head.weight.dtype, torch.bfloat16)",
            "+        self.assertEqual(pipe.model.lm_head.weight.dtype, torch.float32)",
            "out = pipe(\"This is a test\")",
            "self.assertEqual(",
            "out,"
        ]
    },
    {
        "number": 1104,
        "comments": "change API call for math fix",
        "commit_message": "fixed and improved shape handling for exploration\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class EpsilonDecay(Exploration):",
            "",
            "pred = tf.logical_or(x=(timestep < self.start_timestep),",
            "y=(timestep > self.start_timestep + int(self.timesteps)))",
            "-        return tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn)",
            "+        return tf.fill(dims=shape, value=tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))"
        ]
    },
    {
        "number": 1108,
        "comments": "def",
        "commit_message": "[RLlib] Offline Type Annotations (#9676)\n\n* Offline Annotations\n\n* Modifications\n\n* Fixed circular dependencies\n\n* Linter fix\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class _QueueRunner(threading.Thread):",
            "self.placeholders = [tf1.placeholder(dtype) for dtype in dtypes]",
            "self.enqueue_op = queue.enqueue(dict(zip(keys, self.placeholders)))",
            "",
            "-    def enqueue(self, batch):",
            "+    def enqueue(self, batch: SampleBatchType):",
            "data = {",
            "self.placeholders[i]: batch[key]",
            "for i, key in enumerate(self.keys)"
        ]
    },
    {
        "number": 1110,
        "comments": "format",
        "commit_message": "Fix various typos in error messages.\n\nPiperOrigin-RevId: 410160508\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class Sequential(functional.Functional):",
            "# invalid use case of Sequential, but we tolerate it for backwards",
            "# compatibility.",
            "self._use_legacy_deferred_behavior = True",
            "-        self._build_input_shape = tf.nest.map_structure(_get_shape_tuple, inputs)",
            "+        self._build_input_shape = tf.nest.map_structure(",
            "+            _get_shape_tuple, inputs)",
            "if tf.__internal__.tf2.enabled():",
            "logging.warning('Layers in a Sequential model should only have a '",
            "-                          'single input tensor, but we receive a %s input: %s'",
            "-                          '\\nConsider rewriting this model with the Functional '",
            "-                          'API.' % (type(inputs), inputs))",
            "+                          f'single input tensor. Received: inputs={inputs}. '",
            "+                          'Consider rewriting this model with the Functional '",
            "+                          'API.')",
            "else:",
            "self._build_graph_network_for_inferred_shape(inputs.shape, inputs.dtype)"
        ]
    },
    {
        "number": 1114,
        "comments": "remove param for math fix",
        "commit_message": "fix\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class Stft(torch.nn.Module, InversibleInterface):",
            "pad = self.n_fft // 2",
            "ilens = ilens + 2 * pad",
            "",
            "-            olens = (",
            "-                torch.div((ilens - self.n_fft), self.hop_length, rounding_mode=\"floor\")",
            "-                + 1",
            "-            )",
            "+            olens = torch.div((ilens - self.n_fft), self.hop_length) + 1",
            "output.masked_fill_(make_pad_mask(olens, output, 1), 0.0)",
            "else:",
            "olens = None"
        ]
    },
    {
        "number": 1118,
        "comments": "add param for type fix",
        "commit_message": "[Feat] Enabled Torch1.5.1 cpu support (#796)\n\n* Added py151 support\n\n* Enabled 1.5.1 CI\n\n* Typo fix\n\n* Fixed Equalize\n\n* Update setup.py\n\n* Bug fix\n\n* typo fix\n\n* Fixed mypy check\n\n* Update tests_cpu_versions.yml\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "def _scale_channel(im: torch.Tensor) -> torch.Tensor:",
            "# and then normalization by step.",
            "lut = (torch.cumsum(histo, 0) + (step // 2)) // step",
            "# Shift lut, prepending with 0.",
            "-        lut = torch.cat([torch.zeros(1, device=lut.device), lut[:-1]])",
            "+        lut = torch.cat([torch.zeros(1, device=lut.device, dtype=lut.dtype), lut[:-1]])",
            "# Clip the counts to be in range.  This is done",
            "# in the C code for image.point.",
            "return torch.clamp(lut, 0, 255)"
        ]
    },
    {
        "number": 1122,
        "comments": "add param for type fix",
        "commit_message": "[Swin, Swinv2] Fix attn_mask dtype (#18803)\n\n* Add dtype\n\n* Fix Swinv2 as well\n\nCo-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class DonutSwinLayer(nn.Module):",
            "# partition windows",
            "hidden_states_windows = window_partition(shifted_hidden_states, self.window_size)",
            "hidden_states_windows = hidden_states_windows.view(-1, self.window_size * self.window_size, channels)",
            "-        attn_mask = self.get_attn_mask(height_pad, width_pad)",
            "+        attn_mask = self.get_attn_mask(height_pad, width_pad, dtype=hidden_states.dtype)",
            "if attn_mask is not None:",
            "attn_mask = attn_mask.to(hidden_states_windows.device)"
        ]
    },
    {
        "number": 1126,
        "comments": "add API call for math fix",
        "commit_message": "Fix mode regularized gan loss\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "for it in range(1000000):",
            "D_reg = D(G_sample_reg)",
            "",
            "mse = torch.sum((X - G_sample_reg)**2, 1)",
            "-    E_loss = torch.mean(lam1 * mse + lam2 * D_reg)",
            "+    E_loss = torch.mean(lam1 * mse + lam2 * log(D_reg))",
            "",
            "E_loss.backward()",
            "E_solver.step()"
        ]
    },
    {
        "number": 1127,
        "comments": "change param for resource fix",
        "commit_message": "Fix VisualBert Embeddings (#13017)\n\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class VisualBertEmbeddings(nn.Module):",
            "inputs_embeds = self.word_embeddings(input_ids)",
            "",
            "if token_type_ids is None:",
            "-            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.input_embeds.device)",
            "+            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)",
            "",
            "token_type_embeddings = self.token_type_embeddings(token_type_ids)"
        ]
    },
    {
        "number": 1128,
        "comments": "remove API call for type fix",
        "commit_message": "Fix BCELoss adressing  #1192\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class BCELossMasked(nn.Module):",
            "Returns:",
            "loss: An average loss value in range [0, 1] masked by the length.",
            "\"\"\"",
            "-        # mask: (batch, max_len, 1)",
            "target.requires_grad = False",
            "if length is not None:",
            "-            mask = sequence_mask(sequence_length=length, max_len=target.size(1)).float()",
            "-            x = x * mask",
            "-            target = target * mask",
            "+            # mask: (batch, max_len, 1)",
            "+            mask = sequence_mask(sequence_length=length, max_len=target.size(1))",
            "num_items = mask.sum()",
            "+            loss = functional.binary_cross_entropy_with_logits(x.masked_select(mask), target.masked_select(mask), pos_weight=self.pos_weight, reduction=\"sum\")",
            "else:",
            "+            loss = functional.binary_cross_entropy_with_logits(x, target, pos_weight=self.pos_weight, reduction=\"sum\")",
            "num_items = torch.numel(x)",
            "-        loss = functional.binary_cross_entropy_with_logits(x, target, pos_weight=self.pos_weight, reduction=\"sum\")",
            "loss = loss / num_items",
            "return loss"
        ]
    },
    {
        "number": 1129,
        "comments": "change API call for refactor fix",
        "commit_message": "summaries updated and fixed\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class PrioritizedReplay(Memory):",
            "))",
            "",
            "with tf.control_dependencies(control_inputs=assignments):",
            "-            return tf.no_op()",
            "+            return util.no_operation()",
            "",
            "# These are not supported for prioritized replay currently.",
            "def tf_retrieve_episodes(self, n):"
        ]
    },
    {
        "number": 1130,
        "comments": "no API",
        "commit_message": "[Retiarii] Serializer and experiment status fixes (#3421)\n\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def get_module_name(cls):",
            "f'please launch the experiment under the directory where \"{main_file_path.name}\" is located.')",
            "module_name = main_file_path.stem",
            "break",
            "+    if module_name == '__main__':",
            "+        warnings.warn('Callstack exhausted but main module still not found. This will probably cause issues that the '",
            "+                      'function/class cannot be imported.')",
            "",
            "# NOTE: this is hacky. As torchscript retrieves LSTM's source code to do something.",
            "# to make LSTM's source code can be found, we should assign original LSTM's __module__ to",
            "# the wrapped LSTM's __module__",
            "# TODO: find out all the modules that have the same requirement as LSTM",
            "-    if f'{cls.__module__}.{cls.__name__}' == 'torch.nn.modules.rnn.LSTM':",
            "-        module_name = cls.__module__",
            "+    if f'{cls_or_func.__module__}.{cls_or_func.__name__}' == 'torch.nn.modules.rnn.LSTM':",
            "+        module_name = cls_or_func.__module__",
            "",
            "return module_name",
            "",
            "",
            "-def get_full_class_name(cls, relocate_module=False):",
            "+def get_importable_name(cls, relocate_module=False):",
            "module_name = get_module_name(cls) if relocate_module else cls.__module__",
            "return module_name + '.' + cls.__name__"
        ]
    },
    {
        "number": 1132,
        "comments": "add param for type fix",
        "commit_message": "\ud83c\udfcb  Fix nan issue in training fastspeech/fastspeech2.\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TFFastSpeech(tf.keras.Model):",
            "== config.decoder_self_attention_params.hidden_size,",
            "name=\"decoder\",",
            ")",
            "-        self.mel_dense = tf.keras.layers.Dense(units=config.num_mels, name=\"mel_before\")",
            "-        self.postnet = TFTacotronPostnet(config=config, name=\"postnet\")",
            "+        self.mel_dense = tf.keras.layers.Dense(units=config.num_mels, dtype=tf.float32, name=\"mel_before\")",
            "+        self.postnet = TFTacotronPostnet(config=config, dtype=tf.float32, name=\"postnet\")",
            "",
            "self.setup_inference_fn()"
        ]
    },
    {
        "number": 1136,
        "comments": "for loop",
        "commit_message": "model fixes + ipnb fixes\n\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "def main():",
            "",
            "model.eval()",
            "with open(args.output_file, \"w\", encoding='utf-8') as writer:",
            "-        for input_ids, input_mask, segment_ids, example_indices in eval_dataloader:",
            "+        for input_ids, input_mask, example_indices in eval_dataloader:",
            "input_ids = input_ids.to(device)",
            "input_mask = input_mask.float().to(device)",
            "-            segment_ids = segment_ids.to(device)",
            "",
            "-            all_encoder_layers, _ = model(input_ids, segment_ids, input_mask)",
            "+            all_encoder_layers, _ = model(input_ids, token_type_ids=None, attention_mask=input_mask)",
            "",
            "for enc_layers, example_index in zip(all_encoder_layers, example_indices):",
            "feature = features[example_index.item()]"
        ]
    },
    {
        "number": 1140,
        "comments": "add API call for state fix",
        "commit_message": "Fix for restore from checkpoint\n\nResets the buffer index upon restore, ensuring the model is consistent\nwith the agent's behavior - starting a new episode.\n\nThis fixes the following error which occurs on attempt to restore:\n    InvalidArgumentError (see above for traceback): Must have updates.shape = indices.shape + params.shape[1:], got updates.shape [30,1], indices.shape [21], params.shape [100,1]\n  \t[[Node: ppo/observe-timestep/store/ScatterUpdate = ScatterUpdate[T=DT_FLOAT, Tindices=DT_INT32, _class=[\"loc:@ppo/initialize/latest/initialize/state-state\"], use_locking=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ppo/initialize/latest/initialize/state-state, ppo/observe-timestep/store/mod, ppo/strided_slice, ^ppo/observe-timestep/store/AssignSub)]]\n\n",
        "label": "yes",
        "answer": "no",
        "change": [
            "class Model(object):",
            "#     raise TensorForceError(\"Invalid model directory/file.\")",
            "",
            "self.saver.restore(sess=self.session, save_path=file)",
            "+        self.session.run(self.buffer_index_reset_op)",
            "",
            "def get_components(self):",
            "\"\"\""
        ]
    },
    {
        "number": 1141,
        "comments": "customize API",
        "commit_message": "enh: Implements `InferenceModule` as a pipelined module with separate preprocessor, predictor, and postprocessor modules (#2105)\n\n* Adding inference pipeline with seperate pre-processing, predict and post-processing modules\n\n* Update to flatten outputs from predict consistent to support triton\n\n* inference module refactor\n\n* add back InferenceLudwigModel\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* unify modules into inference.py\n\n* cleaned up inaccurate documentation\n\n* clean up\n\n* clean up type hints and update InferenceLudwigModel\n\n* clean up type hint; passes test_torchscript.py\n\n* added typing to inference module for clarity\n\n* remove inference_module_file_name constant\n\n* unified predict module with postproc\n\n* removed InferencePredictor entirely\n\n* add back the old inference module\n\n* add back training set metadata\n\n* revert change to predict module, move feature filtering to postproc\n\n* cleanup inference_module_v0\n\n* cleanup\n\n* adds device placement to InferenceLudwigModel\n\n* adds ability to save/load torchscript on particular devices\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* allows saving torchscript with dict of devices from api.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* correct device inputs\n\n* refactor to expose inference stages (prep for triton refactor)\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove magic 'cpu' string\n\n* remove extraneous constants\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add from_directory classmethod for e2e users\n\n* merge\n\n* merge InferenceModule and InferenceLudwigModel\n\n* add comment\n\n* revert small change\n\n* cleanup\n\n* add to_torchscript functionality\n\n* cleanup\n\n* pushes device logic down into inference stages\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* move device placement upstream to inference module to ensure stage modules are performant\n\n* adds logs for device placement experiments\n\n* removes logs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove stage_to_dict\n\n* clean up how we get input device in predictor_forward\n\n* first commit\n\n* wip\n\n* updated interfaces\n\n* postproc GPU\n\n* add intelligent device placement\n\n* clean up device api\n\n* revert flatten op in inference_module_v0\n\n* remove dtype workaround\n\n* benchmarking code\n\n* add DEVICE constant as good default for loading/saving\n\n* added helpful logging and style\n\n* cleanup\n\n* cleanup, adding docstrings\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* docstring\n\nCo-authored-by: Geoffrey Angus <geoffrey@predibase.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "no",
        "answer": "yes",
        "change": [
            "class H3FeatureMixin(BaseFeatureMixin):",
            "):",
            "column = input_df[feature_config[COLUMN]]",
            "if column.dtype == object:",
            "-            column = column.map(int)",
            "-        column = column.map(H3FeatureMixin.h3_to_list)",
            "+            column = backend.df_engine.map_objects(column, int)",
            "+        column = backend.df_engine.map_objects(column, H3FeatureMixin.h3_to_list)",
            "",
            "proc_df[feature_config[PROC_COLUMN]] = backend.df_engine.map_objects(",
            "column, lambda x: np.array(x, dtype=np.uint8)"
        ]
    },
    {
        "number": 1142,
        "comments": "add API call for math fix",
        "commit_message": "Texture Atlas sampling bug fix\n\nSummary: Fixes the index out of bound errors for texture sampling from a texture atlas: when barycentric coordinates are 1.0, the integer index into the (R, R) per face texture map is R (max can only be R-1).\n\nReviewed By: gkioxari\n\nDifferential Revision: D25543803\n\nfbshipit-source-id: 82d0935b981352b49c1d95d5a17f9cc88bad0a82\n\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class TexturesAtlas(TexturesBase):",
            "# pyre-fixme[16]: `bool` has no attribute `__getitem__`.",
            "mask = (pix_to_face < 0)[..., None]",
            "bary_w01 = torch.where(mask, torch.zeros_like(bary_w01), bary_w01)",
            "-        w_xy = (bary_w01 * R).to(torch.int64)  # (N, H, W, K, 2)",
            "+        # If barycentric coordinates are > 1.0 (in the case of",
            "+        # blur_radius > 0.0), wxy might be > R. We need to clamp this",
            "+        # index to R-1 to index into the texture atlas.",
            "+        w_xy = (bary_w01 * R).to(torch.int64).clamp(max=R - 1)  # (N, H, W, K, 2)",
            "",
            "below_diag = (",
            "bary_w01.sum(dim=-1) * R - w_xy.float().sum(dim=-1)"
        ]
    },
    {
        "number": 1144,
        "comments": "update API call for version fix",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "yes",
        "answer": "yes",
        "change": [
            "class VariationalSparseGP(GPModel):",
            "M = self.Xu.size(0)",
            "Kuu = self.kernel(self.Xu).contiguous()",
            "Kuu.view(-1)[::M + 1] += self.jitter  # add jitter to the diagonal",
            "-        Luu = Kuu.cholesky()",
            "+        Luu = torch.linalg.cholesky(Kuu)",
            "",
            "zero_loc = self.Xu.new_zeros(self.u_loc.shape)",
            "if self.whiten:"
        ]
    },
    {
        "number": 1152,
        "comments": "",
        "commit_message": "Fix TF RNN dynamic behavior\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def rnn(step_function, inputs, initial_states,",
            "new_states = []",
            "",
            "# all this circus is to recover the last vector in the sequence.",
            "-        begin = tf.pack([tf.shape(outputs)[0] - 1] + [0] * (ndim - 1))",
            "-        size = tf.pack([1] + [-1] * (ndim - 1))",
            "-        last_output = tf.slice(outputs, begin, size)",
            "+        slice_begin = tf.pack([tf.shape(outputs)[0] - 1] + [0] * (ndim - 1))",
            "+        slice_size = tf.pack([1] + [-1] * (ndim - 1))",
            "+        last_output = tf.slice(outputs, slice_begin, slice_size)",
            "last_output = tf.squeeze(last_output, [0])",
            "",
            "axes = [1, 0] + list(range(2, len(outputs.get_shape())))"
        ]
    },
    {
        "number": 1155,
        "comments": "",
        "commit_message": "[RLlib] Add all simple learning tests as `framework=tf2`. (#19273)\n\n* Unpin gym and deprecate pendulum v0\n\nMany tests in rllib depended on pendulum v0,\nhowever in gym 0.21, pendulum v0 was deprecated\nin favor of pendulum v1. This may change reward\nthresholds, so will have to potentially rerun\nall of the pendulum v1 benchmarks, or use another\nenvironment in favor. The same applies to frozen\nlake v0 and frozen lake v1\n\nLastly, all of the RLlib tests and Tune tests have\nbeen moved to python 3.7\n\n* fix tune test_sampler::testSampleBoundsAx\n\n* fix re-install ray for py3.7 tests\n\nCo-authored-by: avnishn <avnishn@uw.edu>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def convert_to_numpy(x: TensorStructType, reduce_floats: bool = False):",
            "if torch and isinstance(item, torch.Tensor):",
            "ret = item.cpu().item() if len(item.size()) == 0 else \\",
            "item.detach().cpu().numpy()",
            "-        elif tf and isinstance(item, (tf.Tensor, tf.Variable)):",
            "+        elif tf and isinstance(item, (tf.Tensor, tf.Variable)) and \\",
            "+                hasattr(item, \"numpy\"):",
            "assert tf.executing_eagerly()",
            "ret = item.numpy()",
            "else:"
        ]
    },
    {
        "number": 1159,
        "comments": "",
        "commit_message": "fix(speedup): make the tensor contiguous before randomizing (#5141)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ModelSpeedup:",
            "while not visit_queue.empty():",
            "curnode = visit_queue.get()",
            "self.update_indirect_sparsity(curnode)",
            "-            predecessors = self.torch_graph.find_predecessors(",
            "-                curnode.unique_name)",
            "+            predecessors = set(self.torch_graph.find_predecessors(",
            "+                curnode.unique_name))",
            "for predecessor in predecessors:",
            "out_degree[predecessor] -= 1",
            "if out_degree[predecessor] == 0:"
        ]
    },
    {
        "number": 1163,
        "comments": "",
        "commit_message": "Fix #1686\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class OpenAIGPTDoubleHeadsModel(OpenAIGPTPreTrainedModel):",
            "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')",
            "model = OpenAIGPTDoubleHeadsModel.from_pretrained('openai-gpt')",
            "tokenizer.add_special_tokens({'cls_token': '[CLS]'})  # Add a [CLS] to the vocabulary (we should train it also!)",
            "+        model.resize_token_embeddings(len(tokenizer))",
            "+",
            "choices = [\"Hello, my dog is cute [CLS]\", \"Hello, my cat is cute [CLS]\"]",
            "input_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices",
            "-        mc_token_ids = torch.tensor([input_ids.size(-1), input_ids.size(-1)]).unsqueeze(0)  # Batch size 1",
            "+        mc_token_ids = torch.tensor([input_ids.size(-1)-1, input_ids.size(-1)-1]).unsqueeze(0)  # Batch size 1",
            "+",
            "outputs = model(input_ids, mc_token_ids=mc_token_ids)",
            "lm_prediction_scores, mc_prediction_scores = outputs[:2]"
        ]
    },
    {
        "number": 1168,
        "comments": "",
        "commit_message": "fix concat + tensorflow out type hints\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def reshape(",
            "shape: Union[ivy.NativeShape, Sequence[int]],",
            "*,",
            "copy: Optional[bool] = None,",
            "-    out: Optional[tf.Tensor] = None,",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "if copy:",
            "newarr = tf.experimental.numpy.copy(x)"
        ]
    },
    {
        "number": 1191,
        "comments": "",
        "commit_message": "Speed-up warp_affine and fix bugs in RandomAffine [WIP: to add tests] (#474)\n\n* speed-up warp_affine, rotate, random_crop\n\n* added basic speed test for warp_affine\n\n* fixed centerization for random shear and bug (radians instead of degrees)\n\n* add test versus torchvision\n\n* added convert_affinematrix_to_homography function\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TestInvertAffineTransform:",
            "assert_allclose(matrix_inv, expected)",
            "",
            "def test_gradcheck(self, device):",
            "-        matrix = torch.eye(2, 3).to(device)",
            "+        matrix = torch.eye(2, 3).to(device)[None]",
            "matrix = utils.tensor_to_gradcheck_var(matrix)  # to var",
            "assert gradcheck(kornia.invert_affine_transform, (matrix,),",
            "raise_exception=True)"
        ]
    },
    {
        "number": 1192,
        "comments": "",
        "commit_message": "fix deprecation warnings; remove pydoop dependency; update README\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def train(target, dataset, cluster_spec, ctx):",
            "# passing in None for summary_op to avoid a summary_thread being started.",
            "# Running summaries and training operations in parallel could run out of",
            "# GPU memory.",
            "-      summary_writer = tf.train.SummaryWriter(\"tensorboard_%d\" %(ctx.worker_num), graph=tf.get_default_graph())",
            "+      summary_writer = tf.summary.FileWriter(\"tensorboard_%d\" %(ctx.worker_num), graph=tf.get_default_graph())",
            "sv = tf.train.Supervisor(is_chief=is_chief,",
            "logdir=FLAGS.train_dir,",
            "init_op=init_op,"
        ]
    },
    {
        "number": 1196,
        "comments": "",
        "commit_message": "Fix docs links to PyTorch documentation (#856)\n\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "for _name, _Dist in torch.distributions.__dict__.items():",
            "locals()[_name] = _PyroDist",
            "",
            "_PyroDist.__doc__ = '''",
            "-    Wraps :class:`torch.distributions.{}` with",
            "+    Wraps :class:`{}.{}` with",
            ":class:`~pyro.distributions.torch_distribution.TorchDistributionMixin`.",
            "-    '''.format(_Dist.__name__)",
            "+    '''.format(_Dist.__module__, _Dist.__name__)",
            "",
            "__all__.append(_name)"
        ]
    },
    {
        "number": 1197,
        "comments": "",
        "commit_message": "Fix momentum and epsilon values (#19454)\n\nThe momentum value for PyTorch and TensorFlow batch normalization layers is not equivalent. The TensorFlow value should be (1 - pytorch_momentum) in order to ensure the correct updates are applied to the running mean and running variance calculations. We wouldn't observe a difference loading a pretrained model and performing inference, but evaluation outputs would change after some training steps.\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFData2VecVisionForSemanticSegmentation(TFData2VecVisionPreTrainedModel):",
            "# FPNs",
            "self.fpn1 = [",
            "tf.keras.layers.Conv2DTranspose(config.hidden_size, kernel_size=2, strides=2, name=\"fpn1.0\"),",
            "-            tf.keras.layers.BatchNormalization(name=\"fpn1.1\"),",
            "+            tf.keras.layers.BatchNormalization(name=\"fpn1.1\", momentum=0.9, epsilon=1e-5),",
            "tf.keras.layers.Activation(\"gelu\"),",
            "tf.keras.layers.Conv2DTranspose(config.hidden_size, kernel_size=2, strides=2, name=\"fpn1.3\"),",
            "]"
        ]
    },
    {
        "number": 1199,
        "comments": "",
        "commit_message": "[Flax] Fix sample batch size DreamBooth  (#1129)\n\nfix sample batch size\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def main():",
            "logger.info(f\"Number of class images to sample: {num_new_images}.\")",
            "",
            "sample_dataset = PromptDataset(args.class_prompt, num_new_images)",
            "-            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)",
            "+            total_sample_batch_size = args.sample_batch_size * jax.local_device_count()",
            "+            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=total_sample_batch_size)",
            "",
            "for example in tqdm(",
            "sample_dataloader, desc=\"Generating class images\", disable=not jax.process_index() == 0"
        ]
    },
    {
        "number": 1200,
        "comments": "",
        "commit_message": "Spacy token indexer (#3040)\n\n* add a tokenizer to ud\n\n* add spacy indexer\n\n* allow token_indexers to specify their own type\n\n* dumb hack to allow a whitespace spacy tokenizer...\n\n* pass through token embedder\n\n* add ndarray to TokenType, tests for pass through embedder\n\n* add doc\n\n* remove todo, test\n\n* fix docs\n\n* why is this test flaky\n\n* fix the correct test\n\n* add as_padded_tensor method\n\n* better place for depreciation stuff\n\n* add warning for calling inherited get_padding_token\n\n* ignore type for backward compatability\n\n* mattg comments\n\n* pylint\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TokenCharactersIndexer(TokenIndexer[List[int]]):",
            "# Removes the \"dummy token\".",
            "padded_tokens.pop()",
            "# Truncates all the tokens to the desired length, and return the result.",
            "-        return {key: [list(token[:desired_token_length]) for token in padded_tokens]}",
            "+        return {key: torch.LongTensor([list(token[:desired_token_length])",
            "+                                       for token in padded_tokens])}"
        ]
    },
    {
        "number": 1205,
        "comments": "",
        "commit_message": "Change rnn-cell to fix #103 (#104)\n\n* Change rnn-cell to fix #103\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Model(ModelDesc):",
            "feat, labelidx, labelvalue, labelshape, seqlen = input_vars",
            "label = tf.SparseTensor(labelidx, labelvalue, labelshape)",
            "",
            "-        cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=HIDDEN)",
            "-        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * NLAYER)",
            "+        cell = tf.contrib.rnn.BasicLSTMCell(num_units=HIDDEN)",
            "+        cell = tf.contrib.rnn.MultiRNNCell([cell] * NLAYER)",
            "",
            "initial = cell.zero_state(tf.shape(feat)[0], tf.float32)"
        ]
    },
    {
        "number": 1209,
        "comments": "",
        "commit_message": "Fix broken centered mode in RMSProp and add tests.\n\nPiperOrigin-RevId: 256535597\nChange-Id: I00f58289a75683b3eb83159546e86f070bab3dea\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class RMSProp(base.Module):",
            "ms.assign(tf.square(update) * (1. - decay) + ms * decay)",
            "if self.centered:",
            "mg.assign(update * (1. - decay) + mg * decay)",
            "-          denominator = ms - mg + epsilon",
            "+          denominator = ms - tf.square(mg) + epsilon",
            "else:",
            "denominator = ms + epsilon",
            "mom.assign(momentum * mom + ("
        ]
    },
    {
        "number": 1212,
        "comments": "",
        "commit_message": "Fixed problems with exploration. (#289)\n\n* Cleaned up Agent child and child-classes:\n- Made sure no variables defined  in child classes are required by parent class.\n- Moved some variables shared amongst all types of Agent classes into base Agent class.\n- TODO: Need to a) Move more variables into base Agent class (now defined by each child class, e.g. discount) or b) create intermediary classes e.g. ModelBasedAgent. Needs to be discussed amongst devs.\n- Removed redundant docstring content, e.g. if variable is already defined by a parent class' c'tor and has the exact same functionality there.\n\n* Cleaned up Agent child and child-classes:\n- Made sure no variables defined  in child classes are required by parent class.\n- Moved some variables shared amongst all types of Agent classes into base Agent class.\n- TODO: Need to a) Move more variables into base Agent class (now defined by each child class, e.g. discount) or b) create intermediary classes e.g. ModelBasedAgent. Needs to be discussed amongst devs.\n- Removed redundant docstring content, e.g. if variable is already defined by a parent class' c'tor and has the exact same functionality there.\n\n* Moved `discount` into base Agent class.\n\n* Added LearningAgent class to hold all variables necessary for learning with an optimizable model.\nAll Agent child classes inherit from LearningAgent, except the non-learning ones (RandomAgent and ConstantAgent).\n\nIntroducing the new class (LearningAgent) removed lots of redundant c'tor and initialization code.\n\nObsoleted DDQNAgent (same as DQN with double_q_model==True). Gives an informative warning now (still backwards compatible).\n\n* Cleaned up Agent child and child-classes:\n- Made sure no variables defined  in child classes are required by parent class.\n- Moved some variables shared amongst all types of Agent classes into base Agent class.\n- TODO: Need to a) Move more variables into base Agent class (now defined by each child class, e.g. discount) or b) create intermediary classes e.g. ModelBasedAgent. Needs to be discussed amongst devs.\n- Removed redundant docstring content, e.g. if variable is already defined by a parent class' c'tor and has the exact same functionality there.\n\n* Moved `discount` into base Agent class.\n\n* Added LearningAgent class to hold all variables necessary for learning with an optimizable model.\nAll Agent child classes inherit from LearningAgent, except the non-learning ones (RandomAgent and ConstantAgent).\n\nIntroducing the new class (LearningAgent) removed lots of redundant c'tor and initialization code.\n\nObsoleted DDQNAgent (same as DQN with double_q_model==True). Gives an informative warning now (still backwards compatible).\n\n* Fixed travis import errors LearningAgent.\n\n* - Added LearningAgent properly to agents/__init__.py\n\n* Fixed various pytest failures.\nMoved 'scope' back into base Agent (needed by Random and ConstantAgents).\nRemoved  unnecessary parameters from Random and Constant (non-learning) Agents.\n\n* Fixed action_exploration in `Model` and `Exploration` classes.\n\n* Fixed action_exploration in `Model` and `Exploration` classes.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class EpsilonDecay(Exploration):",
            "epsilon = self.final_epsilon + (2 ** (-half_life_ratio)) * (self.initial_epsilon - self.final_epsilon)",
            "return epsilon",
            "",
            "-        pred = tf.logical_or(x=(timestep < self.start_timestep), y=(timestep > self.start_timestep + self.timesteps))",
            "+        pred = tf.logical_or(x=(timestep < self.start_timestep),",
            "+                             y=(timestep > self.start_timestep + int(self.timesteps)))",
            "return tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn)"
        ]
    },
    {
        "number": 1215,
        "comments": "",
        "commit_message": "fix shape error on cuda (#1385)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class EKFState(object):",
            "S = H.mm(P).mm(H.transpose(-1, -2)) + R  # innovation cov",
            "",
            "K_prefix = self._cov.mm(H.transpose(-1, -2))",
            "-        dx = K_prefix.mm(torch.gesv(dz, S)[0]).squeeze(1)  # K*dz",
            "+        dx = K_prefix.mm(torch.gesv(dz.unsqueeze(1), S)[0]).squeeze(1)  # K*dz",
            "x = self._dynamic_model.geodesic_difference(x, -dx)",
            "",
            "I = eye_like(x, self._dynamic_model.dimension)  # noqa: E741"
        ]
    },
    {
        "number": 1219,
        "comments": "",
        "commit_message": "fix docstring\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TransformerLM(nn.Module, LMInterface):",
            "m = subsequent_mask(ys_mask.size(-1), device=ys_mask.device).unsqueeze(0)",
            "return ys_mask.unsqueeze(-2) & m",
            "",
            "-    def forward(self, x: torch.Tensor, t: torch.Tensor):",
            "+    def forward(self, x: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:",
            "xm = (x != 0)",
            "h, _ = self.encoder(x, self.target_mask(x))",
            "y = self.decoder(h)"
        ]
    },
    {
        "number": 1223,
        "comments": "",
        "commit_message": "Bug fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DistributedFusedAdam(torch.optim.Optimizer):",
            "grp = torch.distributed.new_group(ranks=ranks)",
            "if torch.distributed.get_rank() in ranks:",
            "self._ag_pg.append(grp)",
            "-            self._ag_st = [torch.cuda.Stream()]*self._num_ag_pg",
            "+            self._ag_st = [torch.cuda.Stream() for _ in range(self._num_ag_pg)]",
            "self._l2_grad_norm_st = torch.cuda.Stream() if self._compute_L2_grad_norm else None",
            "self._completion_st = torch.cuda.Stream()"
        ]
    },
    {
        "number": 1227,
        "comments": "",
        "commit_message": "Fix notebooks.\n\nPiperOrigin-RevId: 305891180\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "\"#@title Load the Universal Sentence Encoder's TF Hub module\\n\",",
            "\"from absl import logging\\n\",",
            "\"\\n\",",
            "-        \"import tensorflow.compat.v1 as tf\\n\",",
            "-        \"tf.disable_v2_behavior()\\n\",",
            "+        \"import tensorflow as tf\\n\",",
            "\"\\n\",",
            "\"import tensorflow_hub as hub\\n\",",
            "\"import matplotlib.pyplot as plt\\n\","
        ]
    },
    {
        "number": 1230,
        "comments": "",
        "commit_message": "Deprecate prepare_module (#3166)\n\n* Refactor prepare_module\n\n* Add deprecation warning in prepare_module\n\n* Remove prepare_module in inspect\n\n* Remove prepare_module in patching\n\n* Remove prepare_module in dummy_data\n\n* Remove prepare_module in run_beam\n\n* Remove prepare_module in test_dataset_common\n\n* Fix hash in run_beam\n\n* Remove prepare_module from test_load\n\n* Remove prepare_module from test_metric_common\n\n* Remove prepare_module from test_hf_gcp\n\n* Use deprecated function instead\n\n* Add deprecation to docstring\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class patch_submodule:",
            "Examples:",
            "",
            ">>> import importlib",
            "-        >>> from datasets.load import prepare_module",
            "+        >>> from datasets.load import dataset_module_factory",
            ">>> from datasets.streaming import patch_submodule, xjoin",
            ">>>",
            "-        >>> snli_module_path, _ = prepare_module(\"snli\")",
            "-        >>> snli_module = importlib.import_module(snli_module_path)",
            "+        >>> dataset_module = dataset_module_factory(\"snli\")",
            "+        >>> snli_module = importlib.import_module(dataset_module.module_path)",
            ">>> patcher = patch_submodule(snli_module, \"os.path.join\", xjoin)",
            ">>> patcher.start()",
            ">>> assert snli_module.os.path.join is xjoin"
        ]
    },
    {
        "number": 1231,
        "comments": "",
        "commit_message": "feat(graphstore): support `num_nodes`, enabling `Tuple[FeatureStore, GraphStore]` in `LightningLinkData` (#5270)\n\n* init\n\n* test\n\n* fix\n\n* :(\n\n* return type\n\n* changelog\n\n* minor\n\n* major\n\n* Update torch_geometric/data/hetero_data.py\n\n* Update torch_geometric/data/hetero_data.py\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MyFeatureStore(FeatureStore):",
            "and attr.index == slice(None, None, None)):",
            "return tensor",
            "",
            "-        idx = torch.cat([(index == v).nonzero() for v in attr.index]).view(-1)",
            "+        idx = (torch.cat([(index == v).nonzero() for v in attr.index]).view(-1)",
            "+               if attr.index.numel() > 0 else [])",
            "return tensor[idx]",
            "",
            "def _remove_tensor(self, attr: TensorAttr) -> bool:"
        ]
    },
    {
        "number": 1236,
        "comments": "",
        "commit_message": "Fix tests on CUDA (#2098)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "from kornia.testing import assert_close",
            "class TestOneHot:",
            "def test_smoke(self, device, dtype):",
            "num_classes = 4",
            "-        labels = torch.zeros(2, 2, 1, dtype=torch.int64)",
            "+        labels = torch.zeros(2, 2, 1, dtype=torch.int64, device=device)",
            "labels[0, 0, 0] = 0",
            "labels[0, 1, 0] = 1",
            "labels[1, 0, 0] = 2"
        ]
    },
    {
        "number": 1243,
        "comments": "",
        "commit_message": "Add hypernetwork support to split cross attention v1\n\n* Add hypernetwork support to split_cross_attention_forward_v1\n* Fix device check in esrgan_model.py to use devices.device_esrgan instead of shared.device\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class UpscalerESRGAN(Upscaler):",
            "print(\"Unable to load %s from %s\" % (self.model_path, filename))",
            "return None",
            "",
            "-        pretrained_net = torch.load(filename, map_location='cpu' if shared.device.type == 'mps' else None)",
            "+        pretrained_net = torch.load(filename, map_location='cpu' if devices.device_esrgan.type == 'mps' else None)",
            "crt_model = arch.RRDBNet(3, 3, 64, 23, gc=32)",
            "",
            "pretrained_net = fix_model_layers(crt_model, pretrained_net)"
        ]
    },
    {
        "number": 1244,
        "comments": "",
        "commit_message": "fix test for v0.3\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "for epoch in range(num_epochs):",
            "if j % embedding_log == 0:",
            "print(\"loss_value:{}\".format(loss_value.data[0]))",
            "#we need 3 dimension for tensor to visualize it!",
            "-            out = torch.cat((out, torch.ones(len(out), 1)), 1)",
            "-            writer.add_embedding(out.data, metadata=label_batch.data, label_img=data_batch.data, global_step=n_iter)",
            "+            out = torch.cat((out.data, torch.ones(len(out), 1)), 1)",
            "+            writer.add_embedding(out, metadata=label_batch.data, label_img=data_batch.data, global_step=n_iter)",
            "",
            "writer.close()"
        ]
    },
    {
        "number": 1248,
        "comments": "",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DeepSpeedZeRoOffload(object):",
            "self._prefetch_bucket_sz = int(prefetch_bucket_size)",
            "self._max_reuse_distance_in_numel = int(max_reuse_distance)",
            "self._max_available_parameters_in_numel = int(max_live_parameters)",
            "-        self.__allgather_stream = Stream(",
            "-        ) if overlap_comm else torch.cuda.default_stream()",
            "+        self.__allgather_stream = get_accelerator().Stream(",
            "+        ) if overlap_comm else get_accelerator().default_stream()",
            "",
            "self.forward_hooks = []",
            "self.backward_hooks = []"
        ]
    },
    {
        "number": 1256,
        "comments": "",
        "commit_message": "Fix backprop bug when using non-zero hard loss weight.\n\nSummary:\nGot `RuntimeError: grad can be implicitly created only for scalar outputs` on f224839642.\n\nAfter some digging, found that we don't mean() the has_answer hard_loss so the final loss object becomes a tensor instead of scalar.\n\nAlso change cross_entropy loss to nll_loss per comment on Line 42 of loss.py\n\nReviewed By: stanvp\n\nDifferential Revision: D24349613\n\nfbshipit-source-id: 4109b72a6bab3d53fa5b93e6010f1ce7dcd06dde\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class KLDivergenceCELoss(Loss):",
            "soft_loss *= self.t ** 2  # See https://arxiv.org/pdf/1503.02531.pdf",
            "hard_loss = 0.0",
            "if self.hard_weight > 0.0:",
            "-            hard_loss = F.cross_entropy(",
            "-                logits,",
            "+            hard_loss = F.nll_loss(",
            "+                F.log_softmax(logits, 1, dtype=torch.float32),",
            "hard_targets,",
            "-                reduction=\"mean\" if reduce else \"none\",",
            "weight=self.weight,",
            "+                reduction=\"mean\" if reduce else \"none\",",
            ")",
            "",
            "return ("
        ]
    },
    {
        "number": 1257,
        "comments": "",
        "commit_message": "added record-and-pretrain example, documentation improvements, fixes\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Module(tf.Module):",
            "elif initializer == 'ones':",
            "initializer = tf_util.ones(shape=spec.shape, dtype=spec.type)",
            "elif initializer == 'constant':",
            "-            initializer = tf_util.fill(dims=spec.shape, value=self.initialization_scale)",
            "+            initializer = tf.fill(",
            "+                dims=spec.shape, value=tf_util.constant(value=initialization_scale, dtype=spec.type)",
            "+            )",
            "",
            "# Variable",
            "variable = tf.Variable("
        ]
    },
    {
        "number": 1266,
        "comments": "",
        "commit_message": "Fix doctest CI (#21166)\n\n* fix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "PT_SEQUENCE_CLASSIFICATION_SAMPLE = r\"\"\"",
            "...     \"{checkpoint}\", num_labels=num_labels, problem_type=\"multi_label_classification\"",
            "... )",
            "",
            "-    >>> labels = torch.nn.functional.one_hot(torch.tensor(predicted_class_ids), num_classes=num_labels).to(torch.float)",
            "+    >>> labels = torch.sum(",
            "+    ...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1",
            "+    ... ).to(torch.float)",
            ">>> loss = model(**inputs, labels=labels).loss",
            "```",
            "\"\"\""
        ]
    },
    {
        "number": 1270,
        "comments": "",
        "commit_message": "Fixed nightly errors (#886)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TestNormalizeLAF:",
            "laf = torch.tensor([[1, 0, 1], [0, 1, 1]]).float()",
            "laf = laf.view(1, 1, 2, 3)",
            "img = torch.rand(1, 3, h, w)",
            "-        expected = torch.tensor([[0.2, 0, 0.1], [0, 0.2, 0.2]]).float()",
            "+        expected = torch.tensor([[[[0.2, 0, 0.1], [0, 0.2, 0.2]]]]).float()",
            "lafn = kornia.feature.normalize_laf(laf, img)",
            "assert_allclose(lafn, expected)"
        ]
    },
    {
        "number": 1274,
        "comments": "",
        "commit_message": "Add usage of batch norm in conv test and fix usage of is_training collection\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def batch_normalize(tensor_in, epsilon=1e-5, convnet=True, decay=0.9,",
            "\"\"\"Internal function that updates mean and variance during training\"\"\"",
            "with tf.control_dependencies([ema_assign_op]):",
            "return tf.identity(assign_mean), tf.identity(assign_var)",
            "-        IS_TRAINING = tf.get_collection(\"IS_TRAINING\")[-1]",
            "-        mean, variance = control_flow_ops.cond(IS_TRAINING,",
            "-                                               update_mean_var,",
            "-                                               lambda: (ema_mean, ema_var))",
            "+        is_training = tf.squeeze(tf.get_collection(\"IS_TRAINING\"))",
            "+        mean, variance = tf.python.control_flow_ops.cond(",
            "+            is_training, update_mean_var, lambda: (ema_mean, ema_var))",
            "return tf.nn.batch_norm_with_global_normalization(",
            "tensor_in, mean, variance, beta, gamma, epsilon,",
            "scale_after_normalization=scale_after_normalization)"
        ]
    },
    {
        "number": 1282,
        "comments": "",
        "commit_message": "Fix special values overflow/underflow for amp (#3901)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def multi_perspective_match_pairwise(",
            "norm_value = vector1_norm * vector2_norm.transpose(2, 3)",
            "",
            "# (batch, seq_len1, seq_len2, num_perspectives)",
            "-    return (mul_result / norm_value.clamp(min=eps)).permute(0, 2, 3, 1)",
            "+    return (mul_result / norm_value.clamp(min=tiny_value_of_dtype(norm_value.dtype))).permute(",
            "+        0, 2, 3, 1",
            "+    )",
            "",
            "",
            "class BiMpmMatching(nn.Module, FromParams):"
        ]
    },
    {
        "number": 1283,
        "comments": "",
        "commit_message": "Fix module dict in base finetuning (#8170)\n\n* Fix module dict in base finetuning\n\n* Update CHANGELOG.md\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_complex_nested_model():",
            "assert len(BaseFinetuning.flatten_modules(model)) == 10",
            "",
            "BaseFinetuning.freeze(model.encoder, train_bn=True)",
            "-    assert not model.encoder[0].conv.weight.requires_grad  # Validate a leaf module parameter is frozen",
            "+    assert not model.encoder[0].module_dict[\"conv\"].weight.requires_grad  # Validate a leaf module parameter is frozen",
            "assert not model.encoder[0].parent_param.requires_grad  # Validate the parent module parameter is frozen",
            "assert model.encoder[0].bn.weight.requires_grad"
        ]
    },
    {
        "number": 1294,
        "comments": "",
        "commit_message": "fix SCEModule.sSE (#113)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class SCSEModule(nn.Module):",
            "nn.Conv2d(in_channels // reduction, in_channels, 1),",
            "nn.Sigmoid(),",
            ")",
            "-        self.sSE = nn.Sequential(nn.Conv2d(in_channels, in_channels, 1), nn.Sigmoid())",
            "+        self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())",
            "",
            "def forward(self, x):",
            "return x * self.cSE(x) + x * self.sSE(x)"
        ]
    },
    {
        "number": 1296,
        "comments": "",
        "commit_message": "Fixed a problem with jax interpolate where it got into an infinite recursive loop\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def interpolate(",
            "size = [x.shape[0], *size, x.shape[1]]",
            "",
            "if align_corners or mode == \"area\":",
            "-        return ivy.interpolate(",
            "+        return ivy.functional.experimental.interpolate(",
            "x, size, mode=mode, align_corners=align_corners, antialias=antialias",
            ")",
            "x = jnp.transpose(x, (0, *range(2, dims + 2), 1))"
        ]
    },
    {
        "number": 1302,
        "comments": "",
        "commit_message": "fix memory bloat on restore\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def main(args):  # pylint: disable=redefined-outer-name",
            "pos_weight=torch.tensor(10)) if c.stopnet else None",
            "",
            "if args.restore_path:",
            "-        checkpoint = torch.load(args.restore_path)",
            "+        checkpoint = torch.load(args.restore_path, map_location='cpu')",
            "try:",
            "# TODO: fix optimizer init, model.cuda() needs to be called before",
            "# optimizer restore"
        ]
    },
    {
        "number": 1308,
        "comments": "",
        "commit_message": "[Test] Fix count_nonzero\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def count_nonzero(",
            "def _dtype_count_nonzero(a, axis, dtype):",
            "if dtype is None:",
            "return torch.count_nonzero(a, dim=axis)",
            "-        return torch.tensor(torch.count_nonzero(a, dim=axis), dtype=dtype)",
            "+        return torch.tensor(torch.count_nonzero(a, dim=axis),",
            "+                            dtype=ivy.as_native_dtype(dtype))",
            "",
            "x = _dtype_count_nonzero(a, axis, dtype)",
            "if not keepdims:"
        ]
    },
    {
        "number": 1309,
        "comments": "",
        "commit_message": "Add Image feature (#3163)\n\n* Initial commit\n\n* Add basic decoding\n\n* Replace features.Audio with Audio\n\n* Add Image to package reference\n\n* Use np.array\n\n* Update error msg\n\n* Add mode and channel decoding\n\n* Fix return value\n\n* Finish decoding\n\n* Make CI happy\n\n* Some more fixes\n\n* Minor doc fix\n\n* Remove animated option\n\n* Pin version\n\n* Remove unused imports in setup.py\n\n* Add vision requirements to setup.py\n\n* Add initial tests\n\n* Delete other formats\n\n* Make Image feature hashable\n\n* Add more tests\n\n* Support numpy array in alter data check in TypedSequence\n\n* Fix TypedSequence converion\n\n* Finish tests\n\n* Update Image - add ImageExtensionType and supporting functions\n\n* Update encoding functions\n\n* Add support in TypedSequence for ImageExtensionType\n\n* Add tests\n\n* Remove unused import\n\n* Fix doc and style\n\n* Fix doc indentation\n\n* Improve comment\n\n* Return single image instead of dict\n\n* Return PIL Image and not dict\n\n* Encode dict\n\n* Update tests\n\n* Style\n\n* np.ndarray encoding/decoding\n\n* Minor improvements\n\n* PIL Image support in cast_to_python_objects\n\n* Test cast\n\n* Doc fix\n\n* Extension type fixes\n\n* Style\n\n* Use types_mapper in Dataset.to_pandas\n\n* Add pandas extension array for image type\n\n* Update tests\n\n* image type inference\n\n* Remvoe cast_to_python test after Quentin's change\n\n* Improve tests\n\n* Add storage type\n\n* Improve tests\n\n* Test map that returns np.ndarray\n\n* Rename functions\n\n* Add streaming test\n\n* Use image struct in all situations\n\n* Update src/datasets/features/image.py - encode_example type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Update src/datasets/features/image.py -list_image_compression_formats type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Support str in encode_objects_to_image_dicts\n\n* Update src/datasets/features/image.py - objects_to_list_of_image_dicts type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Style\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class LibrispeechASR(datasets.GeneratorBasedBuilder):",
            "features=datasets.Features(",
            "{",
            "\"file\": datasets.Value(\"string\"),",
            "-                    \"audio\": datasets.features.Audio(sampling_rate=16_000),",
            "+                    \"audio\": datasets.Audio(sampling_rate=16_000),",
            "\"text\": datasets.Value(\"string\"),",
            "\"speaker_id\": datasets.Value(\"int64\"),",
            "\"chapter_id\": datasets.Value(\"int64\"),"
        ]
    },
    {
        "number": 1311,
        "comments": "",
        "commit_message": "fix typing callable in load storage (#1768)\n\n* fix typing callable in load storage\n\n* missing import\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MKDDescriptor(nn.Module):",
            "",
            "",
            "def load_whitening_model(kernel_type: str, training_set: str) -> Dict:",
            "-    whitening_models = torch.hub.load_state_dict_from_url(urls[kernel_type], map_location=lambda storage, loc: storage)",
            "+    storage_fcn: Callable = lambda storage, loc: storage",
            "+    whitening_models = torch.hub.load_state_dict_from_url(",
            "+        urls[kernel_type], map_location=storage_fcn",
            "+    )",
            "whitening_model = whitening_models[training_set]",
            "return whitening_model"
        ]
    },
    {
        "number": 1312,
        "comments": "",
        "commit_message": "changed all asset to if/raise to prevent disable of assert during PYTHONOPTIMISE env (#4655)\n\n* changed all asset to if/raise to prevent disable of assert during PYTHONOPTIMISE env\n\n* minor fix for proper inverson of assert condition\n\n* Changed to AssertionError which is handled at multiple places\n\n* Changed to AssertionError which is handled at multiple places\n\n* added simple test case in test_string to increase test coverage\n\n* added simple test cases in test_string to increase test coverage\n\n* added simple test cases to increase test coverage\n\n* Either None OR More than One worker result found\n\n* changes for review comments\n\n* removed comments, minor changes\n\nCo-authored-by: Vivek Pothina <vivek.pothina@ninjacart.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TorchTensor(AbstractTensor):",
            "",
            "\"\"\"",
            "",
            "-        assert isinstance(self.child, PointerTensor)",
            "+        if not isinstance(self.child, PointerTensor):",
            "+            raise TypeError(\"child should be a PointerTensor\")",
            "",
            "ps = list(pointers)",
            "ps.append(self)"
        ]
    },
    {
        "number": 1313,
        "comments": "",
        "commit_message": "fix the unravel_index's torch backend as it returned a tuple rather than a torch.tensor\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def unravel_index(",
            "for dim in reversed(shape):",
            "output.append(temp % dim)",
            "temp = temp // dim",
            "-    return tuple(reversed(output))",
            "+    return torch.tensor(reversed(output))",
            "",
            "",
            "unravel_index.support_native_out = False"
        ]
    },
    {
        "number": 1315,
        "comments": "",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CanineSelfAttention(nn.Module):",
            "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for",
            "# masked positions, this operation will create a tensor which is 0.0 for",
            "# positions we want to attend and -10000.0 for masked positions.",
            "-                attention_mask = (1.0 - attention_mask.float()) * -10000.0",
            "+                attention_mask = (1.0 - attention_mask.float()) * torch.finfo(attention_scores.dtype).min",
            "# Apply the attention mask (precomputed for all layers in CanineModel forward() function)",
            "attention_scores = attention_scores + attention_mask"
        ]
    },
    {
        "number": 1323,
        "comments": "",
        "commit_message": "feat: models parameters check for ner\n\n* feat: parameters check added to ner\n\n* feat: parameters check added to slotfill\n\n* chore: minor clean-up\n\n* fix: fix conll-2003 model file names and archive names\n\n* refactor: remove blank line\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFModel(NNModel, metaclass=TfModelMeta):",
            "opt_scope = tf.variable_scope(optimizer_scope_name)",
            "with opt_scope:",
            "if learnable_scopes is None:",
            "-                variables_to_train = tf.trainable_variables()",
            "+                variables_to_train = tf.global_variables()",
            "else:",
            "variables_to_train = []",
            "for scope_name in learnable_scopes:",
            "-                    for var in tf.trainable_variables():",
            "+                    for var in tf.global_variables():",
            "if scope_name in var.name:",
            "variables_to_train.append(var)"
        ]
    },
    {
        "number": 1324,
        "comments": "",
        "commit_message": "Fix axis specification in TF.\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def abs(x):",
            "",
            "",
            "def sqrt(x):",
            "-    x = tf.clip_by_value(x, _EPSILON, np.inf)",
            "+    x = tf.clip_by_value(x, 0., np.inf)",
            "return tf.sqrt(x)"
        ]
    },
    {
        "number": 1332,
        "comments": "",
        "commit_message": "Fix //examples/text_embeddings_v2.\n\nDuring refactor to public APIs, it ended up using the wrong filename.\n\nPiperOrigin-RevId: 268192870\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TextEmbeddingModel(tf.train.Checkpoint):",
            "# Assign the table initializer to this instance to ensure the asset",
            "# it depends on is saved with the SavedModel.",
            "self._table_initializer = tf.lookup.TextFileInitializer(",
            "-        vocab_file_path, tf.string, tf.lookup.TextFileIndex.WHOLE_LINE,",
            "+        write_vocabulary_file(self._vocabulary),",
            "+        tf.string, tf.lookup.TextFileIndex.WHOLE_LINE,",
            "tf.int64, tf.lookup.TextFileIndex.LINE_NUMBER)",
            "self._table = tf.lookup.StaticVocabularyTable(",
            "self._table_initializer, num_oov_buckets=oov_buckets)"
        ]
    },
    {
        "number": 1333,
        "comments": "",
        "commit_message": "RANSAC improvements (#1435)\n\n- Added LU solver for RANSAC-homography, it speeds-up the minimal-solver part x10\n- Now find_fundamental weights is optional, as it was for homography\n- Fix test precision and made one xFail test to be compulsory\n- Fixed tests for fundamental matrix, as one have to have >= 8 points (also added to docstring)\n- updates deprecated torch.svd to torch.linalg.svd\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def decompose_essential_matrix(E_mat: torch.Tensor) -> Tuple[torch.Tensor, torch",
            "raise AssertionError(E_mat.shape)",
            "",
            "# decompose matrix by its singular values",
            "-    U, _, V = torch.svd(E_mat)",
            "+    U, _, V = _torch_svd_cast(E_mat)",
            "Vt = V.transpose(-2, -1)",
            "",
            "mask = torch.ones_like(E_mat)"
        ]
    },
    {
        "number": 1338,
        "comments": "",
        "commit_message": "Fixed weight init for fused weight matrices in fused MHA by adding correct gain factor.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class EncdecMultiheadAttn(nn.Module):",
            "",
            "def reset_parameters(self):",
            "nn.init.xavier_uniform_(self.in_proj_weight_q)",
            "-        nn.init.xavier_uniform_(self.in_proj_weight_kv)",
            "+        # in_proj_weight_kv has shape [2 * hidden, hidden] but it should be",
            "+        # initialized like a [hidden, hidden] matrix.",
            "+        # sqrt(6 / (hidden + hidden)) / sqrt(6 / (2 * hidden + hidden)) = sqrt(1.5)",
            "+        # therefore xavier_uniform gain should be set to sqrt(1.5).",
            "+        nn.init.xavier_uniform_(self.in_proj_weight_kv, gain=math.sqrt(1.5))",
            "nn.init.xavier_uniform_(self.out_proj_weight)",
            "if self.bias:",
            "nn.init.constant_(self.in_proj_bias_q, 0.)"
        ]
    },
    {
        "number": 1343,
        "comments": "",
        "commit_message": "small bug fixes for torch gradients module.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def adam_update(ws, dcdws, lr, mw, vw, step, beta1=0.9, beta2=0.999, epsilon=1e-",
            "",
            "def stop_gradient(x, preserve_type=True):",
            "is_var = is_variable(x)",
            "-    # ToDo: work out why _torch.tensor() wrapping is necessary in certain cases, presumably .detach() should be enough.",
            "-    x = _torch.tensor(x.detach())",
            "+    x = x.detach()",
            "if is_var and preserve_type:",
            "-        return variable(x)",
            "+        return x.requires_grad_()",
            "return x"
        ]
    },
    {
        "number": 1349,
        "comments": "",
        "commit_message": "refactor BaseWorker.send_command to accept explicit arguments. (#3487)\n\n* syft: refactor BaseWorker.send_command to accept explicit arguments.\n\nsyft: remove unused variables\n\n* syft: fix linter issues.\n\n* syft: fix argument type of args_ and update docstring for send_command of BaseWoker class.\n\n* syft, pointers: fix int type to tuple conversion.\n\nCo-authored-by: Shubham Gupta <shubamgupta3121@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TorchHook(FrameworkHook):",
            "@wraps(attr)",
            "def overloaded_attr(self_torch, *args, **kwargs):",
            "ptr = hook_self.local_worker.send_command(",
            "-                recipient=self_torch.worker(), message=(f\"{'torch'}.{attr}\", None, args, kwargs)",
            "+                recipient=self_torch.worker(),",
            "+                cmd_name=f\"{'torch'}.{attr}\",",
            "+                args_=args,",
            "+                kwargs_=kwargs,",
            ")",
            "",
            "return ptr.wrap()"
        ]
    },
    {
        "number": 1357,
        "comments": "",
        "commit_message": "fix reshape error\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MultiHeadAttention(nn.Module):",
            "# perform attention, result size = (n_head * mb_size) x len_q x d_v",
            "outputs, attns = self.attention(q_s, k_s, v_s, attn_mask=attn_mask.repeat(n_head, 1, 1))",
            "",
            "-        # back to original mb_size batch",
            "-        outputs = outputs.view(mb_size, len_q, -1)            # mb_size x len_q x (n_head*d_v)",
            "+        # back to original mb_size batch, result size = mb_size x len_q x (n_head*d_v)",
            "+        outputs = torch.cat(torch.split(outputs, mb_size, dim=0), dim=-1)",
            "",
            "# project back to residual size",
            "outputs = self.proj(outputs)"
        ]
    },
    {
        "number": 1363,
        "comments": "",
        "commit_message": "Optimize Stable Diffusion (#371)\n\n* initial commit\n\n* make UNet stream capturable\n\n* try to fix noise_pred value\n\n* remove cuda graph and keep NB\n\n* non blocking unet with PNDMScheduler\n\n* make timesteps np arrays for pndm scheduler\nbecause lists don't get formatted to tensors in `self.set_format`\n\n* make max async in pndm\n\n* use channel last format in unet\n\n* avoid moving timesteps device in each unet call\n\n* avoid memcpy op in `get_timestep_embedding`\n\n* add `channels_last` kwarg to `DiffusionPipeline.from_pretrained`\n\n* update TODO\n\n* replace `channels_last` kwarg with `memory_format` for more generality\n\n* revert the channels_last changes to leave it for another PR\n\n* remove non_blocking when moving input ids to device\n\n* remove blocking from all .to() operations at beginning of pipeline\n\n* fix merging\n\n* fix merging\n\n* model can run in other precisions without autocast\n\n* attn refactoring\n\n* Revert \"attn refactoring\"\n\nThis reverts commit 0c70c0e189cd2c4d8768274c9fcf5b940ee310fb.\n\n* remove restriction to run conv_norm in fp32\n\n* use `baddbmm` instead of `matmul`for better in attention for better perf\n\n* removing all reshapes to test perf\n\n* Revert \"removing all reshapes to test perf\"\n\nThis reverts commit 006ccb8a8c6bc7eb7e512392e692a29d9b1553cd.\n\n* add shapes comments\n\n* hardcore whats needed for jitting\n\n* Revert \"hardcore whats needed for jitting\"\n\nThis reverts commit 2fa9c698eae2890ac5f8e367ca80532ecf94df9a.\n\n* Revert \"remove restriction to run conv_norm in fp32\"\n\nThis reverts commit cec592890c32da3d1b78d38b49e4307aedf459b9.\n\n* revert using baddmm in attention's forward\n\n* cleanup comment\n\n* remove restriction to run conv_norm in fp32. no quality loss was noticed\n\nThis reverts commit cc9bc1339c998ebe9e7d733f910c6d72d9792213.\n\n* add more optimizations techniques to docs\n\n* Revert \"add shapes comments\"\n\nThis reverts commit 31c58eadb8892f95478cdf05229adf678678c5f4.\n\n* apply suggestions\n\n* make quality\n\n* apply suggestions\n\n* styling\n\n* `scheduler.timesteps` are now arrays so we dont need .to()\n\n* remove useless .type()\n\n* use mean instead of max in `test_stable_diffusion_inpaint_pipeline_k_lms`\n\n* move scheduler timestamps to correct device if tensors\n\n* add device to `set_timesteps` in LMSD scheduler\n\n* `self.scheduler.set_timesteps` now uses device arg for schedulers that accept it\n\n* quick fix\n\n* styling\n\n* remove kwargs from schedulers `set_timesteps`\n\n* revert to using max in K-LMS inpaint pipeline test\n\n* Revert \"`self.scheduler.set_timesteps` now uses device arg for schedulers that accept it\"\n\nThis reverts commit 00d5a51e5c20d8d445c8664407ef29608106d899.\n\n* move timesteps to correct device before loop in SD pipeline\n\n* apply previous fix to other SD pipelines\n\n* UNet now accepts tensor timesteps even on wrong device, to avoid errors\n- it shouldnt affect performance if timesteps are alrdy on correct device\n- it does slow down performance if they're on the wrong device\n\n* fix pipeline when timesteps are arrays with strides\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def get_timestep_embedding(",
            "assert len(timesteps.shape) == 1, \"Timesteps should be a 1d-array\"",
            "",
            "half_dim = embedding_dim // 2",
            "-    exponent = -math.log(max_period) * torch.arange(start=0, end=half_dim, dtype=torch.float32)",
            "+    exponent = -math.log(max_period) * torch.arange(",
            "+        start=0, end=half_dim, dtype=torch.float32, device=timesteps.device",
            "+    )",
            "exponent = exponent / (half_dim - downscale_freq_shift)",
            "",
            "-    emb = torch.exp(exponent).to(device=timesteps.device)",
            "+    emb = torch.exp(exponent)",
            "emb = timesteps[:, None].float() * emb[None, :]",
            "",
            "# scale embeddings"
        ]
    },
    {
        "number": 1364,
        "comments": "",
        "commit_message": "PaillierTensor and Arg Type Matching (experimental) (#2740)\n\n* Cleanup directory names\n\nWith syft.frameworks.torch, the directory names have becomee disorganized\nand don't seem to follow any set standard. Also, some are simply too long\nand cumbersome to be typing out all the time (such as differential_privacy).\n\nI have replaced long names with their shorthand options, federated leearning\nto simply fl, differential_privacy to dp. I also changed crypto to mpc\nsince all of the algorithms contained within the folder exclusively\nrelated to secure multi-party computation. If and when we implement\na wider varitey of crypto algorithms, we can reconsider this name. However,\nat present, I would like for us to use a more descriptive directory name.\n\n* Revert changed notbooks\n\nI testeed all the notebooks to ensure that the work with the\nrenamd directories but don't wish to actually change them\n\n* Revert changed notbooks\n\nI testeed all the notebooks to ensure that the work with the\nrenamd directories but don't wish to actually change them\n\n* Revert setup.py change\n\n* Revert docs change\n\n* Revert docs change\n\n* Revert docs change\n\n* Init boilerplate example tensor\n\nI am beginning this project by copy-pasting LoggingTensor as a boilerplate\nexample of how to create a custom tensor type in PySyft. I then changed\nall instances of LoggingTensor to PaillierTensor\n\n* Init experimental notebook\n\nI'll be doing my experimenting and development/testing in this notebook\n\n* Add PaillierTensor to hook_args\n\nIn this commit, I added PaillierTensor to the hook_args\nconfiguration file\n\n* Add support for encrypt, decrypt, and __add__\n\nI added support for basic encryption, decryption, and addition to PaillierTensor.\nData within the tensor is stored as a numpy array of phe scalars, where phe\nis the python package for paillier homomorphic encryption. This will allow\nus to use many of the desirable numpy operations as needed\n\n* Remove unused boilerplate\n\n* Parallelize homomorphic encryption and decryption\n\nSince the encryption and decryption step was quite slow, I parallelizd it\nusing the multiprocessing library. I'm pretty sure I'll be able to do this\nfor all of the paillier operations, greatly increasing the speed of the lib.\n\n* Move thread pool to syft.\n\nI don't want to have to re-initialize the thread pool over and over, so I'm\nadding it as a global syft variable\n\n* Hook args automatically\n\nSince PyTorch has a lot of similar methods to Numpy, by defaeult se can use\nthe normal hooking logic for most methods\n\n* Add experimental support for mis-matching args\n\nOne thing we want to be able to do is to add a tensor of scalars to a tensor of\nencrypted valus and vise versa. This was very tricky to do and we need to\nlook closer to make sure I didn't add any security issues but I think\nit looks right.\n\n* Add comment\n\n* Remove unused code\n\n* Add support for encrypted matrix multiplication\n\n* Fix bug in arithmetic\n\n* Fix bug in matmul\n\n* Fix bug in test\n\n* Remove verbose comments\n\n* Run black\n\n* Add phe to requirements.txt\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_section_1_differential_privacy():",
            "query_result = np.argmax(counts)",
            "query_result",
            "",
            "-    from syft.frameworks.torch.differential_privacy import pate",
            "+    from syft.frameworks.torch.dp import pate",
            "",
            "num_teachers, num_examples, num_labels = (100, 100, 10)",
            "preds = (np.random.rand(num_teachers, num_examples) * num_labels).astype(int)  # fake preds"
        ]
    },
    {
        "number": 1368,
        "comments": "",
        "commit_message": "fixed problem with multi-state/action networks and scoping\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Categorical(Distribution):",
            "logits = [log(prob) for _ in range(util.prod(shape)) for prob in probabilities]",
            "action_size = util.prod(self.shape) * self.num_actions",
            "",
            "-        with tf.name_scope(name=scope):",
            "-            self.logits = Linear(size=action_size, bias=logits, scope='logits')",
            "+        self.logits = Linear(size=action_size, bias=logits, scope='logits')",
            "",
            "super(Categorical, self).__init__(scope, summary_labels)"
        ]
    },
    {
        "number": 1373,
        "comments": "",
        "commit_message": "[Train] Fix accuracy calculation for CIFAR example (#22292)\n\nSame as #21689 except for cifar\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def train_func(config):",
            "train_dataset = Subset(train_dataset, list(range(64)))",
            "validation_dataset = Subset(validation_dataset, list(range(64)))",
            "",
            "-    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"])",
            "-    validation_loader = DataLoader(validation_dataset, batch_size=config[\"batch_size\"])",
            "+    worker_batch_size = config[\"batch_size\"] // train.world_size()",
            "+",
            "+    train_loader = DataLoader(train_dataset, batch_size=worker_batch_size)",
            "+    validation_loader = DataLoader(validation_dataset, batch_size=worker_batch_size)",
            "",
            "train_loader = train.torch.prepare_data_loader(train_loader)",
            "validation_loader = train.torch.prepare_data_loader(validation_loader)"
        ]
    },
    {
        "number": 1397,
        "comments": "",
        "commit_message": "Fixed audio/bag/binary feature tests to work on GPU (#1600)\n\n* Fixed audio/bag/binary feature tests to work on GPU\n\n* Added timeseries feature test (#1601)\n\n* Removed device transfer calls from encoders to features\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def binary_config():",
            "def test_binary_input_feature(binary_config: Dict, encoder: str) -> None:",
            "binary_config.update({\"encoder\": encoder})",
            "binary_input_feature = BinaryInputFeature(binary_config)",
            "-    binary_tensor = torch.randn([SEQ_SIZE, BINARY_W_SIZE], dtype=torch.float32)",
            "+    binary_tensor = torch.randn([SEQ_SIZE, BINARY_W_SIZE], dtype=torch.float32).to(DEVICE)",
            "encoder_output = binary_input_feature(binary_tensor)",
            "assert encoder_output[\"encoder_output\"].shape[1:] == binary_input_feature.output_shape"
        ]
    },
    {
        "number": 1399,
        "comments": "",
        "commit_message": "Drop JIT support for `core.check`, `Boxes`, and others (#2219)\n\n* Drop JIT support for `core.check` API\n\n- Consequently for this, we drop support of JIT on the following items: (in of dynamo)\n  - enhance\n    - AdjustSigmoid\n    - AdjustLog\n    - AddWeighted\n  - geometry\n    - UndistortPoints\n    - bbox and Boxes - follow up on #2218\n    - EuclideanDistance\n    - TransformPoints\n    - HomographyWarper\n    - WarpPerspective\n    - UpscaleDouble\n  - losses\n\n* Update typing with pyupgrade\n* drop all jit related from bbox and boxes\n\nfrom #2218\n* fix/skip failing dynamo tests\n* fix loss hd\n* fix typing\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TestEulerFromQuaternion(BaseTester):",
            "def test_module(self, device, dtype):",
            "pass",
            "",
            "-    def test_jit(self, device, dtype):",
            "+    def test_dynamo(self, device, dtype, torch_optimizer):",
            "q = Quaternion.random(batch_size=1)",
            "q = q.to(device, dtype)",
            "op = euler_from_quaternion",
            "-        op_jit = torch.jit.script(op)",
            "-        assert_close(op(q.w, q.x, q.y, q.z), op_jit(q.w, q.x, q.y, q.z))",
            "+        op_optimized = torch_optimizer(op)",
            "+        assert_close(op(q.w, q.x, q.y, q.z), op_optimized(q.w, q.x, q.y, q.z))",
            "",
            "def test_forth_and_back(self, device, dtype):",
            "q = Quaternion.random(batch_size=2)"
        ]
    },
    {
        "number": 1407,
        "comments": "",
        "commit_message": "Add Cuda tests for pyro.distributions; fix errors (#297)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Gamma(Distribution):",
            "alpha = alpha.expand_as(x)",
            "beta = beta.expand_as(x)",
            "ll_1 = - beta * x",
            "-        ll_2 = (alpha - pyro.ones(x.size())) * torch.log(x)",
            "+        ll_2 = (alpha - 1.0) * torch.log(x)",
            "ll_3 = alpha * torch.log(beta)",
            "ll_4 = - log_gamma(alpha)",
            "return ll_1 + ll_2 + ll_3 + ll_4"
        ]
    },
    {
        "number": 1415,
        "comments": "",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class SwapBufferManager(object):",
            "self.count = count",
            "self.dtype = dtype",
            "self.all_buffers = [",
            "-            torch.zeros(num_elems,",
            "-                        device='cpu',",
            "-                        dtype=dtype).pin_memory() for _ in range(count)",
            "+            get_accelerator().pin_memory(",
            "+                torch.zeros(num_elems,",
            "+                            device='cpu',",
            "+                            dtype=dtype)) for _ in range(count)",
            "]",
            "self.free_buffer_index = [i for i in range(count)]",
            "self.used_buffer_index = {}"
        ]
    },
    {
        "number": 1416,
        "comments": "",
        "commit_message": "Fix PT TF ViTMAE (#16766)\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ViTMAEDecoder(nn.Module):",
            "[ViTMAELayer(decoder_config) for _ in range(config.decoder_num_hidden_layers)]",
            ")",
            "",
            "-        self.decoder_norm = nn.LayerNorm(config.decoder_hidden_size)",
            "+        self.decoder_norm = nn.LayerNorm(config.decoder_hidden_size, eps=config.layer_norm_eps)",
            "self.decoder_pred = nn.Linear(",
            "config.decoder_hidden_size, config.patch_size**2 * config.num_channels, bias=True",
            ")  # encoder to decoder"
        ]
    },
    {
        "number": 1422,
        "comments": "",
        "commit_message": "New logger connector code (#7882)\n\n* New logger connector code\n\n* Update CHANGELOG\n\n* Update requirements\n\n* Fix import path\n\n* Add new suffix\n\n* Tests\n\n* Minor changes\n\n* Rename and reorder\n\n* Fix import\n\n* Formatting\n\n* Fix with seed_everything?\n\n* Fix test?\n\n* Fix test?\n\n* Minor change\n\n* Minor changes\n\n* Minor changes\n\n* Force float\n\n* Fix minimal bug\n\n* Fix minimal bug\n\n* Update with latest changes\n\n* Fix import\n\n* bad merge\n\n* update typing\n\nCo-authored-by: tchaton <thomas@grid.ai>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def _setup_ddp(rank, worldsize):",
            "def _ddp_test_fn(rank, worldsize):",
            "_setup_ddp(rank, worldsize)",
            "tensor = torch.tensor([1.0])",
            "-    actual = LightningModule._LightningModule__sync(tensor, sync_dist=True, sync_dist_op=torch.distributed.ReduceOp.SUM)",
            "+    sync = _Sync(sync_ddp_if_available, should=True, op=torch.distributed.ReduceOp.SUM)",
            "+    actual = sync(tensor)",
            "assert actual.item() == dist.get_world_size(), \"Result-Log does not work properly with DDP and Tensors\""
        ]
    },
    {
        "number": 1430,
        "comments": "",
        "commit_message": "re-organize predict/; fix TF incompatibile change of sparse_softmax_cross_entropy_loss\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Model(ModelDesc):",
            ".FullyConnected('fc1', 512, nl=tf.nn.relu) \\",
            ".FullyConnected('linear', out_dim=self.cifar_classnum, nl=tf.identity)()",
            "",
            "-        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)",
            "+        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
            "cost = tf.reduce_mean(cost, name='cross_entropy_loss')",
            "",
            "wrong = symbf.prediction_incorrect(logits, label)"
        ]
    },
    {
        "number": 1436,
        "comments": "",
        "commit_message": "leaf Variable inplace bug fix (#1619)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Model(nn.Module):",
            "m = self.model[-1]  # Detect() module",
            "for mi, s in zip(m.m, m.stride):  # from",
            "b = mi.bias.view(m.na, -1)  # conv.bias(255) to (3,85)",
            "-            b[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)",
            "-            b[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls",
            "+            b.data[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)",
            "+            b.data[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls",
            "mi.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)",
            "",
            "def _print_biases(self):"
        ]
    },
    {
        "number": 1438,
        "comments": "",
        "commit_message": "[sgd] Fixes TrainingOperator wrap model incorrect (#14353)\n\nCo-authored-by: Amog Kamsetty <amogkamsetty@yahoo.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TrainingOperator:",
            "",
            "logger.debug(\"Registering optimizers.\")",
            "self._optimizers = optimizers",
            "-        if not isinstance(self._optimizers, Iterable):",
            "+        if isinstance(self._optimizers, torch.optim.Optimizer):",
            "self._optimizers = [self._optimizers]",
            "",
            "if schedulers:"
        ]
    },
    {
        "number": 1443,
        "comments": "",
        "commit_message": "Fx support for multiple model architectures (#17393)\n\n* Support for Bart and LayoutLM, and partial support for XLNet\n\n* Support for mbart\n\n* A lot of new models supported\n\n* Support for other models\n\n* LayoutLM fix\n\n* Use strings instead of classes\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class LayoutLMModel(LayoutLMPreTrainedModel):",
            "token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)",
            "",
            "if bbox is None:",
            "-            bbox = torch.zeros(tuple(list(input_shape) + [4]), dtype=torch.long, device=device)",
            "+            bbox = torch.zeros(input_shape + (4,), dtype=torch.long, device=device)",
            "",
            "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)"
        ]
    },
    {
        "number": 1450,
        "comments": "",
        "commit_message": "Fix loss\n\nPlease review @thomwolf but i think this is equivqlent (and it mimics the loss computation of the original loss)\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BertForQuestionAnswering(nn.Module):",
            "",
            "def compute_loss(logits, positions):",
            "max_position = positions.max().item()",
            "-                one_hot = torch.FloatTensor(batch_size, max(max_position, seq_length) +1, device=input_ids.device).zero_()",
            "+                one_hot = torch.FloatTensor(batch_size, max(max_position, seq_length) +1).zero_()",
            "one_hot = one_hot.scatter(1, positions.cpu(), 1) # Second argument need to be LongTensor and not cuda.LongTensor",
            "-                one_hot = one_hot[:, :seq_length]",
            "+                one_hot = one_hot[:, :seq_length].to(input_ids.device)",
            "log_probs = nn.functional.log_softmax(logits, dim = -1).view(batch_size, seq_length)",
            "loss = -torch.mean(torch.sum(one_hot*log_probs), dim = -1)",
            "return loss"
        ]
    },
    {
        "number": 1459,
        "comments": "",
        "commit_message": "enable `disallow_incomplete_defs` on mypy (#2094)\n\n* enable `disallow_incomplete_defs` on mypy\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix `blur_pool2d` doc\n\n* finish v1: works on torch 1.13.1\n\n- Remove JIT support for Boxes3D\n\n* rip off the np typing\n\n* replace `Size` with `Tuple[int, ...]` on augs\n\n* add `Dtype` to kornia.filters.kernels\n\n* minor fix after rebase\n\n* Remove old torch from typing CI\n\n---------\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def vec_like(n: int, tensor: torch.Tensor, shared_memory: bool = False):",
            "if len(tensor.shape) < 1:",
            "raise AssertionError(tensor.shape)",
            "",
            "-    vec = torch.zeros(n, 1, device=tensor.device, dtype=tensor.dtype)",
            "+    vec = zeros(n, 1, device=tensor.device, dtype=tensor.dtype)",
            "return vec[None].expand(tensor.shape[0], n, 1) if shared_memory else vec[None].repeat(tensor.shape[0], 1, 1)"
        ]
    },
    {
        "number": 1462,
        "comments": "",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class LanguageModel(Model):",
            "return_dict = {}",
            "",
            "# If we have target tokens, calculate the loss.",
            "-        token_ids = source.get(\"tokens\")",
            "-        if token_ids is not None:",
            "+        token_id_dict = source.get(\"tokens\")",
            "+        if token_id_dict is not None:",
            "+            token_ids = token_id_dict[\"tokens\"]",
            "assert isinstance(contextual_embeddings, torch.Tensor)",
            "",
            "# Use token_ids to compute targets"
        ]
    },
    {
        "number": 1463,
        "comments": "",
        "commit_message": "Fix SummaryWritter error and similar deprecated warnings (#248)\n\nTF1.0 advises to use FileWriter and similarly pathed functions to replace deprecated ones.\n  All changes here are related to fixing deprecated errors and warnings.\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def main():",
            "",
            "# Save the result as an audio summary.",
            "datestring = str(datetime.now()).replace(' ', 'T')",
            "-    writer = tf.train.SummaryWriter(logdir)",
            "-    tf.audio_summary('generated', decode, wavenet_params['sample_rate'])",
            "-    summaries = tf.merge_all_summaries()",
            "+    writer = tf.summary.FileWriter(logdir)",
            "+    tf.summary.audio('generated', decode, wavenet_params['sample_rate'])",
            "+    summaries = tf.summary.merge_all()",
            "summary_out = sess.run(summaries,",
            "feed_dict={samples: np.reshape(waveform, [-1, 1])})",
            "writer.add_summary(summary_out)"
        ]
    },
    {
        "number": 1465,
        "comments": "",
        "commit_message": "Fix CI with change of name of nlp (#7054)\n\n* nlp -> datasets\n\n* More nlp -> datasets\n\n* Woopsie\n\n* More nlp -> datasets\n\n* One last\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def load_indexes():",
            "",
            "@st.cache(allow_output_mutation=True)",
            "def load_train_data():",
            "-    eli5 = nlp.load_dataset(\"eli5\", name=\"LFQA_reddit\")",
            "+    eli5 = datasets.load_dataset(\"eli5\", name=\"LFQA_reddit\")",
            "eli5_train = eli5[\"train_eli5\"]",
            "eli5_train_q_reps = np.memmap(",
            "\"eli5_questions_reps.dat\", dtype=\"float32\", mode=\"r\", shape=(eli5_train.num_rows, 128)"
        ]
    },
    {
        "number": 1466,
        "comments": "",
        "commit_message": "Fixed failing test for depthwise_conv2d (#9229)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def depthwise_conv2d(",
            "dilations: Optional[Union[int, Tuple[int, int]]] = 1,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    x = torch.tensor(x)",
            "-    filters = torch.tensor(filters)",
            "+    x = torch.as_tensor(x)",
            "+    filters = torch.as_tensor(filters)",
            "strides = [strides] * 2 if isinstance(strides, int) else strides",
            "strides = [strides[1], strides[2]] if len(strides) == 4 else strides",
            "dilations = [dilations] * 2 if isinstance(dilations, int) else dilations",
            "-    filters = ivy.squeeze(filters, 3) if filters.ndim == 4 else filters",
            "+    filters = ivy.squeeze(filters, 3).to_native() if filters.ndim == 4 else filters",
            "",
            "f_w_after_dilation = filters.shape[1] + (",
            "(dilations[1] - 1) * (filters.shape[1] - 1)"
        ]
    },
    {
        "number": 1469,
        "comments": "",
        "commit_message": "Fix test suite when running on MPS-enabled hardware (#14708)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_lite_dataloader_device_placement(src_device_str, dest_device_str):",
            "iterator = iter(lite_dataloader)",
            "",
            "batch0 = next(iterator)",
            "-    # TODO: This should be torch.equal, but not supported on MPS at this time (torch 1.12)",
            "-    assert torch.allclose(batch0, torch.tensor([0, 1], device=dest_device))",
            "+    # TODO: torch.equal is not supported on MPS at this time (torch 1.12)",
            "+    assert torch.equal(batch0, torch.tensor([0, 1], device=dest_device))",
            "",
            "batch1 = next(iterator)",
            "-    # TODO: This should be torch.equal, but not supported on MPS at this time (torch 1.12)",
            "-    assert torch.allclose(batch1[\"data\"], torch.tensor([2, 3], device=dest_device))",
            "+    # TODO: torch.equal is not supported on MPS at this time (torch 1.12)",
            "+    assert torch.equal(batch1[\"data\"], torch.tensor([2, 3], device=dest_device))",
            "",
            "",
            "def test_lite_optimizer_wraps():"
        ]
    },
    {
        "number": 1476,
        "comments": "",
        "commit_message": "Fixes calibration and adds example scripts (#2431)\n\n* Adds calibration to binary and category output feature schema.\n\n* Adds type annotations for create_calibration_module.\n\n* Fixes initialization of calibration module for category features.\n\n* First pass at forest cover and mushroom edibility.\n\n* Fixed brier plot.\n\n* Adds forest cover visualizations.\n\n* Reduce epochs to 1 and default transformer params.\n\n* Adds calibration as an output feature key which should not be nested inside decoder.\n\n* Moved output_features below input_features.\n\nCo-authored-by: Daniel Treiman <daniel@predibase.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BinaryOutputFeature(BinaryFeatureMixin, OutputFeature):",
            "confidence_penalty=self.loss[\"confidence_penalty\"],",
            ")",
            "",
            "-    def create_calibration_module(self, feature) -> torch.nn.Module:",
            "+    def create_calibration_module(self, feature: BinaryOutputFeatureConfig) -> torch.nn.Module:",
            "\"\"\"Creates the appropriate calibration module based on the feature config.",
            "",
            "Today, only one type of calibration (\"temperature_scaling\") is available, but more options may be supported in",
            "the future.",
            "\"\"\"",
            "-        if feature.get(\"calibration\"):",
            "+        if feature.calibration:",
            "calibration_cls = calibration.get_calibration_cls(BINARY, \"temperature_scaling\")",
            "return calibration_cls(binary=True)",
            "return None"
        ]
    },
    {
        "number": 1477,
        "comments": "",
        "commit_message": "Fix layer choice on IT and deprecate \"choices\" and \"length\" (#2386)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class RandomMutator(Mutator):",
            "result = dict()",
            "for mutable in self.mutables:",
            "if isinstance(mutable, LayerChoice):",
            "-                gen_index = torch.randint(high=mutable.length, size=(1, ))",
            "-                result[mutable.key] = F.one_hot(gen_index, num_classes=mutable.length).view(-1).bool()",
            "+                gen_index = torch.randint(high=len(mutable), size=(1, ))",
            "+                result[mutable.key] = F.one_hot(gen_index, num_classes=len(mutable)).view(-1).bool()",
            "elif isinstance(mutable, InputChoice):",
            "if mutable.n_chosen is None:",
            "result[mutable.key] = torch.randint(high=2, size=(mutable.n_candidates,)).view(-1).bool()"
        ]
    },
    {
        "number": 1479,
        "comments": "",
        "commit_message": "Fix visualize activations (#1211)\n\nFix visualize activations (Squashed 4 commits by @keineahnung2345)\n\n# Get activations of a few sample layers\nactivations = model.run_graph([image], [\n    (\"input_image\",        model.keras_model.get_layer(\"input_image\").output)\n])\nleads to the error:\nInvalidArgumentError: input_image:0 is both fed and fetched.\n\nRevise the code according to https://stackoverflow.com/questions/39307108/placeholder-20-is-both-fed-and-fetched\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "\"source\": [",
            "\"# Get activations of a few sample layers\\n\",",
            "\"activations = model.run_graph([image], [\\n\",",
            "-    \"    (\\\"input_image\\\",        model.keras_model.get_layer(\\\"input_image\\\").output),\\n\",",
            "+    \"    (\\\"input_image\\\",        tf.identity(model.keras_model.get_layer(\\\"input_image\\\").output)),\\n\",",
            "\"    (\\\"res2c_out\\\",          model.keras_model.get_layer(\\\"res2c_out\\\").output),\\n\",",
            "\"    (\\\"res3c_out\\\",          model.keras_model.get_layer(\\\"res3c_out\\\").output),\\n\",",
            "\"    (\\\"res4w_out\\\",          model.keras_model.get_layer(\\\"res4w_out\\\").output),  # for resnet100\\n\","
        ]
    },
    {
        "number": 1480,
        "comments": "",
        "commit_message": "Revert \"revert padding bug fix for now\" with fix\n\nThis reverts commit ca5819dec327be9e49412ce69909feea72f5d752.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def sample_autoregressive(partial_sequences,",
            "if has_partial_sequences and remove_partial_sequences:",
            "# remove partial sequences from outputs",
            "partial_length = mtf.reduce_sum(",
            "-            mtf.to_int32(mtf.not_equal(partial_sequences, 0)),",
            "+            mtf.to_int32(mtf.not_equal(partial_sequences, padding_id)),",
            "reduced_dim=length_dim)",
            "outputs = mtf.dynamic_shift(",
            "outputs, -partial_length, length_dim, wrap=False)"
        ]
    },
    {
        "number": 1491,
        "comments": "",
        "commit_message": "fixed categorical bug\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Categorical(Distribution):",
            "_ps, _vs, _one_hot = self._sanitize_input(ps, vs, one_hot)",
            "_vs = self._process_v(_vs)",
            "_ps, _vs = self._process_p(_ps, _vs)",
            "-        sample = Variable(torch.multinomial(_ps.data, 1, replacement=True))",
            "+        sample = Variable(torch.multinomial(_ps.data, 1, replacement=True).type_as(_ps.data))",
            "if _vs is not None:",
            "if isinstance(_vs, np.ndarray):",
            "# always returns a 2-d (unsqueezed 1-d) list"
        ]
    },
    {
        "number": 1495,
        "comments": "",
        "commit_message": "Fix torch meshgrid warnings (#20475)\n\n* fix torch meshgrid warnings\n\n* support lower torch versions\n\n* don't edit examples\n\n* dont edit examples\n\n* fix ci\n\n* fix style\n\n* rebase cleanup\n\n* fix ci again\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BeitRelativePositionBias(nn.Module):",
            "# get pair-wise relative position index for each token inside the window",
            "coords_h = torch.arange(window_size[0])",
            "coords_w = torch.arange(window_size[1])",
            "-        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww",
            "+        coords = torch.stack(meshgrid([coords_h, coords_w], indexing=\"ij\"))  # 2, Wh, Ww",
            "coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww",
            "relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww",
            "relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2"
        ]
    },
    {
        "number": 1507,
        "comments": "",
        "commit_message": "ivy.Shape fixes (#2006)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def indices_where(",
            "def shape(",
            "x: Union[tf.Tensor, tf.Variable],",
            "as_array: bool = False,",
            "-) -> Union[tf.Tensor, tf.Variable, TensorShape]:",
            "+) -> Union[tf.Tensor, ivy.Shape, ivy.Array]:",
            "if as_array:",
            "-        return tf.shape(x)",
            "+        return ivy.array(tf.shape(x))",
            "else:",
            "-        return tuple(x.shape)",
            "+        return ivy.Shape(x.shape)",
            "",
            "",
            "def get_num_dims(x, as_tensor=False):"
        ]
    },
    {
        "number": 1516,
        "comments": "",
        "commit_message": "fix elu activation function (#853)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def selu(x):",
            "\"\"\"",
            "alpha = 1.6732632423543772848170429916717",
            "scale = 1.0507009873554804934193349852946",
            "-    return scale * tf.nn.elu(x, alpha)",
            "+    return scale*tf.where(x>=0.0, x, alpha*tf.nn.elu(x))"
        ]
    },
    {
        "number": 1523,
        "comments": "",
        "commit_message": "0.2.17 (#231)\n\n* delete\n\n* Print Information Changes for AutoKeras v0.3 (#229) resolves #224\n\n* Update CONTRIBUTING.md\n\n* Develop (#187)\n\n* merge (#128)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* rm print\n\n* Issue#37 and Issue #79 Save keras model/autokeras model (#122)\n\n* Issue #37 Export Keras model\n\n* Issue #79 Save autokeras model\n\n* Issue #37 and Issue#79 Fixed comments\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79 Fixed pytests\n\n* Issue #37 and Issue #79\n\n* quick fix test\n\n* Progbar (#143)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* contribute guide\n\n* Add Progress Bar\n\n* Update utils.py\n\n* Update search.py\n\n* update constant (#145)\n\n* [WIP] Issue #158 Imageregressor (#159)\n\n* Develop (#146)\n\n* merge (#128)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* rm print\n\n* Issue#37 and Issue #79 Save keras model/autokeras model (#122)\n\n* Issue #37 Export Keras model\n\n* Issue #79 Save autokeras model\n\n* Issue #37 and Issue#79 Fixed comments\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79 Fixed pytests\n\n* Issue #37 and Issue #79\n\n* quick fix test\n\n* Progbar (#143)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* contribute guide\n\n* Add Progress Bar\n\n* Update utils.py\n\n* Update search.py\n\n* update constant (#145)\n\n* Update setup.py (#147)\n\n* Update setup.py\n\n* Update setup.py\n\n* Update setup.py (#155)\n\n* requirements\n\n* Issue #158 Export ImageRegressor model\n\n* Memory (#161)\n\n* aa\n\n* limit memory\n\n* refactor to_real_layer to member functions\n\n* bug fix (#166)\n\n* doc string changed for augment (#170)\n\nI added proper documentation for class ImageSupervised  arg 'augment'. It is 'None' by default. However, if it is 'None', then it uses Constant.DATA_AUGMENTATION which is 'True'. This is misleading when trying things out.\n\n* Update constant.py\n\n* bug fix (#177)\n\n* memory limit dynamically (#180)\n\n* memory limit dynamically\n\n* test\n\n* test fixed\n\n* [MRG]Dcgan (#175)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* finish workable version of gan\n\n* add unit test and small refactoring\n\n* add unsupervised super class\n\n* Fix test_dcgan ran too long issue, put default param in unsupervised::generate(input_sample=None)\n\n* remove examples/gan.py from repo\n\n* add missing import\n\n* correct model_trainer signature\n\n* fixed the bug in return value of train_model()\n\n* Update setup.py\n\n* [WIP]Update CONTRIBUTING.md (#190)\n\n* Update CONTRIBUTING.md\n\n* Update CONTRIBUTING.md\n\n* Update mkdocs.yml\n\n* code_reuse_example\n\n* Update CONTRIBUTING.md\n\n* update develop (#206) (#207)\n\n* Update CONTRIBUTING.md\n\n* Develop (#187)\n\n* merge (#128)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* rm print\n\n* Issue#37 and Issue #79 Save keras model/autokeras model (#122)\n\n* Issue #37 Export Keras model\n\n* Issue #79 Save autokeras model\n\n* Issue #37 and Issue#79 Fixed comments\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79 Fixed pytests\n\n* Issue #37 and Issue #79\n\n* quick fix test\n\n* Progbar (#143)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* contribute guide\n\n* Add Progress Bar\n\n* Update utils.py\n\n* Update search.py\n\n* update constant (#145)\n\n* [WIP] Issue #158 Imageregressor (#159)\n\n* Develop (#146)\n\n* merge (#128)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* rm print\n\n* Issue#37 and Issue #79 Save keras model/autokeras model (#122)\n\n* Issue #37 Export Keras model\n\n* Issue #79 Save autokeras model\n\n* Issue #37 and Issue#79 Fixed comments\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79 Fixed pytests\n\n* Issue #37 and Issue #79\n\n* quick fix test\n\n* Progbar (#143)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* contribute guide\n\n* Add Progress Bar\n\n* Update utils.py\n\n* Update search.py\n\n* update constant (#145)\n\n* Update setup.py (#147)\n\n* Update setup.py\n\n* Update setup.py\n\n* Update setup.py (#155)\n\n* requirements\n\n* Issue #158 Export ImageRegressor model\n\n* Memory (#161)\n\n* aa\n\n* limit memory\n\n* refactor to_real_layer to member functions\n\n* bug fix (#166)\n\n* doc string changed for augment (#170)\n\nI added proper documentation for class ImageSupervised  arg 'augment'. It is 'None' by default. However, if it is 'None', then it uses Constant.DATA_AUGMENTATION which is 'True'. This is misleading when trying things out.\n\n* Update constant.py\n\n* bug fix (#177)\n\n* memory limit dynamically (#180)\n\n* memory limit dynamically\n\n* test\n\n* test fixed\n\n* [MRG]Dcgan (#175)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* finish workable version of gan\n\n* add unit test and small refactoring\n\n* add unsupervised super class\n\n* Fix test_dcgan ran too long issue, put default param in unsupervised::generate(input_sample=None)\n\n* remove examples/gan.py from repo\n\n* add missing import\n\n* correct model_trainer signature\n\n* fixed the bug in return value of train_model()\n\n* Update setup.py\n\n* [WIP]Update CONTRIBUTING.md (#190)\n\n* Update CONTRIBUTING.md\n\n* Update CONTRIBUTING.md\n\n* Update mkdocs.yml\n\n* code_reuse_example\n\n* Update CONTRIBUTING.md\n\n* Update search.py\n\n* Develop (#215)\n\n* update develop (#206)\n\n* Update CONTRIBUTING.md\n\n* Develop (#187)\n\n* merge (#128)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* rm print\n\n* Issue#37 and Issue #79 Save keras model/autokeras model (#122)\n\n* Issue #37 Export Keras model\n\n* Issue #79 Save autokeras model\n\n* Issue #37 and Issue#79 Fixed comments\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79 Fixed pytests\n\n* Issue #37 and Issue #79\n\n* quick fix test\n\n* Progbar (#143)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* contribute guide\n\n* Add Progress Bar\n\n* Update utils.py\n\n* Update search.py\n\n* update constant (#145)\n\n* [WIP] Issue #158 Imageregressor (#159)\n\n* Develop (#146)\n\n* merge (#128)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* rm print\n\n* Issue#37 and Issue #79 Save keras model/autokeras model (#122)\n\n* Issue #37 Export Keras model\n\n* Issue #79 Save autokeras model\n\n* Issue #37 and Issue#79 Fixed comments\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79 Fixed pytests\n\n* Issue #37 and Issue #79\n\n* quick fix test\n\n* Progbar (#143)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* contribute guide\n\n* Add Progress Bar\n\n* Update utils.py\n\n* Update search.py\n\n* update constant (#145)\n\n* Update setup.py (#147)\n\n* Update setup.py\n\n* Update setup.py\n\n* Update setup.py (#155)\n\n* requirements\n\n* Issue #158 Export ImageRegressor model\n\n* Memory (#161)\n\n* aa\n\n* limit memory\n\n* refactor to_real_layer to member functions\n\n* bug fix (#166)\n\n* doc string changed for augment (#170)\n\nI added proper documentation for class ImageSupervised  arg 'augment'. It is 'None' by default. However, if it is 'None', then it uses Constant.DATA_AUGMENTATION which is 'True'. This is misleading when trying things out.\n\n* Update constant.py\n\n* bug fix (#177)\n\n* memory limit dynamically (#180)\n\n* memory limit dynamically\n\n* test\n\n* test fixed\n\n* [MRG]Dcgan (#175)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* finish workable version of gan\n\n* add unit test and small refactoring\n\n* add unsupervised super class\n\n* Fix test_dcgan ran too long issue, put default param in unsupervised::generate(input_sample=None)\n\n* remove examples/gan.py from repo\n\n* add missing import\n\n* correct model_trainer signature\n\n* fixed the bug in return value of train_model()\n\n* Update setup.py\n\n* [WIP]Update CONTRIBUTING.md (#190)\n\n* Update CONTRIBUTING.md\n\n* Update CONTRIBUTING.md\n\n* Update mkdocs.yml\n\n* code_reuse_example\n\n* Update CONTRIBUTING.md\n\n* bug_fix (#208)\n\n* bug_fix (#214) resolves #212\n\n* Update CONTRIBUTING.md\n\n* Update setup.py\n\n* delete (#217)\n\n* Print Information Changes for AutoKeras v0.3\n\n* 0.2.17\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Searcher:",
            "if not re.search('out of memory', str(e)):",
            "raise e",
            "if self.verbose:",
            "-                print('out of memory')",
            "+                print('\\nCurrent model size is too big. Discontinuing training this model to search for other models.')",
            "Constant.MAX_MODEL_SIZE = graph.size() - 1",
            "return",
            "finally:"
        ]
    },
    {
        "number": 1532,
        "comments": "",
        "commit_message": "small fixes for remainder in all backends, when modulus=False.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def remainder(",
            "res_floored = torch.where(res >= 0, torch.floor(res), torch.ceil(res))",
            "diff = res - res_floored",
            "diff, x2 = ivy.promote_types_of_inputs(diff, x2)",
            "-        return torch.mul(diff, x2, out=out).to(x1.dtype)",
            "+        return torch.round(torch.mul(diff, x2, out=out), out=out).to(x1.dtype)",
            "return torch.remainder(x1, x2, out=out)"
        ]
    },
    {
        "number": 1533,
        "comments": "",
        "commit_message": "Add test-cuda make target, and fix cuda bugs (#338)\n\n* Add test-cuda make target; fix cuda bugs\n\n* Fix more cuda bugs; distributions now passes\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class NormalChol(Distribution):",
            "mu, L = self._sanitize_input(mu, L)",
            "ll_1 = Variable(torch.Tensor([-0.5 * mu.size(0) * np.log(2.0 * np.pi)])",
            ".type_as(mu.data))",
            "+        if L.dim() > 2:",
            "+            raise NotImplementedError(\"torch.diag() does not support tesors of dim > 2\")",
            "ll_2 = -torch.sum(torch.log(torch.diag(L)))",
            "# torch.trtrs() does not support cuda tensors.",
            "x_chols = torch.trtrs((x - mu).unsqueeze(1).data.cpu(), L.data.cpu(), False)"
        ]
    },
    {
        "number": 1536,
        "comments": "",
        "commit_message": "[Fix] Move init dist connection into the setup function (#6506)\n\n* Move connection setup into the setup function. Call setup hook after we set up the accelerator\n\n* Added CHANGELOG.md\n\n* fix setup order in callback test\n\n* fix input arguments in test\n\n* Mock distributed function, remove protection to turn into training type hook\n\n* Remove import\n\n* Add missing mock, ensure custom plugin does not create children process\n\n* Skip test on windows\n\n* Update deepspeed to init connection in setup\n\n* Do not initialize distributed module\n\n* Move DeepSpeed tests to special tests since dist communication is being set up\n\n* Special the test to see if this fixes CI\n\n* Delete accelerator connector test to see if its causing build to fail\n\n* Delete deepspeed test\n\n* Revert \"Delete accelerator connector test to see if its causing build to fail\"\n\nThis reverts commit edde60b8\n\n* Revert \"Delete deepspeed test\"\n\nThis reverts commit 9d317429\n\n* Reverse hook\n\n* Reverse setup hooks to debug again\n\n* Add todo so i know where i left off\n\n* For single device move in pre_dispatch after setup function\n\n* Add additional model to device hook if any additional parameters have been set\n\n* See if we can enable deepspeed tests\n\n* Revert \"See if we can enable deepspeed tests\"\n\nThis reverts commit b5450def\n\n* See if this hook approach works\n\n* Introduce new granular hooks\n\n* Remove import, fix tpu spawn by moving the function to setup\n\n* Added missing special test\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class SingleDevicePlugin(TrainingTypePlugin):",
            "",
            "self._model.to(self.root_device)",
            "",
            "-    def connect(self, model: torch.nn.Module) -> torch.nn.Module:",
            "-        self._model = model",
            "+    def setup(self, model: torch.nn.Module) -> torch.nn.Module:",
            "self.model_to_device()",
            "return self.model"
        ]
    },
    {
        "number": 1537,
        "comments": "",
        "commit_message": "fix adaptive softmax indexing\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class AdaptiveSoftmax(nn.Module):",
            "",
            "head_sz = self.cutoff[0] + len(self.tail)",
            "log_probs[:, :head_sz] = self.lsm(head_y)",
            "-        tail_priors = log_probs[:, self.cutoff[0] - 1: head_sz - 1].clone()",
            "+        tail_priors = log_probs[:, self.cutoff[0] - self.buggy_offset: head_sz - self.buggy_offset].clone()",
            "",
            "for i in range(len(self.tail)):",
            "start = self.cutoff[i]"
        ]
    },
    {
        "number": 1538,
        "comments": "",
        "commit_message": "fix tests\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class UNet2DConditionModel(ModelMixin, ConfigMixin):",
            "# TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can",
            "# This would be a good case for the `match` statement (Python 3.10+)",
            "is_mps = sample.device.type == \"mps\"",
            "-            if torch.is_floating_point(timesteps):",
            "+            if isinstance(timestep, float):",
            "dtype = torch.float32 if is_mps else torch.float64",
            "else:",
            "dtype = torch.int32 if is_mps else torch.int64"
        ]
    },
    {
        "number": 1543,
        "comments": "",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_utilities(head_size):",
            "mask[head_size:, head_size:] = 0.",
            "mask.view(-1)[::size + 1][head_size:] = 1.",
            "arrowhead_full = mask * cov",
            "-    expected = torch.flip(torch.flip(arrowhead_full, (-2, -1)).cholesky(), (-2, -1))",
            "+    expected = torch.flip(",
            "+        torch.linalg.cholesky(torch.flip(arrowhead_full, (-2, -1))), (-2, -1)",
            "+    )",
            "# test if those flip ops give expected upper triangular values",
            "assert_close(expected.triu(), expected)",
            "assert_close(expected.matmul(expected.t()), arrowhead_full)"
        ]
    },
    {
        "number": 1549,
        "comments": "",
        "commit_message": "Replace strided slice with tf.expand_dims (#10078)\n\n* Replace tf.newaxis -> tf.expand_dims\n\n* Fix tests\n\n* Fix tests\n\n* Use reshape when a tensors needs a double expand\n\n* Fix GPT2\n\n* Fix GPT2\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFEmbeddings(tf.keras.layers.Layer):",
            "input_shape = shape_list(inputs_embeds)[:-1]",
            "",
            "if position_ids is None:",
            "-            position_ids = tf.range(start=0, limit=input_shape[-1])[tf.newaxis, :]",
            "+            position_ids = tf.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)",
            "",
            "position_embeds = tf.gather(params=self.position_embeddings, indices=position_ids)",
            "position_embeds = tf.tile(input=position_embeds, multiples=(input_shape[0], 1, 1))"
        ]
    },
    {
        "number": 1552,
        "comments": "",
        "commit_message": "Cleanup the usage of `layer_norm_eps` in some models (#21336)\n\n* fix\n\n* fix\n\n* make style\n\n* For CLIP\n\n* For OwlViT\n\n* For XCLIP\n\n* For CLIPSeg\n\n* For GroupViT\n\n* fix docstrings\n\n* fix docstrings\n\n* For AltCLIP\n\n* For ChineseCLIP\n\n* For Blip\n\n* For GiT\n\n* make style\n\n* update\n\n* update\n\n* update\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class GitProjection(nn.Module):",
            "super().__init__()",
            "self.config = config",
            "self.visual_projection = nn.Sequential(",
            "-            nn.Linear(config.vision_config.hidden_size, config.hidden_size), nn.LayerNorm(config.hidden_size)",
            "+            nn.Linear(config.vision_config.hidden_size, config.hidden_size),",
            "+            nn.LayerNorm(config.hidden_size, eps=config.vision_config.layer_norm_eps),",
            ")",
            "",
            "def forward(self, embeddings: torch.Tensor) -> torch.Tensor:"
        ]
    },
    {
        "number": 1558,
        "comments": "",
        "commit_message": "Fix bug of nas fcos (#3205)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class NASFCOSHead(FCOSHead):",
            "\"\"\"Initialize weights of the head.\"\"\"",
            "# retinanet_bias_init",
            "bias_cls = bias_init_with_prob(0.01)",
            "-        normal_init(self.fcos_reg, std=0.01)",
            "-        normal_init(self.fcos_centerness, std=0.01)",
            "-        normal_init(self.fcos_cls, std=0.01, bias=bias_cls)",
            "+        normal_init(self.conv_reg, std=0.01)",
            "+        normal_init(self.conv_centerness, std=0.01)",
            "+        normal_init(self.conv_cls, std=0.01, bias=bias_cls)",
            "",
            "for branch in [self.cls_convs, self.reg_convs]:",
            "for module in branch.modules():"
        ]
    },
    {
        "number": 1565,
        "comments": "",
        "commit_message": "GH-482: fix CUDA error with CharacterEmbeddings\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CharacterEmbeddings(TokenEmbeddings):",
            "longest_token_in_sentence = max(chars2_length)",
            "tokens_mask = torch.zeros((len(tokens_sorted_by_length), longest_token_in_sentence),",
            "dtype=torch.long, device=flair.device)",
            "+",
            "for i, c in enumerate(tokens_sorted_by_length):",
            "-                tokens_mask[i, :chars2_length[i]] = c",
            "+                tokens_mask[i, :chars2_length[i]] = torch.tensor(c, dtype=torch.long, device=flair.device)",
            "",
            "# chars for rnn processing",
            "chars = tokens_mask"
        ]
    },
    {
        "number": 1568,
        "comments": "",
        "commit_message": "Fix a few issues loading pretrained vit/bit npz weights w/ num_classes=0 __init__ arg. Missed a few other small classifier handling detail on Mlp, GhostNet, Levit. Should fix #713\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MlpMixer(nn.Module):",
            "act_layer=act_layer, drop=drop_rate, drop_path=drop_path_rate)",
            "for _ in range(num_blocks)])",
            "self.norm = norm_layer(embed_dim)",
            "-        self.head = nn.Linear(embed_dim, self.num_classes)  # zero init",
            "+        self.head = nn.Linear(embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()",
            "",
            "self.init_weights(nlhb=nlhb)"
        ]
    },
    {
        "number": 1576,
        "comments": "",
        "commit_message": "AutoBatch `cudnn.benchmark=True` fix (#9448)\n\n* AutoBatch `cudnn.benchmark=True` fix\n\nMay resolve https://github.com/ultralytics/yolov5/issues/9287\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update autobatch.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update autobatch.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update general.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def init_seeds(seed=0, deterministic=False):",
            "torch.manual_seed(seed)",
            "torch.cuda.manual_seed(seed)",
            "torch.cuda.manual_seed_all(seed)  # for Multi-GPU, exception safe",
            "-    torch.backends.cudnn.benchmark = True  # for faster training",
            "+    # torch.backends.cudnn.benchmark = True  # AutoBatch problem https://github.com/ultralytics/yolov5/issues/9287",
            "if deterministic and check_version(torch.__version__, '1.12.0'):  # https://github.com/ultralytics/yolov5/pull/8213",
            "torch.use_deterministic_algorithms(True)",
            "torch.backends.cudnn.deterministic = True"
        ]
    },
    {
        "number": 1579,
        "comments": "",
        "commit_message": "Fixed failing test for linalg inv and updated array_values helper with casting inside _zeroing_and_casting (#4383)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def inv(",
            "*,",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    if tf.math.reduce_any(tf.linalg.det(x) == 0):",
            "+    if tf.math.reduce_any(tf.linalg.det(tf.cast(x, dtype=\"float64\")) == 0):",
            "ret = x",
            "else:",
            "ret = tf.linalg.inv(x)"
        ]
    },
    {
        "number": 1582,
        "comments": "",
        "commit_message": "Use odd shape tensor to represent parameter data in partitioned state (#981)\n\n* use wierd shaped tensor to avoid silent failures when not registering externel params\n\n* fix typo\n\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Init(InsertPostInitMethodToModuleSubClasses):",
            "",
            "see_memory_usage(f'Before partitioning param {param.ds_id} {param.shape}',",
            "force=False)",
            "-            param.data = torch.ones(1).half().to(param.device)",
            "+            param.data = torch.ones(partitioned_param_data_shape).half().to(param.device)",
            "see_memory_usage(f'After partitioning param {param.ds_id} {param.shape}',",
            "force=False)"
        ]
    },
    {
        "number": 1584,
        "comments": "",
        "commit_message": "use functional interface for softmax in attention (#14198)\n\n* use functional interface instead of instantiating module and immediately calling it\n\n* fix torch.nn.functional to nn.functional. Thank you Stas!\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class IntSoftmax(nn.Module):",
            "",
            "def forward(self, x, scaling_factor):",
            "if not self.quant_mode:",
            "-            return nn.Softmax(dim=-1)(x), None",
            "+            return nn.functional.softmax(x, dim=-1), None",
            "",
            "x_int = x / scaling_factor"
        ]
    },
    {
        "number": 1585,
        "comments": "",
        "commit_message": "Don't add keras phase callback if learning phase is fixed (#1205)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def setup_keras_trainer(",
            "input,",
            "get_cost,",
            "lambda: optimizer)",
            "-    if len(keras.backend.learning_phase().consumers()) > 0:",
            "+    if isinstance(keras.backend.learning_phase(), tf.Tensor) and len(keras.backend.learning_phase().consumers()) > 0:",
            "# check if learning_phase is used in this model",
            "trainer.register_callback(KerasPhaseCallback(True))"
        ]
    },
    {
        "number": 1587,
        "comments": "",
        "commit_message": "Finished automatic Torch hooking prototype notebook (#149)\n\n* specify and connect to known_workers on client instantiation\n\n* extending torch hooking prototype to all tensor types\n\n* clean up notebook, add links for navigation\n\n* upstream pull\n\n* fixed some errors, torch.* local execution works again\n\n* finished experimental hooking notebooks\n\n* fixed float tensor init hook in master and retested\n\n* rearranging new notebook\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TorchService(BaseService):",
            "",
            "# FLOAT TENSOR FUNCTIONS",
            "def hook_float_tensor___init__(service_self):",
            "-        def new___init__(self, tensor, owner=service_self, *args, **kwargs):",
            "-            super(torch.FloatTensor, self).__init__(*args, **kwargs)",
            "-            self = owner.register_object(self, False)",
            "+        def new___init__(self, *args):",
            "+            super(torch.FloatTensor, self).__init__()",
            "+            self = service_self.register_object(self, False)",
            "",
            "torch.FloatTensor.__init__ = new___init__"
        ]
    },
    {
        "number": 1590,
        "comments": "",
        "commit_message": "fixed init\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Transformer(TTSInterface, torch.nn.Module):",
            "self._reset_parameters(args)",
            "",
            "def _reset_parameters(self, args):",
            "-        # alpha in scaled positional encoding init",
            "-        self.encoder.embed[-1].alpha.data = torch.tensor(args.initial_encoder_alpha)",
            "-        self.decoder.embed[-1].alpha.data = torch.tensor(args.initial_decoder_alpha)",
            "+        if self.use_scaled_pos_enc:",
            "+            # alpha in scaled positional encoding init",
            "+            self.encoder.embed[-1].alpha.data = torch.tensor(args.initial_encoder_alpha)",
            "+            self.decoder.embed[-1].alpha.data = torch.tensor(args.initial_decoder_alpha)",
            "",
            "if args.transformer_init == \"pytorch\":",
            "return"
        ]
    },
    {
        "number": 1592,
        "comments": "",
        "commit_message": "fixed scatter_sum call\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PNAConv(MessagePassing):",
            "return y",
            "",
            "def aggregate(self, inputs, index, dim_size=None):",
            "-        D = get_degree(inputs, index, self.node_dim, dim_size)",
            "+        D = get_degree(inputs, index, 0, dim_size)",
            "",
            "# aggregators",
            "-        inputs = torch.cat([aggregator(inputs, index, dim=self.node_dim, dim_size=dim_size)",
            "+        inputs = torch.cat([aggregator(inputs, index, dim=0, dim_size=dim_size)",
            "for aggregator in self.aggregators], dim=-1)",
            "# scalers",
            "return torch.cat([scaler(inputs, D, self.avg_d) for scaler in self.scalers], dim=-1)"
        ]
    },
    {
        "number": 1596,
        "comments": "",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class FP16_Optimizer(DeepSpeedOptimizer):",
            "will call ``model.load_state_dict()`` before",
            "``fp16_optimizer_instance.load_state_dict()`` is called.",
            "Example::",
            "-            model = torch.nn.Linear(D_in, D_out).cuda().half()",
            "+            model = torch.nn.Linear(D_in, D_out).to(get_accelerator().device_name()).half()",
            "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)",
            "optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)",
            "..."
        ]
    },
    {
        "number": 1600,
        "comments": "",
        "commit_message": "vae: Fix `UserWarning` (#220)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "reconstruction_function.size_average = False",
            "",
            "",
            "def loss_function(recon_x, x, mu, logvar):",
            "-    BCE = reconstruction_function(recon_x, x)",
            "+    BCE = reconstruction_function(recon_x, x.view(-1, 784))",
            "",
            "# see Appendix B from VAE paper:",
            "# Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014"
        ]
    },
    {
        "number": 1603,
        "comments": "",
        "commit_message": "Refactor for simplification (#9054)\n\n* Refactor for simplification\n\n* cleanup\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def select_device(device='', batch_size=0, newline=True):",
            "assert torch.cuda.is_available() and torch.cuda.device_count() >= len(device.replace(',', '')), \\",
            "f\"Invalid CUDA '--device {device}' requested, use '--device cpu' or pass valid CUDA device(s)\"",
            "",
            "-    if not (cpu or mps) and torch.cuda.is_available():  # prefer GPU if available",
            "+    if not cpu and not mps and torch.cuda.is_available():  # prefer GPU if available",
            "devices = device.split(',') if device else '0'  # range(torch.cuda.device_count())  # i.e. 0,1,6,7",
            "n = len(devices)  # device count",
            "if n > 1 and batch_size > 0:  # check batch_size is divisible by device_count"
        ]
    },
    {
        "number": 1604,
        "comments": "",
        "commit_message": "use commandline-supplied cuda device name instead of cuda:0 for safetensors PR that doesn't fix anything\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def read_state_dict(checkpoint_file, print_global_state=False, map_location=None",
            "if extension.lower() == \".safetensors\":",
            "device = map_location or shared.weight_load_location",
            "if device is None:",
            "-            device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"",
            "+            device = devices.get_cuda_device_string() if torch.cuda.is_available() else \"cpu\"",
            "pl_sd = safetensors.torch.load_file(checkpoint_file, device=device)",
            "else:",
            "pl_sd = torch.load(checkpoint_file, map_location=map_location or shared.weight_load_location)"
        ]
    },
    {
        "number": 1611,
        "comments": "",
        "commit_message": "Fix model loading: use custom scope (#2308)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def RemoteTrainer(estimator, metadata, keras_utils, run_id, dataset_idx):",
            "if LooseVersion(tf.__version__) < LooseVersion(\"2.0.0\"):",
            "model.load_weights(ckpt_file)",
            "else:",
            "-                        model = k.models.load_model(ckpt_file)",
            "+                        # needs to be deserialized in the with scope",
            "+                        with k.utils.custom_object_scope(custom_objects):",
            "+                            model = k.models.load_model(ckpt_file)",
            "serialized_model = keras_utils.serialize_model(model)",
            "else:",
            "with open(ckpt_file, 'rb') as f:"
        ]
    },
    {
        "number": 1613,
        "comments": "",
        "commit_message": "Future 4/n: test & legacy in test/ folder (#13295)\n\n* move: legacy >> test/\n\n* move: tests >> test/\n\n* rename unittests\n\n* update CI\n\n* tests4pl\n\n* tests_pytorch\n\n* proxi\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* ci\n\n* link\n\n* cli\n\n* standalone\n\n* fixing\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* .\n\n* Apply suggestions from code review\n\nCo-authored-by: Akihiro Nitta <nitta@akihironitta.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* alone\n\n* test -> tests\n\n* Standalone fixes\n\n* ci\n\n* Update\n\n* More fixes\n\n* Fix coverage\n\n* Fix mypy\n\n* mypy\n\n* Empty-Commit\n\n* Fix\n\n* mypy just for pl\n\n* Fix standalone\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Akihiro Nitta <nitta@akihironitta.com>\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_lightning_cli_link_arguments(tmpdir):",
            "parser.link_arguments(\"data.batch_size\", \"model.init_args.batch_size\")",
            "parser.link_arguments(\"data.num_classes\", \"model.init_args.num_classes\", apply_on=\"instantiate\")",
            "",
            "-    cli_args[-1] = \"--model=tests.utilities.test_cli.BoringModelRequiredClasses\"",
            "+    cli_args[-1] = \"--model=tests_pytorch.utilities.test_cli.BoringModelRequiredClasses\"",
            "",
            "with mock.patch(\"sys.argv\", [\"any.py\"] + cli_args):",
            "cli = MyLightningCLI("
        ]
    },
    {
        "number": 1615,
        "comments": "",
        "commit_message": "small fix for iou3d\n\nSummary:\nA small numerical fix for IoU for 3D boxes, fixes GH #992\n\n* Adds a check for boxes with zero side areas (invalid boxes)\n* Fixes numerical issue when two boxes have coplanar sides\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D33195691\n\nfbshipit-source-id: 8a34b4d1f1e5ec2edb6d54143930da44bdde0906\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def box3d_overlap(",
            "",
            "_check_coplanar(boxes1, eps)",
            "_check_coplanar(boxes2, eps)",
            "+    _check_nonzero(boxes1, eps)",
            "+    _check_nonzero(boxes2, eps)",
            "",
            "# pyre-fixme[16]: `_box3d_overlap` has no attribute `apply`.",
            "vol, iou = _box3d_overlap.apply(boxes1, boxes2)"
        ]
    },
    {
        "number": 1626,
        "comments": "",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_sample(n_cutpoints, pred_shape):",
            "def test_constraints():",
            "predictor = torch.randn(5)",
            "for cp in (",
            "-        tt([1, 2, 3, 4, 0]),",
            "-        tt([1, 2, 4, 3, 5]),",
            "-        tt([1, 2, 3, 4, 4]),",
            "+        torch.tensor([1, 2, 3, 4, 0]),",
            "+        torch.tensor([1, 2, 4, 3, 5]),",
            "+        torch.tensor([1, 2, 3, 4, 4]),",
            "):",
            "with pytest.raises(ValueError):",
            "OrderedLogistic(predictor, cp)"
        ]
    },
    {
        "number": 1629,
        "comments": "",
        "commit_message": "Traced models serialization and torchscripting fix (#17206)\n\n* Fix torch.jit.script and pickling issues\n\n* Fix get_attr issues\n\n* Fix import in function\n\n* Fix GPT-J and T5 tracing for torch=1.11\n\n* Gate graph surgery on torch version\n\n* Modeling minor changes to enable TorchScripting\n\n* Model serialization / deserialization test\n\n* Remove _assert_is_none users\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MultiHeadSelfAttention(nn.Module):",
            "q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)",
            "scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)",
            "mask = (mask == 0).view(mask_reshp).expand_as(scores)  # (bs, n_heads, q_length, k_length)",
            "-        scores = scores.masked_fill(mask, -float(\"inf\"))  # (bs, n_heads, q_length, k_length)",
            "+        scores = scores.masked_fill(mask, torch.tensor(-float(\"inf\")))  # (bs, n_heads, q_length, k_length)",
            "",
            "weights = nn.functional.softmax(scores, dim=-1)  # (bs, n_heads, q_length, k_length)",
            "weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)"
        ]
    },
    {
        "number": 1643,
        "comments": "",
        "commit_message": "bug fixes for VPG/TRPO/PPO\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class OptimizedStep(MetaOptimizer):",
            "loss_before = fn_compare(reference=reference)",
            "",
            "with tf.control_dependencies(control_inputs=(loss_before,)):",
            "-            applied, diffs = self.optimizer.step(time=time, variables=variables, fn_loss=fn_loss, **kwargs)",
            "+            diffs = self.optimizer.step(time=time, variables=variables, fn_loss=fn_loss, **kwargs)",
            "",
            "-        with tf.control_dependencies(control_inputs=(applied,)):",
            "+        with tf.control_dependencies(control_inputs=diffs):",
            "if fn_reference is None:",
            "loss_step = fn_loss()",
            "else:"
        ]
    },
    {
        "number": 1649,
        "comments": "",
        "commit_message": "add the option to use a `FORCE_CUDA` to force cuda installation on docker (#612)\n\n* add a FORCE_CUDA flag\n\nFollowing discussion [here](https://github.com/facebookresearch/maskrcnn-benchmark/issues/167), this seemed the best solution\n\n* Update Dockerfile\n\n* Update setup.py\n\n* add FORCE_CUDA as an ARG\n\n* \tmodified:   docker/Dockerfile\n\tmodified:   setup.py\n\n* small fix to readme of demo\n\n* remove test print\n\n* keep ARG_CUDA\n\n* remove env value and use the one from ARG\n\n* keep same formatting as source\n\n* change proposed by @miguelvr\n\n* Update INSTALL.md\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def get_extensions():",
            "extra_compile_args = {\"cxx\": []}",
            "define_macros = []",
            "",
            "-    if torch.cuda.is_available() and CUDA_HOME is not None:",
            "+    if (torch.cuda.is_available() and CUDA_HOME is not None) or os.getenv(\"FORCE_CUDA\", \"0\") == \"1\":",
            "extension = CUDAExtension",
            "sources += source_cuda",
            "define_macros += [(\"WITH_CUDA\", None)]"
        ]
    },
    {
        "number": 1651,
        "comments": "",
        "commit_message": "Update to PyTorch 1.11.0 (#3045)\n\n* Run black\n\n* Fix ProvenanceTensor\n\n* Replace torch.triangular_solve -> torch.linalg.solve_triangular\n\n* More fixes to torch.linalg.solve_triangular\n\n* Bump torch version\n\n* Bump Python version 3.6 -> 3.7\n\n* Fix some tests\n\n* Decrease tolerance on stable tests\n\n* Remove obsolete xfail\n\n* Resolve #3032\n\n* Fix solve_triangular in ops.gaussian, revert test weakening\n\n* Fix catching of singular matrices in hmc\n\n* xfail funsor tests\n\n* Work around pandas 1.3 bug\n\n* Allow mypy to install missing stubs\n\n* Clarify triangular_solve test\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class _OMTMVNSample(Function):",
            "loc_grad = sum_leftmost(grad_output, -1)",
            "",
            "identity = eye_like(g, dim)",
            "-        R_inv = torch.triangular_solve(identity, L.t(), transpose=False, upper=True)[0]",
            "+        R_inv = torch.linalg.solve_triangular(L.t(), identity, upper=True)",
            "",
            "z_ja = z.unsqueeze(-1)",
            "g_R_inv = torch.matmul(g, R_inv).unsqueeze(-2)"
        ]
    },
    {
        "number": 1653,
        "comments": "",
        "commit_message": "Fix sync_dist for tpus (#6950)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Result(Dict):",
            "",
            "# sync across workers when using distributed training",
            "sync_fn = sync_fn or sync_ddp_if_available",
            "+",
            "if sync_dist and isinstance(value, (torch.Tensor, numbers.Number)):",
            "is_dist_initialized = torch.distributed.is_available() and torch.distributed.is_initialized()",
            "# TODO: Find a way to make the reduction only once, so we don't need to clone.",
            "-            if is_dist_initialized and isinstance(value, torch.Tensor):",
            "+            if (is_dist_initialized or tpu_distributed) and isinstance(value, torch.Tensor):",
            "value = value.clone()",
            "else:",
            "value = torch.tensor(value, device=device, dtype=torch.float)"
        ]
    },
    {
        "number": 1658,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TestGatedCnnEncoder(AllenNlpTestCase):",
            ")",
            "",
            "token_embeddings = torch.rand(5, 10, 32)",
            "-        mask = torch.ones(5, 10)",
            "-        mask[0, 7:] = 0",
            "-        mask[1, 5:] = 0",
            "+        mask = torch.ones(5, 10).bool()",
            "+        mask[0, 7:] = False",
            "+        mask[1, 5:] = False",
            "",
            "output = cnn_encoder(token_embeddings, mask)",
            "assert len(output) == 3"
        ]
    },
    {
        "number": 1660,
        "comments": "",
        "commit_message": "fix big bug with exclamation mark being used as padding for gpt2 (#50)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_pred_input(params, enc = None):",
            "bos = tf.constant(1, shape=[1, 1], dtype=tf.int64)",
            "src_seq = tf.random.uniform(shape=[1, length], minval=4, maxval=(params['n_vocab'] - 1), dtype=tf.int64)",
            "seq = tf.concat([bos, src_seq], axis=1)",
            "-    seq = tf.pad(seq, [[0, 0], [0, remaining]])",
            "+    seq = tf.pad(seq, [[0, 0], [0, remaining]], constant_values=params['padding_id'])",
            "dataset = tf.data.Dataset.from_tensors(seq)",
            "",
            "dataset = dataset.map(_dummy_labels)"
        ]
    },
    {
        "number": 1665,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TestMaxout(AllenNlpTestCase):",
            "})",
            "maxout = Maxout.from_params(params)",
            "",
            "-        constant_init = lambda tensor: torch.nn.init.constant(tensor, 1.)",
            "+        constant_init = lambda tensor: torch.nn.init.constant_(tensor, 1.)",
            "initializer = InitializerApplicator([(\".*\", constant_init)])",
            "initializer(maxout)",
            "",
            "-        input_tensor = Variable(torch.FloatTensor([[-3, 1]]))",
            "+        input_tensor = torch.FloatTensor([[-3, 1]])",
            "output = maxout(input_tensor).data.numpy()",
            "assert output.shape == (1, 3)",
            "# This output was checked by hand"
        ]
    },
    {
        "number": 1668,
        "comments": "",
        "commit_message": "Fix numpy to torch cls streaming bug (#9112)\n\n* Fix numpy to torch cls streaming bug\n\nResolves https://github.com/ultralytics/yolov5/issues/9111\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def run(",
            "seen, windows, dt = 0, [], (Profile(), Profile(), Profile())",
            "for path, im, im0s, vid_cap, s in dataset:",
            "with dt[0]:",
            "-            im = im.to(device)",
            "+            im = torch.Tensor(im).to(device)",
            "im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32",
            "if len(im.shape) == 3:",
            "im = im[None]  # expand for batch dim"
        ]
    },
    {
        "number": 1672,
        "comments": "",
        "commit_message": "[Past CI] \ud83d\udd25 Leave Past CI failures in the past \ud83d\udd25  (#20861)\n\n* torch.jit._state\n\n* Fix past CI\n\n* Fix for perceiver\n\n* Fix REALM\n\n* Fix for Bloom\n\n* Fix for SwinMode\n\n* Fix for TrajectoryTransformerModel\n\n* Fix for test_wav2vec2_with_lm\n\n* make style\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ModelTesterMixin:",
            "",
            "torch._C._jit_clear_class_registry()",
            "torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()",
            "-        torch.jit._state._clear_class_state()",
            "+        # torch 1.8 has no `_clear_class_state` in `torch.jit._state`",
            "+        if hasattr(torch.jit._state, \"_clear_class_state\"):",
            "+            torch.jit._state._clear_class_state()",
            "",
            "def _create_and_check_torchscript(self, config, inputs_dict):",
            "if not self.test_torchscript:"
        ]
    },
    {
        "number": 1674,
        "comments": "",
        "commit_message": "fix deprecation warnings; remove pydoop dependency; update README\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def map_fun(args, ctx):",
            "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")",
            "",
            "saver = tf.train.Saver()",
            "-      summary_op = tf.merge_all_summaries()",
            "-      init_op = tf.initialize_all_variables()",
            "+      summary_op = tf.summary.merge_all()",
            "+      init_op = tf.global_variables_initializer()",
            "",
            "# Create a \"supervisor\", which oversees the training process and stores model state into HDFS",
            "-    logdir = args.model if hdfs.path.isabs(args.model) else \"hdfs://default/user/{0}/{1}\".format(getpass.getuser(), args.model)",
            "+    logdir = args.model if args.model.startswith(\"hdfs://\") else \"hdfs://default/user/{0}/{1}\".format(getpass.getuser(), args.model)",
            "print(\"tensorflow model path: {0}\".format(logdir))",
            "-    summary_writer = tf.train.SummaryWriter(\"tensorboard_%d\" %(worker_num), graph=tf.get_default_graph())",
            "+    summary_writer = tf.summary.FileWriter(\"tensorboard_%d\" %(worker_num), graph=tf.get_default_graph())",
            "",
            "if args.mode == \"train\":",
            "sv = tf.train.Supervisor(is_chief=(task_index == 0),"
        ]
    },
    {
        "number": 1676,
        "comments": "",
        "commit_message": "Allow saving / loading checkpoints from cloud paths (#683)\n\n* Allow saving / loading checkpoints from cloud paths\n\nAllows saving and loading checkpoints directly from cloud paths like\nAmazon S3 (s3://) and Google Cloud Storage (gs://) by using fsspec.\n\nNote: The user will have to install the relevant dependency for each\nprotocol. Otherwise fsspec will fail and specify which dependency is\nmissing.\n\n* Append suffix _fsspec to save/load function names\n\n* Add a lower bound to the fsspec dependency\n\nSkips the 0 major version.\n\n* Add missing changes from refactor\n\n* Use fsspec for remaining artifacts\n\n* Add test case with path requiring fsspec\n\n* Avoid writing logs to file unless output_path is local\n\n* Document the possibility of using paths supported by fsspec\n\n* Fix style and lint\n\n* Add missing lint fixes\n\n* Add type annotations to new functions\n\n* Use Coqpit method for converting config to dict\n\n* Fix type annotation in semi-new function\n\n* Add return type for load_fsspec\n\n* Fix bug where fs not always created\n\n* Restore the experiment removal functionality\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def main(args):  # pylint: disable=redefined-outer-name",
            "model = setup_model(c)",
            "",
            "# restore model",
            "-    checkpoint = torch.load(args.checkpoint_path, map_location=\"cpu\")",
            "+    checkpoint = load_fsspec(args.checkpoint_path, map_location=\"cpu\")",
            "model.load_state_dict(checkpoint[\"model\"])",
            "",
            "if use_cuda:"
        ]
    },
    {
        "number": 1678,
        "comments": "",
        "commit_message": "fix `bias` keyword argument in TFDebertaEmbeddings (#17940)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFDebertaEmbeddings(tf.keras.layers.Layer):",
            "self.position_biased_input = getattr(config, \"position_biased_input\", True)",
            "self.initializer_range = config.initializer_range",
            "if self.embedding_size != config.hidden_size:",
            "-            self.embed_proj = tf.keras.layers.Dense(config.hidden_size, bias=False)",
            "+            self.embed_proj = tf.keras.layers.Dense(config.hidden_size, use_bias=False)",
            "self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")",
            "self.dropout = TFDebertaStableDropout(config.hidden_dropout_prob, name=\"dropout\")"
        ]
    },
    {
        "number": 1680,
        "comments": "",
        "commit_message": "[RLlib] SAC Torch (incl. Atari learning) (#7984)\n\n* Policy-classes cleanup and torch/tf unification.\n- Make Policy abstract.\n- Add `action_dist` to call to `extra_action_out_fn` (necessary for PPO torch).\n- Move some methods and vars to base Policy\n  (from TFPolicy): num_state_tensors, ACTION_PROB, ACTION_LOGP and some more.\n\n* Fix `clip_action` import from Policy (should probably be moved into utils altogether).\n\n* - Move `is_recurrent()` and `num_state_tensors()` into TFPolicy (from DynamicTFPolicy).\n- Add config to all Policy c'tor calls (as 3rd arg after obs and action spaces).\n\n* Add `config` to c'tor call to TFPolicy.\n\n* Add missing `config` to c'tor call to TFPolicy in marvil_policy.py.\n\n* Fix test_rollout_worker.py::MockPolicy and BadPolicy classes (Policy base class is now abstract).\n\n* Fix LINT errors in Policy classes.\n\n* Implement StatefulPolicy abstract methods in test cases: test_multi_agent_env.py.\n\n* policy.py LINT errors.\n\n* Create a simple TestPolicy to sub-class from when testing Policies (reduces code in some test cases).\n\n* policy.py\n- Remove abstractmethod from `apply_gradients` and `compute_gradients` (these are not required iff `learn_on_batch` implemented).\n- Fix docstring of `num_state_tensors`.\n\n* Make QMIX torch Policy a child of TorchPolicy (instead of Policy).\n\n* QMixPolicy add empty implementations of abstract Policy methods.\n\n* Store Policy's config in self.config in base Policy c'tor.\n\n* - Make only compute_actions in base Policy's an abstractmethod and provide pass\nimplementation to all other methods if not defined.\n- Fix state_batches=None (most Policies don't have internal states).\n\n* Cartpole tf learning.\n\n* Cartpole tf AND torch learning (in ~ same ts).\n\n* Cartpole tf AND torch learning (in ~ same ts). 2\n\n* Cartpole tf (torch syntax-broken) learning (in ~ same ts). 3\n\n* Cartpole tf AND torch learning (in ~ same ts). 4\n\n* Cartpole tf AND torch learning (in ~ same ts). 5\n\n* Cartpole tf AND torch learning (in ~ same ts). 6\n\n* Cartpole tf AND torch learning (in ~ same ts). Pendulum tf learning.\n\n* WIP.\n\n* WIP.\n\n* SAC torch learning Pendulum.\n\n* WIP.\n\n* SAC torch and tf learning Pendulum and Cartpole after cleanup.\n\n* WIP.\n\n* LINT.\n\n* LINT.\n\n* SAC: Move policy.target_model to policy.device as well.\n\n* Fixes and cleanup.\n\n* Fix data-format of tf keras Conv2d layers (broken for some tf-versions which have data_format=\"channels_first\" as default).\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fix and LINT.\n\n* WIP.\n\n* Test fixes and LINT.\n\n* Fixes and LINT.\n\nCo-authored-by: Sven Mika <sven@Svens-MacBook-Pro.local>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ParameterNoise(Exploration):",
            "else:",
            "for i in range(len(self.noise)):",
            "self.noise[i] = torch.normal(",
            "-                    0.0, self.stddev, size=self.noise[i].size())",
            "+                    mean=torch.zeros(self.noise[i].size()), std=self.stddev)",
            "",
            "def _tf_sample_new_noise_op(self):",
            "added_noises = []"
        ]
    },
    {
        "number": 1684,
        "comments": "",
        "commit_message": "amalgam cuda hotfix\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def draw_first_k_couples(k: int, rdims: int, dv: torch.device):",
            "repeats = torch.cat(",
            "[",
            "torch.arange(max_exhaustive_search, dtype=torch.long, device=dv) + 1,",
            "-            torch.tensor([residual_search], dtype=torch.long),",
            "+            torch.tensor([residual_search], dtype=torch.long, device=dv),",
            "]",
            ")",
            "idx_sequence = torch.stack([repeats.repeat_interleave(repeats), arange_sequence(repeats)], dim=-1)"
        ]
    },
    {
        "number": 1685,
        "comments": "",
        "commit_message": "Enable `fixed_arch` on Retiarii (#3972)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DartsLayerChoice(nn.Module):",
            "yield name, p",
            "",
            "def export(self):",
            "-        return torch.argmax(self.alpha).item()",
            "+        return list(self.op_choices.keys())[torch.argmax(self.alpha).item()]",
            "",
            "",
            "class DartsInputChoice(nn.Module):",
            "def __init__(self, input_choice):",
            "super(DartsInputChoice, self).__init__()",
            "-        self.name = input_choice.key",
            "+        self.name = input_choice.label",
            "self.alpha = nn.Parameter(torch.randn(input_choice.n_candidates) * 1e-3)",
            "self.n_chosen = input_choice.n_chosen or 1"
        ]
    },
    {
        "number": 1691,
        "comments": "",
        "commit_message": "Use odd shape tensor to represent parameter data in partitioned state (#981)\n\n* use wierd shaped tensor to avoid silent failures when not registering externel params\n\n* fix typo\n\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_scatter_gather():",
            "with deepspeed.zero.Init():",
            "l = torch.nn.Linear(6, 3)",
            "assert l.weight.ds_status == ZeroParamStatus.NOT_AVAILABLE",
            "-    assert l.weight.numel() == 1",
            "+    assert l.weight.shape == torch.Size(partitioned_param_data_shape)",
            "",
            "# Ensure there is no impact outside the context",
            "l2 = torch.nn.Linear(6, 3)"
        ]
    },
    {
        "number": 1703,
        "comments": "",
        "commit_message": "Fix codacy issues (continue) (#349)\n\n* fixed rein.py | try Exception\n\n* fixed deformable conv2d bug (#346)\n\n* improve docs\n\n* fixed bug of deformable conv2d\n\n* 1) try exception with Exception / 2) use enumerate instead of range(l\u2026 (#347)\n\n* 1) try exception with Exception / 2) use enumerate instead of range(len())\n\n* remove unused import in tfrecord3.py example\n\n* try Exception in tfrecord3.py\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DeformableConv2d(Layer):",
            "offset_params = [osparam for osparam in offset_layer.all_params if osparam not in layer.all_params]",
            "offset_layers = [oslayer for oslayer in offset_layer.all_layers if oslayer not in layer.all_layers]",
            "",
            "-        self.all_params.extend(offset_params)",
            "-        self.all_layers.extend(offset_layers)",
            "-        self.all_drop.update(offset_layer.all_drop)",
            "+        self.all_params.extend(list(offset_params))",
            "+        self.all_layers.extend(list(offset_layers))",
            "+        self.all_drop.update(dict(offset_layer.all_drop))",
            "",
            "# this layer",
            "self.all_layers.extend([self.outputs])"
        ]
    },
    {
        "number": 1719,
        "comments": "",
        "commit_message": "update audio_utils and fix mTEDx example\n\nSummary:\nupdate audio_utils and fix mTEDx example\n- Updated `audio_utils`\n  - Added support for OGG Vorbis (the only supported lossy compressed format)\n  - Added a separate `convert_to_mono()` helper function\n  - Updated `get_waveform()`\n    - added new arguments `frames` and `start` for reading part of audios\n    - added new argument `mono` for auto conversion to mono-channel audio\n    - unified returned waveform shape to channels x length (same as torchaudio default)\n- Updated mTEDx and MUST-C data prep scripts\n  - Replaced `torchaudio.info()` with `soundfile.info()` (the latter is faster and the former has incompatible interface between <0.8 and the latest 0.8)\n  - Replaced `torchaudio.load()` with `get_waveform` for auto conversion to mono channel\n\nReviewed By: jmp84\n\nDifferential Revision: D26901114\n\nfbshipit-source-id: fa9560c9714d51a91157d5141564574d4eee454d\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def extract_fbank_features(",
            "if output_path is not None and output_path.is_file() and not overwrite:",
            "return",
            "",
            "-    _waveform = waveform * (2 ** 15)  # Kaldi compliance: 16-bit signed integers",
            "-    _waveform = _waveform.squeeze().numpy()",
            "+    _waveform = _convert_to_mono(waveform, sample_rate)",
            "+    _waveform = _waveform * (2 ** 15)  # Kaldi compliance: 16-bit signed integers",
            "+    _waveform = _waveform.numpy()",
            "",
            "features = _get_kaldi_fbank(_waveform, sample_rate, n_mel_bins)",
            "if features is None:"
        ]
    },
    {
        "number": 1721,
        "comments": "",
        "commit_message": "Fix TF2 / rllib test (#5846)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MADDPGTFPolicy(MADDPGPostprocessing, TFPolicy):",
            "out = tf.layers.dense(out, units=hidden, activation=activation)",
            "feature = tf.layers.dense(",
            "out, units=act_space.shape[0], activation=None)",
            "-            sampler = RelaxedOneHotCategorical(",
            "+            sampler = tfp.distributions.RelaxedOneHotCategorical(",
            "temperature=1.0, logits=feature).sample()",
            "",
            "return sampler, feature, model, tf.global_variables(scope.name)"
        ]
    },
    {
        "number": 1726,
        "comments": "",
        "commit_message": "Fix Theano tests.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def update_confusion_matrix_variables(variables_to_update,",
            "# Tile the thresholds for every prediction.",
            "thresh_tiled = K.tile(",
            "K.expand_dims(K.constant(thresholds), 1),",
            "-        K.stack([1, num_predictions]))",
            "+        K.cast(",
            "+            K.stack([1, num_predictions]),",
            "+            dtype='int32',",
            "+        )",
            "+    )",
            "",
            "# Tile the predictions for every threshold.",
            "preds_tiled = K.tile(predictions_2d, [num_thresholds, 1])"
        ]
    },
    {
        "number": 1734,
        "comments": "",
        "commit_message": "Restore TF embeddings and attention layers to their previous version (#9890)\n\n* Refacto BERT\n\n* Restore all the concerned models\n\n* Remove print\n\n* Update template\n\n* Apply Sylvain's and Morgan's comments\n\n* Fix cast\n\n* Put the cast inside call\n\n* Remove cond in ebds\n\n* Fix funnel\n\n* Restore previous dot product (attention_scores) computation\n\n* Add ConvBERT and BART\n\n* Make all the S2S models ONNX compliant\n\n* Fix test\n\n* Fix check copies\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def _expand_mask(mask: tf.Tensor, tgt_len: Optional[int] = None, past_key_values",
            "\"\"\"",
            "Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.",
            "\"\"\"",
            "-    bsz, src_len = shape_list(mask)",
            "+    src_len = shape_list(mask)[1]",
            "tgt_len = tgt_len if tgt_len is not None else src_len",
            "-",
            "-    expanded_mask = tf.cast(tf.broadcast_to(mask[:, None, None, :], (bsz, 1, tgt_len, src_len)), tf.float32)",
            "+    expanded_mask = tf.cast(tf.tile(mask[:, None, None, :], (1, 1, tgt_len, 1)), tf.float32)",
            "",
            "return (1.0 - expanded_mask) * LARGE_NEGATIVE"
        ]
    },
    {
        "number": 1736,
        "comments": "",
        "commit_message": "Remove unused `type: ignore` (#1998)\n\n* Remove unused `type: ignore`\n\n- Solve some previous type annotations errors\n- Add `warn_unused_ignores = True` to mypy config\n- Fix all unused `type: ignore` pointed out by mypy\n\n* fix typing for old torch\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def load_image_to_tensor(path_file: str, device: str) -> Tensor:",
            "# for convenience use the torch dlpack parser to get a zero copy torch.Tensor",
            "# TODO: evaluate other potential API so that we can return in numpy, jax, mxnet since",
            "# the kornia_rs cv::Tensor has this ability.",
            "-    th_tensor = torch.utils.dlpack.from_dlpack(cv_tensor)  # type: ignore # HxWx3",
            "+    th_tensor = dlpack.from_dlpack(cv_tensor)  # HxWx3",
            "# move the tensor to the desired device, move the data layout to CHW and clone",
            "# to return an owned data tensor.",
            "return th_tensor.to(torch.device(device)).permute(2, 0, 1).clone()  # CxHxW"
        ]
    },
    {
        "number": 1737,
        "comments": "",
        "commit_message": "Fix gradient clipping typo (#3039)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Trainer(BaseTrainer):",
            "if self.gradient_clipping_config.clipglobalnorm:",
            "torch.nn.utils.clip_grad_norm_(variables, self.gradient_clipping_config.clipglobalnorm)",
            "if self.gradient_clipping_config.clipnorm:",
            "-            torch.nn.utils.clip_grad_norm_(variables, self.gradient_clipping_config.clipglobalnorm)",
            "+            torch.nn.utils.clip_grad_norm_(variables, self.gradient_clipping_config.clipnorm)",
            "if self.gradient_clipping_config.clipvalue:",
            "torch.nn.utils.clip_grad_value_(variables, self.gradient_clipping_config.clipvalue)"
        ]
    },
    {
        "number": 1740,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BooleanAccuracyTest(AllenNlpTestCase):",
            "accuracy = BooleanAccuracy()",
            "predictions = torch.rand([5, 7], device=device)",
            "labels = torch.rand([5, 7], device=device)",
            "-        incorrect_shape_mask = torch.randint(0, 2, [5, 8], device=device)",
            "+        incorrect_shape_mask = torch.randint(0, 2, [5, 8], device=device).bool()",
            "with pytest.raises(ValueError):",
            "accuracy(predictions, labels, incorrect_shape_mask)"
        ]
    },
    {
        "number": 1744,
        "comments": "",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def get_lst_from_rank0(lst: List[int]) -> None:",
            "lst_tensor = torch.tensor(",
            "lst if dist.get_rank() == 0 else [-1] * len(lst),",
            "dtype=int,",
            "-        # device=torch.cuda.current_device(),",
            "-        device=torch.device('cuda:{}'.format(os.environ[\"LOCAL_RANK\"])),",
            "+        # device=get_accelerator().current_device_name(),",
            "+        device=torch.device(get_accelerator().device_name(os.environ[\"LOCAL_RANK\"])),",
            "requires_grad=False,",
            ")",
            "dist.broadcast(lst_tensor, src=0, async_op=False)"
        ]
    },
    {
        "number": 1747,
        "comments": "",
        "commit_message": "refactor(internal): move model store to common store implementation (#2029)\n\n* delete models/store.py\n\n* feat(internal): add Model class\n\n* feat(internal): expose new model APIs\n\n* fix(internal): allow stores to be initialized with an FS\n\n* update internal modules to use new models\n\n* refactor(internal): refactor frameworks to use new modelstore\n\n* test: fix integration tests\n\n* fix(internal): fix SysPath lint\n\n* fix(internal): remove use of cached property\n\n* feat(internal): add bentoml.models.create\n\n* fix(internal): use shutil.move over os.rename\n\n* chore: patch\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nci: don't use docker (#2045)\n\n* ci: don't use docker for checks\n* ci: remove docker from Makefile\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nchore: drop python 3.6 support (#2046)\n\nchore: enable pyright for testing (#2047)\n\nfix: revert&fix typing of di containers (#2044)\n\n* fix: revert&fix typing of di containers\n\n* addpend\n\n* fix: dependencies issues\n\nfix: type\n\nfix: tests\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: CI for onnxmlir installation\n\nfix: onnxmlir tests and move to attrs\n\nfix(onnxmlir): activate env\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: uses attr.define instead\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: remove poetry\n\nfix: simple_di bug doesn't seem to work with attr.define\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: flake8\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: None check behaviour\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* revert\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nfix: schema typing (#2048)\n\nfix(internal): BentoInfo: don't try to parse creation time again (#2049)\n\n* fix: MLflow models\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: onnxmlir\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix(wip): paddlepaddle\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: paddle\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: spacy\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: tensorflow\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: transformers\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* delete models/store.py\n\n* feat(internal): add Model class\n\n* feat(internal): expose new model APIs\n\n* fix(internal): allow stores to be initialized with an FS\n\n* update internal modules to use new models\n\n* refactor(internal): refactor frameworks to use new modelstore\n\n* test: fix integration tests\n\n* fix(internal): fix SysPath lint\n\n* fix(internal): remove use of cached property\n\n* feat(internal): add bentoml.models.create\n\n* fix(internal): use shutil.move over os.rename\n\n* chore: patch\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nci: don't use docker (#2045)\n\n* ci: don't use docker for checks\n* ci: remove docker from Makefile\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nchore: drop python 3.6 support (#2046)\n\nchore: enable pyright for testing (#2047)\n\nfix: revert&fix typing of di containers (#2044)\n\n* fix: revert&fix typing of di containers\n\n* addpend\n\n* fix: dependencies issues\n\nfix: type\n\nfix: tests\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: CI for onnxmlir installation\n\nfix: onnxmlir tests and move to attrs\n\nfix(onnxmlir): activate env\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: uses attr.define instead\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: remove poetry\n\nfix: simple_di bug doesn't seem to work with attr.define\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: flake8\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: None check behaviour\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* revert\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nfix: schema typing (#2048)\n\nfix(internal): BentoInfo: don't try to parse creation time again (#2049)\n\n* fix: MLflow models\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: onnxmlir\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix(wip): paddlepaddle\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: paddle\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: spacy\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: tensorflow\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: transformers\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: fix lint errors\n\n* Update test_onnx_impl.py\n\n* chore: ignore typing\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: CI\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def wrong_module(modelstore, sklearn_onnx_model):",
            ")",
            "def test_onnx_save_load(metadata, save_proc, modelstore, sklearn_onnx_model):",
            "model, data = sklearn_onnx_model",
            "-    info = save_proc(metadata)",
            "-    assert info.metadata is not None",
            "-    assert_have_file_extension(info.path, \".onnx\")",
            "+    model = save_proc(metadata)",
            "+    assert model.info.metadata is not None",
            "+    assert_have_file_extension(model.path, \".onnx\")",
            "",
            "opts = ort.SessionOptions()",
            "opts.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL",
            "opts.log_verbosity_level = 1",
            "-    loaded = bentoml.onnx.load(info.tag, model_store=modelstore, session_options=opts)",
            "+    loaded = bentoml.onnx.load(model.tag, model_store=modelstore, session_options=opts)",
            "assert predict_arr(loaded, data)[0] == 0"
        ]
    },
    {
        "number": 1765,
        "comments": "",
        "commit_message": "Add HF_ prefix to env var MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES (#2409)\n\n* add HF_ prefix to env var MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\n\n* update tests\n\n* style\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_is_small_dataset(",
            "dataset_size, config_max_in_memory_dataset_size, env_max_in_memory_dataset_size, monkeypatch",
            "):",
            "if config_max_in_memory_dataset_size != \"default\":",
            "-        monkeypatch.setattr(datasets.config, \"MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", config_max_in_memory_dataset_size)",
            "+        monkeypatch.setattr(",
            "+            datasets.config, \"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", config_max_in_memory_dataset_size",
            "+        )",
            "",
            "-    max_in_memory_dataset_size = datasets.config.MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES",
            "+    max_in_memory_dataset_size = datasets.config.HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES",
            "if config_max_in_memory_dataset_size == \"default\":",
            "if env_max_in_memory_dataset_size:",
            "assert max_in_memory_dataset_size == env_max_in_memory_dataset_size"
        ]
    },
    {
        "number": 1767,
        "comments": "",
        "commit_message": " Upgrade to modern Python syntax (#1213)\n\n* Upgrade to modern Python syntax\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Use f-strings\n\n* Placate DeepSource Python\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def laf_from_center_scale_ori(xy: torch.Tensor, scale: torch.Tensor, ori: torch.",
            "names = ['xy', 'scale', 'ori']",
            "for var_name, var, req_shape in zip(names, [xy, scale, ori], [(\"B\", \"N\", 2), (\"B\", \"N\", 1, 1), (\"B\", \"N\", 1)]):",
            "if not isinstance(var, torch.Tensor):",
            "-            raise TypeError(\"{} type is not a torch.Tensor. Got {}\".format(var_name, type(var)))",
            "+            raise TypeError(f\"{var_name} type is not a torch.Tensor. Got {type(var)}\")",
            "if len(var.shape) != len(req_shape):  # type: ignore  # because it does not like len(tensor.shape)",
            "raise TypeError(\"{} shape should be must be [{}]. \" \"Got {}\".format(var_name, str(req_shape), var.size()))",
            "for i, dim in enumerate(req_shape):  # type: ignore # because it wants typing for dim"
        ]
    },
    {
        "number": 1774,
        "comments": "",
        "commit_message": "Fixed distributed/execution bugs wrt global_model.\nAdded more documentation to model methods that did not have docstring yet.\nRenamed `ctx` into `context`.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MemoryModel(Model):",
            "tensors=batch",
            ")",
            "",
            "-            optimization = tf.cond(",
            "+            return tf.cond(",
            "pred=optimize,",
            "true_fn=(lambda: self.fn_optimization(**batch)),",
            "false_fn=tf.no_op",
            ")",
            "",
            "-        return optimization",
            "-",
            "def tf_import_experience(self, states, internals, actions, terminal, reward):",
            "\"\"\"",
            "Imports experiences into the TensorFlow memory structure. Can be used to import"
        ]
    },
    {
        "number": 1779,
        "comments": "",
        "commit_message": "fix DepthWarper to accept batches\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DepthWarper(nn.Module):",
            "factor_y = (self.height - 1) / 2",
            "factor_y = factor_y.to(device)",
            "",
            "-        z = 1. / flow[:, 2]  # Nx(H*W)",
            "+        z = 1. / (flow[:, 2] + self.eps)  # Nx(H*W)",
            "x = (flow[:, 0] * z - factor_x) / factor_x",
            "y = (flow[:, 1] * z - factor_y) / factor_y"
        ]
    },
    {
        "number": 1787,
        "comments": "",
        "commit_message": "Adalam fix2 (#1888)\n\n* fix adalam output when No seed point survived.\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class AdalamFilter:",
            ")",
            "k1, k2, d1, d2, o1, o2, s1, s2 = self.__to_torch(k1, k2, d1, d2, o1, o2, s1, s2)",
            "if len(d2) <= 1:",
            "-            return _no_match(d1)",
            "+            idxs, dists = _no_match(d1)",
            "+            if return_dist:",
            "+                return idxs, dists",
            "+            return idxs",
            "distmat = dist_matrix(d1, d2, is_normalized=False)",
            "dd12, nn12 = torch.topk(distmat, k=2, dim=1, largest=False)  # (n1, 2)"
        ]
    },
    {
        "number": 1796,
        "comments": "",
        "commit_message": "fix regression layer to_one_hot (#240)\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def regression(incoming, placeholder=None, optimizer='adam',",
            "if placeholder is None:",
            "pscope = \"TargetsData\" if not name else name",
            "with tf.name_scope(pscope):",
            "-            placeholder = tf.placeholder(shape=input_shape, dtype=dtype, name=\"Y\")",
            "+            p_shape = [None] if to_one_hot else input_shape",
            "+            placeholder = tf.placeholder(shape=p_shape, dtype=dtype, name=\"Y\")",
            "",
            "tf.add_to_collection(tf.GraphKeys.TARGETS, placeholder)"
        ]
    },
    {
        "number": 1803,
        "comments": "",
        "commit_message": "[rllib] Remove dependency on TensorFlow (#4764)\n\n* remove hard tf dep\n\n* add test\n\n* comment fix\n\n* fix test\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def conv2d(x,",
            "filter_size=(3, 3),",
            "stride=(1, 1),",
            "pad=\"SAME\",",
            "-           dtype=tf.float32,",
            "+           dtype=None,",
            "collections=None):",
            "+    if dtype is None:",
            "+        dtype = tf.float32",
            "+",
            "with tf.variable_scope(name):",
            "stride_shape = [1, stride[0], stride[1], 1]",
            "filter_shape = ["
        ]
    },
    {
        "number": 1810,
        "comments": "",
        "commit_message": "fix #721\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def Conv2DTranspose(",
            "bias_regularizer=bias_regularizer,",
            "activity_regularizer=activity_regularizer)",
            "ret = layer.apply(inputs, scope=tf.get_variable_scope())",
            "+        ret = tf.identity(ret, name='output')",
            "",
            "ret.variables = VariableHolder(W=layer.kernel)",
            "if use_bias:",
            "ret.variables.b = layer.bias",
            "-    return tf.identity(ret, name='output')",
            "+    return ret",
            "",
            "",
            "Deconv2D = Conv2DTranspose"
        ]
    },
    {
        "number": 1825,
        "comments": "",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_gaussian_tensordot(dot_dims,",
            "nb = dot_dims",
            "nc = y_dim - dot_dims",
            "try:",
            "-        torch.cholesky(x.precision[..., na:, na:] + y.precision[..., :nb, :nb])",
            "+        torch.linalg.cholesky(x.precision[..., na:, na:] + y.precision[..., :nb, :nb])",
            "except RuntimeError:",
            "pytest.skip(\"Cannot marginalize the common variables of two Gaussians.\")"
        ]
    },
    {
        "number": 1831,
        "comments": "",
        "commit_message": "`sorting` fix up (#1884)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def sort(",
            "*,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    sorted_tensor, _ = torch.sort(x, dim=axis, descending=descending, out=out)",
            "+    sorted_tensor, _ = torch.sort(",
            "+        x, dim=axis, descending=descending, stable=stable, out=out",
            "+    )",
            "return sorted_tensor"
        ]
    },
    {
        "number": 1837,
        "comments": "",
        "commit_message": "Fix pylint errors\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFSEquenceExampleDecoder(data_decoder.DataDecoder):",
            "",
            "# Reshape non-sparse elements just once:",
            "for k, value in all_features.items():",
            "-      if isinstance(value, parsing_ops.FixedLenFeature):",
            "-        example[k] = array_ops.reshape(example[k], value.shape)",
            "+      if isinstance(value, tf.FixedLenFeature):",
            "+        example[k] = tf.reshape(example[k], value.shape)",
            "",
            "if not items:",
            "items = self._items_to_handlers.keys()"
        ]
    },
    {
        "number": 1838,
        "comments": "",
        "commit_message": "Fix dtype issue in TF BART (#15178)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFBartForConditionalGeneration(TFBartPretrainedModel, TFCausalLanguageMode",
            "if inputs[\"labels\"] is not None:",
            "inputs[\"labels\"] = tf.where(",
            "inputs[\"labels\"] == self.config.pad_token_id,",
            "-                tf.fill(shape_list(inputs[\"labels\"]), -100),",
            "+                tf.cast(tf.fill(shape_list(inputs[\"labels\"]), -100), inputs[\"labels\"].dtype),",
            "inputs[\"labels\"],",
            ")",
            "inputs[\"use_cache\"] = False"
        ]
    },
    {
        "number": 1840,
        "comments": "",
        "commit_message": "Fixed a problem with dropout\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def inference_nn4_max_pool_96(images, phase_train=True):",
            "affn1 = _affine(resh1, 896, 128)",
            "if FLAGS.keep_probability<1.0:",
            "affn1 = control_flow_ops.cond(phase_train,",
            "-                                  lambda: tf.nn.dropout(affn1, FLAGS.keep_probability), affn1)",
            "+                                  lambda: tf.nn.dropout(affn1, FLAGS.keep_probability), lambda: affn1)",
            "norm = tf.nn.l2_normalize(affn1, 1, 1e-10)",
            "",
            "return norm"
        ]
    },
    {
        "number": 1843,
        "comments": "",
        "commit_message": "Add Homography Tracker API (#1389)\n\n* rebase commit\n\n* init\n\n* added test data for loftr and image registrator\n\n* real ransac tests\n\n* save\n\n* works\n\n* lint\n\n* \u00fcpdated example\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* docs\n\n* docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* stupid CI OOM\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* lint\n\n* fix dtype in tests\n\n* fix test dtype\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* no-grad for homotest\n\n* fix\n\n* fix\n\n* fix random seed for ransac test\n\n* some formatting\n\n* codespell\n\n* fix signature defaults\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add fixture with data\n\n* remove files\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix deepsource\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* deepsource\n\n* fix super\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* identation fix\n\n* fix positional\n\n* fix mypy\n\n* refactor some tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove doctest\n\n* fix ransac tests\n\n* fix similarity tests\n\n* fix sha path\n\n* fix loftr precision\n\n* xfail some ransac tests\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class FineMatching(nn.Module):",
            "",
            "# compute coordinates from heatmap",
            "coords_normalized = dsnt.spatial_expectation2d(heatmap[None], True)[0]  # [M, 2]",
            "-        grid_normalized = create_meshgrid(W, W, True, heatmap.device).reshape(1, -1, 2)  # [1, WW, 2]",
            "+        grid_normalized = create_meshgrid(",
            "+            W, W, normalized_coordinates=True, device=heatmap.device, dtype=heatmap.dtype",
            "+        ).reshape(1, -1, 2)  # [1, WW, 2]",
            "",
            "# compute std over <x, y>",
            "var = torch.sum(grid_normalized**2 * heatmap.view(-1, WW, 1), dim=1) - coords_normalized**2  # [M, 2]"
        ]
    },
    {
        "number": 1844,
        "comments": "",
        "commit_message": "Small fixes for compatibility with pytorch master (#442)\n\n* Small fixes for campatibility with pytorch master\n\n* Support older pytorch 0.2 torch.nn.functional.softmax\n\n* Work around bugs in torch.cat()\n\n* Replace torch_cat(-) with torch.stack(-).squeeze()\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Histogram(pyro.distributions.Distribution):",
            "vs.append(v)",
            "log_weights.append(log_weight)",
            "",
            "-        log_weights = torch.cat(log_weights)",
            "-        if not isinstance(log_weights, torch.autograd.Variable):",
            "+        log_weights = torch.stack(log_weights).squeeze()  # Work around bug in torch.cat().",
            "+        if not isinstance(log_weights, Variable):",
            "log_weights = Variable(log_weights)",
            "log_z = pyro.util.log_sum_exp(log_weights)",
            "ps = torch.exp(log_weights - log_z.expand_as(log_weights))"
        ]
    },
    {
        "number": 1846,
        "comments": "",
        "commit_message": "[TFWav2Vec2] Fix docs (#12283)\n\n* fix error\n\n* make style check happy\n\nCo-authored-by: chenhaitao <chenhaitao@qiyi.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFWav2Vec2ForCTC(TFWav2Vec2PreTrainedModel):",
            "",
            ">>> # wrap processor as target processor to encode labels",
            ">>> with processor.as_target_processor():",
            "-            >>>     labels = processor(transcription, return_tensors=\"tf\").input_values",
            "+            >>>     labels = processor(transcription, return_tensors=\"tf\").input_ids",
            "",
            ">>> loss = model(input_values, labels=labels).loss",
            "\"\"\""
        ]
    },
    {
        "number": 1851,
        "comments": "",
        "commit_message": "fix bug in action range\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PolicyNetwork(Model):",
            "mean + std * z",
            ")  # TanhNormal distribution as actions; reparameterization trick",
            "",
            "-        action = self.action_range * mean if deterministic else action",
            "+        action = self.action_range * tf.math.tanh(mean) if deterministic else action",
            "return action.numpy()[0]",
            "",
            "def sample_action(self, ):"
        ]
    },
    {
        "number": 1855,
        "comments": "",
        "commit_message": "fix einsum\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CrossAttention(nn.Module):",
            "# attention, what we cannot get enough of",
            "attn = sim.softmax(dim=-1)",
            "",
            "-        out = einsum('b i j, b j d -> b i d', attn, v)",
            "+        out = torch.einsum('b i j, b j d -> b i d', attn, v)",
            "out = rearrange(out, '(b h) n d -> b n (h d)', h=h)",
            "return self.to_out(out)"
        ]
    },
    {
        "number": 1859,
        "comments": "",
        "commit_message": "Optimizing the memory usage in `multi_head_self_attention` and `masked_softmax` (#2405)\n\n* Optimizing the memory usage in `multi_head_self_attention` and `masked_softmax`.\n\nFixes #2185\n\n* Fixes pylint error\n\n* Fixes typo and change the attention dropout back to its original version.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MultiHeadSelfAttention(Seq2SeqEncoder):",
            "keys_per_head = keys_per_head.view(batch_size * num_heads, timesteps, int(self._attention_dim/num_heads))",
            "",
            "# shape (num_heads * batch_size, timesteps, timesteps)",
            "-        scaled_similarities = torch.bmm(queries_per_head, keys_per_head.transpose(1, 2)) / self._scale",
            "+        scaled_similarities = torch.bmm(queries_per_head / self._scale, keys_per_head.transpose(1, 2))",
            "",
            "# shape (num_heads * batch_size, timesteps, timesteps)",
            "# Normalise the distributions, using the same mask for all heads.",
            "-        attention = masked_softmax(scaled_similarities, mask.repeat(1, num_heads).view(batch_size * num_heads, timesteps))",
            "+        attention = masked_softmax(scaled_similarities,",
            "+                                   mask.repeat(1, num_heads).view(batch_size * num_heads, timesteps),",
            "+                                   memory_efficient=True)",
            "attention = self._attention_dropout(attention)",
            "",
            "# Take a weighted sum of the values with respect to the attention"
        ]
    },
    {
        "number": 1860,
        "comments": "",
        "commit_message": "Remove the use of global variables. (#388)\n\n* fixed bug of time distributed layer and release 2 tests\n\n* speed up test mnist\n\n* fixed test file name\n\n* remove unused code\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "y_ = tf.placeholder(tf.int64, shape=[None], name='y_')",
            "# define the network",
            "def mlp(x, is_train=True, reuse=False):",
            "with tf.variable_scope(\"MLP\", reuse=reuse):",
            "-        tl.layers.set_name_reuse(reuse)",
            "network = tl.layers.InputLayer(x, name='input')",
            "network = tl.layers.DropoutLayer(network, keep=0.8, is_fix=True, is_train=is_train, name='drop1')",
            "network = tl.layers.DenseLayer(network, n_units=800, act=tf.nn.relu, name='relu1')"
        ]
    },
    {
        "number": 1867,
        "comments": "",
        "commit_message": "hasattr and DeviceBuffer type fixups\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def get_lazyop_shape(ast:LazyOp): return GenericShape.exec_ast(ast, GenericShape",
            "",
            "# assumes you are using ShapeTracker",
            "# used in GPUBuffer, OpenCLBuffer, and LLVMBuffer",
            "-# type: ignore",
            "-class ExplicitExecAST:",
            "+class ExplicitExecAST(DeviceBuffer):",
            "def __init__(self, shape:Union[ShapeTracker, Tuple[int, ...]], hostbuf=None):",
            "self.st = shape if isinstance(shape, ShapeTracker) else ShapeTracker(tuple(shape))",
            "self.shape = self.st.shape"
        ]
    },
    {
        "number": 1873,
        "comments": "",
        "commit_message": "Fix special values overflow/underflow for amp (#3901)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TestScalarMix(AllenNlpTestCase):",
            "for k in range(3):",
            "mean = numpy.mean(tensors[k].data.numpy()[numpy_mask == 1])",
            "std = numpy.std(tensors[k].data.numpy()[numpy_mask == 1])",
            "-            normed_tensor = (tensors[k].data.numpy() - mean) / (std + 1e-12)",
            "+            normed_tensor = (tensors[k].data.numpy() - mean) / (",
            "+                std + util.tiny_value_of_dtype(torch.float)",
            "+            )",
            "expected_result += normed_tensor * normed_weights[k]",
            "expected_result *= 0.5"
        ]
    },
    {
        "number": 1875,
        "comments": "",
        "commit_message": "[`t5`] Fix T5 inference in `float16` + `bnb` error (#21281)\n\n* attempts to fix:\n\n- upcast input for `T5DenseActDense`\n- add the condition `self.wo.weight.dtype != torch.int8`\n- added tests on `test/mixed_int8`\n- `make fixup`\n\n* fix ci test\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class T5DenseGatedActDense(nn.Module):",
            "",
            "# To make 8bit quantization work for google/flan-t5-xxl, self.wo is kept in float32.",
            "# See https://github.com/huggingface/transformers/issues/20287",
            "-        if hidden_states.dtype != self.wo.weight.dtype:",
            "+        # we also make sure the weights are not in `int8` in case users will force `_keep_in_fp32_modules` to be `None``",
            "+        if hidden_states.dtype != self.wo.weight.dtype and self.wo.weight.dtype != torch.int8:",
            "hidden_states = hidden_states.to(self.wo.weight.dtype)",
            "",
            "hidden_states = self.wo(hidden_states)"
        ]
    },
    {
        "number": 1878,
        "comments": "",
        "commit_message": "Fix comment.\n\nPiperOrigin-RevId: 185501790\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class _ConvND(base.AbstractModule):",
            "while self._mask.shape.ndims < w.shape.ndims:",
            "self._mask = tf.expand_dims(self._mask, -1)",
            "",
            "-    # ResourceVariables currently don't support *=.",
            "+    # tf.Variable & tf.ResourceVariable don't support *=.",
            "w = w * self._mask  # pylint: disable=g-no-augmented-assignment",
            "",
            "return w"
        ]
    },
    {
        "number": 1881,
        "comments": "",
        "commit_message": "Fix chief_only for input callbacks\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ImageNetModel(ModelDesc):",
            "image_dtype = tf.uint8",
            "",
            "def __init__(self, data_format='NCHW'):",
            "-        if data_format == 'NCHW':",
            "-            assert tf.test.is_gpu_available()",
            "self.data_format = data_format",
            "",
            "def _get_inputs(self):"
        ]
    },
    {
        "number": 1895,
        "comments": "",
        "commit_message": "added method closest_valid_dtype, fixed small bugs in iinfo and finfo methods, and other small bug fixes.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Finfo:",
            "return self._torch_finfo.tiny",
            "",
            "",
            "-def finfo(datatype_in):",
            "-    return Finfo(_torch.finfo(datatype_in))",
            "+def finfo(type):",
            "+    return Finfo(_torch.finfo(dtype_from_str(type)))",
            "",
            "",
            "backend = 'torch'"
        ]
    },
    {
        "number": 1896,
        "comments": "",
        "commit_message": "Fix torch meshgrid warnings (#20475)\n\n* fix torch meshgrid warnings\n\n* support lower torch versions\n\n* don't edit examples\n\n* dont edit examples\n\n* fix ci\n\n* fix style\n\n* rebase cleanup\n\n* fix ci again\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MaskFormerSwinSelfAttention(nn.Module):",
            "# get pair-wise relative position index for each token inside the window",
            "coords_h = torch.arange(self.window_size[0])",
            "coords_w = torch.arange(self.window_size[1])",
            "-        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))",
            "+        coords = torch.stack(meshgrid([coords_h, coords_w], indexing=\"ij\"))",
            "coords_flatten = torch.flatten(coords, 1)",
            "relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]",
            "relative_coords = relative_coords.permute(1, 2, 0).contiguous()"
        ]
    },
    {
        "number": 1911,
        "comments": "",
        "commit_message": "Detection() device bug fix (#1455)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Detections:",
            "self.names = names  # class names",
            "self.xyxy = pred  # xyxy pixels",
            "self.xywh = [xyxy2xywh(x) for x in pred]  # xywh pixels",
            "-        gn = [torch.Tensor([*[im.shape[i] for i in [1, 0, 1, 0]], 1., 1.]) for im in imgs]  # normalization gains",
            "+        d = pred[0].device  # device",
            "+        gn = [torch.tensor([*[im.shape[i] for i in [1, 0, 1, 0]], 1., 1.], device=d) for im in imgs]  # normalizations",
            "self.xyxyn = [x / g for x, g in zip(self.xyxy, gn)]  # xyxy normalized",
            "self.xywhn = [x / g for x, g in zip(self.xywh, gn)]  # xywh normalized",
            "self.n = len(self.pred)"
        ]
    },
    {
        "number": 1915,
        "comments": "",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PipelineModule(nn.Module):",
            "self.tied_weight_attrs = {}",
            "",
            "# Offset the random seed by the stage ID.",
            "-        #newseed = torch.cuda.initial_seed() + self._grid.get_stage_id()",
            "+        #newseed = get_accelerator().initial_seed() + self._grid.get_stage_id()",
            "#ds_utils.set_random_seed(newseed)",
            "",
            "-        #with torch.random.fork_rng(devices=[torch.cuda.current_device()]):",
            "+        #with torch.random.fork_rng(devices=[get_accelerator().current_device_name()]):",
            "self._build()",
            "-        self.to(f'cuda:{self.local_rank}')",
            "+        self.to(get_accelerator().device_name(self.local_rank))",
            "",
            "self.tied_comms = self._index_tied_modules()",
            "self._synchronize_tied_weights()"
        ]
    },
    {
        "number": 1928,
        "comments": "",
        "commit_message": "Make most metrics work on GPU (#3851)\n\n* Make most metrics work on GPU\n\n* Make metric tests work on both GPU and CPU\n\n* Add a test for the test utility\n\n* mypy\n\n* Update allennlp/common/testing/test_case.py\n\nCo-Authored-By: Mark Neumann <markn@allenai.org>\n\n* Fix a PR comment\n\n* flake8\n\nCo-authored-by: Mark Neumann <markn@allenai.org>\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BooleanAccuracy(Metric):",
            "# so we'll keep predictions that aren't.",
            "keep = mask.view(batch_size, -1).max(dim=1)[0].float()",
            "else:",
            "-            keep = torch.ones(batch_size).float()",
            "+            keep = torch.ones(batch_size, device=predictions.device).float()",
            "",
            "predictions = predictions.view(batch_size, -1)",
            "gold_labels = gold_labels.view(batch_size, -1)"
        ]
    },
    {
        "number": 1930,
        "comments": "",
        "commit_message": "Fixed GPU check to run lazily to prevent recursive subprocess creation (#1570)\n\nSigned-off-by: Travis Addair <taddair@uber.com>\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MPITests(tf.test.TestCase):",
            "\"\"\"Test on GPU using NCCL that the Adasum correctly computes 2D tensors.\"\"\"",
            "hvd.init()",
            "# TODO support non-MPI Adasum operation",
            "-        if not hvd.mpi_enabled() or not tf.test.is_gpu_available() or not hvd.nccl_built():",
            "+        if not hvd.mpi_enabled() or not hvd.gpu_available('tensorflow') or not hvd.nccl_built():",
            "return",
            "rank = hvd.rank()",
            "rank_tensors = []"
        ]
    },
    {
        "number": 1931,
        "comments": "",
        "commit_message": "made indexing method more explicit in torch shuffle method as a temporary fix to issues with torch.Tensor.__getitem__ being incorrectly reset after wrapping the torch module for compiling. This issue needs further attention.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def seed(seed_value: int = 0) -> None:",
            "",
            "def shuffle(x):",
            "batch_size = x.shape[0]",
            "-    return x[torch.randperm(batch_size)]",
            "+    return torch.index_select(x, 0, torch.randperm(batch_size))"
        ]
    },
    {
        "number": 1932,
        "comments": "",
        "commit_message": "Fix inference api & add more description on inference engine tutorial (#1711)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class HFGPTJLayerPolicy(DSPolicy):",
            "kw = self.client_module.attn.k_proj.weight",
            "vw = self.client_module.attn.v_proj.weight",
            "",
            "-        qkvw = torch.cat((qw, kw, vw), dim=0)",
            "+        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0))",
            "",
            "return self.linear_layer, \\",
            "qkvw, \\"
        ]
    },
    {
        "number": 1936,
        "comments": "",
        "commit_message": "Fixed Weight Decay Regularization in Adam\n\nSee https://arxiv.org/abs/1711.05101\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MultiprocessingTrainer(MultiprocessingEventLoop):",
            "'betas': eval(self.args.adam_betas),",
            "'weight_decay': self.args.weight_decay,",
            "}",
            "-            return torch.optim.Adam(self.model.parameters(), **self._override_optim_state)",
            "+            return Adam(self.model.parameters(), **self._override_optim_state)",
            "elif self.args.optimizer == 'nag':",
            "self._override_optim_state = {",
            "'lr': self.args.lr[0],"
        ]
    },
    {
        "number": 1943,
        "comments": "",
        "commit_message": "fix mypy errors\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ExtractTensorPatches(nn.Module):",
            "kernel[i, i] += 1.0",
            "return kernel.view(window_range, 1, window_size[0], window_size[1])",
            "",
            "-    def forward(self, input: torch.Tensor) -> torch.Tensor:",
            "+    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore",
            "if not torch.is_tensor(input):",
            "raise TypeError(\"Input input type is not a torch.Tensor. Got {}\"",
            ".format(type(input)))"
        ]
    },
    {
        "number": 1947,
        "comments": "",
        "commit_message": "fix length bonus in cuda\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class LengthBonus(ScorerInterface):",
            "torch.float32 scores for y (B)",
            "and next state for ys",
            "\"\"\"",
            "-        return torch.tensor([1.0]).expand(self.n), None",
            "+        return torch.tensor([1.0], device=y.device).expand(self.n), None"
        ]
    },
    {
        "number": 1956,
        "comments": "",
        "commit_message": "GraphGym: Update to latest PyTorch Lightning version (#5135)\n\n* refactor accelerator\n\n* update\n\n* Update torch_geometric/graphgym/train.py\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def create_model(to_device=True, dim_in=None, dim_out=None) -> GraphGymModule:",
            "",
            "model = GraphGymModule(dim_in, dim_out, cfg)",
            "if to_device:",
            "-        model.to(torch.device(cfg.device))",
            "+        model.to(torch.device(cfg.accelerator))",
            "return model"
        ]
    },
    {
        "number": 1958,
        "comments": "",
        "commit_message": "Update asr_inference_streaming.py\n\nFix dtype CI error\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Speech2TextStreaming:",
            "has_enough_samples = False if speech.size(0) <= self.win_length else True",
            "if not has_enough_samples:",
            "if is_final:",
            "-                pad = torch.zeros(self.win_length - speech.size(0))",
            "+                pad = torch.zeros(self.win_length - speech.size(0), dtype=getattr(torch, self.dtype))",
            "speech = torch.cat([speech, pad], dim=0)",
            "else:",
            "feats = None"
        ]
    },
    {
        "number": 1961,
        "comments": "",
        "commit_message": "fix sampling once and for all\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def sample_autoregressive(partial_sequences,",
            "",
            "ids_this_step = mtf.sample_with_temperature(",
            "logits, other_features[\"vocab_dim\"], temperature)",
            "+        ids_this_step = mtf.shift(ids_this_step, offset=1, dim=length_dim, wrap=False)",
            "one_new_id = ids_this_step * mtf.one_hot(position, length_dim, dtype=tf.int32)",
            "-        one_new_id = mtf.shift(one_new_id, offset=1, dim=length_dim, wrap=False)",
            "new_ids = ids + one_new_id",
            "new_position = position + 1",
            "return [new_position, new_ids]"
        ]
    },
    {
        "number": 1965,
        "comments": "",
        "commit_message": "Sharded data sources (#456)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/456\n\nMoves sharding from Data to DataSouce, so that we can have DataSources that can shard in different ways.\n\n- Move shard() utility to data/utils\n- Introduce ShardedDataSource (base class), RowShardedDataSource, and BlockShardedTSVDataSource\n\nDefault behavior is to shard the configured DataSource in a RowShardedDataSource, which results in previous behavior.  RowShardedDataSource is a light wrapper which returns shard(data_source.train)\n\nIf the configured DataSource is an instance of ShardedDataSource, then it has sharding capability and we don't wrap it.\n\nBlockShardedTSVDataSource is a TSV data source, which divides the file into blocks and returns csv readers over each block for each shard. This is useful for when row order is important in the file, e.g. when rows are sentences of a long document.  Used for MaskedLM training. Blocks are the same byte size, but not guaranteed to have the same number of rows.  Therefore the Data object needs to set epoch_size for distributed training to work correctly.\n\nIntroduce epoch_size for base Data object as an option.  If set, epochs are set to be a fixed number of batches.  Underlying dataset is cycled over infinitely.  If not set, behavior is as before.\n\nRemove epoch size from batch samplers, since it's no longer needed.\n\nConsolidate DisjointMultitaskData and CrossLingualLMData to share more code.\n\nReviewed By: kartikayk\n\nDifferential Revision: D14840136\n\nfbshipit-source-id: 1eaba18c730a39a069ee701827195954e77b22fc\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def create_module(",
            "name = type(module).__name__",
            "if getattr(module_config, \"load_path\", None):",
            "print(f\"Loading state of module {name} from {module_config.load_path} ...\")",
            "-        module.load_state_dict(torch.load(module_config.load_path))",
            "+        module.load_state_dict(torch.load(module_config.load_path, map_location=\"cpu\"))",
            "if getattr(module_config, \"freeze\", False):",
            "print(f\"Freezing the parameters of module {name} ...\")",
            "module.freeze()"
        ]
    },
    {
        "number": 1974,
        "comments": "",
        "commit_message": "Fix #139. Broken SKResNets after BlurPool addition, as a plus, SKResNets support AA now too.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class SelectiveKernelConv(nn.Module):",
            "groups = min(out_channels, groups)",
            "",
            "conv_kwargs = dict(",
            "-            stride=stride, groups=groups, drop_block=drop_block, act_layer=act_layer, norm_layer=norm_layer)",
            "+            stride=stride, groups=groups, drop_block=drop_block, act_layer=act_layer, norm_layer=norm_layer,",
            "+            aa_layer=aa_layer)",
            "self.paths = nn.ModuleList([",
            "ConvBnAct(in_channels, out_channels, kernel_size=k, dilation=d, **conv_kwargs)",
            "for k, d in zip(kernel_size, dilation)])"
        ]
    },
    {
        "number": 1979,
        "comments": "",
        "commit_message": "fixed imports in tests and gpt2 config test\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFSequenceSummary(tf.keras.layers.Layer):",
            "if training and self.first_dropout is not None:",
            "output = self.first_dropout(output)",
            "",
            "-        output = self.summary(output)",
            "+        if self.summary is not None:",
            "+            output = self.summary(output)",
            "",
            "if self.activation is not None:",
            "output = self.activation(output)"
        ]
    },
    {
        "number": 1986,
        "comments": "",
        "commit_message": "Fix tests (#14289)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BeitModelTest(ModelTesterMixin, unittest.TestCase):",
            "# this can then be incorporated into _prepare_for_class in test_modeling_common.py",
            "elif model_class.__name__ == \"BeitForSemanticSegmentation\":",
            "batch_size, num_channels, height, width = inputs_dict[\"pixel_values\"].shape",
            "-                inputs_dict[\"labels\"] = torch.zeros([self.model_tester.batch_size, height, width]).long()",
            "+                inputs_dict[\"labels\"] = torch.zeros(",
            "+                    [self.model_tester.batch_size, height, width], device=torch_device",
            "+                ).long()",
            "model = model_class(config)",
            "model.to(torch_device)",
            "model.train()"
        ]
    },
    {
        "number": 1988,
        "comments": "",
        "commit_message": "Mma refactor (#2087)\n\nSummary:\nFixing issues ([3546](https://github.com/pytorch/fairseq/issues/3546)) with latency augmented training for mma due to the change of fairseq APIs\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2087\n\nReviewed By: hygong-fb\n\nDifferential Revision: D29851286\n\nPulled By: xutaima\n\nfbshipit-source-id: 6c3077db06b89c23b312b28527d7395a725f3b3a\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TransformerEncoderLayerBase(nn.Module):",
            "# the attention weight (before softmax) for some padded element in query",
            "# will become -inf, which results in NaN in model parameters",
            "if attn_mask is not None:",
            "-            attn_mask = attn_mask.masked_fill(attn_mask.to(torch.bool), -1e8)",
            "+            attn_mask = attn_mask.masked_fill(",
            "+                attn_mask.to(torch.bool),",
            "+                -1e8 if x.dtype == torch.float32 else -1e4",
            "+            )",
            "",
            "residual = x",
            "if self.normalize_before:"
        ]
    },
    {
        "number": 1989,
        "comments": "",
        "commit_message": "Fixing Huber Loss calculation for affine portion.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class QModel(Model):",
            "",
            "# If loss clipping is used, calculate the huber loss",
            "if config.clip_loss > 0.0:",
            "-                huber_loss = tf.where(condition=(tf.abs(delta) < config.clip_loss), x=(0.5 * self.loss_per_instance), y=(tf.abs(delta) - 0.5))",
            "+                huber_loss = tf.where(condition=(tf.abs(delta) < config.clip_loss), x=(0.5 * self.loss_per_instance),",
            "+                                      y=config.clip_loss * tf.abs(delta) - 0.5 * config.clip_loss ** 2)",
            "self.q_loss = tf.reduce_mean(input_tensor=huber_loss, axis=0)",
            "else:",
            "self.q_loss = tf.reduce_mean(input_tensor=self.loss_per_instance, axis=0)"
        ]
    },
    {
        "number": 1997,
        "comments": "",
        "commit_message": "Fix GPU issue on the blazeface detector.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BlazeFace(nn.Module):",
            "for i in range(raw_box_tensor.shape[0]):",
            "boxes = detection_boxes[i, mask[i]]",
            "scores = detection_scores[i, mask[i]].unsqueeze(dim=-1)",
            "-            output_detections.append(torch.cat((boxes, scores), dim=-1))",
            "+            output_detections.append(torch.cat((boxes, scores), dim=-1).to('cpu'))",
            "",
            "return output_detections"
        ]
    },
    {
        "number": 1999,
        "comments": "",
        "commit_message": "Metrics in distributed setting (#4525)\n\n* initial commit to ensure that metrics work correctly in distributed setting\n\n* updating global_distributed_metric to take metric object\n\n* adding distributed f1 score\n\n* adding distributed attachment scores\n\n* bug fix\n\n* adding distributed boolean accuracy\n\n* adding distributed entropy\n\n* adding distributed evalb\n\n* adding distributed mean_absolute_error\n\n* adding distributed sequence accuracy\n\n* adding distributed unigram recall\n\n* making models compatible with distributed metrics\n\n* adding distributed auc\n\n* adding distributed bleu\n\n* adding missing argument\n\n* initial commit to ensure that metrics work correctly in distributed setting\n\n* updating global_distributed_metric to take metric object\n\n* adding distributed f1 score\n\n* adding distributed attachment scores\n\n* bug fix\n\n* adding distributed boolean accuracy\n\n* adding distributed entropy\n\n* adding distributed evalb\n\n* adding distributed mean_absolute_error\n\n* adding distributed sequence accuracy\n\n* adding distributed unigram recall\n\n* making models compatible with distributed metrics\n\n* adding distributed auc\n\n* adding distributed bleu\n\n* adding missing argument\n\n* changing start method\n\n* removing unnecessary argument\n\n* adding remaining metrics, removing extra argument\n\n* allowing float values\n\n* bug fix\n\n* more bug fixes\n\n* changing average to return float\n\n* adding timeout for distributed test\n\n* testing unequal batches\n\n* adding distributed auc\n\n* adding distributed spearman correlation\n\n* adding distributed covariance and pearson correlation\n\n* changing distributed test to function, misc changes\n\n* checking batch lengths explicitly to raise errors\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Perplexity(Average):",
            "\"\"\"",
            "average_loss = super().get_metric(reset)",
            "if average_loss == 0:",
            "-            return 0.0",
            "+            perplexity = 0.0",
            "",
            "# Exponentiate the loss to compute perplexity",
            "-        return float(torch.exp(average_loss))",
            "+        perplexity = float(torch.exp(average_loss))",
            "+",
            "+        return perplexity"
        ]
    },
    {
        "number": 2003,
        "comments": "",
        "commit_message": "fix issue with mixed precision and gradient clipping\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DecoderTrainer(nn.Module):",
            "index = unet_number - 1",
            "unet = self.decoder.unets[index]",
            "",
            "-        if exists(self.max_grad_norm):",
            "-            nn.utils.clip_grad_norm_(unet.parameters(), self.max_grad_norm)",
            "-",
            "optimizer = getattr(self, f'optim{index}')",
            "scaler = getattr(self, f'scaler{index}')",
            "",
            "+        if exists(self.max_grad_norm):",
            "+            scaler.unscale_(optimizer)",
            "+            nn.utils.clip_grad_norm_(unet.parameters(), self.max_grad_norm)",
            "+",
            "scaler.step(optimizer)",
            "scaler.update()",
            "optimizer.zero_grad()"
        ]
    },
    {
        "number": 2005,
        "comments": "",
        "commit_message": "fix missing \"models\" in pipeline test module (#17090)\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def get_tiny_config_from_class(configuration_class):",
            "",
            "try:",
            "model_slug = model_type.replace(\"-\", \"_\")",
            "-        module = importlib.import_module(f\".test_modeling_{model_slug}\", package=f\"tests.{model_slug}\")",
            "+        module = importlib.import_module(f\".test_modeling_{model_slug}\", package=f\"tests.models.{model_slug}\")",
            "model_tester_class = getattr(module, f\"{camel_case_model_name}ModelTester\", None)",
            "except (ImportError, AttributeError):",
            "logger.error(f\"No model tester class for {configuration_class.__name__}\")"
        ]
    },
    {
        "number": 2009,
        "comments": "",
        "commit_message": "[Testing] Fix LINT/sphinx errors. (#8874)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "from ray.rllib.utils import try_import_torch",
            "_, nn = try_import_torch()",
            "",
            "",
            "-class VisionNetwork(TorchModelV2):",
            "+class VisionNetwork(TorchModelV2, nn.Module):",
            "\"\"\"Generic vision network.\"\"\"",
            "",
            "def __init__(self, obs_space, action_space, num_outputs, model_config,",
            "name):",
            "TorchModelV2.__init__(self, obs_space, action_space, num_outputs,",
            "model_config, name)",
            "+        nn.Module.__init__(self)",
            "",
            "activation = get_activation_fn(",
            "model_config.get(\"conv_activation\"), framework=\"torch\")"
        ]
    },
    {
        "number": 2023,
        "comments": "",
        "commit_message": "Fix a dimension bug in Transform2d (#2144)\n\nThe dimension does not match when `inner_dim` is not equal to `in_channels`.\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Transformer2DModel(ModelMixin, ConfigMixin):",
            "if self.is_input_continuous:",
            "# TODO: should use out_channels for continous projections",
            "if use_linear_projection:",
            "-                self.proj_out = nn.Linear(in_channels, inner_dim)",
            "+                self.proj_out = nn.Linear(inner_dim, in_channels)",
            "else:",
            "self.proj_out = nn.Conv2d(inner_dim, in_channels, kernel_size=1, stride=1, padding=0)",
            "elif self.is_input_vectorized:"
        ]
    },
    {
        "number": 2024,
        "comments": "",
        "commit_message": "fix self.init_hidden never used\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BiLSTM_CRF(nn.Module):",
            "def _get_lstm_features(self, sentence):",
            "self.hidden = self.init_hidden()",
            "embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)",
            "-        lstm_out, self.hidden = self.lstm(embeds)",
            "+        lstm_out, self.hidden = self.lstm(embeds, self.hidden)",
            "lstm_out = lstm_out.view(len(sentence), self.hidden_dim)",
            "lstm_feats = self.hidden2tag(lstm_out)",
            "return lstm_feats"
        ]
    },
    {
        "number": 2029,
        "comments": "",
        "commit_message": "fix for pyTorch 1.2 (#549)\n\n* min pytorch 1.2\n\n* fix IterableDataset\n\n* upgrade torchvision\n\n* fix msg\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TrainerDataLoadingMixin(object):",
            "self.get_val_dataloaders()",
            "",
            "# support IterableDataset for train data",
            "-        self.is_iterable_train_dataloader = isinstance(self.get_train_dataloader().dataset, IterableDataset)",
            "+        self.is_iterable_train_dataloader = (",
            "+            EXIST_ITER_DATASET and isinstance(self.get_train_dataloader().dataset, IterableDataset))",
            "if self.is_iterable_train_dataloader and not isinstance(self.val_check_interval, int):",
            "m = '''",
            "When using an iterableDataset for train_dataloader,"
        ]
    },
    {
        "number": 2033,
        "comments": "",
        "commit_message": "\ud83d\udc1b  fix tensorflow logging\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def configure_logger(verbose: bool) -> None:",
            "verbose (bool):",
            "`True` to use verbose logger, `False` otherwise.",
            "\"\"\"",
            "-    tf_logger = tf_logging.get_logger()",
            "+    tf_logger = tf.get_logger()",
            "tf_logger.handlers = [handler]",
            "if verbose:",
            "-        environ['TF_CPP_MIN_LOG_LEVEL'] = '1'",
            "tf_logging.set_verbosity(tf_logging.INFO)",
            "logger.setLevel(logging.DEBUG)",
            "else:",
            "warnings.filterwarnings('ignore')",
            "-        environ['TF_CPP_MIN_LOG_LEVEL'] = '3'",
            "tf_logging.set_verbosity(tf_logging.ERROR)"
        ]
    },
    {
        "number": 2041,
        "comments": "",
        "commit_message": "Adding LM Head to Transfo-XL and first step to fixing problem with Adaptive Embeddings in TransfoXL (#3286)\n\n* first commit\n\n* work in progress\n\n* make language generation task pass\n\n* update to working version for LM\n\n* delete print\n\n* remove dead code\n\n* make style\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFModelTesterMixin:",
            "",
            "for model_class in self.all_model_classes:",
            "model = model_class(config)",
            "-            assert isinstance(model.get_input_embeddings(), tf.keras.layers.Layer)",
            "+            assert isinstance(model.get_input_embeddings(), (tf.keras.layers.Layer, TFAdaptiveEmbedding))",
            "x = model.get_output_embeddings()",
            "assert x is None or isinstance(x, tf.keras.layers.Layer)"
        ]
    },
    {
        "number": 2042,
        "comments": "",
        "commit_message": "fix document for add_image (#311)\n\n* organize functions, move the image processing code outside make_np.\n\n* made add_image flexible\n\n* change video API\n\n* fixes #286. fix image_with_boxes\n\n* fixed #285\n\n* closes #208 closes #210\n\n* fix lint\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "for n_iter in range(100):",
            "if n_iter % 10 == 0:",
            "x = vutils.make_grid(x, normalize=True, scale_each=True)",
            "writer.add_image('Image', x, n_iter)  # Tensor",
            "-        writer.add_image_with_boxes('imagebox', x, torch.Tensor([[10, 10, 40, 40]]), n_iter)",
            "+        writer.add_image_with_boxes('imagebox', x, torch.Tensor([[10, 10, 40, 40], [40, 40, 60, 60]]), n_iter)",
            "x = torch.zeros(sample_rate * 2)",
            "for i in range(x.size(0)):",
            "# sound amplitude should in [-1, 1]"
        ]
    },
    {
        "number": 2048,
        "comments": "",
        "commit_message": "fix shape comparison when reusing placeholders. fix #1329\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def build_or_reuse_placeholder(tensor_spec):",
            "assert \"Placeholder\" in tensor.op.type, \"Tensor {} exists but is not a placeholder!\".format(name)",
            "assert tensor_spec.is_compatible_with(tensor), \\",
            "\"Tensor {} exists but is not compatible with the signature!\".format(tensor)",
            "-        if tensor.shape == tensor_spec.shape:",
            "+        if tensor.shape.as_list() == tensor_spec.shape.as_list():",
            "# It might be desirable to use a placeholder of a different shape in some tower",
            "# (e.g., a less specific shape)",
            "+",
            "+            # Comparing `tensor.shape` directly doesn't work, because",
            "+            # tensorflow thinks `tf.Dimension(None)` and `tf.Dimension(None)` are not equal.",
            "return tensor",
            "except KeyError:",
            "pass"
        ]
    },
    {
        "number": 2049,
        "comments": "",
        "commit_message": "fixes\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class InstanceNormalization(Layer):",
            "",
            "reciprocal_stddev = tf.math.rsqrt(x=tf.maximum(x=variance, y=epsilon))",
            "",
            "-        x = (x - mean) * reciprocal_stddev",
            "+        x = (x - tf.stop_gradient(input=mean)) * tf.stop_gradient(input=reciprocal_stddev)",
            "",
            "return x"
        ]
    },
    {
        "number": 2073,
        "comments": "",
        "commit_message": "fixed and improved shape handling for exploration\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class EpsilonAnneal(Exploration):",
            "return self.initial_epsilon + completed_ratio * (self.final_epsilon - self.initial_epsilon)",
            "",
            "pred = tf.logical_or(x=(timestep < self.start_timestep), y=(timestep > self.start_timestep + self.timesteps))",
            "-        return tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn)",
            "+        return tf.constant(dims=shape, value=tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))"
        ]
    },
    {
        "number": 2083,
        "comments": "",
        "commit_message": "WIP Fix Avg&Max pool in CellStem0\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CellStem0(nn.Module):",
            "self.comb_iter_0_left = TwoSeparables(42, 42, 5, 2, 2, bias=False)",
            "self.comb_iter_0_right = TwoSeparables(96, 42, 7, 2, 3, bias=False)",
            "",
            "-        self.comb_iter_1_left = nn.AvgPool2d(3, stride=2, padding=1)",
            "+        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)",
            "self.comb_iter_1_right = TwoSeparables(96, 42, 7, 2, 3, bias=False)",
            "",
            "-        self.comb_iter_2_left = nn.MaxPool2d(3, stride=2, padding=1)",
            "+        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1)",
            "self.comb_iter_2_right = TwoSeparables(96, 42, 5, 2, 2, bias=False)",
            "",
            "self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1)"
        ]
    },
    {
        "number": 2087,
        "comments": "",
        "commit_message": "various fixes\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TransducerDecoder(AbsDecoder):",
            "dec_states = self.create_batch_states(dec_states, [d[1] for d in done])",
            "",
            "if use_lm:",
            "-            lm_labels = torch.LongTensor([h.yseq[-1] for h in hyps], device=self.device)",
            "+            lm_labels = torch.LongTensor(",
            "+                [h.yseq[-1] for h in hyps], device=self.device",
            "+            ).view(final_batch, 1)",
            "",
            "return dec_out, dec_states, lm_labels"
        ]
    },
    {
        "number": 2106,
        "comments": "",
        "commit_message": "--classes bug fix #17\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def non_max_suppression(prediction, conf_thres=0.1, iou_thres=0.6, fast=False, c",
            "",
            "# Filter by class",
            "if classes:",
            "-            x = x[(j.view(-1, 1) == torch.tensor(classes, device=j.device)).any(1)]",
            "+            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]",
            "",
            "# Apply finite constraint",
            "# if not torch.isfinite(x).all():"
        ]
    },
    {
        "number": 2112,
        "comments": "",
        "commit_message": "fix: ci (#2306)\n\nFixed: keras, onnx, paddle, spacy\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_statsmodels_save_load(metadata, holt_model):  # noqa # pylint: disable",
            "",
            "",
            "@pytest.mark.parametrize(\"exc\", [BentoMLException])",
            "-def test_get_model_info_exc(exc, holt_model):",
            "+def test_load_model_exc(exc, holt_model):",
            "tag = wrong_module(holt_model)",
            "with pytest.raises(exc):",
            "-        bentoml._internal.frameworks.statsmodels._get_model_info(tag)",
            "+        bentoml._internal.frameworks.statsmodels.load(tag)",
            "",
            "",
            "def test_statsmodels_runner_setup_run_batch(save_proc, holt_model):"
        ]
    },
    {
        "number": 2114,
        "comments": "",
        "commit_message": "Searching Fix Up (#2349)\n\nCo-authored-by: jiahanxie353<765130715@qq.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def argmin(",
            "return ret",
            "",
            "",
            "-def nonzero(",
            "-    x: Union[tf.Tensor, tf.Variable],",
            "-) -> Union[tf.Tensor, tf.Variable]:",
            "-    return tf.experimental.numpy.nonzero(x)",
            "+def nonzero(x: Union[tf.Tensor, tf.Variable]) -> Tuple[Union[tf.Tensor, tf.Variable]]:",
            "+    return tuple(tf.experimental.numpy.nonzero(x))",
            "",
            "",
            "def where("
        ]
    },
    {
        "number": 2117,
        "comments": "",
        "commit_message": "Fix yolov3 example (#577)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "if __name__ == \"__main__\":",
            "cv2.destroyAllWindows()",
            "elif url.startswith('http'):",
            "img_stream = io.BytesIO(fetch(url))",
            "-    img = cv2.imdecode(np.fromstring(img_stream.read(), np.uint8), 1)",
            "+    img = cv2.imdecode(np.frombuffer(img_stream.read(), np.uint8), 1)",
            "else:",
            "img = cv2.imread(url)"
        ]
    },
    {
        "number": 2119,
        "comments": "",
        "commit_message": "upgrade to pytorch 1.2 (#3182)\n\n* first attempt at pytorch 1.2\n\n* explicit is better than implicit\n\n* more explicit\n\n* attempt to fix flaky tests\n\n* pylint\n\n* no disable dropout\n\n* disable dropout by default\n\n* restore dropout, don't deepcopy\n\n* change batch size for biaffine_dependency_parser_multilang_test, maybe that will make it pass? :(\n\n* try batch size 10\n\n* ignore bad gradient parameter\n\n* cleanup\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BagOfWordCountsTokenEmbedder(TokenEmbedder):",
            "# also mask out positions corresponding to oov",
            "mask *= (inputs != self._oov_idx).long()",
            "for document, doc_mask in zip(inputs, mask):",
            "-            document = torch.masked_select(document, doc_mask.byte())",
            "+            document = torch.masked_select(document, doc_mask.to(dtype=torch.bool))",
            "vec = torch.bincount(document, minlength=self.vocab_size).float()",
            "vec = vec.view(1, -1)",
            "bag_of_words_vectors.append(vec)"
        ]
    },
    {
        "number": 2132,
        "comments": "",
        "commit_message": "Config-first Datasets API (ludwig.datasets refactor) (#2479)\n\n* Adds README and stub for reading dataset configs.\n\n* Adds __init__.py for configs, moves circular import into function scope in ludwig/datasets/__init__.py\n\n* Print config files in datasets folder.\n\n* First pass at automatic archive extraction.\n\n* Implemented downloading and extract.\n\n* Refactor DatasetConfig into its own file.\n\n* Fixed bugs downloading kaggle dataset.\n\n* Makes registry store dataset instances, not classes. Also comments out import_submodules for testing.\n\n* Typo fix.\n\n* Only pass data files on to load_unprocessed_dataframe, symlink directories.\n\n* Downloading dataset files into existing directory if exists.\n\n* Refactor: make datasets fully config-first, lazy load dataset loaders.\n\n* Implemented agnews custom loader.\n\n* Implements train/validation/test split by files, and globbing support\n\n* Adds _glob_multiple\n\n* Adds adult_census_income, agnews, allstate_claims_severity.\n\n* Implements sha256 verification, adds more datasets up to creditcard_fraud.\n\n* Adds checksums, dbpedia, electricity\n\n* Fixes gzip file name returned as string not list, adds up to forest_cover dataset.\n\n* Adds datasets up to reuters_r8\n\n* Adds all datasets which don't require a custom class.\n\n* Restore dataset import behavior by implementing module __getattr__\n\n* Adds KDD datasets.\n\n* Adds ieee_fraud.\n\n* Adds imbalanced_insurance, insurance_lite.\n\n* Adds mnist.\n\n* Completes implementation of all of the built-in datasets.\n\n* Made cache_dir optional, read from environment variable if set.\n\n* Upgrades datasets tests.\n\n* Adds test for new dataset config API.  Also adds scripts for dataset link checking.\n\n* Fixes loading allstate claims severity dataset.\n\n* Use @lru_cache(1), @cache not supported in python < 3.9\n\n* Deletes dataset registry, updates automl test utils\n\n* Fix imports of datasets API.\n\n* Adds more detail to sha256: docstring and basic README\n\n* Copy-paste link oops.\n\n* Fixes handling of nested archive types like .tar.bz  Also adds a LUDWIG_CACHE and export to the README\n\n* Adds link for twitter bots.\n\n* Fix order of splits in README.md\n\n* typo\n\n* Adds verify as a phase in doc string.\n\n* Support .pqt, .pq extensions for parquet.\n\n* Handle nested archives with longer file extensions like .csv.zip\n\n* Handle nested .gz types properly too.  Check all extensions with .endswith\n\n* Handle all archive types with .endswith\n\n* Update ludwig/datasets/loaders/split_loaders.py\n\nCo-authored-by: Joppe Geluykens <joppe@rvrie.com>\n\n* Adds explanation for export, fixes preserve_paths (should be relative to processed_dataset_dir)\n\n* Resolve preserved paths relative to raw dataset dir before move.\n\n* Catch runtime exception from extracting sub-archives.\n\nCo-authored-by: Daniel Treiman <daniel@predibase.com>\nCo-authored-by: Joppe Geluykens <joppe@rvrie.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def get_dataset_golden_types_path(dataset_name: str) -> str:",
            "return str(Path(__file__).resolve().parent / \"golden\" / f\"{dataset_name}.types.json\")",
            "",
            "",
            "-def get_dataset_object(dataset_name: str) -> BaseDataset:",
            "+def get_dataset_object(dataset_name: str) -> DatasetLoader:",
            "\"\"\"Returns a Ludwig dataset instance for the given dataset.\"\"\"",
            "-    return dataset_registry[dataset_name]()",
            "+    return ludwig.datasets.get_dataset(dataset_name)"
        ]
    },
    {
        "number": 2133,
        "comments": "",
        "commit_message": "Fix code according to the coding standards\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Decoder(torch.nn.Module):",
            "else:",
            "local_scores = functional.log_softmax(self.output(z_list[-1]), dim=1).data",
            "if lpz is not None:",
            "-                    local_att_best_scores, local_att_best_ids = torch.topk(local_scores, int(beam*1.5), dim=1)",
            "+                    local_att_best_scores, local_att_best_ids = torch.topk(local_scores, int(beam * CTC_SCORING_RATIO), dim=1)",
            "ctc_scores, ctc_states = ctc_prefix_score(hyp['yseq'], local_att_best_ids[0], hyp['ctc_prev'])",
            "-                    joint_scores = (1. - ctc_weight) * (local_att_best_scores[0].numpy() + hyp['score']) + ctc_weight * ctc_scores",
            "-                    joint_best_ids = np.argsort(joint_scores)[:-beam-1:-1]",
            "+                    joint_scores = (1. - ctc_weight) * \\",
            "+                        (local_att_best_scores[0].numpy() + hyp['score']) + ctc_weight * ctc_scores",
            "+                    joint_best_ids = np.argsort(joint_scores)[:-beam - 1:-1]",
            "local_best_ids = local_att_best_ids.numpy()[:, joint_best_ids]",
            "local_best_scores = local_att_best_scores.numpy()[:, joint_best_ids]",
            "else:"
        ]
    },
    {
        "number": 2136,
        "comments": "",
        "commit_message": "[RLlib] Fix all example scripts to run on GPUs. (#11105)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TorchFastModel(TorchModelV2, nn.Module):",
            "",
            "@override(ModelV2)",
            "def forward(self, input_dict, state, seq_lens):",
            "-        self._output = self.bias + \\",
            "-            torch.zeros(size=(input_dict[\"obs\"].shape[0], self.num_outputs))",
            "+        self._output = self.bias + torch.zeros(",
            "+            size=(input_dict[\"obs\"].shape[0], self.num_outputs)).to(",
            "+                self.bias.device)",
            "return self._output, []",
            "",
            "@override(ModelV2)"
        ]
    },
    {
        "number": 2139,
        "comments": "",
        "commit_message": "Fix typo error of tf.compat.v1.keras.experimental for export_saved_model and load_from_saved_model\n\nThis PR fixes the Typo error for tf.compat.v1.keras.experimental.export_saved_model and tf.compat.v1.keras.experimental.load_from_saved_model\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def load_from_saved_model(saved_model_path, custom_objects=None):",
            "",
            "# Save the tf.keras model in the SavedModel format.",
            "path = '/tmp/simple_keras_model'",
            "-    tf.keras.experimental.export_saved_model(model, path)",
            "+    tf.compat.v1.keras.experimental.export_saved_model(model, path)",
            "",
            "# Load the saved keras model back.",
            "-    new_model = tf.keras.experimental.load_from_saved_model(path)",
            "+    new_model = tf.compat.v1.keras.experimental.load_from_saved_model(path)",
            "new_model.summary()",
            "```"
        ]
    },
    {
        "number": 2142,
        "comments": "",
        "commit_message": "fixing a bug in trainer for histograms (#1498)\n\notherwise it would raise an error in line 505 (expected object of type torch.FloatTensor but found type torch.cuda.FloatTensor for argument #2 'other')\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Trainer:",
            "for name, param in self._model.named_parameters():",
            "param_updates[name].sub_(param.detach().cpu())",
            "update_norm = torch.norm(param_updates[name].view(-1, ))",
            "-                    param_norm = torch.norm(param.view(-1, ))",
            "+                    param_norm = torch.norm(param.view(-1, )).cpu()",
            "self._tensorboard.add_train_scalar(\"gradient_update/\" + name,",
            "update_norm / (param_norm + 1e-7),",
            "batch_num_total)"
        ]
    },
    {
        "number": 2147,
        "comments": "",
        "commit_message": "train.py GPU memory fix (#3590)\n\n* train.py GPU memory fix\n\n* ema\n\n* cuda\n\n* cuda\n\n* zeros input\n\n* to device\n\n* batch index 0\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def train(hyp, opt, device, tb_writer=None):",
            "if tb_writer and ni == 0:",
            "with warnings.catch_warnings():",
            "warnings.simplefilter('ignore')  # suppress jit trace warning",
            "-                            tb_writer.add_graph(torch.jit.trace(de_parallel(model), imgs, strict=False), [])  # graph",
            "+                            tb_writer.add_graph(torch.jit.trace(de_parallel(model), imgs[0:1], strict=False), [])",
            "elif plots and ni == 10 and wandb_logger.wandb:",
            "wandb_logger.log({'Mosaics': [wandb_logger.wandb.Image(str(x), caption=x.name) for x in",
            "save_dir.glob('train*.jpg') if x.exists()]})"
        ]
    },
    {
        "number": 2150,
        "comments": "",
        "commit_message": "Merge pull request #7872 from PyTorchLightning/refactor/logger-poc-changes\n\nRandom fixes for logger connector PoC\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def sync_ddp_if_available(",
            "Return:",
            "reduced value",
            "\"\"\"",
            "-    if torch.distributed.is_available() and torch.distributed.is_initialized():",
            "+    if torch.distributed.is_available() and torch.distributed.is_initialized() or tpu_distributed():",
            "return sync_ddp(result, group=group, reduce_op=reduce_op)",
            "return result"
        ]
    },
    {
        "number": 2151,
        "comments": "",
        "commit_message": "fix old usage of leakyrelu\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def LeakyReLU(x, alpha, name='output'):",
            "x (tf.Tensor): input",
            "alpha (float): the slope.",
            "\"\"\"",
            "+    log_deprecated(\"LeakyReLU\", \"Use tf.nn.leaky_relu in TF 1.4 instead!\", \"2018-03-30\")",
            "return tf.maximum(x, alpha * x, name=name)"
        ]
    },
    {
        "number": 2159,
        "comments": "",
        "commit_message": "Fix flaky ray nightly image test (#2493)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ImageFeatureMixin(BaseFeatureMixin):",
            "if isinstance(img_entry, bytes):",
            "img = read_image_from_bytes_obj(img_entry, num_channels)",
            "elif isinstance(img_entry, np.ndarray):",
            "-            img = torch.from_numpy(img_entry).permute(2, 0, 1)",
            "+            img = torch.from_numpy(np.array(img_entry, copy=True)).permute(2, 0, 1)",
            "else:",
            "img = img_entry"
        ]
    },
    {
        "number": 2162,
        "comments": "",
        "commit_message": "Data Input Refactoring\n\n- Data input pipeline can now deal properly with missing target data.\nFixes #101\n- Moved some of the featurizer functionality into the data reader and\nmade featurizer implicit part of the model.\n- Misc: Replace concat with concat_v2\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def beam_search_step(logits, beam_state, config):",
            "",
            "# Append new ids to current predictions",
            "next_predictions = tf.gather(beam_state.predictions, next_beam_ids)",
            "-  next_predictions = tf.concat(1, [",
            "+  next_predictions = tf.concat_v2([",
            "next_predictions[:, 0:time_ - 1],",
            "tf.to_int32(tf.expand_dims(next_word_ids, 1)), next_predictions[:, time_:]",
            "-  ])",
            "+  ], 1)",
            "",
            "next_beam_state = BeamState(",
            "time=time_,"
        ]
    },
    {
        "number": 2163,
        "comments": "",
        "commit_message": "Fix `args.gradient_accumulation_steps` used before assigment.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def main():",
            "",
            "model = BertForSequenceClassification(bert_config, len(label_list))",
            "if args.init_checkpoint is not None:",
            "-        model.load_state_dict(torch.load(args.init_checkpoint, map_location='cpu'))",
            "+        model.bert.load_state_dict(torch.load(args.init_checkpoint, map_location='cpu'))",
            "model.to(device)",
            "",
            "if args.local_rank != -1:"
        ]
    },
    {
        "number": 2171,
        "comments": "",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class FeaturesTest(TestCase):",
            "casted_obj = cast_to_python_objects(obj)",
            "self.assertDictEqual(casted_obj, expected_obj)",
            "",
            "-    @patch(\"nlp.features._cast_to_python_objects\", side_effect=_cast_to_python_objects)",
            "+    @patch(\"datasets.features._cast_to_python_objects\", side_effect=_cast_to_python_objects)",
            "def test_dont_iterate_over_each_element_in_a_list(self, mocked_cast):",
            "obj = {\"col_1\": [[1, 2], [3, 4], [5, 6]]}",
            "cast_to_python_objects(obj)"
        ]
    },
    {
        "number": 2172,
        "comments": "",
        "commit_message": "fixed failing test_to_dev for torch.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "torch_scatter = None",
            "def dev(x: torch.Tensor, as_native: bool = False) -> Union[ivy.Device, torch.device]:",
            "dv = x.device",
            "if as_native:",
            "-        return dv",
            "+        return torch.device(dv)",
            "return as_ivy_dev(dv)"
        ]
    },
    {
        "number": 2178,
        "comments": "",
        "commit_message": "fix TF 2.0 version of T5 - update conversion script\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFPreTrainedModel(tf.keras.Model):",
            "Returns:",
            "tf.Tensor with dummy inputs",
            "\"\"\"",
            "-        return tf.constant(DUMMY_INPUTS)",
            "+        return {'input_ids': tf.constant(DUMMY_INPUTS)}",
            "",
            "def __init__(self, config, *inputs, **kwargs):",
            "super(TFPreTrainedModel, self).__init__(*inputs, **kwargs)"
        ]
    },
    {
        "number": 2179,
        "comments": "",
        "commit_message": "Fix converting only float type tensors in Lite (#10429)\n\n* fix\n\n* less code\n\n* add test case\n\n* add test cases\n\n* update input\n\n* add test cases\n\n* add type hint\n\n* add changelog note\n\nCo-authored-by: Kaushik B <45285388+kaushikb11@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_lite_module_forward_conversion(precision, input_type, expected_type):",
            "assert precision != 16 or torch.is_autocast_enabled()",
            "return forward_input",
            "",
            "-    module = Mock(wraps=torch.nn.Linear(1, 1), side_effect=check_autocast)",
            "+    module = Mock(wraps=torch.nn.Identity(), side_effect=check_autocast)",
            "lite_module = _LiteModule(module, lite._precision_plugin).to(device)",
            "-    out = lite_module(torch.rand(1, dtype=input_type, device=device))",
            "+    out = lite_module(torch.tensor([1, 2, 3], dtype=input_type, device=device))",
            "assert module.call_args[0][0].dtype == expected_type",
            "-    assert out.dtype == torch.get_default_dtype()",
            "+    assert out.dtype == input_type or out.dtype == torch.get_default_dtype()",
            "",
            "",
            "def test_lite_dataloader_iterator():"
        ]
    },
    {
        "number": 2186,
        "comments": "",
        "commit_message": "Various fixes\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "with tf.Graph().as_default():",
            "num_filters=FLAGS.num_filters)",
            "",
            "# Define Training procedure",
            "-        global_step = tf.Variable(0, name=\"global_step\")",
            "+        global_step = tf.Variable(0, name=\"global_step\", trainable=False)",
            "optimizer = tf.train.AdamOptimizer(1e-4)",
            "grads_and_vars = optimizer.compute_gradients(cnn.loss)",
            "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)"
        ]
    },
    {
        "number": 2193,
        "comments": "",
        "commit_message": "another meshgrid fix for old pytorch\n\nSummary: Try to fix circleci again.\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D34752188\n\nfbshipit-source-id: 5966c585b61d77df1d8dd97c24383cf74dfb1fae\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def meshgrid_ij(",
            "\"\"\"",
            "Like torch.meshgrid was before PyTorch 1.10.0, i.e. with indexing set to ij",
            "\"\"\"",
            "-    if \"indexing\" in torch.meshgrid.__kwdefaults__:",
            "+    if (",
            "+        torch.meshgrid.__kwdefaults__ is not None",
            "+        and \"indexing\" in torch.meshgrid.__kwdefaults__",
            "+    ):",
            "# PyTorch >= 1.10.0",
            "return torch.meshgrid(*A, indexing=\"ij\")",
            "return torch.meshgrid(*A)"
        ]
    },
    {
        "number": 2209,
        "comments": "",
        "commit_message": "fix warnings in deberta (#19458)\n\n* fix warnings in deberta\n\n* fix copies\n\n* Revert \"fix copies\"\n\nThis reverts commit 324cb3fed11e04190ba7b4662644baa8143b60be.\n\n* fix copies\n\n* fix copies again\n\n* revert changes to whitespace that make style did since it results in an infinite chain of fix-copies\n\n* argh\n\nCo-authored-by: Sander Land <sander@chatdesk.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DisentangledSelfAttention(nn.Module):",
            "dim=-1,",
            "index=p2c_pos.squeeze(0).expand([query_layer.size(0), key_layer.size(-2), key_layer.size(-2)]),",
            ").transpose(-1, -2)",
            "-            score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)",
            "+            score += p2c_att / scale.to(dtype=p2c_att.dtype)",
            "",
            "return score"
        ]
    },
    {
        "number": 2212,
        "comments": "",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DepLabelIndexer(TokenIndexer[int]):",
            "return {index_name: [vocabulary.get_token_index(dep_label, self.namespace) for dep_label in dep_labels]}",
            "",
            "@overrides",
            "-    def get_padding_lengths(self, token: int) -> Dict[str, int]:  # pylint: disable=unused-argument",
            "+    def get_padding_lengths(self, token: int) -> Dict[str, int]:",
            "return {}",
            "",
            "@overrides",
            "def as_padded_tensor(self,",
            "tokens: Dict[str, List[int]],",
            "desired_num_tokens: Dict[str, int],",
            "-                         padding_lengths: Dict[str, int]) -> Dict[str, torch.Tensor]:  # pylint: disable=unused-argument",
            "+                         padding_lengths: Dict[str, int]) -> Dict[str, torch.Tensor]:",
            "return {key: torch.LongTensor(pad_sequence_to_length(val, desired_num_tokens[key]))",
            "for key, val in tokens.items()}"
        ]
    },
    {
        "number": 2218,
        "comments": "",
        "commit_message": "tutorial fixes from #336. Wheels with cuda10.1.\n\nSummary:\nAdd a document to explain how to run the tutorials.\nFix API of TexturesVertex in fit_textured_mesh.\nPrepare cuda 10.1 wheels (not 10.2) for linux to be available on pypi - this matches what colab has.\nChange the tutorials to use these new wheels.\n\nReviewed By: gkioxari\n\nDifferential Revision: D23324479\n\nfbshipit-source-id: 60e92a3f46a2d878f811b7703638f8d1dae143d9\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "\"    new_src_mesh = src_mesh.offset_verts(deform_verts)\\n\",",
            "\"    \\n\",",
            "\"    # Add per vertex colors to texture the mesh\\n\",",
            "-    \"    new_src_mesh.textures = TexturesVertex(verts_rgb=sphere_verts_rgb) \\n\",",
            "+    \"    new_src_mesh.textures = TexturesVertex(verts_features=sphere_verts_rgb) \\n\",",
            "\"    \\n\",",
            "\"    # Losses to smooth /regularize the mesh shape\\n\",",
            "\"    loss = {k: torch.tensor(0.0, device=device) for k in losses}\\n\","
        ]
    },
    {
        "number": 2225,
        "comments": "",
        "commit_message": "rnn->encoders, removes _th suffix, same filenames for chainer and pytorch, modify tests to not rely on _th\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def main():",
            "np.random.seed(args.seed)",
            "",
            "if args.backend == \"pytorch\":",
            "-        from espnet.tts.pytorch.tts_pytorch import train",
            "+        from espnet.tts.pytorch.tts import train",
            "train(args)",
            "else:",
            "raise NotImplementedError(\"Only pytorch is supported.\")"
        ]
    },
    {
        "number": 2227,
        "comments": "",
        "commit_message": "fix mypy\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DefaultClassifier(Classifier[DT], typing.Generic[DT]):",
            "progress_bar.set_description(\"Batch inference\")",
            "dataloader = progress_bar",
            "",
            "-            overall_loss = 0",
            "+            overall_loss = torch.zeros(1, device=flair.device)",
            "label_count = 0",
            "for batch in dataloader:",
            "# stop if all sentences are empty"
        ]
    },
    {
        "number": 2228,
        "comments": "",
        "commit_message": "Fix double precision casting complex buffers (#8208)\n\n* Fix double precision casting complex buffers\n\n* Update CHANGELOG.md\n\n* Fixes\n\n* Fixes\n\n* Fix\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DoublePrecisionPlugin(PrecisionPlugin):",
            "incoming floating point data to double (``torch.float64``) precision. Does not alter `optimizers` or",
            "`lr_schedulers`.",
            "\"\"\"",
            "-        model = cast(pl.LightningModule, model.to(dtype=torch.float64))",
            "+        model = cast(pl.LightningModule, model.double())",
            "model = LightningDoublePrecisionModule(model)",
            "",
            "return super().connect(model, optimizers, lr_schedulers)"
        ]
    },
    {
        "number": 2233,
        "comments": "",
        "commit_message": "Layer API Refactoring (#675)\n\n* Layer API Refactoring\n\n* private method decorator added\n\n* Input Layers added\n\n* Lazy Import on ROI pooling\n\n* @zsdonghao change proposal added\n\n* UTF-8 header added\n\n* gitignore updated\n\n* Python2 error correction\n\n* Changelog Updated\n\n* Python Headers Uniformized + Codacy Error Fix + LSTMStateTuple missing import\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TileLayer(Layer):",
            "",
            "@deprecated_alias(layer='prev_layer', end_support_version=1.9)  # TODO remove this line for the 1.9 release",
            "def __init__(self, prev_layer, multiples=None, name='tile'):",
            "+",
            "super(TileLayer, self).__init__(prev_layer=prev_layer, name=name)",
            "",
            "logging.info(\"TileLayer  %s: multiples:%s\" % (name, multiples))",
            "",
            "-        self.inputs = prev_layer.outputs",
            "-",
            "with tf.variable_scope(name):",
            "self.outputs = tf.tile(self.inputs, multiples=multiples)",
            "",
            "-        self.all_layers.append(self.outputs)",
            "+        self._add_layers(self.outputs)"
        ]
    },
    {
        "number": 2234,
        "comments": "",
        "commit_message": "ReduceLROnPlateau bug fix (#1126)\n\n* bug fix and test\n\n* update CHANGELOG.md\n\nCo-authored-by: Nicki Skafte <nugginea@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Trainer(",
            "if 'scheduler' not in scheduler:",
            "raise ValueError(f'Lr scheduler should have key `scheduler`',",
            "' with item being a lr scheduler')",
            "-                scheduler['reduce_on_plateau'] = \\",
            "-                    isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau)",
            "+                scheduler['reduce_on_plateau'] = isinstance(",
            "+                    scheduler['scheduler'], optim.lr_scheduler.ReduceLROnPlateau)",
            "",
            "lr_schedulers.append({**default_config, **scheduler})"
        ]
    },
    {
        "number": 2237,
        "comments": "",
        "commit_message": "fix tower-summary bug\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class QueueInputTrainer(Trainer):",
            "kept_summaries[k] = copy.copy(tf.get_collection(k))",
            "logger.info(\"Graph built for tower {}.\".format(i))",
            "for k in coll_keys:",
            "-                del tf.get_collection(k)[:]",
            "-                tf.get_collection(k).extend(kept_summaries[k])",
            "+                del tf.get_collection_ref(k)[:]",
            "+                tf.get_collection_ref(k).extend(kept_summaries[k])",
            "grads = QueueInputTrainer._average_grads(grad_list)",
            "cost_var = cost_var_t0",
            "else:"
        ]
    },
    {
        "number": 2239,
        "comments": "",
        "commit_message": "[torch] Enable GPU training (#1508)\n\n* Added more descriptive description_summary, fixed bug in TabNet combiner\n\n* Updated modules to do proper GPU conversion\n\n* Functional GPU training\n\n* Tabnet working, removed RayRemoteTrainer\n\n* Temporary changes to unblock GPU training\n\n* Functional test_seq_encoder\n\n* Cleaned up PR\n\n* Fixed failing CPU tests\n\n* Responded to comments\n\n* Fix failing GPU tests in test_api.py\n\n* [torch] Fix GPU tests for test_visualizations (#1539)\n\nUpdated torch.aranges to registered buffers in embedding_modules.py\n\n* [torch] Fix test_encoder.py for GPU (#1544)\n\n* Accept and move model to GPU as part of Predictor init (#1559)\n\n* [torch] Fixed tests in test_experiment.py (#1562)\n\n* Fixed tests in tests/integration_tests/test_experiment.py\n\n* Moved set embed and inputs to correct device in tests/ludwig/encoders/test_set_encoders.py\n\n* Moved set date and inputs to correct device in tests/ludwig/encoders/test_date_encoders.py\n\n* Moved bag embed and inputs to correct device in tests/ludwig/encoders/test_bag_encoders.py\n\n* Moved sequence embed and inputs to correct device in tests/ludwig/features/test_sequence_features.py\n\n* [torch] Fixed GPU issues in combiners.py (#1568)\n\n* Fix GPU tests in test_collect.py (#1569)\n\n* Added model to Predictor initialization\n\n* Fixed embedding modules tests (#1554)\n\n* Fixed embedding modules tests\n* Moved embedding modules to device in tests, removed duplicate argument\n\nCo-authored-by: Shreya Rajpal <shreya.rajpal@gmail.com>\n\n* Fixed encoders tests except test_h3_rnn_embed (#1556)\n\n* Fixed encoders tests except test_h3_rnn_embed\n\n* Addressed Shreya's comments\n\n* Moved embedding modules to device, used registered buffers\n\nCo-authored-by: Shreya Rajpal <shreya.rajpal@gmail.com>\n\n* Pass ECD object to Predictor\n\n* Removed model arg from predict function\n\nCo-authored-by: Jeffrey Tang <jeff@predibase.com>\nCo-authored-by: Jeffrey Tang <810895+jeffreyftang@users.noreply.github.com>\nCo-authored-by: Animesh Kumar <anmshkmr@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def run_api_experiment(input_features, output_features, data_csv):",
            "model_weights = get_weights(model.model)",
            "loaded_weights = get_weights(loaded_model.model)",
            "for model_weight, loaded_weight in zip(model_weights, loaded_weights):",
            "-            assert np.allclose(model_weight, loaded_weight)",
            "+            assert torch.allclose(model_weight, loaded_weight)",
            "finally:",
            "# Remove results/intermediate data saved to disk",
            "shutil.rmtree(output_dir, ignore_errors=True)"
        ]
    },
    {
        "number": 2242,
        "comments": "",
        "commit_message": "api version fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def anchor_target_single(flat_anchors,",
            "num_valid_anchors = anchors.shape[0]",
            "bbox_targets = torch.zeros_like(anchors)",
            "bbox_weights = torch.zeros_like(anchors)",
            "-    labels = anchors.new_zeros((num_valid_anchors, ))",
            "-    label_weights = anchors.new_zeros((num_valid_anchors, ))",
            "+    labels = gt_labels.new_zeros(num_valid_anchors)",
            "+    label_weights = gt_labels.new_zeros(num_valid_anchors, dtype=torch.float)",
            "",
            "pos_inds = sampling_result.pos_inds",
            "neg_inds = sampling_result.neg_inds"
        ]
    },
    {
        "number": 2251,
        "comments": "",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_pow():",
            "def test_tensor_ops():",
            "pi = 3.141592654",
            "X = Uniform(0, 1).expand([5, 5]).rv",
            "-    a = tt([[1, 2, 3, 4, 5]])",
            "+    a = torch.tensor([[1, 2, 3, 4, 5]])",
            "b = a.T",
            "X = abs(pi*(-X + a - 3*b))",
            "x = X.dist.sample()"
        ]
    },
    {
        "number": 2258,
        "comments": "",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "__all__ = ['ConcatWith']",
            "@layer_register(use_scope=False, log_shape=False)",
            "def ConcatWith(x, dim, tensor):",
            "\"\"\"",
            "-    A wrapper around `tf.concat` to support `LinearWrap`",
            "+    A wrapper around `tf.concat_v2` to support `LinearWrap`",
            ":param x: the input tensor",
            ":param dim: the dimension along which to concatenate",
            ":param tensor: a tensor or list of tensor to concatenate with x.",
            "x will be at the beginning",
            "-    :return: tf.concat(dim, [x] + [tensor])",
            "+    :return: tf.concat_v2([x] + [tensor], dim)",
            "\"\"\"",
            "if type(tensor) != list:",
            "tensor = [tensor]",
            "-    return tf.concat(dim, [x] + tensor)",
            "+    return tf.concat_v2([x] + tensor, dim)"
        ]
    },
    {
        "number": 2270,
        "comments": "",
        "commit_message": "Add test to ensure models can take int64 inputs (#17210)\n\n* Add test to ensure models can take int64 inputs\n\n* is_integer is an attribute, not a method\n\n* Fix test when some inputs aren't tensors\n\n* Add casts to blenderbot and blenderbot-small\n\n* Add casts to the other failing models\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def flatten(index, name=\"segmented_flatten\"):",
            "for _ in range(index.batch_dims, index.indices.shape.rank):",
            "offset = tf.expand_dims(offset, -1)",
            "",
            "-    indices = offset + index.indices",
            "+    indices = tf.cast(offset, index.indices.dtype) + index.indices",
            "return IndexMap(indices=tf.reshape(indices, [-1]), num_segments=index.num_segments * batch_size, batch_dims=0)"
        ]
    },
    {
        "number": 2272,
        "comments": "",
        "commit_message": "fix DiffPool benchmark\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def get_dataset(name, sparse=True, cleaned=False):",
            "for i, data in enumerate(dataset):",
            "if data.num_nodes <= num_nodes:",
            "indices.append(i)",
            "-        dataset = dataset[torch.tensor(indices)]",
            "+        dataset = dataset.copy(torch.tensor(indices))",
            "",
            "if dataset.transform is None:",
            "dataset.transform = T.ToDense(num_nodes)"
        ]
    },
    {
        "number": 2273,
        "comments": "",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CholeskyTransform(Transform):",
            "return isinstance(other, CholeskyTransform)",
            "",
            "def _call(self, x):",
            "-        return torch.cholesky(x)",
            "+        return torch.linalg.cholesky(x)",
            "",
            "def _inverse(self, y):",
            "return torch.matmul(y, torch.transpose(y, -2, -1))"
        ]
    },
    {
        "number": 2283,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ScalarMix(torch.nn.Module):",
            "return self.gamma * sum(pieces)",
            "",
            "else:",
            "-            mask_float = mask.float()",
            "-            broadcast_mask = mask_float.unsqueeze(-1)",
            "+            broadcast_mask = mask.unsqueeze(-1)",
            "input_dim = tensors[0].size(-1)",
            "-            num_elements_not_masked = torch.sum(mask_float) * input_dim",
            "+            num_elements_not_masked = torch.sum(mask) * input_dim",
            "",
            "pieces = []",
            "for weight, tensor in zip(normed_weights, tensors):"
        ]
    },
    {
        "number": 2285,
        "comments": "",
        "commit_message": "fix loading always best_model when continue\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def get_last_checkpoint(path):",
            "key_file_names = [fn for fn in file_names if key in fn]",
            "if last_model is None and len(key_file_names) > 0:",
            "last_model = max(key_file_names, key=os.path.getctime)",
            "-            last_model_num = os.path.getctime(last_model)",
            "+            last_model_num = torch.load(last_model)['step']",
            "",
            "if last_model is not None:",
            "last_models[key] = last_model"
        ]
    },
    {
        "number": 2286,
        "comments": "",
        "commit_message": "Fix Flask REST API (#7210)\n\n* Update restapi.py\n\n* Update restapi.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Cleanup\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def predict():",
            "if __name__ == \"__main__\":",
            "parser = argparse.ArgumentParser(description=\"Flask API exposing YOLOv5 model\")",
            "parser.add_argument(\"--port\", default=5000, type=int, help=\"port number\")",
            "-    args = parser.parse_args()",
            "+    opt = parser.parse_args()",
            "+",
            "+    # Fix known issue urllib.error.HTTPError 403: rate limit exceeded https://github.com/ultralytics/yolov5/pull/7210",
            "+    torch.hub._validate_not_a_forked_repo = lambda a, b, c: True",
            "",
            "model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", force_reload=True)  # force_reload to recache",
            "-    app.run(host=\"0.0.0.0\", port=args.port)  # debug=True causes Restarting with stat",
            "+    app.run(host=\"0.0.0.0\", port=opt.port)  # debug=True causes Restarting with stat"
        ]
    },
    {
        "number": 2289,
        "comments": "",
        "commit_message": "config, benchmark notebook, synthesis fixed\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def synthesis(model,",
            "style_mel = compute_style_mel(style_wav, ap, use_cuda)",
            "# preprocess the given text",
            "inputs = text_to_seqvec(text, CONFIG, use_cuda)",
            "-    speaker_id = speaker_id_var = torch.from_numpy(speaker_id).unsqueeze(0)",
            "+    speaker_id = np.asarray(speaker_id)",
            "+    speaker_id = torch.from_numpy(speaker_id).unsqueeze(0)",
            "if use_cuda:",
            "speaker_id.cuda()",
            "# synthesize voice",
            "decoder_output, postnet_output, alignments, stop_tokens = run_model(",
            "-        model, inputs, CONFIG, truncated, style_mel)",
            "+        model, inputs, speaker_id, CONFIG, truncated, style_mel)",
            "# convert outputs to numpy",
            "postnet_output, decoder_output, alignment = parse_outputs(",
            "postnet_output, decoder_output, alignments)"
        ]
    },
    {
        "number": 2305,
        "comments": "",
        "commit_message": "fix: add get_value in tests\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class AttentionTest(tf.test.TestCase, parameterized.TestCase):",
            "attention_layer.concat_score_weight = 1",
            "attention_layer.build(input_shape=([1, 1, 1], [1, 1, 1]))",
            "attention_layer.scale = 2.",
            "-    actual = attention_layer._calculate_scores(query=q, key=k)",
            "+    actual = keras.backend.get_value(",
            "+            attention_layer._calculate_scores(query=q, key=k))",
            "",
            "# Expected tensor of shape [1, 1, 1].",
            "# expected000 = tanh(2*(1.1+1.6)) = 0.9999592018254402"
        ]
    },
    {
        "number": 2308,
        "comments": "",
        "commit_message": "fix sharded_ddp mode\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Trainer:",
            "iterator: Iterable[Dict[str, torch.Tensor]],",
            "reporter: SubReporter,",
            "options: TrainerOptions,",
            "+        distributed_option: DistributedOption,",
            ") -> None:",
            "assert check_argument_types()",
            "ngpu = options.ngpu",
            "no_forward_run = options.no_forward_run",
            "-        distributed = isinstance(model, torch.nn.parallel.DistributedDataParallel)",
            "+        distributed = distributed_option.distributed",
            "",
            "model.eval()"
        ]
    },
    {
        "number": 2309,
        "comments": "",
        "commit_message": "fix path to allow for restoring saved models\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TensorFlowEstimator(BaseEstimator):",
            "if not os.path.exists(saver_filename):",
            "raise ValueError(\"Restore folder doesn't contain saver defintion.\")",
            "with open(saver_filename) as fsaver:",
            "-                saver_def = tf.python.training.saver_pb2.SaverDef()",
            "+                saver_def = tf.python.training.saver.saver_pb2.SaverDef()",
            "text_format.Merge(fsaver.read(), saver_def)",
            "self._saver = tf.train.Saver(saver_def=saver_def)"
        ]
    },
    {
        "number": 2319,
        "comments": "",
        "commit_message": "fix tests\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "if is_torch_available():",
            "",
            "# TensorFlow",
            "if is_tf_available():",
            "-    logger.info(\"TensorFlow version {} available.\".format(tf.__version__))",
            "-",
            "from .modeling_tf_utils import TFPreTrainedModel, TFSharedEmbeddings, TFSequenceSummary",
            "from .modeling_tf_auto import (TFAutoModel, TFAutoModelForSequenceClassification, TFAutoModelForQuestionAnswering,",
            "TFAutoModelWithLMHead)"
        ]
    },
    {
        "number": 2338,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class SpearmanCorrelationTest(AllenNlpTestCase):",
            "predictions_labels_ = [(predictions1, labels1), (predictions2, labels2)]",
            "",
            "# Random binary mask",
            "-        mask = torch.randint(0, 2, size=(batch_size, num_labels), device=device)",
            "+        mask = torch.randint(0, 2, size=(batch_size, num_labels), device=device).bool()",
            "",
            "for predictions, labels in predictions_labels_:",
            "spearman_correlation.reset()"
        ]
    },
    {
        "number": 2345,
        "comments": "",
        "commit_message": "FX tracing improvement (#14321)\n\n* Change the way tracing happens, enabling dynamic axes out of the box\n\n* Update the tests and modeling xlnet\n\n* Add the non recoding of leaf modules to avoid recording more values for the methods to record than what will be seen at tracing time (which would otherwise desynchronize the recorded values and the values that need to be given to the proxies during tracing, causing errors).\n\n* Comments and making tracing work for gpt-j and xlnet\n\n* Refactore things related to num_choices (and batch_size, sequence_length)\n\n* Update fx to work on PyTorch 1.10\n\n* Postpone autowrap_function feature usage for later\n\n* Add copyrights\n\n* Remove unnecessary file\n\n* Fix issue with add_new_model_like\n\n* Apply suggestions\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class GPTNeoForSequenceClassification(GPTNeoPreTrainedModel):",
            "f\"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"",
            ")",
            "",
            "-        pooled_logits = logits[torch.arange(batch_size), sequence_lengths]",
            "+        pooled_logits = logits[torch.arange(batch_size, device=self.device), sequence_lengths]",
            "",
            "loss = None",
            "if labels is not None:"
        ]
    },
    {
        "number": 2359,
        "comments": "",
        "commit_message": "Fix imports & docstrings\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def serialize_model_as_bytecode(model):",
            "archive.addfile(tarinfo=info, fileobj=f)",
            "tf.io.gfile.rmtree(temp_dir)",
            "b.seek(0)",
            "-  return (asarray(memoryview(b.read())), )",
            "+  return (numpy.asarray(memoryview(b.read())), )"
        ]
    },
    {
        "number": 2360,
        "comments": "",
        "commit_message": "Several micro optimizations (#4833)\n\n* benchmark transfers\n\n* create tensors directl on device when possible\n\n* fix\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Entropy(Metric):",
            "_count = 1",
            "",
            "if is_distributed():",
            "-            count = torch.tensor(_count).to(device)",
            "+            count = torch.tensor(_count, device=device)",
            "dist.all_reduce(_entropy, op=dist.ReduceOp.SUM)",
            "dist.all_reduce(count, op=dist.ReduceOp.SUM)",
            "_count = count.item()"
        ]
    },
    {
        "number": 2361,
        "comments": "",
        "commit_message": "pyrdown fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PyrDown(nn.Module):",
            "input, self.kernel, self.border_type)",
            "",
            "# reject even rows and columns.",
            "-        out: torch.Tensor = x_blur[..., ::2, ::2]",
            "+        out: torch.Tensor = F.avg_pool2d(x_blur, 2,2)",
            "return out"
        ]
    },
    {
        "number": 2368,
        "comments": "",
        "commit_message": "more consistent assigner (#2536)\n\n* added background label to assign function\n\n* fixed the background label as -1 in all assigners\n\nCo-authored-by: wangxinjiang <wangxinjiang@sensetime.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ATSSAssigner(BaseAssigner):",
            "max_overlaps != -INF] = argmax_overlaps[max_overlaps != -INF] + 1",
            "",
            "if gt_labels is not None:",
            "-            assigned_labels = assigned_gt_inds.new_zeros((num_bboxes, ))",
            "+            assigned_labels = assigned_gt_inds.new_full((num_bboxes, ), -1)",
            "pos_inds = torch.nonzero(assigned_gt_inds > 0).squeeze()",
            "if pos_inds.numel() > 0:",
            "assigned_labels[pos_inds] = gt_labels["
        ]
    },
    {
        "number": 2373,
        "comments": "",
        "commit_message": "Fixed weight init for fused weight matrices in fused MHA by adding correct gain factor.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class SelfMultiheadAttn(nn.Module):",
            "nn.init.xavier_uniform_(self.k_weight)",
            "nn.init.xavier_uniform_(self.v_weight)",
            "else:",
            "-            nn.init.xavier_uniform_(self.in_proj_weight)",
            "+            # in_proj_weight has shape [3 * hidden, hidden] but it should be",
            "+            # initialized like a [hidden, hidden] matrix.",
            "+            # sqrt(6 / (hidden + hidden)) / sqrt(6 / (3 * hidden + hidden)) = sqrt(2)",
            "+            # therefore xavier_uniform gain should be set to sqrt(2).",
            "+            nn.init.xavier_uniform_(self.in_proj_weight, gain=math.sqrt(2))",
            "nn.init.xavier_uniform_(self.out_proj_weight)",
            "if self.bias:",
            "if self.separate_qkv_params:"
        ]
    },
    {
        "number": 2376,
        "comments": "",
        "commit_message": "Enable direct `--weights URL` definition (#3373)\n\n* Enable direct `--weights URL` definition\n\n@KalenMike this PR will enable direct --weights URL definition. Example use case:\n```\npython train.py --weights https://storage.googleapis.com/bucket/dir/model.pt\n```\n\n* cleanup\n\n* bug fixes\n\n* weights = attempt_download(weights)\n\n* Update experimental.py\n\n* Update hubconf.py\n\n* return bug fix\n\n* comment mirror\n\n* min_bytes\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def attempt_load(weights, map_location=None, inplace=True):",
            "# Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a",
            "model = Ensemble()",
            "for w in weights if isinstance(weights, list) else [weights]:",
            "-        attempt_download(w)",
            "-        ckpt = torch.load(w, map_location=map_location)  # load",
            "+        ckpt = torch.load(attempt_download(w), map_location=map_location)  # load",
            "model.append(ckpt['ema' if ckpt.get('ema') else 'model'].float().fuse().eval())  # FP32 model",
            "",
            "# Compatibility updates"
        ]
    },
    {
        "number": 2381,
        "comments": "",
        "commit_message": "[RLlib] Minor cleanup in preparation to tf2.x support. (#9130)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* Fixes.\n\n* Fixes and LINT.\n\n* WIP.\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def stats(policy, train_batch):",
            "",
            "def grad_stats(policy, train_batch, grads):",
            "return {",
            "-        \"grad_gnorm\": tf.global_norm(grads),",
            "+        \"grad_gnorm\": tf.linalg.global_norm(grads),",
            "}"
        ]
    },
    {
        "number": 2385,
        "comments": "",
        "commit_message": "reduced redundant arguments to reptile method, and other small fixes.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def execute_with_gradients(func, xs, retain_grads=False):",
            "else:",
            "y = func_ret",
            "rest = tuple()",
            "-    x_grads_flat = _mx.autograd.grad(y, retain_graph=retain_grads, variables=[v for k, v in xs.to_iterator()])",
            "+    x_grads_flat = _mx.autograd.grad(y, [v for k, v in xs.to_iterator()], retain_graph=retain_grads,",
            "+                                     create_graph=retain_grads)",
            "return (y, xs.from_flat_list(x_grads_flat), *rest)"
        ]
    },
    {
        "number": 2386,
        "comments": "",
        "commit_message": "Update wavegrad.py\n\nThis should fix the issue https://github.com/mozilla/TTS/issues/581\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Wavegrad(nn.Module):",
            "self.noise_level = self.noise_level.to(y_0)",
            "if len(y_0.shape) == 3:",
            "y_0 = y_0.squeeze(1)",
            "-        s = torch.randint(1, self.num_steps + 1, [y_0.shape[0]])",
            "-        l_a, l_b = self.noise_level[s-1], self.noise_level[s]",
            "+        s = torch.randint(0, self.num_steps - 1, [y_0.shape[0]])",
            "+        l_a, l_b = self.noise_level[s], self.noise_level[s+1]",
            "noise_scale = l_a + torch.rand(y_0.shape[0]).to(y_0) * (l_b - l_a)",
            "noise_scale = noise_scale.unsqueeze(1)",
            "noise = torch.randn_like(y_0)"
        ]
    },
    {
        "number": 2387,
        "comments": "",
        "commit_message": "Use preprocessing layers for categorical encoding (#1090)\n\n* removed sigmoid layer\n\n* added lookup\n\n* bug fix\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CategoricalToNumerical(block_module.Block):",
            "encoding.append(keras_layers.INT)",
            "else:",
            "encoding.append(keras_layers.NONE)",
            "-        return keras_layers.CategoricalEncoding(encoding)(input_node)",
            "+        return keras_layers.MultiColumnCategoricalEncoding(encoding)(input_node)"
        ]
    },
    {
        "number": 2391,
        "comments": "",
        "commit_message": "fix resuming when multigpu mode:   espnet2/tasks/abs_task.py\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class AbsTask(ABC):",
            ")",
            "model.to(device)",
            "if model_file is not None:",
            "+            if device == \"cuda\":",
            "+                # NOTE(kamo): \"cuda\" for torch.load always indicates cuda:0",
            "+                #   in PyTorch<=1.4",
            "+                device = f\"cuda:{torch.cuda.current_device()}\"",
            "model.load_state_dict(torch.load(model_file, map_location=device))",
            "",
            "return model, args"
        ]
    },
    {
        "number": 2395,
        "comments": "",
        "commit_message": "fix mypy errors\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class SpatialSoftArgmax2d(nn.Module):",
            "",
            "# compute softmax with max substraction trick",
            "exp_x = torch.exp(x - torch.max(x, dim=-1, keepdim=True)[0])",
            "-        exp_x_sum = 1.0 / (exp_x.sum(dim=-1, keepdim=True) + self.eps)",
            "+        exp_x_sum = torch.tensor(",
            "+            1.0) / (exp_x.sum(dim=-1, keepdim=True) + self.eps)",
            "",
            "# create coordinates grid",
            "pos_y, pos_x = create_meshgrid(input, self.normalized_coordinates)"
        ]
    },
    {
        "number": 2409,
        "comments": "",
        "commit_message": "Fix spurious warning in TF TokenClassification models (#15435)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFTokenClassificationLoss:",
            "# make sure only labels that are not equal to -100",
            "# are taken into account as loss",
            "if tf.math.reduce_any(labels == -1):",
            "-            warnings.warn(\"Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\")",
            "+            tf.print(\"Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\")",
            "active_loss = tf.reshape(labels, (-1,)) != -1",
            "else:",
            "active_loss = tf.reshape(labels, (-1,)) != -100"
        ]
    },
    {
        "number": 2410,
        "comments": "",
        "commit_message": "Add PyTorch AMP check (#7917)\n\n* Add PyTorch AMP check\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Cleanup\n\n* Cleanup\n\n* Cleanup\n\n* Robust for DDP\n\n* Fixes\n\n* Add amp enabled boolean to check_train_batch_size\n\n* Simplify\n\n* space to prefix\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "from copy import deepcopy",
            "",
            "import numpy as np",
            "import torch",
            "-from torch.cuda import amp",
            "",
            "from utils.general import LOGGER, colorstr",
            "from utils.torch_utils import profile",
            "",
            "",
            "-def check_train_batch_size(model, imgsz=640):",
            "+def check_train_batch_size(model, imgsz=640, amp=True):",
            "# Check YOLOv5 training batch size",
            "-    with amp.autocast():",
            "+    with torch.cuda.amp.autocast(amp):",
            "return autobatch(deepcopy(model).train(), imgsz)  # compute optimal batch size"
        ]
    },
    {
        "number": 2411,
        "comments": "",
        "commit_message": "Add Speaker embedding, fix tacotron-2 missing code.\n\n* add speaker embedding for fastspeech encoder and decoder.\n* use softplus activation function for speaker embedding.\n* training always = True for prenet.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_tacotron2_trainable(n_speakers, n_chars, max_input_length, max_mel_leng",
            "post_mel_preds, \\",
            "stop_preds, \\",
            "alignment_history = model(input_ids,",
            "-                                          tf.constant([max_mel_length, max_mel_length]),",
            "+                                          tf.constant([max_input_length, max_input_length]),",
            "speaker_ids,",
            "mel_outputs,",
            "-                                          mel_lengths)",
            "+                                          mel_lengths,",
            "+                                          training=True)",
            "loss_before = tf.keras.losses.MeanSquaredError()(mel_outputs, mel_preds)",
            "loss_after = tf.keras.losses.MeanSquaredError()(mel_outputs, post_mel_preds)"
        ]
    },
    {
        "number": 2412,
        "comments": "",
        "commit_message": "[RLlib] CQL loss fn fixes, MuJoCo + Pendulum benchmarks, offline-RL example script w/ json file. (#15603)\n\nCo-authored-by: Sven Mika <sven@anyscale.io>\nCo-authored-by: sven1977 <svenmika1977@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def apply_grad_clipping(policy, optimizer, loss):",
            "",
            "",
            "def atanh(x):",
            "-    return 0.5 * torch.log((1 + x) / (1 - x))",
            "+    return 0.5 * torch.log(",
            "+        (1 + x).clamp(min=SMALL_NUMBER) / (1 - x).clamp(min=SMALL_NUMBER))",
            "",
            "",
            "def convert_to_non_torch_type(stats):"
        ]
    },
    {
        "number": 2415,
        "comments": "",
        "commit_message": "Fix logging when loss is in FP16 (#1047)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1047\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D20068167\n\nPulled By: myleott\n\nfbshipit-source-id: 46ccca05ed225103a1b2aa6fa80f8fbfe91b040b\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Trainer(object):",
            "# convert logging_outputs to CPU to avoid unnecessary",
            "# device-to-host transfers in reduce_metrics",
            "logging_outputs = utils.apply_to_sample(",
            "-                lambda t: t.to(device='cpu', non_blocking=True),",
            "+                lambda t: t.to(device='cpu', non_blocking=True, dtype=torch.double),",
            "logging_outputs",
            ")"
        ]
    },
    {
        "number": 2418,
        "comments": "",
        "commit_message": "misc fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def batch_flatten(x):",
            "return tf.reshape(x, [-1, total_dim])",
            "",
            "def logSoftmax(x):",
            "-    with tf.variable_scope('logSoftmax'):",
            "+    with tf.op_scope([x], 'logSoftmax'):",
            "z = x - tf.reduce_max(x, 1, keep_dims=True)",
            "logprob = z - tf.log(tf.reduce_sum(tf.exp(z), 1, keep_dims=True))",
            "return logprob"
        ]
    },
    {
        "number": 2423,
        "comments": "",
        "commit_message": "fix flake8 issues (#2570)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\n- [x] applies flake8 fixes to main branch (https://github.com/fairinternal/fairseq-py/issues/2546) - still more to be fixed\n\nFix GPU tests:\n- [x] when torch.ao.quantization import doesn't work use torch.quantization\n- [x] build apex from earlier commit in circleci so that its compatible with pytorch 1.8 and 1.9\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2570\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D32955312\n\nPulled By: dianaml0\n\nfbshipit-source-id: e163cbd4998f171f819e31b0682c1c0f1986f9e1\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def emulate_int8_channel(w, scale=None, zero_point=None, bits=8):",
            "",
            "def emulate_int8_tensor(w, scale=None, zero_point=None, bits=8):",
            "if scale is None:",
            "-        obs = torch.ao.quantization.observer.MinMaxObserver()",
            "+        obs = quantization.observer.MinMaxObserver()",
            "obs.to(device=w.device)",
            "_ = obs(w)",
            "scale, zero_point = obs.calculate_qparams()"
        ]
    },
    {
        "number": 2430,
        "comments": "",
        "commit_message": "Fix torch version comparisons (#18460)\n\nComparisons like\nversion.parse(torch.__version__) > version.parse(\"1.6\")\nare True for torch==1.6.0+cu101 or torch==1.6.0+cpu\n\nversion.parse(version.parse(torch.__version__).base_version) are preferred (and available in pytorch_utils.py\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def is_torch_tf32_available():",
            "return False",
            "if int(torch.version.cuda.split(\".\")[0]) < 11:",
            "return False",
            "-    if version.parse(torch.__version__) < version.parse(\"1.7\"):",
            "+    if version.parse(version.parse(torch.__version__).base_version) < version.parse(\"1.7\"):",
            "return False",
            "",
            "return True"
        ]
    },
    {
        "number": 2433,
        "comments": "",
        "commit_message": "fix sample_weight for BinAcc\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def binary_accuracy(y_true, y_pred, threshold=0.5):",
            "prediction values are 1 or 0.",
            "",
            "Returns:",
            "-    Binary accuracy values. shape = `[batch_size, d0, .. dN-1]`",
            "+    Binary accuracy values. shape = `[batch_size, d0, .. dN]`",
            "\"\"\"",
            "y_pred = tf.convert_to_tensor(y_pred)",
            "threshold = tf.cast(threshold, y_pred.dtype)",
            "y_pred = tf.cast(y_pred > threshold, y_pred.dtype)",
            "-  return backend.mean(tf.equal(y_true, y_pred), axis=-1)",
            "+  return tf.cast(tf.equal(y_true, y_pred), tf.int8)",
            "",
            "",
            "@keras_export('keras.metrics.categorical_accuracy')"
        ]
    },
    {
        "number": 2438,
        "comments": "",
        "commit_message": "[RLlib] DDPG PyTorch version. (#7953)\n\nThe DDPG/TD3 algorithms currently do not have a PyTorch implementation. This PR adds PyTorch support for DDPG/TD3 to RLlib.\nThis PR:\n- Depends on the re-factor PR for DDPG (Functional Algorithm API).\n- Adds learning regression tests for the PyTorch version of DDPG and a DDPG (torch)\n- Updates the documentation to reflect that DDPG and TD3 now support PyTorch.\n\n* Learning Pendulum-v0 on torch version (same config as tf). Wall time a little slower (~20% than tf).\n* Fix GPU target model problem.\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def get_variable(value,",
            "tf_name, initializer=value, dtype=dtype, trainable=trainable)",
            "elif framework == \"torch\" and torch_tensor is True:",
            "torch, _ = try_import_torch()",
            "-        var_ = torch.from_numpy(value).to(device)",
            "+        var_ = torch.from_numpy(value)",
            "+        if device:",
            "+            var_ = var_.to(device)",
            "var_.requires_grad = trainable",
            "return var_",
            "# torch or None: Return python primitive."
        ]
    },
    {
        "number": 2443,
        "comments": "",
        "commit_message": "fix a type mismatch in NAT quantization run\n\nSummary:\nFix a type mismatch which was found after patching NAT on top of quantization.\nNing suggested this fix. Need to further understand: why this only appears after patching quantization diff?\n\nReviewed By: kahne, jhcross\n\nDifferential Revision: D18147726\n\nfbshipit-source-id: a51becc9ad58a637a0180074eaa2b46990ab9f84\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def fill_tensors(x, mask, y, padding_idx: int):",
            "x = expand_2d_or_3d_tensor(x, y.size(1), padding_idx)",
            "x[mask] = y",
            "elif x.size(1) > y.size(1):",
            "-        x[mask] = torch.tensor(padding_idx)",
            "+        x[mask] = torch.tensor(padding_idx).type_as(x)",
            "if x.dim() == 2:",
            "x[mask, :y.size(1)] = y",
            "else:"
        ]
    },
    {
        "number": 2447,
        "comments": "",
        "commit_message": "Tf model outputs (#6247)\n\n* TF outputs and test on BERT\n\n* Albert to DistilBert\n\n* All remaining TF models except T5\n\n* Documentation\n\n* One file forgotten\n\n* TF outputs and test on BERT\n\n* Albert to DistilBert\n\n* All remaining TF models except T5\n\n* Documentation\n\n* One file forgotten\n\n* Add new models and fix issues\n\n* Quality improvements\n\n* Add T5\n\n* A bit of cleanup\n\n* Fix for slow tests\n\n* Style\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class XLNetModel(XLNetPreTrainedModel):",
            "# Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)",
            "output = output.permute(1, 0, 2).contiguous()",
            "",
            "-        # TODO Teven: fix this test to only use use_cache.",
            "if not use_cache:",
            "new_mems = None"
        ]
    },
    {
        "number": 2451,
        "comments": "",
        "commit_message": "Fix TF2 / rllib test (#5846)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MyKerasRNN(RecurrentTFModelV2):",
            "shape=(None, obs_space.shape[0]), name=\"inputs\")",
            "state_in_h = tf.keras.layers.Input(shape=(cell_size, ), name=\"h\")",
            "state_in_c = tf.keras.layers.Input(shape=(cell_size, ), name=\"c\")",
            "-        seq_in = tf.keras.layers.Input(shape=(), name=\"seq_in\")",
            "+        seq_in = tf.keras.layers.Input(shape=(), name=\"seq_in\", dtype=tf.int32)",
            "",
            "# Preprocess observation with a hidden layer and send to LSTM cell",
            "dense1 = tf.keras.layers.Dense("
        ]
    },
    {
        "number": 2461,
        "comments": "",
        "commit_message": "Merge pull request #7872 from PyTorchLightning/refactor/logger-poc-changes\n\nRandom fixes for logger connector PoC\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ModelPruning(Callback):",
            "def _wrap_pruning_fn(pruning_fn: Callable, **kwargs: Any) -> Callable:",
            "return partial(pruning_fn, **kwargs)",
            "",
            "-    def make_pruning_permanent(self, pl_module: LightningModule) -> None:",
            "+    def make_pruning_permanent(self, module: nn.Module) -> None:",
            "\"\"\"",
            "Removes pruning buffers from any pruned modules",
            "",
            "Adapted from https://github.com/pytorch/pytorch/blob/1.7.1/torch/nn/utils/prune.py#L1176-L1180",
            "\"\"\"",
            "-        for _, module in pl_module.named_modules():",
            "+        for _, module in module.named_modules():",
            "for k in list(module._forward_pre_hooks):",
            "hook = module._forward_pre_hooks[k]",
            "if isinstance(hook, pytorch_prune.BasePruningMethod):"
        ]
    },
    {
        "number": 2465,
        "comments": "",
        "commit_message": "fix mulacc when both strides are 0\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "torch_fxn_for_op : Dict[Op, Callable] = {**base_fxn_for_op, **{",
            "MovementOps.PAD: lambda x, padding: torch.nn.functional.pad(x, [item for sublist in padding[::-1] for item in sublist]),",
            "MovementOps.STRIDED: lambda x, arg: x.contiguous().as_strided([y[0] for y in arg], [y[1] for y in arg]),",
            "ProcessingOps.CONV: lambda x,w,C: C.px == C.px_ and C.py == C.py_ and torch.conv2d(x, w, stride=(C.sy, C.sx), groups=C.groups, dilation=(C.dy, C.dx), padding=(C.py, C.px)),",
            "-  FusedOps.MULACC: einsum_mulacc(torch.einsum, lambda x: x.stride())",
            "+  FusedOps.MULACC: einsum_mulacc(torch.einsum, lambda x: x.stride(), lambda x,s: x.expand(s))",
            "}}",
            "",
            "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else (\"mps\" if getenv(\"MPS\", 0) else \"cpu\"))"
        ]
    },
    {
        "number": 2466,
        "comments": "",
        "commit_message": "fixed standardize preprocessor\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Standardize(Preprocessor):",
            "else:",
            "axes = tuple(range(1, util.rank(tensor)))",
            "",
            "-        mean, variance = tf.nn.moments(x=tensor, axes=axes)",
            "-        return (tensor - mean) / tf.maximum(x=variance, y=util.epsilon)",
            "+        mean, variance = tf.nn.moments(x=tensor, axes=axes, keep_dims=True)",
            "+        return (tensor - mean) / tf.maximum(x=tf.sqrt(variance), y=util.epsilon)"
        ]
    },
    {
        "number": 2471,
        "comments": "",
        "commit_message": "Fix test.py and remove bbox iou compute for numpy arrays\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ListDataset(Dataset):",
            "if np.random.random() < 0.5:",
            "img, labels = horisontal_flip(img, labels)",
            "",
            "-        boxes = torch.zeros((len(labels), 6))",
            "-        boxes[:, 1:] = labels",
            "+        # Add dummy label if there are none",
            "+        num_labels = 1 if labels is None else len(labels)",
            "+        boxes = torch.zeros((num_labels, 6))",
            "+        if labels is not None:",
            "+            boxes[:, 1:] = labels",
            "",
            "return img_path, img, boxes"
        ]
    },
    {
        "number": 2477,
        "comments": "",
        "commit_message": "AccumGrad supports sparse update (fix #435)\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class AccumGradOptimizer(ProxyOptimizer):",
            "grads_and_vars = FilterNoneGrad().process(grads_and_vars)",
            "vs = []",
            "for g, v in grads_and_vars:",
            "-            assert isinstance(g, tf.Tensor) and isinstance(v, tf.Variable), \\",
            "-                \"AccumGradOptimizer only works for dense update! \" \\",
            "-                \"Types of v and g are {} and {}\".format(type(v), type(g))",
            "+            assert isinstance(g, (tf.Tensor, tf.IndexedSlices)) and isinstance(v, tf.Variable), \\",
            "+                \"AccumGradOptimizer does not work for the gradient of {}! \" \\",
            "+                \"Types of v and g are {} and {}\".format(v.op.name, type(v), type(g))",
            "vs.append(v)",
            "",
            "with tf.control_dependencies(None):"
        ]
    },
    {
        "number": 2488,
        "comments": "",
        "commit_message": "`MultiAggregation` and `aggregation_resolver` (#4749)\n\n* Add MulAggregation and MultiAggregation\n\n* Fix import issue\n\n* Support torch_geometric.nn.aggr package, note: jit errors to fix\n\n* Add tests for MulAggregation, MultiAggregation, aggregation_resolver and message_passing interface\n\n* Formatting\n\n* Fix __repr for gen aggrs\n\n* Move resolver\n\n* Fix test for MulAggregation\n\n* Add test for new mp interface\n\n* Add test for MultiAggregation\n\n* Minor fix\n\n* Add warming for MulAggregation with 'ptr'\n\n* Resolve aggr to Aggregation module, remove aggrs logic\n\n* changelog\n\n* Fix mul aggregation\n\n* update\n\n* update\n\n* update\n\n* update\n\n* reset\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_gen_aggregation(Aggregation, learn):",
            "ptr = torch.tensor([0, 2, 5, 6])",
            "",
            "aggr = Aggregation(learn=learn)",
            "-    assert str(aggr) == f'{Aggregation.__name__}()'",
            "+    assert str(aggr) == f'{Aggregation.__name__}(learn={learn})'",
            "",
            "out = aggr(x, index)",
            "assert out.size() == (3, x.size(1))"
        ]
    },
    {
        "number": 2490,
        "comments": "",
        "commit_message": "Use packaging.version for version comparisons (#2310)\n\n* Use packaging.version for version comparisons\n\nThe distutils package is deprecated\u00b9 and relies on PEP 386\u00b2 version\ncomparisons, which have been superseded by PEP 440\u00b3 which is implemented\nthrough the packaging module.\n\nWith more recent distutils versions, provided through setuptools\nvendoring, we are seeing the following exception during version\ncomparisons:\n\n> TypeError: '<' not supported between instances of 'str' and 'int'\n\nThis is fixed by this migration.\n\n[1] https://docs.python.org/3/library/distutils.html\n[2] https://peps.python.org/pep-0386/\n[3] https://peps.python.org/pep-0440/\n\n* Improve espeak version detection robustness\n\nOn many modern systems espeak is just a symlink to espeak-ng. In that\ncase looking for the 3rd word in the version output will break the\nversion comparison, when it finds `text-to-speech:`, instead of a proper\nversion.\n\nThis will not break during runtime, where espeak-ng would be\nprioritized, but the phonemizer and tokenizer tests force the backend\nto `espeak`, which exhibits this breakage.\n\nThis improves the version detection by simply looking for the version\nafter the \"text-to-speech:\" token.\n\n* Replace distuils.copy_tree with shutil.copytree\n\nThe distutils module is deprecated and slated for removal in Python\n3.12. Its usage should be replaced, in this case by a compatible method\nfrom shutil.\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class InvConvNear(nn.Module):",
            "self.no_jacobian = no_jacobian",
            "self.weight_inv = None",
            "",
            "-        if LooseVersion(torch.__version__) < LooseVersion(\"1.9\"):",
            "+        if Version(torch.__version__) < Version(\"1.9\"):",
            "w_init = torch.qr(torch.FloatTensor(self.num_splits, self.num_splits).normal_())[0]",
            "else:",
            "w_init = torch.linalg.qr(torch.FloatTensor(self.num_splits, self.num_splits).normal_(), \"complete\")[0]"
        ]
    },
    {
        "number": 2491,
        "comments": "",
        "commit_message": "add from homogeneous zero grad test and fix it\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def convert_points_from_homogeneous(",
            "# set the results of division by zeror/near-zero to 1.0",
            "# follow the convention of opencv:",
            "# https://github.com/opencv/opencv/pull/14411/files",
            "+    mask_valid_points = torch.abs(z_vec) > eps",
            "scale: torch.Tensor = torch.where(",
            "-        torch.abs(z_vec) > eps,",
            "-        torch.tensor(1.) / z_vec,",
            "+        mask_valid_points,",
            "+        torch.tensor(1.) / z_vec.masked_fill(~mask_valid_points, eps),",
            "torch.ones_like(z_vec))",
            "",
            "return scale * points[..., :-1]"
        ]
    },
    {
        "number": 2499,
        "comments": "",
        "commit_message": "Traced models serialization and torchscripting fix (#17206)\n\n* Fix torch.jit.script and pickling issues\n\n* Fix get_attr issues\n\n* Fix import in function\n\n* Fix GPT-J and T5 tracing for torch=1.11\n\n* Gate graph surgery on torch version\n\n* Modeling minor changes to enable TorchScripting\n\n* Model serialization / deserialization test\n\n* Remove _assert_is_none users\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class GPTJForSequenceClassification(GPTJPreTrainedModel):",
            "\"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"",
            ")",
            "",
            "-        pooled_logits = logits[torch.arange(batch_size, device=self.device), sequence_lengths]",
            "+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]",
            "",
            "loss = None",
            "if labels is not None:"
        ]
    },
    {
        "number": 2500,
        "comments": "",
        "commit_message": "Fix TFAlbertForSequenceClassification classifier dropout probability. It was set to config.hidden_dropout_prob, but should be config.classifier_dropout_prob. (#3928)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFAlbertForSequenceClassification(TFAlbertPreTrainedModel):",
            "self.num_labels = config.num_labels",
            "",
            "self.albert = TFAlbertMainLayer(config, name=\"albert\")",
            "-        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)",
            "+        self.dropout = tf.keras.layers.Dropout(config.classifier_dropout_prob)",
            "self.classifier = tf.keras.layers.Dense(",
            "config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"",
            ")"
        ]
    },
    {
        "number": 2511,
        "comments": "",
        "commit_message": "Add fp16 support (#963)\n\n* Update imgwarp.py\n\n* update fix\n\n* move _torch_inverse_cast under utils\n\n* fix some tests in pyramid\n\n* fix issues\n\n* implement other autocasting functions\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def get_perspective_transform(src, dst):",
            "], dim=1)",
            "",
            "# solve the system Ax = b",
            "-    X, LU = torch.solve(b, A)",
            "+    X, LU = _torch_solve_cast(b, A)",
            "",
            "# create variable to return",
            "batch_size = src.shape[0]"
        ]
    },
    {
        "number": 2523,
        "comments": "",
        "commit_message": "fixed agents horizon estimation\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Estimator(CircularBuffer):",
            "x=tf.zeros_like(tensor=discounts, dtype=util.tf_dtype(dtype='float')),",
            "y=discounts",
            ")",
            "-            reward = reward + discounts * horizon_estimate",
            "+            reward = reward + discounts * tf.stop_gradient(input=horizon_estimate)",
            "# TODO: stop gradients?",
            "",
            "return reward"
        ]
    },
    {
        "number": 2525,
        "comments": "",
        "commit_message": "[AIR] Fix bug where `TensorflowPredictor.predict` creates extra axis (#25199)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TensorflowPredictor(Predictor):",
            "if feature_columns:",
            "data = data[feature_columns]",
            "data = data.values",
            "-        else:",
            "-            data = data[:, feature_columns]",
            "",
            "tensor = tf.convert_to_tensor(data, dtype=dtype)"
        ]
    },
    {
        "number": 2527,
        "comments": "",
        "commit_message": "ReLU6 rework (#687)\n\n* leaky_relu deprecated and leaky_relu6 and PReLU6Layer added\n\n* cleaning\n\n* doc corrections\n\n* YAPF fix\n\n* leaky_twice_relu6 function added\n\n* recenter initializer\n\n* YAPF correction\n\n* Update test_activations.py\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ZeroPad2d(Layer):",
            "if not isinstance(padding, (int, tuple)):",
            "raise AssertionError(\"Padding should be of type `int` or `tuple`\")",
            "",
            "-        self.outputs = tf.keras.layers.ZeroPadding2D(padding=padding, name=name)(self.inputs)",
            "+        self.outputs = tf.keras.layers.ZeroPadding2D(padding=padding, name=name)(self.inputs)  # TODO: Stop using Keras",
            "+",
            "self._add_layers(self.outputs)"
        ]
    },
    {
        "number": 2530,
        "comments": "",
        "commit_message": "Fixes #3276 (#4116)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Result(Dict):",
            "else:",
            "tbptt_reduce_fx = meta[k]['tbptt_reduce_fx']",
            "",
            "+            if isinstance(value, list):",
            "+                value = torch.tensor(value)",
            "+",
            "if isinstance(value, dict):",
            "# TODO: recursive reduce:",
            "_recursive_fx_apply(value, tbptt_reduce_fx)"
        ]
    },
    {
        "number": 2536,
        "comments": "",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Model(ModelDesc):",
            "if nrpool != 0:  # pool + passthrough if nrpool == 0",
            "x4 = Conv2D('poolproj', x4, nrpool, 1)",
            "outs.append(x4)",
            "-                return tf.concat(3, outs, name='concat')",
            "+                return tf.concat_v2(outs, 3, name='concat')",
            "",
            "with argscope(Conv2D, nl=BNReLU, use_bias=False):",
            "l = Conv2D('conv0', image, 64, 7, stride=2)"
        ]
    },
    {
        "number": 2542,
        "comments": "",
        "commit_message": "fix for unet hijack breaking the train tab\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "th = TorchHijackForUnet()",
            "",
            "# Below are monkey patches to enable upcasting a float16 UNet for float32 sampling",
            "def apply_model(orig_func, self, x_noisy, t, cond, **kwargs):",
            "-    for y in cond.keys():",
            "-        cond[y] = [x.to(devices.dtype_unet) if isinstance(x, torch.Tensor) else x for x in cond[y]]",
            "+",
            "+    if isinstance(cond, dict):",
            "+        for y in cond.keys():",
            "+            cond[y] = [x.to(devices.dtype_unet) if isinstance(x, torch.Tensor) else x for x in cond[y]]",
            "+",
            "with devices.autocast():",
            "return orig_func(self, x_noisy.to(devices.dtype_unet), t.to(devices.dtype_unet), cond, **kwargs).float()"
        ]
    },
    {
        "number": 2552,
        "comments": "",
        "commit_message": "small fix to torch linspace.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def linspace(start, stop, num, axis=None, dev_str=None):",
            "res = [linspace_method(start, stp, num, device=str_to_dev(dev_str)) for stp in stop]",
            "else:",
            "return linspace_method(start, stop, num, device=str_to_dev(dev_str))",
            "-    res = _torch.cat(res, -1).reshape(start_shape + [num])",
            "+    res = _torch.cat(res, -1).reshape(sos_shape + [num])",
            "if axis is not None:",
            "res = _torch.transpose(res, axis, -1)",
            "return res.to(str_to_dev(dev_str))"
        ]
    },
    {
        "number": 2563,
        "comments": "",
        "commit_message": "More cuda fixes (#379)\n\nDefault use_cuda=torch.Tensor.is_cuda in irange and friends\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class GaussianChainTests(TestCase):",
            "(self.N, reparameterized, n_repa_nodes, self.N))",
            "if self.N < 0:",
            "def array_to_string(y):",
            "-                    return str(map(lambda x: \"%.3f\" % x.data.numpy()[0], y))",
            "+                    return str(map(lambda x: \"%.3f\" % x.data.cpu().numpy()[0], y))",
            "",
            "print(\"lambdas: \" + array_to_string(self.lambdas))",
            "print(\"target_mus: \" + array_to_string(self.target_mus[1:]))"
        ]
    },
    {
        "number": 2564,
        "comments": "",
        "commit_message": "Fix two default args in DenseNet blocks... fix #1427\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DenseBlock(nn.ModuleDict):",
            "",
            "",
            "class DenseTransition(nn.Sequential):",
            "-    def __init__(self, num_input_features, num_output_features, norm_layer=nn.BatchNorm2d, aa_layer=None):",
            "+    def __init__(self, num_input_features, num_output_features, norm_layer=BatchNormAct2d, aa_layer=None):",
            "super(DenseTransition, self).__init__()",
            "self.add_module('norm', norm_layer(num_input_features))",
            "self.add_module('conv', nn.Conv2d("
        ]
    },
    {
        "number": 2565,
        "comments": "",
        "commit_message": "fix tests\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PANConv(MessagePassing):",
            "",
            "tmp = SparseTensor.eye(adj_t.size(0), adj_t.size(1), has_value=True,",
            "dtype=dtype, device=adj_t.device())",
            "-        tmp = tmp.mul_nnz(self.weight[0])",
            "+        tmp = tmp.mul_nnz(self.weight[0], layout='coo')",
            "",
            "outs = [tmp]",
            "for i in range(1, self.filter_size + 1):",
            "tmp = tmp @ adj_t",
            "-            tmp = tmp.mul_nnz(self.weight[i])",
            "+            tmp = tmp.mul_nnz(self.weight[i], layout='coo')",
            "outs += [tmp]",
            "",
            "row = torch.cat([out.storage.row() for out in outs], dim=0)"
        ]
    },
    {
        "number": 2568,
        "comments": "",
        "commit_message": "fix(ci): onnxmlir, spacy test cases (#1819)\n\n* fix(ci): onnxmlir and tensorflow test cases\n\n* fix: pycaret numba deps\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class NativeModel(tf.Module):",
            "self.dense = lambda inputs: tf.matmul(inputs, self.weights)",
            "",
            "@tf.function(",
            "-        input_signature=[tf.TensorSpec(shape=None, dtype=tf.float64, name=\"inputs\")]",
            "+        input_signature=[tf.TensorSpec(shape=[1, 5], dtype=tf.float64, name=\"inputs\")]",
            ")",
            "def __call__(self, inputs):",
            "return self.dense(inputs)"
        ]
    },
    {
        "number": 2570,
        "comments": "",
        "commit_message": "Bugfix: support both Keras and TF-Keras optimizers in hvd.load_model (#652)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def load_model(keras, wrap_optimizer, filepath, custom_optimizers, custom_object",
            "horovod_objects = {",
            "subclass.__name__.lower(): wrap_optimizer(subclass)",
            "for subclass in keras.optimizers.Optimizer.__subclasses__()",
            "-        if subclass.__module__ == 'keras.optimizers'",
            "+        if subclass.__module__ == keras.optimizers.Optimizer.__module__",
            "}",
            "",
            "if custom_optimizers is not None:"
        ]
    },
    {
        "number": 2575,
        "comments": "",
        "commit_message": "support DeConv1dLayer, fix bugs and refine docs\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BinaryConv2d(Layer):",
            "name=self.name",
            ")",
            "if self.b_init:",
            "-            outputs = tf.nn.bias_add(outputs, self.b, name='bias_add')",
            "+            outputs = tf.nn.bias_add(outputs, self.b, data_format=self.data_format, name='bias_add')",
            "if self.act:",
            "outputs = self.act(outputs)",
            "return outputs"
        ]
    },
    {
        "number": 2585,
        "comments": "",
        "commit_message": "Fix compressor op_types (#1670)\n\n* fix compressor op_types\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Quantizer(Compressor):",
            "Base quantizer for pytorch quantizer",
            "\"\"\"",
            "",
            "-    def __call__(self, model):",
            "-        self.compress(model)",
            "-        return model",
            "-",
            "def quantize_weight(self, weight, config, op, op_type, op_name):",
            "\"\"\"user should know where dequantize goes and implement it in quantize method",
            "we now do not provide dequantize method"
        ]
    },
    {
        "number": 2589,
        "comments": "",
        "commit_message": "Fix SEW-D implementation differences (#14191)\n\n* Fix SEW-D\n\n* Update tests\n\n* isort\n",
        "label": "",
        "answer": "yes",
        "change": [
            "SEWD_INPUTS_DOCSTRING = r\"\"\"",
            "\"The bare SEW-D Model transformer outputting raw hidden-states without any specific head on top.\",",
            "SEWD_START_DOCSTRING,",
            ")",
            "-# Copied from transformers.models.sew.modeling_sew.SEWModel with SEW->SEWD",
            "+# Copied from transformers.models.sew.modeling_sew.SEWModel with SEW->SEWD, layer_norm_eps->feature_layer_norm_eps",
            "class SEWDModel(SEWDPreTrainedModel):",
            "def __init__(self, config: SEWDConfig):",
            "super().__init__(config)",
            "self.config = config",
            "self.feature_extractor = SEWDFeatureExtractor(config)",
            "-        self.layer_norm = nn.LayerNorm(config.conv_dim[-1], eps=config.layer_norm_eps)",
            "+        self.layer_norm = nn.LayerNorm(config.conv_dim[-1], eps=config.feature_layer_norm_eps)",
            "",
            "self.project_features = config.conv_dim[-1] != config.hidden_size",
            "if self.project_features:"
        ]
    },
    {
        "number": 2592,
        "comments": "",
        "commit_message": "Fixing and error related to Floor Division (#4221)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes https://github.com/pytorch/fairseq/issues/4058\nWhile using the library the following warnings are shown which sometimes hinder the workflow. The warnings are\n\n`<USER_PATH>/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n  beams_buf = indices_buf // vocab_size`\n\n`<USER_PATH>/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n  unfin_idx = bbsz_idx // beam_size`\n\nThe methodology was simple, instead of using the `//`, it was replaced by `torch.div(arg1, arg2, rounding_mode='trunc')` and the variable alues do not change for both before and after, just the warning is resolved.\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\nYes, I did! Thanks!\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/4221\n\nReviewed By: arbabu123\n\nDifferential Revision: D34538147\n\nPulled By: alexeib\n\nfbshipit-source-id: 143897a249129a163b6a30ba9b5cf5595ef42330\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class SequenceGenerator(nn.Module):",
            "cum_unfin.append(prev)",
            "cum_fin_tensor = torch.tensor(cum_unfin, dtype=torch.int).to(bbsz_idx)",
            "",
            "-        unfin_idx = bbsz_idx // beam_size",
            "+        unfin_idx = torch.div(bbsz_idx, beam_size, rounding_mode='trunc')",
            "sent = unfin_idx + torch.index_select(cum_fin_tensor, 0, unfin_idx)",
            "",
            "# Create a set of \"{sent}{unfin_idx}\", where"
        ]
    },
    {
        "number": 2593,
        "comments": "",
        "commit_message": "Creation functions backend test fixes (#2373)\n\nFixes from_dlpack() and logspace() from failing unit tests\n",
        "label": "",
        "answer": "yes",
        "change": [
            "eye.unsupported_dtypes = (\"uint16\",)",
            "def from_dlpack(",
            "x: Union[tf.Tensor, tf.Variable], *, out: Union[tf.Tensor, tf.Variable] = None",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    return tf.experimental.dlpack.from_dlpack(x)",
            "+    dlcapsule = tf.experimental.dlpack.to_dlpack(x)",
            "+    return tf.experimental.dlpack.from_dlpack(dlcapsule)",
            "",
            "",
            "def full("
        ]
    },
    {
        "number": 2598,
        "comments": "",
        "commit_message": "[Feat] improve testing framework (#560)\n\n* Parametrize device and dtype fixtures using CLI options\n\n* improve tests parametrise n MakeFile\n\n* add coverage tot tests and update ci config file\n\n* test pep8 using pytest-flak8\n\n* add pytest-mypy, update failing tests and remove verify script\n\n* update build-docs command in MakeFile\n\n* fix performance test issue with non cuda device\n\n* add pydocstyle, adapt normalisation module to pydocsyle\n\n* fix augmentation  docs rendering\n\nCo-authored-by: Aiden Nibali <dismaldenizen@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TestRandomRotation:",
            "torch.manual_seed(0)  # for random reproductibility",
            "",
            "@torch.jit.script",
            "-        def op_script(data: torch.Tensor) -> torch.Tensor:",
            "-",
            "+        def op_script(data: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:",
            "return kornia.random_rotation(data, degrees=45.0)",
            "",
            "input = torch.tensor([[1., 0., 0., 2.],"
        ]
    },
    {
        "number": 2606,
        "comments": "",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_conditional_whiten(Xnew, X, kernel, f_loc, f_scale_tril, loc, cov):",
            "loc0, cov0 = conditional(Xnew, X, kernel, f_loc, f_scale_tril, full_cov=True,",
            "whiten=False)",
            "Kff = kernel(X) + torch.eye(3) * 1e-6",
            "-    Lff = Kff.cholesky()",
            "+    Lff = torch.linalg.cholesky(Kff)",
            "whiten_f_loc = Lff.inverse().matmul(f_loc)",
            "whiten_f_scale_tril = Lff.inverse().matmul(f_scale_tril)",
            "loc1, cov1 = conditional(Xnew, X, kernel, whiten_f_loc, whiten_f_scale_tril,"
        ]
    },
    {
        "number": 2627,
        "comments": "",
        "commit_message": "Fix Accuracy metric (#186)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def accuracy(pr, gt, threshold=0.5, ignore_channels=None):",
            "pr = _threshold(pr, threshold=threshold)",
            "pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)",
            "",
            "-    tp = torch.sum(gt == pr)",
            "+    tp = torch.sum(gt == pr, dtype=pr.dtype)",
            "score = tp / gt.view(-1).shape[0]",
            "return score"
        ]
    },
    {
        "number": 2628,
        "comments": "",
        "commit_message": "Function contains unused argument (#1240)\n\n* Function contains unused argument\n\n* fix linter\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def unsharp_mask(",
            ">>> output.shape",
            "torch.Size([2, 4, 5, 5])",
            "\"\"\"",
            "-    data_blur: torch.Tensor = gaussian_blur2d(input, kernel_size, sigma)",
            "+    data_blur: torch.Tensor = gaussian_blur2d(input, kernel_size, sigma, border_type)",
            "data_sharpened: torch.Tensor = input + (input - data_blur)",
            "return data_sharpened"
        ]
    },
    {
        "number": 2629,
        "comments": "",
        "commit_message": "Drop torch 1.6 testing (#10390)\n\n* Drop torch 1.6 support\n\n* Drop 1.6 support\n\n* Update CHANGELOG\n\n* Fixes\n\n* Split change\n\n* Undo change\n\n* 1.7 -> 1.7.1\n\nhttps://github.com/pytorch/pytorch/issues/47354\n\n* Force trigger nightly\n\n* Update .github/workflows/events-nightly.yml\n\nCo-authored-by: Aki Nitta <nitta@akihironitta.com>\n\n* Revert 1.7.1 change - try wildcard\n\n* Update adjust versions and test it\n\n* Undo test changes\n\n* Revert \"Undo test changes\"\n\nThis reverts commit 3a6acadd115e86f02d83a788f1978372ab6764f3.\n\n* Update CHANGELOG.md\n\nCo-authored-by: Aki Nitta <nitta@akihironitta.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def pl_worker_init_function(worker_id: int, rank: Optional[int] = None) -> None:",
            "np.random.seed(ss.generate_state(4))",
            "# Spawn distinct SeedSequences for the PyTorch PRNG and the stdlib random module",
            "torch_ss, stdlib_ss = ss.spawn(2)",
            "-    # PyTorch 1.7 and above takes a 64-bit seed",
            "-    dtype = np.uint64 if _TORCH_GREATER_EQUAL_1_7 else np.uint32",
            "-    torch.manual_seed(torch_ss.generate_state(1, dtype=dtype)[0])",
            "+    torch.manual_seed(torch_ss.generate_state(1, dtype=np.uint64)[0])",
            "# use 128 bits expressed as an integer",
            "stdlib_seed = (stdlib_ss.generate_state(2, dtype=np.uint64).astype(object) * [1 << 64, 1]).sum()",
            "random.seed(stdlib_seed)"
        ]
    },
    {
        "number": 2633,
        "comments": "",
        "commit_message": "fixing size mismatch\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Tacotron(nn.Module):",
            "self.encoder = Encoder(embedding_dim)",
            "self.decoder = Decoder(256, mel_dim, r)",
            "self.postnet = PostCBHG(mel_dim)",
            "-        self.last_linear = nn.Linear(256, linear_dim)",
            "+        self.last_linear = nn.Linear(self.postnet.cbhg.gru_features * 2, linear_dim)",
            "",
            "def forward(self, characters, mel_specs=None, mask=None):",
            "B = characters.size(0)"
        ]
    },
    {
        "number": 2638,
        "comments": "",
        "commit_message": "fixes and clean-up\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Conv2dSubsampling(torch.nn.Module):",
            "if self.output is not None:",
            "sequence = self.output(sequence)",
            "",
            "-        if mask is not None:",
            "-            return sequence, self.create_new_mask(mask)",
            "-",
            "-        return sequence, None",
            "+        return sequence, self.create_new_mask(mask)",
            "",
            "def create_new_conformer_mask(self, mask: torch.Tensor) -> torch.Tensor:",
            "\"\"\"Create new conformer mask for output sequences."
        ]
    },
    {
        "number": 2645,
        "comments": "",
        "commit_message": "Fix network for TF 0.10\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class WaveNet(object):",
            "shifted = tf.slice(encoded, [0, 0, 1, 0], [-1, -1, tf.shape(encoded)[2] - 1, -1])",
            "shifted = tf.pad(shifted, [[0, 0], [0, 0], [0, 1], [0, 0]])",
            "",
            "-            loss = tf.nn.softmax_cross_entropy_with_logits(raw_output, tf.reshape(shifted, [-1, self.channels]))",
            "+            prediction = tf.reshape(raw_output, [-1, self.channels])",
            "+            loss = tf.nn.softmax_cross_entropy_with_logits(prediction, tf.reshape(shifted, [-1, self.channels]))",
            "reduced_loss =  tf.reduce_mean(loss)",
            "",
            "tf.scalar_summary('loss', reduced_loss)"
        ]
    },
    {
        "number": 2646,
        "comments": "",
        "commit_message": "Test github actions for CI (#2531)\n\n* Test github actions for CI\n\n* Update ci.yml\n\n* Update ci.yml\n\n* Update ci.yml\n\n* remove .travis.yml; add ninja\n\n* modify readme; add coveralls\n\n* fix multiple dep in finish\n\n* change to coveralls token\n\n* attempt to fix coverall agg\n\n* add coverage badge\n\n* update python, pytorch versions\n\n* Add license for lint\n\n* update wheel, setuptools\n\n* run on ubuntu 20.04\n\n* add python headers\n\n* debug issues with build\n\n* use python3-dev\n\n* debug by adding numpy\n\n* install cibuildwheel\n\n* comment out dependency no lap\n\n* Install gcc-8 and g++8; reenable lap dependency\n\n* Install g++ during docs stage\n\n* Install gcc before using pip\n\n* update torch dep for funsor\n\n* Attempt to fix test_minipyro.py\n\nCo-authored-by: Neeraj Pradhan <neerajprad@fb.com>\nCo-authored-by: Fritz Obermeyer <fritz.obermeyer@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def elbo_test_case(backend, jit, expected_elbo, data, steps=None):",
            "if backend == \"pyro\":",
            "# TODO: this is a difference between the two implementations",
            "elbo = elbo.loss",
            "-        assert elbo(constrained_model, guide_constrained_model, data) == approx(expected_elbo, rel=0.1)",
            "+        with torch.no_grad():",
            "+            actual = elbo(constrained_model, guide_constrained_model, data)",
            "+        assert actual == approx(expected_elbo, rel=0.1)"
        ]
    },
    {
        "number": 2647,
        "comments": "",
        "commit_message": "Fix #954 by bringing traceable _assert into timm to allow compat w/ PyTorch < 1.8\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PatchEmbed(nn.Module):",
            "",
            "def forward(self, x):",
            "B, C, H, W = x.shape",
            "-        torch._assert(H == self.img_size[0], f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\")",
            "-        torch._assert(W == self.img_size[1], f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\")",
            "+        _assert(H == self.img_size[0], f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\")",
            "+        _assert(W == self.img_size[1], f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\")",
            "x = self.proj(x)",
            "if self.flatten:",
            "x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC"
        ]
    },
    {
        "number": 2648,
        "comments": "",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Layer_Pooling_Test(CustomTestCase):",
            "cls.network = tl.layers.DenseLayer(cls.network, 800, tf.nn.relu, name='relu2')",
            "cls.network = tl.layers.DropoutLayer(cls.network, keep=0.5, name='drop3')",
            "",
            "-        cls.network = tl.layers.DenseLayer(cls.network, n_units=10, act=tf.identity, name='output')",
            "+        cls.network = tl.layers.DenseLayer(cls.network, n_units=10, name='output')",
            "",
            "# define cost function and metric.",
            "cls.y = cls.network.outputs"
        ]
    },
    {
        "number": 2650,
        "comments": "",
        "commit_message": "coarse rasterization bug fix\n\nSummary:\nFix a bug which resulted in a rendering artifacts if the image size was not a multiple of 16.\nFix: Revert coarse rasterization to original implementation and only update fine rasterization to reverse the ordering of Y and X axis. This is much simpler than the previous approach!\n\nAdditional changes:\n- updated mesh rendering end-end tests to check outputs from both naive and coarse to fine rasterization.\n- added pointcloud rendering end-end tests\n\nReviewed By: gkioxari\n\nDifferential Revision: D21102725\n\nfbshipit-source-id: 2e7e1b013dd6dd12b3a00b79eb8167deddb2e89a\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TestRasterizeMeshes(TestCaseMixin, unittest.TestCase):",
            "bin_size,",
            "max_faces_per_bin,",
            ")",
            "-        # Flip x and y axis of output before comparing to expected",
            "+",
            "bin_faces_same = (bin_faces.squeeze() == bin_faces_expected).all()",
            "self.assertTrue(bin_faces_same.item() == 1)"
        ]
    },
    {
        "number": 2652,
        "comments": "",
        "commit_message": "Reproducibility 3/3 (#1924)\n\n* make tests deterministic\n\n* run slow tests\n\n* prepare for testing\n\n* finish\n\n* refactor\n\n* add print statements\n\n* finish more\n\n* correct some test failures\n\n* more fixes\n\n* set up to correct tests\n\n* more corrections\n\n* up\n\n* fix more\n\n* more prints\n\n* add\n\n* up\n\n* up\n\n* up\n\n* uP\n\n* uP\n\n* more fixes\n\n* uP\n\n* up\n\n* up\n\n* up\n\n* up\n\n* fix more\n\n* up\n\n* up\n\n* clean tests\n\n* up\n\n* up\n\n* up\n\n* more fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* make\n\n* correct\n\n* finish\n\n* finish\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class LDMTextToImagePipelineNightlyTests(unittest.TestCase):",
            "torch.cuda.empty_cache()",
            "",
            "def get_inputs(self, device, dtype=torch.float32, seed=0):",
            "-        generator = torch.Generator(device=device).manual_seed(seed)",
            "+        generator = torch.manual_seed(seed)",
            "latents = np.random.RandomState(seed).standard_normal((1, 4, 32, 32))",
            "latents = torch.from_numpy(latents).to(device=device, dtype=dtype)",
            "inputs = {"
        ]
    },
    {
        "number": 2654,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Entropy(Metric):",
            "def __call__(",
            "self,  # type: ignore",
            "logits: torch.Tensor,",
            "-        mask: Optional[torch.Tensor] = None,",
            "+        mask: Optional[torch.BoolTensor] = None,",
            "):",
            "\"\"\"",
            "# Parameters",
            "",
            "logits : `torch.Tensor`, required.",
            "A tensor of unnormalized log probabilities of shape (batch_size, ..., num_classes).",
            "-        mask : `torch.Tensor`, optional (default = None).",
            "+        mask : `torch.BoolTensor`, optional (default = None).",
            "A masking tensor of shape (batch_size, ...).",
            "\"\"\"",
            "logits, mask = self.detach_tensors(logits, mask)",
            "",
            "if mask is None:",
            "-            mask = torch.ones(logits.size()[:-1], device=logits.device)",
            "+            mask = torch.ones(logits.size()[:-1], device=logits.device).bool()",
            "",
            "log_probs = torch.nn.functional.log_softmax(logits, dim=-1)",
            "probabilities = torch.exp(log_probs) * mask.unsqueeze(-1)"
        ]
    },
    {
        "number": 2660,
        "comments": "",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DeepSpeedTransformerInference(nn.Module):",
            "merge_count,",
            "mlp_extra_grouping)",
            "",
            "-        device = torch.cuda.current_device()  #if config.bigscience_bloom else 'cpu'",
            "+        device = get_accelerator().current_device_name(",
            "+        )  # if config.bigscience_bloom else 'cpu'",
            "self.norm_w = nn.Parameter(torch.empty(self.config.hidden_size,",
            "dtype=data_type,",
            "device=device),"
        ]
    },
    {
        "number": 2662,
        "comments": "",
        "commit_message": "clean up, fixes and refactor\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def get_transducer_task_io(",
            "encoder_out_lens = list(map(int, encoder_out_lens))",
            "",
            "t_len = torch.IntTensor(encoder_out_lens).to(device)",
            "-    u_len = torch.IntTensor([y.size(0) for y in ys]).to(device)",
            "+    u_len = torch.IntTensor([y.size(0) for y in labels_unpad]).to(device)",
            "",
            "-    return target, t_len, u_len",
            "+    return decoder_in, target, t_len, u_len"
        ]
    },
    {
        "number": 2664,
        "comments": "",
        "commit_message": "Fix a few issues loading pretrained vit/bit npz weights w/ num_classes=0 __init__ arg. Missed a few other small classifier handling detail on Mlp, GhostNet, Levit. Should fix #713\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def _load_weights(model: nn.Module, checkpoint_path: str, prefix: str = 'resnet/",
            "model.stem.conv.weight.copy_(stem_conv_w)",
            "model.norm.weight.copy_(t2p(weights[f'{prefix}group_norm/gamma']))",
            "model.norm.bias.copy_(t2p(weights[f'{prefix}group_norm/beta']))",
            "-    if model.head.fc.weight.shape[0] == weights[f'{prefix}head/conv2d/kernel'].shape[-1]:",
            "+    if isinstance(model.head.fc, nn.Conv2d) and \\",
            "+            model.head.fc.weight.shape[0] == weights[f'{prefix}head/conv2d/kernel'].shape[-1]:",
            "model.head.fc.weight.copy_(t2p(weights[f'{prefix}head/conv2d/kernel']))",
            "model.head.fc.bias.copy_(t2p(weights[f'{prefix}head/conv2d/bias']))",
            "for i, (sname, stage) in enumerate(model.stages.named_children()):"
        ]
    },
    {
        "number": 2673,
        "comments": "",
        "commit_message": "fix internal index.Tensor test on wrong device\n\nSummary: After landing https://github.com/pytorch/pytorch/pull/69607, that made it an error to use indexing with `cpu_tensor[cuda_indices]`. There was one outstanding test in fbcode that incorrectly used indexing in that way, which is fixed here\n\nReviewed By: bottler, osalpekar\n\nDifferential Revision: D37128838\n\nfbshipit-source-id: 611b6f717b5b5d89fa61fd9ebeb513ad7e65a656\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TestTexturesVertex(TestCaseMixin, unittest.TestCase):",
            ")",
            "",
            "# define TexturesVertex",
            "-        verts_texture = torch.rand(verts.shape)",
            "+        verts_texture = torch.rand(verts.shape, device=device)",
            "textures = TexturesVertex(verts_features=verts_texture)",
            "",
            "# compute packed faces"
        ]
    },
    {
        "number": 2674,
        "comments": "",
        "commit_message": "TorchSTFT to device fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TorchSTFT():",
            "center=True,",
            "pad_mode=\"reflect\",  # compatible with audio.py",
            "normalized=False,",
            "-                       onesided=True)",
            "+                       onesided=True,",
            "+                       return_complex=False)",
            "M = o[:, :, :, 0]",
            "P = o[:, :, :, 1]",
            "return torch.sqrt(torch.clamp(M ** 2 + P ** 2, min=1e-8))"
        ]
    },
    {
        "number": 2701,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class SpanField(Field[torch.Tensor]):",
            "@overrides",
            "def as_tensor(self,",
            "padding_lengths: Dict[str, int],",
            "-                  cuda_device: int = -1,",
            "-                  for_training: bool = True) -> torch.Tensor:",
            "+                  cuda_device: int = -1) -> torch.Tensor:",
            "# pylint: disable=unused-argument",
            "-        tensor = Variable(torch.LongTensor([self.span_start, self.span_end]), volatile=not for_training)",
            "+        tensor = torch.LongTensor([self.span_start, self.span_end])",
            "return tensor if cuda_device == -1 else tensor.cuda(cuda_device)",
            "",
            "@overrides"
        ]
    },
    {
        "number": 2711,
        "comments": "",
        "commit_message": "fix: check dtype when loading models (#872)\n\n* fix: check dtype when loading models\n\n* fix: black\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def load_openai_model(",
            "",
            "# model from OpenAI state dict is in manually cast fp16 mode, must be converted for AMP/fp32/bf16 use",
            "model = model.to(device)",
            "-        if dtype == torch.float32 or dtype.startswith('amp'):",
            "+        if dtype == torch.float32 or (",
            "+            isinstance(dtype, str) and dtype.startswith('amp')",
            "+        ):",
            "model.float()",
            "elif dtype == torch.bfloat16:",
            "convert_weights_to_lp(model, dtype=torch.bfloat16)"
        ]
    },
    {
        "number": 2716,
        "comments": "",
        "commit_message": "Update to PyTorch 1.11.0 (#3045)\n\n* Run black\n\n* Fix ProvenanceTensor\n\n* Replace torch.triangular_solve -> torch.linalg.solve_triangular\n\n* More fixes to torch.linalg.solve_triangular\n\n* Bump torch version\n\n* Bump Python version 3.6 -> 3.7\n\n* Fix some tests\n\n* Decrease tolerance on stable tests\n\n* Remove obsolete xfail\n\n* Resolve #3032\n\n* Fix solve_triangular in ops.gaussian, revert test weakening\n\n* Fix catching of singular matrices in hmc\n\n* xfail funsor tests\n\n* Work around pandas 1.3 bug\n\n* Allow mypy to install missing stubs\n\n* Clarify triangular_solve test\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def _triu_inverse(x):",
            "return x.reciprocal()",
            "else:",
            "identity = torch.eye(x.size(-1), dtype=x.dtype, device=x.device)",
            "-        return torch.triangular_solve(identity, x, upper=True)[0]",
            "+        return torch.linalg.solve_triangular(x, identity, upper=True)",
            "",
            "",
            "class BlockMassMatrix:"
        ]
    },
    {
        "number": 2723,
        "comments": "",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario \u0160a\u0161ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Newsroom(datasets.GeneratorBasedBuilder):",
            "data_dir = os.path.abspath(os.path.expanduser(dl_manager.manual_dir))",
            "if not os.path.exists(data_dir):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('newsroom', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {}\".format(",
            "-                    data_dir, self.manual_download_instructions",
            "-                )",
            "+                f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('newsroom', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ]
    },
    {
        "number": 2727,
        "comments": "",
        "commit_message": "Small fix at apply_to_network\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DomainClient(Client):",
            "",
            "return response",
            "",
            "-    def apply_to_network(self, target: str, reason: str):",
            "+    def apply_to_network(self,",
            "+            target: str,",
            "+            reason: str,",
            "+            route_index: int = 0):",
            "self.association.create(",
            "target=target,",
            "-            sender=self.conn.base_url.replace(\"/api/v1\", \"\"),",
            "+            sender=self.routes[route_index].connection.base_url.replace(\"/api/v1\", \"\"),",
            "reason=reason,",
            "node_name=self.name,",
            ")"
        ]
    },
    {
        "number": 2728,
        "comments": "",
        "commit_message": "Fix OOM while loading best model state (#3506)\n\nFixes #3499\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Checkpointer(Registrable):",
            "if self._serialization_dir:",
            "logger.info(\"loading best weights\")",
            "best_model_state_path = os.path.join(self._serialization_dir, \"best.th\")",
            "-            return torch.load(best_model_state_path)",
            "+            return torch.load(best_model_state_path, map_location=nn_util.device_mapping(-1))",
            "else:",
            "logger.info(",
            "\"cannot load best weights without `serialization_dir`, \""
        ]
    },
    {
        "number": 2733,
        "comments": "",
        "commit_message": "misc fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def class_balanced_sigmoid_cross_entropy(logits, label, name='cross_entropy_loss",
            ":param label: size: the ground truth in {0,1}, of the same shape as logits.",
            ":returns: a scalar. class-balanced cross entropy loss",
            "\"\"\"",
            "-    z = batch_flatten(logits)",
            "-    y = tf.cast(batch_flatten(label), tf.float32)",
            "+    y = tf.cast(label, tf.float32)",
            "",
            "count_neg = tf.reduce_sum(1. - y)",
            "count_pos = tf.reduce_sum(y)",
            "beta = count_neg / (count_neg + count_pos)",
            "",
            "pos_weight = beta / (1 - beta)",
            "-    cost = tf.nn.weighted_cross_entropy_with_logits(z, y, pos_weight)",
            "+    cost = tf.nn.weighted_cross_entropy_with_logits(logits, y, pos_weight)",
            "cost = tf.reduce_mean(cost * (1 - beta), name=name)",
            "",
            "#logstable = tf.log(1 + tf.exp(-tf.abs(z)))"
        ]
    },
    {
        "number": 2738,
        "comments": "",
        "commit_message": "fix colorspace bug & support multi-gpu and multi-processing (#312)\n\n* fix colorspace bug of ffmpeg stream, add multi-gpu and multi-processing suport for inference_realesrgan_video.py\n\n* fix code format\n\nCo-authored-by: yanzewu <yanzewu@tencent.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class RealESRGANer():",
            "self.half = half",
            "",
            "# initialize model",
            "-        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
            "+        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else device",
            "# if the model_path starts with https, it will first download models to the folder: realesrgan/weights",
            "if model_path.startswith('https://'):",
            "model_path = load_file_from_url("
        ]
    },
    {
        "number": 2741,
        "comments": "",
        "commit_message": "GH-1660: fix mean pooling in PooledFlairEmbeddings\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PooledFlairEmbeddings(TokenEmbeddings):",
            "",
            "# set aggregation operation",
            "if self.pooling == \"mean\":",
            "-                                aggregated_embedding = torch.mean(self.word_embeddings[token.text], local_embedding)",
            "+                                aggregated_embedding = torch.add(self.word_embeddings[token.text], local_embedding)",
            "elif self.pooling == \"fade\":",
            "aggregated_embedding = torch.add(self.word_embeddings[token.text], local_embedding)",
            "aggregated_embedding /= 2"
        ]
    },
    {
        "number": 2743,
        "comments": "",
        "commit_message": "Fix cafferesnet101 (fc -> last_linear)\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ResNet(nn.Module):",
            "",
            "x = self.avgpool(x)",
            "x = x.view(x.size(0), -1)",
            "-    x = self.fc(x)",
            "+    x = self.last_linear(x)",
            "",
            "return x"
        ]
    },
    {
        "number": 2769,
        "comments": "",
        "commit_message": "bug fix in hed\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Model(ModelDesc):",
            "for idx, b in enumerate([b1, b2, b3, b4, b5, final_map]):",
            "output = tf.nn.sigmoid(b, name='output{}'.format(idx+1))",
            "xentropy = class_balanced_sigmoid_cross_entropy(",
            "-                tf.squeeze(b, [3]), edgemap,",
            "+                b, edgemap,",
            "name='xentropy{}'.format(idx+1))",
            "costs.append(xentropy)"
        ]
    },
    {
        "number": 2771,
        "comments": "",
        "commit_message": "fix mypy error\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def focal_loss(",
            "device=input.device, dtype=input.dtype)",
            "",
            "# compute the actual focal loss",
            "-    weight = torch.pow(1. - input_soft, gamma)",
            "+    weight = torch.pow(-input_soft + 1., gamma)",
            "",
            "focal = -alpha * weight * torch.log(input_soft)",
            "loss_tmp = torch.sum(target_one_hot * focal, dim=1)"
        ]
    },
    {
        "number": 2772,
        "comments": "",
        "commit_message": "Fix integration slow tests (#10670)\n\n* PoC\n\n* Fix slow tests for the PT1.8 Embedding problem\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class SqueezeBertModelIntegrationTest(unittest.TestCase):",
            "def test_inference_classification_head(self):",
            "model = SqueezeBertForSequenceClassification.from_pretrained(\"squeezebert/squeezebert-mnli\")",
            "",
            "-        input_ids = torch.tensor([[0, 29414, 232, 328, 740, 1140, 12695, 69, 13, 1588, 2]])",
            "+        input_ids = torch.tensor([[1, 29414, 232, 328, 740, 1140, 12695, 69, 13, 1588, 2]])",
            "output = model(input_ids)[0]",
            "expected_shape = torch.Size((1, 3))",
            "self.assertEqual(output.shape, expected_shape)",
            "-        expected_tensor = torch.tensor([[0.5075, 0.0682, -0.5881]])",
            "+        expected_tensor = torch.tensor([[0.6401, -0.0349, -0.6041]])",
            "self.assertTrue(torch.allclose(output, expected_tensor, atol=1e-4))"
        ]
    },
    {
        "number": 2775,
        "comments": "",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Model(ModelDesc):",
            "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
            "cost = tf.reduce_mean(cost, name='cross_entropy_loss')",
            "",
            "-        wrong = tf.to_float(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), name='incorrect_vector')",
            "+        wrong = tf.cast(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), tf.float32, name='incorrect_vector')",
            "summary.add_moving_summary(tf.reduce_mean(wrong, name='train_error'))",
            "",
            "wd_cost = tf.multiply(1e-5, regularize_cost('fc.*/W', tf.nn.l2_loss),"
        ]
    },
    {
        "number": 2783,
        "comments": "",
        "commit_message": "Pytorch v0.4 compatibility and minor fixes\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CNNLayerVisualization():",
            "self.conv_output = x[0, self.selected_filter]",
            "# Loss function is the mean of the output of the selected layer/filter",
            "# We try to minimize the mean of the output of that specific filter",
            "-            loss = torch.mean(self.conv_output)",
            "-            print('Iteration:', str(i), 'Loss:', \"{0:.2f}\".format(loss.data.numpy()[0]))",
            "+            loss = -torch.mean(self.conv_output)",
            "+            print('Iteration:', str(i), 'Loss:', \"{0:.2f}\".format(loss.data.numpy()))",
            "# Backward",
            "loss.backward()",
            "# Update image"
        ]
    },
    {
        "number": 2785,
        "comments": "",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MultiHeadAttention(nn.Module):",
            "q = q / math.sqrt(dim_per_head)  # (bs, n_heads, qlen, dim_per_head)",
            "scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, qlen, klen)",
            "mask = (mask == 0).view(mask_reshape).expand_as(scores)  # (bs, n_heads, qlen, klen)",
            "-        scores.masked_fill_(mask, -float(\"inf\"))  # (bs, n_heads, qlen, klen)",
            "+        scores.masked_fill_(mask, torch.finfo(scores.dtype).min)  # (bs, n_heads, qlen, klen)",
            "",
            "weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)  # (bs, n_heads, qlen, klen)",
            "weights = nn.functional.dropout(weights, p=self.dropout, training=self.training)  # (bs, n_heads, qlen, klen)"
        ]
    },
    {
        "number": 2787,
        "comments": "",
        "commit_message": "fix mean_iou documentation issue\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def mean_iou(",
            "# iterate over classes",
            "for class_id in range(num_classes):",
            "tp: torch.Tensor = conf_mat[..., None, class_id, class_id]",
            "-        total = torch.sum(conf_mat[..., class_id, :], dim=-1, keepdim=True) + \\",
            "+        total: torch.Tensor = \\",
            "+            torch.sum(conf_mat[..., class_id, :], dim=-1, keepdim=True) + \\",
            "torch.sum(conf_mat[..., :, class_id], dim=-1, keepdim=True)",
            "iou_val: torch.Tensor = tp / (total.float() - tp + 1e-6)",
            "ious[..., class_id:class_id + 1] += iou_val"
        ]
    },
    {
        "number": 2791,
        "comments": "",
        "commit_message": "fixing run_generation\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def main():",
            "if requires_preprocessing:",
            "prepare_input = PREPROCESSING_FUNCTIONS.get(args.model_type)",
            "prompt_text, model_kwargs = prepare_input(args, model, tokenizer, prompt_text)",
            "-    encoded_prompt = torch.tensor(tokenizer.encode(prompt_text, add_special_tokens=False)).unsqueeze(0)",
            "+    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors='pt')",
            "",
            "output_sequences = model.generate(",
            "-        intput_ids=encoded_prompt,",
            "-        length=args.length,",
            "+        input_ids=encoded_prompt,",
            "+        max_length=args.length,",
            "temperature=args.temperature,",
            "top_k=args.k,",
            "top_p=args.p,"
        ]
    },
    {
        "number": 2794,
        "comments": "",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def rgba_to_rgb(image: torch.Tensor) -> torch.Tensor:",
            "g_new: torch.Tensor = a_one * g + a * g",
            "b_new: torch.Tensor = a_one * b + a * b",
            "",
            "-    return torch.cat([r, g, b], dim=-3)",
            "+    return torch.cat([r_new, g_new, b_new], dim=-3)",
            "",
            "",
            "def rgba_to_bgr(image: torch.Tensor) -> torch.Tensor:"
        ]
    },
    {
        "number": 2795,
        "comments": "",
        "commit_message": "`random_uniform` fix (#2289)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def random_uniform(",
            "device: str,",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "+    shape = _check_bounds_and_get_shape(low, high, shape)",
            "low = tf.cast(low, dtype)",
            "high = tf.cast(high, dtype)",
            "with tf.device(device):",
            "-        return tf.random.uniform(shape if shape else (), low, high, dtype=dtype)",
            "+        return tf.random.uniform(shape, low, high, dtype=dtype)",
            "",
            "",
            "def random_normal("
        ]
    },
    {
        "number": 2798,
        "comments": "",
        "commit_message": "FL MNIST example with autograd tracing (#3567)\n\n* FL MNIST example with autograd tracing\n\n* docstring coverage\n\n* Address review comments\n\n* Formatting fix\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PlanTranslatorTorchscript(AbstractPlanTranslator):",
            "translation_plan = self.plan.copy()",
            "translation_plan.forward = None",
            "",
            "-        args_shape = translation_plan.get_args_shape()",
            "-        args = PlaceHolder.create_placeholders(args_shape)",
            "+        args = translation_plan.create_dummy_args()",
            "",
            "-        # To avoid storing Plan state tensors in torchscript, they will be send as parameters",
            "+        # jit.trace clones input args and can change their type, so we have to skip types check",
            "+        # TODO see if type check can be made less strict,",
            "+        #  e.g. tensor/custom tensor/nn.Parameter could be considered same type",
            "+        translation_plan.validate_input_types = False",
            "+",
            "+        # To avoid storing Plan state tensors in torchscript, they will be sent as parameters",
            "# we trace wrapper func, which accepts state parameters as last arg",
            "# and sets them into the Plan before executing the Plan",
            "def wrap_stateful_plan(*args):"
        ]
    },
    {
        "number": 2799,
        "comments": "",
        "commit_message": "fix error if stop_at_token is None\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def sample_autoregressive(partial_sequences,",
            "partial_sequences_eos_count = 0",
            "else:",
            "initial_states = context_first_part.new_states",
            "-    partial_sequences_eos_count = mtf.reduce_sum(",
            "-        mtf.to_int32(mtf.equal(partial_sequences, stop_at_token)),",
            "-        reduced_dim=length_dim)",
            "+    if stop_at_token is not None:",
            "+        partial_sequences_eos_count = mtf.reduce_sum(",
            "+            mtf.to_int32(mtf.equal(partial_sequences, stop_at_token)),",
            "+            reduced_dim=length_dim)",
            "",
            "def cond_fn(position, ids, *unused_states):",
            "\"\"\"Should we run another loop iteration.\"\"\""
        ]
    },
    {
        "number": 2802,
        "comments": "",
        "commit_message": "Uint8 fix (#2105)\n\n* fix (uint8 index warning): use bool for inside flags\n\n* fix (unint8 index): convert to bool in the final to avoid 1.1 constriant\n\n* fix (unmap): convert inds in unmap to be bool type\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def unmap(data, count, inds, fill=0):",
            "size count) \"\"\"",
            "if data.dim() == 1:",
            "ret = data.new_full((count, ), fill)",
            "-        ret[inds] = data",
            "+        ret[inds.type(torch.bool)] = data",
            "else:",
            "new_size = (count, ) + data.size()[1:]",
            "ret = data.new_full(new_size, fill)",
            "-        ret[inds, :] = data",
            "+        ret[inds.type(torch.bool), :] = data",
            "return ret"
        ]
    },
    {
        "number": 2812,
        "comments": "",
        "commit_message": "Heterogeneous Graph Support + GraphGym (#3068)\n\n* added HGT DBLP example\n\n* typo\n\n* Merge PyG master (#52)\n\n* Adding the Facebok Page-Page dataset\n\n* type hints\n\n* documentation CI\n\n* py 3.8\n\n* fix links\n\n* fix links\n\n* fail on warning\n\n* fail on warning\n\n* fix doc\n\nCo-authored-by: benedekrozemberczki <benedek.rozemberczki@gmail.com>\n\n* revert\n\n* Fix Documentation Rendering (#51)\n\n* fix doc rendering\n\n* fix linting\n\n* retrigger checks\n\n* remove pytorch 1.7.0 legacy code (#50)\n\n* Fix `copy.deepcopy` within lazy `nn.dense.Linear` (#44)\n\n* fix deepcopy within lazy Linear\n\n* fix merge\n\n* assert exception\n\n* example to doc\n\n* resolve conflict\n\n* resolve conflict\n\n* Add Figure and Equation to `to_hetero` docstring (#60)\n\n* add tex\n\n* add svg + docstring\n\n* typo\n\n* added equation\n\n* Message Passing Hooks (#53)\n\n* add hooks\n\n* docstring\n\n* add docstring\n\n* allow modification of inputs/output\n\n* add test for modifying output\n\n* add additional asserts for modifying output test\n\n* Rename `HeteroData.get_edges` and `HeteroData.get_nodes` (#58)\n\n* rename to_edges and to_nodes\n\n* typo\n\n* `HeteroConv` (#64)\n\n* clean heteroconv\n\n* init\n\n* init\n\n* clean up\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix documentation\n\n* bipartite function\n\n* fix test CI\n\n* remove pillow version\n\n* clean up for merge\n\n* Merge PyG master (#69)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\n\n* re-merge\n\n* add lazy column to GNN cheatsheet (#70)\n\n* `to_hetero_with_bases(model)` (#63)\n\n* update\n\n* fix linting\n\n* basisconv\n\n* add ValueError\n\n* to_hetero_with_bases impl done\n\n* add test\n\n* add comments\n\n* add comments\n\n* docstring\n\n* typo\n\n* update figure\n\n* svg\n\n* typo\n\n* add test\n\n* update\n\n* add rgcn equality test\n\n* typos\n\n* update\n\n* typos\n\n* update figures\n\n* generate new svgs\n\n* fix assignment\n\n* rename\n\n* delete sorted edge types\n\n* rename\n\n* add legend\n\n* fix typo\n\n* Test: Check equal outputs of `to_hetero` and `RGCNConv` (#59)\n\n* check equal output\n\n* add sparsetensor test\n\n* check equal output\n\n* add sparsetensor test\n\n* rename\n\n* linting\n\n* add missing import\n\n* `HeteroData` support for `T.NormalizeFeatures` (#56)\n\n* normalize features\n\n* allow normalization of any feature\n\n* in-place div\n\n* normalize features\n\n* allow normalization of any feature\n\n* in-place div\n\n* fix test\n\n* no need to re-assign\n\n* `HeteroData` support for `T.AddSelfLoops` (#54)\n\n* hetero support for AddSelfLoops\n\n* check for edge_index attribute\n\n* f-string\n\n* retrigger checks\n\n* revert bipartite changes\n\n* hetero support for AddSelfLoops\n\n* check for edge_index attribute\n\n* f-string\n\n* retrigger checks\n\n* revert bipartite changes\n\n* merge master\n\n* merge master\n\n* `HeteroData` support for `T.ToSparseTensor` (#55)\n\n* hetero support for ToSparseTensor\n\n* add test\n\n* customize the attribute of SparseTensor.value\n\n* rework sort_edge_index\n\n* hetero support for ToSparseTensor\n\n* add test\n\n* customize the attribute of SparseTensor.value\n\n* rework sort_edge_index\n\n* linting\n\n* `HeteroData` support for `T.ToUndirected` (#57)\n\n* to_undirected\n\n* revert bipartite changes\n\n* coalesce + undirected enhancement\n\n* merge master\n\n* revert bipartite changes\n\n* coalesce + undirected enhancement\n\n* merge master\n\n* clean up\n\n* new default relation type\n\n* fix tests\n\n* resolve merge conflicts\n\n* resolve merge conflicts 2\n\n* resolve merge conflicts 3\n\n* Merge PyG master (#74)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\n* clean up doc for to_dense_batch\n\n* clean up\n\n* add legend to cheatsheet\n\n* Improve terminology (#2837)\n\nI think the previous version of the document uses the term 'symmetric' incorrectly. A symmetric matrix is a square matrix that is is equal to its transpose (https://en.wikipedia.org/wiki/Symmetric_matrix). However, the text is only talking about the shape of the matrix, not its content. Hence, 'square (matrix)' would be the correct term to use.\n\n* Add batch_size input to to_dense_batch (#2838)\n\n* Add batch_size input to to_dense_batch\n\n* to_dense_batch fix typo in batch_size param use\n\n* add typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Added return_attention_weights to TransformerConv. (#2807)\n\n* added return_weights functionality to tranformer\n\n* added return attn weights tests\n\n* flake8\n\n* added typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* MD17 (#2843)\n\n* Added MD17 dataset\n\n* Updated Documentation\n\n* Added link to sGDML website in doc\n\n* fixed typos in doc and made train variable description clearer\n\n* clean up\n\n* fix linting\n\n* fix doc warning\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* update doc\n\n* remove forward doc\n\n* add static graph support info to cheatsheet\n\n* fix num_nodes in case edge_index is empty\n\n* fix math formula\n\n* faster GDC import\n\n* lazy import\n\n* lazy import for datasets\n\n* lazy import for nn\n\n* Sequential jittable + traceable\n\n* typo\n\n* typo\n\n* update doc\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Sch\u00e4fer <moritz.schaefer@protonmail.com>\n\n* re-add\n\n* GraphGym cleaned version (#82)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Update .gitignore\n\n* Integrate GraphGym into PyG (#85)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Integrate graphgym into pyg, keep user API in project root\n\n* fix merge conflict\n\n* fix lint errors\n\n* Make optional dependencies\n\n* merge LICENSE from GraphGym\n\n* add import\n\n* clean up LICENSE\n\n* fix import\n\n* resolve merge conflicts\n\n* resolve merge conflicts 2\n\n* Merge PyG master (#87)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\n* clean up doc for to_dense_batch\n\n* clean up\n\n* add legend to cheatsheet\n\n* Improve terminology (#2837)\n\nI think the previous version of the document uses the term 'symmetric' incorrectly. A symmetric matrix is a square matrix that is is equal to its transpose (https://en.wikipedia.org/wiki/Symmetric_matrix). However, the text is only talking about the shape of the matrix, not its content. Hence, 'square (matrix)' would be the correct term to use.\n\n* Add batch_size input to to_dense_batch (#2838)\n\n* Add batch_size input to to_dense_batch\n\n* to_dense_batch fix typo in batch_size param use\n\n* add typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Added return_attention_weights to TransformerConv. (#2807)\n\n* added return_weights functionality to tranformer\n\n* added return attn weights tests\n\n* flake8\n\n* added typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* MD17 (#2843)\n\n* Added MD17 dataset\n\n* Updated Documentation\n\n* Added link to sGDML website in doc\n\n* fixed typos in doc and made train variable description clearer\n\n* clean up\n\n* fix linting\n\n* fix doc warning\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* update doc\n\n* remove forward doc\n\n* add static graph support info to cheatsheet\n\n* fix num_nodes in case edge_index is empty\n\n* fix math formula\n\n* faster GDC import\n\n* lazy import\n\n* lazy import for datasets\n\n* lazy import for nn\n\n* Sequential jittable + traceable\n\n* typo\n\n* typo\n\n* update doc\n\n* Simple models (#2869)\n\n* Inclusion of new backbone models\n\n* Eliminating head from asap.py\n\n* small correction\n\n* Create test_gcn.py\n\n* Update __init__.py\n\n* Update test_gcn.py\n\n* Left only the convolutional simple models\n\n* Tests included\n\n* update\n\n* clean up\n\n* clean up v2\n\n* fix activation\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* Example for MemPooling. (#2729)\n\n* example for mem pooling\n\n* backprop on kl loss is done at the end of an epoch. Keys in memory layers are trained only on kl loss.\n\n* added learning rate decay. Using PROTIENS_full\n\n* flake8\n\n* reduced lr. increased weight decay\n\n* changed download location\n\n* added comments\n\n* clean up\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typos\n\n* fix removeisolatednodes transform in case 'data.num_nodes' is present\n\n* fix XConv with dilation > 1\n\n* fix XConv with dilation > 1\n\n* rgcn link prediction  (#2734)\n\n* implemented LinkPrediction dataset for loading FB15k237\n\n* implemented evaluation for relational link prediction\n\n* implemented R-GCNConf link prediction example\n\n* fixed bug: wrong initial objects in negative_sampling\n\n* changed file downloader urllib.request.urlretrieve  to pytorch.data.download_url; renamed LinkPrediction class to RelationalLinkPredictionDataset\n\n* update dataset\n\n* update example script\n\n* rename\n\nCo-authored-by: Moritz <moritzblum>\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix gnnexplainer draw kwargs\n\n* remove python-louvain dependency\n\n* allow customization of output in MP jit mode\n\n* fix test for py3.6\n\n* changed normalisation to same norm from instance norm to be robust to small var (#2917)\n\n* add CITATION.cff\n\n* format\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* add basetransform ABC (#2924)\n\n* clean up BaseTransform\n\n* clean up GATConv and add comments\n\n* add max_num_neighbors as an additional argument\n\n* fix jit GATConv on PyTorch 1.8.0\n\n* fix doc\n\n* fix gnn explainer with existing self-loops\n\n* Rgcn link pred fix (#2946)\n\n* added regularization, removed typo in test\n\n* clean up\n\nCo-authored-by: Moritz <moritzblum>\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Correct gini coefficient mathcal formula (#2932)\n\n* typo\n\n* typo\n\n* Update from_networkx (#2923)\n\n* Update from_networkx\n\n* Update test\n\n* Update convert.py\n\n* Minor corrections\n\n* Update test_convert.py\n\n* Corrections\n\n* Update test_convert.py\n\n* Case where there are no edges\n\n* Correcting how edge_attr are concatenated\n\n* clean up + new test\n\n* remove unused code\n\n* add union type\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix deterministic ordering in from_networkx\n\n* recursive-include *.jinja files\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Sch\u00e4fer <moritz.schaefer@protonmail.com>\nCo-authored-by: PabloAMC <pmorenocf@alumnos.unex.es>\nCo-authored-by: Moritz Blum <31183934+moritzblum@users.noreply.github.com>\nCo-authored-by: fbragman <fbragman@users.noreply.github.com>\nCo-authored-by: Christopher Lee <2824685+CCInc@users.noreply.github.com>\nCo-authored-by: Tim Daubensch\u00fctz <tim@daubenschuetz.de>\n\n* resolve merge conflicts 3\n\n* resolve merge conflicts 4\n\n* Implementation of the `HGTLoader` + `ogbn-mag` example (#73)\n\n* first try\n\n* update\n\n* HGT Loader\n\n* typo\n\n* first try\n\n* update\n\n* HGT Loader\n\n* typo\n\n* bugfixes\n\n* lazy GATConv\n\n* bugfix\n\n* bugfix\n\n* full working pipeline\n\n* update\n\n* rename\n\n* docstring\n\n* typos\n\n* update\n\n* typo\n\n* typo\n\n* typo\n\n* added comments\n\n* add test\n\n* add tests\n\n* fix example\n\n* rename\n\n* linting\n\n* Random split functionalities (#72)\n\n* link split\n\n* create split\n\n* example tests\n\n* link split tests\n\n* fix linting\n\n* update docstring\n\n* undirected option, refactor and docs\n\n* add num nodes as argument to neg sampling\n\n* clean up + remove single object\n\n* update example\n\n* typo\n\n* fix compose\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* add basetransform\n\n* typo\n\n* typo\n\n* fix test\n\n* Improve `torch_geometric.data` Documentation (#98)\n\n* update data doc\n\n* typo\n\n* typo\n\n* note\n\n* typo\n\n* add docstring\n\n* only show inherited members for data and hetero_data\n\n* documentation update for batch and dataset\n\n* update doc\n\n* update\n\n* fix\n\n* record_stream\n\n* update\n\n* typo\n\n* add/fix data functionality\n\n* linting\n\n* typo\n\n* `_parent` memory leak fix (#103)\n\n* memory leak fix\n\n* Clean up\n\n* clean up\n\n* bugfix tests\n\n* typos\n\n* fix test\n\n* fix test\n\n* rename reverse\n\n* (Heterogeneous) `NeighborLoader` (#92)\n\n* initial commit\n\n* typo\n\n* neighbor loader functionality + tests\n\n* docstring\n\n* fix docstring\n\n* skip tests\n\n* fix share_memory_\n\n* typo\n\n* typo\n\n* update example\n\n* typo\n\n* share_strategy\n\n* fix cuda calls\n\n* better print\n\n* fix size\n\n* fix print\n\n* final commit\n\n* fix\n\n* some todos\n\n* preprocessed features\n\n* fix to_undirected\n\n* more documentation\n\n* update doc\n\n* fix doc\n\n* fix doc\n\n* Add benchmark code and the example with existing graph classification examples (#93)\n\n* add benchmarking utilities\n\n* update graph classification benchmark\n\n* improve code style\n\n* add pytorch-memlab for benchmark code\n\n* skip some tests when cuda is not available\n\n* add type hint when appropriate\n\n* add seed_everything to improve code\n\n* code refactoring\n\n* code refactoring\n\n* code refactoring\n\n* code improvement\n\n* remove unnecessary dataloader import\n\n* change benchmark interface with decorator\n\n* documentation improvement\n\n* linting\n\n* linting part 2\n\n* linting part 3\n\n* seed_everything\n\n* create utils file\n\n* update\n\n* use utils functions\n\n* fix test\n\n* update the profiler to the latest torch (1.8.1+)\n\n* refactor profiler and add more documentation\n\n* refactor profiler and add more documentation\n\n* resolve lint errors\n\n* resolve lint errors\n\n* update\n\n* clean up test and profile\n\n* fix linting\n\n* add to doc\n\n* fix doc\n\n* typo\n\n* update benchmark\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* Move `HGTLoader` to `torch_geometric.loader` + clean up (#104)\n\n* move files\n\n* use utils functions\n\n* fix example\n\n* update\n\n* fix tests\n\n* fix seed\n\n* fix linear test\n\n* rename\n\n* Support GraphGym custom modules outside PyG package (#102)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Integrate graphgym into pyg, keep user API in project root\n\n* fix merge conflict\n\n* fix lint errors\n\n* Make optional dependencies\n\n* merge LICENSE from GraphGym\n\n* Enable adding GraphGym customized modules outside PyG package\n\n* lint\n\n* Rename `AddTrainValTestMask` to `RandomNodeSplit` (#108)\n\n* initial commit\n\n* rename example\n\n* remove AddTrainValTestMask\n\n* fix linting\n\n* create optimizer config and scheduler config separately (#113)\n\n* create optimizer config and scheduler config separately\n\n* fix format\n\n* import explicitly\n\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\n\n* Heterogeneous Graph Tutorial (#83)\n\n* add HG tutorial roadmap\n\n* started working on hg tutorial\n\n* hg_tutorial, some text and .tex figure\n\n* added svg\n\n* hg tutorial content\n\n* fix CI\n\n* text and structure\n\n* finished first draft\n\n* fixed one code example\n\n* fixing conventions\n\n* fixing links\n\n* update svg\n\n* some smaller improvements of tutorial\n\n* improvements on tutorial\n\n* hg-tutorial: fixed compiling issue, added detailed content\n\n* added absolute links\n\n* fixed warnings\n\n* streamlined dataset section\n\n* update svg\n\n* update tutorial\n\n* update 2\n\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\n\n* typo\n\n* Move data loaders to `torch_geometric.loader` (#110)\n\n* move graphsaint\n\n* deprecations\n\n* move clusterloader\n\n* deprecations\n\n* type hints\n\n* move shadow\n\n* typo\n\n* typo\n\n* move datalistloader\n\n* dense data loader\n\n* random node sampler\n\n* fix doc\n\n* Lazy GNN operators (#89)\n\n* lazy cheb conv\n\n* lazy GraphConv\n\n* lazy GATv2Conv\n\n* lazy TAGConv\n\n* lazy FAConv\n\n* lazy FeaStConv\n\n* lazy NNConv\n\n* typo\n\n* fix tests\n\n* lazy SuperGATConv\n\n* lazy SuperGATConv fix\n\n* lazy SplineConv\n\n* fix lazy check\n\n* lazy GravNetConv\n\n* arma conv lazy\n\n* dense linear in gmmconv\n\n* typo\n\n* add test\n\n* lazy GMMConv\n\n* doc\n\n* rename (#116)\n\n* Revisit `MetaPath2Vec` (#114)\n\n* revisit metapath2vec\n\n* update\n\n* typo\n\n* update\n\n* fix doc\n\n* update\n\n* check for attributes rather than key\n\n* Clean up `torch_geometric.profile` further (#111)\n\n* remove print_layer_stats\n\n* typos\n\n* update\n\n* readme highlights and quick tour (#99)\n\n* readme highlights and quick tour\n\n* arch\n\n* arch image\n\n* arch overview\n\n* list categories\n\n* categorization\n\n* category description\n\n* Update README.md\n\nfrom Matthias\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* improved highlights\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* minor\n\n* update readme\n\n* update\n\n* update\n\n* update\n\n* update\n\n* fix url\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* move ops\n\n* toc\n\n* typo\n\n* typo\n\n* add svgs\n\n* update figure\n\n* fix links\n\n* fix size\n\n* fix size\n\n* typo\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* fix broken links\n\n* fix links\n\n* Heterogeneous Graph Sampler Tutorial (#117)\n\n* initial commit\n\n* address comments\n\n* remove todo\n\n* typo\n\n* Conversion between heterogenous and homogeneous graph objects (#115)\n\n* temp checkpoint (wip, will remove)\n\n* (wip) typed graph conversion\n\n* (wip) typed graph conversion\n\n* (wip) typed graph conversion\n\n* update\n\n* typo\n\n* delete examples\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix test\n\n* update doc\n\n* deprecate NeighborSampler (#119)\n\n* Move `torch_geometric.data.DataLoader` to `torch_geometric.loader.DataLoader` (#120)\n\n* move dataloader\n\n* rename\n\n* typos\n\n* typos\n\n* fix __cat_dim__\n\n* updategp\n\n* Deprecate `train_test_split_edges` + Modifications to `RandomLinkSplit` (#121)\n\n* deprecate train_test_split_edges\n\n* to device transform\n\n* fix example\n\n* add split_labels argument\n\n* fix autoencoder example\n\n* typos\n\n* add docstring\n\n* ARGVA\n\n* seal\n\n* adress comments\n\n* Create example to load `*.csv` and transfer to `HeteroData` (#76)\n\n* create example to load csv file and transfer to heter-data\n\n* add ipython notebook version load csv with documentation\n\n* address comment\n\n* first version of csv loading doc\n\n* first version of csv loading doc\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestions csv tutorial\n\n* example script load csv + extract fix\n\n* fixed edge index stacking dimension in example and jupyter nb\n\n* linting\n\n* linting2\n\n* rename\n\n* update\n\n* update\n\n* update\n\n* typo\n\n* typo\n\n* update\n\n* rename\n\n* update tutorial\n\n* typo\n\n* address comments\n\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* fix\n\n* typo\n\n* update\n\n* fix\n\n* fix\n\nCo-authored-by: benedekrozemberczki <benedek.rozemberczki@gmail.com>\nCo-authored-by: Rex Ying <rexying@stanford.edu>\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Sch\u00e4fer <moritz.schaefer@protonmail.com>\nCo-authored-by: Jiaxuan <youjiaxuan@gmail.com>\nCo-authored-by: PabloAMC <pmorenocf@alumnos.unex.es>\nCo-authored-by: Moritz Blum <31183934+moritzblum@users.noreply.github.com>\nCo-authored-by: fbragman <fbragman@users.noreply.github.com>\nCo-authored-by: Christopher Lee <2824685+CCInc@users.noreply.github.com>\nCo-authored-by: Tim Daubensch\u00fctz <tim@daubenschuetz.de>\nCo-authored-by: Yue Zhao <yzhao062@gmail.com>\nCo-authored-by: Dong Wang <dongw89@gmail.com>\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ShaDowKHopSampler(torch.utils.data.DataLoader):",
            "batch.edge_index = torch.stack([row, col], dim=0)",
            "",
            "for k, v in self.data:",
            "-            if k in ['edge_index', 'adj_t']:",
            "+            if k in ['edge_index', 'adj_t', 'num_nodes']:",
            "continue",
            "if k == 'y' and v.size(0) == self.data.num_nodes:",
            "batch[k] = v[n_id][root_n_id]"
        ]
    },
    {
        "number": 2822,
        "comments": "",
        "commit_message": "Refactoring, cleanup, improved test coverage.\n* Add eca_nfnet_l2 weights, 84.7 @ 384x384\n* All 'non-std' (ie transformer / mlp) models have classifier / default_cfg test added\n* Fix #694 reset_classifer / num_features / forward_features / num_classes=0 consistency for transformer / mlp models\n* Add direct loading of npz to vision transformer (pure transformer so far, hybrid to come)\n* Rename vit_deit* to deit_*\n* Remove some deprecated vit hybrid model defs\n* Clean up classifier flatten for conv classifiers and unusual cases (mobilenetv3/ghostnet)\n* Remove explicit model fns for levit conv, just pass in arg\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Twins(nn.Module):",
            "",
            "def reset_classifier(self, num_classes, global_pool=''):",
            "self.num_classes = num_classes",
            "-        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()",
            "+        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()",
            "",
            "def _init_weights(self, m):",
            "if isinstance(m, nn.Linear):"
        ]
    },
    {
        "number": 2825,
        "comments": "",
        "commit_message": "Metric aggregation testing (#3517)\n\n* aggregation testing\n\n* add more tests\n\n* mse\n\n* more tests\n\n* fix tests\n\n* fix doctest\n\n* fix codefactor\n\n* fix import error\n\n* fix doctest\n\n* revert docfix\n\n* test for model integration\n\n* fix integration test\n\n* added test cases\n\n* fix rmsle\n\n* aggregation testing\n\n* add more tests\n\n* mse\n\n* more tests\n\n* fix tests\n\n* fix doctest\n\n* fix codefactor\n\n* fix import error\n\n* fix doctest\n\n* revert docfix\n\n* test for model integration\n\n* fix integration test\n\n* fix psnr\n\n* add warning/valueerror to embedding similarity\n\n* fixed f scores\n\n* disable some test\n\n* fix tests\n\n* fixing codefactor\n\n* fix pep8\n\n* changelog\n\n* fix doctest\n\n* cleaning test\n\n* fix pickle error\n\n* pickle fix\n\n* fix pickle error\n\n* Apply suggestions from code review\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* code cleanup + changes based on suggestions\n\n* update based on suggestion\n\n* update based on suggestions\n\n* Apply suggestions from code review\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\nCo-authored-by: Nicki Skafte <nugginea@gmail.com>\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_rmsle(pred, target, expected):",
            "])",
            "def test_psnr_with_skimage(pred, target):",
            "score = psnr(pred=torch.tensor(pred),",
            "-                 target=torch.tensor(target))",
            "+                 target=torch.tensor(target), data_range=3)",
            "sk_score = ski_psnr(np.array(pred), np.array(target), data_range=3)",
            "assert torch.allclose(score, torch.tensor(sk_score, dtype=torch.float), atol=1e-3)"
        ]
    },
    {
        "number": 2827,
        "comments": "",
        "commit_message": "Make most metrics work on GPU (#3851)\n\n* Make most metrics work on GPU\n\n* Make metric tests work on both GPU and CPU\n\n* Add a test for the test utility\n\n* mypy\n\n* Update allennlp/common/testing/test_case.py\n\nCo-Authored-By: Mark Neumann <markn@allenai.org>\n\n* Fix a PR comment\n\n* flake8\n\nCo-authored-by: Mark Neumann <markn@allenai.org>\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Metric(Registrable):",
            "raise NotImplementedError",
            "",
            "@staticmethod",
            "-    def unwrap_to_tensors(*tensors: torch.Tensor):",
            "+    def detach_tensors(*tensors: torch.Tensor) -> Iterable[torch.Tensor]:",
            "\"\"\"",
            "If you actually passed gradient-tracking Tensors to a Metric, there will be",
            "a huge memory leak, because it will prevent garbage collection for the computation",
            "-        graph. This method ensures that you're using tensors directly and that they are on",
            "-        the CPU.",
            "+        graph. This method ensures the tensors are detached.",
            "\"\"\"",
            "-        return (x.detach().cpu() if isinstance(x, torch.Tensor) else x for x in tensors)",
            "+        # Check if it's actually a tensor in case something else was passed.",
            "+        return (x.detach() if isinstance(x, torch.Tensor) else x for x in tensors)"
        ]
    },
    {
        "number": 2845,
        "comments": "",
        "commit_message": "Type promotion fixes (#2516)\n\n* casting fixes\n\n* lint fixes\n\n* changes\n\n* more changes\n\n* lint fixes\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def floor_divide(",
            "if (not np.all(x2)) or (np.any(x2) == -0):  # check for division by zero",
            "ret = np.floor_divide(x1, x2)",
            "else:",
            "-        ret = tf.math.floordiv(x1, x2)",
            "+        ret = tf.experimental.numpy.floor_divide(x1, x2)",
            "",
            "if (any(isinf(x1)) and any(isfinite(x2))) or (any(isfinite(x1)) and any(isinf(x2))):",
            "return ivy.full_like(ret, floor(divide(x1, x2)), dtype=ret.dtype)"
        ]
    },
    {
        "number": 2859,
        "comments": "",
        "commit_message": "fix(DDIM scheduler): use correct dtype for noise (#742)\n\nOtherwise, it crashes when eta > 0 with float16.\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DDIMScheduler(SchedulerMixin, ConfigMixin):",
            "prev_sample = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction",
            "",
            "if eta > 0:",
            "+            # randn_like does not support generator https://github.com/pytorch/pytorch/issues/27072",
            "device = model_output.device if torch.is_tensor(model_output) else \"cpu\"",
            "-            noise = torch.randn(model_output.shape, generator=generator).to(device)",
            "+            noise = torch.randn(model_output.shape, dtype=model_output.dtype, generator=generator).to(device)",
            "variance = self._get_variance(timestep, prev_timestep) ** (0.5) * eta * noise",
            "",
            "prev_sample = prev_sample + variance"
        ]
    },
    {
        "number": 2865,
        "comments": "",
        "commit_message": "Fixed issue #21053 (#21065)\n\nCo-authored-by: susnato <susnato@tensorflow123456@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFGPT2MainLayer(tf.keras.layers.Layer):",
            "# indices on GPU, returning zeros instead. This is a dangerous silent behavior.",
            "tf.debugging.assert_less(",
            "input_ids,",
            "-                tf.cast(self.vocab_size, dtype=input_ids.dtype),",
            "+                tf.cast(self.config.vocab_size, dtype=input_ids.dtype),",
            "message=(",
            "\"input_ids must be smaller than the embedding layer's input dimension (got\"",
            "f\" {tf.math.reduce_max(input_ids)} >= {self.vocab_size})\""
        ]
    },
    {
        "number": 2877,
        "comments": "",
        "commit_message": "add pat change (#3414)\n\n* add pat change\n\n* fix grid roi head\n\n* fix comments\n\n* clean\n\n* revert change\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def patch_norm_fp32(module):",
            "\"\"\"",
            "if isinstance(module, (nn.modules.batchnorm._BatchNorm, nn.GroupNorm)):",
            "module.float()",
            "-        if isinstance(module, nn.GroupNorm) or torch.__version__ < '1.3':",
            "+        if isinstance(module, nn.GroupNorm) or torch.__version__ == 'parrots':",
            "module.forward = patch_forward_method(module.forward, torch.half,",
            "torch.float)",
            "for child in module.children():"
        ]
    },
    {
        "number": 2882,
        "comments": "",
        "commit_message": "[RLlib] Fix all example scripts to run on GPUs. (#11105)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TorchBinaryAutoregressiveDistribution(TorchDistributionWrapper):",
            "",
            "def _a1_distribution(self):",
            "BATCH = self.inputs.shape[0]",
            "-        a1_logits, _ = self.model.action_module(self.inputs,",
            "-                                                torch.zeros((BATCH, 1)))",
            "+        zeros = torch.zeros((BATCH, 1)).to(self.inputs.device)",
            "+        a1_logits, _ = self.model.action_module(self.inputs, zeros)",
            "a1_dist = TorchCategorical(a1_logits)",
            "return a1_dist"
        ]
    },
    {
        "number": 2883,
        "comments": "",
        "commit_message": "Small fixes for compatibility with pytorch master (#442)\n\n* Small fixes for campatibility with pytorch master\n\n* Support older pytorch 0.2 torch.nn.functional.softmax\n\n* Work around bugs in torch.cat()\n\n* Replace torch_cat(-) with torch.stack(-).squeeze()\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Fixture(object):",
            "",
            "def _convert_logits_to_ps(self, dist_params):",
            "if 'logits' in dist_params:",
            "-            logits = torch.Tensor(dist_params.pop('logits'))",
            "+            logits = Variable(torch.Tensor(dist_params.pop('logits')))",
            "is_multidimensional = self.get_test_distribution_name() != 'Bernoulli'",
            "ps, _ = get_probs_and_logits(logits=logits, is_multidimensional=is_multidimensional)",
            "dist_params['ps'] = list(ps.data.cpu().numpy())"
        ]
    },
    {
        "number": 2884,
        "comments": "",
        "commit_message": "geddataset fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class GEDDataset(InMemoryDataset):",
            "xs += [assoc[x]]",
            "ys += [assoc[y]]",
            "gs += [g]",
            "-            x, y, g = torch.tensor(xs), torch.tensor(ys), torch.tensor(gs)",
            "+            x, y = torch.tensor(xs), torch.tensor(ys)",
            "+            g = torch.tensor(gs, dtype=torch.float)",
            "mat[x, y], mat[y, x] = g, g",
            "",
            "path = osp.join(self.processed_dir, '{}_ged.pt'.format(self.name))"
        ]
    },
    {
        "number": 2886,
        "comments": "",
        "commit_message": "Add automatic reparametrization strategies (#2884)\n\n* Sketch automatic reparametrization strategies\n\n* Clean up\n\n* Revise interface; get first tests to pass\n\n* Add an end-to-end smoke test\n\n* Add more tests, fix handling of ROHC\n\n* Simplify\n\n* Address review comments\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TransformReparam(Reparam):",
            "is_observed = msg[\"is_observed\"]",
            "",
            "fn, event_dim = self._unwrap(fn)",
            "-        assert isinstance(fn, dist.TransformedDistribution)",
            "+        assert isinstance(fn, torch.distributions.TransformedDistribution)",
            "",
            "# Differentiably invert transform.",
            "value_base = value"
        ]
    },
    {
        "number": 2891,
        "comments": "",
        "commit_message": "fix error if num_microbatches = 1\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def block(params, scope, past, append_dim, train=False):",
            "def model(features, labels, params, mesh, past=None):",
            "\"\"\"A GPT style model implemented in mesh tensorlfow.\"\"\"",
            "results = {}",
            "+    sequence_dim = mtf.Dimension('sequence', params[\"n_ctx\"])",
            "if params[\"num_microbatches\"] > 1:",
            "x = features[\"inputs\"]",
            "labels = features[\"labels\"]",
            "batch_dim = x.shape[0]",
            "-",
            "-",
            "else:",
            "+      batch_dim = mtf.Dimension('batch', params[\"train_batch_size\"])",
            "x = mtf.import_tf_tensor(mesh, features, mtf.Shape([batch_dim, sequence_dim]))",
            "# In this case, labels are simply input shifted one token to the right",
            "# this op is done in the input_fn",
            "# define mtf dims",
            "-      batch_dim = mtf.Dimension('batch', params[\"train_batch_size\"])",
            "labels = mtf.import_tf_tensor(mesh, labels, mtf.Shape([batch_dim, sequence_dim]))",
            "",
            "-    sequence_dim = mtf.Dimension('sequence', params[\"n_ctx\"])",
            "",
            "# we need this because gathering when both the args have the same dimension in them it breaks stuff.",
            "# this dim is specifically for the weights"
        ]
    },
    {
        "number": 2893,
        "comments": "",
        "commit_message": "Fix conflict\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Conv2dSubsampling(torch.nn.Module):",
            "torch.nn.ReLU()",
            ")",
            "self.out = torch.nn.Sequential(",
            "-            torch.nn.Linear(odim * (idim // 4), odim),",
            "+            torch.nn.Linear(odim * (((idim - 1) // 2 - 1) // 2), odim),",
            "PositionalEncoding(odim, dropout_rate)",
            ")"
        ]
    },
    {
        "number": 2899,
        "comments": "",
        "commit_message": "test fixes\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class GlowTTSTrainTest(unittest.TestCase):",
            "assert (param - param_ref).sum() == 0, param",
            "count += 1",
            "",
            "-        optimizer = optim.Adam(model.parameters(), lr=c.lr)",
            "+        optimizer = optim.Adam(model.parameters(), lr=0.001)",
            "for _ in range(5):",
            "+            optimizer.zero_grad()",
            "z, logdet, y_mean, y_log_scale, alignments, o_dur_log, o_total_dur = model.forward(",
            "input_dummy, input_lengths, mel_spec, mel_lengths, None)",
            "-            optimizer.zero_grad()",
            "loss_dict = criterion(z, y_mean, y_log_scale, logdet, mel_lengths,",
            "o_dur_log, o_total_dur, input_lengths)",
            "loss = loss_dict['loss']"
        ]
    },
    {
        "number": 2904,
        "comments": "",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def rpn_losses(anchor_labels, anchor_boxes, label_logits, box_logits):",
            "# But the total RPN loss will be fine.  TODO make the summary op smarter",
            "placeholder = 0.",
            "label_loss = tf.nn.sigmoid_cross_entropy_with_logits(",
            "-        labels=tf.to_float(valid_anchor_labels), logits=valid_label_logits)",
            "+        labels=tf.cast(valid_anchor_labels, tf.float32), logits=valid_label_logits)",
            "label_loss = tf.reduce_sum(label_loss) * (1. / cfg.RPN.BATCH_PER_IM)",
            "label_loss = tf.where(tf.equal(nr_valid, 0), placeholder, label_loss, name='label_loss')"
        ]
    },
    {
        "number": 2920,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Embedding(TokenEmbedder):",
            "if weight is None:",
            "weight = torch.FloatTensor(num_embeddings, embedding_dim)",
            "self.weight = torch.nn.Parameter(weight, requires_grad=trainable)",
            "-            torch.nn.init.xavier_uniform(self.weight.data)",
            "+            torch.nn.init.xavier_uniform_(self.weight)",
            "else:",
            "if weight.size() != (num_embeddings, embedding_dim):",
            "raise ConfigurationError(\"A weight matrix was passed with contradictory embedding shapes.\")"
        ]
    },
    {
        "number": 2923,
        "comments": "",
        "commit_message": "[`bnb`] Let's make the daily CI green \ud83c\udf4f  (#21597)\n\n* fix bnb slow test\n\n* make fixup\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MixedInt8Test(BaseMixedInt8Test):",
            "super().setUp()",
            "",
            "# Models and tokenizer",
            "-        self.model_fp16 = AutoModelForCausalLM.from_pretrained(self.model_name, torch_dtype=\"auto\", device_map=\"auto\")",
            "+        self.model_fp16 = AutoModelForCausalLM.from_pretrained(",
            "+            self.model_name, torch_dtype=torch.float16, device_map=\"auto\"",
            "+        )",
            "self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map=\"auto\")",
            "",
            "def tearDown(self):"
        ]
    },
    {
        "number": 2943,
        "comments": "",
        "commit_message": "lint fixes\n\nSummary:\nRan the linter.\nTODO: need to update the linter as per D21353065.\n\nReviewed By: bottler\n\nDifferential Revision: D21362270\n\nfbshipit-source-id: ad0e781de0a29f565ad25c43bc94a19b1828c020\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Meshes(object):",
            "return",
            "",
            "if self.isempty():",
            "-            self._edges_packed = -torch.ones(",
            "-                (0, 2), dtype=torch.int64, device=self.device",
            "+            self._edges_packed = torch.full(",
            "+                (0, 2), fill_value=-1, dtype=torch.int64, device=self.device",
            ")",
            "self._edges_packed_to_mesh_idx = torch.zeros(",
            "(0,), dtype=torch.int64, device=self.device"
        ]
    },
    {
        "number": 2948,
        "comments": "",
        "commit_message": "delaunay fixes in cases num_nodes < 3, resizing of images in PascalVOC and WILLOW\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class InMemoryDataset(Dataset):",
            "for key in keys:",
            "item = data_list[0][key]",
            "if torch.is_tensor(item):",
            "-                data[key] = torch.cat(",
            "-                    data[key], dim=data.__cat_dim__(key, data_list[0][key]))",
            "+                data[key] = torch.cat(data[key],",
            "+                                      dim=data.__cat_dim__(key, item))",
            "elif isinstance(item, int) or isinstance(item, float):",
            "data[key] = torch.tensor(data[key])"
        ]
    },
    {
        "number": 2956,
        "comments": "",
        "commit_message": "Fix model export for DocClassificationModel with dense features (#394)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/394\n\nDocClassificationTask supports dense features, but exporting a DocClassification model with dense features to caffe2 doesn't work. Also, exporting any model that uses both text features (word/char) and dense features together doesn't work. This diff fixes export for dense features.\n\nThere are two things that needed fixing:\n\n- exporter puts model inputs in a list like this: `[features, feature lengths]`. However, dense features don't go through the representation layer - so they need to be hardcoded to be at the end of model inputs (ugh). The order of features needs to be `[features, feature lengths, dense features]`\n- dummy_model_input for all other fields has two entries. dummy_model_input for dense_features has one entry\n\nIn the long run, exporter should be tightly coupled with the model itself - we should enforce that model inputs have the same order while training and exporting. But that requires a lot of refactoring.\n\nThis is a blocking feature for [on-device M-suggestions](https://fb.workplace.com/groups/323789728341681/permalink/326686971385290/). A public test for that is scheduled next week.\nPersonalization models in M-suggestions need dense features.\n\nReviewed By: snisarg, gardenia22\n\nDifferential Revision: D14473020\n\nfbshipit-source-id: e7e9eec5d3f3ac8fb017bb88d24af4b9f860e316\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class FloatVectorField(Field):",
            ")",
            "self.dim_error_check = dim_error_check  # dims in data should match config",
            "self.dummy_model_input = torch.tensor(",
            "-            [[1.0] * dim], dtype=torch.float, device=\"cpu\"",
            "+            [[1.0] * dim, [1.0] * dim], dtype=torch.float, device=\"cpu\"",
            ")",
            "",
            "def _parse_vector(self, s):"
        ]
    },
    {
        "number": 2958,
        "comments": "",
        "commit_message": "batch fix if items are only sparsely available\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Data(object):",
            "@property",
            "def num_edges(self):",
            "for key, item in self('edge_index', 'edge_attr'):",
            "-            return item.size(self.cat_dim(key))",
            "+            return item.size(self.cat_dim(key, item))",
            "return None",
            "",
            "@property"
        ]
    },
    {
        "number": 2959,
        "comments": "",
        "commit_message": "Should be fixed finally\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def main():",
            "np.random.seed(args.seed)",
            "",
            "if args.backend == \"pytorch\":",
            "-        from espnet.lmpytorch.tts_pytorch import train",
            "+        from espnet.tts.pytorch.tts_pytorch import train",
            "train(args)",
            "else:",
            "raise NotImplementedError(\"Only pytorch is supported.\")"
        ]
    },
    {
        "number": 2969,
        "comments": "",
        "commit_message": "fix bugs in WGAN, WGAN-GP, DRAGAN\n\noutput of discriminator is changed into linear output (logit value) from\nsigmoid output\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class WGAN_GP(object):",
            "alpha = tf.random_uniform(shape=self.inputs.get_shape(), minval=0.,maxval=1.)",
            "differences = G - self.inputs # This is different from MAGAN",
            "interpolates = self.inputs + (alpha * differences)",
            "-        D_inter,_,_=self.discriminator(interpolates, is_training=True, reuse=True)",
            "+        _,D_inter,_=self.discriminator(interpolates, is_training=True, reuse=True)",
            "gradients = tf.gradients(D_inter, [interpolates])[0]",
            "slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))",
            "gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2)"
        ]
    },
    {
        "number": 2971,
        "comments": "",
        "commit_message": "Fixed bug: replaced bce_loss_with_logits with bce_loss (#7096)\n\n* Fixed bug: replaced bce_loss_with_logits with bec_loss\n\n* Fixed bug: removed sigmoid activation from forward pass\n\n* switched names for scores and logits\n\nCo-authored-by: Alexey Misev <amisev@fb.com>\nCo-authored-by: Alexey Misev <alexey@MacBook-Pro-Natalia.local>\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TransferLearningModel(pl.LightningModule):",
            "# 1. Forward pass:",
            "x, y = batch",
            "y_logits = self.forward(x)",
            "+        y_scores = torch.sigmoid(y_logits)",
            "y_true = y.view((-1, 1)).type_as(x)",
            "",
            "# 2. Compute loss",
            "self.log(\"val_loss\", self.loss(y_logits, y_true), prog_bar=True)",
            "",
            "# 3. Compute accuracy:",
            "-        self.log(\"val_acc\", self.valid_acc(y_logits, y_true.int()), prog_bar=True)",
            "+        self.log(\"val_acc\", self.valid_acc(y_scores, y_true.int()), prog_bar=True)",
            "",
            "def configure_optimizers(self):",
            "parameters = list(self.parameters())"
        ]
    },
    {
        "number": 2972,
        "comments": "",
        "commit_message": "fix BDDMPipeline\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BDDMPipeline(DiffusionPipeline):",
            "num_prediction_steps = len(self.noise_scheduler)",
            "for t in tqdm.tqdm(reversed(range(num_prediction_steps)), total=num_prediction_steps):",
            "# 1. predict noise residual",
            "-            with torch.no_grad():",
            "-                t = (torch.tensor(timestep_values[t]) * torch.ones((1, 1))).to(torch_device)",
            "-                residual = self.diffwave(audio, mel_spectrogram, t)",
            "+            ts = (torch.tensor(timestep_values[t]) * torch.ones((1, 1))).to(torch_device)",
            "+            residual = self.diffwave((audio, mel_spectrogram, ts))",
            "",
            "# 2. predict previous mean of audio x_t-1",
            "pred_prev_audio = self.noise_scheduler.step(residual, audio, t)"
        ]
    },
    {
        "number": 2975,
        "comments": "",
        "commit_message": "tacotron parse output bug fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Decoder(nn.Module):",
            "self.attention = inputs.data.new(B, T).zero_()",
            "self.attention_cum = inputs.data.new(B, T).zero_()",
            "",
            "-    def _parse_outputs(self, outputs, stop_tokens, attentions):",
            "+    def _parse_outputs(self, outputs, attentions, stop_tokens):",
            "# Back to batch first",
            "attentions = torch.stack(attentions).transpose(0, 1)",
            "outputs = torch.stack(outputs).transpose(0, 1).contiguous()",
            "-        stop_tokens = torch.stack(stop_tokens).transpose(0, 1)",
            "-        return outputs, stop_tokens, attentions",
            "+        stop_tokens = torch.stack(stop_tokens).transpose(0, 1).squeeze(-1)",
            "+        return outputs, attentions, stop_tokens",
            "",
            "def decode(self,",
            "inputs,"
        ]
    },
    {
        "number": 2980,
        "comments": "",
        "commit_message": "fix initial image in ddim\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class LatentDiffusion(DiffusionPipeline):",
            "num_trained_timesteps = self.noise_scheduler.timesteps",
            "inference_step_times = range(0, num_trained_timesteps, num_trained_timesteps // num_inference_steps)",
            "",
            "-        image = self.noise_scheduler.sample_noise(",
            "+        image = torch.randn(",
            "(batch_size, self.unet.in_channels, self.unet.image_size, self.unet.image_size),",
            "device=torch_device,",
            "generator=generator,"
        ]
    },
    {
        "number": 2982,
        "comments": "",
        "commit_message": "Sift test fix (#341)\n\n* fix filter2d docs\n\n* speed-up patch local feature tests\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TestSIFTDescriptor:",
            "assert_allclose(out, expected, atol=1e-3, rtol=1e-3)",
            "",
            "def test_gradcheck(self):",
            "-        batch_size, channels, height, width = 1, 1, 41, 41",
            "+        batch_size, channels, height, width = 1, 1, 13, 13",
            "patches = torch.rand(batch_size, channels, height, width)",
            "patches = utils.tensor_to_gradcheck_var(patches)  # to var",
            "-        assert gradcheck(sift_describe, (patches, 41),",
            "+        assert gradcheck(sift_describe, (patches, 13),",
            "raise_exception=True)"
        ]
    },
    {
        "number": 2983,
        "comments": "",
        "commit_message": "fixed trace, svd and rint in numpy frontend and trace in frontend (#10442)\n\nThe sum on the return of the torch backend is supposed to be calculated on the main diagonal, set the dim to always be  `-1` \n",
        "label": "",
        "answer": "yes",
        "change": [
            "def trace(",
            "if len(x) == 0:",
            "return ivy.array([])",
            "ret = torch.diagonal(x, offset=offset, dim1=axis1, dim2=axis2)",
            "-    ret = torch.sum(ret)",
            "+    ret = torch.sum(ret, dim=-1)",
            "return ret"
        ]
    },
    {
        "number": 2986,
        "comments": "",
        "commit_message": "[torch] remove deprecated uint8 in favor of bool (#21384)\n\n* uint8 -> bool\n\n* fix copies\n\n* style\n\n* update test modeling commen when checking attention buffers\n\n* style\n\n* use logical not on random mask instead of subtraction with 1\n\n* remove torch uint8\n\n* quality\n\n* remove modified modeling utils\n\n* Update based on review\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n\n---------\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class XSoftmax(torch.autograd.Function):",
            "g, self, r_mask, g.op(\"Constant\", value_t=torch.tensor(torch.finfo(self.type().dtype()).min))",
            ")",
            "output = softmax(g, output, dim)",
            "-        return masked_fill(g, output, r_mask, g.op(\"Constant\", value_t=torch.tensor(0, dtype=torch.uint8)))",
            "+        return masked_fill(g, output, r_mask, g.op(\"Constant\", value_t=torch.tensor(0, dtype=torch.bool)))",
            "",
            "",
            "# Copied from transformers.models.deberta.modeling_deberta.DropoutContext"
        ]
    },
    {
        "number": 2990,
        "comments": "",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_gaussian_hmm_distribution(diag, sample_shape, batch_shape, num_steps, h",
            "actual_std = actual_cov.diagonal(dim1=-2, dim2=-1).sqrt()",
            "actual_corr = actual_cov / (actual_std.unsqueeze(-1) * actual_std.unsqueeze(-2))",
            "",
            "-            expected_cov = g.precision.cholesky().cholesky_inverse()",
            "+            expected_cov = torch.linalg.cholesky(g.precision).cholesky_inverse()",
            "expected_mean = expected_cov.matmul(g.info_vec.unsqueeze(-1)).squeeze(-1)",
            "expected_std = expected_cov.diagonal(dim1=-2, dim2=-1).sqrt()",
            "expected_corr = expected_cov / (expected_std.unsqueeze(-1) * expected_std.unsqueeze(-2))"
        ]
    },
    {
        "number": 3012,
        "comments": "",
        "commit_message": "[rllib] Fix APEX priorities returning zero all the time (#5980)\n\n* fix\n\n* move example tests to end\n\n* level err\n\n* guard against none\n\n* no trace test\n\n* ignore thumbs\n\n* np\n\n* fix multi node\n\n* fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def _convert_to_tf(x):",
            "return x",
            "",
            "if x is not None:",
            "-        x = tf.nest.map_structure(tf.convert_to_tensor, x)",
            "+        x = tf.nest.map_structure(",
            "+            lambda f: tf.convert_to_tensor(f) if f is not None else None, x)",
            "return x"
        ]
    },
    {
        "number": 3017,
        "comments": "",
        "commit_message": "Assure blank is not scored by CTC prefix scorer\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CTCPrefixScoreTH(object):",
            "r_prev, s_prev, f_min_prev, f_max_prev = state",
            "",
            "# select input dimensions for scoring",
            "-        if self.scoring_num > 0 and prep_scores is not None:",
            "-            scoring_ids = torch.topk(prep_scores, self.scoring_num, 1)[1]",
            "+        if self.scoring_num > 0 and pre_scores is not None:",
            "+            pre_scores[:, self.blank] = self.logzero  # ignore blank from pre-selection",
            "+            scoring_ids = torch.topk(pre_scores, self.scoring_num, 1)[1]",
            "scoring_idmap = torch.full((self.n_bb, self.odim), -1, dtype=torch.long, device=self.device)",
            "snum = scoring_ids.size(1)",
            "scoring_idmap[self.bb_idx, scoring_ids] = torch.arange(snum, device=self.device)"
        ]
    },
    {
        "number": 3021,
        "comments": "",
        "commit_message": "fixed mask\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MultiHeadedAttention(nn.Module):",
            "",
            "scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)  # (batch, head, time1, time2)",
            "if mask is not None:",
            "-            mask.unsqueeze_(1).eq_(0)  # (batch, 1, time1, time2)",
            "+            mask = mask.unsqueeze(1).eq(0)  # (batch, 1, time1, time2)",
            "scores = scores.masked_fill(mask, MIN_VALUE)",
            "self.attn = torch.softmax(scores, dim=-1).masked_fill(mask, 0.0)  # (batch, head, time1, time2)",
            "else:",
            "-            self.attn = torch.softmax(scores, dim=-1)",
            "+            self.attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)",
            "",
            "p_attn = self.dropout(self.attn)",
            "x = torch.matmul(p_attn, v)  # (batch, head, time1, d_k)"
        ]
    },
    {
        "number": 3026,
        "comments": "",
        "commit_message": "[rllib] Execute PPO using training workflow (#8206)\n\n* wip\n\n* add kl\n\n* kl\n\n* works now\n\n* doc update\n\n* reorg\n\n* add ddppo\n\n* add stats\n\n* fix fetch\n\n* comment\n\n* fix learner stat regression\n\n* test fixes\n\n* fix test\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Trainer(Trainable):",
            "logger.info(\"Executing eagerly, with eager_tracing={}\".format(",
            "\"True\" if config.get(\"eager_tracing\") else \"False\"))",
            "",
            "-        if tf and not tf.executing_eagerly():",
            "+        if tf and not tf.executing_eagerly() and not config.get(\"use_pytorch\"):",
            "logger.info(\"Tip: set 'eager': true or the --eager flag to enable \"",
            "\"TensorFlow eager execution\")"
        ]
    },
    {
        "number": 3035,
        "comments": "",
        "commit_message": "fix of the rnn_seq2seq in order to be compatible with tensorflow 0.7\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def rnn_seq2seq(encoder_inputs, decoder_inputs, encoder_cell, decoder_cell=None,",
            "List of tensors for outputs and states for trianing and sampling sub-graphs.",
            "\"\"\"",
            "with tf.variable_scope(scope or \"rnn_seq2seq\"):",
            "-        _, enc_states = tf.nn.rnn(encoder_cell, encoder_inputs, dtype=dtype)",
            "-        return rnn_decoder(decoder_inputs, enc_states[-1], decoder_cell or encoder_cell)",
            "-",
            "+        _, last_enc_state = tf.nn.rnn(encoder_cell, encoder_inputs, dtype=dtype)",
            "+        return rnn_decoder(decoder_inputs, last_enc_state, decoder_cell or encoder_cell)"
        ]
    },
    {
        "number": 3043,
        "comments": "",
        "commit_message": "Reproducibility 3/3 (#1924)\n\n* make tests deterministic\n\n* run slow tests\n\n* prepare for testing\n\n* finish\n\n* refactor\n\n* add print statements\n\n* finish more\n\n* correct some test failures\n\n* more fixes\n\n* set up to correct tests\n\n* more corrections\n\n* up\n\n* fix more\n\n* more prints\n\n* add\n\n* up\n\n* up\n\n* up\n\n* uP\n\n* uP\n\n* more fixes\n\n* uP\n\n* up\n\n* up\n\n* up\n\n* up\n\n* fix more\n\n* up\n\n* up\n\n* clean tests\n\n* up\n\n* up\n\n* up\n\n* more fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* make\n\n* correct\n\n* finish\n\n* finish\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PipelineIntegrationTests(unittest.TestCase):",
            "pipe = pipe.to(device)",
            "pipe.set_progress_bar_config(disable=None)",
            "",
            "-        generator = torch.Generator(device=device).manual_seed(0)",
            "+        generator = torch.manual_seed(0)",
            "output = pipe(generator=generator, num_inference_steps=100, audio_length_in_s=4.096)",
            "audio = output.audios",
            "",
            "audio_slice = audio[0, -3:, -3:]",
            "",
            "assert audio.shape == (1, 2, pipe.unet.sample_size)",
            "-        expected_slice = np.array([-0.1693, -0.1698, -0.1447, -0.3044, -0.3203, -0.2937])",
            "+        expected_slice = np.array([-0.0367, -0.0488, -0.0771, -0.0525, -0.0444, -0.0341])",
            "+",
            "assert np.abs(audio_slice.flatten() - expected_slice).max() < 1e-2"
        ]
    },
    {
        "number": 3046,
        "comments": "",
        "commit_message": "Use packaging to handle versions (#2777)\n\n* Get Python version from platform module\n\n* Set PY_VERSION as version class\n\n* Set PYARROW_VERSION as version class\n\n* Set TORCH_VERSION as version class\n\n* Set TF_VERSION as version class\n\n* Set JAX_VERSION as version class\n\n* Set BEAM_VERSION as version class\n\n* Set RARFILE_VERSION as version class\n\n* Use version class to validate PyArrow version at import\n\n* Use version class in SCRIPTS_VERSION at import\n\n* Use config.PYARROW_VERSION for parquet submodules\n\n* Fix style\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class LocalDatasetTest(parameterized.TestCase):",
            "",
            "def get_packaged_dataset_names():",
            "packaged_datasets = [{\"testcase_name\": x, \"dataset_name\": x} for x in _PACKAGED_DATASETS_MODULES.keys()]",
            "-    if version.parse(pa.__version__) < version.parse(\"3.0.0\"):  # parquet is not supported for pyarrow<3.0.0",
            "+    if datasets.config.PYARROW_VERSION.major < 3:  # parquet is not supported for pyarrow<3.0.0",
            "packaged_datasets = [pd for pd in packaged_datasets if pd[\"dataset_name\"] != \"parquet\"]",
            "return packaged_datasets"
        ]
    },
    {
        "number": 3048,
        "comments": "",
        "commit_message": "topk pooling example, bugfixes\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TopKPooling(torch.nn.Module):",
            "",
            "weight = F.normalize(self.weight, p=2, dim=-1)",
            "score = (x * weight).sum(dim=-1)",
            "-        perm = self.topk(score, self.k, batch)",
            "-",
            "-        x = x[perm] * self.tanh(score[perm])",
            "+        perm = self.topk(score, self.ratio, batch)",
            "+        x = x[perm] * torch.tanh(score[perm]).view(-1, 1)",
            "batch = batch[perm]",
            "edge_index, edge_attr = self.filter_adj(",
            "-            edge_index, edge_attr, perm, num_nodes=x.size(0))",
            "+            edge_index, edge_attr, perm, num_nodes=score.size(0))",
            "",
            "-        return x, edge_index, edge_attr, batch",
            "+        return x, edge_index, edge_attr, batch, perm",
            "",
            "def __repr__(self):",
            "return '{}({})'.format(self.__class__.__name__, self.ratio)"
        ]
    },
    {
        "number": 3075,
        "comments": "",
        "commit_message": "Stability improvements (#99)\n\n* bugfix & torch fx implementation\n\n* update torch tensorrt installation\n\n* fix sparseml colab\n\n* fix sparseml training on gpu\n\n* restored lost fixes\n\n* fixed torch_tensorrt install\n\n* fix quantization\n\n* improved tests & bugfix\n\n* prevent deepsparse installation for arm cpu\n\n* bugfix tvm & improved tests\n\n* added tests for tensorflow and onnx\n\n* fix test torchscript\n\n* fix tensorrt static quant\n\n* update notebooks\n\n* fix deepsparse bugs and implemented tests\n\n* add test for sparseml compressor\n\n* bug fixes on tensorflow backend and added tests\n\n* add version limit to tensorflow due to protobuffers 2.x not being supported from tf2onnx\n\n* update onnx version\n\n* remove numpy update\n\n* restored cpu tests\n\n* add python 3.10 in cpu tests\n\n* limit tensorflow gpu usage\n\n* fix python 3.10\n\n* improved tests\n\n* add warmup in model latency computation & add original model latency\n\n* fix pytorch tensorrt for transformers models\n\n* fixed bugs on onnx model handling\n\n* added onnx simplifier to fix tensorrt in onnx pipeline\n\n* fix deepsparse support to NO_COMPILER_INSTALLATION flag\n\n* fix model to onnx conversion problem and tensorrt issue with static quantization\n\n* add valerio citation code\n\n* added readme to notebooks folder\n\n* added tensorflow and onnx notebooks\n\n* style fix\n\n* fix tensor RT bug with static quantization when using new version of polygraphy and update pytorch resnet50 notebook\n\n* fix huggingface bug when passing tokenizer to optimize_model\n\n* updated notebooks readme and bugfix\n\n* minor fixes & added pruning with intel neural compressor\n\n* fixes and added test for intel pruning\n\n* fixes & added neural compressor quantization\n\n* fix test intel pruning compressor\n\n* added tests for neural compressor optimizer & bug fixes\n\n* removed transformers from requirements\n\n* changed openvino dynamic shape\n\n* computing latency using different data\n\n* bugfix openvino\n\n* make onnxsim optional\n\n* install onnx_sim only on intel machines\n\n* added bf16 and dynamic_quantization to neural_compressor\n\n* check output of compiled models\n\n* internal fixes\n\n* fix when no optimized model is found\n\nCo-authored-by: Valerio Sofi <v.sofi@nebuly.ai>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TensorflowBackendInferenceLearner(TensorflowBaseInferenceLearner):",
            "super(TensorflowBackendInferenceLearner, self).__init__(**kwargs)",
            "self.model = tf_model",
            "",
            "-    @tf.function(jit_compile=True)",
            "def run(self, *input_tensors: tf.Tensor) -> Tuple[tf.Tensor, ...]:",
            "-        res = self.model.predict(*input_tensors)",
            "+        res = self.model.predict(input_tensors)",
            "if not isinstance(res, tuple):",
            "return (res,)",
            "return res"
        ]
    },
    {
        "number": 3077,
        "comments": "",
        "commit_message": "test fixed points\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class FixedPoints(object):",
            "choice = np.random.choice(data.num_nodes, self.num, replace=True)",
            "",
            "for key, item in data:",
            "-            if item.size(0) == num_nodes:",
            "+            if torch.is_tensor(item) and item.size(0) == num_nodes:",
            "data[key] = item[choice]",
            "",
            "return data"
        ]
    },
    {
        "number": 3092,
        "comments": "",
        "commit_message": "Fix missed head transpose\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class XLNetRelativeAttention(nn.Module):",
            "",
            "# Mask heads if we want to",
            "if head_mask is not None:",
            "-            attn_prob = attn_prob * head_mask",
            "+            attn_prob = attn_prob * torch.einsum('ijbn->bnij', head_mask)",
            "",
            "# attention output",
            "attn_vec = torch.einsum('bnij,jbnd->ibnd', attn_prob, v_head_h)"
        ]
    },
    {
        "number": 3094,
        "comments": "",
        "commit_message": "Optimize Stable Diffusion (#371)\n\n* initial commit\n\n* make UNet stream capturable\n\n* try to fix noise_pred value\n\n* remove cuda graph and keep NB\n\n* non blocking unet with PNDMScheduler\n\n* make timesteps np arrays for pndm scheduler\nbecause lists don't get formatted to tensors in `self.set_format`\n\n* make max async in pndm\n\n* use channel last format in unet\n\n* avoid moving timesteps device in each unet call\n\n* avoid memcpy op in `get_timestep_embedding`\n\n* add `channels_last` kwarg to `DiffusionPipeline.from_pretrained`\n\n* update TODO\n\n* replace `channels_last` kwarg with `memory_format` for more generality\n\n* revert the channels_last changes to leave it for another PR\n\n* remove non_blocking when moving input ids to device\n\n* remove blocking from all .to() operations at beginning of pipeline\n\n* fix merging\n\n* fix merging\n\n* model can run in other precisions without autocast\n\n* attn refactoring\n\n* Revert \"attn refactoring\"\n\nThis reverts commit 0c70c0e189cd2c4d8768274c9fcf5b940ee310fb.\n\n* remove restriction to run conv_norm in fp32\n\n* use `baddbmm` instead of `matmul`for better in attention for better perf\n\n* removing all reshapes to test perf\n\n* Revert \"removing all reshapes to test perf\"\n\nThis reverts commit 006ccb8a8c6bc7eb7e512392e692a29d9b1553cd.\n\n* add shapes comments\n\n* hardcore whats needed for jitting\n\n* Revert \"hardcore whats needed for jitting\"\n\nThis reverts commit 2fa9c698eae2890ac5f8e367ca80532ecf94df9a.\n\n* Revert \"remove restriction to run conv_norm in fp32\"\n\nThis reverts commit cec592890c32da3d1b78d38b49e4307aedf459b9.\n\n* revert using baddmm in attention's forward\n\n* cleanup comment\n\n* remove restriction to run conv_norm in fp32. no quality loss was noticed\n\nThis reverts commit cc9bc1339c998ebe9e7d733f910c6d72d9792213.\n\n* add more optimizations techniques to docs\n\n* Revert \"add shapes comments\"\n\nThis reverts commit 31c58eadb8892f95478cdf05229adf678678c5f4.\n\n* apply suggestions\n\n* make quality\n\n* apply suggestions\n\n* styling\n\n* `scheduler.timesteps` are now arrays so we dont need .to()\n\n* remove useless .type()\n\n* use mean instead of max in `test_stable_diffusion_inpaint_pipeline_k_lms`\n\n* move scheduler timestamps to correct device if tensors\n\n* add device to `set_timesteps` in LMSD scheduler\n\n* `self.scheduler.set_timesteps` now uses device arg for schedulers that accept it\n\n* quick fix\n\n* styling\n\n* remove kwargs from schedulers `set_timesteps`\n\n* revert to using max in K-LMS inpaint pipeline test\n\n* Revert \"`self.scheduler.set_timesteps` now uses device arg for schedulers that accept it\"\n\nThis reverts commit 00d5a51e5c20d8d445c8664407ef29608106d899.\n\n* move timesteps to correct device before loop in SD pipeline\n\n* apply previous fix to other SD pipelines\n\n* UNet now accepts tensor timesteps even on wrong device, to avoid errors\n- it shouldnt affect performance if timesteps are alrdy on correct device\n- it does slow down performance if they're on the wrong device\n\n* fix pipeline when timesteps are arrays with strides\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class LMSDiscreteScheduler(SchedulerMixin, ConfigMixin):",
            "sigmas = np.array(((1 - self.alphas_cumprod) / self.alphas_cumprod) ** 0.5)",
            "sigmas = np.interp(timesteps, np.arange(0, len(sigmas)), sigmas)",
            "sigmas = np.concatenate([sigmas, [0.0]]).astype(np.float32)",
            "-        self.sigmas = torch.from_numpy(sigmas)",
            "-        self.timesteps = torch.from_numpy(timesteps)",
            "+        self.sigmas = torch.from_numpy(sigmas).to(device=device)",
            "+        self.timesteps = torch.from_numpy(timesteps).to(device=device)",
            "",
            "self.derivatives = []"
        ]
    },
    {
        "number": 3100,
        "comments": "",
        "commit_message": "Use preprocessing layers for categorical encoding (#1090)\n\n* removed sigmoid layer\n\n* added lookup\n\n* bug fix\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ClassificationHead(head_module.Head):",
            "output_node = layers.Dropout(dropout_rate)(output_node)",
            "output_node = layers.Dense(self.output_shape[-1])(output_node)",
            "if self.loss == 'binary_crossentropy':",
            "-            output_node = keras_layers.Sigmoid(name=self.name)(output_node)",
            "+            output_node = layers.Activation(activations.sigmoid,",
            "+                                            name=self.name)(output_node)",
            "else:",
            "output_node = layers.Softmax(name=self.name)(output_node)",
            "return output_node"
        ]
    },
    {
        "number": 3104,
        "comments": "",
        "commit_message": "fixed lm model loading in cpu calculation\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def recog(args):",
            "if args.rnnlm:",
            "rnnlm = lm_train_th.ClassifierWithState(",
            "lm_train_th.RNNLM(len(train_args.char_list), 650))",
            "-        rnnlm.load_state_dict(torch.load(args.rnnlm))",
            "+        rnnlm.load_state_dict(torch.load(args.rnnlm, map_location=cpu_loader))",
            "else:",
            "rnnlm = None"
        ]
    },
    {
        "number": 3118,
        "comments": "",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TestBiMPMMatching(AllenNlpTestCase):",
            "test1_fw, test1_bw = torch.split(test1, d // 2, dim=-1)",
            "test2_fw, test2_bw = torch.split(test2, d // 2, dim=-1)",
            "",
            "-        ml_fw = BiMpmMatching.from_params(Params({\"is_forward\": True, \"num_perspectives\": l}))",
            "-        ml_bw = BiMpmMatching.from_params(Params({\"is_forward\": False, \"num_perspectives\": l}))",
            "+        ml_fw = BiMpmMatching.from_params(Params({\"is_forward\": True, \"num_perspectives\": n}))",
            "+        ml_bw = BiMpmMatching.from_params(Params({\"is_forward\": False, \"num_perspectives\": n}))",
            "",
            "vecs_p_fw, vecs_h_fw = ml_fw(test1_fw, mask1, test2_fw, mask2)",
            "vecs_p_bw, vecs_h_bw = ml_bw(test1_bw, mask1, test2_bw, mask2)",
            "vecs_p, vecs_h = torch.cat(vecs_p_fw + vecs_p_bw, dim=2), torch.cat(vecs_h_fw + vecs_h_bw, dim=2)",
            "",
            "-        assert vecs_p.size() == torch.Size([batch, len1, 10 + 10 * l])",
            "-        assert vecs_h.size() == torch.Size([batch, len2, 10 + 10 * l])",
            "+        assert vecs_p.size() == torch.Size([batch, len1, 10 + 10 * n])",
            "+        assert vecs_h.size() == torch.Size([batch, len2, 10 + 10 * n])",
            "assert ml_fw.get_output_dim() == ml_bw.get_output_dim() == vecs_p.size(2) // 2 == vecs_h.size(2) // 2"
        ]
    },
    {
        "number": 3156,
        "comments": "",
        "commit_message": "Fix Fairseq FP16 Optimizer (#1460)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/1460\n\nAccounting for D23593137 in building FP16 PyText Optimizer\n\nReviewed By: mwu1993\n\nDifferential Revision: D23727851\n\nfbshipit-source-id: bc9445a3f32da8b18524fcfc2fd9eb99975d0362\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class FP16OptimizerFairseq(Fairseq_FP16OptimizerMixin, FP16Optimizer):",
            "",
            "# reset fp32_optimizer param groups to using master weights",
            "fp32_param_group = self.fp32_optimizer.param_groups[0]",
            "-        fp32_param_group[\"params\"] = [self.fp32_params]",
            "+        fp32_param_group[\"params\"] = [self.fp32_params[torch.cuda.current_device()]]",
            "self.fp32_optimizer.param_groups = []",
            "self.fp32_optimizer.add_param_group(fp32_param_group)"
        ]
    },
    {
        "number": 3161,
        "comments": "",
        "commit_message": "bugfix in ori\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class LAFOrienter(nn.Module):",
            "self.patch_size,",
            "self.patch_size)",
            "angles_radians: torch.Tensor = self.angle_detector(patches).view(B, N)",
            "-        laf_out: torch.Tensor = set_laf_orientation(laf, rad2deg(angles_radians))",
            "+        prev_angle = get_laf_orientation(laf).view_as(angles_radians)",
            "+        laf_out: torch.Tensor = set_laf_orientation(laf, rad2deg(angles_radians) + prev_angle)",
            "return laf_out"
        ]
    },
    {
        "number": 3184,
        "comments": "",
        "commit_message": "Add usage of batch norm in conv test and fix usage of is_training collection\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class OpsTest(tf.test.TestCase):",
            "filter_shape = (5, 5)",
            "vals = np.random.randn(batch_size, input_shape[0], input_shape[1], 1)",
            "with self.test_session() as sess:",
            "+            tf.add_to_collection(\"IS_TRAINING\", True)",
            "tensor_in = tf.placeholder(tf.float32, [batch_size, input_shape[0],",
            "input_shape[1], 1])",
            "-            res = ops.conv2d(tensor_in, n_filters, filter_shape)",
            "+            res = ops.conv2d(",
            "+                tensor_in, n_filters, filter_shape, batch_norm=True)",
            "sess.run(tf.initialize_all_variables())",
            "conv = sess.run(res, feed_dict={tensor_in.name: vals})",
            "self.assertEqual(conv.shape, (batch_size, input_shape[0],"
        ]
    },
    {
        "number": 3187,
        "comments": "",
        "commit_message": "Add TFEncoderDecoderModel + Add cross-attention to some TF models (#13222)\n\n* Add cross attentions to TFGPT2Model\n\n* Add TFEncoderDecoderModel\n\n* Add TFBaseModelOutputWithPoolingAndCrossAttentions\n\n* Add cross attentions to TFBertModel\n\n* Fix past or past_key_values argument issue\n\n* Fix generation\n\n* Fix save and load\n\n* Add some checks and comments\n\n* Clean the code that deals with past keys/values\n\n* Add kwargs to processing_inputs\n\n* Add serving_output to TFEncoderDecoderModel\n\n* Some cleaning + fix use_cache value issue\n\n* Fix tests + add bert2bert/bert2gpt2 tests\n\n* Fix more tests\n\n* Ignore crossattention.bias when loading GPT2 weights into TFGPT2\n\n* Fix return_dict_in_generate in tf generation\n\n* Fix is_token_logit_eos_token bug in tf generation\n\n* Finalize the tests after fixing some bugs\n\n* Fix another is_token_logit_eos_token bug in tf generation\n\n* Add/Update docs\n\n* Add TFBertEncoderDecoderModelTest\n\n* Clean test script\n\n* Add TFEncoderDecoderModel to the library\n\n* Add cross attentions to TFRobertaModel\n\n* Add TFRobertaEncoderDecoderModelTest\n\n* make style\n\n* Change the way of position_ids computation\n\n* bug fix\n\n* Fix copies in tf_albert\n\n* Remove some copied from and apply some fix-copies\n\n* Remove some copied\n\n* Add cross attentions to some other TF models\n\n* Remove encoder_hidden_states from TFLayoutLMModel.call for now\n\n* Make style\n\n* Fix TFRemBertForCausalLM\n\n* Revert the change to longformer + Remove copies\n\n* Revert the change to albert and convbert + Remove copies\n\n* make quality\n\n* make style\n\n* Add TFRembertEncoderDecoderModelTest\n\n* make quality and fix-copies\n\n* test TFRobertaForCausalLM\n\n* Fixes for failed tests\n\n* Fixes for failed tests\n\n* fix more tests\n\n* Fixes for failed tests\n\n* Fix Auto mapping order\n\n* Fix TFRemBertEncoder return value\n\n* fix tf_rembert\n\n* Check copies are OK\n\n* Fix missing TFBaseModelOutputWithPastAndCrossAttentions is not defined\n\n* Add TFEncoderDecoderModelSaveLoadTests\n\n* fix tf weight loading\n\n* check the change of use_cache\n\n* Revert the change\n\n* Add missing test_for_causal_lm for TFRobertaModelTest\n\n* Try cleaning past\n\n* fix _reorder_cache\n\n* Revert some files to original versions\n\n* Keep as many copies as possible\n\n* Apply suggested changes - Use raise ValueError instead of assert\n\n* Move import to top\n\n* Fix wrong require_torch\n\n* Replace more assert by raise ValueError\n\n* Add test_pt_tf_model_equivalence (the test won't pass for now)\n\n* add test for loading/saving\n\n* finish\n\n* finish\n\n* Remove test_pt_tf_model_equivalence\n\n* Update tf modeling template\n\n* Remove pooling, added in the prev. commit, from MainLayer\n\n* Update tf modeling test template\n\n* Move inputs[\"use_cache\"] = False to modeling_tf_utils.py\n\n* Fix torch.Tensor in the comment\n\n* fix use_cache\n\n* Fix missing use_cache in ElectraConfig\n\n* Add a note to from_pretrained\n\n* Fix style\n\n* Change test_encoder_decoder_save_load_from_encoder_decoder_from_pt\n\n* Fix TFMLP (in TFGPT2) activation issue\n\n* Fix None past_key_values value in serving_output\n\n* Don't call get_encoderdecoder_model in TFEncoderDecoderModelTest.test_configuration_tie until we have a TF checkpoint on Hub\n\n* Apply review suggestions - style for cross_attns in serving_output\n\n* Apply review suggestions - change assert + docstrings\n\n* break the error message to respect the char limit\n\n* deprecate the argument past\n\n* fix docstring style\n\n* Update the encoder-decoder rst file\n\n* fix Unknown interpreted text role \"method\"\n\n* fix typo\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFConvBertEmbeddings(tf.keras.layers.Layer):",
            "token_type_ids = tf.fill(dims=input_shape, value=0)",
            "",
            "if position_ids is None:",
            "-            position_ids = tf.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)",
            "+            position_ids = tf.expand_dims(",
            "+                tf.range(start=past_key_values_length, limit=input_shape[1] + past_key_values_length), axis=0",
            "+            )",
            "",
            "position_embeds = tf.gather(params=self.position_embeddings, indices=position_ids)",
            "position_embeds = tf.tile(input=position_embeds, multiples=(input_shape[0], 1, 1))"
        ]
    },
    {
        "number": 3193,
        "comments": "",
        "commit_message": "Fix T5/mT5 tests (#18029)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFMT5ModelIntegrationTest(unittest.TestCase):",
            "labels = tokenizer(\"Hi I am\", return_tensors=\"tf\").input_ids",
            "",
            "loss = model(input_ids, labels=labels).loss",
            "-        mtf_score = -tf.math.reduce_sum(loss).numpy()",
            "+        mtf_score = -tf.math.reduce_mean(loss).numpy()",
            "",
            "-        EXPECTED_SCORE = -84.9127",
            "+        EXPECTED_SCORE = -21.210594",
            "self.assertTrue(abs(mtf_score - EXPECTED_SCORE) < 2e-4)"
        ]
    },
    {
        "number": 3209,
        "comments": "",
        "commit_message": "Add Image feature (#3163)\n\n* Initial commit\n\n* Add basic decoding\n\n* Replace features.Audio with Audio\n\n* Add Image to package reference\n\n* Use np.array\n\n* Update error msg\n\n* Add mode and channel decoding\n\n* Fix return value\n\n* Finish decoding\n\n* Make CI happy\n\n* Some more fixes\n\n* Minor doc fix\n\n* Remove animated option\n\n* Pin version\n\n* Remove unused imports in setup.py\n\n* Add vision requirements to setup.py\n\n* Add initial tests\n\n* Delete other formats\n\n* Make Image feature hashable\n\n* Add more tests\n\n* Support numpy array in alter data check in TypedSequence\n\n* Fix TypedSequence converion\n\n* Finish tests\n\n* Update Image - add ImageExtensionType and supporting functions\n\n* Update encoding functions\n\n* Add support in TypedSequence for ImageExtensionType\n\n* Add tests\n\n* Remove unused import\n\n* Fix doc and style\n\n* Fix doc indentation\n\n* Improve comment\n\n* Return single image instead of dict\n\n* Return PIL Image and not dict\n\n* Encode dict\n\n* Update tests\n\n* Style\n\n* np.ndarray encoding/decoding\n\n* Minor improvements\n\n* PIL Image support in cast_to_python_objects\n\n* Test cast\n\n* Doc fix\n\n* Extension type fixes\n\n* Style\n\n* Use types_mapper in Dataset.to_pandas\n\n* Add pandas extension array for image type\n\n* Update tests\n\n* image type inference\n\n* Remvoe cast_to_python test after Quentin's change\n\n* Improve tests\n\n* Add storage type\n\n* Improve tests\n\n* Test map that returns np.ndarray\n\n* Rename functions\n\n* Add streaming test\n\n* Use image struct in all situations\n\n* Update src/datasets/features/image.py - encode_example type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Update src/datasets/features/image.py -list_image_compression_formats type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Support str in encode_objects_to_image_dicts\n\n* Update src/datasets/features/image.py - objects_to_list_of_image_dicts type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Style\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Covost2(datasets.GeneratorBasedBuilder):",
            "features=datasets.Features(",
            "client_id=datasets.Value(\"string\"),",
            "file=datasets.Value(\"string\"),",
            "-                audio=datasets.features.Audio(sampling_rate=16_000),",
            "+                audio=datasets.Audio(sampling_rate=16_000),",
            "sentence=datasets.Value(\"string\"),",
            "translation=datasets.Value(\"string\"),",
            "id=datasets.Value(\"string\"),"
        ]
    },
    {
        "number": 3220,
        "comments": "",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Iterative(Solver):",
            "next_step = self.next_step(*args)",
            "step = (lambda: self.step(*args))",
            "do_nothing = (lambda: args)",
            "-                args = tf.cond(pred=next_step, true_fn=step, false_fn=do_nothing)",
            "+                args = self.cond(pred=next_step, true_fn=step, false_fn=do_nothing)",
            "",
            "else:",
            "# TensorFlow while loop",
            "-            args = tf.while_loop(",
            "+            args = self.while_loop(",
            "cond=self.next_step, body=self.step, loop_vars=args,",
            "maximum_iterations=self.max_iterations",
            ")"
        ]
    },
    {
        "number": 3223,
        "comments": "",
        "commit_message": "dist.destroy_process_group() bug fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def train(hyp):",
            "if not opt.evolve:",
            "plot_results()  # save as results.png",
            "print('%g epochs completed in %.3f hours.\\n' % (epoch - start_epoch + 1, (time.time() - t0) / 3600))",
            "-    dist.destroy_process_group() if torch.cuda.device_count() > 1 else None",
            "+    dist.destroy_process_group() if device.type != 'cpu' and torch.cuda.device_count() > 1 else None",
            "torch.cuda.empty_cache()",
            "return results"
        ]
    },
    {
        "number": 3236,
        "comments": "",
        "commit_message": "[T5] Fix speed degradation bug t5 (#10496)\n\n* fix speed degradation bug t5\n\n* fix for all models\n\n* fix code quality\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class T5Block(nn.Module):",
            "",
            "# Apply Feed Forward layer",
            "hidden_states = self.layer[-1](hidden_states)",
            "-        if torch.isinf(hidden_states).any():",
            "+",
            "+        # clamp inf values to enable fp16 training",
            "+        if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():",
            "clamp_value = torch.finfo(hidden_states.dtype).max - 1000",
            "hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)",
            "+",
            "outputs = (hidden_states,)",
            "",
            "outputs = outputs + (present_key_value_state,) + attention_outputs"
        ]
    },
    {
        "number": 3238,
        "comments": "",
        "commit_message": "fixing doctest in pinhole (#1743)\n\n* fixing doctest in pinhole\n\n* Update pinhole.py\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PinholeCamera:",
            ">>> _ = torch.manual_seed(0)",
            ">>> x = torch.rand(1, 2)",
            ">>> depth = torch.ones(1, 1)",
            "-            >>> I = torch.eye(4)[None]",
            "+            >>> K = torch.eye(4)[None]",
            ">>> E = torch.eye(4)[None]",
            ">>> h = torch.ones(1)",
            ">>> w = torch.ones(1)",
            ">>> pinhole = kornia.geometry.camera.PinholeCamera(K, E, h, w)",
            "-            >>> pinhole.unproject_points(x, depth)",
            "+            >>> pinhole.unproject(x, depth)",
            "tensor([[0.4963, 0.7682, 1.0000]])",
            "\"\"\"",
            "P = self.intrinsics @ self.extrinsics"
        ]
    },
    {
        "number": 3239,
        "comments": "",
        "commit_message": "Fix module mismatch in hook for func like torch.argmax\n>\n>\nCo-authored-by: Alexis Thual <alexisthual@gmail.com>\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TorchHook:",
            "if type(native_func) in [types.FunctionType, types.BuiltinFunctionType]:",
            "# 3. Build the hooked function",
            "new_func = self.get_hooked_func(native_func)",
            "-                # 4. Move the native function",
            "-                setattr(torch_module, f\"native_{func}\", native_func)",
            "+                # 4. Move the native function to its original module",
            "+                # /!\\ Can be different from the torch_module!",
            "+                # Ex: in torch.py `torch.argmax = torch.functional.argmax`",
            "+                # ... So torch.argmax.__module__ is 'torch.functional' != 'torch'",
            "+                setattr(eval(native_func.__module__), f\"native_{func}\", native_func)",
            "# 5. Put instead the hooked one",
            "setattr(torch_module, func, new_func)"
        ]
    },
    {
        "number": 3240,
        "comments": "",
        "commit_message": "bugfix/3185 transpose (#3252)\n\n* change t() to transpose() as xla devices do not support .t() on 1-dim tensor\n\n* detach tensor before copying\n\n* Revert \"detach tensor before copying\"\n\nThis reverts commit 37cc7bbe\n\n* changed dims\n\n* added test_result_obj_on_tpu\n\n* detach before copying\n\n* detach before copying\n\n* detach before copying\n\n* replace torch.cat with sum\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class EvalResult(Result):",
            "",
            "def weighted_mean(result, weights):",
            "weights = weights.to(result.device)",
            "-    numerator = torch.dot(result.float(), weights.t().float())",
            "+    numerator = torch.dot(result.float(), weights.transpose(-1, 0).float())",
            "result = numerator / weights.sum().float()",
            "return result"
        ]
    },
    {
        "number": 3242,
        "comments": "",
        "commit_message": "fix DistributedTrainer (fix #505)\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TowerContext(object):",
            "global _CurrentTowerContext",
            "assert _CurrentTowerContext is None, \"Cannot nest TowerContext!\"",
            "_CurrentTowerContext = self",
            "-        curr_vs = tf.get_variable_scope()",
            "-        assert curr_vs.name == '', \"Cannot nest TowerContext with an existing variable scope!\"",
            "+        if self.is_training:",
            "+            curr_vs = tf.get_variable_scope()",
            "+            assert curr_vs.name == '', \"In training, cannot nest TowerContext with an existing variable scope!\"",
            "",
            "self._ctxs = self._get_scopes()",
            "self._ctxs.append(self._collection_guard)"
        ]
    },
    {
        "number": 3244,
        "comments": "",
        "commit_message": "Fixes to Multihead Attention with LayerNorm and Dropout-Add (#860)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class SelfMultiheadAttn(nn.Module):",
            "self.register_parameter('lyr_norm_beta_weights', None)",
            "self.lyr_nrm_gamma_weights = None",
            "self.lyr_nrm_beta_weights  = None",
            "-                self.lyr_nrm = torch.nn.LayerNorm(embed_dim)",
            "+                self.lyr_nrm = FusedLayerNorm(embed_dim)",
            "self.reset_parameters()",
            "",
            "if self.include_norm_add:"
        ]
    },
    {
        "number": 3250,
        "comments": "",
        "commit_message": "batch and test fixed\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ScaleSpaceDetector(nn.Module):",
            "max_coords_best = _scale_index_to_scale(max_coords_best, sigmas_oct)",
            "",
            "# Create local affine frames (LAFs)",
            "-            rotmat = angle_to_rotation_matrix(torch.zeros(B, N))",
            "+            rotmat = angle_to_rotation_matrix(torch.zeros(B, N).to(max_coords_best.device).to(max_coords_best.dtype))",
            "current_lafs = torch.cat([self.mr_size * max_coords_best[:, :, 0].view(B, N, 1, 1) * rotmat,",
            "max_coords_best[:, :, 1:3].view(B, N, 2, 1)], dim=3)",
            "# Normalize LAFs"
        ]
    },
    {
        "number": 3251,
        "comments": "",
        "commit_message": "fixed BertForMultipleChoice model init and forward pass\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BertForMultipleChoice(BertPreTrainedModel):",
            "self.num_choices = num_choices",
            "self.bert = BertModel(config)",
            "self.dropout = nn.Dropout(config.hidden_dropout_prob)",
            "-        self.classifier = nn.Linear(config.hidden_size, 1)",
            "+        self.classifier = nn.Linear(config.hidden_size, num_choices)",
            "self.apply(self.init_bert_weights)",
            "",
            "def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):",
            "flat_input_ids = input_ids.view(-1, input_ids.size(-1))",
            "-        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))",
            "-        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1))",
            "+        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None",
            "+        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None",
            "_, pooled_output = self.bert(flat_input_ids, flat_token_type_ids, flat_attention_mask, output_all_encoded_layers=False)",
            "pooled_output = self.dropout(pooled_output)",
            "logits = self.classifier(pooled_output)"
        ]
    },
    {
        "number": 3255,
        "comments": "",
        "commit_message": "(mostly) remove from_params (#1191)\n\n* remove from_params\n\n* more progress on merge\n\n* still not working\n\n* get more tests to pass\n\n* more work\n\n* fixing tests\n\n* more\n\n* progress\n\n* yo\n\n* all tests passing\n\n* remove print statements\n\n* get things working\n\n* oops\n\n* more cleanup\n\n* pylint cleanup\n\n* break stuff out\n\n* add some from_params tests\n\n* add missing api doc\n\n* appease the CI overlords\n\n* remove weird empty file\n\n* address pr feedback\n\n* more PR feedback\n\n* remove from_params from ESIM model\n\n* mypy and pylint\n\n* sphinx\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class LearningRateScheduler(Registrable):",
            "self.lr_scheduler.step_batch(batch_num_total)",
            "return",
            "",
            "+    # Requires custom from_params",
            "@classmethod",
            "-    def from_params(cls, optimizer: torch.optim.Optimizer, params: Params):",
            "+    def from_params(cls, optimizer: torch.optim.Optimizer, params: Params):  # type: ignore",
            "+        # pylint: disable=arguments-differ",
            "scheduler = params.pop_choice(\"type\", LearningRateScheduler.list_available())",
            "",
            "schedulers = LearningRateScheduler.by_name(scheduler)(optimizer, **params.as_dict())  # type: ignore"
        ]
    },
    {
        "number": 3257,
        "comments": "",
        "commit_message": "Fix #7304 (#7305)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Trainer:",
            "elif is_torch_tpu_available():",
            "# tpu-comment: Get all predictions and labels from all worker shards of eval dataset",
            "if preds is not None:",
            "-                preds = nested_xla_mesh_reduce(\"eval_preds\", preds)",
            "+                preds = nested_xla_mesh_reduce(preds, \"eval_preds\")",
            "if label_ids is not None:",
            "-                label_ids = nested_xla_mesh_reduce(\"eval_label_ids\", label_ids, torch.cat)",
            "+                label_ids = nested_xla_mesh_reduce(label_ids, \"eval_label_ids\")",
            "if eval_losses is not None:",
            "eval_losses = xm.mesh_reduce(\"eval_losses\", torch.tensor(eval_losses), torch.cat).tolist()"
        ]
    },
    {
        "number": 3269,
        "comments": "",
        "commit_message": "fixed affine 2d shearing matrix translations (#612)\n\n* fixed affine 2d shearing matrix translations\n\n* Updated test_affine.py\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def get_affine_matrix2d(translations: torch.Tensor, center: torch.Tensor, scale:",
            "sy_tan = torch.tan(sy)  # type: ignore",
            "zeros = torch.zeros_like(sx)  # type: ignore",
            "ones = torch.ones_like(sx)  # type: ignore",
            "-        shear_mat = torch.stack([ones, -sx_tan, sx_tan * x,  # type: ignore   # noqa: E241",
            "-                                 -sy_tan, ones + sx_tan * sy_tan, sy_tan * (-sx_tan * x + y)],  # noqa: E241",
            "+        shear_mat = torch.stack([ones, -sx_tan, sx_tan * y,  # type: ignore   # noqa: E241",
            "+                                 -sy_tan, ones + sx_tan * sy_tan, sy_tan * (sx_tan * y + x)],  # noqa: E241",
            "dim=-1).view(-1, 2, 3)",
            "shear_mat = convert_affinematrix_to_homography(shear_mat)",
            "transform_h = transform_h @ shear_mat"
        ]
    },
    {
        "number": 3271,
        "comments": "",
        "commit_message": "Mask t5 relative position bias then head pruned (#17968)\n\n* add position bias head masking if heads pruned\n\n* fix pruning function in t5 encoder\n\n* make style\n\n* make fix-copies\n\n* Revert added folder\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class T5EncoderModel(T5PreTrainedModel):",
            "class PreTrainedModel",
            "\"\"\"",
            "for layer, heads in heads_to_prune.items():",
            "-            self.encoder.layer[layer].attention.prune_heads(heads)",
            "+            self.encoder.block[layer].layer[0].SelfAttention.prune_heads(heads)",
            "",
            "@add_start_docstrings_to_model_forward(T5_ENCODER_INPUTS_DOCSTRING)",
            "@replace_return_docstrings(output_type=BaseModelOutput, config_class=_CONFIG_FOR_DOC)"
        ]
    },
    {
        "number": 3274,
        "comments": "",
        "commit_message": "Make most metrics work on GPU (#3851)\n\n* Make most metrics work on GPU\n\n* Make metric tests work on both GPU and CPU\n\n* Add a test for the test utility\n\n* mypy\n\n* Update allennlp/common/testing/test_case.py\n\nCo-Authored-By: Mark Neumann <markn@allenai.org>\n\n* Fix a PR comment\n\n* flake8\n\nCo-authored-by: Mark Neumann <markn@allenai.org>\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Auc(Metric):",
            "if self._all_gold_labels.shape[0] == 0:",
            "return 0.5",
            "false_positive_rates, true_positive_rates, _ = metrics.roc_curve(",
            "-            self._all_gold_labels.numpy(),",
            "-            self._all_predictions.numpy(),",
            "+            self._all_gold_labels.cpu().numpy(),",
            "+            self._all_predictions.cpu().numpy(),",
            "pos_label=self._positive_label,",
            ")",
            "auc = metrics.auc(false_positive_rates, true_positive_rates)"
        ]
    },
    {
        "number": 3277,
        "comments": "",
        "commit_message": "Fixed failing test for statistical einsum (#6073)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def einsum(",
            "*operands: torch.Tensor,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "+    dtype = _get_promoted_type_of_operands(operands)",
            "operands = (operand.to(torch.float32) for operand in operands)",
            "-    return torch.einsum(equation, *operands)",
            "+    return torch.einsum(equation, *operands).to(dtype)"
        ]
    },
    {
        "number": 3282,
        "comments": "",
        "commit_message": "Bug fixes\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class AttentionRNNCell(nn.Module):",
            "memory_dim (int): memory vector (decoder autogression) feature dimension.",
            "align_model (str): 'b' for Bahdanau, 'ls' Location Sensitive alignment.",
            "\"\"\"",
            "-        super(AttentionRNN, self).__init__()",
            "+        super(AttentionRNNCell, self).__init__()",
            "self.align_model = align_model",
            "self.rnn_cell = nn.GRUCell(out_dim + memory_dim, out_dim)",
            "# pick bahdanau or location sensitive attention"
        ]
    },
    {
        "number": 3284,
        "comments": "",
        "commit_message": "[gpu] Fixup fdd61b19928e87a5354c36923182e801bfedb31b\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class GPT2ModelTest(ModelTesterMixin, unittest.TestCase):",
            "",
            "# append to next input_ids and attn_mask",
            "next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)",
            "-            attn_mask = torch.cat([attn_mask, torch.ones((attn_mask.shape[0], 1)).long()], dim=1)",
            "+            attn_mask = torch.cat(",
            "+                [attn_mask, torch.ones((attn_mask.shape[0], 1), dtype=torch.long, device=torch_device)], dim=1",
            "+            )",
            "",
            "# get two different outputs",
            "output_from_no_past, _ = model(next_input_ids, attention_mask=attn_mask)"
        ]
    },
    {
        "number": 3297,
        "comments": "",
        "commit_message": "Fix ELBO term scaling\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "D_prior = tf.nn.sigmoid(D(X, z))",
            "X_samples, _ = P(z)",
            "",
            "disc = tf.reduce_mean(-D_sample)",
            "-loglike = -tf.reduce_mean(",
            "-    tf.nn.sigmoid_cross_entropy_with_logits(logits=X_logits, labels=X)",
            "+nll = tf.reduce_sum(",
            "+    tf.nn.sigmoid_cross_entropy_with_logits(logits=X_logits, labels=X),",
            "+    axis=1",
            ")",
            "+loglike = -tf.reduce_mean(nll)",
            "",
            "elbo = disc + loglike",
            "D_loss = tf.reduce_mean(log(D_q) + log(1. - D_prior))"
        ]
    },
    {
        "number": 3301,
        "comments": "",
        "commit_message": "Fix heuristic in util.get_token_ids_from_text_field_tensors (#4184)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ELMoTokenCharactersIndexer(TokenIndexer):",
            "def padding_token():",
            "return [0] * ELMoCharacterMapper.max_word_length",
            "",
            "-        tensor_dict[\"tokens\"] = torch.LongTensor(",
            "+        tensor_dict[\"elmo_tokens\"] = torch.LongTensor(",
            "pad_sequence_to_length(",
            "-                tokens[\"tokens\"], padding_lengths[\"tokens\"], default_value=padding_token",
            "+                tokens[\"elmo_tokens\"], padding_lengths[\"elmo_tokens\"], default_value=padding_token",
            ")",
            ")",
            "return tensor_dict"
        ]
    },
    {
        "number": 3310,
        "comments": "",
        "commit_message": "Make tutorials about encrypted DNN faster using FSS (#3933)\n\n* Improve Enc Lin Alg tutorial efficiency by 10%\n\n* Update tutorials on encryption computation to feature FSS\n\n* Update tutorial 11\n\n* Fix enc lin reg\n\n* Revert Enc lin alg to snn\n\n* Make fss keygen parallel\n\n* rm print\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PrimitiveStorage:",
            "assert (",
            "n_party == 2",
            "), f\"The FSS protocol only works for 2 workers, {n_party} were provided.\"",
            "-            alpha, s_00, s_01, *CW = fss_class.keygen(n_values=n_instances)",
            "+            alpha, s_00, s_01, *CW = sy.frameworks.torch.mpc.fss.keygen(n_values=n_instances, op=op)",
            "# simulate sharing TODO clean this",
            "mask = np.random.randint(0, 2 ** n, alpha.shape, dtype=alpha.dtype)",
            "return [((alpha - mask) % 2 ** n, s_00, *CW), (mask, s_01, *CW)]"
        ]
    },
    {
        "number": 3313,
        "comments": "",
        "commit_message": "YOLOv5 Apple Metal Performance Shader (MPS) support (#7878)\n\n* Apple Metal Performance Shader (MPS) device support\n\nFollowing https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/\n\nShould work with Apple M1 devices with PyTorch nightly installed with command `--device mps`. Usage examples:\n```bash\npython train.py --device mps\npython detect.py --device mps\npython val.py --device mps\n```\n\n* Update device strategy to fix MPS issue\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def select_device(device='', batch_size=0, newline=True):",
            "for i, d in enumerate(devices):",
            "p = torch.cuda.get_device_properties(i)",
            "s += f\"{'' if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / (1 << 20):.0f}MiB)\\n\"  # bytes to MB",
            "+    elif mps:",
            "+        s += 'MPS\\n'",
            "else:",
            "s += 'CPU\\n'",
            "",
            "if not newline:",
            "s = s.rstrip()",
            "LOGGER.info(s.encode().decode('ascii', 'ignore') if platform.system() == 'Windows' else s)  # emoji-safe",
            "-    return torch.device('cuda:0' if cuda else 'cpu')",
            "+    return torch.device('cuda:0' if cuda else 'mps' if mps else 'cpu')",
            "",
            "",
            "def time_sync():"
        ]
    },
    {
        "number": 3315,
        "comments": "",
        "commit_message": "Fix wrong variable collection in distributed training (#431)\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def _get_cached_vs(name):",
            "@contextmanager",
            "def _enter_vs_reuse_ns(name):",
            "vs = _get_cached_vs(name)",
            "+    # XXX Not good to enter the cached vs directly, because this will clean-up custom getter",
            "+    # with tf.variable_scope(name, reuse=tf.AUTO_REUSE):    # available in 1.4 only",
            "with tf.variable_scope(vs):",
            "with tf.name_scope(vs.original_name_scope):",
            "yield vs"
        ]
    },
    {
        "number": 3325,
        "comments": "",
        "commit_message": "Fixes to enable dev branch to move to pytorch-1.2 (#2001)\n\n* Fixes to enable dev branch to move to pytorch-1.2\n\n* change name for published cpu wheels\n\n* fix failing unit tests\n\n* change docs conf\n\n* change tracking example\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_elbo_zip(gate, rate):",
            "dist1 = dist.Delta(torch.tensor(0.))",
            "dist0 = dist.Poisson(rate)",
            "with pyro.plate(\"data\", len(data)):",
            "-            mask = pyro.sample(\"mask\", dist.Bernoulli(gate), infer={\"enumerate\": \"parallel\"}).byte()",
            "+            mask = pyro.sample(\"mask\", dist.Bernoulli(gate), infer={\"enumerate\": \"parallel\"}).bool()",
            "pyro.sample(\"obs\", dist.MaskedMixture(mask, dist0, dist1), obs=data)",
            "",
            "def guide(data):",
            "pass",
            "",
            "-    gate = pyro.param(\"gate\", torch.tensor(gate), constraint=constraints.unit_interval)",
            "-    rate = pyro.param(\"rate\", torch.tensor(rate), constraint=constraints.positive)",
            "+    pyro.param(\"gate\", torch.tensor(gate), constraint=constraints.unit_interval)",
            "+    pyro.param(\"rate\", torch.tensor(rate), constraint=constraints.positive)",
            "",
            "data = torch.tensor([0., 1., 2.])",
            "elbo = TraceEnum_ELBO(max_plate_nesting=1, strict_enumeration_warning=False)"
        ]
    },
    {
        "number": 3327,
        "comments": "",
        "commit_message": "Fix typo again...\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Conv1dLinear(torch.nn.Module):",
            "super(Conv1dLinear, self).__init__()",
            "self.w_1 = torch.nn.Conv1d(in_chans, hidden_chans, kernel_size,",
            "stride=1, padding=(kernel_size - 1) // 2)",
            "-        self.w_2 = nn.Linear(hidden_chans, in_chans)",
            "+        self.w_2 = torch.nn.Linear(hidden_chans, in_chans)",
            "self.dropout = torch.nn.Dropout(dropout_rate)",
            "",
            "def forward(self, x):"
        ]
    },
    {
        "number": 3329,
        "comments": "",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario \u0160a\u0161ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MsrTextCompression(datasets.GeneratorBasedBuilder):",
            "data_dir = os.path.abspath(os.path.expanduser(dl_manager.manual_dir))",
            "if not os.path.exists(data_dir):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_text_compression', data_dir=...)` per the manual download instructions: {}\".format(",
            "-                    data_dir, self.manual_download_instructions",
            "-                )",
            "+                f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_text_compression', data_dir=...)` per the manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ]
    },
    {
        "number": 3332,
        "comments": "",
        "commit_message": "Another fix to vecdot (#4639)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def vecdot(",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "dtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))",
            "-    x1, x2 = x1.to(dtype=torch.float32), x2.to(dtype=torch.float32)",
            "+    if dtype != \"float64\":",
            "+        x1, x2 = x1.to(dtype=torch.float32), x2.to(dtype=torch.float32)",
            "return torch.tensordot(x1, x2, dims=([axis], [axis]), out=out).to(dtype=dtype)"
        ]
    },
    {
        "number": 3341,
        "comments": "",
        "commit_message": "[Ray RLlib] Fix tree import (#7662)\n\n* Rollback.\n\n* Fix import tree error by adding meaningful error and replacing by tf.nest wherever possible.\n\n* LINT.\n\n* LINT.\n\n* Fix.\n\n* Fix log-likelihood test case failing on travis.\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFPolicy(Policy):",
            "",
            "# build output signatures",
            "output_signature = self._extra_output_signature_def()",
            "-        for i, a in enumerate(tree.flatten(self._sampled_action)):",
            "+        for i, a in enumerate(tf.nest.flatten(self._sampled_action)):",
            "output_signature[\"actions_{}\".format(i)] = \\",
            "tf.saved_model.utils.build_tensor_info(a)"
        ]
    },
    {
        "number": 3342,
        "comments": "",
        "commit_message": "adopt torch.testing.assert_close (#1031)\n\n* adopt torch.testing.assert_close\n\n* use torch.testing.assert_close\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add TODO\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TestRandomPerspective:",
            "assert len(out_perspective) == 2",
            "assert out_perspective[0].shape == x_data.shape",
            "assert out_perspective[1].shape == (1, 3, 3)",
            "-        assert_allclose(out_perspective[0], expected_output, atol=1e-4, rtol=1e-4)",
            "-        assert_allclose(out_perspective[1], expected_transform, atol=1e-4, rtol=1e-4)",
            "+        assert_close(out_perspective[0], expected_output, atol=1e-4, rtol=1e-4)",
            "+        assert_close(out_perspective[1], expected_transform, atol=1e-4, rtol=1e-4)",
            "assert aug.inverse(out_perspective).shape == x_data.shape",
            "",
            "def test_gradcheck(self, device):"
        ]
    },
    {
        "number": 3344,
        "comments": "",
        "commit_message": "various fixes and improvements for preprocessing/exploration update, changed Sequence preprocessor\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Categorical(Distribution):",
            "maxval=(1.0 - util.epsilon)",
            ")",
            "gumbel_distribution = -tf.log(x=-tf.log(x=uniform_distribution))",
            "-        sampled = tf.argmax(input=(logits + gumbel_distribution), axis=-1)",
            "+        sampled = tf.argmax(input=(logits + gumbel_distribution), axis=-1, output_type=util.tf_dtype('int'))",
            "",
            "return tf.where(condition=deterministic, x=definite, y=sampled)"
        ]
    },
    {
        "number": 3345,
        "comments": "",
        "commit_message": "Fix FP16 version comparison\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TransformerDecoder(FairseqIncrementalDecoder):",
            "if k in state_dict:",
            "state_dict['decoder.layers.{}.{}.{}'.format(i, new, m)] = state_dict[k]",
            "del state_dict[k]",
            "-        if state_dict.get('decoder.version', torch.Tensor([1]))[0] < 2:",
            "+        if utils.item(state_dict.get('decoder.version', torch.Tensor([1]))[0]) < 2:",
            "# earlier checkpoints did not normalize after the stack of layers",
            "self.layer_norm = None",
            "self.normalize = False"
        ]
    },
    {
        "number": 3355,
        "comments": "",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CausalSelfAttention(nn.Module):",
            "# [ batch_size x n_heads x sequence_length x sequence_length ]",
            "attn_weights = (torch.matmul(query, key.transpose(-2, -1))) * (1.0 / math.sqrt(key.size(-1)))",
            "attn_weights = attn_weights.masked_fill(",
            "-            self.mask[:, :, :sequence_length, :sequence_length] == 0, float(\"-inf\")",
            "+            self.mask[:, :, :sequence_length, :sequence_length] == 0, torch.finfo(attn_weights.dtype).min",
            ")",
            "attn_weights = F.softmax(attn_weights, dim=-1)",
            "self._attn_map = attn_weights.clone()"
        ]
    },
    {
        "number": 3363,
        "comments": "",
        "commit_message": "Fix TFDebertaV2ConvLayer in TFDebertaV2Model (#16031)\n\n* fix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFDebertaV2ConvLayer(tf.keras.layers.Layer):",
            "else:",
            "if len(shape_list(input_mask)) != len(shape_list(layer_norm_input)):",
            "if len(shape_list(input_mask)) == 4:",
            "-                    mask = tf.squeeze(tf.squeeze(input_mask, axis=1), axis=1)",
            "-                mask = tf.cast(tf.expand_dims(input_mask, axis=2), tf.float32)",
            "+                    input_mask = tf.squeeze(tf.squeeze(input_mask, axis=1), axis=1)",
            "+                input_mask = tf.cast(tf.expand_dims(input_mask, axis=2), tf.float32)",
            "",
            "-            output_states = output * mask",
            "+            output_states = output * input_mask",
            "",
            "return output_states"
        ]
    },
    {
        "number": 3372,
        "comments": "",
        "commit_message": "Fix integration tests (#12066)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class LukeModelIntegrationTests(unittest.TestCase):",
            "expected_shape = torch.Size((1, 1, 1024))",
            "self.assertEqual(outputs.entity_last_hidden_state.shape, expected_shape)",
            "",
            "-        expected_slice = torch.tensor([[0.0466, -0.0106, -0.0179]])",
            "+        expected_slice = torch.tensor([[0.0466, -0.0106, -0.0179]]).to(torch_device)",
            "self.assertTrue(torch.allclose(outputs.entity_last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))"
        ]
    },
    {
        "number": 3376,
        "comments": "",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "Returns:",
            "\"\"\"",
            "",
            "",
            "-class Rouge(nlp.Metric):",
            "+class Rouge(datasets.Metric):",
            "def _info(self):",
            "-        return nlp.MetricInfo(",
            "+        return datasets.MetricInfo(",
            "description=_DESCRIPTION,",
            "citation=_CITATION,",
            "inputs_description=_KWARGS_DESCRIPTION,",
            "-            features=nlp.Features(",
            "+            features=datasets.Features(",
            "{",
            "-                    \"predictions\": nlp.Value(\"string\", id=\"sequence\"),",
            "-                    \"references\": nlp.Value(\"string\", id=\"sequence\"),",
            "+                    \"predictions\": datasets.Value(\"string\", id=\"sequence\"),",
            "+                    \"references\": datasets.Value(\"string\", id=\"sequence\"),",
            "}",
            "),",
            "codebase_urls=[\"https://github.com/google-research/google-research/tree/master/rouge\"],"
        ]
    },
    {
        "number": 3378,
        "comments": "",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def benchmark_iterating():",
            "]",
            "with tempfile.TemporaryDirectory() as tmp_dir:",
            "print(\"generating dataset\")",
            "-        features = nlp.Features({\"list\": nlp.Sequence(nlp.Value(\"float32\")), \"numbers\": nlp.Value(\"float32\")})",
            "+        features = datasets.Features(",
            "+            {\"list\": datasets.Sequence(datasets.Value(\"float32\")), \"numbers\": datasets.Value(\"float32\")}",
            "+        )",
            "dataset = generate_example_dataset(",
            "os.path.join(tmp_dir, \"dataset.arrow\"),",
            "features,"
        ]
    },
    {
        "number": 3383,
        "comments": "",
        "commit_message": "[`bnb`] fix `bnb` decoders bug (#21688)\n\n* fix `bnb` decoders bug\n\n* make fixup\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MixedInt8T5Test(unittest.TestCase):",
            "`flan-t5-small` uses `T5DenseGatedActDense` whereas `t5-small` uses `T5DenseReluDense`. We need to test",
            "both cases.",
            "\"\"\"",
            "+        import bitsandbytes as bnb",
            "+",
            "from transformers import T5ForConditionalGeneration",
            "",
            "# test with `t5-small`",
            "model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_8bit=True, device_map=\"auto\")",
            "+",
            "+        # there was a bug with decoders - this test checks that it is fixed",
            "+        self.assertTrue(isinstance(model.decoder.block[0].layer[0].SelfAttention.q, bnb.nn.Linear8bitLt))",
            "+",
            "encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(0)",
            "_ = model.generate(**encoded_input)"
        ]
    },
    {
        "number": 3384,
        "comments": "",
        "commit_message": "Tutorial fixing (#635)\n\n* TF bug fixing in Tutorials\n\n* Error fix in #476\n\n* Issue with Flags in Tutorials Fixed\n\n* Missing import fixed\n\n* Changelog Update\n\n* VGG19 import error fix\n\n* Error fixing in VGG tutorials\n\n* TFRecord Shape Error Fix\n\n* Sess Initialization Error Fix\n\n* Squeezenet model loading from \"models\" dir\n\n* PTB tutorials import issue fixed\n\n* mobilenet load from dir \"models\"\n\n* YAPF error fix\n\n* Missing Import fixed\n\n* Various Fixes on Tutorials\n\n* YAPF error correct\n\n* Update CHANGELOG.md\n\n* update VGG16 tutorial, auto download model\n\n* Python 3 Unicode Encoding Error\n\n* Deprecation Warning Fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "sess = tf.InteractiveSession()",
            "network.print_params(False)",
            "",
            "saver = tf.train.Saver()",
            "-if not os.path.isfile(\"inception_v3.ckpt\"):",
            "+if not os.path.isfile(MODEL_PATH):",
            "raise Exception(",
            "\"Please download inception_v3 ckpt from https://github.com/tensorflow/models/tree/master/research/slim\"",
            ")",
            "",
            "try:  # TF12+",
            "-    saver.restore(sess, \"./inception_v3.ckpt\")",
            "+    saver.restore(sess, MODEL_PATH)",
            "except Exception:  # TF11",
            "-    saver.restore(sess, \"inception_v3.ckpt\")",
            "+    saver.restore(sess, MODEL_PATH)",
            "print(\"Model Restored\")",
            "",
            "y = network.outputs"
        ]
    },
    {
        "number": 3396,
        "comments": "",
        "commit_message": "improve logging and fix wd computation in inference\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def regularize_cost_from_collection(name='regularize_cost'):",
            "\"\"\"",
            "regularization_losses = set(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))",
            "ctx = get_current_tower_context()",
            "+    if not ctx.is_training:",
            "+        # Currently cannot build the wd_cost correctly at inference,",
            "+        # because ths vs_name used in inference can be '', therefore the",
            "+        # variable filter will fail",
            "+        return None",
            "+",
            "if len(regularization_losses) > 0:",
            "# NOTE: this collection doesn't grow with towers.",
            "# It is only added with variables that are newly created."
        ]
    },
    {
        "number": 3400,
        "comments": "",
        "commit_message": "Fix TF version of imagenet loader (fix #1085)\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def fbresnet_mapper(isTrain):",
            "return image",
            "",
            "def lighting(image, std, eigval, eigvec):",
            "-        v = tf.random_uniform(shape=[3]) * std * eigval",
            "+        v = tf.random_normal(shape=[3], stddev=std) * eigval",
            "inc = tf.matmul(eigvec, tf.reshape(v, [3, 1]))",
            "image = tf.cast(tf.cast(image, tf.float32) + tf.reshape(inc, [3]), image.dtype)",
            "return image"
        ]
    },
    {
        "number": 3401,
        "comments": "",
        "commit_message": "fix Average metric (#4624)\n\n* fix Average metric\n\n* try spawn\n\n* try again\n\n* oops\n\n* clean up, fix evalb too\n\n* use different start method for GPU vs CPU tests\n\n* add comment\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Average(Metric):",
            "_total_value = list(self.detach_tensors(value))[0]",
            "_count = 1",
            "if is_distributed():",
            "-            device = torch.device(\"cpu\")",
            "+            device = torch.device(\"cuda\" if dist.get_backend() == \"nccl\" else \"cpu\")",
            "count = torch.tensor(_count).to(device)",
            "total_value = torch.tensor(_total_value).to(device)",
            "dist.all_reduce(count, op=dist.ReduceOp.SUM)"
        ]
    },
    {
        "number": 3406,
        "comments": "",
        "commit_message": "Replace some fs operations by tf.gfile for other fs support. (fix #416)\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def get_checkpoint_path(model_path):",
            "logger.warn(",
            "\"Checkpoint path {} is auto-corrected to {}.\".format(model_path, new_path))",
            "model_path = new_path",
            "-    assert os.path.isfile(model_path) or os.path.isfile(model_path + '.index'), model_path",
            "+    assert tf.gfile.Exists(model_path) or tf.gfile.Exists(model_path + '.index'), model_path",
            "return model_path"
        ]
    },
    {
        "number": 3408,
        "comments": "",
        "commit_message": "GH-462: fix cuda errors in classifier and unit tests\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TextClassifier(flair.nn.Model):",
            "self.document_embeddings.embed(sentences)",
            "",
            "text_embedding_list = [sentence.get_embedding().unsqueeze(0) for sentence in sentences]",
            "-        text_embedding_tensor = torch.cat(text_embedding_list, 0)",
            "+        text_embedding_tensor = torch.cat(text_embedding_list, 0).to(flair.device)",
            "",
            "label_scores = self.decoder(text_embedding_tensor)"
        ]
    },
    {
        "number": 3415,
        "comments": "",
        "commit_message": "fix gpu decoding\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def recog_v2(args):",
            "for idx, name in enumerate(js.keys(), 1):",
            "logging.info('(%d/%d) decoding ' + name, idx, len(js.keys()))",
            "batch = [(name, js[name])]",
            "-            enc = model.encode(load_inputs_and_targets(batch)[0][0])",
            "+            feat = load_inputs_and_targets(batch)[0][0]",
            "+            enc = model.encode(torch.as_tensor(feat).to(device))",
            "nbest_hyps = beam_search(",
            "x=enc,",
            "sos=model.sos,"
        ]
    },
    {
        "number": 3419,
        "comments": "",
        "commit_message": "Fixed error: Expected object of device type cuda but got device type cpu for argument\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DecoderRNNT(torch.nn.Module):",
            "normscore = recog_args.score_norm_transducer",
            "",
            "z_list, c_list = self.zero_state(h.unsqueeze(0))",
            "-        eys = torch.zeros((1, self.embed_dim))",
            "+        eys = to_device(self, torch.zeros((1, self.embed_dim)))",
            "",
            "_, (z_list, c_list) = self.rnn_forward(eys, None)"
        ]
    },
    {
        "number": 3423,
        "comments": "",
        "commit_message": "fix(data): fix sampler bug for single device (#22)\n\nCo-authored-by: liusongtao <liusongtao@megvii.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Exp(MyExp):",
            "",
            "if is_distributed:",
            "batch_size = batch_size // dist.get_world_size()",
            "-            sampler = InfiniteSampler(",
            "-                len(self.dataset), seed=self.seed if self.seed else 0",
            "-            )",
            "-        else:",
            "-            sampler = torch.utils.data.RandomSampler(self.dataset)",
            "+",
            "+        sampler = InfiniteSampler(",
            "+            len(self.dataset), seed=self.seed if self.seed else 0",
            "+        )",
            "",
            "batch_sampler = YoloBatchSampler(",
            "sampler=sampler,"
        ]
    },
    {
        "number": 3431,
        "comments": "",
        "commit_message": "fix small bug\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class UNetUnconditionalModel(ModelMixin, ConfigMixin):",
            "prev_output_channel = output_channel",
            "",
            "# out",
            "-        self.conv_norm_out = nn.GroupNorm(num_channels=block_channels[0], num_groups=32, eps=1e-5)",
            "+        self.conv_norm_out = nn.GroupNorm(num_channels=block_channels[0], num_groups=32, eps=resnet_eps)",
            "self.conv_act = nn.SiLU()",
            "self.conv_out = nn.Conv2d(block_channels[0], out_channels, 3, padding=1)"
        ]
    },
    {
        "number": 3436,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PassThroughEncoder(Seq2SeqEncoder):",
            "else:",
            "# We should mask out the output instead of the input.",
            "# But here, output = input, so we directly mask out the input.",
            "-            return inputs * mask.unsqueeze(dim=-1).float()",
            "+            return inputs * mask.unsqueeze(dim=-1)"
        ]
    },
    {
        "number": 3437,
        "comments": "",
        "commit_message": "small fix as zeta of torch does support native out\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def zeta(",
            "*,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    return torch.special.zeta(x, q)",
            "+    return torch.special.zeta(x, q, out=out)",
            "",
            "",
            "-zeta.support_native_out = False",
            "+zeta.support_native_out = True",
            "",
            "",
            "def gradient("
        ]
    },
    {
        "number": 3441,
        "comments": "",
        "commit_message": "Adds CLIP to models exportable with ONNX (#18515)\n\n* onnx config for clip\n\n* default opset as 14\n\n* changes from the original repo\n\n* input values order fix\n\n* outputs fix\n\n* remove unused import\n\n* ran make fix-copies\n\n* black format\n\n* review comments: forward ref, import fix, model change revert, .to cleanup\n\n* make style\n\n* formatting fixes\n\n* revert groupvit\n\n* comment for cast to int32\n\n* comment fix\n\n* make .T as .t() for onnx conversion\n\n* ran make fix-copies\n\n* remove unneeded comment\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* fix copies\n\n* remove comment\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class GroupViTTextTransformer(nn.Module):",
            "",
            "# text_embeds.shape = [batch_size, sequence_length, transformer.width]",
            "# take features from the eot embedding (eot_token is the highest number in each sequence)",
            "-        pooled_output = last_hidden_state[torch.arange(last_hidden_state.shape[0]), input_ids.argmax(dim=-1)]",
            "+        # casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14",
            "+        pooled_output = last_hidden_state[",
            "+            torch.arange(last_hidden_state.shape[0]), input_ids.to(torch.int).argmax(dim=-1)",
            "+        ]",
            "",
            "if not return_dict:",
            "return (last_hidden_state, pooled_output) + encoder_outputs[1:]"
        ]
    },
    {
        "number": 3442,
        "comments": "",
        "commit_message": "gradient norm clipping should be done right before calling the optimiser - fixing run_glue and run_ner as well\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def train(args, train_dataset, model, tokenizer, labels, pad_token_label_id):",
            "if args.fp16:",
            "with amp.scale_loss(loss, optimizer) as scaled_loss:",
            "scaled_loss.backward()",
            "-                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)",
            "else:",
            "loss.backward()",
            "-                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)",
            "",
            "tr_loss += loss.item()",
            "if (step + 1) % args.gradient_accumulation_steps == 0:",
            "+                if args.fp16:",
            "+                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)",
            "+                else:",
            "+                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)",
            "+",
            "scheduler.step()  # Update learning rate schedule",
            "optimizer.step()",
            "model.zero_grad()"
        ]
    },
    {
        "number": 3452,
        "comments": "",
        "commit_message": "Improve mnist examples and fix bugs. (#382)\n\n* simplified mnist example\n\n* simplied mnist example\n\n* simplied mnist example\n\n* simplied mnist example\n\n* simplied mnist example\n\n* IMPORTANT : fixed D_TYPE bug / as we splited the layers into many file, the D_TYPE in core.py cant change in other files\n\n* update layer config.\n\n* format code\n\n* decouple set_keep.\n\n* fix hao comments.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PReluLayer(Layer):",
            "",
            "# with tf.name_scope(name) as scope:",
            "with tf.variable_scope(name):",
            "-            alphas = tf.get_variable(name='alphas', shape=w_shape, initializer=a_init, dtype=D_TYPE, **a_init_args)",
            "+            alphas = tf.get_variable(name='alphas', shape=w_shape, initializer=a_init, dtype=LayersConfig.tf_dtype, **a_init_args)",
            "try:  # TF 1.0",
            "self.outputs = tf.nn.relu(self.inputs) + tf.multiply(alphas, (self.inputs - tf.abs(self.inputs))) * 0.5",
            "except Exception:  # TF 0.12"
        ]
    },
    {
        "number": 3456,
        "comments": "",
        "commit_message": "Fix torchelastic detection with non-distributed installations (#13142)\n\n* Fix torchelastic detection under Mac\n\n* CHANGELOG\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TorchElasticEnvironment(ClusterEnvironment):",
            "def detect() -> bool:",
            "\"\"\"Returns ``True`` if the current process was launched using the torchelastic command.\"\"\"",
            "if _TORCH_GREATER_EQUAL_1_9_1:",
            "-            return torch.distributed.is_torchelastic_launched()",
            "+            # if not available (for example on MacOS), `is_torchelastic_launched` is not defined",
            "+            return torch.distributed.is_available() and torch.distributed.is_torchelastic_launched()",
            "required_env_vars = {\"RANK\", \"GROUP_RANK\", \"LOCAL_RANK\", \"LOCAL_WORLD_SIZE\"}",
            "return required_env_vars.issubset(os.environ.keys())"
        ]
    }
]