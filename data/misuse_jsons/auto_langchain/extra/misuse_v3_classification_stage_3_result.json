[
    {
        "number": 8824,
        "label": "yes",
        "change": [
            "def _replace_global_by_local(kwargs):",
            "if 'collections' in kwargs:",
            "collections = kwargs['collections']",
            "if not collections:",
            "-        collections = {tf.GraphKeys.GLOBAL_VARIABLES}",
            "+        collections = {tfv1.GraphKeys.GLOBAL_VARIABLES}",
            "else:",
            "collections = set(collections.copy())",
            "-    collections.remove(tf.GraphKeys.GLOBAL_VARIABLES)",
            "-    collections.add(tf.GraphKeys.LOCAL_VARIABLES)",
            "+    collections.remove(tfv1.GraphKeys.GLOBAL_VARIABLES)",
            "+    collections.add(tfv1.GraphKeys.LOCAL_VARIABLES)",
            "kwargs['collections'] = list(collections)"
        ],
        "comments": "",
        "Symptom": "unexpected output",
        "Root_Cause": "deprecation management error",
        "Action": "change",
        "Element": "api call"
    },
    {
        "number": 8832,
        "label": "yes",
        "change": [
            "class OrthogonalRegularizer(Regularizer):",
            "size = inputs.shape[1]",
            "product_no_diagonal = product * (1. - tf.eye(size, dtype=inputs.dtype))",
            "num_pairs = size * (size - 1.) / 2.",
            "-    return self.factor * 0.5 * tf.reduce_sum(product_no_diagonal) / num_pairs",
            "+    return self.factor * 0.5 * tf.reduce_sum(tf.abs(product_no_diagonal)) / num_pairs",
            "",
            "def get_config(self):",
            "return {'factor': float(self.factor), 'mode': self.mode}"
        ],
        "comments": "",
        "Symptom": "unexpected output",
        "Root_Cause": "algorithm error",
        "Action": "change",
        "Element": "api call"
    },
    {
        "number": 8839,
        "label": "no",
        "change": [
            "def maxpool_deriv(x_sh):",
            "# 3)",
            "t = k_sh.get()",
            "k = t % n",
            "-    E_k = torch.zeros(n)",
            "+    E_k = torch.zeros(n).long()",
            "E_k[k] = 1",
            "E_sh = E_k.share(alice, bob, **no_wrap)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8842,
        "label": "no",
        "change": [
            "def add(",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "x1, x2 = ivy.promote_types_of_inputs(x1, x2)",
            "-    return tf.experimental.numpy.add(x1, x2)",
            "+    return tf.add(x1, x2)",
            "",
            "",
            "def asin("
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8849,
        "label": "no",
        "change": [
            "class ToDense(object):",
            "else:",
            "edge_attr = data.edge_attr",
            "",
            "-        size = torch.Size([num_nodes, num_nodes, *list(edge_attr.size())[1:]])",
            "+        size = torch.Size([num_nodes, num_nodes] + list(edge_attr.size())[1:])",
            "adj = torch.sparse_coo_tensor(data.edge_index, edge_attr, size)",
            "data.adj = adj.to_dense()",
            "data.edge_index = None"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8850,
        "label": "no",
        "change": [
            "def protobuf_serialize(obj: np.ndarray) -> NumpyProto:",
            "# same original unsigned values on the other side",
            "obj = obj.astype(DTYPE_REFACTOR[original_dtype])",
            "",
            "-    tensor = torch.from_numpy(obj).clone()",
            "+    # Cloning seems to cause the worker to freeze if the array is larger than around",
            "+    # 800k in data and since we are serializing it immediately afterwards I don't",
            "+    # think its needed anyway",
            "+    # tensor = torch.from_numpy(obj).clone()",
            "+    tensor = torch.from_numpy(obj)",
            "tensor_bytes = tensor_serializer(tensor)",
            "dtype = original_dtype.name",
            "return NumpyProto(proto_data=tensor_bytes, dtype=dtype)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8851,
        "label": "no",
        "change": [
            "class DoubleConvFCBBoxHead(BBoxHead):",
            "out_dim_reg = 4 if self.reg_class_agnostic else 4 * self.num_classes",
            "self.fc_reg = nn.Linear(self.conv_out_channels, out_dim_reg)",
            "",
            "-        self.fc_cls = nn.Linear(self.fc_out_channels, self.num_classes)",
            "+        self.fc_cls = nn.Linear(self.fc_out_channels, self.num_classes + 1)",
            "self.relu = nn.ReLU(inplace=True)",
            "",
            "def _add_conv_branch(self):"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8852,
        "label": "no",
        "change": [
            "class PipelineIntegrationTests(unittest.TestCase):",
            "def test_dance_diffusion_fp16(self):",
            "device = torch_device",
            "",
            "-        pipe = DanceDiffusionPipeline.from_pretrained(",
            "-            \"harmonai/maestro-150k\", torch_dtype=torch.float16, device_map=\"auto\"",
            "-        )",
            "+        pipe = DanceDiffusionPipeline.from_pretrained(\"harmonai/maestro-150k\", torch_dtype=torch.float16)",
            "pipe = pipe.to(device)",
            "pipe.set_progress_bar_config(disable=None)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8855,
        "label": "no",
        "change": [
            "class Model(ModelDesc):",
            "# seqlen is 1 in inference. don't need loop_function",
            "outputs, last_state = rnn.rnn(cell, input_list, initial, scope='rnnlm')",
            "self.last_state = tf.identity(last_state, 'last_state')",
            "+",
            "# seqlen x (Bxrnnsize)",
            "-        output = tf.reshape(tf.concat(1, outputs), [-1, param.rnn_size])  # (seqlenxB) x rnnsize",
            "+        output = tf.reshape(tf.concat(1, outputs), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize",
            "logits = FullyConnected('fc', output, param.vocab_size, nl=tf.identity)",
            "self.prob = tf.nn.softmax(logits / param.softmax_temprature)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8857,
        "label": "no",
        "change": [
            "from seq2seq.configurable import Configurable",
            "def att_sum_bahdanau(v_att, keys, query):",
            "\"\"\"Calculates a batch- and timweise dot product with a variable\"\"\"",
            "return tf.reduce_sum(",
            "-      v_att * math_ops.tanh(keys + tf.expand_dims(query, 1)), [2])",
            "+      v_att * tf.tanh(keys + tf.expand_dims(query, 1)), [2])",
            "",
            "@function.Defun(tf.float32, tf.float32, func_name=\"att_sum_dot\", noinline=True)",
            "def att_sum_dot(keys, query):"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8860,
        "label": "no",
        "change": [
            "class MBartIntegrationTests(unittest.TestCase):",
            "expected_shape = (*summary.shape, config.vocab_size)",
            "self.assertEqual(logits.shape, expected_shape)",
            "",
            "-",
            "-@require_torch",
            "-class MBartTokenizerTests(MBartIntegrationTests):",
            "def test_enro_tokenizer_prepare_translation_batch(self):",
            "batch = self.tokenizer.prepare_translation_batch(",
            "self.src_text, tgt_texts=self.tgt_text, max_length=len(self.expected_src_tokens),"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8870,
        "label": "no",
        "change": [
            "class GoalOrientedBotNetwork(TFModel):",
            "_train_op = optimizer(learning_rate).minimize(loss, name='train_op')",
            "# TODO: check clipping of gradients",
            "#optimizer = tf.train.AdamOptimizer(learning_rate)",
            "+        #clip_rate = 1.",
            "#gards_and_vars = optimizer.compute_gradients(loss, tf.trainable_variables())",
            "-        #grads_and_vars = [(tf.clip_by_norm(grad, 1), var) for grad, var in grads_and_vars]",
            "+        #grads_and_vars = [(tf.clip_by_norm(grad, clip_rate), var) for grad, var in grads_and_vars]",
            "#optimizer.apply_gradients(grads_and_vars)",
            "return _train_op"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8871,
        "label": "no",
        "change": [
            "class TorchSTFT(nn.Module):  # pylint: disable=abstract-method",
            "if use_mel:",
            "self._build_mel_basis()",
            "",
            "-    @torch.no_grad()",
            "def __call__(self, x):",
            "\"\"\"Compute spectrogram frames by torch based stft."
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8878,
        "label": "no",
        "change": [
            "class MsrZhenTranslationParity(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_zhen_translation_parity', data_dir=...)`. Manual download instructions: {})\".format(",
            "-                    path_to_manual_file, self.manual_download_instructions",
            "-                )",
            "+                f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_zhen_translation_parity', data_dir=...)`. Manual download instructions: {self.manual_download_instructions})\"",
            ")",
            "return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"path\": path_to_manual_file})]"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8879,
        "label": "no",
        "change": [
            "class TFXLNetMainLayer(tf.keras.layers.Layer):",
            "if dtype is not None and dtype != tf.float32:",
            "fwd_pos_seq = tf.cast(fwd_pos_seq, dtype=dtype)",
            "if self.clamp_len > 0:",
            "-                fwd_pos_seq = tf.clip_by_value(fwd_pos_seq, -clamp_len, clamp_len)",
            "+                fwd_pos_seq = tf.clip_by_value(fwd_pos_seq, -self.clamp_len, self.clamp_len)",
            "pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq, bsz)",
            "",
            "return pos_emb"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8889,
        "label": "no",
        "change": [
            "def test_paa_head_loss():",
            "nms=dict(type='nms', iou_threshold=0.6),",
            "max_per_img=100))",
            "rescale = False",
            "-    self._get_bboxes_single(",
            "+    self._get_bboxes(",
            "cls_scores,",
            "bbox_preds,",
            "iou_preds,"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8890,
        "label": "no",
        "change": [
            "def flip(tensor, is_label=False):",
            "tensor = np.expand_dims(tensor, axis=0)",
            "tensor = torch.from_numpy(tensor)",
            "if was_cuda:",
            "-       tensor = tensor.cuda()",
            "+        tensor = tensor.cuda()",
            "return tensor"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8897,
        "label": "no",
        "change": [
            "def matrix_transpose(",
            "*,",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    return tf.experimental.numpy.swapaxes(x, -1, -2)",
            "+    return tf.linalg.matrix_transpose(x)",
            "",
            "",
            "# noinspection PyUnusedLocal,PyShadowingBuiltins"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8898,
        "label": "no",
        "change": [
            "class SpeechT5ForSpeechToSpeech(SpeechT5PreTrainedModel):",
            "predicted mel spectrogram, or a tensor with shape `(num_frames,)` containing the speech waveform.",
            "\"\"\"",
            "if speaker_embeddings is None:",
            "-            speaker_embeddings = torch.zeros((1, 512))",
            "+            speaker_embeddings = torch.zeros((1, 512), device=input_values.device)",
            "",
            "return _generate_speech(",
            "self,"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8899,
        "label": "no",
        "change": [
            "class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):",
            "for layer in self.model.layers:",
            "for weight in layer.weights:",
            "weight_name = weight.name.replace(':', '_')",
            "-            tf.summary.histogram(weight_name, weight, step=epoch)",
            "+            # Add a suffix to prevent summary tag name collision.",
            "+            histogram_weight_name = weight_name + '/histogram'",
            "+            tf.summary.histogram(histogram_weight_name, weight, step=epoch)",
            "if self.write_images:",
            "-              self._log_weight_as_image(weight, weight_name, epoch)",
            "+              # Add a suffix to prevent summary tag name collision.",
            "+              image_weight_name = weight_name + '/image'",
            "+              self._log_weight_as_image(weight, image_weight_name, epoch)",
            "self._train_writer.flush()",
            "",
            "def _log_weight_as_image(self, weight, weight_name, epoch):"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8900,
        "label": "no",
        "change": [
            "def get_learning_rate(batch):",
            "DECAY_STEP,          # Decay step.",
            "DECAY_RATE,          # Decay rate.",
            "staircase=True)",
            "-    learing_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!",
            "+    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!",
            "return learning_rate",
            "",
            "def get_bn_decay(batch):"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8901,
        "label": "yes",
        "change": [
            "def test(data,",
            "",
            "# Load model",
            "google_utils.attempt_download(weights)",
            "-        model = torch.load(weights, map_location=device)['model'].float().fuse()  # load to FP32",
            "+        model = torch.load(weights, map_location=device)['model'].float().fuse().to(device)  # load to FP32",
            "imgsz = check_img_size(imgsz, s=model.stride.max())  # check img_size",
            "",
            "# Multi-GPU disabled, incompatible with .half() https://github.com/ultralytics/yolov5/issues/99"
        ],
        "comments": "",
        "Symptom": "low efficiency",
        "Root_Cause": "deprecation management error",
        "Action": "addition",
        "Element": "api call"
    },
    {
        "number": 8906,
        "label": "no",
        "change": [
            "def generate_smooth_grad(Backprop, prep_img, target_class, param_n, param_sigma_",
            "smooth_grad = np.zeros(prep_img.size()[1:])",
            "",
            "mean = 0",
            "-    sigma = param_sigma_multiplier / (torch.max(prep_img) - torch.min(prep_img)).data[0]",
            "+    sigma = param_sigma_multiplier / (torch.max(prep_img) - torch.min(prep_img)).item()",
            "for x in range(param_n):",
            "# Generate noise",
            "noise = Variable(prep_img.data.new(prep_img.size()).normal_(mean, sigma**2))"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8913,
        "label": "no",
        "change": [
            "def single_gpu_test(model,",
            "for bbox_results, mask_results in result]",
            "results.extend(result)",
            "",
            "-        batch_size = len(result)",
            "for _ in range(batch_size):",
            "prog_bar.update()",
            "return results"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8921,
        "label": "yes",
        "change": [
            "class SOSNet(nn.Module):",
            "self.desc_norm = nn.Sequential(nn.LocalResponseNorm(256, alpha=256.0, beta=0.5, k=0.0))",
            "# load pretrained model",
            "if pretrained:",
            "-            pretrained_dict = torch.hub.load_state_dict_from_url(urls['lib'], map_location=lambda storage, loc: storage)",
            "+            storage_fcn: Callable = lambda storage, loc: storage",
            "+            pretrained_dict = torch.hub.load_state_dict_from_url(",
            "+                urls['lib'], map_location=storage_fcn",
            "+            )",
            "self.load_state_dict(pretrained_dict, strict=True)",
            "self.eval()",
            "return"
        ],
        "comments": "",
        "Symptom": "low efficiency",
        "Root_Cause": "deprecation management error",
        "Action": "change",
        "Element": "api parameter"
    },
    {
        "number": 8926,
        "label": "no",
        "change": [
            "def test_sigmoid(x, dtype, tensor_fn, dev, call):",
            "# cardinality test",
            "assert ret.shape == x.shape",
            "# value test",
            "-    assert np.allclose(call(ivy.sigmoid, x), ivy.functional.backends.numpy.sigmoid(ivy.to_numpy(x)))",
            "-",
            "+    assert np.allclose(call(ivy.sigmoid, x), ivy_np.sigmoid(ivy.to_numpy(x)))",
            "",
            "",
            "# softmax"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8929,
        "label": "no",
        "change": [
            "class Callbacks(Callback):",
            "",
            "def trigger_epoch(self):",
            "self.train.trigger_epoch()",
            "+        # TODO test callbacks can be run async?",
            "self.test.trigger_epoch()"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8931,
        "label": "no",
        "change": [
            "def sigmoid(x):",
            "Returns:",
            "A tensor.",
            "\"\"\"",
            "-    return tf.sigmoid(x)",
            "+    return tf.math.sigmoid(x)",
            "",
            "",
            "@keras_export(\"keras.backend.hard_sigmoid\")"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8938,
        "label": "no",
        "change": [
            "class TorchBackend(Backend):",
            "self.init_process_group(backend, timeout, init_method)",
            "",
            "def init_process_group(self, backend, timeout, init_method):",
            "-        return torch.distributed.init_process_group(backend,",
            "-                                                    timeout=timeout,",
            "-                                                    init_method=init_method)",
            "+        if not torch.distributed.is_initialized():",
            "+            torch.distributed.init_process_group(backend,",
            "+                                                 timeout=timeout,",
            "+                                                 init_method=init_method)",
            "",
            "def all_reduce(self,",
            "tensor,"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8947,
        "label": "yes",
        "change": [
            "class TorchDiagGaussian(TorchDistributionWrapper):",
            "@override(ActionDistribution)",
            "def __init__(self, inputs, model):",
            "super().__init__(inputs, model)",
            "-        mean, log_std = torch.chunk(inputs, 2, dim=1)",
            "+        mean, log_std = torch.chunk(self.inputs, 2, dim=1)",
            "self.dist = torch.distributions.normal.Normal(mean, torch.exp(log_std))",
            "",
            "@override(ActionDistribution)"
        ],
        "comments": "",
        "Symptom": "unexpected output",
        "Root_Cause": "null reference error",
        "Action": "change",
        "Element": "api parameter"
    },
    {
        "number": 8949,
        "label": "no",
        "change": [
            "class Schedule(metaclass=ABCMeta):",
            "\"\"\"",
            "# By default (most of the time), tf should work with python code.",
            "# Override only if necessary.",
            "-        return tf.constant(self._value(t))",
            "+        return self._value(t)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8950,
        "label": "no",
        "change": [
            "class RoCBertModelIntegrationTest(unittest.TestCase):",
            "# convert to tokens is: ['[CLS]', '巴', '*', '黎', '是', '法', '国', '的', '首', '都', '[SEP]']",
            "expected_output = torch.tensor([[101, 2349, 115, 7944, 3221, 3791, 1744, 4638, 7674, 6963, 102]])",
            "",
            "-        self.assertEqual(output_ids, expected_output)",
            "+        assert torch.allclose(output_ids, expected_output)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8951,
        "label": "yes",
        "change": [
            "def clip(",
            "*,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "+    assert torch.all(torch.less(x_min, x_max)), \"Min value must be less than max.\"",
            "if hasattr(x_min, \"dtype\"):",
            "promoted_type = torch.promote_types(x_min.dtype, x_max.dtype)",
            "promoted_type = torch.promote_types(promoted_type, x.dtype)"
        ],
        "comments": "",
        "Symptom": "unexpected output",
        "Root_Cause": "algorithm error",
        "Action": "change",
        "Element": "api condition check"
    },
    {
        "number": 8952,
        "label": "no",
        "change": [
            "logger = logging.getLogger(__name__)",
            "",
            "_SLOWMO_DDP_DISABLED = False",
            "try:",
            "-    from fairscale.experimental.nn.data_parallel import SlowMoBaseAlgorithm, SlowMoDistributedDataParallel",
            "+    from fairscale.experimental.nn.data_parallel import (",
            "+        SlowMoBaseAlgorithm,",
            "+        SlowMoDistributedDataParallel,",
            "+    )",
            "except ImportError:",
            "_SLOWMO_DDP_DISABLED = True"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8954,
        "label": "no",
        "change": [
            "class AntiSymmetricConv(torch.nn.Module):",
            "",
            "def reset_parameters(self):",
            "torch.nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))",
            "-        ones(self.eye)",
            "self.phi.reset_parameters()",
            "zeros(self.bias)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8961,
        "label": "no",
        "change": [
            "def seed(seed_value: int = 0) -> None:",
            "",
            "def shuffle(x: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "batch_size = x.shape[0]",
            "-    return torch.index_select(x, 0, torch.randperm(batch_size, out=out), out=out)",
            "+    return torch.index_select(x, 0, torch.randperm(batch_size), out=out)",
            "",
            "",
            "shuffle.support_native_out = True"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8963,
        "label": "no",
        "change": [
            "class TFFastSpeechOutput(tf.keras.layers.Layer):",
            "self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)",
            "",
            "def call(self, inputs, training=False):",
            "-        \"\"\"Call logic\"\"\"",
            "+        \"\"\"Call logic.\"\"\"",
            "hidden_states, input_tensor = inputs",
            "",
            "hidden_states = self.dense(hidden_states)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8976,
        "label": "no",
        "change": [
            "def main():",
            "",
            "model = build_detector(",
            "cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)",
            "-    train_dataset = datasets.get_dataset(cfg.data.train)",
            "+    train_dataset = get_dataset(cfg.data.train)",
            "train_detector(",
            "model,",
            "train_dataset,"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8993,
        "label": "no",
        "change": [
            "def BatchNormV1(x, use_local_stat=None, decay=0.9, epsilon=1e-5):",
            "",
            "if use_local_stat:",
            "batch = tf.cast(tf.shape(x)[0], tf.float32)",
            "-        mul = tf.select(tf.equal(batch, 1.0), 1.0, batch / (batch - 1))",
            "+        mul = tf.where(tf.equal(batch, 1.0), 1.0, batch / (batch - 1))",
            "batch_var = batch_var * mul  # use unbiased variance estimator in training",
            "",
            "with tf.control_dependencies([ema_apply_op] if ctx.is_training else []):"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 8997,
        "label": "yes",
        "change": [
            "def train(args, logdir):",
            "steps_per_epoch=hp.train1.steps_per_epoch,",
            "# session_config=session_conf",
            ")",
            "-    ckpt = args.ckpt if args.ckpt else tf.train.latest_checkpoint(logdir)",
            "+    ckpt = '{}/{}'.format(logdir, args.ckpt) if args.ckpt else tf.train.latest_checkpoint(logdir)",
            "if ckpt:",
            "train_conf.session_init = SaverRestore(ckpt)"
        ],
        "comments": "",
        "Symptom": "unexpected output",
        "Root_Cause": "data conversion error",
        "Action": "change",
        "Element": "api parameter"
    },
    {
        "number": 9000,
        "label": "no",
        "change": [
            "\"outputs\": [],",
            "\"source\": [",
            "\"#@title Set up SDE parameters\\n\",",
            "-        \"tf.reset_default_graph()\\n\",",
            "+        \"tf.compat.v1.reset_default_graph()\\n\",",
            "\"\\n\",",
            "\"dtype = np.float64 #@param\\n\",",
            "\"num_samples = 100000 #@param\\n\","
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9001,
        "label": "no",
        "change": [
            "class CTC(torch.nn.Module):",
            "self.probs = None  # for visualization",
            "",
            "# In case of Pytorch >= 1.7.0, CTC will be always builtin",
            "-        self.ctc_type = (",
            "-            ctc_type",
            "-            if V(torch.__version__) < V(\"1.7.0\")",
            "-            else \"builtin\"",
            "-        )",
            "+        self.ctc_type = ctc_type if V(torch.__version__) < V(\"1.7.0\") else \"builtin\"",
            "",
            "if ctc_type != self.ctc_type:",
            "logging.warning(f\"CTC was set to {self.ctc_type} due to PyTorch version.\")"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9003,
        "label": "no",
        "change": [
            "class ZCAWhitening(nn.Module):",
            "",
            "T, mean, T_inv = zca_mean(x, self.dim, self.unbiased, self.eps, self.compute_inv)",
            "",
            "-        self.mean_vector: torch.Tensor = mean",
            "+        self.mean_vector = mean",
            "self.transform_matrix: torch.Tensor = T",
            "if T_inv is None:",
            "self.transform_inv: Optional[torch.Tensor] = torch.empty([0])"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9007,
        "label": "no",
        "change": [
            "def convert_xlm_checkpoint_to_pytorch(xlm_checkpoint_path, pytorch_dump_folder_p",
            "model = chkpt['model']",
            "",
            "config = chkpt['params']",
            "-    config = dict((n, v) for n, v in config.items() if not isinstance(v, (torch.Tensor, numpy.ndarray)))",
            "+    config = dict((n, v) for n, v in config.items() if not isinstance(v, (torch.FloatTensor, numpy.ndarray)))",
            "",
            "vocab = chkpt['dico_word2id']",
            "vocab = dict((s + '</w>' if s.find('@@') == -1 and i > 13 else s.replace('@@', ''), i) for s, i in vocab.items())"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9015,
        "label": "no",
        "change": [
            "def invert_affine_transform(matrix: torch.Tensor) -> torch.Tensor:",
            "if not (len(matrix.shape) == 3 or matrix.shape[-2:] == (2, 3)):",
            "raise ValueError(\"Input matrix must be a Bx2x3 tensor. Got {}\"",
            ".format(matrix.shape))",
            "-    matrix_tmp: torch.Tensor = F.pad(matrix, (0, 0, 0, 1), \"constant\", 0.0)",
            "+    matrix_tmp: torch.Tensor = F.pad(matrix, [0, 0, 0, 1], \"constant\", 0.0)",
            "matrix_tmp[..., 2, 2] += 1.0",
            "",
            "matrix_inv: torch.Tensor = torch.inverse(matrix_tmp)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9016,
        "label": "no",
        "change": [
            "class TestBottomHat:",
            "None, None, :, :",
            "]",
            "assert_allclose(",
            "-            bottom_hat(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected,",
            "-            atol=1e-3, rtol=1e-3",
            "+            bottom_hat(tensor, torch.ones_like(structural_element), structuring_element=structural_element),",
            "+            expected,",
            "+            atol=1e-3,",
            "+            rtol=1e-3,",
            ")",
            "",
            "def test_exception(self, device, dtype):"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9026,
        "label": "yes",
        "change": [
            "def is_variable(x):",
            "",
            "",
            "def execute_with_gradients(func, xs, retain_grads=False):",
            "-    with _tf.GradientTape() as tape:",
            "+    with _tf.GradientTape(persistent=retain_grads, watch_accessed_variables=False) as tape:",
            "+        tape.watch(xs)",
            "func_ret = func(xs)",
            "if isinstance(func_ret, tuple):",
            "y = func_ret[0]"
        ],
        "comments": "",
        "Symptom": "low efficiency",
        "Root_Cause": "deprecation management error",
        "Action": "change",
        "Element": "api call"
    },
    {
        "number": 9030,
        "label": "no",
        "change": [
            "def test_heterogeneous_random_node_loader():",
            "data['paper'].node_id = torch.arange(100)",
            "data['author'].x = torch.randn(200, 128)",
            "data['author'].node_id = torch.arange(200)",
            "-    data['paper', 'author'].edge_index = get_edge_index(100, 200, 500)",
            "+    data['paper', 'author'].edge_index = get_random_edge_index(100, 200, 500)",
            "data['paper', 'author'].edge_attr = torch.randn(500, 32)",
            "-    data['author', 'paper'].edge_index = get_edge_index(200, 100, 400)",
            "+    data['author', 'paper'].edge_index = get_random_edge_index(200, 100, 400)",
            "data['author', 'paper'].edge_attr = torch.randn(400, 32)",
            "-    data['paper', 'paper'].edge_index = get_edge_index(100, 100, 600)",
            "+    data['paper', 'paper'].edge_index = get_random_edge_index(100, 100, 600)",
            "data['paper', 'paper'].edge_attr = torch.randn(600, 32)",
            "",
            "loader = RandomNodeLoader(data, num_parts=4, shuffle=True)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9041,
        "label": "no",
        "change": [
            "class DDPMPipeline(DiffusionPipeline):",
            "",
            "if self.device.type == \"mps\":",
            "# randn does not work reproducibly on mps",
            "-            image = torch.randn(image_shape, generator=generator)",
            "+            image = randn_tensor(image_shape, generator=generator)",
            "image = image.to(self.device)",
            "else:",
            "-            image = torch.randn(image_shape, generator=generator, device=self.device)",
            "+            image = randn_tensor(image_shape, generator=generator, device=self.device)",
            "",
            "# set step values",
            "self.scheduler.set_timesteps(num_inference_steps)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9044,
        "label": "no",
        "change": [
            "class WaveRNN(nn.Module) :",
            "# Compute the fine gates",
            "u = F.sigmoid(R_fine_u + I_fine_u + b_fine_u)",
            "r = F.sigmoid(R_fine_r + I_fine_r + b_fine_r)",
            "-                e = F.tanh(r * R_fine_e + I_fine_e + b_fine_e)",
            "+                e = torch.tanh(r * R_fine_e + I_fine_e + b_fine_e)",
            "hidden_fine = u * hidden_fine + (1. - u) * e",
            "",
            "# Compute the fine output"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9046,
        "label": "no",
        "change": [
            "class ThresholdsTest(tf.test.TestCase, parameterized.TestCase):",
            "self.assertAllClose(v1, v2)",
            "",
            "",
            "-if __name__ == '__main__':",
            "-  tf.test.main()",
            "+if __name__ == \"__main__\":",
            "+    tf.test.main()"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9047,
        "label": "no",
        "change": [
            "target = tf.one_hot(indices, 2)",
            "",
            "lr = 0.001",
            "model = tf.keras.Sequential([tf.keras.layers.Dense(2, activation='softmax')])",
            "-optimizer = tf.optimizers.SGD(lr * hvd.size())",
            "+optimizer = optimizers.SGD(lr * hvd.size())",
            "",
            "hostname = os.environ.get('HOROVOD_HOSTNAME')",
            "start_rank = int(os.environ.get('HOROVOD_RANK', 0))"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9048,
        "label": "yes",
        "change": [
            "def test_sequence_tagger_param_selector(results_base_path, tasks_base_path):",
            "",
            "@pytest.mark.integration",
            "def test_text_classifier_param_selector(results_base_path, tasks_base_path):",
            "-    corpus = flair.datasets.ClassificationCorpus(tasks_base_path / \"imdb\")",
            "+    corpus = flair.datasets.ClassificationCorpus(tasks_base_path / \"imdb\", label_type=\"sentiment\")",
            "label_type = \"sentiment\"",
            "",
            "search_space = SearchSpace()",
            "",
            "# document embeddings parameter",
            "-    search_space.add(Parameter.TRANSFORMER_MODEL, hp.choice, options=[\"albert-base-v1\"])",
            "+    search_space.add(Parameter.TRANSFORMER_MODEL, hp.choice, options=[\"sshleifer/tiny-distilbert-base-cased\"])",
            "search_space.add(Parameter.LAYERS, hp.choice, options=[\"-1\", \"-2\"])",
            "",
            "# training parameter"
        ],
        "comments": "",
        "Symptom": "unexpected output",
        "Root_Cause": "data conversion error",
        "Action": "update",
        "Element": "api parameter"
    },
    {
        "number": 9050,
        "label": "no",
        "change": [
            "def get_rotation_matrix2d(center: torch.Tensor, angle: torch.Tensor, scale: torc",
            "",
            "# convert angle and apply scale",
            "rotation_matrix: torch.Tensor = angle_to_rotation_matrix(angle)",
            "-    scaling_matrix: torch.Tensor = (",
            "-        torch.zeros((2, 2), device=rotation_matrix.device, dtype=rotation_matrix.dtype)",
            "-        .fill_diagonal_(1)",
            "-        .repeat(rotation_matrix.size(0), 1, 1)",
            "+    scaling_matrix: torch.Tensor = torch.eye(2, device=rotation_matrix.device, dtype=rotation_matrix.dtype).repeat(",
            "+        rotation_matrix.size(0), 1, 1",
            ")",
            "",
            "scaling_matrix = scaling_matrix * scale.unsqueeze(dim=2).repeat(1, 1, 2)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9053,
        "label": "no",
        "change": [
            "class Model(ModelDesc):",
            ".Conv2D('conv4_7_CPM', 128)())",
            "",
            "def add_stage(stage, l):",
            "-            l = tf.concat(3, [l, shared, pool_center], name='concat_stage{}'.format(stage))",
            "+            l = tf.concat_v2([l, shared, pool_center], 3,",
            "+                             name='concat_stage{}'.format(stage))",
            "for i in range(1, 6):",
            "l = Conv2D('Mconv{}_stage{}'.format(i, stage), l, 128)",
            "l = Conv2D('Mconv6_stage{}'.format(stage), l, 128, kernel_shape=1)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9055,
        "label": "no",
        "change": [
            "def diff(",
            "prepend: Optional[Union[tf.Tensor, tf.Variable, int, float, list, tuple]] = None,",
            "append: Optional[Union[tf.Tensor, tf.Variable, int, float, list, tuple]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    x = tf.experimental.numpy.append(prepend, x, axis=axis)",
            "-    x = tf.experimental.numpy.append(x, append, axis=axis)",
            "+    if prepend != None:",
            "+        x = tf.experimental.numpy.append(prepend, x, axis=axis)",
            "+    if append != None:",
            "+        x = tf.experimental.numpy.append(x, append, axis=axis)",
            "return tf.experimental.numpy.diff(x, n=n, axis=axis)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9061,
        "label": "no",
        "change": [
            "def build_vtrace_loss(policy, model, dist_class, train_batch):",
            "values = model.value_function()",
            "",
            "if policy.is_recurrent():",
            "-        max_seq_len = tf.reduce_max(train_batch[\"seq_lens\"]) - 1",
            "+        max_seq_len = tf.reduce_max(train_batch[\"seq_lens\"])",
            "mask = tf.sequence_mask(train_batch[\"seq_lens\"], max_seq_len)",
            "mask = tf.reshape(mask, [-1])",
            "else:"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9063,
        "label": "no",
        "change": [
            "def diff(",
            "axis: Optional[int] = -1,",
            "prepend: Optional[Union[tf.Tensor, tf.Variable, int, float, list, tuple]] = None,",
            "append: Optional[Union[tf.Tensor, tf.Variable, int, float, list, tuple]] = None,",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "if prepend is not None:",
            "x = tf.experimental.numpy.append(prepend, x, axis=axis)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9068,
        "label": "no",
        "change": [
            "def _expand_binary_labels(labels, label_weights, label_channels):",
            "# in other files such as in ghm_loss, the _expand_binary_labels",
            "# is used for multi-class classification.",
            "bin_labels = labels.new_full((labels.size(0), label_channels), 0)",
            "-    inds = torch.nonzero(labels >= 1).squeeze()",
            "+    inds = torch.nonzero(labels >= 1, as_tuple=False).squeeze()",
            "if inds.numel() > 0:",
            "bin_labels[inds, labels[inds] - 1] = 1",
            "if label_weights is None:"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9070,
        "label": "no",
        "change": [
            "class TextClassifier(flair.nn.DefaultClassifier[Sentence]):",
            "text_embedding_list = [sentence.get_embedding(embedding_names).unsqueeze(0) for sentence in sentences]",
            "text_embedding_tensor = torch.cat(text_embedding_list, 0).to(flair.device)",
            "",
            "-        # send through decoder to get logits",
            "-        scores = self.decoder(text_embedding_tensor)",
            "-",
            "labels = []",
            "for sentence in sentences:",
            "labels.append([label.value for label in sentence.get_labels(self.label_type)])",
            "",
            "if return_label_candidates:",
            "label_candidates = [Label(value=\"<None>\") for sentence in sentences]",
            "-            return scores, labels, sentences, label_candidates",
            "+            return text_embedding_tensor, labels, sentences, label_candidates",
            "",
            "-        return scores, labels",
            "+        return text_embedding_tensor, labels",
            "",
            "def _get_state_dict(self):",
            "model_state = {"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9075,
        "label": "no",
        "change": [
            "def empty(shape: Union[int, Tuple[int]],",
            "def array(object_in, dtype=None, dev=None):",
            "dtype = dtype_from_str(default_dtype(dtype, object_in))",
            "dev = default_device(dev)",
            "-    with _tf.device(dev_from_str(dev)):",
            "+    with tf.device(dev_from_str(dev)):",
            "try:",
            "-            tensor = _tf.convert_to_tensor(object_in, dtype=dtype)",
            "+            tensor = tf.convert_to_tensor(object_in, dtype=dtype)",
            "except (TypeError, ValueError):",
            "-            tensor = _tf.convert_to_tensor(ivy.nested_map(object_in, lambda x: _tf.cast(x, dtype)), dtype=dtype)",
            "+            tensor = tf.convert_to_tensor(ivy.nested_map(object_in, lambda x: tf.cast(x, dtype)), dtype=dtype)",
            "if dtype is None:",
            "return tensor",
            "-        return _tf.cast(tensor, dtype)",
            "+        return tf.cast(tensor, dtype)",
            "",
            "",
            "asarray = array"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9076,
        "label": "no",
        "change": [
            "test_model.summary()",
            "",
            "loss, acc = test_model.evaluate(x_test,",
            "keras.utils.to_categorical(y_test),",
            "-                                classes)",
            "+                                num_classes)",
            "print('\\nTest accuracy: {0}'.format(acc))"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9078,
        "label": "no",
        "change": [
            "def auc(",
            "if (dx, 0).all():",
            "direction = -1.",
            "else:",
            "-                raise ValueError(\"Reordering is not turned on, and \"",
            "-                                 \"the x array is not increasing: %s\" % x)",
            "+                # TODO: Update message on removing reorder",
            "+                raise ValueError(\"Reorder is not turned on, and the 'x' array is\"",
            "+                                 f\" neither increasing or decreasing: {x}\")",
            "",
            "return direction * torch.trapz(y, x)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9079,
        "label": "no",
        "change": [
            "class BeitUperHead(nn.Module):",
            "used_backbone_levels = len(laterals)",
            "for i in range(used_backbone_levels - 1, 0, -1):",
            "prev_shape = laterals[i - 1].shape[2:]",
            "-            laterals[i - 1] += nn.functional.interpolate(",
            "+            laterals[i - 1] = laterals[i - 1] + nn.functional.interpolate(",
            "laterals[i], size=prev_shape, mode=\"bilinear\", align_corners=self.align_corners",
            ")"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9085,
        "label": "no",
        "change": [
            "def main(args):  # pylint: disable=redefined-outer-name",
            "raise Exception(\"The %s  not is a loss supported\" % c.loss)",
            "",
            "if args.restore_path:",
            "-        checkpoint = torch.load(args.restore_path)",
            "+        checkpoint = load_fsspec(args.restore_path)",
            "try:",
            "model.load_state_dict(checkpoint[\"model\"])"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9088,
        "label": "no",
        "change": [
            "def generate_examples(features: dict, num_examples=100, seq_shapes=None):",
            "def generate_example_dataset(dataset_path, features, num_examples=100, seq_shapes=None):",
            "dummy_data = generate_examples(features, num_examples=num_examples, seq_shapes=seq_shapes)",
            "",
            "-    with datasets.ArrowWriter(features=features, path=dataset_path) as writer:",
            "+    with ArrowWriter(features=features, path=dataset_path) as writer:",
            "for key, record in dummy_data:",
            "example = features.encode_example(record)",
            "writer.write(example)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9089,
        "label": "no",
        "change": [
            "class DistributedModel(object):",
            "self.batch_shape = [None]",
            "self.deterministic_mode = config.get('deterministic_mode', False)",
            "self.alpha = config.get('alpha', 0.001)",
            "-        # self.init_op = tf.global_variables_initializer()",
            "-",
            "self.optimizer = None",
            "",
            "self.worker_device = \"/job:worker/task:{}/cpu:0\".format(task_index)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9092,
        "label": "no",
        "change": [
            "def _nullspace(A):",
            "",
            "Return the smallest singular value and the corresponding vector.",
            "\"\"\"",
            "-    u, s, vh = torch.svd(A)",
            "+    _, s, vh = torch.svd(A)",
            "return s[..., -1], vh[..., -1]"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9096,
        "label": "no",
        "change": [
            "def rnn(step_function, inputs, initial_states,",
            "mask = tf.cast(mask, tf.bool)",
            "else:",
            "# Transpose not supported by bool tensor types, hence round-trip to uint8.",
            "-        mask = tf.cast(tf.transpose(tf.cast(mask, tf.uint8), (1, 0, 2)), tf.bool)",
            "+        mask = tf.cast(tf.transpose(tf.cast(mask, tf.uint8), axes), tf.bool)",
            "",
            "mask_list = tf.unpack(mask)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9098,
        "label": "yes",
        "change": [
            "\"\\n\",",
            "\"\\n\",",
            "\"\\\"\\\"\\\"fm part\\\"\\\"\\\"\\n\",",
            "-    \"fm_first_order = tf.nn.embedding_lookup(weights['feature_embeddings'],feat_index)\\n\",",
            "+    \"fm_first_order = tf.nn.embedding_lookup(weights['feature_bias'],feat_index)\\n\",",
            "\"fm_first_order = tf.reduce_sum(tf.multiply(fm_first_order,reshaped_feat_value),2)\\n\",",
            "\"\\n\",",
            "\"summed_features_emb = tf.reduce_sum(embeddings,1)\\n\","
        ],
        "comments": "",
        "Symptom": "unexpected output",
        "Root_Cause": "algorithm error",
        "Action": "change",
        "Element": "api call"
    },
    {
        "number": 9100,
        "label": "no",
        "change": [
            "class TriviaQa(datasets.GeneratorBasedBuilder):",
            "try:",
            "with open(os.path.join(file_dir, fname), encoding=\"utf-8\") as f:",
            "new_item[context_field] = f.read()",
            "-                    except (IOError, datasets.Value(\"errors\").NotFoundError):",
            "+                    except (IOError, FileNotFoundError):",
            "logger.info(\"File does not exist, skipping: %s\", fname)",
            "continue",
            "new_items.append(new_item)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9103,
        "label": "no",
        "change": [
            "class TensorBoard(Callback):",
            "if K.backend() != 'tensorflow':",
            "raise RuntimeError('TensorBoard callback only works '",
            "'with the TensorFlow backend.')",
            "+        global tf, projector",
            "+        import tensorflow as tf",
            "+        from tensorflow.contrib.tensorboard.plugins import projector",
            "self.log_dir = log_dir",
            "self.histogram_freq = histogram_freq",
            "self.merged = None"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9108,
        "label": "no",
        "change": [
            "class RNNLM(nn.Module):",
            "super(RNNLM, self).__init__()",
            "self.embed = nn.Embedding(n_vocab, n_units)",
            "self.lstm = nn.ModuleList(",
            "-            [nn.LSTMCell(n_units, n_units) for _ in six.moves.range(n_layers)])",
            "+            [nn.LSTMCell(n_units, n_units) for _ in range(n_layers)])",
            "self.dropout = nn.ModuleList(",
            "-            [nn.Dropout() for _ in six.moves.range(n_layers + 1)])",
            "+            [nn.Dropout() for _ in range(n_layers + 1)])",
            "self.lo = nn.Linear(n_units, n_vocab)",
            "self.n_layers = n_layers",
            "self.n_units = n_units"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9109,
        "label": "no",
        "change": [
            "adaptive number of epochs.",
            "",
            "",
            "# Initialize the text regressor.",
            "-reg = ak.TextRegressor(overwrite=True, max_trials=1)  # It tries 10 different models.",
            "+reg = ak.TextRegressor(",
            "+    overwrite=True, max_trials=10  # It tries 10 different models.",
            "+)",
            "# Feed the text regressor with training data.",
            "reg.fit(x_train, y_train, epochs=2)",
            "# Predict with the best model."
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9116,
        "label": "no",
        "change": [
            "def test_torchscript_e2e_text(tmpdir, csv_filename):",
            "",
            "",
            "@pytest.mark.skipif(",
            "-    torch.torch_version.TorchVersion(torchtext.__version__) < (0, 13, 0),",
            "-    reason=\"requires torchtext 0.13.0 or higher\",",
            "+    torch.torch_version.TorchVersion(torchtext.__version__) < (0, 14, 0),",
            "+    reason=\"requires torchtext 0.14.0 or higher\",",
            ")",
            "def test_torchscript_e2e_text_hf_tokenizer(tmpdir, csv_filename):",
            "data_csv_path = os.path.join(tmpdir, csv_filename)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9117,
        "label": "no",
        "change": [
            "class MishActivation(nn.Module):",
            "",
            "def __init__(self):",
            "super().__init__()",
            "-        if version.parse(version.parse(torch.__version__).base_version) < version.parse(\"1.9\"):",
            "+        if version.parse(torch.__version__) < version.parse(\"1.9.0\"):",
            "self.act = self._mish_python",
            "else:",
            "self.act = nn.functional.mish"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9122,
        "label": "no",
        "change": [
            "class TestFusedAdam(unittest.TestCase):",
            "self.assertLessEqual(max_abs_diff, self.max_abs_diff)",
            "self.assertLessEqual(max_rel_diff, self.max_rel_diff)",
            "",
            "-    def test_double(self):",
            "-        self.gen_single_type_test(param_type=torch.double)",
            "-",
            "def test_float(self):",
            "self.gen_single_type_test(param_type=torch.float)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9123,
        "label": "no",
        "change": [
            "def variational_dropout(units, keep_prob, fixed_mask_dims=(1,)):",
            "noise_shape = [units_shape[n] for n in range(len(units.shape))]",
            "for dim in fixed_mask_dims:",
            "noise_shape[dim] = 1",
            "-    return tf.nn.dropout(units, keep_prob, noise_shape)",
            "+    return tf.nn.dropout(units, rate=1-keep_prob, noise_shape=noise_shape)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9125,
        "label": "no",
        "change": [
            "def tile(",
            "/,",
            "reps: Sequence[int],",
            "*,",
            "-    out: Optional[Union[tf.Tensor, tf.Variable]] = None",
            "-) -> Union[tf.Tensor, tf.Variable] :",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            "+) -> Union[tf.Tensor, tf.Variable]:",
            "if x.shape == ():",
            "x = tf.reshape(x, (-1,))",
            "if isinstance(reps, Number):"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9126,
        "label": "no",
        "change": [
            "def test_plugin_setup_optimizers_in_pre_dispatch(tmpdir, delay_dispatch):",
            "assert len(self.trainer.optimizers) > 0",
            "",
            "class CustomPlugin(SingleDevicePlugin):",
            "+",
            "@property",
            "def setup_optimizers_in_pre_dispatch(self) -> bool:",
            "return delay_dispatch",
            "",
            "model = TestModel()",
            "-    trainer = Trainer(",
            "-        default_root_dir=tmpdir,",
            "-        fast_dev_run=True,",
            "-        plugins=CustomPlugin(device=torch.device(\"cpu\"))",
            "-    )",
            "+    trainer = Trainer(default_root_dir=tmpdir, fast_dev_run=True, plugins=CustomPlugin(device=torch.device(\"cpu\")))",
            "trainer.fit(model)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9128,
        "label": "no",
        "change": [
            "class RAGenerator(BaseGenerator):",
            "embeddings = self.retriever.embed_passages(docs)",
            "",
            "embeddings_in_tensor = torch.cat(",
            "-            [torch.from_numpy(embedding).unsqueeze(0) for embedding in embeddings],",
            "+            [torch.from_numpy(embedding).float().unsqueeze(0) for embedding in embeddings],",
            "dim=0",
            ")"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9131,
        "label": "no",
        "change": [
            "class TestTutorialCode(unittest.TestCase):",
            "batch_size=8,",
            "first_update=100,",
            "target_sync_frequency=50,",
            "-            preprocessing=preprocessing",
            "+            states_preprocessing_spec=states_preprocessing_spec",
            ")",
            "",
            "agent.close()"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9132,
        "label": "no",
        "change": [
            "class SpeedupTestCase(TestCase):",
            "net.load_state_dict(state_dict)",
            "net.eval()",
            "",
            "-        data = torch.randn(BATCH_SIZE, 3, 224, 224).to(device)",
            "+        data = torch.randn(BATCH_SIZE, 3, 128, 128).to(device)",
            "ms = ModelSpeedup(net, data, MASK_FILE)",
            "ms.speedup_model()",
            "ms.bound_model(data)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9134,
        "label": "yes",
        "change": [
            "def dropout(tensor_in, prob, name=None):",
            "with tf.op_scope([tensor_in], name, \"dropout\") as name:",
            "if isinstance(prob, float):",
            "prob = tf.get_variable(\"prob\", [],",
            "-                                   initializer=tf.constant_initializer(prob))",
            "+                                   initializer=tf.constant_initializer(prob),",
            "+                                   trainable=False)",
            "tf.add_to_collection(DROPOUTS, prob)",
            "return tf.nn.dropout(tensor_in, prob)"
        ],
        "comments": "",
        "Symptom": "low efficiency",
        "Root_Cause": "deprecation management error",
        "Action": "change",
        "Element": "api parameter"
    },
    {
        "number": 9135,
        "label": "no",
        "change": [
            "class PiT(nn.Module):",
            "x += self.pos_embedding",
            "x = self.dropout(x)",
            "",
            "-        return self.layers(x)",
            "+        x = self.layers(x)",
            "+",
            "+        return self.mlp_head(x[:, 0])"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9139,
        "label": "no",
        "change": [
            "class ConvFCBBoxHead(BBoxHead):",
            "self.relu = nn.ReLU(inplace=True)",
            "# reconstruct fc_cls and fc_reg since input channels are changed",
            "if self.with_cls:",
            "-            self.fc_cls = nn.Linear(self.cls_last_dim, self.num_classes)",
            "+            self.fc_cls = nn.Linear(self.cls_last_dim, self.num_classes + 1)",
            "if self.with_reg:",
            "out_dim_reg = (4 if self.reg_class_agnostic else 4 *",
            "self.num_classes)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9141,
        "label": "no",
        "change": [
            "def test_point_pair_features():",
            "assert data.pos.tolist() == pos.tolist()",
            "assert data.norm.tolist() == norm.tolist()",
            "assert data.edge_index.tolist() == edge_index.tolist()",
            "-    assert torch.allclose(",
            "-        data.edge_attr,",
            "-        torch.Tensor([[1, 1, 0, 0, 0], [1, 1, PI, PI, 0]]),",
            "-        atol=1e-04)",
            "+    assert torch.allclose(data.edge_attr,",
            "+                          torch.Tensor([[1, 1, 0, 0, 0], [1, 1, PI, PI, 0]]),",
            "+                          atol=1e-04)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9144,
        "label": "no",
        "change": [
            "# global",
            "import torch",
            "-from typing import Union, Optional, Tuple, List",
            "",
            "",
            "def argsort(x: torch.Tensor,",
            "axis: int = -1,",
            "descending: bool = False,",
            "stable: bool = True)\\",
            "-            -> torch.Tensor:",
            "+        -> torch.Tensor:",
            "return torch.argsort(x, dim=axis, descending=descending)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9145,
        "label": "no",
        "change": [
            "class GanBasedTrainer(metaclass=abc.ABCMeta):",
            "\"\"\"Write variables to tensorboard.\"\"\"",
            "with self.writer.as_default():",
            "for key, value in list_metrics.items():",
            "-                tf.summary.scalar(stage + \"_\" + key, value.result(), step=self.steps)",
            "+                tf.summary.scalar(stage + \"/\" + key, value.result(), step=self.steps)",
            "self.writer.flush()"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9147,
        "label": "no",
        "change": [
            "class SpeechT5ForSpeechToSpeechIntegrationTests(unittest.TestCase):",
            "input_speech = self._load_datasamples(1)",
            "input_values = processor(audio=input_speech, return_tensors=\"pt\").input_values.to(torch_device)",
            "",
            "-        speaker_embeddings = torch.zeros((1, 512))",
            "+        speaker_embeddings = torch.zeros((1, 512), device=torch_device)",
            "generated_speech = model.generate_speech(input_values, speaker_embeddings=speaker_embeddings)",
            "",
            "self.assertEqual(generated_speech.shape[1], model.config.num_mel_bins)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9149,
        "label": "no",
        "change": [
            "class QuantizationAwareTraining(Callback):",
            "model.qconfig = self._qconfig",
            "",
            "if self._check_feasible_fuse(model):",
            "-            torch.quantization.fuse_modules(model, self._modules_to_fuse, inplace=True)",
            "+            fuse_modules(model, self._modules_to_fuse, inplace=True)",
            "",
            "# Prepare the model for QAT. This inserts observers and fake_quants in",
            "# the model that will observe weight and activation tensors during calibration."
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9153,
        "label": "no",
        "change": [
            "def triangulate_points(",
            "_, _, V = torch.svd(X)",
            "",
            "points3d_h = V[..., -1]",
            "-    points3d: torch.Tensor = kornia.convert_points_from_homogeneous(points3d_h)",
            "+    points3d: torch.Tensor = convert_points_from_homogeneous(points3d_h)",
            "return points3d"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9155,
        "label": "no",
        "change": [
            "class TrainerDataLoadingMixin(ABC):",
            "dataloader = self._flatten_dl_only(dataloader)",
            "",
            "if self.accelerator_backend is not None:",
            "-            self.accelerator_backend.barrier('get_dataloaders')",
            "+            self.training_type_plugin.barrier('get_dataloaders')",
            "return dataloader",
            "",
            "def _flatten_dl_only(self, dataloaders):"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9156,
        "label": "no",
        "change": [
            "def preproc_image(",
            "if drop_image_content:",
            "d.pop('blob', 'tensor')",
            "",
            "-    tensors_batch = torch.stack(tensors_batch).type(torch.float32)",
            "+    tensors_batch = torch.stack(tensors_batch).type(dtype)",
            "",
            "if return_np:",
            "tensors_batch = tensors_batch.cpu().numpy()"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9163,
        "label": "no",
        "change": [
            "def main():",
            "# train",
            "logging.info('backend = ' + args.backend)",
            "if args.backend == \"chainer\":",
            "-        from espnet.lmchainer.lm_chainer import train",
            "+        from espnet.lm.chainer.lm_chainer import train",
            "train(args)",
            "elif args.backend == \"pytorch\":",
            "-        from espnet.lmpytorch.lm_pytorch import train",
            "+        from espnet.lm.pytorch.lm_pytorch import train",
            "train(args)",
            "else:",
            "raise ValueError(\"Only chainer and pytorch are supported.\")"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9165,
        "label": "no",
        "change": [
            "def get_random_cuda_device() -> str:",
            "any device without having to set the device explicitly.",
            "\"\"\"",
            "num_devices = torch.cuda.device_count()",
            "-    rand_device_id = torch.randint(high=num_devices, size=(1,)).item()",
            "-    return \"cuda:%d\" % rand_device_id",
            "+    device_id = (",
            "+        torch.randint(high=num_devices, size=(1,)).item() if num_devices > 1 else 0",
            "+    )",
            "+    return \"cuda:%d\" % device_id",
            "",
            "",
            "class TestCaseMixin(unittest.TestCase):"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9171,
        "label": "no",
        "change": [
            "def torch_resume(snapshot_path, trainer):",
            "# retore optimizer states",
            "trainer.updater.get_optimizer('main').load_state_dict(snapshot_dict['optimizer'])",
            "",
            "-    return trainer",
            "+    # delete opened snapshot",
            "+    del snapshot_dict"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9177,
        "label": "no",
        "change": [
            "class OrnsteinUhlenbeckProcess(Exploration):",
            "",
            "super(OrnsteinUhlenbeckProcess, self).__init__(scope=scope, summary_labels=summary_labels)",
            "",
            "-    def tf_explore(self, episode, timestep, action_shape):",
            "-        normal_sample = tf.random_normal(shape=action_shape.shape, mean=0.0, stddev=1.0)",
            "+    def tf_explore(self, episode, timestep, action_spec):",
            "+        normal_sample = tf.random_normal(shape=action_spec['shape'], mean=0.0, stddev=1.0)",
            "state = tf.get_variable(",
            "name='ornstein_uhlenbeck',",
            "dtype=util.tf_dtype('float'),",
            "-            shape=action_shape.shape,",
            "+            shape=action_spec['shape'],",
            "initializer=tf.constant_initializer(self.mu)",
            ")",
            "return tf.assign_add(ref=state, value=(self.theta * (self.mu - state) + self.sigma * normal_sample))"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9178,
        "label": "no",
        "change": [
            "class BasicGNN(torch.nn.Module):",
            "pbar.set_description('Inference')",
            "",
            "x_all = loader.data.x.cpu()",
            "-        x_all = x_all.to(dtype)",
            "loader.data.n_id = torch.arange(x_all.size(0))",
            "",
            "for i in range(self.num_layers):"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9184,
        "label": "no",
        "change": [
            "class AutoModel(object):",
            "pipeline = self.tuner.get_best_pipeline()",
            "model = self.tuner.get_best_model()",
            "dataset = pipeline.transform_x(dataset)",
            "+        dataset = tf.data.Dataset.zip((dataset, dataset))",
            "y = model.predict(dataset, **kwargs)",
            "y = utils.predict_with_adaptive_batch_size(",
            "model=model, batch_size=batch_size, x=dataset, **kwargs"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9185,
        "label": "no",
        "change": [
            "class TFNoBadWordsLogitsProcessor(TFLogitsProcessor):",
            "def _len_greater_than_cur_len():",
            "# Otherwise, if the bad sequence is longer than the current length they can't ever match",
            "return tf.cond(",
            "-                    tf.math.greater(self.bad_word_seqs_len[bad_word_seq_number], row_input_ids.shape[0]),",
            "+                    tf.math.greater(self.bad_word_seqs_len[bad_word_seq_number], tf.shape(row_input_ids)[0]),",
            "lambda: tf.zeros((), dtype=tf.bool),",
            "_match_found,",
            ")"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9188,
        "label": "no",
        "change": [
            "def assert_ok_model_acc(trainer, key='test_acc', thr=0.5):",
            "",
            "def reset_seed():",
            "seed = RANDOM_SEEDS.pop()",
            "-    torch.manual_seed(seed)",
            "-    np.random.seed(seed)",
            "+    seed_everything(seed)",
            "",
            "",
            "def set_random_master_port():"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9189,
        "label": "no",
        "change": [
            "class AsyncPPOPolicyGraph(LearningRateSchedule, TFPolicyGraph):",
            "existing_state_in = existing_inputs[9:-1]",
            "existing_seq_lens = existing_inputs[-1]",
            "else:",
            "-            actions = tf.placeholder(tf.int64, actions_shape, name=\"ac\")",
            "+            actions = ModelCatalog.get_action_placeholder(action_space)",
            "dones = tf.placeholder(tf.bool, [None], name=\"dones\")",
            "rewards = tf.placeholder(tf.float32, [None], name=\"rewards\")",
            "behaviour_logits = tf.placeholder("
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9190,
        "label": "no",
        "change": [
            "def test_mask_invalid_shape(batch_shape, mask_shape):",
            "",
            "",
            "def test_kl_divergence():",
            "-    mask = torch.tensor([[0, 1], [1, 1]]).byte()",
            "+    mask = torch.tensor([[0, 1], [1, 1]]).bool()",
            "p = Normal(torch.randn(2, 2), torch.randn(2, 2).exp())",
            "q = Normal(torch.randn(2, 2), torch.randn(2, 2).exp())",
            "expected = kl_divergence(p.to_event(2), q.to_event(2))"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9191,
        "label": "no",
        "change": [
            "class FastTextClassifier(object):",
            "self.prediction_probs = tf.nn.softmax(self.network.outputs)",
            "self.predictions = tf.argmax(",
            "self.network.outputs, axis=1, output_type=tf.int32)",
            "+        # self.predictions = tf.cast(tf.argmax(             # for TF < 1.2",
            "+        #     self.network.outputs, axis=1), tf.int32)",
            "",
            "# Evaluation",
            "are_predictions_correct = tf.equal(self.predictions, self.labels)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9197,
        "label": "yes",
        "change": [
            "class Seq2SeqModel(ModelBase):",
            "if \"embedding\" in variable.name:",
            "tmp = tf.clip_by_norm(",
            "gradient.values, self.params[\"optimizer.clip_embed_gradients\"])",
            "-        grad = tf.IndexedSlices(tmp, gradient.indices, gradient.dense_shape)",
            "+        gradient = tf.IndexedSlices(tmp, gradient.indices, gradient.dense_shape)",
            "clipped_gradients.append(gradient)",
            "variables.append(variable)",
            "return list(zip(clipped_gradients, variables))"
        ],
        "comments": "",
        "Symptom": "low efficiency",
        "Root_Cause": "algorithm error",
        "Action": "change",
        "Element": "api call"
    },
    {
        "number": 9199,
        "label": "no",
        "change": [
            "def train_concise_ch11(trainer_fn, hyperparams, data_iter, num_epochs=4):",
            "optimizer.zero_grad()",
            "out = net(X)",
            "y = y.reshape(out.shape)",
            "-            l = loss(out, y)/2",
            "-            l.backward()",
            "+            l = loss(out, y)",
            "+            l.mean().backward()",
            "optimizer.step()",
            "n += X.shape[0]",
            "if n % 200 == 0:",
            "timer.stop()",
            "animator.add(n/X.shape[0]/len(data_iter),",
            "-                             (d2l.evaluate_loss(net, data_iter, loss)/2,))",
            "+                             (d2l.evaluate_loss(net, data_iter, loss),))",
            "timer.start()",
            "print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9204,
        "label": "no",
        "change": [
            "class SequenceLabelField(Field[torch.Tensor]):",
            "@overrides",
            "def as_tensor(self,",
            "padding_lengths: Dict[str, int],",
            "-                  cuda_device: int = -1,",
            "-                  for_training: bool = True) -> torch.Tensor:",
            "+                  cuda_device: int = -1) -> torch.Tensor:",
            "desired_num_tokens = padding_lengths['num_tokens']",
            "padded_tags = pad_sequence_to_length(self._indexed_labels, desired_num_tokens)",
            "-        tensor = Variable(torch.LongTensor(padded_tags), volatile=not for_training)",
            "+        tensor = torch.LongTensor(padded_tags)",
            "return tensor if cuda_device == -1 else tensor.cuda(cuda_device)",
            "",
            "@overrides"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9216,
        "label": "no",
        "change": [
            "def multinomial(",
            ")",
            "/ population_size",
            ")",
            "-    return torch.multinomial(probs.float(),",
            "-                             num_samples,",
            "-                             replace).to(default_device(device))",
            "+    return torch.multinomial(probs.float(), num_samples, replace).to(",
            "+        default_device(device)",
            "+    )",
            "",
            "",
            "def randint("
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9221,
        "label": "no",
        "change": [
            "matrix_rank.support_native_out = True",
            "",
            "",
            "def matrix_transpose(",
            "-    x: torch.Tensor,",
            "-    /,",
            "-    *,",
            "-    conjugate: bool = False,",
            "-    out: Optional[torch.Tensor] = None",
            "+    x: torch.Tensor, /, *, conjugate: bool = False, out: Optional[torch.Tensor] = None",
            ") -> torch.Tensor:",
            "if conjugate:",
            "torch.conj(x)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9223,
        "label": "no",
        "change": [
            "def _gather(input_):",
            "",
            "tensor_list = [torch.empty_like(input_) for _ in range(world_size)]",
            "tensor_list[rank] = input_",
            "-    torch.distributed.all_gather(tensor_list, input_, group=get_tensor_model_parallel_group())",
            "+    torch.distributed.all_gather(",
            "+        tensor_list, input_, group=get_tensor_model_parallel_group()",
            "+    )",
            "",
            "# Note: torch.cat already creates a contiguous tensor.",
            "output = torch.cat(tensor_list, dim=last_dim).contiguous()"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9224,
        "label": "no",
        "change": [
            "class FastRCNNPredictor(nn.Module):",
            "self.bbox_pred = nn.Linear(num_inputs, num_classes * 4)",
            "",
            "nn.init.normal_(self.cls_score.weight, mean=0, std=0.01)",
            "-        nn.init.constant_(self.cls_score.weight, 0)",
            "+        nn.init.constant_(self.cls_score.bias, 0)",
            "",
            "nn.init.normal_(self.bbox_pred.weight, mean=0, std=0.001)",
            "-        nn.init.constant_(self.bbox_pred.weight, 0)",
            "+        nn.init.constant_(self.bbox_pred.bias, 0)",
            "",
            "def forward(self, x):",
            "x = self.avgpool(x)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9225,
        "label": "no",
        "change": [
            "def adam_update(ws, dcdws, lr, mw, vw, step, beta1=0.9, beta2=0.999, epsilon=1e-",
            "",
            "",
            "def stop_gradient(x):",
            "-    return x.detach()",
            "+    # ToDo: work out why _torch.tensor() wrapping is necessary in certain cases, presumably .detach() should be enough.",
            "+    return _torch.tensor(x.detach())"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9226,
        "label": "no",
        "change": [
            "class OrnsteinUhlenbeckProcess(Exploration):",
            "",
            "def tf_explore(self, episode, timestep, action_shape):",
            "normal_sample = tf.random_normal(shape=action_shape.shape, mean=0.0, stddev=1.0)",
            "-        state = tf.get_variable(name='ornstein_uhlenbeck', dtype=util.tf_dtype('float'), shape=action_shape.shape,",
            "-                                initializer=tf.constant_initializer(self.mu))",
            "+        state = tf.get_variable(",
            "+            name='ornstein_uhlenbeck',",
            "+            dtype=util.tf_dtype('float'),",
            "+            shape=action_shape.shape,",
            "+            initializer=tf.constant_initializer(self.mu)",
            "+        )",
            "return tf.assign_add(ref=state, value=(self.theta * (self.mu - state) + self.sigma * normal_sample))"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9236,
        "label": "yes",
        "change": [
            "def median(",
            "keepdims: Optional[bool] = False,",
            "out: Optional[torch.tensor] = None,",
            ") -> torch.tensor:",
            "+    temp = input",
            "if hasattr(axis, \"__iter__\"):",
            "for dim in axis:",
            "-            input = torch.median(",
            "-                input,",
            "+            temp = torch.median(",
            "+                temp,",
            "dim=dim,",
            "keepdim=keepdims,",
            ")[0]"
        ],
        "comments": "",
        "Symptom": "unexpected output",
        "Root_Cause": "null reference error",
        "Action": "addition",
        "Element": "api call"
    },
    {
        "number": 9240,
        "label": "no",
        "change": [
            "class Pipeline(_ScikitCompat):",
            "",
            "def get_inference_context(self):",
            "inference_context = (",
            "-            torch.inference_mode if version.parse(torch.__version__) >= version.parse(\"1.9.0\") else torch.no_grad",
            "+            torch.inference_mode",
            "+            if version.parse(version.parse(torch.__version__).base_version) >= version.parse(\"1.9.0\")",
            "+            else torch.no_grad",
            ")",
            "return inference_context"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9241,
        "label": "no",
        "change": [
            "def argsort(",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "if out is not None:",
            "-        out = tuple([torch.zeros(out.shape, dtype=out.dtype), out])",
            "+        out = tuple([torch.zeros(x.shape, dtype=x.dtype), out.long()])",
            "_, sorted_indices = torch.sort(",
            "x, dim=axis, descending=descending, stable=stable, out=out",
            ")"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9243,
        "label": "no",
        "change": [
            "class SpatialTransformer2dAffineLayer(Layer):",
            "if out_size is None:",
            "out_size = [40, 40]",
            "",
            "-        logging.info(\"SpatialTransformer2dAffineLayer %s: in_size:%s out_size:%s\" % (name, self.inputs.get_shape().as_list(), out_size))",
            "+        logging.info(",
            "+            \"SpatialTransformer2dAffineLayer %s: in_size:%s out_size:%s\" %",
            "+            (name, self.inputs.get_shape().as_list(), out_size)",
            "+        )",
            "",
            "with tf.variable_scope(name) as vs:",
            "# 1. make the localisation network to [batch, 6] via Flatten and Dense."
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9245,
        "label": "no",
        "change": [
            "class Model(torch.nn.Module, Registrable):",
            "",
            "# Load state dict. We pass `strict=False` so PyTorch doesn't raise a RuntimeError",
            "# if the state dict is missing keys because we handle this case below.",
            "-        model_state = torch.load(weights_file, map_location=util.device_mapping(cuda_device))",
            "+        model_state = util.load_state_dict(weights_file, cuda_device=cuda_device)",
            "missing_keys, unexpected_keys = model.load_state_dict(model_state, strict=False)",
            "",
            "# Modules might define a class variable called `authorized_missing_keys`,"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9246,
        "label": "yes",
        "change": [
            "class Dropout(Layer):",
            "",
            "skip_dropout = tf.math.logical_not(x=Module.retrieve_tensor(name='update'))",
            "zero = tf.constant(value=0.0, dtype=util.tf_dtype(dtype='float'))",
            "-        skip_dropout = tf.math.logical_or(x=apply_dropout, y=tf.math.equal(x=rate, y=zero))",
            "+        skip_dropout = tf.math.logical_or(x=skip_dropout, y=tf.math.equal(x=rate, y=zero))",
            "return self.cond(pred=skip_dropout, true_fn=no_dropout, false_fn=apply_dropout)"
        ],
        "comments": "",
        "Symptom": "unexpected output",
        "Root_Cause": "algorithm error",
        "Action": "change",
        "Element": "api condition check"
    },
    {
        "number": 9255,
        "label": "no",
        "change": [
            "def test_dnn_beamformer(use_frontend, use_beamformer, bnmask, num_spkrs, m_str):",
            "ws2 = b.get_mvdr_vector(psd_speech2, psd_speech1 + psd_noise, u2)",
            "",
            "enhanced1 = b.apply_beamforming_vector(ws1, feat).transpose(-1, -2)",
            "-    enhanced2 = b.apply_beamforming_vector(ws1, feat).transpose(-1, -2)",
            "+    enhanced2 = b.apply_beamforming_vector(ws2, feat).transpose(-1, -2)",
            "",
            "assert torch.equal(enhanced1.real, enhanced[0].real)",
            "assert torch.equal(enhanced2.real, enhanced[1].real)",
            "assert torch.equal(enhanced1.imag, enhanced[0].imag)",
            "-    assert torch.equal(enhanced2.imag, enhanced[1].imag)",
            "\\ No newline at end of file",
            "+    assert torch.equal(enhanced2.imag, enhanced[1].imag)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9256,
        "label": "no",
        "change": [
            "def _sample_tree_approx(edge_logits):",
            "# the complete graph. The id of an edge (v1,v2) is k = v1+v2*(v2-1)/2.",
            "edge_ids = torch.empty((E,), dtype=torch.long)",
            "# This maps each vertex to whether it is a member of the cumulative tree.",
            "-    components = torch.zeros(V, dtype=torch.uint8)",
            "+    components = torch.zeros(V, dtype=torch.bool)",
            "",
            "# Sample the first edge at random.",
            "probs = (edge_logits - edge_logits.max()).exp()"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9260,
        "label": "yes",
        "change": [
            "def test_cpu_amp_precision_context_manager(tmpdir):",
            "assert not hasattr(plugin, \"scaler\")",
            "context_manager = plugin.autocast_context_manager()",
            "assert isinstance(context_manager, torch.cpu.amp.autocast)",
            "-    assert context_manager.dtype == torch.bfloat16",
            "+    assert context_manager.fast_dtype == torch.bfloat16",
            "",
            "",
            "@pytest.mark.skipif(not _TORCH_CPU_AMP_AVAILABLE, reason=\"Torch CPU AMP is not available.\")"
        ],
        "comments": "",
        "Symptom": "unexpected output",
        "Root_Cause": "data conversion error",
        "Action": "update",
        "Element": "api condition check"
    },
    {
        "number": 9265,
        "label": "no",
        "change": [
            "class LanguageModel(Model):",
            "mask: torch.Tensor,",
            "direction: int) -> torch.Tensor:",
            "# Need to shift the mask in the correct direction",
            "-        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()",
            "+        zero_col = token_embeddings.new_zeros(mask.size(0), 1).to(dtype=torch.bool)",
            "if direction == 0:",
            "# forward direction, get token to right",
            "shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9270,
        "label": "yes",
        "change": [
            "class Model(ModelDesc):",
            "zc = tf.one_hot(ids, 10, name='zc_train')",
            "zc = tf.placeholder_with_default(zc, [None, 10], name='zc')",
            "",
            "-        z = tf.random_uniform(tf.pack([tf.shape(zc)[0], 90]), -1, 1, name='z_train')",
            "+        z = tf.random_uniform(tf.stack([tf.shape(zc)[0], 90]), -1, 1, name='z_train')",
            "z = tf.placeholder_with_default(z, [None, 90], name='z')",
            "-        z = tf.concat(1, [zc, z], name='fullz')",
            "+        z = tf.concat_v2([zc, z], 1, name='fullz')",
            "",
            "with argscope([Conv2D, Deconv2D, FullyConnected],",
            "W_init=tf.truncated_normal_initializer(stddev=0.02)):"
        ],
        "comments": "",
        "Symptom": "unexpected output",
        "Root_Cause": "deprecation management error",
        "Action": "update",
        "Element": "api call"
    },
    {
        "number": 9276,
        "label": "no",
        "change": [
            "class TextClassificationModel(nn.Module):",
            "",
            "def __init__(self, vocab_size, embed_dim, num_class):",
            "super(TextClassificationModel, self).__init__()",
            "-        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)",
            "+        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)",
            "self.fc = nn.Linear(embed_dim, num_class)",
            "self.init_weights()"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9277,
        "label": "no",
        "change": [
            "def get_model(inputs, is_training):",
            "cost: the cost to minimize. scalar variable",
            "\"\"\"",
            "is_training = bool(is_training)",
            "-    keep_prob = tf.constant(0.5 if is_training else 1.0)",
            "+    keep_prob = tf.constant(0.5 if is_training else 0.0)",
            "",
            "image, label = inputs",
            "image = tf.expand_dims(image, 3)    # add a single channel"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9280,
        "label": "no",
        "change": [
            "def confusion_matrix(",
            "\"\"\"",
            "num_classes = get_num_classes(pred, target, None)",
            "",
            "-    unique_labels = target.view(-1) * num_classes + pred.view(-1)",
            "+    unique_labels = (target.view(-1) * num_classes + pred.view(-1)).to(torch.int)",
            "",
            "bins = torch.bincount(unique_labels, minlength=num_classes ** 2)",
            "cm = bins.reshape(num_classes, num_classes).squeeze().float()"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9284,
        "label": "no",
        "change": [
            "def test_ga_anchor_head_loss():",
            "gt_bboxes = [",
            "torch.Tensor([[23.6667, 23.8757, 238.6326, 151.8874]]).cuda(),",
            "]",
            "-        gt_labels = [torch.LongTensor([2])]",
            "+        gt_labels = [torch.LongTensor([2]).cuda()]",
            "one_gt_losses = head.loss(cls_scores, bbox_preds, shape_preds,",
            "loc_preds, gt_bboxes, gt_labels, img_metas,",
            "gt_bboxes_ignore)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9287,
        "label": "no",
        "change": [
            "class Reporter:",
            "seconds=time.perf_counter() - sub_reporter.start_time",
            ")",
            "stats[\"total_count\"] = sub_reporter.total_count",
            "-        if LooseVersion(torch.__version__) >= LooseVersion(\"1.4.0\"):",
            "+        if V(torch.__version__) >= V(\"1.4.0\"):",
            "if torch.cuda.is_initialized():",
            "stats[\"gpu_max_cached_mem_GB\"] = (",
            "torch.cuda.max_memory_reserved() / 2**30"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9289,
        "label": "no",
        "change": [
            "class TFOptimizer(Optimizer):",
            "loss = fn_loss()",
            "",
            "with tf.control_dependencies(control_inputs=(loss,)):",
            "-            vars_before = [tf.add(x=var, y=0.0) for var in variables]",
            "+            vars_before = [var + 0.0 for var in variables]",
            "",
            "with tf.control_dependencies(control_inputs=vars_before):",
            "applied = self.optimizer.minimize(loss=loss, var_list=variables)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9290,
        "label": "no",
        "change": [
            "from allennlp.modules.attention.cosine_attention import CosineAttention",
            "",
            "",
            "class TestCosineAttention(AllenNlpTestCase):",
            "-",
            "def test_can_init_cosine(self):",
            "legacy_attention = Attention.from_params(Params({\"type\": \"cosine\"}))",
            "isinstance(legacy_attention, CosineAttention)",
            "",
            "def test_cosine_similarity(self):",
            "linear = CosineAttention(normalize=False)",
            "-        output = linear(torch.FloatTensor([[0, 0, 0], [1, 1, 1]]),",
            "-                        torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]))",
            "+        output = linear(",
            "+            torch.FloatTensor([[0, 0, 0], [1, 1, 1]]),",
            "+            torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]),",
            "+        )",
            "",
            "assert_almost_equal(output.numpy(), numpy.array([[0.0, 0.0], [0.9948, 0.9973]]), decimal=2)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9301,
        "label": "no",
        "change": [
            "class ShaDowKHopSampler(torch.utils.data.DataLoader):",
            "sparse_sizes=(n_id.numel(), n_id.numel()),",
            "is_sorted=True)",
            "",
            "-        batch = Batch(torch.ops.torch_sparse.ptr2ind(ptr, n_id.numel()), ptr)",
            "+        batch = Batch(batch=torch.ops.torch_sparse.ptr2ind(ptr, n_id.numel()),",
            "+                      ptr=ptr)",
            "batch.root_n_id = root_n_id",
            "",
            "if self.is_sparse_tensor:"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9302,
        "label": "no",
        "change": [
            "class Clip(Preprocessor):",
            "Clip by min/max.",
            "\"\"\"",
            "",
            "-    def __init__(self, min, max, scope='clip', summary_labels=()):",
            "-        self.min = min",
            "-        self.max = max",
            "-        super(Clip, self).__init__(scope, summary_labels)",
            "+    def __init__(self, min_value, max_value, scope='clip', summary_labels=()):",
            "+        self.min_value = min_value",
            "+        self.max_value = max_value",
            "+        super(Clip, self).__init__(scope=scope, summary_labels=summary_labels)",
            "",
            "def tf_process(self, tensor):",
            "-        return tf.clip_by_value(t=tensor, clip_value_min=self.min, clip_value_max=self.max)",
            "+        return tf.clip_by_value(t=tensor, clip_value_min=self.min_value, clip_value_max=self.max_value)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9327,
        "label": "no",
        "change": [
            "class DeepQNetwork(ValueFunction):",
            "",
            "with tf.name_scope(\"update\"):",
            "self.q_targets = tf.placeholder(tf.float32, [None], name='q_targets')",
            "-            self.actions = tf.placeholder(tf.float32, [None, self.action_count], name='actions')",
            "+            self.actions = tf.placeholder(tf.int32, [None, self.action_count], name='actions')",
            "",
            "# Q values for actions taken in batch",
            "+            print(self.actions)",
            "actions_one_hot = tf.one_hot(self.actions, self.action_count, 1.0, 0.0, name='action_one_hot')",
            "q_values_actions_taken = tf.reduce_sum(self.training_output * actions_one_hot, reduction_indices=1,",
            "name='q_acted')"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9328,
        "label": "yes",
        "change": [
            "class SignatureDict(NestedDict):",
            "assert isinstance(arg, TensorDict)",
            "args.append(spec.kwargs_to_args(kwargs=arg))",
            "else:",
            "-                assert isinstance(arg, (tf.IndexedSlices, tf.Tensor, tf.Variable))",
            "+                assert isinstance(arg, (tf.IndexedSlices, tf.Tensor, tf.Variable)), (name, spec, arg)",
            "args.append(arg)",
            "return args"
        ],
        "comments": "",
        "Symptom": "unexpected output",
        "Root_Cause": "argument error",
        "Action": "change",
        "Element": "api condition check"
    },
    {
        "number": 9335,
        "label": "no",
        "change": [
            "class TestRGBShift:",
            "",
            "def test_rgb_shift(self, device, dtype):",
            "r_shift, g_shift, b_shift = 0.1, 0.2, -0.3",
            "-        image = torch.tensor([[[[0.2, 0.]], [[0.3, 0.5]], [[0.4, 0.7]]]], device=device, dtype=dtype)",
            "+        image = torch.tensor([[[[0.2, 0.0]], [[0.3, 0.5]], [[0.4, 0.7]]]], device=device, dtype=dtype)",
            "shifted = kornia.enhance.shift_rgb(image, r_shift, g_shift, b_shift)",
            "expected = torch.tensor([[[[0.3, 0.1]], [[0.5, 0.7]], [[0.1, 0.4]]]], device=device, dtype=dtype)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9346,
        "label": "no",
        "change": [
            "class NewDataset(datasets.GeneratorBasedBuilder):",
            "DEFAULT_CONFIG_NAME = \"first_domain\"  # It's not mandatory to have a default configuration. Just use one if it make sense.",
            "",
            "def _info(self):",
            "-        # TODO: This method pecifies the datasets.DatasetInfo object which contains informations and typings for the dataset",
            "+        # TODO: This method specifies the datasets.DatasetInfo object which contains informations and typings for the dataset",
            "if self.config.name == \"first_domain\":  # This is the name of the configuration selected in BUILDER_CONFIGS above",
            "features = datasets.Features(",
            "{"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9352,
        "label": "no",
        "change": [
            "class Accelerator(object):",
            ":paramref:`~pytorch_lightning.trainer.trainer.Trainer.truncated_bptt_steps` > 0.",
            "",
            "\"\"\"",
            "-        batch = self.to_device(args[0])",
            "-",
            "-        args[0] = batch",
            "+        args[0] = self.to_device(args[0])",
            "",
            "with self.precision_plugin.train_step_context(), self.training_type_plugin.train_step_context():",
            "return self.training_type_plugin.training_step(*args)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9355,
        "label": "no",
        "change": [
            "class ZCAWhitening(nn.Module):",
            "T, mean, T_inv = zca_mean(x, self.dim, self.unbiased, self.eps, self.compute_inv)",
            "",
            "self.mean_vector = mean",
            "-        self.transform_matrix: torch.Tensor = T",
            "+        self.transform_matrix = T",
            "if T_inv is None:",
            "-            self.transform_inv: Optional[torch.Tensor] = torch.empty([0])",
            "+            self.transform_inv = torch.empty([0])",
            "else:",
            "self.transform_inv = T_inv"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9359,
        "label": "no",
        "change": [
            "def quantile(",
            "# backends the quantile has to be in range [0, 1].",
            "q = q * 100",
            "",
            "-    # The quantile instance method in other backends is equivalent of",
            "+    # The quantile instance method in other backends is equivalent of",
            "# percentile instance method in tensorflow_probability",
            "result = tfp.stats.percentile(",
            "-        a,",
            "-        q,",
            "-        axis=axis,",
            "-        interpolation=interpolation,",
            "-        keepdims=keepdims",
            "+        a, q, axis=axis, interpolation=interpolation, keepdims=keepdims",
            ")",
            "return result"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9361,
        "label": "no",
        "change": [
            "class SquadModel(LRScheduledTFModel):",
            "self.cc_ph = tf.placeholder(shape=(None, None, self.char_limit), dtype=tf.int32, name='cc_ph')",
            "self.q_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name='q_ph')",
            "self.qc_ph = tf.placeholder(shape=(None, None, self.char_limit), dtype=tf.int32, name='qc_ph')",
            "-        self.y1_ph = tf.placeholder(shape=(None, ), dtype=tf.int32, name='y1_ph')",
            "-        self.y2_ph = tf.placeholder(shape=(None, ), dtype=tf.int32, name='y2_ph')",
            "+        self.y1_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name='y1_ph')",
            "+        self.y2_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name='y2_ph')",
            "",
            "self.lear_rate_ph = tf.placeholder_with_default(0.0, shape=[], name='learning_rate')",
            "self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=[], name='keep_prob_ph')"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9362,
        "label": "no",
        "change": [
            "class TFTapasPreTrainedModel(TFPreTrainedModel):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "\"attention_mask\": tf.TensorSpec((None, None), tf.float32, name=\"attention_mask\"),",
            "-                \"token_type_ids\": tf.TensorSpec((None, None, None), tf.int64, name=\"token_type_ids\"),",
            "+                \"token_type_ids\": tf.TensorSpec((None, None, None), tf.int32, name=\"token_type_ids\"),",
            "}",
            "]",
            ")"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9363,
        "label": "no",
        "change": [
            "def einsum(",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "dtype = _get_promoted_type_of_operands(operands)",
            "-    operands = (ivy.astype(operand, torch.float32, copy=False) for operand in operands)",
            "+    operands = (",
            "+        ivy.astype(operand, torch.float32, copy=False).to_native()",
            "+        for operand in operands",
            "+    )",
            "return ivy.astype(torch.einsum(equation, *operands), dtype, copy=False)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9367,
        "label": "no",
        "change": [
            "class PureDocAttention(RepresentationBase):",
            "self.representation_dim = self.dense.out_dim",
            "",
            "def forward(",
            "-        self, embedded_tokens: torch.Tensor, seq_lengths: torch.Tensor, *args",
            "+        self, embedded_tokens: torch.Tensor, seq_lengths: torch.Tensor = None, *args",
            ") -> Any:",
            "rep = self.dropout(embedded_tokens)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9370,
        "label": "no",
        "change": [
            "class TestPreprocessors(unittest.TestCase):",
            "if __name__ == \"__main__\":",
            "# Call this on startup to prevet TF from complaining further down the line about",
            "# not calling in on startup.",
            "-    tf.enable_eager_execution()",
            "import pytest",
            "import sys"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9373,
        "label": "no",
        "change": [
            "class QueueInputTrainer(SimpleFeedfreeTrainer):",
            "\"\"\"",
            "config.data = QueueInput(config.dataset, input_queue)",
            "if predict_tower is not None:",
            "-            logger.warn(\"[Deprecated] Argument `predict_tower` is deprecated for trainer. Use TrainConfig.predict_tower instead!\")",
            "+            logger.warn(\"[Deprecated] Argument `predict_tower` is deprecated for trainer. \"",
            "+                        \"Use TrainConfig.predict_tower instead!\")",
            "config.predict_tower = predict_tower",
            "assert len(config.tower) == 1, \\",
            "\"QueueInputTrainer doesn't support multigpu! Use Sync/AsyncMultiGPUTrainer instead.\""
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9375,
        "label": "no",
        "change": [
            "class PostProcessOptimizer(ProxyOptimizer):",
            "for _, var in grads_and_vars:",
            "with self._maybe_colocate(var):",
            "op = self._func(var)",
            "-                    assert isinstance(op, tf.Operation), op",
            "if op is not None:",
            "+                        assert isinstance(op, tf.Operation), op",
            "ops.append(op)",
            "update_op = tf.group(update_op, *ops, name=name)",
            "return update_op"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9381,
        "label": "no",
        "change": [
            "from pyro.distributions import (Bernoulli, Beta, Categorical, Cauchy, Dirichlet,",
            "",
            "",
            "def T(arr):",
            "-    return Variable(torch.Tensor(arr))",
            "+    return Variable(torch.DoubleTensor(arr))",
            "",
            "",
            "TOOL = 'timeit'"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9387,
        "label": "no",
        "change": [
            "class CrfTagger(Model):",
            "class_probabilities[i, j, tag_id] = 1",
            "",
            "for metric in self.metrics.values():",
            "-                metric(class_probabilities, tags, mask.float())",
            "+                metric(class_probabilities, tags, mask)",
            "if self.calculate_span_f1:",
            "-                self._f1_metric(class_probabilities, tags, mask.float())",
            "+                self._f1_metric(class_probabilities, tags, mask)",
            "if metadata is not None:",
            "output[\"words\"] = [x[\"words\"] for x in metadata]",
            "return output"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9389,
        "label": "no",
        "change": [
            "def test_dynamic_lr(scheduler, num_steps):",
            "assert opt.state_dict()['param_groups'][0]['lr'] == 0.02",
            "assert opt.state_dict()['param_groups'][0]['initial_lr'] == 0.01",
            "assert abs(pyro.param('loc').item()) > 1e-5",
            "-            assert abs(pyro.param('scale').item()) - 0.5 > 1e-5",
            "+            assert abs(pyro.param('scale').item() - 0.5) > 1e-5",
            "",
            "",
            "@pytest.mark.parametrize('factory', [optim.Adam, optim.ClippedAdam, optim.RMSprop, optim.SGD])"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9406,
        "label": "no",
        "change": [
            "class Food101(datasets.GeneratorBasedBuilder):",
            "if file_path[len(_IMAGES_DIR) : -len(\".jpg\")] in files_to_keep:",
            "label = file_path.split(\"/\")[2]",
            "yield file_path, {",
            "-                        \"image\": {\"filename\": file_path.split(\"/\")[-1], \"data\": file_obj.read()},",
            "+                        \"image\": {\"path\": file_path, \"bytes\": file_obj.read()},",
            "\"label\": label,",
            "}"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9408,
        "label": "no",
        "change": [
            "class ParamRestore(SessionInit):",
            "self.prms = param_dict",
            "",
            "def init(self, sess):",
            "+        sess.run(tf.initialize_all_variables())",
            "variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)",
            "var_dict = dict([v.name, v] for v in variables)",
            "for name, value in self.prms.iteritems():"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9409,
        "label": "no",
        "change": [
            "def build_vtrace_loss(policy, model, dist_class, train_batch):",
            "values = model.value_function()",
            "",
            "if policy.is_recurrent():",
            "-        max_seq_len = torch.max(train_batch[\"seq_lens\"]) - 1",
            "-        mask = sequence_mask(train_batch[\"seq_lens\"], max_seq_len)",
            "-        mask = torch.reshape(mask, [-1])",
            "+        max_seq_len = torch.max(train_batch[\"seq_lens\"])",
            "+        mask_orig = sequence_mask(train_batch[\"seq_lens\"], max_seq_len)",
            "+        mask = torch.reshape(mask_orig, [-1])",
            "else:",
            "mask = torch.ones_like(rewards)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9411,
        "label": "no",
        "change": [
            "class StableDiffusionPipelineIntegrationTests(unittest.TestCase):",
            "pipeline_id = \"CompVis/stable-diffusion-v1-4\"",
            "prompt = \"Andromeda galaxy in a bottle\"",
            "",
            "-        pipeline = StableDiffusionPipeline.from_pretrained(",
            "-            pipeline_id, revision=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\"",
            "-        )",
            "+        pipeline = StableDiffusionPipeline.from_pretrained(pipeline_id, revision=\"fp16\", torch_dtype=torch.float16)",
            "pipeline.enable_attention_slicing(1)",
            "pipeline.enable_sequential_cpu_offload()"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9412,
        "label": "yes",
        "change": [
            "def collect_env():",
            "env_info['GCC'] = gcc",
            "",
            "env_info['PyTorch'] = torch.__version__",
            "-    env_info['PyTorch compiling details'] = torch.__config__.show()",
            "+    env_info['PyTorch compiling details'] = get_build_config()",
            "",
            "env_info['TorchVision'] = torchvision.__version__"
        ],
        "comments": "",
        "Symptom": "low efficiency",
        "Root_Cause": "deprecation management error",
        "Action": "update",
        "Element": "api call"
    },
    {
        "number": 9426,
        "label": "no",
        "change": [
            "class KerasModel(ModelArtifact):",
            "",
            "if os.path.isfile(cls.__get_model_json__path(path)):",
            "# load keras model via json and weights since json file are in path",
            "-            with cls.sess.as_default():",
            "+            with cls.sess.as_default():  # pylint: disable=not-context-manager",
            "with open(cls.__get_model_json__path(path), 'r') as json_file:",
            "model_json = json_file.read()",
            "obj = tfk.models.model_from_json("
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9428,
        "label": "no",
        "change": [
            "def test_lm():",
            "# numpy.testing.assert_equal(rnnlm_ch.predictor.lo.W.data, rnnlm_th.predictor.lo.weight.data.numpy())",
            "",
            "# test prediction equality",
            "-    if torch.__version__.startswith(\"0.3.\"):",
            "+    if torch_is_old:",
            "x = torch.autograd.Variable(",
            "torch.from_numpy(numpy.random.randint(n_vocab, size=(batchsize))),",
            "volatile=True).long()"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9430,
        "label": "no",
        "change": [
            "def conv1d(x, scope, nf, *, w_init_stdev=0.02, params=None, scale=False):",
            "c = mtf.layers.conv1d(x, nf, name=scope, filter_size=1, stride=1,",
            "filter_initializer=tf.random_normal_initializer(stddev=w_init_stdev, dtype=dt))",
            "with tf.variable_scope(scope):",
            "-        singleton = mtf.Dimension('singleton', 1)",
            "+        singletona = mtf.Dimension('singletona', 1)",
            "+        singletonb = mtf.Dimension('singletonb', 1)",
            "",
            "b = mtf.get_variable(x.mesh, 'b', [nf], initializer=tf.constant_initializer(0, dtype=tf.bfloat16), dtype=dt)",
            "# NWC",
            "-        b = mtf.reshape(b, [singleton, singleton, nf])",
            "+        b = mtf.reshape(b, [singletona, singletonb, nf])",
            "",
            "c += b",
            "return c"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9438,
        "label": "no",
        "change": [
            "class Dice(object):",
            "prob = probs[key]",
            "prob._pyro_dims = queries[key]._pyro_dims",
            "mask = prob > 0",
            "-                    if not mask.all():",
            "+                    if torch._C._get_tracing_state() or not mask.all():",
            "mask._pyro_dims = prob._pyro_dims",
            "cost, prob, mask = packed.broadcast_all(cost, prob, mask)",
            "prob = prob[mask]"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9444,
        "label": "no",
        "change": [
            "def logspace(",
            "base=10.0,",
            "axis=None,",
            "*,",
            "+    dtype: tf.DType,",
            "device: str,",
            "out: Union[tf.Tensor, tf.Variable] = None",
            "):",
            "-    power_seq = linspace(start, stop, num, axis, dtype=None, device=device)",
            "+    power_seq = ivy.linspace(start, stop, num, axis, dtype=dtype, device=device)",
            "return base**power_seq"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9445,
        "label": "no",
        "change": [
            "def check_progress_call():",
            "else:",
            "preview_visibility = gr_show(True)",
            "",
            "-    shared.state.current_progress_index += 1",
            "-",
            "return f\"<span style='display: none'>{time.time()}</span><p>{progressbar}</p>\", preview_visibility, image"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9448,
        "label": "no",
        "change": [
            "def get_config():",
            "optimizer=tf.train.MomentumOptimizer(lr, 0.9),",
            "callbacks=Callbacks([",
            "StatPrinter(),",
            "-            PeriodicSaver(),",
            "-            ValidationError(dataset_test, prefix='test'),",
            "+            ModelSaver(),",
            "+            ClassificationError(dataset_test, prefix='validation'),",
            "ScheduledHyperParamSetter('learning_rate',",
            "[(1, 0.1), (20, 0.01), (33, 0.001), (60, 0.0001)])",
            "]),"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9449,
        "label": "no",
        "change": [
            "def spatial_soft_argmax2d(",
            ">>> coords = kornia.spatial_soft_argmax2d(input, False)",
            "tensor([[[1.0000, 1.0000]]])",
            "\"\"\"",
            "-    input_soft: torch.Tensor = dsnt.spatial_softmax_2d(input, temperature)",
            "-    output: torch.Tensor = dsnt.spatial_softargmax_2d(input_soft,",
            "-                                                      normalized_coordinates)",
            "+    input_soft: torch.Tensor = dsnt.spatial_softmax2d(input, temperature)",
            "+    output: torch.Tensor = dsnt.spatial_expectation2d(input_soft, normalized_coordinates)",
            "return output"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9467,
        "label": "no",
        "change": [
            "def unique_values(x: torch.Tensor) -> torch.Tensor:",
            "return ret",
            "",
            "",
            "-def unique_counts(",
            "-    x: torch.Tensor",
            "-) -> Tuple[torch.Tensor, torch.Tensor]:",
            "+def unique_counts(x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:",
            "v, c = torch.unique(torch.reshape(x, [-1]), return_counts=True)",
            "nan_idx = torch.where(torch.isnan(v))",
            "c[nan_idx] = 1"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9472,
        "label": "no",
        "change": [
            "class Stft(torch.nn.Module, InversibleInterface):",
            "pad = self.n_fft // 2",
            "ilens = ilens + 2 * pad",
            "",
            "-            olens = (",
            "-                torch.div((ilens - self.n_fft), self.hop_length, rounding_mode=\"floor\")",
            "-                + 1",
            "-            )",
            "+            olens = (ilens - self.n_fft) // self.hop_length + 1",
            "output.masked_fill_(make_pad_mask(olens, output, 1), 0.0)",
            "else:",
            "olens = None"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9473,
        "label": "no",
        "change": [
            "def convert_transfo_xl_checkpoint_to_pytorch(tf_checkpoint_path,",
            "model = TransfoXLModel(config)",
            "",
            "# Build TF to PyTorch weights loading map",
            "-        tf_to_pt_map = build_tf_to_pytorch_map(model.transformer, config)",
            "+        tf_to_pt_map = build_tf_to_pytorch_map(model, config)",
            "",
            "# Load weights from TF model",
            "init_vars = tf.train.list_variables(tf_path)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9475,
        "label": "no",
        "change": [
            "def inv(",
            "return ret",
            "else:",
            "cofactor = tf.transpose(tf.linalg.inv(x)) * tf.linalg.det(x)",
            "-            inverse = tf.math.multiply(tf.math.divide(",
            "-                1, tf.linalg.det(x)), tf.transpose(cofactor))",
            "+            inverse = tf.math.multiply(",
            "+                tf.math.divide(1, tf.linalg.det(x)), tf.transpose(cofactor)",
            "+            )",
            "ret = inverse",
            "return ret"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9477,
        "label": "no",
        "change": [
            "def triangulate_points(",
            "# 1. Solve the system Ax=0 with smallest eigenvalue",
            "# 2. Return homogeneous coordinates",
            "",
            "-    U, S, V = torch.svd(X)",
            "+    _, _, V = torch.svd(X)",
            "",
            "points3d_h = V[..., -1]",
            "points3d: torch.Tensor = kornia.convert_points_from_homogeneous(points3d_h)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9480,
        "label": "no",
        "change": [
            "def match_snn(desc1: torch.Tensor, desc2: torch.Tensor,",
            "match_dists = ratio[mask]",
            "idxs_in1 = torch.arange(0, idxs_in_2.size(0), device=dm.device)[mask]",
            "idxs_in_2 = idxs_in_2[:, 0][mask]",
            "-    matches_idxs = torch.cat([idxs_in1.view(-1, 1), idxs_in_2.cpu().view(-1, 1)], dim=1)",
            "+    matches_idxs = torch.cat([idxs_in1.view(-1, 1), idxs_in_2.view(-1, 1)], dim=1)",
            "return match_dists.view(-1, 1), matches_idxs.view(-1, 2)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9482,
        "label": "no",
        "change": [
            "slogdet.support_native_out = True",
            "",
            "",
            "def solve(",
            "-    x1: torch.Tensor, x2: torch.Tensor, *, out: Optional[torch.Tensor]",
            "+    x1: torch.Tensor,",
            "+    x2: torch.Tensor,",
            "+    *,",
            "+    out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "if x1.dtype != torch.float:",
            "x1 = x1.type(torch.float)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9486,
        "label": "no",
        "change": [
            "class VersatileDiffusionImageVariationPipelineIntegrationTests(unittest.TestCase",
            "image_slice = image[0, 253:256, 253:256, -1]",
            "",
            "assert image.shape == (1, 512, 512, 3)",
            "-        expected_slice = np.array([0.1205, 0.1914, 0.2289, 0.0883, 0.1595, 0.1683, 0.0703, 0.1493, 0.1298])",
            "+        expected_slice = np.array([0.0441, 0.0469, 0.0507, 0.0575, 0.0632, 0.0650, 0.0865, 0.0909, 0.0945])",
            "+",
            "assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9490,
        "label": "yes",
        "change": [
            "class LabelSmoother:",
            "# will ignore them in any case.",
            "labels.clamp_min_(0)",
            "nll_loss = log_probs.gather(dim=-1, index=labels)",
            "-        smoothed_loss = log_probs.sum(dim=-1, keepdim=True)",
            "+        # works for fp16 input tensor too, by internally upcasting it to fp32",
            "+        smoothed_loss = log_probs.sum(dim=-1, keepdim=True, dtype=torch.float32)",
            "",
            "nll_loss.masked_fill_(padding_mask, 0.0)",
            "smoothed_loss.masked_fill_(padding_mask, 0.0)"
        ],
        "comments": "",
        "Symptom": "unexpected output",
        "Root_Cause": "data conversion error",
        "Action": "update",
        "Element": "api parameter"
    },
    {
        "number": 9497,
        "label": "no",
        "change": [
            "def fmod(",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "result = tf.math.floormod(x1, x2, name=None)",
            "temp = [result, x1]",
            "-    return tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp, fn_output_signature=result.dtype)",
            "+    return tf.map_fn(",
            "+        lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]),",
            "+        temp,",
            "+        fn_output_signature=result.dtype,",
            "+    )",
            "",
            "",
            "def fmax("
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9500,
        "label": "no",
        "change": [
            "def test_gae():",
            "loss = model.loss(z, data.train_pos_edge_index, data.train_neg_adj_mask)",
            "assert loss.item() > 0",
            "",
            "-    auc, ap = model.evaluate(z, data.val_pos_edge_index,",
            "-                             data.val_neg_edge_index)",
            "+    auc, ap = model.test(z, data.val_pos_edge_index, data.val_neg_edge_index)",
            "assert auc >= 0 and auc <= 1 and ap >= 0 and ap <= 1"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9505,
        "label": "no",
        "change": [
            "def one_hot(",
            "raise ValueError(\"The number of classes must be bigger than one.\" \" Got: {}\".format(num_classes))",
            "",
            "shape = labels.shape",
            "-    one_hot = torch.zeros((shape[0], num_classes) + shape[1:], device=device, dtype=dtype)",
            "+    one_hot = zeros((shape[0], num_classes) + shape[1:], device=device, dtype=dtype)",
            "",
            "return one_hot.scatter_(1, labels.unsqueeze(1), 1.0) + eps"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9513,
        "label": "no",
        "change": [
            "class TFModel(Trainable, Inferable, metaclass=TfModelMeta):",
            "print('\\n:: Model saved to {} \\n'.format(self.model_path_.as_posix()))",
            "",
            "def get_checkpoint_state(self):",
            "-        return tf.train.get_checkpoint_state(self.model_path_.as_posix())",
            "+        return tf.train.get_checkpoint_state(self.model_path_.parent)",
            "",
            "@check_path_exists()",
            "def load(self):"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9518,
        "label": "no",
        "change": [
            "class _BinaryPreprocessing(torch.nn.Module):",
            "v = torch.stack(v)",
            "",
            "if torch.jit.isinstance(v, torch.Tensor):",
            "-            return v.to(dtype=torch.bool)",
            "+            return v.to(dtype=torch.float32)",
            "",
            "v = [s.strip() for s in v]",
            "if self.should_lower:",
            "v = [s.lower() for s in v]",
            "indices = [self.str2bool.get(s, False) for s in v]",
            "-        return torch.tensor(indices, dtype=torch.bool)",
            "+        return torch.tensor(indices, dtype=torch.float32)",
            "",
            "",
            "class _BinaryPostprocessing(torch.nn.Module):"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9524,
        "label": "no",
        "change": [
            "def add_input_distortions(flip_left_right, random_crop, random_scale,",
            "precrop_shape_as_int = tf.cast(precrop_shape, dtype=tf.int32)",
            "precropped_image = tf.image.resize_bilinear(decoded_image_4d,",
            "precrop_shape_as_int)",
            "-  precropped_image_3d = tf.squeeze(precropped_image, squeeze_dims=[0])",
            "+  precropped_image_3d = tf.squeeze(precropped_image, axis=[0])",
            "cropped_image = tf.random_crop(precropped_image_3d,",
            "[input_height, input_width, input_depth])",
            "if flip_left_right:"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9531,
        "label": "no",
        "change": [
            "class ShaDowKHopSampler(torch.utils.data.DataLoader):",
            "batch.edge_index = torch.stack([row, col], dim=0)",
            "",
            "for k, v in self.data:",
            "-            if k in ['edge_index', 'adj_t', 'num_nodes']:",
            "+            if k in ['edge_index', 'adj_t']:",
            "continue",
            "if k == 'y' and v.size(0) == self.data.num_nodes:",
            "batch[k] = v[n_id][root_n_id]"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9535,
        "label": "yes",
        "change": [
            "def prod(",
            "dtype = tf.uint64",
            "if ivy.exists(out):",
            "return ivy.inplace_update(",
            "-            out, tf.experimental.numpy.prod(x, axis=axis, keepdims=keepdims)",
            "+            out,",
            "+            tf.experimental.numpy.prod(x, axis=axis, dtype=dtype, keepdims=keepdims),",
            ")",
            "else:",
            "return tf.experimental.numpy.prod(x, axis, dtype, keepdims)"
        ],
        "comments": "",
        "Symptom": "unexpected output",
        "Root_Cause": "data conversion error",
        "Action": "update",
        "Element": "api parameter"
    },
    {
        "number": 9550,
        "label": "no",
        "change": [
            "class PGProbRatioModel(PGModel):",
            "log_probs.append(log_prob)",
            "log_prob = tf.reduce_mean(input_tensor=tf.concat(values=log_probs, axis=1), axis=1)",
            "prob_ratio = tf.exp(x=(log_prob - reference))",
            "-        return tf.reduce_mean(input_tensor=(-prob_ratio * reward), axis=0)",
            "+        return tf.reduce_mean(input_tensor=(prob_ratio * reward), axis=0)",
            "",
            "def get_optimizer_kwargs(self, states, actions, terminal, reward, internals):",
            "kwargs = super(PGProbRatioModel, self).get_optimizer_kwargs("
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9555,
        "label": "no",
        "change": [
            "class DiffusionPriorNetwork(nn.Module):",
            "learned_queries = repeat(self.learned_query, 'd -> b 1 d', b = batch)",
            "",
            "if self.self_cond:",
            "-            learned_queries = torch.cat((image_embed, self_cond), dim = -2)",
            "+            learned_queries = torch.cat((self_cond, learned_queries), dim = -2)",
            "",
            "tokens = torch.cat((",
            "text_encodings,"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9558,
        "label": "no",
        "change": [
            "class TriAdaptiveModel(nn.Module):",
            ":param global_step: number of current training step.",
            ":param kwargs: Placeholder for passing generic parameters.",
            "Note: Contains the batch (as dict of tensors), when called from Trainer.train().",
            "-        :return loss: torch.Tensor that is the per sample loss (len: batch_size)",
            "+        :return: loss: torch.Tensor that is the per sample loss (len: batch_size)",
            "\"\"\"",
            "all_losses = self.logits_to_loss_per_head(logits, **kwargs)",
            "# This aggregates the loss per sample across multiple prediction heads"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9559,
        "label": "no",
        "change": [
            "def loss(self, net_out):",
            "loss = tf.multiply(loss, wght)",
            "loss = tf.reshape(loss, [-1, H*W*B*(4 + 1 + C)])",
            "loss = tf.reduce_sum(loss, 1)",
            "-    self.loss = .5 * tf.reduce_mean(loss)",
            "-",
            "-",
            "+    self.loss = .5 * tf.reduce_mean(loss)",
            "\\ No newline at end of file"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9561,
        "label": "no",
        "change": [
            "class TestLAFOrienter:",
            "def test_toy(self, device):",
            "ori = LAFOrienter(32).to(device)",
            "inp = torch.zeros(1, 1, 19, 19, device=device)",
            "-        inp[:, :, :10, :] = 1",
            "+        inp[:, :, :, :10] = 1",
            "laf = torch.tensor([[[[0, 5., 8.], [5.0, 0., 8.]]]], device=device)",
            "new_laf = ori(laf, inp)",
            "-        expected = torch.tensor([[[[5., 0., 8.], [0., 5., 8.]]]], device=device)",
            "+        expected = torch.tensor([[[[0., 5., 8.], [-5.0, 0, 8.]]]], device=device)",
            "assert_allclose(new_laf, expected)",
            "",
            "def test_gradcheck(self, device):"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9574,
        "label": "no",
        "change": [
            "class ARGVA(ARGA):",
            "def loss(self, mu, logvar, pos_edge_index, neg_adj_mask):",
            "z = self.sample(mu, logvar)",
            "recon_loss = self.reconstruction_loss(z, pos_edge_index, neg_adj_mask)",
            "-        kl_loss = self.kl_loss(z, mu, logvar)",
            "+        kl_loss = self.kl_loss(mu, logvar)",
            "d_loss = self.discriminator_loss(*self.discriminate(z))",
            "return recon_loss + kl_loss + d_loss"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9575,
        "label": "yes",
        "change": [
            "def is_cuda_available() -> bool:",
            "Unlike :func:`torch.cuda.is_available`, this function will do its best not to create a CUDA context for fork",
            "support, if the platform allows it.",
            "\"\"\"",
            "-    if \"fork\" not in torch.multiprocessing.get_all_start_methods():",
            "+    if \"fork\" not in torch.multiprocessing.get_all_start_methods() or _is_forking_disabled():",
            "return torch.cuda.is_available()",
            "with multiprocessing.get_context(\"fork\").Pool(1) as pool:",
            "return pool.apply(torch.cuda.is_available)"
        ],
        "comments": "",
        "Symptom": "unexpected output",
        "Root_Cause": "deprecation management error",
        "Action": "change",
        "Element": "api condition check"
    },
    {
        "number": 9579,
        "label": "no",
        "change": [
            "from .random_scale import RandomScale",
            "",
            "class RandomScaleTest(TestCase):",
            "def test_random_scale(self):",
            "-        position = [[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7]]",
            "-        position = torch.FloatTensor(position)",
            "+        position = [[0, 0, 0], [1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]]",
            "+        position = torch.FloatTensor(position) - 2",
            "data = Data(None, None, position, None)",
            "-        data = RandomScale(1)(data)",
            "+        data = RandomScale(2)(data)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9584,
        "label": "no",
        "change": [
            "def get_masks(slen, lengths, causal, padding_mask=None):",
            "\"\"\"",
            "Generate hidden states mask, and optionally an attention mask.",
            "\"\"\"",
            "-    bs = lengths.size(0)",
            "+    alen = torch.arange(slen, dtype=torch.long, device=lengths.device)",
            "if padding_mask is not None:",
            "mask = padding_mask",
            "else:",
            "assert lengths.max().item() <= slen",
            "-        alen = torch.arange(slen, dtype=torch.long, device=lengths.device)",
            "mask = alen < lengths[:, None]",
            "",
            "# attention mask is the same as mask, or triangular inferior attention (causal)",
            "if causal:",
            "+        bs = lengths.size(0)",
            "attn_mask = alen[None, None, :].repeat(bs, slen, 1) <= alen[None, :, None]",
            "else:",
            "attn_mask = mask"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9588,
        "label": "no",
        "change": [
            "from .modeling_tf_utils import TFPreTrainedModel, TFSharedEmbeddings, shape_list",
            "logger = logging.getLogger(__name__)",
            "",
            "TF_T5_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"t5-small\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-tf_model.h5\",",
            "-    \"t5-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-tf_model.h5\",",
            "-    \"t5-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-large-tf_model.h5\",",
            "-    \"t5-3b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-3b-tf_model.h5\",",
            "-    \"t5-11b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-11b-tf_model.h5\",",
            "+    \"t5-small\": \"https://cdn.huggingface.co/t5-small-tf_model.h5\",",
            "+    \"t5-base\": \"https://cdn.huggingface.co/t5-base-tf_model.h5\",",
            "+    \"t5-large\": \"https://cdn.huggingface.co/t5-large-tf_model.h5\",",
            "+    \"t5-3b\": \"https://cdn.huggingface.co/t5-3b-tf_model.h5\",",
            "+    \"t5-11b\": \"https://cdn.huggingface.co/t5-11b-tf_model.h5\",",
            "}",
            "",
            "####################################################"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9589,
        "label": "no",
        "change": [
            "def setup(app):",
            "",
            "# @jpchen's hack to get rtd builder to install latest pytorch",
            "if 'READTHEDOCS' in os.environ:",
            "-    os.system('pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp36-cp36m-linux_x86_64.whl')",
            "+    os.system('pip install torch==1.2.0+cpu -f https://download.pytorch.org/whl/torch_stable.html')"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9599,
        "label": "no",
        "change": [
            "def image_histogram2d(",
            "",
            "hist = torch.sum(kernel_values, dim=(-2, -1)).permute(1, 2, 0)",
            "if return_pdf:",
            "-        normalization = torch.sum(hist, dim=-1).unsqueeze(0) + eps",
            "+        normalization = torch.sum(hist, dim=-1, keepdim=True) + eps",
            "pdf = hist / normalization",
            "if image.dim() == 2:",
            "hist = hist.squeeze()"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9607,
        "label": "no",
        "change": [
            "class SampledSoftmaxLoss(torch.nn.Module):",
            "",
            "if embeddings.shape[0] == 0:",
            "# empty batch",
            "-            return torch.tensor(0.0).to(embeddings.device)",
            "+            return torch.tensor(0.0, device=embeddings.device)",
            "",
            "if not self.training:",
            "return self._forward_eval(embeddings, targets)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9615,
        "label": "no",
        "change": [
            "class TestRgbToHsv(BaseTester):",
            "assert_close(kornia.color.rgb_to_hsv(data), expected)",
            "",
            "def test_nan_rgb_to_hsv(self, device, dtype):",
            "-        data = torch.zeros(1, 5, 5, device=device, dtype=dtype)  # 3x5x5",
            "-        data = data.repeat(3, 1, 1)  # 2x3x5x5",
            "-",
            "+        data = torch.zeros(3, 5, 5, device=device, dtype=dtype)  # 3x5x5",
            "expected = torch.zeros_like(data)  # 3x5x5",
            "assert_close(kornia.color.rgb_to_hsv(data), expected)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9616,
        "label": "no",
        "change": [
            "class Curiosity(Exploration):",
            "{",
            "SampleBatch.OBS: torch.cat(",
            "[",
            "-                        torch.from_numpy(sample_batch[SampleBatch.OBS]),",
            "-                        torch.from_numpy(sample_batch[SampleBatch.NEXT_OBS]),",
            "+                        torch.from_numpy(sample_batch[SampleBatch.OBS]).to(",
            "+                            policy.device",
            "+                        ),",
            "+                        torch.from_numpy(sample_batch[SampleBatch.NEXT_OBS]).to(",
            "+                            policy.device",
            "+                        ),",
            "]",
            ")",
            "}"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9620,
        "label": "no",
        "change": [
            "def add_moving_summary(*args, **kwargs):",
            "ema_var, c, decay,",
            "zero_debias=True, name=name + '_EMA_apply')",
            "ema_ops.append(ema_op)",
            "-        # cannot add it into colocate group -- will force everything to cpus",
            "-        tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary",
            "+        with tf.name_scope(None):",
            "+            # cannot add it into colocate group -- will force everything to cpus",
            "+            tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary",
            "if coll is not None:",
            "for op in ema_ops:",
            "# TODO a new collection to summary every step?"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9625,
        "label": "no",
        "change": [
            "def train_test_split(pd_dataframe):",
            "Training data - 45 initial at-bats and hits for each player.",
            "Validation data - Full season at-bats and hits for each player.",
            "\"\"\"",
            "-    train_data = torch.tensor(pd_dataframe.as_matrix([\"At-Bats\", \"Hits\"]), dtype=torch.float)",
            "-    test_data = torch.tensor(pd_dataframe.as_matrix([\"SeasonAt-Bats\", \"SeasonHits\"]), dtype=torch.float)",
            "+    train_data = torch.tensor(pd_dataframe[[\"At-Bats\", \"Hits\"]].values, dtype=torch.float)",
            "+    test_data = torch.tensor(pd_dataframe[[\"SeasonAt-Bats\", \"SeasonHits\"]].values, dtype=torch.float)",
            "first_name = pd_dataframe[\"FirstName\"].values",
            "last_name = pd_dataframe[\"LastName\"].values",
            "player_names = [\" \".join([first, last]) for first, last in zip(first_name, last_name)]"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9630,
        "label": "no",
        "change": [
            "class CovidQaUcsd(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure the file is present in the directory specified in the data_dir specified in the input {} `datasets.load_dataset('covid_qa_ucsd', 'en', data_dir=...)`. Manual download instructions: {})\".format(",
            "-                    path_to_manual_file, dl_manager.manual_dir, self.manual_download_instructions",
            "-                )",
            "+                f\"{path_to_manual_file} does not exist. Make sure the file is present in the directory specified in the data_dir specified in the input {dl_manager.manual_dir} `datasets.load_dataset('covid_qa_ucsd', 'en', data_dir=...)`. Manual download instructions: {self.manual_download_instructions})\"",
            ")",
            "",
            "return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepath\": path_to_manual_file})]"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9631,
        "label": "yes",
        "change": [
            "class ConditionalDetrModelIntegrationTests(unittest.TestCase):",
            "results = feature_extractor.post_process_object_detection(",
            "outputs, threshold=0.3, target_sizes=[image.size[::-1]]",
            ")[0]",
            "-        expected_scores = torch.tensor([0.8330, 0.8313, 0.8039, 0.6829, 0.5355])",
            "+        expected_scores = torch.tensor([0.8330, 0.8313, 0.8039, 0.6829, 0.5355]).to(torch_device)",
            "expected_labels = [75, 17, 17, 75, 63]",
            "-        expected_slice_boxes = torch.tensor([38.3089, 72.1022, 177.6293, 118.4512])",
            "+        expected_slice_boxes = torch.tensor([38.3089, 72.1022, 177.6293, 118.4512]).to(torch_device)",
            "",
            "self.assertEqual(len(results[\"scores\"]), 5)",
            "self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-4))"
        ],
        "comments": "",
        "Symptom": "unexpected output",
        "Root_Cause": "deprecation management error",
        "Action": "change",
        "Element": "api call"
    },
    {
        "number": 9632,
        "label": "no",
        "change": [
            "class EnqueueThread(threading.Thread):",
            "feed = dict(zip(self.placehdrs, dp))",
            "# print 'qsize:', self.sess.run([self.op, self.size_op], feed_dict=feed)[1]",
            "self.op.run(feed_dict=feed)",
            "-            except tf.errors.CancelledError as e:",
            "+            except tf.errors.CancelledError:",
            "pass",
            "except Exception:",
            "logger.exception(\"Exception in EnqueueThread:\")"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9633,
        "label": "yes",
        "change": [
            "class GraphVarParam(HyperParam):",
            "self._readable_name, self.var_name = get_op_var_name(name)",
            "",
            "def setup_graph(self):",
            "-        all_vars = tf.all_variables()",
            "+        try:",
            "+            all_vars = tf.global_variables()",
            "+        except:",
            "+            # TODO",
            "+            all_vars = tf.all_variables()",
            "+",
            "for v in all_vars:",
            "if v.name == self.var_name:",
            "self.var = v"
        ],
        "comments": "",
        "Symptom": "low efficiency",
        "Root_Cause": "deprecation management error",
        "Action": "update",
        "Element": "api call"
    },
    {
        "number": 9636,
        "label": "yes",
        "change": [
            "class FAUST(Dataset):",
            "index = self.index[:, i]",
            "weight = torch.FloatTensor(index.size(1)).fill_(1)",
            "input = torch.FloatTensor(position.size(0)).fill_(1)",
            "-        adj = torch.sparse.FloatTensor(index, weight, torch.Size([75, 75]))",
            "+        adj = torch.sparse.FloatTensor(index, weight, torch.Size([6890, 6890]))",
            "data = (input, adj, position)",
            "",
            "if self.correspondence:"
        ],
        "comments": "",
        "Symptom": "low efficiency",
        "Root_Cause": "data conversion error",
        "Action": "update",
        "Element": "api parameter"
    },
    {
        "number": 9642,
        "label": "no",
        "change": [
            "class AlbertForPreTraining(AlbertPreTrainedModel):",
            ">>> from transformers import AlbertTokenizer, AlbertForPreTraining",
            ">>> import torch",
            "",
            "-        >>> tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')",
            "-        >>> model = AlbertForPreTraining.from_pretrained('albert-base-v2')",
            "+        >>> tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")",
            "+        >>> model = AlbertForPreTraining.from_pretrained(\"albert-base-v2\")",
            "",
            "-        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1",
            "+        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(",
            "+        ...     0",
            "+        >>> )  # Batch size 1",
            ">>> outputs = model(input_ids)",
            "",
            ">>> prediction_logits = outputs.prediction_logits"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9643,
        "label": "no",
        "change": [
            "class AudioFeatureMixin(BaseFeatureMixin):",
            "",
            "feature_length = audio_feature.shape[0]",
            "broadcast_feature_length = min(feature_length, max_length)",
            "-        audio_feature_padded = torch.full((max_length, feature_dim), padding_value, dtype=torch.float32)",
            "+        audio_feature_padded = torch.full(",
            "+            (max_length, feature_dim), padding_value, dtype=torch.float32, device=audio_feature.device",
            "+        )",
            "audio_feature_padded[:broadcast_feature_length, :] = audio_feature[:max_length, :]",
            "",
            "return audio_feature_padded"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9648,
        "label": "no",
        "change": [
            "class Transformer(nn.Module):",
            "attn_mask: torch.tensor(bs, seq_length) Attention mask on the sequence.",
            "",
            "Returns:",
            "-            hidden_state: torch.tensor(bs, seq_length, dim) Sequence of hiddens states in the last (top)",
            "+            hidden_state: torch.tensor(bs, seq_length, dim) Sequence of hidden states in the last (top)",
            "layer all_hidden_states: Tuple[torch.tensor(bs, seq_length, dim)]",
            "Tuple of length n_layers with the hidden states from each layer.",
            "Optional: only if output_hidden_states=True"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9650,
        "label": "no",
        "change": [
            "class LineSearch(Iterative):",
            "# Trivial operation to enforce control dependency",
            "return tf.less(x=value, y=value)  # == False",
            "",
            "-        improved = tf.cond(",
            "+        improved = self.cond(",
            "pred=(improvement > last_improvement),",
            "true_fn=(lambda: True),",
            "false_fn=undo_deltas"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9655,
        "label": "no",
        "change": [
            "class PrioritizedReplay(Memory):",
            "value=tf.zeros(shape=tf.shape(self.batch_indices), dtype=tf.int32)",
            ")",
            "with tf.control_dependencies(control_inputs=(assignment,)):",
            "-            priority_indices = tf.cond(",
            "+            priority_indices = self.cond(",
            "pred=num_priority_elements > 0,",
            "true_fn=sampling_fn,",
            "false_fn=lambda: tf.zeros(shape=(num_priority_elements,), dtype=tf.int32)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9664,
        "label": "no",
        "change": [
            "class LoadImagesAndLabels(Dataset):",
            "n = len(shapes) // 4",
            "img4, label4, path4, shapes4 = [], [], path[:n], shapes[:n]",
            "",
            "-        ho = torch.tensor([[0., 0, 0, 1, 0, 0]])",
            "-        wo = torch.tensor([[0., 0, 1, 0, 0, 0]])",
            "-        s = torch.tensor([[1, 1, .5, .5, .5, .5]])  # scale",
            "+        ho = torch.tensor([[0.0, 0, 0, 1, 0, 0]])",
            "+        wo = torch.tensor([[0.0, 0, 1, 0, 0, 0]])",
            "+        s = torch.tensor([[1, 1, 0.5, 0.5, 0.5, 0.5]])  # scale",
            "for i in range(n):  # zidane torch.zeros(16,3,720,1280)  # BCHW",
            "i *= 4",
            "if random.random() < 0.5:",
            "-                im = F.interpolate(img[i].unsqueeze(0).float(), scale_factor=2., mode='bilinear', align_corners=False)[",
            "+                im = F.interpolate(img[i].unsqueeze(0).float(), scale_factor=2.0, mode='bilinear', align_corners=False)[",
            "0].type(img[i].type())",
            "l = label[i]",
            "else:"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9665,
        "label": "no",
        "change": [
            "class Energy(AbsFeatsExtract):",
            "",
            "# input_stft: (..., F, 2) -> (..., F)",
            "input_power = input_stft[..., 0] ** 2 + input_stft[..., 1] ** 2",
            "-        energy = torch.sqrt(torch.clamp(input_power.sum(dim=1), min=1.0e-10))",
            "+        # sum over frequency (B, N, F) -> (B, N)",
            "+        energy = torch.sqrt(torch.clamp(input_power.sum(dim=2), min=1.0e-10))",
            "",
            "# (Optional): Adjust length to match with the mel-spectrogram",
            "if feats_lengths is not None:"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9671,
        "label": "no",
        "change": [
            "def bbox_transform_inv_tf(boxes, deltas):",
            "dh = deltas[:, 3]",
            "",
            "pred_ctr_x = tf.add(tf.multiply(dx, widths), ctr_x)",
            "-  pred_ctr_y = tf.add(tf.multiply(dy, widths), ctr_y)",
            "+  pred_ctr_y = tf.add(tf.multiply(dy, heights), ctr_y)",
            "pred_w = tf.multiply(tf.exp(dw), widths)",
            "pred_h = tf.multiply(tf.exp(dh), heights)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9675,
        "label": "no",
        "change": [
            "class NaiveBeta(Beta):",
            "alpha_beta = torch.stack([alpha, beta], -1)",
            "self._gamma = Gamma(alpha_beta, torch.ones_like(alpha_beta))",
            "",
            "-    def sample(self, sample_shape=torch.Size()):",
            "-        gammas = self._gamma.sample(sample_shape)",
            "+    def rsample(self, sample_shape=torch.Size()):",
            "+        gammas = self._gamma.rsample(sample_shape)",
            "probs = gammas / gammas.sum(-1, True)",
            "return probs[..., 0]"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9676,
        "label": "no",
        "change": [
            "class LightningModule(",
            "def __to_tensor(self, value: numbers.Number) -> torch.Tensor:",
            "return torch.tensor(value, device=self.device)",
            "",
            "-    def log_grad_norm(self, grad_norm_dict: Dict[str, torch.Tensor]) -> None:",
            "+    def log_grad_norm(self, grad_norm_dict: Dict[str, float]) -> None:",
            "\"\"\"Override this method to change the default behaviour of ``log_grad_norm``.",
            "",
            "Args:"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9683,
        "label": "no",
        "change": [
            "def add_points_features_to_volume_densities_features(",
            "",
            "# init the volumetric grid sizes if uninitialized",
            "if grid_sizes is None:",
            "-        grid_sizes = torch.LongTensor(list(volume_densities.shape[2:])).to(",
            "-            volume_densities",
            "+        # grid sizes shape (minibatch, 3)",
            "+        grid_sizes = (",
            "+            torch.LongTensor(list(volume_densities.shape[2:]))",
            "+            .to(volume_densities)",
            "+            .expand(volume_densities.shape[0], 3)",
            ")",
            "",
            "# flatten densities and features"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9686,
        "label": "no",
        "change": [
            "class RunningStandardize(Preprocessor):",
            "# Standardize tensor",
            "return (tensor - mean_estimate) / tf.maximum(x=tf.sqrt(x=variance_estimate), y=util.epsilon)",
            "",
            "-            return tf.cond(pred=(count > 1.0), true_fn=later_run, false_fn=first_run)",
            "+            return self.cond(pred=(count > 1.0), true_fn=later_run, false_fn=first_run)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9689,
        "label": "no",
        "change": [
            "class PPO(object):",
            "advantage = tfdc_r - self.critic(s)",
            "closs = tf.reduce_mean(tf.square(advantage))",
            "grad = tape.gradient(closs, self.critic.trainable_weights)",
            "-        tf.optimizers.Adam(C_LR).apply_gradients(zip(grad, self.critic.trainable_weights))",
            "+        self.critic_opt.apply_gradients(zip(grad, self.critic.trainable_weights))",
            "",
            "def cal_adv(self, tfs, tfdc_r):",
            "'''"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9692,
        "label": "no",
        "change": [
            "class NaturalGradient(Optimizer):",
            "# Solve the following system for delta' via the conjugate gradient solver.",
            "# [delta' * F] * delta' = -grad(loss)",
            "# --> delta'  (= lambda * delta)",
            "-        deltas = self.solver.solve(fn_x=fisher_matrix_product, x_init=None, b=[-grad for grad in loss_gradients], f_args=())",
            "+        deltas = self.solver.solve(fn_x=fisher_matrix_product, x_init=None, b=[-grad for grad in loss_gradients])",
            "",
            "# delta' * F",
            "delta_fisher_matrix_product = fisher_matrix_product(deltas=deltas)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9696,
        "label": "no",
        "change": [
            "def random_crop_size_generator(",
            "# Element-wise w, h condition",
            "cond = ((0 < w) * (w < size[0]) * (0 < h) * (h < size[1])).int()",
            "",
            "-    # torch.argmax is not reproducible accross devices: https://github.com/pytorch/pytorch/issues/17738",
            "-    # Here, we will select the first occurance of the duplicated elements.",
            "+    # torch.argmax is not reproducible across devices: https://github.com/pytorch/pytorch/issues/17738",
            "+    # Here, we will select the first occurrence of the duplicated elements.",
            "cond_bool, argmax_dim1 = ((cond.cumsum(1) == 1) & cond.bool()).max(1)",
            "h_out = w[torch.arange(0, batch_size, device=device, dtype=torch.long), argmax_dim1]",
            "w_out = h[torch.arange(0, batch_size, device=device, dtype=torch.long), argmax_dim1]"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9698,
        "label": "yes",
        "change": [
            "class TestElmo(AllenNlpTestCase):",
            "dataset = Dataset(instances)",
            "vocab = Vocabulary()",
            "dataset.index_instances(vocab)",
            "-        character_ids = dataset.as_array_dict()['elmo']['character_ids']",
            "+        character_ids = dataset.as_tensor_dict()['elmo']['character_ids']",
            "",
            "-        output = elmo(Variable(torch.from_numpy(character_ids)))",
            "+        output = elmo(character_ids)",
            "elmo_representations = output['elmo_representations']",
            "mask = output['mask']"
        ],
        "comments": "",
        "Symptom": "low efficiency",
        "Root_Cause": "data conversion error",
        "Action": "update",
        "Element": "api call"
    },
    {
        "number": 9700,
        "label": "no",
        "change": [
            "class CrfTagger(Model):",
            "\"\"\"",
            "# Parameters",
            "",
            "-        tokens : ``Dict[str, torch.LongTensor]``, required",
            "+        tokens : ``TextFieldTensors``, required",
            "The output of ``TextField.as_array()``, which should typically be passed directly to a",
            "``TextFieldEmbedder``. This output is a dictionary mapping keys to ``TokenIndexer``",
            "tensors.  At its most basic, using a ``SingleIdTokenIndexer`` this is : ``{\"tokens\":"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9704,
        "label": "no",
        "change": [
            "\"\\n\",",
            "\"```python\\n\",",
            "\"data = Variable(torch.zeros(10, 1))\\n\",",
            "-    \"data[0:6, 0].data = torch.ones(6)  # 6 heads and 4 tails\\n\",",
            "+    \"data.data[0:6, 0] = torch.ones(6)  # 6 heads and 4 tails\\n\",",
            "\"```\\n\",",
            "\"\\n\",",
            "\"Then we have:\\n\","
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9705,
        "label": "no",
        "change": [
            "class AutoResumeTrainConfig(TrainConfig):",
            "if not dir:",
            "return None",
            "path = os.path.join(dir, 'checkpoint')",
            "-        if not tf.gfile.Exists(path):",
            "+        if not tfv1.gfile.Exists(path):",
            "return None",
            "return SaverRestore(path)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9707,
        "label": "no",
        "change": [
            "def point_line_distance(p, v0, v1):",
            "",
            "v1v0 = v1 - v0",
            "l2 = v1v0.dot(v1v0)  # |v1 - v0|^2",
            "-    if l2 == 0.0:",
            "-        return torch.sqrt((p - v1).dot(p - v1))  # v0 == v1",
            "+    if l2 <= kEpsilon:",
            "+        return (p - v1).dot(p - v1)  # v0 == v1",
            "",
            "t = (v1v0).dot(p - v0) / l2",
            "t = torch.clamp(t, min=0.0, max=1.0)"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    },
    {
        "number": 9709,
        "label": "no",
        "change": [
            "class StableDiffusionInpaintPipeline(DiffusionPipeline):",
            "latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents",
            "",
            "# concat latents, mask, masked_image_latents in the channel dimension",
            "-            latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1)",
            "-",
            "latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)",
            "+            latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1)",
            "",
            "# predict the noise residual",
            "noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample"
        ],
        "comments": "",
        "Symptom": "",
        "Root_Cause": "",
        "Action": "",
        "Element": ""
    }
]