[
    {
        "commit_hash": "edb78974b84b324df08ec198e6ff7f267ee23d33",
        "index": "721197fe..365f4712 100644",
        "commit_message": "upgrade detection / some v2 compat fix\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _replace_global_by_local(kwargs):",
            "if 'collections' in kwargs:",
            "collections = kwargs['collections']",
            "if not collections:",
            "-        collections = {tf.GraphKeys.GLOBAL_VARIABLES}",
            "+        collections = {tfv1.GraphKeys.GLOBAL_VARIABLES}",
            "else:",
            "collections = set(collections.copy())",
            "-    collections.remove(tf.GraphKeys.GLOBAL_VARIABLES)",
            "-    collections.add(tf.GraphKeys.LOCAL_VARIABLES)",
            "+    collections.remove(tfv1.GraphKeys.GLOBAL_VARIABLES)",
            "+    collections.add(tfv1.GraphKeys.LOCAL_VARIABLES)",
            "kwargs['collections'] = list(collections)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tfv1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tfv1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tfv1')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 3,
        "number": 8824,
        "neg_line": [
            "-collections = {tf.GraphKeys.GLOBAL_VARIABLES}",
            "-collections.remove(tf.GraphKeys.GLOBAL_VARIABLES)",
            "-collections.add(tf.GraphKeys.LOCAL_VARIABLES)"
        ],
        "pos_line": [
            "+collections = {tfv1.GraphKeys.GLOBAL_VARIABLES}",
            "+collections.remove(tfv1.GraphKeys.GLOBAL_VARIABLES)",
            "+collections.add(tfv1.GraphKeys.LOCAL_VARIABLES)"
        ],
        "core_change": "-collections = {tf.GraphKeys.GLOBAL_VARIABLES} +collections = {tfv1.GraphKeys.GLOBAL_VARIABLES} -collections.remove(tf.GraphKeys.GLOBAL_VARIABLES) -collections.add(tf.GraphKeys.LOCAL_VARIABLES) +collections.remove(tfv1.GraphKeys.GLOBAL_VARIABLES) +collections.add(tfv1.GraphKeys.LOCAL_VARIABLES)",
        "core_API": "copy"
    },
    {
        "commit_hash": "2944bc68a9372606c1fd5de2f76e00076b5d26e9",
        "index": "00f4da0d..03688adf 100644",
        "commit_message": "Update regularizers.py\n\nFix OrthogonalRegularizer to implement the (1,1) matrix norm. \nIssue: https://github.com/keras-team/keras/issues/16518\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class OrthogonalRegularizer(Regularizer):",
            "size = inputs.shape[1]",
            "product_no_diagonal = product * (1. - tf.eye(size, dtype=inputs.dtype))",
            "num_pairs = size * (size - 1.) / 2.",
            "-    return self.factor * 0.5 * tf.reduce_sum(product_no_diagonal) / num_pairs",
            "+    return self.factor * 0.5 * tf.reduce_sum(tf.abs(product_no_diagonal)) / num_pairs",
            "",
            "def get_config(self):",
            "return {'factor': float(self.factor), 'mode': self.mode}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('(', '('), position=0, insert_id=2057744)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=2057745)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=2057746)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2057747)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2057748)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2057749)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'abs'), position=2, insert_id=2057750)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 8832,
        "neg_line": [
            "-return self.factor * 0.5 * tf.reduce_sum(product_no_diagonal) / num_pairs"
        ],
        "pos_line": [
            "+return self.factor * 0.5 * tf.reduce_sum(tf.abs(product_no_diagonal)) / num_pairs"
        ],
        "core_change": "-return self.factor * 0.5 * tf.reduce_sum(product_no_diagonal) / num_pairs +return self.factor * 0.5 * tf.reduce_sum(tf.abs(product_no_diagonal)) / num_pairs",
        "core_API": "eye"
    },
    {
        "commit_hash": "51bff2ed2cc7b455c7718370ace8a395d49f7cab",
        "index": "2e210a26f..86750e30f 100644",
        "commit_message": "Float tensor additive sharing (#3094)\n\n* Convert torch.zeros from float to long tensor\n\n* Catch FloatTensor sharing attempt\n\n* format correction\n\n* reformatting\n\n* fix maxpool TensorFloat sharing\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def maxpool_deriv(x_sh):",
            "# 3)",
            "t = k_sh.get()",
            "k = t % n",
            "-    E_k = torch.zeros(n)",
            "+    E_k = torch.zeros(n).long()",
            "E_k[k] = 1",
            "E_sh = E_k.share(alice, bob, **no_wrap)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=798659)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=798660)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=798661)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'long'), position=2, insert_id=798662)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=798663)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=798664)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 8839,
        "neg_line": [
            "-E_k = torch.zeros(n)"
        ],
        "pos_line": [
            "+E_k = torch.zeros(n).long()"
        ],
        "core_change": "-E_k = torch.zeros(n) +E_k = torch.zeros(n).long()",
        "core_API": "get"
    },
    {
        "commit_hash": "19e186a7d9ac05c14d5cc0f6a99eb30217cd85c9",
        "index": "c6608377db..9bcffd8360 100644",
        "commit_message": "fix tests  in tensorflow math frontend\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def add(",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "x1, x2 = ivy.promote_types_of_inputs(x1, x2)",
            "-    return tf.experimental.numpy.add(x1, x2)",
            "+    return tf.add(x1, x2)",
            "",
            "",
            "def asin("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=experimental))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=numpy))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 8842,
        "neg_line": [
            "-return tf.experimental.numpy.add(x1, x2)"
        ],
        "pos_line": [
            "+return tf.add(x1, x2)"
        ],
        "core_change": "-return tf.experimental.numpy.add(x1, x2) +return tf.add(x1, x2)",
        "core_API": "promote_types_of_inputs"
    },
    {
        "commit_hash": "f9ec87999d7b2cd40c2fc6a47e1f393d4b670fc5",
        "index": "51131345..69330882 100644",
        "commit_message": "py 2.7 fix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ToDense(object):",
            "else:",
            "edge_attr = data.edge_attr",
            "",
            "-        size = torch.Size([num_nodes, num_nodes, *list(edge_attr.size())[1:]])",
            "+        size = torch.Size([num_nodes, num_nodes] + list(edge_attr.size())[1:])",
            "adj = torch.sparse_coo_tensor(data.edge_index, edge_attr, size)",
            "data.adj = adj.to_dense()",
            "data.edge_index = None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=1068203)",
            "Insert(target_node=IN(type=binary_operator), node=('list', None), position=0, insert_id=1068204)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=1068205)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=subscript), position=2)",
            "Move(target_node=IN(type=list), node=ASTNode(type=[, text=[), position=0)",
            "Move(target_node=IN(type=list), node=ASTNode(type=identifier, text=num_nodes), position=1)",
            "Move(target_node=IN(type=list), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=list), node=ASTNode(type=identifier, text=num_nodes), position=3)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=1068206)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=*, text=*))",
            "Delete(target_node=ASTNode(type=list_splat))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 8849,
        "neg_line": [
            "-size = torch.Size([num_nodes, num_nodes, *list(edge_attr.size())[1:]])"
        ],
        "pos_line": [
            "+size = torch.Size([num_nodes, num_nodes] + list(edge_attr.size())[1:])"
        ],
        "core_change": "-size = torch.Size([num_nodes, num_nodes, *list(edge_attr.size())[1:]]) +size = torch.Size([num_nodes, num_nodes] + list(edge_attr.size())[1:])",
        "core_API": "Size"
    },
    {
        "commit_hash": "0e049fde5114bcd9e1877a83ef3fce80732b16f0",
        "index": "5c32688d1..8842977e9 100644",
        "commit_message": "Fix issue with worker crashing with larger numpy arrays\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def protobuf_serialize(obj: np.ndarray) -> NumpyProto:",
            "# same original unsigned values on the other side",
            "obj = obj.astype(DTYPE_REFACTOR[original_dtype])",
            "",
            "-    tensor = torch.from_numpy(obj).clone()",
            "+    # Cloning seems to cause the worker to freeze if the array is larger than around",
            "+    # 800k in data and since we are serializing it immediately afterwards I don't",
            "+    # think its needed anyway",
            "+    # tensor = torch.from_numpy(obj).clone()",
            "+    tensor = torch.from_numpy(obj)",
            "tensor_bytes = tensor_serializer(tensor)",
            "dtype = original_dtype.name",
            "return NumpyProto(proto_data=tensor_bytes, dtype=dtype)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=clone))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 8850,
        "neg_line": [
            "-tensor = torch.from_numpy(obj).clone()"
        ],
        "pos_line": [
            "+# Cloning seems to cause the worker to freeze if the array is larger than around",
            "+# 800k in data and since we are serializing it immediately afterwards I don't",
            "+# think its needed anyway",
            "+# tensor = torch.from_numpy(obj).clone()",
            "+tensor = torch.from_numpy(obj)"
        ],
        "core_change": "-tensor = torch.from_numpy(obj).clone() +# Cloning seems to cause the worker to freeze if the array is larger than around +# 800k in data and since we are serializing it immediately afterwards I don't +# think its needed anyway +# tensor = torch.from_numpy(obj).clone() +tensor = torch.from_numpy(obj)",
        "core_API": "astype"
    },
    {
        "commit_hash": "70eedbd35148a4045ffc9caf81b7cf4acf32c48a",
        "index": "c5b66f0a..c77768b9 100644",
        "commit_message": "Refactor category orders in heads (#2374)\n\n* Refactor (all): all category -1 in anchor/bbox head and anchor/bbox target assign\n\n* Fix (datasets): remove label + 1 in datasets\n\n* Fix (bbox_head): fix bug of fc_cls that forget + 1\n\n* Fix (atss_head & free_anchor): fix cat -1 bugs\n\n* Fix (mask_head): remove label + 1 in mask heads\n\n* fix atss\n\n* Fix (rpn): fix cross_entropy_loss bug of RPN\n\n* Fix (anchor_head): fix typo\n\n* Refactor (anchor_head): use background_label rather than num_classes to indicate background class\n\n* Refactor (docstring): add and reformat docstrings\n\n* fix mask iou head\n\n* Fix (mask_head): fix cat -1 bug\n\n* Fix (mask_head): fix bug in mask inference\n\n* Add (tests): add tests for mask rcnn and mask heads\n\n* Refactor (unittest): refactor test_forward\n\n* Refactor (new_empty): use new_full rather than new_empty\n\n* Refactor (background_label): check background_label\n\n* Add TODO\n\n* Refactor (unittest): allow BP in unittest\n\nCo-authored-by: Cao Yuhang <yhcao6@gmail.com>\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class DoubleConvFCBBoxHead(BBoxHead):",
            "out_dim_reg = 4 if self.reg_class_agnostic else 4 * self.num_classes",
            "self.fc_reg = nn.Linear(self.conv_out_channels, out_dim_reg)",
            "",
            "-        self.fc_cls = nn.Linear(self.fc_out_channels, self.num_classes)",
            "+        self.fc_cls = nn.Linear(self.fc_out_channels, self.num_classes + 1)",
            "self.relu = nn.ReLU(inplace=True)",
            "",
            "def _add_conv_branch(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=3, insert_id=1426892)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=1426893)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=2, insert_id=1426894)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 8851,
        "neg_line": [
            "-self.fc_cls = nn.Linear(self.fc_out_channels, self.num_classes)"
        ],
        "pos_line": [
            "+self.fc_cls = nn.Linear(self.fc_out_channels, self.num_classes + 1)"
        ],
        "core_change": "-self.fc_cls = nn.Linear(self.fc_out_channels, self.num_classes) +self.fc_cls = nn.Linear(self.fc_out_channels, self.num_classes + 1)",
        "core_API": "Linear"
    },
    {
        "commit_hash": "7482178162b779506a54538f2cf2565c8b88c597",
        "index": "737d1c57..72e67e44 100644",
        "commit_message": "default fast model loading ðŸ”¥ (#1115)\n\n* make accelerate hard dep\n\n* default fast init\n\n* move params to cpu when device map is None\n\n* handle device_map=None\n\n* handle torch < 1.9\n\n* remove device_map=\"auto\"\n\n* style\n\n* add accelerate in torch extra\n\n* remove accelerate from extras[\"test\"]\n\n* raise an error if torch is available but not accelerate\n\n* update installation docs\n\n* Apply suggestions from code review\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* improve defautl loading speed even further, allow disabling fats loading\n\n* address review comments\n\n* adapt the tests\n\n* fix test_stable_diffusion_fast_load\n\n* fix test_read_init\n\n* temp fix for dummy checks\n\n* Trigger Build\n\n* Apply suggestions from code review\n\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PipelineIntegrationTests(unittest.TestCase):",
            "def test_dance_diffusion_fp16(self):",
            "device = torch_device",
            "",
            "-        pipe = DanceDiffusionPipeline.from_pretrained(",
            "-            \"harmonai/maestro-150k\", torch_dtype=torch.float16, device_map=\"auto\"",
            "-        )",
            "+        pipe = DanceDiffusionPipeline.from_pretrained(\"harmonai/maestro-150k\", torch_dtype=torch.float16)",
            "pipe = pipe.to(device)",
            "pipe.set_progress_bar_config(disable=None)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=device_map))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"auto\"))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 5,
        "number": 8852,
        "neg_line": [
            "-pipe = DanceDiffusionPipeline.from_pretrained(",
            "-\"harmonai/maestro-150k\", torch_dtype=torch.float16, device_map=\"auto\"",
            "-)"
        ],
        "pos_line": [
            "+pipe = DanceDiffusionPipeline.from_pretrained(\"harmonai/maestro-150k\", torch_dtype=torch.float16)"
        ],
        "core_change": "-pipe = DanceDiffusionPipeline.from_pretrained( -\"harmonai/maestro-150k\", torch_dtype=torch.float16, device_map=\"auto\" -) +pipe = DanceDiffusionPipeline.from_pretrained(\"harmonai/maestro-150k\", torch_dtype=torch.float16)",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "744defbe46c39a010a7aedc2be220e037c96c91b",
        "index": "987b74c2..d386423d 100755",
        "commit_message": "bug fix in dataflow\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "# seqlen is 1 in inference. don't need loop_function",
            "outputs, last_state = rnn.rnn(cell, input_list, initial, scope='rnnlm')",
            "self.last_state = tf.identity(last_state, 'last_state')",
            "+",
            "# seqlen x (Bxrnnsize)",
            "-        output = tf.reshape(tf.concat(1, outputs), [-1, param.rnn_size])  # (seqlenxB) x rnnsize",
            "+        output = tf.reshape(tf.concat(1, outputs), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize",
            "logits = FullyConnected('fc', output, param.vocab_size, nl=tf.identity)",
            "self.prob = tf.nn.softmax(logits / param.softmax_temprature)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 8855,
        "neg_line": [
            "-output = tf.reshape(tf.concat(1, outputs), [-1, param.rnn_size])  # (seqlenxB) x rnnsize"
        ],
        "pos_line": [
            "+",
            "+output = tf.reshape(tf.concat(1, outputs), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize"
        ],
        "core_change": "+ -output = tf.reshape(tf.concat(1, outputs), [-1, param.rnn_size])  # (seqlenxB) x rnnsize +output = tf.reshape(tf.concat(1, outputs), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize",
        "core_API": "rnn"
    },
    {
        "commit_hash": "dac779a5473a40a8231b9dcf95587bcb15400c1a",
        "index": "b111bb3..a1fe736 100644",
        "commit_message": "Fix pylint errors\n\n",
        "file": "seq2seq.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "from seq2seq.configurable import Configurable",
            "def att_sum_bahdanau(v_att, keys, query):",
            "\"\"\"Calculates a batch- and timweise dot product with a variable\"\"\"",
            "return tf.reduce_sum(",
            "-      v_att * math_ops.tanh(keys + tf.expand_dims(query, 1)), [2])",
            "+      v_att * tf.tanh(keys + tf.expand_dims(query, 1)), [2])",
            "",
            "@function.Defun(tf.float32, tf.float32, func_name=\"att_sum_dot\", noinline=True)",
            "def att_sum_dot(keys, query):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=math_ops), value='tf')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8857,
        "neg_line": [
            "-v_att * math_ops.tanh(keys + tf.expand_dims(query, 1)), [2])"
        ],
        "pos_line": [
            "+v_att * tf.tanh(keys + tf.expand_dims(query, 1)), [2])"
        ],
        "core_change": "-v_att * math_ops.tanh(keys + tf.expand_dims(query, 1)), [2]) +v_att * tf.tanh(keys + tf.expand_dims(query, 1)), [2])",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "5620033115e571013325d017bcca92991b0a4ace",
        "index": "8e5daa96f..43e9f3f6d 100644",
        "commit_message": "[mbart] Fix fp16 testing logic (#4949)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MBartIntegrationTests(unittest.TestCase):",
            "expected_shape = (*summary.shape, config.vocab_size)",
            "self.assertEqual(logits.shape, expected_shape)",
            "",
            "-",
            "-@require_torch",
            "-class MBartTokenizerTests(MBartIntegrationTests):",
            "def test_enro_tokenizer_prepare_translation_batch(self):",
            "batch = self.tokenizer.prepare_translation_batch(",
            "self.src_text, tgt_texts=self.tgt_text, max_length=len(self.expected_src_tokens),"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=@, text=@))",
            "Delete(target_node=ASTNode(type=identifier, text=require_torch))",
            "Delete(target_node=ASTNode(type=decorator))",
            "Delete(target_node=ASTNode(type=class, text=class))",
            "Delete(target_node=ASTNode(type=identifier, text=MBartTokenizerTests))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=MBartIntegrationTests))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block, text=))",
            "Delete(target_node=ASTNode(type=class_definition))",
            "Delete(target_node=ASTNode(type=decorated_definition))"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 8860,
        "neg_line": [
            "-",
            "-@require_torch",
            "-class MBartTokenizerTests(MBartIntegrationTests):"
        ],
        "pos_line": [],
        "core_change": "- -@require_torch -class MBartTokenizerTests(MBartIntegrationTests):",
        "core_API": "assertEqual"
    },
    {
        "commit_hash": "ab282059bde4f0d8a073e99c437a9fa5f52f8de1",
        "index": "438056364..be5fdb0af 100644",
        "commit_message": "fix: fix metrics calculation\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class GoalOrientedBotNetwork(TFModel):",
            "_train_op = optimizer(learning_rate).minimize(loss, name='train_op')",
            "# TODO: check clipping of gradients",
            "#optimizer = tf.train.AdamOptimizer(learning_rate)",
            "+        #clip_rate = 1.",
            "#gards_and_vars = optimizer.compute_gradients(loss, tf.trainable_variables())",
            "-        #grads_and_vars = [(tf.clip_by_norm(grad, 1), var) for grad, var in grads_and_vars]",
            "+        #grads_and_vars = [(tf.clip_by_norm(grad, clip_rate), var) for grad, var in grads_and_vars]",
            "#optimizer.apply_gradients(grads_and_vars)",
            "return _train_op"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 8870,
        "neg_line": [
            "-#grads_and_vars = [(tf.clip_by_norm(grad, 1), var) for grad, var in grads_and_vars]"
        ],
        "pos_line": [
            "+#clip_rate = 1.",
            "+#grads_and_vars = [(tf.clip_by_norm(grad, clip_rate), var) for grad, var in grads_and_vars]"
        ],
        "core_change": "+#clip_rate = 1. -#grads_and_vars = [(tf.clip_by_norm(grad, 1), var) for grad, var in grads_and_vars] +#grads_and_vars = [(tf.clip_by_norm(grad, clip_rate), var) for grad, var in grads_and_vars]",
        "core_API": "AdamOptimizer"
    },
    {
        "commit_hash": "006b1d3aaaded24e6f29f9d8dfce238a49186ff8",
        "index": "516c62a6..bbc4bc73 100644",
        "commit_message": "bug fix\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchSTFT(nn.Module):  # pylint: disable=abstract-method",
            "if use_mel:",
            "self._build_mel_basis()",
            "",
            "-    @torch.no_grad()",
            "def __call__(self, x):",
            "\"\"\"Compute spectrogram frames by torch based stft."
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=function_definition), position=3)",
            "Delete(target_node=ASTNode(type=@, text=@))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=no_grad))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=decorator))",
            "Delete(target_node=ASTNode(type=decorated_definition))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 8871,
        "neg_line": [
            "-@torch.no_grad()"
        ],
        "pos_line": [],
        "core_change": "-@torch.no_grad()",
        "core_API": "_build_mel_basis"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "a85e6fb78..efc9a1816 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario Å aÅ¡ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class MsrZhenTranslationParity(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_zhen_translation_parity', data_dir=...)`. Manual download instructions: {})\".format(",
            "-                    path_to_manual_file, self.manual_download_instructions",
            "-                )",
            "+                f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_zhen_translation_parity', data_dir=...)`. Manual download instructions: {self.manual_download_instructions})\"",
            ")",
            "return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"path\": path_to_manual_file})]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'msr_zhen_translation_parity\\', data_dir=...)`. Manual download instructions: {self.manual_download_instructions})\"'), position=1, insert_id=1781625)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_zhen_translation_parity', data_dir=...)`. Manual download instructions: {})\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=path_to_manual_file))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 8878,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_zhen_translation_parity', data_dir=...)`. Manual download instructions: {})\".format(",
            "-path_to_manual_file, self.manual_download_instructions",
            "-)"
        ],
        "pos_line": [
            "+f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_zhen_translation_parity', data_dir=...)`. Manual download instructions: {self.manual_download_instructions})\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_zhen_translation_parity', data_dir=...)`. Manual download instructions: {})\".format( -path_to_manual_file, self.manual_download_instructions -) +f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_zhen_translation_parity', data_dir=...)`. Manual download instructions: {self.manual_download_instructions})\"",
        "core_API": "exists"
    },
    {
        "commit_hash": "2ab78325f079781009d5300cc85ce7f6af6c2576",
        "index": "e913a0513..9e48856a6 100644",
        "commit_message": "Fix F821 flake8 warning (x47).\n\nIgnore warnings related to Python 2, because it's going away soon.\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFXLNetMainLayer(tf.keras.layers.Layer):",
            "if dtype is not None and dtype != tf.float32:",
            "fwd_pos_seq = tf.cast(fwd_pos_seq, dtype=dtype)",
            "if self.clamp_len > 0:",
            "-                fwd_pos_seq = tf.clip_by_value(fwd_pos_seq, -clamp_len, clamp_len)",
            "+                fwd_pos_seq = tf.clip_by_value(fwd_pos_seq, -self.clamp_len, self.clamp_len)",
            "pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq, bsz)",
            "",
            "return pos_emb"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=5, insert_id=2384658)",
            "Insert(target_node=ASTNode(type=unary_operator), node=('attribute', None), position=1, insert_id=2384659)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2384660)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2384661)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=clamp_len), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2384662)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2384663)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=clamp_len), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 8879,
        "neg_line": [
            "-fwd_pos_seq = tf.clip_by_value(fwd_pos_seq, -clamp_len, clamp_len)"
        ],
        "pos_line": [
            "+fwd_pos_seq = tf.clip_by_value(fwd_pos_seq, -self.clamp_len, self.clamp_len)"
        ],
        "core_change": "-fwd_pos_seq = tf.clip_by_value(fwd_pos_seq, -clamp_len, clamp_len) +fwd_pos_seq = tf.clip_by_value(fwd_pos_seq, -self.clamp_len, self.clamp_len)",
        "core_API": "cast"
    },
    {
        "commit_hash": "4febf34fc9f26832631bcaaf6e0938cab563db32",
        "index": "358e660d..262e89d2 100644",
        "commit_message": "[Refactor]: Support batch inference traceable by ONNX in SSD, YOLOv3, FSAF, RetinaNet, and FCOS (#4699)\n\n* Support batch infer in RetinaNet\n\n* Suport batch multiclass_nms\n\n* Revert multiclass_nms\n\n* Fix api deprecated warning\n\n* do not repeat anchors\n\n* Move img_shapes\n\n* Update Yolov3\n\n* Support FCOS\n\n* Support RPN\n\n* Fix RPN topk_inds error\n\n* make batch exportable to onnx for yolohead\n\n* make fcos_head exportable to onnx with batch dim\n\n* Support ATSS\n\n* Support CornerNet and centripetalNet\n\n* Update RetinaNet and delta_xywh\n\n* Remove ugly code\n\n* Remove ugly code of FCOS\n\n* Remove ugly code of ATSS/YOLOV3\n\n* Support RPN and revert bbox_head\n\n* expand anchors to batch and remove BG class when use deploy_nms_pre\n\n* Update\n\n* Use dim=-1 instead of dim=2\n\n* Rename anchor_head method\n\n* Keep the original format output when nms is not use\n\n* Rename method and unified code style\n\n* Fix paa_head and unittest\n\n* Fix FASF onnx export error\n\n* Fix error\n\n* fix single stage img_shapes for onnx\n\n* move conf_thr\n\n* fix rpn_head for onnx\n\n* Add distance2bbox unittest\n\n* Remove TODO\n\n* Fix RPN\n\n* Update docstrs\n\nCo-authored-by: maningsheng <maningsheng@sensetime.com>\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_paa_head_loss():",
            "nms=dict(type='nms', iou_threshold=0.6),",
            "max_per_img=100))",
            "rescale = False",
            "-    self._get_bboxes_single(",
            "+    self._get_bboxes(",
            "cls_scores,",
            "bbox_preds,",
            "iou_preds,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=_get_bboxes_single), value='_get_bboxes')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8889,
        "neg_line": [
            "-self._get_bboxes_single("
        ],
        "pos_line": [
            "+self._get_bboxes("
        ],
        "core_change": "-self._get_bboxes_single( +self._get_bboxes(",
        "core_API": "_get_bboxes_single"
    },
    {
        "commit_hash": "b981dbb7e1f11d002338d96793903c458b7d4fa1",
        "index": "51d24c9..a51fe39 100644",
        "commit_message": "Make linter happy, fix travis status icon\n\n",
        "file": "face-alignment.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def flip(tensor, is_label=False):",
            "tensor = np.expand_dims(tensor, axis=0)",
            "tensor = torch.from_numpy(tensor)",
            "if was_cuda:",
            "-       tensor = tensor.cuda()",
            "+        tensor = tensor.cuda()",
            "return tensor"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 8890,
        "neg_line": [
            "-tensor = tensor.cuda()"
        ],
        "pos_line": [
            "+tensor = tensor.cuda()"
        ],
        "core_change": "-tensor = tensor.cuda() +tensor = tensor.cuda()",
        "core_API": "expand_dims"
    },
    {
        "commit_hash": "c33d02130e85a7fd8111f7cfb7aad1c9c6708519",
        "index": "b527830b61..88c9f5a97d 100644",
        "commit_message": "ndarray shape and T, numpy transpose edge cases and small fixes (#7006)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def matrix_transpose(",
            "*,",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    return tf.experimental.numpy.swapaxes(x, -1, -2)",
            "+    return tf.linalg.matrix_transpose(x)",
            "",
            "",
            "# noinspection PyUnusedLocal,PyShadowingBuiltins"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=numpy), value='matrix_transpose')",
            "Update(target_node=ASTNode(type=identifier, text=experimental), value='linalg')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=swapaxes))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=unary_operator, text=-2))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 8897,
        "neg_line": [
            "-return tf.experimental.numpy.swapaxes(x, -1, -2)"
        ],
        "pos_line": [
            "+return tf.linalg.matrix_transpose(x)"
        ],
        "core_change": "-return tf.experimental.numpy.swapaxes(x, -1, -2) +return tf.linalg.matrix_transpose(x)",
        "core_API": "swapaxes"
    },
    {
        "commit_hash": "0db5d911fc94604f9568b4b212e005ec4600d157",
        "index": "e76470dee..d5b284238 100644",
        "commit_message": "Fix `SpeechT5ForSpeechToSpeechIntegrationTests` device issue (#21460)\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpeechT5ForSpeechToSpeech(SpeechT5PreTrainedModel):",
            "predicted mel spectrogram, or a tensor with shape `(num_frames,)` containing the speech waveform.",
            "\"\"\"",
            "if speaker_embeddings is None:",
            "-            speaker_embeddings = torch.zeros((1, 512))",
            "+            speaker_embeddings = torch.zeros((1, 512), device=input_values.device)",
            "",
            "return _generate_speech(",
            "self,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1176928)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1176929)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1176930)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1176931)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1176932)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'input_values'), position=0, insert_id=1176933)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1176934)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1176935)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 8898,
        "neg_line": [
            "-speaker_embeddings = torch.zeros((1, 512))"
        ],
        "pos_line": [
            "+speaker_embeddings = torch.zeros((1, 512), device=input_values.device)"
        ],
        "core_change": "-speaker_embeddings = torch.zeros((1, 512)) +speaker_embeddings = torch.zeros((1, 512), device=input_values.device)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "1132da66c8852093901201822320ff3aba754b43",
        "index": "bf85326f..42600f30 100644",
        "commit_message": "Fix summary tag name collision when both histogram and image plugins are enabled.\n\nCurrent workaround is adding the corresponding suffix (`\\histogram` or `\\image`)  to the weight name to distinguish summaries for different plugins.\n\nPiperOrigin-RevId: 444998362\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):",
            "for layer in self.model.layers:",
            "for weight in layer.weights:",
            "weight_name = weight.name.replace(':', '_')",
            "-            tf.summary.histogram(weight_name, weight, step=epoch)",
            "+            # Add a suffix to prevent summary tag name collision.",
            "+            histogram_weight_name = weight_name + '/histogram'",
            "+            tf.summary.histogram(histogram_weight_name, weight, step=epoch)",
            "if self.write_images:",
            "-              self._log_weight_as_image(weight, weight_name, epoch)",
            "+              # Add a suffix to prevent summary tag name collision.",
            "+              image_weight_name = weight_name + '/image'",
            "+              self._log_weight_as_image(weight, image_weight_name, epoch)",
            "self._train_writer.flush()",
            "",
            "def _log_weight_as_image(self, weight, weight_name, epoch):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=2058944)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2058945)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'histogram_weight_name'), position=0, insert_id=2058946)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2058947)",
            "Insert(target_node=IN(type=assignment), node=('binary_operator', None), position=2, insert_id=2058948)",
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=2058949)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'weight_name'), position=0, insert_id=2058950)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=2058951)",
            "Insert(target_node=IN(type=binary_operator), node=('string', \"'/histogram'\"), position=2, insert_id=2058952)",
            "Update(target_node=ASTNode(type=identifier, text=weight_name), value='histogram_weight_name')",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2058953)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'image_weight_name'), position=0, insert_id=2058954)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2058955)",
            "Insert(target_node=IN(type=assignment), node=('binary_operator', None), position=2, insert_id=2058956)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'weight_name'), position=0, insert_id=2058957)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=2058958)",
            "Insert(target_node=IN(type=binary_operator), node=('string', \"'/image'\"), position=2, insert_id=2058959)",
            "Update(target_node=ASTNode(type=identifier, text=weight_name), value='image_weight_name')"
        ],
        "plus_line": 6,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 8899,
        "neg_line": [
            "-tf.summary.histogram(weight_name, weight, step=epoch)",
            "-self._log_weight_as_image(weight, weight_name, epoch)"
        ],
        "pos_line": [
            "+# Add a suffix to prevent summary tag name collision.",
            "+histogram_weight_name = weight_name + '/histogram'",
            "+tf.summary.histogram(histogram_weight_name, weight, step=epoch)",
            "+# Add a suffix to prevent summary tag name collision.",
            "+image_weight_name = weight_name + '/image'",
            "+self._log_weight_as_image(weight, image_weight_name, epoch)"
        ],
        "core_change": "-tf.summary.histogram(weight_name, weight, step=epoch) +# Add a suffix to prevent summary tag name collision. +histogram_weight_name = weight_name + '/histogram' +tf.summary.histogram(histogram_weight_name, weight, step=epoch) -self._log_weight_as_image(weight, weight_name, epoch) +# Add a suffix to prevent summary tag name collision. +image_weight_name = weight_name + '/image' +self._log_weight_as_image(weight, image_weight_name, epoch)",
        "core_API": "replace"
    },
    {
        "commit_hash": "a753fe5978947a01834c6fa9a6c83ab257da3746",
        "index": "63dabc9..453b01c 100644",
        "commit_message": "fixed a typo\n",
        "file": "pointnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def get_learning_rate(batch):",
            "DECAY_STEP,          # Decay step.",
            "DECAY_RATE,          # Decay rate.",
            "staircase=True)",
            "-    learing_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!",
            "+    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!",
            "return learning_rate",
            "",
            "def get_bn_decay(batch):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=learing_rate), value='learning_rate')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8900,
        "neg_line": [
            "-learing_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!"
        ],
        "pos_line": [
            "+learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!"
        ],
        "core_change": "-learing_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE! +learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!",
        "core_API": "maximum"
    },
    {
        "commit_hash": "6b95d6d2c06879f0f64f004e1f166246f7e90093",
        "index": "2d6ab54..1cfae95 100644",
        "commit_message": ".to(device) bug fix\n\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test(data,",
            "",
            "# Load model",
            "google_utils.attempt_download(weights)",
            "-        model = torch.load(weights, map_location=device)['model'].float().fuse()  # load to FP32",
            "+        model = torch.load(weights, map_location=device)['model'].float().fuse().to(device)  # load to FP32",
            "imgsz = check_img_size(imgsz, s=model.stride.max())  # check img_size",
            "",
            "# Multi-GPU disabled, incompatible with .half() https://github.com/ultralytics/yolov5/issues/99"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1302458)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1302459)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1302460)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1302461)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1302462)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'device'), position=1, insert_id=1302463)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1302464)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 8901,
        "neg_line": [
            "-model = torch.load(weights, map_location=device)['model'].float().fuse()  # load to FP32"
        ],
        "pos_line": [
            "+model = torch.load(weights, map_location=device)['model'].float().fuse().to(device)  # load to FP32"
        ],
        "core_change": "-model = torch.load(weights, map_location=device)['model'].float().fuse()  # load to FP32 +model = torch.load(weights, map_location=device)['model'].float().fuse().to(device)  # load to FP32",
        "core_API": "attempt_download"
    },
    {
        "commit_hash": "6c16951d1f4e5407698c478fc53dd08b8e149f4f",
        "index": "204aa27..6c0fc12 100644",
        "commit_message": "Pytorch v0.4 compatibility and minor fixes\n\n",
        "file": "pytorch-cnn-visualizations.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def generate_smooth_grad(Backprop, prep_img, target_class, param_n, param_sigma_",
            "smooth_grad = np.zeros(prep_img.size()[1:])",
            "",
            "mean = 0",
            "-    sigma = param_sigma_multiplier / (torch.max(prep_img) - torch.min(prep_img)).data[0]",
            "+    sigma = param_sigma_multiplier / (torch.max(prep_img) - torch.min(prep_img)).item()",
            "for x in range(param_n):",
            "# Generate noise",
            "noise = Variable(prep_img.data.new(prep_img.size()).normal_(mean, sigma**2))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=887684)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=887685)",
            "Update(target_node=ASTNode(type=identifier, text=data), value='item')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=887686)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=887687)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 8906,
        "neg_line": [
            "-sigma = param_sigma_multiplier / (torch.max(prep_img) - torch.min(prep_img)).data[0]"
        ],
        "pos_line": [
            "+sigma = param_sigma_multiplier / (torch.max(prep_img) - torch.min(prep_img)).item()"
        ],
        "core_change": "-sigma = param_sigma_multiplier / (torch.max(prep_img) - torch.min(prep_img)).data[0] +sigma = param_sigma_multiplier / (torch.max(prep_img) - torch.min(prep_img)).item()",
        "core_API": "zeros"
    },
    {
        "commit_hash": "eb82656546d68ec92e40a1ebbcdc2ca29c21d8ae",
        "index": "caa55684..37bf794b 100644",
        "commit_message": "Fix show during test (#3705)\n\n* fix show during test\n\n* sup show batch\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def single_gpu_test(model,",
            "for bbox_results, mask_results in result]",
            "results.extend(result)",
            "",
            "-        batch_size = len(result)",
            "for _ in range(batch_size):",
            "prog_bar.update()",
            "return results"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=function_definition), node=ASTNode(type=parameters), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=batch_size), value='for')",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=_), position=1)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=in, text=in), position=2)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=range), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=result), value='batch_size')",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=len))",
            "Delete(target_node=ASTNode(type=identifier, text=for))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=batch_size))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parameters))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 8913,
        "neg_line": [
            "-batch_size = len(result)"
        ],
        "pos_line": [],
        "core_change": "-batch_size = len(result)",
        "core_API": "extend"
    },
    {
        "commit_hash": "27c8143c671b25740a8a587b96e182bb7fa4ba6f",
        "index": "a9674a80..6fa49031 100644",
        "commit_message": "fix typing callable in load storage (#1768)\n\n* fix typing callable in load storage\n\n* missing import\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SOSNet(nn.Module):",
            "self.desc_norm = nn.Sequential(nn.LocalResponseNorm(256, alpha=256.0, beta=0.5, k=0.0))",
            "# load pretrained model",
            "if pretrained:",
            "-            pretrained_dict = torch.hub.load_state_dict_from_url(urls['lib'], map_location=lambda storage, loc: storage)",
            "+            storage_fcn: Callable = lambda storage, loc: storage",
            "+            pretrained_dict = torch.hub.load_state_dict_from_url(",
            "+                urls['lib'], map_location=storage_fcn",
            "+            )",
            "self.load_state_dict(pretrained_dict, strict=True)",
            "self.eval()",
            "return"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=402688)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=402689)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'storage_fcn'), position=0, insert_id=402690)",
            "Insert(target_node=IN(type=assignment), node=(':', ':'), position=1, insert_id=402691)",
            "Insert(target_node=IN(type=assignment), node=('type', None), position=2, insert_id=402692)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=3, insert_id=402693)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=lambda), position=4)",
            "Insert(target_node=IN(type=type), node=('identifier', 'Callable'), position=0, insert_id=402694)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'storage_fcn'), position=2, insert_id=402695)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 8921,
        "neg_line": [
            "-pretrained_dict = torch.hub.load_state_dict_from_url(urls['lib'], map_location=lambda storage, loc: storage)"
        ],
        "pos_line": [
            "+storage_fcn: Callable = lambda storage, loc: storage",
            "+pretrained_dict = torch.hub.load_state_dict_from_url(",
            "+urls['lib'], map_location=storage_fcn",
            "+)"
        ],
        "core_change": "-pretrained_dict = torch.hub.load_state_dict_from_url(urls['lib'], map_location=lambda storage, loc: storage) +storage_fcn: Callable = lambda storage, loc: storage +pretrained_dict = torch.hub.load_state_dict_from_url( +urls['lib'], map_location=storage_fcn +)",
        "core_API": "Sequential"
    },
    {
        "commit_hash": "3b8a1638ad25e5ea5ab357a312d75081b0905fec",
        "index": "0e97591b6f..0adb8040cb 100644",
        "commit_message": "add link to health check issue and format fixes\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "def test_sigmoid(x, dtype, tensor_fn, dev, call):",
            "# cardinality test",
            "assert ret.shape == x.shape",
            "# value test",
            "-    assert np.allclose(call(ivy.sigmoid, x), ivy.functional.backends.numpy.sigmoid(ivy.to_numpy(x)))",
            "-",
            "+    assert np.allclose(call(ivy.sigmoid, x), ivy_np.sigmoid(ivy.to_numpy(x)))",
            "",
            "",
            "# softmax"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=ivy), value='ivy_np')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=ivy), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=functional))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=backends))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=numpy))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 8926,
        "neg_line": [
            "-assert np.allclose(call(ivy.sigmoid, x), ivy.functional.backends.numpy.sigmoid(ivy.to_numpy(x)))",
            "-"
        ],
        "pos_line": [
            "+assert np.allclose(call(ivy.sigmoid, x), ivy_np.sigmoid(ivy.to_numpy(x)))"
        ],
        "core_change": "-assert np.allclose(call(ivy.sigmoid, x), ivy.functional.backends.numpy.sigmoid(ivy.to_numpy(x))) - +assert np.allclose(call(ivy.sigmoid, x), ivy_np.sigmoid(ivy.to_numpy(x)))",
        "core_API": "allclose"
    },
    {
        "commit_hash": "26edfabeb2e2933d198376813d2c9f0a879ca6d6",
        "index": "3e5a6531..995659d1 100644",
        "commit_message": "misc fix\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Callbacks(Callback):",
            "",
            "def trigger_epoch(self):",
            "self.train.trigger_epoch()",
            "+        # TODO test callbacks can be run async?",
            "self.test.trigger_epoch()"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 8929,
        "neg_line": [],
        "pos_line": [
            "+# TODO test callbacks can be run async?"
        ],
        "core_change": "+# TODO test callbacks can be run async?",
        "core_API": "trigger_epoch"
    },
    {
        "commit_hash": "3c193de4fee4779c63ec12ea0b55005b4ac57120",
        "index": "b2cc3869..918d9805 100644",
        "commit_message": "Fix linting and sigmoid func\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def sigmoid(x):",
            "Returns:",
            "A tensor.",
            "\"\"\"",
            "-    return tf.sigmoid(x)",
            "+    return tf.math.sigmoid(x)",
            "",
            "",
            "@keras_export(\"keras.backend.hard_sigmoid\")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 8931,
        "neg_line": [
            "-return tf.sigmoid(x)"
        ],
        "pos_line": [
            "+return tf.math.sigmoid(x)"
        ],
        "core_change": "-return tf.sigmoid(x) +return tf.math.sigmoid(x)",
        "core_API": "sigmoid"
    },
    {
        "commit_hash": "9f5895cb7a7db8248295038543d75a4334ee5c44",
        "index": "17802c2e..f7731536 100644",
        "commit_message": "fix issue in accelerate. (#2121)\n\nCo-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchBackend(Backend):",
            "self.init_process_group(backend, timeout, init_method)",
            "",
            "def init_process_group(self, backend, timeout, init_method):",
            "-        return torch.distributed.init_process_group(backend,",
            "-                                                    timeout=timeout,",
            "-                                                    init_method=init_method)",
            "+        if not torch.distributed.is_initialized():",
            "+            torch.distributed.init_process_group(backend,",
            "+                                                 timeout=timeout,",
            "+                                                 init_method=init_method)",
            "",
            "def all_reduce(self,",
            "tensor,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('if_statement', None), position=0, insert_id=77441)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=77442)",
            "Insert(target_node=IN(type=if_statement), node=('not_operator', None), position=1, insert_id=77443)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=77444)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=77445)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=77446)",
            "Insert(target_node=IN(type=not_operator), node=('call', None), position=1, insert_id=77447)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=77448)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=77449)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=77450)",
            "Move(target_node=IN(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=77451)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=77452)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_initialized'), position=2, insert_id=77453)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=77454)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=77455)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=77456)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=77457)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'distributed'), position=2, insert_id=77458)",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 8938,
        "neg_line": [
            "-return torch.distributed.init_process_group(backend,",
            "-timeout=timeout,",
            "-init_method=init_method)"
        ],
        "pos_line": [
            "+if not torch.distributed.is_initialized():",
            "+torch.distributed.init_process_group(backend,",
            "+timeout=timeout,",
            "+init_method=init_method)"
        ],
        "core_change": "-return torch.distributed.init_process_group(backend, -timeout=timeout, -init_method=init_method) +if not torch.distributed.is_initialized(): +torch.distributed.init_process_group(backend, +timeout=timeout, +init_method=init_method)",
        "core_API": "init_process_group"
    },
    {
        "commit_hash": "c773824f4f22a14375a2451264d1b5708addd0f9",
        "index": "64cc07a9f..748012bf8 100644",
        "commit_message": "[RLlib] Bug fixes and tests in DiagGaussian (#8676)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchDiagGaussian(TorchDistributionWrapper):",
            "@override(ActionDistribution)",
            "def __init__(self, inputs, model):",
            "super().__init__(inputs, model)",
            "-        mean, log_std = torch.chunk(inputs, 2, dim=1)",
            "+        mean, log_std = torch.chunk(self.inputs, 2, dim=1)",
            "self.dist = torch.distributions.normal.Normal(mean, torch.exp(log_std))",
            "",
            "@override(ActionDistribution)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1123731)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1123732)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1123733)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=inputs), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 8947,
        "neg_line": [
            "-mean, log_std = torch.chunk(inputs, 2, dim=1)"
        ],
        "pos_line": [
            "+mean, log_std = torch.chunk(self.inputs, 2, dim=1)"
        ],
        "core_change": "-mean, log_std = torch.chunk(inputs, 2, dim=1) +mean, log_std = torch.chunk(self.inputs, 2, dim=1)",
        "core_API": "chunk"
    },
    {
        "commit_hash": "f6bd12eb18056ea0ca0364b6e458f96ec0ff7585",
        "index": "ca21bd595b..ea73fe4b09 100644",
        "commit_message": "[RLlib] Add tensor-based tests for Schedules and fix some bugs related to using Schedules with tensor time input. (#9782)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Schedule(metaclass=ABCMeta):",
            "\"\"\"",
            "# By default (most of the time), tf should work with python code.",
            "# Override only if necessary.",
            "-        return tf.constant(self._value(t))",
            "+        return self._value(t)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=constant))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 8949,
        "neg_line": [
            "-return tf.constant(self._value(t))"
        ],
        "pos_line": [
            "+return self._value(t)"
        ],
        "core_change": "-return tf.constant(self._value(t)) +return self._value(t)",
        "core_API": "constant"
    },
    {
        "commit_hash": "39a72125e7671a1029b8e8c3a72a52ab1d59d4e4",
        "index": "1f814a17b..bc7893b9c 100644",
        "commit_message": "fix both failing RoCBert tests (#20469)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RoCBertModelIntegrationTest(unittest.TestCase):",
            "# convert to tokens is: ['[CLS]', 'å·´', '*', 'é»Ž', 'æ˜¯', 'æ³•', 'å›½', 'çš„', 'é¦–', 'éƒ½', '[SEP]']",
            "expected_output = torch.tensor([[101, 2349, 115, 7944, 3221, 3791, 1744, 4638, 7674, 6963, 102]])",
            "",
            "-        self.assertEqual(output_ids, expected_output)",
            "+        assert torch.allclose(output_ids, expected_output)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('assert_statement', None), position=2, insert_id=1183023)",
            "Insert(target_node=IN(type=assert_statement), node=('assert', 'assert'), position=0, insert_id=1183024)",
            "Move(target_node=IN(type=assert_statement), node=ASTNode(type=call), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=self), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=assertEqual), value='allclose')",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 8950,
        "neg_line": [
            "-self.assertEqual(output_ids, expected_output)"
        ],
        "pos_line": [
            "+assert torch.allclose(output_ids, expected_output)"
        ],
        "core_change": "-self.assertEqual(output_ids, expected_output) +assert torch.allclose(output_ids, expected_output)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "c8ea48d095b39aa3d75dbc1b301ae231a5ca4097",
        "index": "260cd8f118..1da6047de0 100644",
        "commit_message": "add min and max assertion for clip to all backends. (#3753)\n\n* add min and max assertion for clip to all backends.\n\n* remove min, max assertion for clip from `Ivy` backend.\n\n* fix core test for clip.\n\n* small reformat for clip core test.\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def clip(",
            "*,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "+    assert torch.all(torch.less(x_min, x_max)), \"Min value must be less than max.\"",
            "if hasattr(x_min, \"dtype\"):",
            "promoted_type = torch.promote_types(x_min.dtype, x_max.dtype)",
            "promoted_type = torch.promote_types(promoted_type, x.dtype)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=335863)",
            "Insert(target_node=IN(type=block), node=('assert_statement', None), position=0, insert_id=335864)",
            "Insert(target_node=IN(type=assert_statement), node=('assert', 'assert'), position=0, insert_id=335865)",
            "Insert(target_node=IN(type=assert_statement), node=('call', None), position=1, insert_id=335866)",
            "Insert(target_node=IN(type=assert_statement), node=(',', ','), position=2, insert_id=335867)",
            "Insert(target_node=IN(type=assert_statement), node=('string', '\"Min value must be less than max.\"'), position=3, insert_id=335868)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=335869)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=335870)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=335871)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=335872)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'all'), position=2, insert_id=335873)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=335874)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=335875)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=335876)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=335877)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=335878)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=335879)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=335880)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'less'), position=2, insert_id=335881)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=335882)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'x_min'), position=1, insert_id=335883)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=335884)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'x_max'), position=3, insert_id=335885)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=335886)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 25,
        "number": 8951,
        "neg_line": [],
        "pos_line": [
            "+assert torch.all(torch.less(x_min, x_max)), \"Min value must be less than max.\""
        ],
        "core_change": "+assert torch.all(torch.less(x_min, x_max)), \"Min value must be less than max.\"",
        "core_API": "all"
    },
    {
        "commit_hash": "0dfd6b624081fc4e1c72fc74ae0cd2de199c334c",
        "index": "de8d6ac1..7c4ab558 100644",
        "commit_message": "Add linting with black (#2678)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding ï¿½\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2678\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D32653381\n\nPulled By: dianaml0\n\nfbshipit-source-id: 2810d14867cd7d64f4d340740e2b590b82de47fe\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "logger = logging.getLogger(__name__)",
            "",
            "_SLOWMO_DDP_DISABLED = False",
            "try:",
            "-    from fairscale.experimental.nn.data_parallel import SlowMoBaseAlgorithm, SlowMoDistributedDataParallel",
            "+    from fairscale.experimental.nn.data_parallel import (",
            "+        SlowMoBaseAlgorithm,",
            "+        SlowMoDistributedDataParallel,",
            "+    )",
            "except ImportError:",
            "_SLOWMO_DDP_DISABLED = True"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=import_from_statement), node=('(', '('), position=3, insert_id=1355509)",
            "Insert(target_node=ASTNode(type=import_from_statement), node=(',', ','), position=7, insert_id=1355510)",
            "Insert(target_node=ASTNode(type=import_from_statement), node=(')', ')'), position=8, insert_id=1355511)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 8952,
        "neg_line": [
            "-from fairscale.experimental.nn.data_parallel import SlowMoBaseAlgorithm, SlowMoDistributedDataParallel"
        ],
        "pos_line": [
            "+from fairscale.experimental.nn.data_parallel import (",
            "+SlowMoBaseAlgorithm,",
            "+SlowMoDistributedDataParallel,",
            "+)"
        ],
        "core_change": "-from fairscale.experimental.nn.data_parallel import SlowMoBaseAlgorithm, SlowMoDistributedDataParallel +from fairscale.experimental.nn.data_parallel import ( +SlowMoBaseAlgorithm, +SlowMoDistributedDataParallel, +)",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "8f66689c67a51fc5a425f6f39c37d66f2cf6c7c1",
        "index": "b21c3f87..dc30176b 100644",
        "commit_message": "Fixed a bug in `AsymmetricConv` (#6595)\n\nHi, \nI fixed a bug in antisymmetric_conv.\nSpecifically, I changed the definition of `self.eye` (gradients don't\nneed to be computed for this Tensor) and fixed the doc.\n\nBest,\nAlessio\n\n---------\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AntiSymmetricConv(torch.nn.Module):",
            "",
            "def reset_parameters(self):",
            "torch.nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))",
            "-        ones(self.eye)",
            "self.phi.reset_parameters()",
            "zeros(self.bias)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=ones))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=eye))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 8954,
        "neg_line": [
            "-ones(self.eye)"
        ],
        "pos_line": [],
        "core_change": "-ones(self.eye)",
        "core_API": "kaiming_uniform_"
    },
    {
        "commit_hash": "5dccb6e2949936f2839dae1354abcff24745cd22",
        "index": "91598437cc..91626b7c12 100644",
        "commit_message": "`random` submodule fix (#2368)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def seed(seed_value: int = 0) -> None:",
            "",
            "def shuffle(x: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "batch_size = x.shape[0]",
            "-    return torch.index_select(x, 0, torch.randperm(batch_size, out=out), out=out)",
            "+    return torch.index_select(x, 0, torch.randperm(batch_size), out=out)",
            "",
            "",
            "shuffle.support_native_out = True"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 8961,
        "neg_line": [
            "-return torch.index_select(x, 0, torch.randperm(batch_size, out=out), out=out)"
        ],
        "pos_line": [
            "+return torch.index_select(x, 0, torch.randperm(batch_size), out=out)"
        ],
        "core_change": "-return torch.index_select(x, 0, torch.randperm(batch_size, out=out), out=out) +return torch.index_select(x, 0, torch.randperm(batch_size), out=out)",
        "core_API": "index_select"
    },
    {
        "commit_hash": "221aae6af1df80fa4025ac3c6cce2f4e2c3dbc92",
        "index": "3f579a8..8d3f4a2 100644",
        "commit_message": "fix flake8\n\n",
        "file": "TensorFlowTTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class TFFastSpeechOutput(tf.keras.layers.Layer):",
            "self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)",
            "",
            "def call(self, inputs, training=False):",
            "-        \"\"\"Call logic\"\"\"",
            "+        \"\"\"Call logic.\"\"\"",
            "hidden_states, input_tensor = inputs",
            "",
            "hidden_states = self.dense(hidden_states)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Call logic\"\"\"), value='\"\"\"Call logic.\"\"\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8963,
        "neg_line": [
            "-\"\"\"Call logic\"\"\""
        ],
        "pos_line": [
            "+\"\"\"Call logic.\"\"\""
        ],
        "core_change": "-\"\"\"Call logic\"\"\" +\"\"\"Call logic.\"\"\"",
        "core_API": "Dropout"
    },
    {
        "commit_hash": "9baf0a8bb739ac3b5741769cbd4838e788e546cd",
        "index": "49c46f05..bd47e66b 100644",
        "commit_message": "fix some problems; support multiple proposal_files and img_prefixes\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def main():",
            "",
            "model = build_detector(",
            "cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)",
            "-    train_dataset = datasets.get_dataset(cfg.data.train)",
            "+    train_dataset = get_dataset(cfg.data.train)",
            "train_detector(",
            "model,",
            "train_dataset,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=get_dataset), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=datasets))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 8976,
        "neg_line": [
            "-train_dataset = datasets.get_dataset(cfg.data.train)"
        ],
        "pos_line": [
            "+train_dataset = get_dataset(cfg.data.train)"
        ],
        "core_change": "-train_dataset = datasets.get_dataset(cfg.data.train) +train_dataset = get_dataset(cfg.data.train)",
        "core_API": "get_dataset"
    },
    {
        "commit_hash": "d52d68ebb5defa54511ae7c99dc2d9ea259d54bb",
        "index": "9de1d4ee..65955002 100644",
        "commit_message": "bug fix in summary. fix TF comptability break. update PTB readme\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def BatchNormV1(x, use_local_stat=None, decay=0.9, epsilon=1e-5):",
            "",
            "if use_local_stat:",
            "batch = tf.cast(tf.shape(x)[0], tf.float32)",
            "-        mul = tf.select(tf.equal(batch, 1.0), 1.0, batch / (batch - 1))",
            "+        mul = tf.where(tf.equal(batch, 1.0), 1.0, batch / (batch - 1))",
            "batch_var = batch_var * mul  # use unbiased variance estimator in training",
            "",
            "with tf.control_dependencies([ema_apply_op] if ctx.is_training else []):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=select), value='where')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 8993,
        "neg_line": [
            "-mul = tf.select(tf.equal(batch, 1.0), 1.0, batch / (batch - 1))"
        ],
        "pos_line": [
            "+mul = tf.where(tf.equal(batch, 1.0), 1.0, batch / (batch - 1))"
        ],
        "core_change": "-mul = tf.select(tf.equal(batch, 1.0), 1.0, batch / (batch - 1)) +mul = tf.where(tf.equal(batch, 1.0), 1.0, batch / (batch - 1))",
        "core_API": "cast"
    },
    {
        "commit_hash": "4eb0d8fe6a407793ff6cdff4eba53eaecddfaecd",
        "index": "32f7c27..e0dd8ee 100644",
        "commit_message": "fix: ignore update net1 in train2\n\n",
        "file": "deep-voice-conversion.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def train(args, logdir):",
            "steps_per_epoch=hp.train1.steps_per_epoch,",
            "# session_config=session_conf",
            ")",
            "-    ckpt = args.ckpt if args.ckpt else tf.train.latest_checkpoint(logdir)",
            "+    ckpt = '{}/{}'.format(logdir, args.ckpt) if args.ckpt else tf.train.latest_checkpoint(logdir)",
            "if ckpt:",
            "train_conf.session_init = SaverRestore(ckpt)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=conditional_expression), node=('call', None), position=0, insert_id=1919866)",
            "Insert(target_node=ASTNode(type=conditional_expression), node=('attribute', None), position=3, insert_id=1919867)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1919868)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1919869)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'args'), position=0, insert_id=1919870)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1919871)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ckpt'), position=2, insert_id=1919872)",
            "Insert(target_node=IN(type=attribute), node=('string', \"'{}/{}'\"), position=0, insert_id=1919873)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1919874)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'format'), position=2, insert_id=1919875)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1919876)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'logdir'), position=1, insert_id=1919877)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1919878)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1919879)",
            "Delete(target_node=ASTNode(type=identifier, text=args))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=ckpt))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 8997,
        "neg_line": [
            "-ckpt = args.ckpt if args.ckpt else tf.train.latest_checkpoint(logdir)"
        ],
        "pos_line": [
            "+ckpt = '{}/{}'.format(logdir, args.ckpt) if args.ckpt else tf.train.latest_checkpoint(logdir)"
        ],
        "core_change": "-ckpt = args.ckpt if args.ckpt else tf.train.latest_checkpoint(logdir) +ckpt = '{}/{}'.format(logdir, args.ckpt) if args.ckpt else tf.train.latest_checkpoint(logdir)",
        "core_API": "latest_checkpoint"
    },
    {
        "commit_hash": "bf7333ae183a57717d338950905fc6c69d6ecb19",
        "index": "96de9f2f..5fad167e 100644",
        "commit_message": "Fix the jupyter notebook examples, e.g. requiring tensorflow 2.1 and providing v1 backward compatibility\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "\"outputs\": [],",
            "\"source\": [",
            "\"#@title Set up SDE parameters\\n\",",
            "-        \"tf.reset_default_graph()\\n\",",
            "+        \"tf.compat.v1.reset_default_graph()\\n\",",
            "\"\\n\",",
            "\"dtype = np.float64 #@param\\n\",",
            "\"num_samples = 100000 #@param\\n\","
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"tf.reset_default_graph()\\n\"), value='\"tf.compat.v1.reset_default_graph()\\\\n\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9000,
        "neg_line": [
            "-\"tf.reset_default_graph()\\n\","
        ],
        "pos_line": [
            "+\"tf.compat.v1.reset_default_graph()\\n\","
        ],
        "core_change": "-\"tf.reset_default_graph()\\n\", +\"tf.compat.v1.reset_default_graph()\\n\",",
        "core_API": "reset_default_graph"
    },
    {
        "commit_hash": "bb0d0aaa9e9f9076ac88aad425ad2f2caef369a7",
        "index": "c974df09b..96b2e4f52 100644",
        "commit_message": "fix code style\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CTC(torch.nn.Module):",
            "self.probs = None  # for visualization",
            "",
            "# In case of Pytorch >= 1.7.0, CTC will be always builtin",
            "-        self.ctc_type = (",
            "-            ctc_type",
            "-            if V(torch.__version__) < V(\"1.7.0\")",
            "-            else \"builtin\"",
            "-        )",
            "+        self.ctc_type = ctc_type if V(torch.__version__) < V(\"1.7.0\") else \"builtin\"",
            "",
            "if ctc_type != self.ctc_type:",
            "logging.warning(f\"CTC was set to {self.ctc_type} due to PyTorch version.\")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=conditional_expression), position=2)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))"
        ],
        "plus_line": 1,
        "minus_line": 5,
        "AST_diff_line": 4,
        "number": 9001,
        "neg_line": [
            "-self.ctc_type = (",
            "-ctc_type",
            "-if V(torch.__version__) < V(\"1.7.0\")",
            "-else \"builtin\"",
            "-)"
        ],
        "pos_line": [
            "+self.ctc_type = ctc_type if V(torch.__version__) < V(\"1.7.0\") else \"builtin\""
        ],
        "core_change": "-self.ctc_type = ( -ctc_type -if V(torch.__version__) < V(\"1.7.0\") -else \"builtin\" -) +self.ctc_type = ctc_type if V(torch.__version__) < V(\"1.7.0\") else \"builtin\"",
        "core_API": "warning"
    },
    {
        "commit_hash": "27c8143c671b25740a8a587b96e182bb7fa4ba6f",
        "index": "1a425278..446e9bdb 100644",
        "commit_message": "fix typing callable in load storage (#1768)\n\n* fix typing callable in load storage\n\n* missing import\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ZCAWhitening(nn.Module):",
            "",
            "T, mean, T_inv = zca_mean(x, self.dim, self.unbiased, self.eps, self.compute_inv)",
            "",
            "-        self.mean_vector: torch.Tensor = mean",
            "+        self.mean_vector = mean",
            "self.transform_matrix: torch.Tensor = T",
            "if T_inv is None:",
            "self.transform_inv: Optional[torch.Tensor] = torch.empty([0])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=type))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 9003,
        "neg_line": [
            "-self.mean_vector: torch.Tensor = mean"
        ],
        "pos_line": [
            "+self.mean_vector = mean"
        ],
        "core_change": "-self.mean_vector: torch.Tensor = mean +self.mean_vector = mean",
        "core_API": "empty"
    },
    {
        "commit_hash": "0227b4a940ceacde43b703d74d03bed2603188e7",
        "index": "8825f3c0d..bf4b99b6e 100755",
        "commit_message": "fix #827\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def convert_xlm_checkpoint_to_pytorch(xlm_checkpoint_path, pytorch_dump_folder_p",
            "model = chkpt['model']",
            "",
            "config = chkpt['params']",
            "-    config = dict((n, v) for n, v in config.items() if not isinstance(v, (torch.Tensor, numpy.ndarray)))",
            "+    config = dict((n, v) for n, v in config.items() if not isinstance(v, (torch.FloatTensor, numpy.ndarray)))",
            "",
            "vocab = chkpt['dico_word2id']",
            "vocab = dict((s + '</w>' if s.find('@@') == -1 and i > 13 else s.replace('@@', ''), i) for s, i in vocab.items())"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='FloatTensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9007,
        "neg_line": [
            "-config = dict((n, v) for n, v in config.items() if not isinstance(v, (torch.Tensor, numpy.ndarray)))"
        ],
        "pos_line": [
            "+config = dict((n, v) for n, v in config.items() if not isinstance(v, (torch.FloatTensor, numpy.ndarray)))"
        ],
        "core_change": "-config = dict((n, v) for n, v in config.items() if not isinstance(v, (torch.Tensor, numpy.ndarray))) +config = dict((n, v) for n, v in config.items() if not isinstance(v, (torch.FloatTensor, numpy.ndarray)))",
        "core_API": "items"
    },
    {
        "commit_hash": "3ae1de67aa8c771056facad561615b5e27b09b8f",
        "index": "2d49ae52..ed2b7ed6 100644",
        "commit_message": "fix mypy errors\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def invert_affine_transform(matrix: torch.Tensor) -> torch.Tensor:",
            "if not (len(matrix.shape) == 3 or matrix.shape[-2:] == (2, 3)):",
            "raise ValueError(\"Input matrix must be a Bx2x3 tensor. Got {}\"",
            ".format(matrix.shape))",
            "-    matrix_tmp: torch.Tensor = F.pad(matrix, (0, 0, 0, 1), \"constant\", 0.0)",
            "+    matrix_tmp: torch.Tensor = F.pad(matrix, [0, 0, 0, 1], \"constant\", 0.0)",
            "matrix_tmp[..., 2, 2] += 1.0",
            "",
            "matrix_inv: torch.Tensor = torch.inverse(matrix_tmp)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('list', None), position=3, insert_id=465083)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=465084)",
            "Move(target_node=IN(type=list), node=ASTNode(type=integer, text=0), position=1)",
            "Move(target_node=IN(type=list), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=list), node=ASTNode(type=integer, text=0), position=3)",
            "Move(target_node=IN(type=list), node=ASTNode(type=,, text=,), position=4)",
            "Move(target_node=IN(type=list), node=ASTNode(type=integer, text=0), position=5)",
            "Move(target_node=IN(type=list), node=ASTNode(type=,, text=,), position=6)",
            "Move(target_node=IN(type=list), node=ASTNode(type=integer, text=1), position=7)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=8, insert_id=465085)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 9015,
        "neg_line": [
            "-matrix_tmp: torch.Tensor = F.pad(matrix, (0, 0, 0, 1), \"constant\", 0.0)"
        ],
        "pos_line": [
            "+matrix_tmp: torch.Tensor = F.pad(matrix, [0, 0, 0, 1], \"constant\", 0.0)"
        ],
        "core_change": "-matrix_tmp: torch.Tensor = F.pad(matrix, (0, 0, 0, 1), \"constant\", 0.0) +matrix_tmp: torch.Tensor = F.pad(matrix, [0, 0, 0, 1], \"constant\", 0.0)",
        "core_API": "pad"
    },
    {
        "commit_hash": "cd51772bbe4eec0addd4e149b2f47f999020e7a0",
        "index": "03dedd3e..5bdffef4 100644",
        "commit_message": "remove numpy import (#1116)\n\n* remove numpy import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* undo and add setup develop in gh actions\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove create_checkerboard\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestBottomHat:",
            "None, None, :, :",
            "]",
            "assert_allclose(",
            "-            bottom_hat(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected,",
            "-            atol=1e-3, rtol=1e-3",
            "+            bottom_hat(tensor, torch.ones_like(structural_element), structuring_element=structural_element),",
            "+            expected,",
            "+            atol=1e-3,",
            "+            rtol=1e-3,",
            ")",
            "",
            "def test_exception(self, device, dtype):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=423213)"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 9016,
        "neg_line": [
            "-bottom_hat(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected,",
            "-atol=1e-3, rtol=1e-3"
        ],
        "pos_line": [
            "+bottom_hat(tensor, torch.ones_like(structural_element), structuring_element=structural_element),",
            "+expected,",
            "+atol=1e-3,",
            "+rtol=1e-3,"
        ],
        "core_change": "-bottom_hat(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected, -atol=1e-3, rtol=1e-3 +bottom_hat(tensor, torch.ones_like(structural_element), structuring_element=structural_element), +expected, +atol=1e-3, +rtol=1e-3,",
        "core_API": "ones_like"
    },
    {
        "commit_hash": "13b7d8132e999c8cb658fdad11850c55a0192279",
        "index": "32cf82c591..8c86fbf487 100644",
        "commit_message": "tested maml with multiple inner gradient steps, and fixed small bug in tensorflow execute_with_gradients method.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def is_variable(x):",
            "",
            "",
            "def execute_with_gradients(func, xs, retain_grads=False):",
            "-    with _tf.GradientTape() as tape:",
            "+    with _tf.GradientTape(persistent=retain_grads, watch_accessed_variables=False) as tape:",
            "+        tape.watch(xs)",
            "func_ret = func(xs)",
            "if isinstance(func_ret, tuple):",
            "y = func_ret[0]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=with_statement), node=('block', None), position=3, insert_id=2036172)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2036173)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2036174)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2036175)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2036176)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tape'), position=0, insert_id=2036177)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2036178)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'watch'), position=2, insert_id=2036179)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2036180)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'xs'), position=1, insert_id=2036181)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2036182)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2036183)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2036184)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2036185)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'persistent'), position=0, insert_id=2036186)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2036187)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'retain_grads'), position=2, insert_id=2036188)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'watch_accessed_variables'), position=0, insert_id=2036189)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2036190)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=2036191)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 9026,
        "neg_line": [
            "-with _tf.GradientTape() as tape:"
        ],
        "pos_line": [
            "+with _tf.GradientTape(persistent=retain_grads, watch_accessed_variables=False) as tape:",
            "+tape.watch(xs)"
        ],
        "core_change": "-with _tf.GradientTape() as tape: +with _tf.GradientTape(persistent=retain_grads, watch_accessed_variables=False) as tape: +tape.watch(xs)",
        "core_API": "GradientTape"
    },
    {
        "commit_hash": "e8b41cfeec0b70b692e1a042e6f1e542f1fe446d",
        "index": "556ddc7d..1adef019 100644",
        "commit_message": "Fix `DBLP` download, move `get_edge_index` to `testing` package (#6901)\n\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_heterogeneous_random_node_loader():",
            "data['paper'].node_id = torch.arange(100)",
            "data['author'].x = torch.randn(200, 128)",
            "data['author'].node_id = torch.arange(200)",
            "-    data['paper', 'author'].edge_index = get_edge_index(100, 200, 500)",
            "+    data['paper', 'author'].edge_index = get_random_edge_index(100, 200, 500)",
            "data['paper', 'author'].edge_attr = torch.randn(500, 32)",
            "-    data['author', 'paper'].edge_index = get_edge_index(200, 100, 400)",
            "+    data['author', 'paper'].edge_index = get_random_edge_index(200, 100, 400)",
            "data['author', 'paper'].edge_attr = torch.randn(400, 32)",
            "-    data['paper', 'paper'].edge_index = get_edge_index(100, 100, 600)",
            "+    data['paper', 'paper'].edge_index = get_random_edge_index(100, 100, 600)",
            "data['paper', 'paper'].edge_attr = torch.randn(600, 32)",
            "",
            "loader = RandomNodeLoader(data, num_parts=4, shuffle=True)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=get_edge_index), value='get_random_edge_index')",
            "Update(target_node=ASTNode(type=identifier, text=get_edge_index), value='get_random_edge_index')",
            "Update(target_node=ASTNode(type=identifier, text=get_edge_index), value='get_random_edge_index')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 3,
        "number": 9030,
        "neg_line": [
            "-data['paper', 'author'].edge_index = get_edge_index(100, 200, 500)",
            "-data['author', 'paper'].edge_index = get_edge_index(200, 100, 400)",
            "-data['paper', 'paper'].edge_index = get_edge_index(100, 100, 600)"
        ],
        "pos_line": [
            "+data['paper', 'author'].edge_index = get_random_edge_index(100, 200, 500)",
            "+data['author', 'paper'].edge_index = get_random_edge_index(200, 100, 400)",
            "+data['paper', 'paper'].edge_index = get_random_edge_index(100, 100, 600)"
        ],
        "core_change": "-data['paper', 'author'].edge_index = get_edge_index(100, 200, 500) +data['paper', 'author'].edge_index = get_random_edge_index(100, 200, 500) -data['author', 'paper'].edge_index = get_edge_index(200, 100, 400) +data['author', 'paper'].edge_index = get_random_edge_index(200, 100, 400) -data['paper', 'paper'].edge_index = get_edge_index(100, 100, 600) +data['paper', 'paper'].edge_index = get_random_edge_index(100, 100, 600)",
        "core_API": "arange"
    },
    {
        "commit_hash": "9b6385488617ee99188b60c0525cbe4ec7e8f2a0",
        "index": "f10e3aa9..32b42cc6 100644",
        "commit_message": "Improve reproduceability 2/3 (#1906)\n\n* [Repro] Correct reproducability\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* need better image\n\n* allow conversion from no state dict checkpoints\n\n* up\n\n* up\n\n* up\n\n* up\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* next try\n\n* up\n\n* up\n\n* better name\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\n* correct more\n\n* up\n\n* replace all torch randn\n\n* fix\n\n* correct\n\n* correct\n\n* finish\n\n* fix more\n\n* up\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DDPMPipeline(DiffusionPipeline):",
            "",
            "if self.device.type == \"mps\":",
            "# randn does not work reproducibly on mps",
            "-            image = torch.randn(image_shape, generator=generator)",
            "+            image = randn_tensor(image_shape, generator=generator)",
            "image = image.to(self.device)",
            "else:",
            "-            image = torch.randn(image_shape, generator=generator, device=self.device)",
            "+            image = randn_tensor(image_shape, generator=generator, device=self.device)",
            "",
            "# set step values",
            "self.scheduler.set_timesteps(num_inference_steps)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='randn_tensor')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='randn_tensor')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=randn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=randn))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 9041,
        "neg_line": [
            "-image = torch.randn(image_shape, generator=generator)",
            "-image = torch.randn(image_shape, generator=generator, device=self.device)"
        ],
        "pos_line": [
            "+image = randn_tensor(image_shape, generator=generator)",
            "+image = randn_tensor(image_shape, generator=generator, device=self.device)"
        ],
        "core_change": "-image = torch.randn(image_shape, generator=generator) +image = randn_tensor(image_shape, generator=generator) -image = torch.randn(image_shape, generator=generator, device=self.device) +image = randn_tensor(image_shape, generator=generator, device=self.device)",
        "core_API": "randn"
    },
    {
        "commit_hash": "ad22997614a80d26ac26fc36d667c4e513a1180c",
        "index": "1d973d9..17b33b2 100644",
        "commit_message": "fixed the issues #372 (#379)\n\nä¿®å¤äº†ä¸€äº›å‚æ•°ä¼ é€’é€ æˆçš„é—®é¢˜ï¼ŒæŠŠè¿‡æ—¶çš„torch.nn.functional.tanh()æ”¹æˆäº†torch.tanh()\n",
        "file": "MockingBird.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class WaveRNN(nn.Module) :",
            "# Compute the fine gates",
            "u = F.sigmoid(R_fine_u + I_fine_u + b_fine_u)",
            "r = F.sigmoid(R_fine_r + I_fine_r + b_fine_r)",
            "-                e = F.tanh(r * R_fine_e + I_fine_e + b_fine_e)",
            "+                e = torch.tanh(r * R_fine_e + I_fine_e + b_fine_e)",
            "hidden_fine = u * hidden_fine + (1. - u) * e",
            "",
            "# Compute the fine output"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=F), value='torch')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9044,
        "neg_line": [
            "-e = F.tanh(r * R_fine_e + I_fine_e + b_fine_e)"
        ],
        "pos_line": [
            "+e = torch.tanh(r * R_fine_e + I_fine_e + b_fine_e)"
        ],
        "core_change": "-e = F.tanh(r * R_fine_e + I_fine_e + b_fine_e) +e = torch.tanh(r * R_fine_e + I_fine_e + b_fine_e)",
        "core_API": "sigmoid"
    },
    {
        "commit_hash": "a2c41a7f9ca27bd23a82a7fc6d26189fac6f868d",
        "index": "ecd13c47..2776d1bb 100644",
        "commit_message": "Very minor doc fixes.\n\nPiperOrigin-RevId: 456160001\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ThresholdsTest(tf.test.TestCase, parameterized.TestCase):",
            "self.assertAllClose(v1, v2)",
            "",
            "",
            "-if __name__ == '__main__':",
            "-  tf.test.main()",
            "+if __name__ == \"__main__\":",
            "+    tf.test.main()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='__main__'), value='\"__main__\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9046,
        "neg_line": [
            "-if __name__ == '__main__':",
            "-tf.test.main()"
        ],
        "pos_line": [
            "+if __name__ == \"__main__\":",
            "+tf.test.main()"
        ],
        "core_change": "-if __name__ == '__main__': -tf.test.main() +if __name__ == \"__main__\": +tf.test.main()",
        "core_API": "assertAllClose"
    },
    {
        "commit_hash": "305280def38dedfc447bef4ce1ed140f78fcadbf",
        "index": "401a3fa..e18e30e 100644",
        "commit_message": "Fix broken TF DistributedOptimizer with Keras 2.11+ (#3822)\n\nSigned-off-by: Nicolas Castet <ncastet@nvidia.com>\n",
        "file": "horovod.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "target = tf.one_hot(indices, 2)",
            "",
            "lr = 0.001",
            "model = tf.keras.Sequential([tf.keras.layers.Dense(2, activation='softmax')])",
            "-optimizer = tf.optimizers.SGD(lr * hvd.size())",
            "+optimizer = optimizers.SGD(lr * hvd.size())",
            "",
            "hostname = os.environ.get('HOROVOD_HOSTNAME')",
            "start_rank = int(os.environ.get('HOROVOD_RANK', 0))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=optimizers), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 4,
        "number": 9047,
        "neg_line": [
            "-optimizer = tf.optimizers.SGD(lr * hvd.size())"
        ],
        "pos_line": [
            "+optimizer = optimizers.SGD(lr * hvd.size())"
        ],
        "core_change": "-optimizer = tf.optimizers.SGD(lr * hvd.size()) +optimizer = optimizers.SGD(lr * hvd.size())",
        "core_API": "one_hot"
    },
    {
        "commit_hash": "7b540950315530bea9b4a734f0fdae98cc742678",
        "index": "8b1a9606..3fbdbfe2 100644",
        "commit_message": "Fix unit tests\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def test_sequence_tagger_param_selector(results_base_path, tasks_base_path):",
            "",
            "@pytest.mark.integration",
            "def test_text_classifier_param_selector(results_base_path, tasks_base_path):",
            "-    corpus = flair.datasets.ClassificationCorpus(tasks_base_path / \"imdb\")",
            "+    corpus = flair.datasets.ClassificationCorpus(tasks_base_path / \"imdb\", label_type=\"sentiment\")",
            "label_type = \"sentiment\"",
            "",
            "search_space = SearchSpace()",
            "",
            "# document embeddings parameter",
            "-    search_space.add(Parameter.TRANSFORMER_MODEL, hp.choice, options=[\"albert-base-v1\"])",
            "+    search_space.add(Parameter.TRANSFORMER_MODEL, hp.choice, options=[\"sshleifer/tiny-distilbert-base-cased\"])",
            "search_space.add(Parameter.LAYERS, hp.choice, options=[\"-1\", \"-2\"])",
            "",
            "# training parameter"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"albert-base-v1\"), value='\"sshleifer/tiny-distilbert-base-cased\"')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1792820)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1792821)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'label_type'), position=0, insert_id=1792822)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1792823)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"sentiment\"'), position=2, insert_id=1792824)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 9048,
        "neg_line": [
            "-corpus = flair.datasets.ClassificationCorpus(tasks_base_path / \"imdb\")",
            "-search_space.add(Parameter.TRANSFORMER_MODEL, hp.choice, options=[\"albert-base-v1\"])"
        ],
        "pos_line": [
            "+corpus = flair.datasets.ClassificationCorpus(tasks_base_path / \"imdb\", label_type=\"sentiment\")",
            "+search_space.add(Parameter.TRANSFORMER_MODEL, hp.choice, options=[\"sshleifer/tiny-distilbert-base-cased\"])"
        ],
        "core_change": "-corpus = flair.datasets.ClassificationCorpus(tasks_base_path / \"imdb\") +corpus = flair.datasets.ClassificationCorpus(tasks_base_path / \"imdb\", label_type=\"sentiment\") -search_space.add(Parameter.TRANSFORMER_MODEL, hp.choice, options=[\"albert-base-v1\"]) +search_space.add(Parameter.TRANSFORMER_MODEL, hp.choice, options=[\"sshleifer/tiny-distilbert-base-cased\"])",
        "core_API": "ClassificationCorpus"
    },
    {
        "commit_hash": "bb0051fb22517f1c015d798aafca224b5d13da20",
        "index": "f0faa58f..bcfbd43b 100644",
        "commit_message": "Fixed bugs in patchsequential. Remove fill_diagonal operation for better ONNX support. (#1178)\n\n* Minor fixes\n\n* update\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_rotation_matrix2d(center: torch.Tensor, angle: torch.Tensor, scale: torc",
            "",
            "# convert angle and apply scale",
            "rotation_matrix: torch.Tensor = angle_to_rotation_matrix(angle)",
            "-    scaling_matrix: torch.Tensor = (",
            "-        torch.zeros((2, 2), device=rotation_matrix.device, dtype=rotation_matrix.dtype)",
            "-        .fill_diagonal_(1)",
            "-        .repeat(rotation_matrix.size(0), 1, 1)",
            "+    scaling_matrix: torch.Tensor = torch.eye(2, device=rotation_matrix.device, dtype=rotation_matrix.dtype).repeat(",
            "+        rotation_matrix.size(0), 1, 1",
            ")",
            "",
            "scaling_matrix = scaling_matrix * scale.unsqueeze(dim=2).repeat(1, 1, 2)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=call), position=2)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=fill_diagonal_), value='repeat')",
            "Update(target_node=ASTNode(type=identifier, text=zeros), value='eye')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=integer, text=2), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=2))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=repeat))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))"
        ],
        "plus_line": 2,
        "minus_line": 4,
        "AST_diff_line": 22,
        "number": 9050,
        "neg_line": [
            "-scaling_matrix: torch.Tensor = (",
            "-torch.zeros((2, 2), device=rotation_matrix.device, dtype=rotation_matrix.dtype)",
            "-.fill_diagonal_(1)",
            "-.repeat(rotation_matrix.size(0), 1, 1)"
        ],
        "pos_line": [
            "+scaling_matrix: torch.Tensor = torch.eye(2, device=rotation_matrix.device, dtype=rotation_matrix.dtype).repeat(",
            "+rotation_matrix.size(0), 1, 1"
        ],
        "core_change": "-scaling_matrix: torch.Tensor = ( -torch.zeros((2, 2), device=rotation_matrix.device, dtype=rotation_matrix.dtype) -.fill_diagonal_(1) -.repeat(rotation_matrix.size(0), 1, 1) +scaling_matrix: torch.Tensor = torch.eye(2, device=rotation_matrix.device, dtype=rotation_matrix.dtype).repeat( +rotation_matrix.size(0), 1, 1",
        "core_API": "zeros"
    },
    {
        "commit_hash": "e94abf66783f5e24ab37adc754ac902e827a4b4d",
        "index": "849eacb3..2071177b 100755",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            ".Conv2D('conv4_7_CPM', 128)())",
            "",
            "def add_stage(stage, l):",
            "-            l = tf.concat(3, [l, shared, pool_center], name='concat_stage{}'.format(stage))",
            "+            l = tf.concat_v2([l, shared, pool_center], 3,",
            "+                             name='concat_stage{}'.format(stage))",
            "for i in range(1, 6):",
            "l = Conv2D('Mconv{}_stage{}'.format(i, stage), l, 128)",
            "l = Conv2D('Mconv6_stage{}'.format(stage), l, 128, kernel_shape=1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=concat), value='concat_v2')",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '3'), position=4, insert_id=2308252)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=5, insert_id=2308253)",
            "Delete(target_node=ASTNode(type=integer, text=3))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 9053,
        "neg_line": [
            "-l = tf.concat(3, [l, shared, pool_center], name='concat_stage{}'.format(stage))"
        ],
        "pos_line": [
            "+l = tf.concat_v2([l, shared, pool_center], 3,",
            "+name='concat_stage{}'.format(stage))"
        ],
        "core_change": "-l = tf.concat(3, [l, shared, pool_center], name='concat_stage{}'.format(stage)) +l = tf.concat_v2([l, shared, pool_center], 3, +name='concat_stage{}'.format(stage))",
        "core_API": "concat"
    },
    {
        "commit_hash": "7b3f01415d85001b0d4443226d4fd8c352f0662a",
        "index": "95cd8de245..c22c8aaf8d 100644",
        "commit_message": "fix diff for tensorflow as the append function used does not work with None\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def diff(",
            "prepend: Optional[Union[tf.Tensor, tf.Variable, int, float, list, tuple]] = None,",
            "append: Optional[Union[tf.Tensor, tf.Variable, int, float, list, tuple]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    x = tf.experimental.numpy.append(prepend, x, axis=axis)",
            "-    x = tf.experimental.numpy.append(x, append, axis=axis)",
            "+    if prepend != None:",
            "+        x = tf.experimental.numpy.append(prepend, x, axis=axis)",
            "+    if append != None:",
            "+        x = tf.experimental.numpy.append(x, append, axis=axis)",
            "return tf.experimental.numpy.diff(x, n=n, axis=axis)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('if_statement', None), position=0, insert_id=1969414)",
            "Insert(target_node=ASTNode(type=block), node=('if_statement', None), position=1, insert_id=1969415)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1969416)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1969417)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1969418)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1969419)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1969420)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1969421)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1969422)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1969423)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'prepend'), position=0, insert_id=1969424)",
            "Insert(target_node=IN(type=comparison_operator), node=('!=', '!='), position=1, insert_id=1969425)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=2, insert_id=1969426)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'append'), position=0, insert_id=1969427)",
            "Insert(target_node=IN(type=comparison_operator), node=('!=', '!='), position=1, insert_id=1969428)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=2, insert_id=1969429)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 9055,
        "neg_line": [
            "-x = tf.experimental.numpy.append(prepend, x, axis=axis)",
            "-x = tf.experimental.numpy.append(x, append, axis=axis)"
        ],
        "pos_line": [
            "+if prepend != None:",
            "+x = tf.experimental.numpy.append(prepend, x, axis=axis)",
            "+if append != None:",
            "+x = tf.experimental.numpy.append(x, append, axis=axis)"
        ],
        "core_change": "-x = tf.experimental.numpy.append(prepend, x, axis=axis) -x = tf.experimental.numpy.append(x, append, axis=axis) +if prepend != None: +x = tf.experimental.numpy.append(prepend, x, axis=axis) +if append != None: +x = tf.experimental.numpy.append(x, append, axis=axis)",
        "core_API": "append"
    },
    {
        "commit_hash": "c74dc58f8b38bb947b903d3f92efbd8d1cd675ce",
        "index": "edd78fa39c..05d173738b 100644",
        "commit_message": "[RLlib] Fix `use_lstm` flag for ModelV2 (w/o ModelV1 wrapping) and add it for PyTorch. (#8734)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def build_vtrace_loss(policy, model, dist_class, train_batch):",
            "values = model.value_function()",
            "",
            "if policy.is_recurrent():",
            "-        max_seq_len = tf.reduce_max(train_batch[\"seq_lens\"]) - 1",
            "+        max_seq_len = tf.reduce_max(train_batch[\"seq_lens\"])",
            "mask = tf.sequence_mask(train_batch[\"seq_lens\"], max_seq_len)",
            "mask = tf.reshape(mask, [-1])",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 9061,
        "neg_line": [
            "-max_seq_len = tf.reduce_max(train_batch[\"seq_lens\"]) - 1"
        ],
        "pos_line": [
            "+max_seq_len = tf.reduce_max(train_batch[\"seq_lens\"])"
        ],
        "core_change": "-max_seq_len = tf.reduce_max(train_batch[\"seq_lens\"]) - 1 +max_seq_len = tf.reduce_max(train_batch[\"seq_lens\"])",
        "core_API": "value_function"
    },
    {
        "commit_hash": "e9059b9183add3934ad8e74783ad207af37deb20",
        "index": "7d8004fc4e..90b61e37da 100644",
        "commit_message": "fix kw_only missmatch for diff function with out arg missing for jax, torch and tensorflow as opposed to np backend and ivy functional\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def diff(",
            "axis: Optional[int] = -1,",
            "prepend: Optional[Union[tf.Tensor, tf.Variable, int, float, list, tuple]] = None,",
            "append: Optional[Union[tf.Tensor, tf.Variable, int, float, list, tuple]] = None,",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "if prepend is not None:",
            "x = tf.experimental.numpy.append(prepend, x, axis=axis)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('typed_default_parameter', None), position=7, insert_id=1966518)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=1966519)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('identifier', 'out'), position=0, insert_id=1966520)",
            "Insert(target_node=IN(type=typed_default_parameter), node=(':', ':'), position=1, insert_id=1966521)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('type', None), position=2, insert_id=1966522)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('=', '='), position=3, insert_id=1966523)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('none', 'None'), position=4, insert_id=1966524)",
            "Insert(target_node=IN(type=type), node=('subscript', None), position=0, insert_id=1966525)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Optional'), position=0, insert_id=1966526)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1966527)",
            "Insert(target_node=IN(type=subscript), node=('subscript', None), position=2, insert_id=1966528)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1966529)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Union'), position=0, insert_id=1966530)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1966531)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=2, insert_id=1966532)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=1966533)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=4, insert_id=1966534)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=5, insert_id=1966535)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1966536)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1966537)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=1966538)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1966539)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1966540)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Variable'), position=2, insert_id=1966541)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 24,
        "number": 9063,
        "neg_line": [],
        "pos_line": [
            "+out: Optional[Union[tf.Tensor, tf.Variable]] = None,"
        ],
        "core_change": "+out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
        "core_API": "append"
    },
    {
        "commit_hash": "d215aa258e77f43b6c36950bc9a484d5d849eb4f",
        "index": "f6f0ae69..e830a6bd 100644",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _expand_binary_labels(labels, label_weights, label_channels):",
            "# in other files such as in ghm_loss, the _expand_binary_labels",
            "# is used for multi-class classification.",
            "bin_labels = labels.new_full((labels.size(0), label_channels), 0)",
            "-    inds = torch.nonzero(labels >= 1).squeeze()",
            "+    inds = torch.nonzero(labels >= 1, as_tuple=False).squeeze()",
            "if inds.numel() > 0:",
            "bin_labels[inds, labels[inds] - 1] = 1",
            "if label_weights is None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=638811)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=638812)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'as_tuple'), position=0, insert_id=638813)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=638814)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=638815)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 9068,
        "neg_line": [
            "-inds = torch.nonzero(labels >= 1).squeeze()"
        ],
        "pos_line": [
            "+inds = torch.nonzero(labels >= 1, as_tuple=False).squeeze()"
        ],
        "core_change": "-inds = torch.nonzero(labels >= 1).squeeze() +inds = torch.nonzero(labels >= 1, as_tuple=False).squeeze()",
        "core_API": "new_full"
    },
    {
        "commit_hash": "6655b7f8595d8108cdda1f597e4ab0529fc0f569",
        "index": "2b4dcc33..6cac106c 100644",
        "commit_message": "Fix errors in unit tests\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TextClassifier(flair.nn.DefaultClassifier[Sentence]):",
            "text_embedding_list = [sentence.get_embedding(embedding_names).unsqueeze(0) for sentence in sentences]",
            "text_embedding_tensor = torch.cat(text_embedding_list, 0).to(flair.device)",
            "",
            "-        # send through decoder to get logits",
            "-        scores = self.decoder(text_embedding_tensor)",
            "-",
            "labels = []",
            "for sentence in sentences:",
            "labels.append([label.value for label in sentence.get_labels(self.label_type)])",
            "",
            "if return_label_candidates:",
            "label_candidates = [Label(value=\"<None>\") for sentence in sentences]",
            "-            return scores, labels, sentences, label_candidates",
            "+            return text_embedding_tensor, labels, sentences, label_candidates",
            "",
            "-        return scores, labels",
            "+        return text_embedding_tensor, labels",
            "",
            "def _get_state_dict(self):",
            "model_state = {"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=scores), value='text_embedding_tensor')",
            "Update(target_node=ASTNode(type=identifier, text=scores), value='text_embedding_tensor')",
            "Delete(target_node=ASTNode(type=identifier, text=scores))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=decoder))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=text_embedding_tensor))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 2,
        "minus_line": 4,
        "AST_diff_line": 15,
        "number": 9070,
        "neg_line": [
            "-# send through decoder to get logits",
            "-scores = self.decoder(text_embedding_tensor)",
            "-",
            "-return scores, labels, sentences, label_candidates",
            "-return scores, labels"
        ],
        "pos_line": [
            "+return text_embedding_tensor, labels, sentences, label_candidates",
            "+return text_embedding_tensor, labels"
        ],
        "core_change": "-# send through decoder to get logits -scores = self.decoder(text_embedding_tensor) - -return scores, labels, sentences, label_candidates +return text_embedding_tensor, labels, sentences, label_candidates -return scores, labels +return text_embedding_tensor, labels",
        "core_API": "get_embedding"
    },
    {
        "commit_hash": "61edd59ca21d8c667a72aaf2fd4c619b9b384cc9",
        "index": "e4e9b542c9..3781c3abe8 100644",
        "commit_message": "fixing some imports etc.\n\nsome leftover from commit c358f3ac1fbbbd3c805fb257304f90c76bc63bc8\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def empty(shape: Union[int, Tuple[int]],",
            "def array(object_in, dtype=None, dev=None):",
            "dtype = dtype_from_str(default_dtype(dtype, object_in))",
            "dev = default_device(dev)",
            "-    with _tf.device(dev_from_str(dev)):",
            "+    with tf.device(dev_from_str(dev)):",
            "try:",
            "-            tensor = _tf.convert_to_tensor(object_in, dtype=dtype)",
            "+            tensor = tf.convert_to_tensor(object_in, dtype=dtype)",
            "except (TypeError, ValueError):",
            "-            tensor = _tf.convert_to_tensor(ivy.nested_map(object_in, lambda x: _tf.cast(x, dtype)), dtype=dtype)",
            "+            tensor = tf.convert_to_tensor(ivy.nested_map(object_in, lambda x: tf.cast(x, dtype)), dtype=dtype)",
            "if dtype is None:",
            "return tensor",
            "-        return _tf.cast(tensor, dtype)",
            "+        return tf.cast(tensor, dtype)",
            "",
            "",
            "asarray = array"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=_tf), value='tf')",
            "Update(target_node=ASTNode(type=identifier, text=_tf), value='tf')",
            "Update(target_node=ASTNode(type=identifier, text=_tf), value='tf')",
            "Update(target_node=ASTNode(type=identifier, text=_tf), value='tf')",
            "Update(target_node=ASTNode(type=identifier, text=_tf), value='tf')"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 5,
        "number": 9075,
        "neg_line": [
            "-with _tf.device(dev_from_str(dev)):",
            "-tensor = _tf.convert_to_tensor(object_in, dtype=dtype)",
            "-tensor = _tf.convert_to_tensor(ivy.nested_map(object_in, lambda x: _tf.cast(x, dtype)), dtype=dtype)",
            "-return _tf.cast(tensor, dtype)"
        ],
        "pos_line": [
            "+with tf.device(dev_from_str(dev)):",
            "+tensor = tf.convert_to_tensor(object_in, dtype=dtype)",
            "+tensor = tf.convert_to_tensor(ivy.nested_map(object_in, lambda x: tf.cast(x, dtype)), dtype=dtype)",
            "+return tf.cast(tensor, dtype)"
        ],
        "core_change": "-with _tf.device(dev_from_str(dev)): +with tf.device(dev_from_str(dev)): -tensor = _tf.convert_to_tensor(object_in, dtype=dtype) +tensor = tf.convert_to_tensor(object_in, dtype=dtype) -tensor = _tf.convert_to_tensor(ivy.nested_map(object_in, lambda x: _tf.cast(x, dtype)), dtype=dtype) +tensor = tf.convert_to_tensor(ivy.nested_map(object_in, lambda x: tf.cast(x, dtype)), dtype=dtype) -return _tf.cast(tensor, dtype) +return tf.cast(tensor, dtype)",
        "core_API": "device"
    },
    {
        "commit_hash": "40fd9ca3c877d2e29a2cf077902d61f38aa161f2",
        "index": "3c092f58..f574cfa9 100644",
        "commit_message": " Replace literal constant 10 with variable num_classes in example/ (#8041)\n\n* Replace literal constant 10 with variable num_classes\n\n* Fix PEP8 errors\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "test_model.summary()",
            "",
            "loss, acc = test_model.evaluate(x_test,",
            "keras.utils.to_categorical(y_test),",
            "-                                classes)",
            "+                                num_classes)",
            "print('\\nTest accuracy: {0}'.format(acc))"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=classes), value='num_classes')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9076,
        "neg_line": [
            "-classes)"
        ],
        "pos_line": [
            "+num_classes)"
        ],
        "core_change": "-classes) +num_classes)",
        "core_API": "summary"
    },
    {
        "commit_hash": "6ad299573f290768341a054ea6d2d776ae079fa3",
        "index": "abf661e46..a831611fb 100644",
        "commit_message": "[Metrics] Fix/4237 auc unstable reorder (#4281)\n\n* =Add deprecation warning for auc reorder\n\n* =Add test for deprecation warning for auc reorder\n\n* Update CHANGELOG\n\n* Add reorder deprecation warning to auc docstring\n\n* Fix pep8 f-string error\n\n* remove duplicate import\n\nCo-authored-by: Nicki Skafte <skaftenicki@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def auc(",
            "if (dx, 0).all():",
            "direction = -1.",
            "else:",
            "-                raise ValueError(\"Reordering is not turned on, and \"",
            "-                                 \"the x array is not increasing: %s\" % x)",
            "+                # TODO: Update message on removing reorder",
            "+                raise ValueError(\"Reorder is not turned on, and the 'x' array is\"",
            "+                                 f\" neither increasing or decreasing: {x}\")",
            "",
            "return direction * torch.trapz(y, x)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=concatenated_string), position=1)",
            "Update(target_node=ASTNode(type=string, text=\"Reordering is not turned on, and \"), value='\"Reorder is not turned on, and the \\'x\\' array is\"')",
            "Update(target_node=ASTNode(type=string, text=\"the x array is not increasing: %s\"), value='f\" neither increasing or decreasing: {x}\"')",
            "Delete(target_node=ASTNode(type=%, text=%))",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 9078,
        "neg_line": [
            "-raise ValueError(\"Reordering is not turned on, and \"",
            "-\"the x array is not increasing: %s\" % x)"
        ],
        "pos_line": [
            "+# TODO: Update message on removing reorder",
            "+raise ValueError(\"Reorder is not turned on, and the 'x' array is\"",
            "+f\" neither increasing or decreasing: {x}\")"
        ],
        "core_change": "-raise ValueError(\"Reordering is not turned on, and \" -\"the x array is not increasing: %s\" % x) +# TODO: Update message on removing reorder +raise ValueError(\"Reorder is not turned on, and the 'x' array is\" +f\" neither increasing or decreasing: {x}\")",
        "core_API": "trapz"
    },
    {
        "commit_hash": "bbaa3effbd9bda26c177d5cfa811e9988d9ce480",
        "index": "bf496d92a..2b8d6884b 100755",
        "commit_message": "Fixes Beit training for PyTorch 1.10+ (#14249)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "class BeitUperHead(nn.Module):",
            "used_backbone_levels = len(laterals)",
            "for i in range(used_backbone_levels - 1, 0, -1):",
            "prev_shape = laterals[i - 1].shape[2:]",
            "-            laterals[i - 1] += nn.functional.interpolate(",
            "+            laterals[i - 1] = laterals[i - 1] + nn.functional.interpolate(",
            "laterals[i], size=prev_shape, mode=\"bilinear\", align_corners=self.align_corners",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=1755266)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=subscript), position=0)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1755267)",
            "Insert(target_node=IN(type=assignment), node=('binary_operator', None), position=2, insert_id=1755268)",
            "Insert(target_node=IN(type=binary_operator), node=('subscript', None), position=0, insert_id=1755269)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=1755270)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'laterals'), position=0, insert_id=1755271)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1755272)",
            "Insert(target_node=IN(type=subscript), node=('binary_operator', None), position=2, insert_id=1755273)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1755274)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'i'), position=0, insert_id=1755275)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=1755276)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=2, insert_id=1755277)",
            "Delete(target_node=ASTNode(type=+=, text=+=))",
            "Delete(target_node=ASTNode(type=augmented_assignment))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 9079,
        "neg_line": [
            "-laterals[i - 1] += nn.functional.interpolate("
        ],
        "pos_line": [
            "+laterals[i - 1] = laterals[i - 1] + nn.functional.interpolate("
        ],
        "core_change": "-laterals[i - 1] += nn.functional.interpolate( +laterals[i - 1] = laterals[i - 1] + nn.functional.interpolate(",
        "core_API": "interpolate"
    },
    {
        "commit_hash": "ced4cfdbbf4496ec0d95483e470933b4fed4f95a",
        "index": "2bb5bfc7..43867239 100644",
        "commit_message": "Allow saving / loading checkpoints from cloud paths (#683)\n\n* Allow saving / loading checkpoints from cloud paths\n\nAllows saving and loading checkpoints directly from cloud paths like\nAmazon S3 (s3://) and Google Cloud Storage (gs://) by using fsspec.\n\nNote: The user will have to install the relevant dependency for each\nprotocol. Otherwise fsspec will fail and specify which dependency is\nmissing.\n\n* Append suffix _fsspec to save/load function names\n\n* Add a lower bound to the fsspec dependency\n\nSkips the 0 major version.\n\n* Add missing changes from refactor\n\n* Use fsspec for remaining artifacts\n\n* Add test case with path requiring fsspec\n\n* Avoid writing logs to file unless output_path is local\n\n* Document the possibility of using paths supported by fsspec\n\n* Fix style and lint\n\n* Add missing lint fixes\n\n* Add type annotations to new functions\n\n* Use Coqpit method for converting config to dict\n\n* Fix type annotation in semi-new function\n\n* Add return type for load_fsspec\n\n* Fix bug where fs not always created\n\n* Restore the experiment removal functionality\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main(args):  # pylint: disable=redefined-outer-name",
            "raise Exception(\"The %s  not is a loss supported\" % c.loss)",
            "",
            "if args.restore_path:",
            "-        checkpoint = torch.load(args.restore_path)",
            "+        checkpoint = load_fsspec(args.restore_path)",
            "try:",
            "model.load_state_dict(checkpoint[\"model\"])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='load_fsspec')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=load))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 9085,
        "neg_line": [
            "-checkpoint = torch.load(args.restore_path)"
        ],
        "pos_line": [
            "+checkpoint = load_fsspec(args.restore_path)"
        ],
        "core_change": "-checkpoint = torch.load(args.restore_path) +checkpoint = load_fsspec(args.restore_path)",
        "core_API": "load"
    },
    {
        "commit_hash": "ba4d30c42e0702bd894c36777d7d2c0adf74516c",
        "index": "6e304f6a9..feb13d9c8 100644",
        "commit_message": "Module namespace cleanup for v2.0 (#3875)\n\n* Imports cleaning\n\n* Small change\n\n* Remove unused methods\n\n* Small fix\n\n* Additional fix\n\n* Final fix\n\n* Fix benchmark test\n\n* Fix benchmark test #2\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def generate_examples(features: dict, num_examples=100, seq_shapes=None):",
            "def generate_example_dataset(dataset_path, features, num_examples=100, seq_shapes=None):",
            "dummy_data = generate_examples(features, num_examples=num_examples, seq_shapes=seq_shapes)",
            "",
            "-    with datasets.ArrowWriter(features=features, path=dataset_path) as writer:",
            "+    with ArrowWriter(features=features, path=dataset_path) as writer:",
            "for key, record in dummy_data:",
            "example = features.encode_example(record)",
            "writer.write(example)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=ArrowWriter), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=datasets))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 9088,
        "neg_line": [
            "-with datasets.ArrowWriter(features=features, path=dataset_path) as writer:"
        ],
        "pos_line": [
            "+with ArrowWriter(features=features, path=dataset_path) as writer:"
        ],
        "core_change": "-with datasets.ArrowWriter(features=features, path=dataset_path) as writer: +with ArrowWriter(features=features, path=dataset_path) as writer:",
        "core_API": "ArrowWriter"
    },
    {
        "commit_hash": "9981fe2a647c22233b224d4f3cf5e840a1b1e321",
        "index": "b27b1fb7..c0c52af2 100644",
        "commit_message": "fixed dqn init after refactor\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DistributedModel(object):",
            "self.batch_shape = [None]",
            "self.deterministic_mode = config.get('deterministic_mode', False)",
            "self.alpha = config.get('alpha', 0.001)",
            "-        # self.init_op = tf.global_variables_initializer()",
            "-",
            "self.optimizer = None",
            "",
            "self.worker_device = \"/job:worker/task:{}/cpu:0\".format(task_index)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 9089,
        "neg_line": [
            "-# self.init_op = tf.global_variables_initializer()",
            "-"
        ],
        "pos_line": [],
        "core_change": "-# self.init_op = tf.global_variables_initializer() -",
        "core_API": "get"
    },
    {
        "commit_hash": "d08bc09495a289519a774e881f399cf864178f9b",
        "index": "cb0edce0..3ec57471 100644",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _nullspace(A):",
            "",
            "Return the smallest singular value and the corresponding vector.",
            "\"\"\"",
            "-    u, s, vh = torch.svd(A)",
            "+    _, s, vh = torch.svd(A)",
            "return s[..., -1], vh[..., -1]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=u), value='_')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9092,
        "neg_line": [
            "-u, s, vh = torch.svd(A)"
        ],
        "pos_line": [
            "+_, s, vh = torch.svd(A)"
        ],
        "core_change": "-u, s, vh = torch.svd(A) +_, s, vh = torch.svd(A)",
        "core_API": "svd"
    },
    {
        "commit_hash": "ea7f400d57cfb03397326eb2f8e972670d534cc2",
        "index": "46ae1860..e39b52f8 100644",
        "commit_message": "Support >3d mask matrices.\n\nThe change to the dimshuffle/transpose call to support >3d inputs was\ncorrect for the inputs array but did not apply to the mask array. This\nfixes that.\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def rnn(step_function, inputs, initial_states,",
            "mask = tf.cast(mask, tf.bool)",
            "else:",
            "# Transpose not supported by bool tensor types, hence round-trip to uint8.",
            "-        mask = tf.cast(tf.transpose(tf.cast(mask, tf.uint8), (1, 0, 2)), tf.bool)",
            "+        mask = tf.cast(tf.transpose(tf.cast(mask, tf.uint8), axes), tf.bool)",
            "",
            "mask_list = tf.unpack(mask)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'axes'), position=3, insert_id=2119694)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=2))",
            "Delete(target_node=ASTNode(type=tuple))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 9096,
        "neg_line": [
            "-mask = tf.cast(tf.transpose(tf.cast(mask, tf.uint8), (1, 0, 2)), tf.bool)"
        ],
        "pos_line": [
            "+mask = tf.cast(tf.transpose(tf.cast(mask, tf.uint8), axes), tf.bool)"
        ],
        "core_change": "-mask = tf.cast(tf.transpose(tf.cast(mask, tf.uint8), (1, 0, 2)), tf.bool) +mask = tf.cast(tf.transpose(tf.cast(mask, tf.uint8), axes), tf.bool)",
        "core_API": "cast"
    },
    {
        "commit_hash": "dce42be8aeaef2581f33a9661e2df540d7b1d68e",
        "index": "41343ce..8b33e33 100644",
        "commit_message": "fix feature bias error\n\n",
        "file": "tensorflow_practice.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "\"\\n\",",
            "\"\\n\",",
            "\"\\\"\\\"\\\"fm part\\\"\\\"\\\"\\n\",",
            "-    \"fm_first_order = tf.nn.embedding_lookup(weights['feature_embeddings'],feat_index)\\n\",",
            "+    \"fm_first_order = tf.nn.embedding_lookup(weights['feature_bias'],feat_index)\\n\",",
            "\"fm_first_order = tf.reduce_sum(tf.multiply(fm_first_order,reshaped_feat_value),2)\\n\",",
            "\"\\n\",",
            "\"summed_features_emb = tf.reduce_sum(embeddings,1)\\n\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"fm_first_order = tf.nn.embedding_lookup(weights['feature_embeddings'],feat_index)\\n\"), value='\"fm_first_order = tf.nn.embedding_lookup(weights[\\'feature_bias\\'],feat_index)\\\\n\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9098,
        "neg_line": [
            "-\"fm_first_order = tf.nn.embedding_lookup(weights['feature_embeddings'],feat_index)\\n\","
        ],
        "pos_line": [
            "+\"fm_first_order = tf.nn.embedding_lookup(weights['feature_bias'],feat_index)\\n\","
        ],
        "core_change": "-\"fm_first_order = tf.nn.embedding_lookup(weights['feature_embeddings'],feat_index)\\n\", +\"fm_first_order = tf.nn.embedding_lookup(weights['feature_bias'],feat_index)\\n\",",
        "core_API": "embedding_lookup"
    },
    {
        "commit_hash": "a82c8fcf40146b2207400d6c863bb4da972ea199",
        "index": "2ebaf4e4a..77c1328d0 100644",
        "commit_message": "fix trivia_qa unfiltered (#2995)\n\n\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class TriviaQa(datasets.GeneratorBasedBuilder):",
            "try:",
            "with open(os.path.join(file_dir, fname), encoding=\"utf-8\") as f:",
            "new_item[context_field] = f.read()",
            "-                    except (IOError, datasets.Value(\"errors\").NotFoundError):",
            "+                    except (IOError, FileNotFoundError):",
            "logger.info(\"File does not exist, skipping: %s\", fname)",
            "continue",
            "new_items.append(new_item)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=datasets), value='FileNotFoundError')",
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=identifier, text=datasets), position=3)",
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=), text=)), position=4)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Value))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"errors\"))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=NotFoundError))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 9100,
        "neg_line": [
            "-except (IOError, datasets.Value(\"errors\").NotFoundError):"
        ],
        "pos_line": [
            "+except (IOError, FileNotFoundError):"
        ],
        "core_change": "-except (IOError, datasets.Value(\"errors\").NotFoundError): +except (IOError, FileNotFoundError):",
        "core_API": "join"
    },
    {
        "commit_hash": "b8515d4b7b46a2d17c3ebda63482f3b992db27d3",
        "index": "b763078a..50f60651 100644",
        "commit_message": "Fixes #7408. Move the import of the tensorboard projector plugin to the TensorBoard __init__ method (#7886)\n\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "class TensorBoard(Callback):",
            "if K.backend() != 'tensorflow':",
            "raise RuntimeError('TensorBoard callback only works '",
            "'with the TensorFlow backend.')",
            "+        global tf, projector",
            "+        import tensorflow as tf",
            "+        from tensorflow.contrib.tensorboard.plugins import projector",
            "self.log_dir = log_dir",
            "self.histogram_freq = histogram_freq",
            "self.merged = None"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('global_statement', None), position=3, insert_id=2451893)",
            "Insert(target_node=ASTNode(type=module), node=('import_statement', None), position=4, insert_id=2451894)",
            "Insert(target_node=ASTNode(type=module), node=('import_from_statement', None), position=5, insert_id=2451895)",
            "Insert(target_node=IN(type=global_statement), node=('global', 'global'), position=0, insert_id=2451896)",
            "Insert(target_node=IN(type=global_statement), node=('identifier', 'tf'), position=1, insert_id=2451897)",
            "Insert(target_node=IN(type=global_statement), node=(',', ','), position=2, insert_id=2451898)",
            "Insert(target_node=IN(type=global_statement), node=('identifier', 'projector'), position=3, insert_id=2451899)",
            "Insert(target_node=IN(type=import_statement), node=('import', 'import'), position=0, insert_id=2451900)",
            "Insert(target_node=IN(type=import_statement), node=('aliased_import', None), position=1, insert_id=2451901)",
            "Insert(target_node=IN(type=import_from_statement), node=('from', 'from'), position=0, insert_id=2451902)",
            "Insert(target_node=IN(type=import_from_statement), node=('dotted_name', None), position=1, insert_id=2451903)",
            "Insert(target_node=IN(type=import_from_statement), node=('import', 'import'), position=2, insert_id=2451904)",
            "Insert(target_node=IN(type=import_from_statement), node=('dotted_name', None), position=3, insert_id=2451905)",
            "Insert(target_node=IN(type=aliased_import), node=('dotted_name', None), position=0, insert_id=2451906)",
            "Insert(target_node=IN(type=aliased_import), node=('as', 'as'), position=1, insert_id=2451907)",
            "Insert(target_node=IN(type=aliased_import), node=('identifier', 'tf'), position=2, insert_id=2451908)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'tensorflow'), position=0, insert_id=2451909)",
            "Insert(target_node=IN(type=dotted_name), node=('.', '.'), position=1, insert_id=2451910)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'contrib'), position=2, insert_id=2451911)",
            "Insert(target_node=IN(type=dotted_name), node=('.', '.'), position=3, insert_id=2451912)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'tensorboard'), position=4, insert_id=2451913)",
            "Insert(target_node=IN(type=dotted_name), node=('.', '.'), position=5, insert_id=2451914)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'plugins'), position=6, insert_id=2451915)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'projector'), position=0, insert_id=2451916)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'tensorflow'), position=0, insert_id=2451917)"
        ],
        "plus_line": 3,
        "minus_line": 0,
        "AST_diff_line": 25,
        "number": 9103,
        "neg_line": [],
        "pos_line": [
            "+global tf, projector",
            "+import tensorflow as tf",
            "+from tensorflow.contrib.tensorboard.plugins import projector"
        ],
        "core_change": "+global tf, projector +import tensorflow as tf +from tensorflow.contrib.tensorboard.plugins import projector",
        "core_API": "backend"
    },
    {
        "commit_hash": "cd9808ce5158725f53cabb7943b4ebe83ae84abd",
        "index": "25e83ae6f..16dfe1307 100644",
        "commit_message": "Fix test scripts and some code\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class RNNLM(nn.Module):",
            "super(RNNLM, self).__init__()",
            "self.embed = nn.Embedding(n_vocab, n_units)",
            "self.lstm = nn.ModuleList(",
            "-            [nn.LSTMCell(n_units, n_units) for _ in six.moves.range(n_layers)])",
            "+            [nn.LSTMCell(n_units, n_units) for _ in range(n_layers)])",
            "self.dropout = nn.ModuleList(",
            "-            [nn.Dropout() for _ in six.moves.range(n_layers + 1)])",
            "+            [nn.Dropout() for _ in range(n_layers + 1)])",
            "self.lo = nn.Linear(n_units, n_vocab)",
            "self.n_layers = n_layers",
            "self.n_units = n_units"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1344016)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1344017)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1344018)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ModuleList'), position=2, insert_id=1344019)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=range), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=range), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=six))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=moves))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=ModuleList))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=six))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=moves))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 23,
        "number": 9108,
        "neg_line": [
            "-[nn.LSTMCell(n_units, n_units) for _ in six.moves.range(n_layers)])",
            "-[nn.Dropout() for _ in six.moves.range(n_layers + 1)])"
        ],
        "pos_line": [
            "+[nn.LSTMCell(n_units, n_units) for _ in range(n_layers)])",
            "+[nn.Dropout() for _ in range(n_layers + 1)])"
        ],
        "core_change": "-[nn.LSTMCell(n_units, n_units) for _ in six.moves.range(n_layers)]) +[nn.LSTMCell(n_units, n_units) for _ in range(n_layers)]) -[nn.Dropout() for _ in six.moves.range(n_layers + 1)]) +[nn.Dropout() for _ in range(n_layers + 1)])",
        "core_API": "Embedding"
    },
    {
        "commit_hash": "7ac768d11f740cacc1fb76c349b12e4228676450",
        "index": "9568ec6..c83754e 100644",
        "commit_message": "fix: documentation allign code and comment (#1782)\n\n* fix: allign code and comment\n\n* lint\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "adaptive number of epochs.",
            "",
            "",
            "# Initialize the text regressor.",
            "-reg = ak.TextRegressor(overwrite=True, max_trials=1)  # It tries 10 different models.",
            "+reg = ak.TextRegressor(",
            "+    overwrite=True, max_trials=10  # It tries 10 different models.",
            "+)",
            "# Feed the text regressor with training data.",
            "reg.fit(x_train, y_train, epochs=2)",
            "# Predict with the best model."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=1), value='10')"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 1,
        "number": 9109,
        "neg_line": [
            "-reg = ak.TextRegressor(overwrite=True, max_trials=1)  # It tries 10 different models."
        ],
        "pos_line": [
            "+reg = ak.TextRegressor(",
            "+overwrite=True, max_trials=10  # It tries 10 different models.",
            "+)"
        ],
        "core_change": "-reg = ak.TextRegressor(overwrite=True, max_trials=1)  # It tries 10 different models. +reg = ak.TextRegressor( +overwrite=True, max_trials=10  # It tries 10 different models. +)",
        "core_API": "TextRegressor"
    },
    {
        "commit_hash": "f3fbfbbe7e4c772d60dbc4374811d3a959699f2b",
        "index": "f4879c00..d31e0c0d 100644",
        "commit_message": "[TorchScript] Add user-defined HF Bert tokenizers (#2733)\n\n* first working set\n\n* wip todo: add never_split kwarg\n\n* adds new never_split kwarg\n\n* clean up\n\n* get tests passing\n\n* updated py38 tests\n\n* pr revisions\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* logging > logger\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_torchscript_e2e_text(tmpdir, csv_filename):",
            "",
            "",
            "@pytest.mark.skipif(",
            "-    torch.torch_version.TorchVersion(torchtext.__version__) < (0, 13, 0),",
            "-    reason=\"requires torchtext 0.13.0 or higher\",",
            "+    torch.torch_version.TorchVersion(torchtext.__version__) < (0, 14, 0),",
            "+    reason=\"requires torchtext 0.14.0 or higher\",",
            ")",
            "def test_torchscript_e2e_text_hf_tokenizer(tmpdir, csv_filename):",
            "data_csv_path = os.path.join(tmpdir, csv_filename)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"requires torchtext 0.13.0 or higher\"), value='\"requires torchtext 0.14.0 or higher\"')",
            "Update(target_node=ASTNode(type=integer, text=13), value='14')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 9116,
        "neg_line": [
            "-torch.torch_version.TorchVersion(torchtext.__version__) < (0, 13, 0),",
            "-reason=\"requires torchtext 0.13.0 or higher\","
        ],
        "pos_line": [
            "+torch.torch_version.TorchVersion(torchtext.__version__) < (0, 14, 0),",
            "+reason=\"requires torchtext 0.14.0 or higher\","
        ],
        "core_change": "-torch.torch_version.TorchVersion(torchtext.__version__) < (0, 13, 0), -reason=\"requires torchtext 0.13.0 or higher\", +torch.torch_version.TorchVersion(torchtext.__version__) < (0, 14, 0), +reason=\"requires torchtext 0.14.0 or higher\",",
        "core_API": "skipif"
    },
    {
        "commit_hash": "baa00f65aeb7467075c87c9f769784ac3b86e94c",
        "index": "aaf4d8162..021596245 100644",
        "commit_message": "Fix exception thrown using MishActivation (#19739)\n\n* Fix exception thrown using MishActivation\n\n* Update activations.py\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MishActivation(nn.Module):",
            "",
            "def __init__(self):",
            "super().__init__()",
            "-        if version.parse(version.parse(torch.__version__).base_version) < version.parse(\"1.9\"):",
            "+        if version.parse(torch.__version__) < version.parse(\"1.9.0\"):",
            "self.act = self._mish_python",
            "else:",
            "self.act = nn.functional.mish"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=comparison_operator), node=ASTNode(type=call), position=0)",
            "Update(target_node=ASTNode(type=string, text=\"1.9\"), value='\"1.9.0\"')",
            "Delete(target_node=ASTNode(type=identifier, text=version))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=parse))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=base_version))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 9117,
        "neg_line": [
            "-if version.parse(version.parse(torch.__version__).base_version) < version.parse(\"1.9\"):"
        ],
        "pos_line": [
            "+if version.parse(torch.__version__) < version.parse(\"1.9.0\"):"
        ],
        "core_change": "-if version.parse(version.parse(torch.__version__).base_version) < version.parse(\"1.9\"): +if version.parse(torch.__version__) < version.parse(\"1.9.0\"):",
        "core_API": "parse"
    },
    {
        "commit_hash": "7fa74925e74b6ae186293fbd26a411a09ba4d775",
        "index": "e71fed1..7aaf20a 100644",
        "commit_message": "Fix issues in fused_dam (#469)\n\n* move import of amp_C to __init__()\n\n* make fp16/32 separate lists to support mixed param types, disable double test\n\n* make zero_grad consistent between adam/novograd/lamb\n\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestFusedAdam(unittest.TestCase):",
            "self.assertLessEqual(max_abs_diff, self.max_abs_diff)",
            "self.assertLessEqual(max_rel_diff, self.max_rel_diff)",
            "",
            "-    def test_double(self):",
            "-        self.gen_single_type_test(param_type=torch.double)",
            "-",
            "def test_float(self):",
            "self.gen_single_type_test(param_type=torch.float)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=def, text=def))",
            "Delete(target_node=ASTNode(type=identifier, text=test_double))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parameters))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=gen_single_type_test))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=param_type))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=double))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=function_definition))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 25,
        "number": 9122,
        "neg_line": [
            "-def test_double(self):",
            "-self.gen_single_type_test(param_type=torch.double)",
            "-"
        ],
        "pos_line": [],
        "core_change": "-def test_double(self): -self.gen_single_type_test(param_type=torch.double) -",
        "core_API": "assertLessEqual"
    },
    {
        "commit_hash": "349cfbdebfb05d2faf51ae7c82eca81c97ed5c49",
        "index": "b5b4e54c5..7cb8298fb 100644",
        "commit_message": "docs: add a new gobot tutorial (#982)\n\n* fix: fix super convergence tutor\n\n* fix: fix link in classification tutor\n\n* feat: add data prep & database build to gobot tutor\n\n* feat: make ner config dependent on variables only\n\n* feat: support empty slot vals\n\n* feat: in the middle of big refactor of ner\n\n* feat: slot-filling section in gobot tutor\n\n* feat: add simpler dstc2 format\n\n* feat: simplify tutor & simplify data\n\n* feat: refactor user state & add typings & and format strings\n\n* feat: update gobot tutor\n\n* feat: table name is optional in sqlite_database, rewrite to f-string\n\n* feat: add diagram pics\n\n* feat: add diagrams to tutor, add text\n\n* feat: fix imgs & add tutor plan\n\n* fix: fix simple data reader and ner iterator\n\n* fix: use simple dstc reader everywhere\n\n* feat: fix several tf warnings\n\n* fix: multiuser gobot support\n\n* feat: update gobot_dstc2 model\n\n* fix: error in DefaultTemplate\n\n* feat: update gobot_dstc2_best model\n\n* fix: fix debug mode\n\n* fix: dstc2_ner_iterator\n\n* feat: make api_call_action unrequired\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def variational_dropout(units, keep_prob, fixed_mask_dims=(1,)):",
            "noise_shape = [units_shape[n] for n in range(len(units.shape))]",
            "for dim in fixed_mask_dims:",
            "noise_shape[dim] = 1",
            "-    return tf.nn.dropout(units, keep_prob, noise_shape)",
            "+    return tf.nn.dropout(units, rate=1-keep_prob, noise_shape=noise_shape)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1922847)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=6, insert_id=1922848)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'rate'), position=0, insert_id=1922849)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1922850)",
            "Insert(target_node=IN(type=keyword_argument), node=('binary_operator', None), position=2, insert_id=1922851)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=noise_shape), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1922852)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'noise_shape'), position=2, insert_id=1922853)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=0, insert_id=1922854)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=1922855)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=keep_prob), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 9123,
        "neg_line": [
            "-return tf.nn.dropout(units, keep_prob, noise_shape)"
        ],
        "pos_line": [
            "+return tf.nn.dropout(units, rate=1-keep_prob, noise_shape=noise_shape)"
        ],
        "core_change": "-return tf.nn.dropout(units, keep_prob, noise_shape) +return tf.nn.dropout(units, rate=1-keep_prob, noise_shape=noise_shape)",
        "core_API": "dropout"
    },
    {
        "commit_hash": "094e1c775178fd85e91e8e6df5c42335bbd4c85d",
        "index": "8bd0759bea..d26d24fe1e 100644",
        "commit_message": "lint fix & general reformatting\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def tile(",
            "/,",
            "reps: Sequence[int],",
            "*,",
            "-    out: Optional[Union[tf.Tensor, tf.Variable]] = None",
            "-) -> Union[tf.Tensor, tf.Variable] :",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            "+) -> Union[tf.Tensor, tf.Variable]:",
            "if x.shape == ():",
            "x = tf.reshape(x, (-1,))",
            "if isinstance(reps, Number):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=1997905)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9125,
        "neg_line": [
            "-out: Optional[Union[tf.Tensor, tf.Variable]] = None",
            "-) -> Union[tf.Tensor, tf.Variable] :"
        ],
        "pos_line": [
            "+out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            "+) -> Union[tf.Tensor, tf.Variable]:"
        ],
        "core_change": "-out: Optional[Union[tf.Tensor, tf.Variable]] = None -) -> Union[tf.Tensor, tf.Variable] : +out: Optional[Union[tf.Tensor, tf.Variable]] = None, +) -> Union[tf.Tensor, tf.Variable]:",
        "core_API": "reshape"
    },
    {
        "commit_hash": "fd5cb7fcc32bd0962ba9b978489cd6014cfa6a6f",
        "index": "bcb351984..46379a9d1 100644",
        "commit_message": "Add PyTorch 1.8 Profiler 5/5 (#6618)\n\n* Refactor profilers\n\n* Update PassThrough\n\n* WIP - This is broken and will change\n\n* Update pytorch_lightning/profiler/pytorch.py\n\nCo-authored-by: thomas chaton <thomas@grid.ai>\n\n* resolve tests\n\n* resolve tests\n\n* find output\n\n* try something\n\n* update\n\n* add support for test and predict\n\n* update\n\n* update\n\n* use getattr\n\n* test\n\n* test\n\n* update\n\n* tests\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* remove file\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* test\n\n* update#\n\n* update\n\n* update tests\n\n* update\n\n* add suport for 1.8\n\n* rename records\n\n* add support for 1.8\n\n* update\n\n* resolve flake8\n\n* resolve test\n\n* Refactor basic profilers\n\n* Fixes\n\n* Unused import\n\n* Introduce setup\n\n* Profile on all ranks. Print to stdout on 0\n\n* Introduce dirpath + filename\n\n* CHANGELOG\n\n* Add tests. Address comments\n\n* add `on_run_stage_setup`\n\n* add on_run_stage_setup function\n\n* update\n\n* add test for RegisterRecordFunction\n\n* update lightnng flow direction\n\n* move variable to private\n\n* remove trace\n\n* Undo code that should be in 3/4\n\n* Multi-stage multi-rank\n\n* 2/5 changes\n\n* Pass stage in __del__\n\n* Remove TODOs\n\n* Describe on_evaluation_end. Add tests\n\n* Typo\n\n* Address comments\n\n* deepcopy tests\n\n* Advanced teardown\n\n* Fix teardown test\n\n* Fix tests\n\n* Minor change\n\n* Update CHANGELOG.md\n\n* Fix test\n\n* Quick fixes\n\n* Fix 6522\n\n* resolve ddp tests\n\n* resolve tests\n\n* resolve some tests\n\n* update tests\n\n* resolve tests\n\n* update\n\n* resolve tests\n\n* resolve some tests\n\n* Missed fixes from 3/5\n\n* Fixes\n\n* resolve some tests\n\n* resolve test for 1.7.1\n\n* Broken refactor\n\n* Missed stage\n\n* Minor changes\n\n* resolve tests\n\n* Update CHANGELOG\n\n* resolve bug\n\n* remove print\n\n* Typo\n\n* Cleanup\n\n* resolve ddp test\n\n* remove barrier\n\n* update profiler\n\n* update\n\n* Smaller model\n\n* update\n\n* resolve tests\n\n* update\n\n* Minor changes. CHANGELOG\n\n* Minimize diff\n\n* update to 1.8.1\n\n* RunIf. Extra code. Check segfault\n\n* resolve tests\n\n* Typo. Bad merge\n\n* Fixing a bad merge\n\n* replace for kineto\n\n* Update pytorch_lightning/profiler/pytorch.py\n\nCo-authored-by: ananthsub <ananth.subramaniam@gmail.com>\n\n* Update pytorch_lightning/profiler/pytorch.py\n\nCo-authored-by: ananthsub <ananth.subramaniam@gmail.com>\n\n* Minor changes\n\n* Bad merge\n\n* Use lists for flexibility\n\n* Use sets\n\n* predict_step\n\n* Ananth's suggestion\n\n* update\n\n* Docs\n\n* Update pl_examples/basic_examples/profiler_example.py\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* update example\n\n* update example\n\nCo-authored-by: Carlos Mocholi <carlossmocholi@gmail.com>\nCo-authored-by: ananthsub <ananth.subramaniam@gmail.com>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_plugin_setup_optimizers_in_pre_dispatch(tmpdir, delay_dispatch):",
            "assert len(self.trainer.optimizers) > 0",
            "",
            "class CustomPlugin(SingleDevicePlugin):",
            "+",
            "@property",
            "def setup_optimizers_in_pre_dispatch(self) -> bool:",
            "return delay_dispatch",
            "",
            "model = TestModel()",
            "-    trainer = Trainer(",
            "-        default_root_dir=tmpdir,",
            "-        fast_dev_run=True,",
            "-        plugins=CustomPlugin(device=torch.device(\"cpu\"))",
            "-    )",
            "+    trainer = Trainer(default_root_dir=tmpdir, fast_dev_run=True, plugins=CustomPlugin(device=torch.device(\"cpu\")))",
            "trainer.fit(model)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 5,
        "AST_diff_line": 17,
        "number": 9126,
        "neg_line": [
            "-trainer = Trainer(",
            "-default_root_dir=tmpdir,",
            "-fast_dev_run=True,",
            "-plugins=CustomPlugin(device=torch.device(\"cpu\"))",
            "-)"
        ],
        "pos_line": [
            "+",
            "+trainer = Trainer(default_root_dir=tmpdir, fast_dev_run=True, plugins=CustomPlugin(device=torch.device(\"cpu\")))"
        ],
        "core_change": "+ -trainer = Trainer( -default_root_dir=tmpdir, -fast_dev_run=True, -plugins=CustomPlugin(device=torch.device(\"cpu\")) -) +trainer = Trainer(default_root_dir=tmpdir, fast_dev_run=True, plugins=CustomPlugin(device=torch.device(\"cpu\")))",
        "core_API": "device"
    },
    {
        "commit_hash": "9f7f95221f87763240f7da5bf680b3538db2f8cc",
        "index": "97e5509d..99b64269 100644",
        "commit_message": "Milvus integration (#771)\n\n* Initial commit for Milvus integration\n\n* Add latest docstring and tutorial changes\n\n* Updating implementation of Milvus document store\n\n* Add latest docstring and tutorial changes\n\n* Adding tests and updating doc string\n\n* Add latest docstring and tutorial changes\n\n* Fixing issue caught by tests\n\n* Addressing review comments\n\n* Fixing mypy detected issue\n\n* Fixing issue caught in test about sorting of vector ids\n\n* fixing test\n\n* Fixing generator test failure\n\n* update docstrings\n\n* Addressing review comments about multiple network call while fetching embedding from milvus server\n\n* Add latest docstring and tutorial changes\n\n* Ignoring mypy issue while converting vector_id to int\n\nCo-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>\nCo-authored-by: Malte Pietsch <malte.pietsch@deepset.ai>\n",
        "file": "haystack.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RAGenerator(BaseGenerator):",
            "embeddings = self.retriever.embed_passages(docs)",
            "",
            "embeddings_in_tensor = torch.cat(",
            "-            [torch.from_numpy(embedding).unsqueeze(0) for embedding in embeddings],",
            "+            [torch.from_numpy(embedding).float().unsqueeze(0) for embedding in embeddings],",
            "dim=0",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=246381)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=246382)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=246383)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float'), position=2, insert_id=246384)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=246385)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=246386)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 9128,
        "neg_line": [
            "-[torch.from_numpy(embedding).unsqueeze(0) for embedding in embeddings],"
        ],
        "pos_line": [
            "+[torch.from_numpy(embedding).float().unsqueeze(0) for embedding in embeddings],"
        ],
        "core_change": "-[torch.from_numpy(embedding).unsqueeze(0) for embedding in embeddings], +[torch.from_numpy(embedding).float().unsqueeze(0) for embedding in embeddings],",
        "core_API": "embed_passages"
    },
    {
        "commit_hash": "302628fb4c5e4d9fba6f7619356c49f491b88056",
        "index": "65a1674b..03a2a36c 100644",
        "commit_message": "bugfixe in baseline and other places\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TestTutorialCode(unittest.TestCase):",
            "batch_size=8,",
            "first_update=100,",
            "target_sync_frequency=50,",
            "-            preprocessing=preprocessing",
            "+            states_preprocessing_spec=states_preprocessing_spec",
            ")",
            "",
            "agent.close()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=preprocessing), value='states_preprocessing_spec')",
            "Update(target_node=ASTNode(type=identifier, text=preprocessing), value='states_preprocessing_spec')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 9131,
        "neg_line": [
            "-preprocessing=preprocessing"
        ],
        "pos_line": [
            "+states_preprocessing_spec=states_preprocessing_spec"
        ],
        "core_change": "-preprocessing=preprocessing +states_preprocessing_spec=states_preprocessing_spec",
        "core_API": "close"
    },
    {
        "commit_hash": "be652aa4375b84549dd0a462aee23b80556fe532",
        "index": "95e4313d..f6af22b3 100644",
        "commit_message": "Fix Windows UT (#3065)\n\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpeedupTestCase(TestCase):",
            "net.load_state_dict(state_dict)",
            "net.eval()",
            "",
            "-        data = torch.randn(BATCH_SIZE, 3, 224, 224).to(device)",
            "+        data = torch.randn(BATCH_SIZE, 3, 128, 128).to(device)",
            "ms = ModelSpeedup(net, data, MASK_FILE)",
            "ms.speedup_model()",
            "ms.bound_model(data)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=224), value='128')",
            "Update(target_node=ASTNode(type=integer, text=224), value='128')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 9132,
        "neg_line": [
            "-data = torch.randn(BATCH_SIZE, 3, 224, 224).to(device)"
        ],
        "pos_line": [
            "+data = torch.randn(BATCH_SIZE, 3, 128, 128).to(device)"
        ],
        "core_change": "-data = torch.randn(BATCH_SIZE, 3, 224, 224).to(device) +data = torch.randn(BATCH_SIZE, 3, 128, 128).to(device)",
        "core_API": "load_state_dict"
    },
    {
        "commit_hash": "5ff25c9f61e69c6f4b92611093b25b188fd4086b",
        "index": "5cf9aed..b0c5836 100644",
        "commit_message": "Fixes #100: do not train keep probability in dropout op\n\n",
        "file": "skflow.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def dropout(tensor_in, prob, name=None):",
            "with tf.op_scope([tensor_in], name, \"dropout\") as name:",
            "if isinstance(prob, float):",
            "prob = tf.get_variable(\"prob\", [],",
            "-                                   initializer=tf.constant_initializer(prob))",
            "+                                   initializer=tf.constant_initializer(prob),",
            "+                                   trainable=False)",
            "tf.add_to_collection(DROPOUTS, prob)",
            "return tf.nn.dropout(tensor_in, prob)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=2170458)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=2170459)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'trainable'), position=0, insert_id=2170460)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2170461)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=2170462)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 9134,
        "neg_line": [
            "-initializer=tf.constant_initializer(prob))"
        ],
        "pos_line": [
            "+initializer=tf.constant_initializer(prob),",
            "+trainable=False)"
        ],
        "core_change": "-initializer=tf.constant_initializer(prob)) +initializer=tf.constant_initializer(prob), +trainable=False)",
        "core_API": "op_scope"
    },
    {
        "commit_hash": "34f78294d387aabaee2733aff5089b75685c46ef",
        "index": "0758dfb..e3e5c3d 100644",
        "commit_message": "fix pooling bugs across a few new archs\n\n",
        "file": "vit-pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class PiT(nn.Module):",
            "x += self.pos_embedding",
            "x = self.dropout(x)",
            "",
            "-        return self.layers(x)",
            "+        x = self.layers(x)",
            "+",
            "+        return self.mlp_head(x[:, 0])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1562751)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1562752)",
            "Insert(target_node=ASTNode(type=return_statement), node=('call', None), position=1, insert_id=1562753)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'x'), position=0, insert_id=1562754)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1562755)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1562756)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1562757)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1562758)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1562759)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'mlp_head'), position=2, insert_id=1562760)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1562761)",
            "Insert(target_node=IN(type=argument_list), node=('subscript', None), position=1, insert_id=1562762)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1562763)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'x'), position=0, insert_id=1562764)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1562765)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=2, insert_id=1562766)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=1562767)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=4, insert_id=1562768)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=5, insert_id=1562769)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=1562770)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 9135,
        "neg_line": [
            "-return self.layers(x)"
        ],
        "pos_line": [
            "+x = self.layers(x)",
            "+",
            "+return self.mlp_head(x[:, 0])"
        ],
        "core_change": "-return self.layers(x) +x = self.layers(x) + +return self.mlp_head(x[:, 0])",
        "core_API": "dropout"
    },
    {
        "commit_hash": "70eedbd35148a4045ffc9caf81b7cf4acf32c48a",
        "index": "a30765e2..cdee8e29 100644",
        "commit_message": "Refactor category orders in heads (#2374)\n\n* Refactor (all): all category -1 in anchor/bbox head and anchor/bbox target assign\n\n* Fix (datasets): remove label + 1 in datasets\n\n* Fix (bbox_head): fix bug of fc_cls that forget + 1\n\n* Fix (atss_head & free_anchor): fix cat -1 bugs\n\n* Fix (mask_head): remove label + 1 in mask heads\n\n* fix atss\n\n* Fix (rpn): fix cross_entropy_loss bug of RPN\n\n* Fix (anchor_head): fix typo\n\n* Refactor (anchor_head): use background_label rather than num_classes to indicate background class\n\n* Refactor (docstring): add and reformat docstrings\n\n* fix mask iou head\n\n* Fix (mask_head): fix cat -1 bug\n\n* Fix (mask_head): fix bug in mask inference\n\n* Add (tests): add tests for mask rcnn and mask heads\n\n* Refactor (unittest): refactor test_forward\n\n* Refactor (new_empty): use new_full rather than new_empty\n\n* Refactor (background_label): check background_label\n\n* Add TODO\n\n* Refactor (unittest): allow BP in unittest\n\nCo-authored-by: Cao Yuhang <yhcao6@gmail.com>\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class ConvFCBBoxHead(BBoxHead):",
            "self.relu = nn.ReLU(inplace=True)",
            "# reconstruct fc_cls and fc_reg since input channels are changed",
            "if self.with_cls:",
            "-            self.fc_cls = nn.Linear(self.cls_last_dim, self.num_classes)",
            "+            self.fc_cls = nn.Linear(self.cls_last_dim, self.num_classes + 1)",
            "if self.with_reg:",
            "out_dim_reg = (4 if self.reg_class_agnostic else 4 *",
            "self.num_classes)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=3, insert_id=1426887)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=1426888)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=2, insert_id=1426889)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 9139,
        "neg_line": [
            "-self.fc_cls = nn.Linear(self.cls_last_dim, self.num_classes)"
        ],
        "pos_line": [
            "+self.fc_cls = nn.Linear(self.cls_last_dim, self.num_classes + 1)"
        ],
        "core_change": "-self.fc_cls = nn.Linear(self.cls_last_dim, self.num_classes) +self.fc_cls = nn.Linear(self.cls_last_dim, self.num_classes + 1)",
        "core_API": "ReLU"
    },
    {
        "commit_hash": "f6c76230938f8c5cd09ef85913a414c37862b206",
        "index": "a5910d7c..33bb9299 100644",
        "commit_message": "Add Pre-commit bot fixes (#3722)\n\n* flake8\n\n* flake8\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_point_pair_features():",
            "assert data.pos.tolist() == pos.tolist()",
            "assert data.norm.tolist() == norm.tolist()",
            "assert data.edge_index.tolist() == edge_index.tolist()",
            "-    assert torch.allclose(",
            "-        data.edge_attr,",
            "-        torch.Tensor([[1, 1, 0, 0, 0], [1, 1, PI, PI, 0]]),",
            "-        atol=1e-04)",
            "+    assert torch.allclose(data.edge_attr,",
            "+                          torch.Tensor([[1, 1, 0, 0, 0], [1, 1, PI, PI, 0]]),",
            "+                          atol=1e-04)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 9141,
        "neg_line": [
            "-assert torch.allclose(",
            "-data.edge_attr,",
            "-torch.Tensor([[1, 1, 0, 0, 0], [1, 1, PI, PI, 0]]),",
            "-atol=1e-04)"
        ],
        "pos_line": [
            "+assert torch.allclose(data.edge_attr,",
            "+torch.Tensor([[1, 1, 0, 0, 0], [1, 1, PI, PI, 0]]),",
            "+atol=1e-04)"
        ],
        "core_change": "-assert torch.allclose( -data.edge_attr, -torch.Tensor([[1, 1, 0, 0, 0], [1, 1, PI, PI, 0]]), -atol=1e-04) +assert torch.allclose(data.edge_attr, +torch.Tensor([[1, 1, 0, 0, 0], [1, 1, PI, PI, 0]]), +atol=1e-04)",
        "core_API": "tolist"
    },
    {
        "commit_hash": "2a6fcc6cab0a0371698bc9fa375c67e633817c86",
        "index": "5b00da7871..6858c2f3e9 100644",
        "commit_message": "formatting fixes for Array API submodule in Torch backend.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "# global",
            "import torch",
            "-from typing import Union, Optional, Tuple, List",
            "",
            "",
            "def argsort(x: torch.Tensor,",
            "axis: int = -1,",
            "descending: bool = False,",
            "stable: bool = True)\\",
            "-            -> torch.Tensor:",
            "+        -> torch.Tensor:",
            "return torch.argsort(x, dim=axis, descending=descending)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=from, text=from))",
            "Delete(target_node=ASTNode(type=identifier, text=typing))",
            "Delete(target_node=ASTNode(type=dotted_name))",
            "Delete(target_node=ASTNode(type=import, text=import))",
            "Delete(target_node=ASTNode(type=identifier, text=Union))",
            "Delete(target_node=ASTNode(type=dotted_name))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=Optional))",
            "Delete(target_node=ASTNode(type=dotted_name))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=Tuple))",
            "Delete(target_node=ASTNode(type=dotted_name))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=List))",
            "Delete(target_node=ASTNode(type=dotted_name))",
            "Delete(target_node=ASTNode(type=import_from_statement))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 9144,
        "neg_line": [
            "-from typing import Union, Optional, Tuple, List",
            "--> torch.Tensor:"
        ],
        "pos_line": [
            "+-> torch.Tensor:"
        ],
        "core_change": "-from typing import Union, Optional, Tuple, List --> torch.Tensor: +-> torch.Tensor:",
        "core_API": "argsort"
    },
    {
        "commit_hash": "3f211d36c26c82aa4c1ffb772a6cbcb2546fc796",
        "index": "3dd0dc9..eea7b9f 100644",
        "commit_message": "group tensorboard and fix name checkpoint\n\n",
        "file": "TensorFlowTTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class GanBasedTrainer(metaclass=abc.ABCMeta):",
            "\"\"\"Write variables to tensorboard.\"\"\"",
            "with self.writer.as_default():",
            "for key, value in list_metrics.items():",
            "-                tf.summary.scalar(stage + \"_\" + key, value.result(), step=self.steps)",
            "+                tf.summary.scalar(stage + \"/\" + key, value.result(), step=self.steps)",
            "self.writer.flush()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"_\"), value='\"/\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9145,
        "neg_line": [
            "-tf.summary.scalar(stage + \"_\" + key, value.result(), step=self.steps)"
        ],
        "pos_line": [
            "+tf.summary.scalar(stage + \"/\" + key, value.result(), step=self.steps)"
        ],
        "core_change": "-tf.summary.scalar(stage + \"_\" + key, value.result(), step=self.steps) +tf.summary.scalar(stage + \"/\" + key, value.result(), step=self.steps)",
        "core_API": "as_default"
    },
    {
        "commit_hash": "0db5d911fc94604f9568b4b212e005ec4600d157",
        "index": "6c37135b4..a8dd0ec7c 100644",
        "commit_message": "Fix `SpeechT5ForSpeechToSpeechIntegrationTests` device issue (#21460)\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpeechT5ForSpeechToSpeechIntegrationTests(unittest.TestCase):",
            "input_speech = self._load_datasamples(1)",
            "input_values = processor(audio=input_speech, return_tensors=\"pt\").input_values.to(torch_device)",
            "",
            "-        speaker_embeddings = torch.zeros((1, 512))",
            "+        speaker_embeddings = torch.zeros((1, 512), device=torch_device)",
            "generated_speech = model.generate_speech(input_values, speaker_embeddings=speaker_embeddings)",
            "",
            "self.assertEqual(generated_speech.shape[1], model.config.num_mel_bins)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1176938)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1176939)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1176940)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1176941)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'torch_device'), position=2, insert_id=1176942)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 9147,
        "neg_line": [
            "-speaker_embeddings = torch.zeros((1, 512))"
        ],
        "pos_line": [
            "+speaker_embeddings = torch.zeros((1, 512), device=torch_device)"
        ],
        "core_change": "-speaker_embeddings = torch.zeros((1, 512)) +speaker_embeddings = torch.zeros((1, 512), device=torch_device)",
        "core_API": "_load_datasamples"
    },
    {
        "commit_hash": "456cc87954d2afe6defcdb5beb8b6e6132a70892",
        "index": "4b0b3f702..2ae1262eb 100644",
        "commit_message": "Fuse_modules in a qat-respecting way (#12891)\n\n* Fuse_modules in a qat-respecting way\n\n* Add compatibility for PyTorch <1.11\n\nIn older pytorch versions, `fuse_modules` used the `Module.training`\nflag to determine wheter fusion should be QAT-compliant or not, refer\nhttps://github.com/pytorch/pytorch/releases/tag/v1.11.0\n\n* Add CHANGELOG for pull #12891\n\n* Fix conditional import of fuse_modules_qat\n\n`torch.ao.quantization.fuse_modules_qat` was actually added in\ntorch 1.11.\n\n* Update CHANGELOG.md\n\nCo-authored-by: Akihiro Nitta <nitta@akihironitta.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class QuantizationAwareTraining(Callback):",
            "model.qconfig = self._qconfig",
            "",
            "if self._check_feasible_fuse(model):",
            "-            torch.quantization.fuse_modules(model, self._modules_to_fuse, inplace=True)",
            "+            fuse_modules(model, self._modules_to_fuse, inplace=True)",
            "",
            "# Prepare the model for QAT. This inserts observers and fake_quants in",
            "# the model that will observe weight and activation tensors during calibration."
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=fuse_modules), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=quantization))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 9149,
        "neg_line": [
            "-torch.quantization.fuse_modules(model, self._modules_to_fuse, inplace=True)"
        ],
        "pos_line": [
            "+fuse_modules(model, self._modules_to_fuse, inplace=True)"
        ],
        "core_change": "-torch.quantization.fuse_modules(model, self._modules_to_fuse, inplace=True) +fuse_modules(model, self._modules_to_fuse, inplace=True)",
        "core_API": "_check_feasible_fuse"
    },
    {
        "commit_hash": "c317aaa3de289fc0a69e6b1001ffc586bb4d9c9a",
        "index": "7e55352c..56745933 100644",
        "commit_message": "Refactor library namespaces [pre-release][0.6-rc1] (#1412)\n\n* flake fixes\n\n* initial flake8 fixeS\n\n* remove top level from kornia.color\n\n* kornia filters\n\n* kornia losses\n\n* kornia features\n\n* geomtry and all ok\n\n* removed jit module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* apply formatting and few fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add keep block for isort\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* skip init\n\n* fix the docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove old code\n\n* simplify ci workflow\n\n* fix circular dependency\n\n* few format fixes\n\n* fix code format test\n\n* remove kornia.jit imports\n\n* final fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* ci fixes\n\n* add versioneer\n\n* fix pnp import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* exclude version files from pre-commit\n\n* exclude files precommit\n\n* update to pytorch 1.10 and add fixes\n\n* Update tests_cpu.yml\n\n* Update setup_dev_env.sh\n\n* Update tests_cpu.yml\n\n* undo versioneer\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix no_grad\n\n* Apply suggestions from code review\n\n* fix skip windows tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update test_integrated.py\n\n* Apply suggestions from code review\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def triangulate_points(",
            "_, _, V = torch.svd(X)",
            "",
            "points3d_h = V[..., -1]",
            "-    points3d: torch.Tensor = kornia.convert_points_from_homogeneous(points3d_h)",
            "+    points3d: torch.Tensor = convert_points_from_homogeneous(points3d_h)",
            "return points3d"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=convert_points_from_homogeneous), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=kornia))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 9153,
        "neg_line": [
            "-points3d: torch.Tensor = kornia.convert_points_from_homogeneous(points3d_h)"
        ],
        "pos_line": [
            "+points3d: torch.Tensor = convert_points_from_homogeneous(points3d_h)"
        ],
        "core_change": "-points3d: torch.Tensor = kornia.convert_points_from_homogeneous(points3d_h) +points3d: torch.Tensor = convert_points_from_homogeneous(points3d_h)",
        "core_API": "svd"
    },
    {
        "commit_hash": "da6dbc8d1d128cf783d7151b012a5502bbd52bf5",
        "index": "f319dd659..b02f76836 100644",
        "commit_message": "PoC: Accelerator refactor (#5743)\n\n* restoring the result from subprocess\n\n* fix queue.get() order for results\n\n* add missing \"block_backward_sync\" context manager\n\n* add missing \"block_backward_sync\" context manager\n\n* fix sync_batchnorm\n\n* fix supported gpu-ids for tuple\n\n* fix clip gradients and inf recursion\n\n* accelerator selection: added cluster_environment plugin\n\n* fix torchelastic test\n\n* fix reduce early stopping decision for DDP\n\n* fix tests: callbacks, conversion to lightning optimizer\n\n* fix lightning optimizer does not pickle\n\n* fix setting benchmark and deterministic option\n\n* fix slurm amp test\n\n* fix prepare_data test and determine node_rank\n\n* fix retrieving last path when testing\n\n* remove obsolete plugin argument\n\n* fix test: test_trainer_config\n\n* fix torchscript tests\n\n* fix trainer.model access\n\n* move properties\n\n* fix test_transfer_batch_hook\n\n* fix auto_select_gpus\n\n* fix omegaconf test\n\n* fix test that needs to simulate slurm ddp\n\n* add horovod plugin\n\n* fix test with named arguments\n\n* clean up whitespace\n\n* fix datamodules test\n\n* remove old accelerators\n\n* fix naming\n\n* move old plugins\n\n* move to plugins\n\n* create precision subpackage\n\n* create training_type subpackage\n\n* fix all new import errors\n\n* fix wrong arguments order passed to test\n\n* fix LR finder\n\n* Added sharded training type and amp plugin\n\n* Move clip grad to precision plugin\n\n* Added sharded spawn, select accelerators based on distributed_backend + enable custom fp16 plugin automatically\n\n* Fix import issue, attempting to fix tests\n\n* Fix initial test\n\n* Reflect hook logic from master, should wrap model after move to device\n\n* Optional state consolidation, since master has optimizers not wrapped\n\n* change attribute for instance test\n\n* reset optimizers\n\noptimizers are not used in main process, so state would be wrong.\n\n* legacy\n\n* imports in accel\n\n* legacy2\n\n* trainer imports\n\n* fix import errors after rebase\n\n* move hook to new setup location\n\n* provide unwrapping logic\n\n* fix trainer callback system\n\n* added ddp2 implementation\n\n* fix imports .legacy\n\n* move plugins\n\n* restore legacy\n\n* drop test.py from root\n\n* add tpu accelerator and plugins\n\n* fixes\n\n* fix lightning optimizer merge\n\n* reset bugreportmodel\n\n* unwrapping\n\n* step routing forward\n\n* model access\n\n* unwrap\n\n* opt\n\n* integrate distrib_type\n\n* sync changes\n\n* sync\n\n* fixes\n\n* add forgotten generators\n\n* add missing logic\n\n* update\n\n* import\n\n* missed imports\n\n* import fixes\n\n* isort\n\n* mv f\n\n* changelog\n\n* format\n\n* move helper to parallel plugin\n\n* d\n\n* add world size\n\n* clean up\n\n* duplicate\n\n* activate ddp_sharded and tpu\n\n* set nvidia flags\n\n* remove unused colab var\n\n* use_tpu <-> on_tpu attrs\n\n* make some ddp_cpu and clusterplugin tests pass\n\n* Ref/accelerator connector (#5742)\n\n* final cleanup\n\nCo-authored-by: Adrian WÃ¤lchli <aedu.waelchli@gmail.com>\n\n* connector cleanup\n\nCo-authored-by: Adrian WÃ¤lchli <aedu.waelchli@gmail.com>\n\n* trainer cleanup\n\nCo-authored-by: Adrian WÃ¤lchli <aedu.waelchli@gmail.com>\n\n* accelerator cleanup + missing logic in accelerator connector\n\nCo-authored-by: Adrian WÃ¤lchli <aedu.waelchli@gmail.com>\n\n* add missing changes to callbacks\n\nCo-authored-by: Adrian WÃ¤lchli <aedu.waelchli@gmail.com>\n\n* reflect accelerator changes to lightning module\n\nCo-authored-by: Adrian WÃ¤lchli <aedu.waelchli@gmail.com>\n\n* clean cluster envs\n\nCo-authored-by: Adrian WÃ¤lchli <aedu.waelchli@gmail.com>\n\n* cleanup plugins\n\nCo-authored-by: Adrian WÃ¤lchli <aedu.waelchli@gmail.com>\n\n* add broadcasting\n\nCo-authored-by: Adrian WÃ¤lchli <aedu.waelchli@gmail.com>\n\n* yapf\n\n* remove plugin connector\n\nCo-authored-by: Adrian WÃ¤lchli <aedu.waelchli@gmail.com>\n\n* plugins\n\n* manual optimization\n\n* update optimizer routing\n\n* add rank to torchelastic\n\n* fix memory mixed precision\n\n* setstate on trainer for pickling in ddp spawn\n\n* add predict method\n\n* add back commented accelerator code\n\n* adapt test for sync_batch_norm to new plugin\n\n* fix deprecated tests\n\n* fix ddp cpu choice when no num_processes are given\n\n* yapf format\n\n* skip a memory test that cannot pass anymore\n\n* fix pickle error in spawn plugin\n\n* x\n\n* avoid\n\n* x\n\n* fix cyclic import in docs build\n\n* add support for sharded\n\n* update typing\n\n* add sharded and sharded_spawn to distributed types\n\n* make unwrap model default\n\n* refactor LightningShardedDataParallel similar to LightningDistributedDataParallel\n\n* update sharded spawn to reflect changes\n\n* update sharded to reflect changes\n\n* Merge 1.1.5 changes\n\n* fix merge\n\n* fix merge\n\n* yapf isort\n\n* fix merge\n\n* yapf isort\n\n* fix indentation in test\n\n* copy over reinit scheduler implementation from dev1.2\n\n* fix apex tracking calls with dev_debugger\n\n* reduce diff to dev1.2, clean up\n\n* fix trainer config test  when gpus>0 and num_processes >0 and ddp_cpu\n\n* sort plugin tests legacy/new\n\n* fix error handling for amp on cpu\n\n* fix merge\n\n\nfix merge\n\n\nfix merge\n\n* [Feat] Resolve manual_backward (#5837)\n\n* resolve manual_backward\n\n* resolve flake8\n\n* update\n\n* resolve for ddp_spawn\n\n* resolve flake8\n\n* resolve flake8\n\n* resolve flake8\n\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-88-60.ec2.internal>\n\n* fix tests/accelerator tests on cpu\n\n* [BugFix] Resolve manual optimization (#5852)\n\n* resolve manual_optimization\n\n* update\n\n* update\n\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-88-60.ec2.internal>\n\n* Remove copy trainer parameters to happen earlier within the loop and add safe guard to get ref model (#5856)\n\n* resovle a bug\n\n* Accelerator refactor sharded rpc (#5854)\n\n* rpc branch\n\n* merge\n\n* update handling of rpc\n\n* make devices etc. Optional in RPC\n\n* set devices etc. later if necessary\n\n* remove devices from sequential\n\n* make devices optional in rpc\n\n* fix import\n\n* uncomment everything\n\n* fix cluster selection\n\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-88-60.ec2.internal>\n\n* resolve bug\n\n* fix assert in rpc test\n\n* resolve a test\n\n* fix docs compilation\n\n* accelerator refactor - fix for sharded parity test (#5866)\n\n* fix memory issue with ddp_spawn\n\n* x\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n* x\n\n* Remove DDP2 as this does not apply\n\n* Add missing pre optimizer hook to ensure lambda closure is called\n\n* fix apex docstring\n\n* [accelerator][BugFix] Resolve some test for 1 gpu (#5863)\n\n* update\n\n* revert init\n\n* resolve a bug\n\n* update\n\n* resolve flake8\n\n* update\n\n* update\n\n* update\n\n* revert init\n\n* resolve a bug\n\n* update\n\n* resolve flake8\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* revert init\n\n* resolve a bug\n\n* update\n\n* resolve flake8\n\n* update\n\n* update\n\n* update\n\n* revert init\n\n* update\n\n* resolve flake8\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* all_gather\n\n* update\n\n* make plugins work, add misconfig for RPC\n\n* update\n\n* update\n\n* remove breaking test\n\n* resolve some tests\n\n* resolve flake8\n\n* revert to ddp_spawn\n\nCo-authored-by: root <root@ip-172-31-88-60.ec2.internal>\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-88-60.ec2.internal>\nCo-authored-by: Justus Schock <justus.schock@rwth-aachen.de>\n\n* yapf isort\n\n* resolve flake8\n\n* fix apex doctests\n\n* fix apex doctests 2\n\n* resolve docs\n\n* update drone\n\n* clean env\n\n* update\n\n* update\n\n* update\n\n* update\n\n* merge\n\n* Fix RPC related tests, clean out old API, update for new accelerator API [skip ci] (#5881)\n\n* Fix RPC related tests, clean out old API, update for new accelerator API\n\n* Move tests out of legacy folder, update paths and names\n\n* Update test_remove_1-4.py\n\n* Expose properties for tpu cores/gpus/num_gpus\n\n* Add root GPU property\n\n* Move properties to properties.py\n\n* move tests that were previously in drone\n\n* Fix root GPU property (#5908)\n\n* Move root GPU to property, remove horovod set as this is handled in horovod plugin, ensure we mock correctly to set GPU accelerator\n\n* Add missing tests back\n\n* fix best model path transfer when no checkpoint callback available\n\n* Fix setup hook order [wip] (#5858)\n\n* Call trainer setup hook before accelerator setup\n\n* Add test case\n\n* add new test\n\n* typo\n\n* fix callback order in test\n\nCo-authored-by: tchaton <thomas@grid.ai>\nCo-authored-by: Adrian WÃ¤lchli <aedu.waelchli@gmail.com>\n\n* rename ddp sequential -> rpc sequential for special test\n\n* revert\n\n* fix stupid merge problem\n\n* Use property in connector for sampler (#5913)\n\n* merge the import conflicts\n\n* fix spawning of processes in slurm\n\n* [wip] Fix some bugs for TPU [skip ci] (#5878)\n\n* fixed for single tpu\n\n* fixed spawn\n\n* fixed spawn\n\n* update\n\n* update\n\n* wip\n\n* resolve bugs\n\n* resolve bug\n\n* update on comment\n\n* removed decorator\n\n* resolve comments\n\n* set to 4\n\n* update\n\n* update\n\n* need cleaning\n\n* update\n\n* update\n\n* update\n\n* resolve flake8\n\n* resolve bugs\n\n* exclude broadcast\n\n* resolve bugs\n\n* change test\n\n* update\n\n* update\n\n* skip if meet fails\n\n* properly raise trace\n\n* update\n\n* add catch\n\n* wrap test\n\n* resolve typo\n\n* update\n\n* typo\n\nCo-authored-by: Lezwon Castelino <lezwon@gmail.com>\nCo-authored-by: Your Name <you@example.com>\n\n* resolve some tests\n\n* update\n\n* fix imports\n\n* update\n\n* resolve flake8\n\n* update azure pipeline\n\n* skip a sharded test on cpu that requires a gpu\n\n* resolve tpus\n\n* resolve bug\n\n* resolve flake8\n\n* update\n\n* updat utils\n\n* revert permission change on files\n\n* suggestions from carlos\n\nCo-authored-by: Carlos MocholÃ­ <carlossmocholi@gmail.com>\n\n* remove unrelated formatting changes\n\n* remove incomplete comment\n\n* Update pytorch_lightning/accelerators/__init__.py\n\nCo-authored-by: Carlos MocholÃ­ <carlossmocholi@gmail.com>\n\n* remove unrelated formatting change\n\n* add types\n\n* warn 1.7 ddp manual backward only if ddp kwarg unset\n\n* yapf + isort\n\n* pep8 unused imports\n\n* fix cyclic import in docs\n\n* Apply suggestions from code review\n\n* typer in accelerator.py\n\n* typo\n\n* Apply suggestions from code review\n\n* formatting\n\n* update on comments\n\n* update typo\n\n* Update pytorch_lightning/trainer/properties.py\n\nCo-authored-by: Adrian WÃ¤lchli <aedu.waelchli@gmail.com>\n\n* update\n\n* suggestion from code review\n\n* suggestion from code review\n\nCo-authored-by: Adrian WÃ¤lchli <aedu.waelchli@gmail.com>\nCo-authored-by: SeanNaren <sean@grid.ai>\nCo-authored-by: Jirka Borovec <jirka.borovec@seznam.cz>\nCo-authored-by: chaton <thomas@grid.ai>\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-88-60.ec2.internal>\nCo-authored-by: Sean Naren <sean.narenthiran@gmail.com>\nCo-authored-by: root <root@ip-172-31-88-60.ec2.internal>\nCo-authored-by: Lezwon Castelino <lezwon@gmail.com>\nCo-authored-by: Your Name <you@example.com>\nCo-authored-by: Carlos MocholÃ­ <carlossmocholi@gmail.com>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: mergify[bot] <37929162+mergify[bot]@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "dataloader",
        "change": [
            "class TrainerDataLoadingMixin(ABC):",
            "dataloader = self._flatten_dl_only(dataloader)",
            "",
            "if self.accelerator_backend is not None:",
            "-            self.accelerator_backend.barrier('get_dataloaders')",
            "+            self.training_type_plugin.barrier('get_dataloaders')",
            "return dataloader",
            "",
            "def _flatten_dl_only(self, dataloaders):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=accelerator_backend), value='training_type_plugin')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9155,
        "neg_line": [
            "-self.accelerator_backend.barrier('get_dataloaders')"
        ],
        "pos_line": [
            "+self.training_type_plugin.barrier('get_dataloaders')"
        ],
        "core_change": "-self.accelerator_backend.barrier('get_dataloaders') +self.training_type_plugin.barrier('get_dataloaders')",
        "core_API": "_flatten_dl_only"
    },
    {
        "commit_hash": "1fe3a5a01123dcfea8a7981fc5aea212d42c1299",
        "index": "c7ac6a55..594cd88a 100644",
        "commit_message": "feat: add fp16 inference support (torch/onnx) (#871)\n\n* feat: add fp16 inference in clip_torch\n\n* Revert \"feat: add fp16 inference in clip_torch\"\n\nThis reverts commit 326e2655f423a67a5d6ea5c28eed8d81da037df0.\n\n* feat: add fp16 inference in clip_torch\n\n* fix: device\n\n* fix: str to torch.dtype\n\n* fix: layernorm\n\n* feat: add fp16 inference in clip_trt\n\n* feat: add fp16 inference in clip_onnx\n\n* fix: housekeeping\n\n* fix: ci\n\n* fix: ci\n\n* fix: ci\n\n* fix: ci and get test path\n\n* fix: dtype amp and gpu test dependency\n\n* fix: layernorm\n\n* fix: cast dtype in visiontransformer\n\n* fix: clip_onnx\n\n* fix: clip_onnx\n\n* fix: convert onnx to fp16\n\n* fix: dtype in preproc images\n\n* fix: dtype in preproc images\n\n* fix: typo\n\n* fix: dtype in clip_torch and fp16 in trt\n\n* fix: remove plain text in trt_test\n\n* fix: test\n\n* fix: typo\n\n* fix: stash\n\n* Revert \"fix: stash\"\n\nThis reverts commit f72fd99f4333443e998edbf4be352fe81557f611.\n\n* fix: for test\n\n* fix: onnx\n\n* fix: for test\n\n* fix: for test\n\n* fix: trt\n\n* fix: convert onnx to fp16 before convert trt\n\n* fix: discard changes in trt\n\n* fix: optimize fp16 test\n\n* fix: move __cast_dtype__\n\n* Revert \"fix: move __cast_dtype__\"\n\nThis reverts commit edf46292f144aee1150cd1f16565a2236a5ce5fe.\n\n* fix: ci\n",
        "file": "clip-as-service.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def preproc_image(",
            "if drop_image_content:",
            "d.pop('blob', 'tensor')",
            "",
            "-    tensors_batch = torch.stack(tensors_batch).type(torch.float32)",
            "+    tensors_batch = torch.stack(tensors_batch).type(dtype)",
            "",
            "if return_np:",
            "tensors_batch = tensors_batch.cpu().numpy()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='dtype')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=torch), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float32))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 9156,
        "neg_line": [
            "-tensors_batch = torch.stack(tensors_batch).type(torch.float32)"
        ],
        "pos_line": [
            "+tensors_batch = torch.stack(tensors_batch).type(dtype)"
        ],
        "core_change": "-tensors_batch = torch.stack(tensors_batch).type(torch.float32) +tensors_batch = torch.stack(tensors_batch).type(dtype)",
        "core_API": "pop"
    },
    {
        "commit_hash": "786be2a3f8b8160640614638f2fddc11572e2bc8",
        "index": "c5ca354d0..a1afb8c17 100755",
        "commit_message": "Should be fixed finally\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "# train",
            "logging.info('backend = ' + args.backend)",
            "if args.backend == \"chainer\":",
            "-        from espnet.lmchainer.lm_chainer import train",
            "+        from espnet.lm.chainer.lm_chainer import train",
            "train(args)",
            "elif args.backend == \"pytorch\":",
            "-        from espnet.lmpytorch.lm_pytorch import train",
            "+        from espnet.lm.pytorch.lm_pytorch import train",
            "train(args)",
            "else:",
            "raise ValueError(\"Only chainer and pytorch are supported.\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=import_from_statement), node=ASTNode(type=dotted_name), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=lmpytorch), value='lm')",
            "Insert(target_node=ASTNode(type=dotted_name), node=('identifier', 'pytorch'), position=4, insert_id=178802)",
            "Insert(target_node=ASTNode(type=dotted_name), node=('.', '.'), position=5, insert_id=178803)",
            "Insert(target_node=ASTNode(type=import_from_statement), node=('dotted_name', None), position=3, insert_id=178804)",
            "Update(target_node=ASTNode(type=identifier, text=lmchainer), value='lm')",
            "Insert(target_node=ASTNode(type=dotted_name), node=('identifier', 'chainer'), position=4, insert_id=178805)",
            "Insert(target_node=ASTNode(type=dotted_name), node=('.', '.'), position=5, insert_id=178806)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'train'), position=0, insert_id=178807)",
            "Delete(target_node=ASTNode(type=identifier, text=train))",
            "Delete(target_node=ASTNode(type=dotted_name))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 9163,
        "neg_line": [
            "-from espnet.lmchainer.lm_chainer import train",
            "-from espnet.lmpytorch.lm_pytorch import train"
        ],
        "pos_line": [
            "+from espnet.lm.chainer.lm_chainer import train",
            "+from espnet.lm.pytorch.lm_pytorch import train"
        ],
        "core_change": "-from espnet.lmchainer.lm_chainer import train +from espnet.lm.chainer.lm_chainer import train -from espnet.lmpytorch.lm_pytorch import train +from espnet.lm.pytorch.lm_pytorch import train",
        "core_API": "info"
    },
    {
        "commit_hash": "cf84dacf2ed3d38b5d79070c23993a5265a4ee4b",
        "index": "7f6898a..ddf1fc3 100644",
        "commit_message": "fix get cuda device test error\n\nSummary:\nCuda test failing on circle with the error `random_ expects 'from' to be less than 'to', but got from=0 >= to=0`\n\nThis is because the `high` value in `torch.randint` is 1 more than the highest value in the distribution from which a value is drawn. So if there is only 1 cuda device available then the low and high are 0.\n\nReviewed By: gkioxari\n\nDifferential Revision: D21236669\n\nfbshipit-source-id: 46c312d431c474f1f2c50747b1d5e7afbd7df3a9\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_random_cuda_device() -> str:",
            "any device without having to set the device explicitly.",
            "\"\"\"",
            "num_devices = torch.cuda.device_count()",
            "-    rand_device_id = torch.randint(high=num_devices, size=(1,)).item()",
            "-    return \"cuda:%d\" % rand_device_id",
            "+    device_id = (",
            "+        torch.randint(high=num_devices, size=(1,)).item() if num_devices > 1 else 0",
            "+    )",
            "+    return \"cuda:%d\" % device_id",
            "",
            "",
            "class TestCaseMixin(unittest.TestCase):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=rand_device_id), value='device_id')"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 9165,
        "neg_line": [
            "-rand_device_id = torch.randint(high=num_devices, size=(1,)).item()",
            "-return \"cuda:%d\" % rand_device_id"
        ],
        "pos_line": [
            "+device_id = (",
            "+torch.randint(high=num_devices, size=(1,)).item() if num_devices > 1 else 0",
            "+)",
            "+return \"cuda:%d\" % device_id"
        ],
        "core_change": "-rand_device_id = torch.randint(high=num_devices, size=(1,)).item() -return \"cuda:%d\" % rand_device_id +device_id = ( +torch.randint(high=num_devices, size=(1,)).item() if num_devices > 1 else 0 +) +return \"cuda:%d\" % device_id",
        "core_API": "device_count"
    },
    {
        "commit_hash": "71417f70d9c1d2b29dcd42e885d7dd813413dabc",
        "index": "71cbfa97d..a41c459e9 100644",
        "commit_message": "fixed loading\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def torch_resume(snapshot_path, trainer):",
            "# retore optimizer states",
            "trainer.updater.get_optimizer('main').load_state_dict(snapshot_dict['optimizer'])",
            "",
            "-    return trainer",
            "+    # delete opened snapshot",
            "+    del snapshot_dict"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('delete_statement', None), position=2, insert_id=181741)",
            "Insert(target_node=IN(type=delete_statement), node=('del', 'del'), position=0, insert_id=181742)",
            "Update(target_node=ASTNode(type=identifier, text=trainer), value='snapshot_dict')",
            "Move(target_node=IN(type=delete_statement), node=ASTNode(type=identifier, text=trainer), position=1)",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 9171,
        "neg_line": [
            "-return trainer"
        ],
        "pos_line": [
            "+# delete opened snapshot",
            "+del snapshot_dict"
        ],
        "core_change": "-return trainer +# delete opened snapshot +del snapshot_dict",
        "core_API": "get_optimizer"
    },
    {
        "commit_hash": "157795d55000bd30d3cece3de9b5cf1f282c27be",
        "index": "6e36b97f..6bc787a9 100755",
        "commit_message": "fixed ornstein-uhlenbeck exploration variable; pass action_spec instead of action_shape to tf_exploration\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class OrnsteinUhlenbeckProcess(Exploration):",
            "",
            "super(OrnsteinUhlenbeckProcess, self).__init__(scope=scope, summary_labels=summary_labels)",
            "",
            "-    def tf_explore(self, episode, timestep, action_shape):",
            "-        normal_sample = tf.random_normal(shape=action_shape.shape, mean=0.0, stddev=1.0)",
            "+    def tf_explore(self, episode, timestep, action_spec):",
            "+        normal_sample = tf.random_normal(shape=action_spec['shape'], mean=0.0, stddev=1.0)",
            "state = tf.get_variable(",
            "name='ornstein_uhlenbeck',",
            "dtype=util.tf_dtype('float'),",
            "-            shape=action_shape.shape,",
            "+            shape=action_spec['shape'],",
            "initializer=tf.constant_initializer(self.mu)",
            ")",
            "return tf.assign_add(ref=state, value=(self.theta * (self.mu - state) + self.sigma * normal_sample))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=action_shape), value='action_spec')",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('subscript', None), position=2, insert_id=2235546)",
            "Update(target_node=ASTNode(type=identifier, text=action_shape), value='action_spec')",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=action_shape), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2235547)",
            "Insert(target_node=IN(type=subscript), node=('string', \"'shape'\"), position=2, insert_id=2235548)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2235549)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('subscript', None), position=2, insert_id=2235550)",
            "Update(target_node=ASTNode(type=identifier, text=action_shape), value='action_spec')",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=action_shape), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2235551)",
            "Insert(target_node=IN(type=subscript), node=('string', \"'shape'\"), position=2, insert_id=2235552)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2235553)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=shape))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=shape))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 19,
        "number": 9177,
        "neg_line": [
            "-def tf_explore(self, episode, timestep, action_shape):",
            "-normal_sample = tf.random_normal(shape=action_shape.shape, mean=0.0, stddev=1.0)",
            "-shape=action_shape.shape,"
        ],
        "pos_line": [
            "+def tf_explore(self, episode, timestep, action_spec):",
            "+normal_sample = tf.random_normal(shape=action_spec['shape'], mean=0.0, stddev=1.0)",
            "+shape=action_spec['shape'],"
        ],
        "core_change": "-def tf_explore(self, episode, timestep, action_shape): -normal_sample = tf.random_normal(shape=action_shape.shape, mean=0.0, stddev=1.0) +def tf_explore(self, episode, timestep, action_spec): +normal_sample = tf.random_normal(shape=action_spec['shape'], mean=0.0, stddev=1.0) -shape=action_shape.shape, +shape=action_spec['shape'],",
        "core_API": "random_normal"
    },
    {
        "commit_hash": "6f0be2641099da8268da63a9311eef3484124680",
        "index": "dbb22c25..9d2d8d71 100644",
        "commit_message": "Fix inference benchmark (#5341)\n\n* update\n\n* changelog\n\n* update\n\n* update\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BasicGNN(torch.nn.Module):",
            "pbar.set_description('Inference')",
            "",
            "x_all = loader.data.x.cpu()",
            "-        x_all = x_all.to(dtype)",
            "loader.data.n_id = torch.arange(x_all.size(0))",
            "",
            "for i in range(self.num_layers):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=x_all))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=x_all))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 9178,
        "neg_line": [
            "-x_all = x_all.to(dtype)"
        ],
        "pos_line": [],
        "core_change": "-x_all = x_all.to(dtype)",
        "core_API": "set_description"
    },
    {
        "commit_hash": "81350a12be769568a0f87c695ced10c12316e8aa",
        "index": "53db399..a6b71de 100644",
        "commit_message": "bug fix for predict on multi modal data (#1452)\n\n\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class AutoModel(object):",
            "pipeline = self.tuner.get_best_pipeline()",
            "model = self.tuner.get_best_model()",
            "dataset = pipeline.transform_x(dataset)",
            "+        dataset = tf.data.Dataset.zip((dataset, dataset))",
            "y = model.predict(dataset, **kwargs)",
            "y = utils.predict_with_adaptive_batch_size(",
            "model=model, batch_size=batch_size, x=dataset, **kwargs"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1886927)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1886928)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'dataset'), position=0, insert_id=1886929)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1886930)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1886931)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1886932)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1886933)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1886934)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1886935)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'zip'), position=2, insert_id=1886936)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1886937)",
            "Insert(target_node=IN(type=argument_list), node=('tuple', None), position=1, insert_id=1886938)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1886939)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1886940)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1886941)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Dataset'), position=2, insert_id=1886942)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=1886943)",
            "Insert(target_node=IN(type=tuple), node=('identifier', 'dataset'), position=1, insert_id=1886944)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=1886945)",
            "Insert(target_node=IN(type=tuple), node=('identifier', 'dataset'), position=3, insert_id=1886946)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=4, insert_id=1886947)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1886948)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1886949)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'data'), position=2, insert_id=1886950)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 24,
        "number": 9184,
        "neg_line": [],
        "pos_line": [
            "+dataset = tf.data.Dataset.zip((dataset, dataset))"
        ],
        "core_change": "+dataset = tf.data.Dataset.zip((dataset, dataset))",
        "core_API": "get_best_pipeline"
    },
    {
        "commit_hash": "45a1475462b00bd74428106259c36d627077b4cf",
        "index": "7d49bcb85..7b3f87621 100644",
        "commit_message": "Fix TF bad words filter with XLA (#18286)\n\n* Fix bad words filter in XLA generation\n\n* Remove my cool debug breakpoints (again)\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFNoBadWordsLogitsProcessor(TFLogitsProcessor):",
            "def _len_greater_than_cur_len():",
            "# Otherwise, if the bad sequence is longer than the current length they can't ever match",
            "return tf.cond(",
            "-                    tf.math.greater(self.bad_word_seqs_len[bad_word_seq_number], row_input_ids.shape[0]),",
            "+                    tf.math.greater(self.bad_word_seqs_len[bad_word_seq_number], tf.shape(row_input_ids)[0]),",
            "lambda: tf.zeros((), dtype=tf.bool),",
            "_match_found,",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=subscript), node=('call', None), position=0, insert_id=2362580)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2362581)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2362582)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2362583)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=row_input_ids), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2362584)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 9185,
        "neg_line": [
            "-tf.math.greater(self.bad_word_seqs_len[bad_word_seq_number], row_input_ids.shape[0]),"
        ],
        "pos_line": [
            "+tf.math.greater(self.bad_word_seqs_len[bad_word_seq_number], tf.shape(row_input_ids)[0]),"
        ],
        "core_change": "-tf.math.greater(self.bad_word_seqs_len[bad_word_seq_number], row_input_ids.shape[0]), +tf.math.greater(self.bad_word_seqs_len[bad_word_seq_number], tf.shape(row_input_ids)[0]),",
        "core_API": "cond"
    },
    {
        "commit_hash": "619f984c362a2e5fd8f04ae24ea0eb8ce9d9e57a",
        "index": "a193a92ff..973972d60 100644",
        "commit_message": "Option to provide seed to random generators to ensure reproducibility (#1572)\n\n* Option to provide seed to random generators to ensure reproducibility\n\nI added small function in utilities which imports torch, numpy, python\nrandom and sets seed for all of the libraries to ensure reproducibility\nof results.\n\n* Apply recommendations from core contributors on seeding\n\n1. Moved the seeding code to another file\n2. Make deterministic as a parameter for trainer class\n3. Add assertions for seeding numpy\n4. Added warnings\n5. torch.manual_seed should be enough for seeding torch\n\n* Revert \"Apply recommendations from core contributors on seeding\"\n\nThis reverts commit a213c8e6882eec8a9e7408b9418926d2db7c5461.\n\n* Revert \"Revert \"Apply recommendations from core contributors on seeding\"\"\n\nThis reverts commit 59b2da53c62878de7aab0aa3feb3115e105eea06.\n\n* Change in test, for correct seeding\n\n* Allow seed equal to 0\n\n* Allow seed to be uint32.max\n\n* Added deterministic to benchmarks\n\n* Cuda manual seed as in benchmark seeding\n\n* Seeding should be done before model initialization\n\n* cuda manual_seed is not necessary\n\n* Fixing seed test_cpu_lbfgs\n\nOn some seeds seems like lbfgs doesn't converge.\nSo I fixed the seed during testing.\n\n* rebasing issue with old reproducibility.py\n\n* Improved documentation and ability to seed before initializing Train\nclass\n\n* Change in docs\n\n* Removed seed from trainer, update for documentation\n\n* Typo in the docs\n\n* Added seed_everything to _all_\n\n* Fixing old changes\n\n* Model initialization should be earlier then Trainer\n\n* Update pytorch_lightning/trainer/__init__.py\n\nFrom Example to testcode\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* Fixing according to the contributors suggestions\n\n* Moving horovod deterministic to Trainer class\n\n* deterministic flag affects horovod docs update\n\n* Improved static typing\n\n* Added deterministic to test runners of horovod\n\nIt is failing on some versions, not very predictable\n\n* static seeds for horovod tests\n\n* Change for reset_seed function in tests\n\n* Seeding horovod using reset_seed from tutils\n\n* Update pytorch_lightning/trainer/__init__.py\n\n* chlog\n\n* Update trainer.py\n\n* change \"testcode\" to \"Example\" in trainer init documentation\n\n* Update pytorch_lightning/trainer/seed.py, first line in comment\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Jirka <jirka.borovec@seznam.cz>\nCo-authored-by: William Falcon <waf2107@columbia.edu>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def assert_ok_model_acc(trainer, key='test_acc', thr=0.5):",
            "",
            "def reset_seed():",
            "seed = RANDOM_SEEDS.pop()",
            "-    torch.manual_seed(seed)",
            "-    np.random.seed(seed)",
            "+    seed_everything(seed)",
            "",
            "",
            "def set_random_master_port():"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='seed_everything')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_seed))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=np))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=random))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=seed))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=seed))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 9188,
        "neg_line": [
            "-torch.manual_seed(seed)",
            "-np.random.seed(seed)"
        ],
        "pos_line": [
            "+seed_everything(seed)"
        ],
        "core_change": "-torch.manual_seed(seed) -np.random.seed(seed) +seed_everything(seed)",
        "core_API": "pop"
    },
    {
        "commit_hash": "b0332551dd8832a866d171229dcaa9a92998e2c7",
        "index": "362f93b072..d613c64a7f 100644",
        "commit_message": "[rllib] Fix APPO + continuous spaces, feed prev_rew/act to A3C properly (#4286)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class AsyncPPOPolicyGraph(LearningRateSchedule, TFPolicyGraph):",
            "existing_state_in = existing_inputs[9:-1]",
            "existing_seq_lens = existing_inputs[-1]",
            "else:",
            "-            actions = tf.placeholder(tf.int64, actions_shape, name=\"ac\")",
            "+            actions = ModelCatalog.get_action_placeholder(action_space)",
            "dones = tf.placeholder(tf.bool, [None], name=\"dones\")",
            "rewards = tf.placeholder(tf.float32, [None], name=\"rewards\")",
            "behaviour_logits = tf.placeholder("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='ModelCatalog')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=placeholder), value='get_action_placeholder')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='action_space')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=tf), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=int64))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=actions_shape))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=name))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"ac\"))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 9189,
        "neg_line": [
            "-actions = tf.placeholder(tf.int64, actions_shape, name=\"ac\")"
        ],
        "pos_line": [
            "+actions = ModelCatalog.get_action_placeholder(action_space)"
        ],
        "core_change": "-actions = tf.placeholder(tf.int64, actions_shape, name=\"ac\") +actions = ModelCatalog.get_action_placeholder(action_space)",
        "core_API": "placeholder"
    },
    {
        "commit_hash": "ce8f42b12ecd522e3ef7251e0d5f5175075a3fb4",
        "index": "03f9d62d..3495b841 100644",
        "commit_message": "Fixes to enable dev branch to move to pytorch-1.2 (#2001)\n\n* Fixes to enable dev branch to move to pytorch-1.2\n\n* change name for published cpu wheels\n\n* fix failing unit tests\n\n* change docs conf\n\n* change tracking example\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_mask_invalid_shape(batch_shape, mask_shape):",
            "",
            "",
            "def test_kl_divergence():",
            "-    mask = torch.tensor([[0, 1], [1, 1]]).byte()",
            "+    mask = torch.tensor([[0, 1], [1, 1]]).bool()",
            "p = Normal(torch.randn(2, 2), torch.randn(2, 2).exp())",
            "q = Normal(torch.randn(2, 2), torch.randn(2, 2).exp())",
            "expected = kl_divergence(p.to_event(2), q.to_event(2))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=byte), value='bool')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9190,
        "neg_line": [
            "-mask = torch.tensor([[0, 1], [1, 1]]).byte()"
        ],
        "pos_line": [
            "+mask = torch.tensor([[0, 1], [1, 1]]).bool()"
        ],
        "core_change": "-mask = torch.tensor([[0, 1], [1, 1]]).byte() +mask = torch.tensor([[0, 1], [1, 1]]).bool()",
        "core_API": "tensor"
    },
    {
        "commit_hash": "3878d6719055b07d888bcf9b3d67d4547f0eb340",
        "index": "21a8181b..fee52093 100644",
        "commit_message": "fix average emb inp for py3 / update imdb fast text example for TF 1.1\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class FastTextClassifier(object):",
            "self.prediction_probs = tf.nn.softmax(self.network.outputs)",
            "self.predictions = tf.argmax(",
            "self.network.outputs, axis=1, output_type=tf.int32)",
            "+        # self.predictions = tf.cast(tf.argmax(             # for TF < 1.2",
            "+        #     self.network.outputs, axis=1), tf.int32)",
            "",
            "# Evaluation",
            "are_predictions_correct = tf.equal(self.predictions, self.labels)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 9191,
        "neg_line": [],
        "pos_line": [
            "+# self.predictions = tf.cast(tf.argmax(             # for TF < 1.2",
            "+#     self.network.outputs, axis=1), tf.int32)"
        ],
        "core_change": "+# self.predictions = tf.cast(tf.argmax(             # for TF < 1.2 +#     self.network.outputs, axis=1), tf.int32)",
        "core_API": "softmax"
    },
    {
        "commit_hash": "8b6be6084a0c38800ac5175eab4f64485eb63d55",
        "index": "b520203..423ffb7 100644",
        "commit_message": "Fix variable naming\n\n",
        "file": "seq2seq.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Seq2SeqModel(ModelBase):",
            "if \"embedding\" in variable.name:",
            "tmp = tf.clip_by_norm(",
            "gradient.values, self.params[\"optimizer.clip_embed_gradients\"])",
            "-        grad = tf.IndexedSlices(tmp, gradient.indices, gradient.dense_shape)",
            "+        gradient = tf.IndexedSlices(tmp, gradient.indices, gradient.dense_shape)",
            "clipped_gradients.append(gradient)",
            "variables.append(variable)",
            "return list(zip(clipped_gradients, variables))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=grad), value='gradient')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9197,
        "neg_line": [
            "-grad = tf.IndexedSlices(tmp, gradient.indices, gradient.dense_shape)"
        ],
        "pos_line": [
            "+gradient = tf.IndexedSlices(tmp, gradient.indices, gradient.dense_shape)"
        ],
        "core_change": "-grad = tf.IndexedSlices(tmp, gradient.indices, gradient.dense_shape) +gradient = tf.IndexedSlices(tmp, gradient.indices, gradient.dense_shape)",
        "core_API": "clip_by_norm"
    },
    {
        "commit_hash": "099b04e37abd98a3fcc0609304f95e57465a4d62",
        "index": "a6f6edaf..44ae6749 100644",
        "commit_message": "[PyTorch][TF] Fix L2Loss 1/2 factor minibatch-sgd (#1987)\n\n\n",
        "file": "d2l-en.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def train_concise_ch11(trainer_fn, hyperparams, data_iter, num_epochs=4):",
            "optimizer.zero_grad()",
            "out = net(X)",
            "y = y.reshape(out.shape)",
            "-            l = loss(out, y)/2",
            "-            l.backward()",
            "+            l = loss(out, y)",
            "+            l.mean().backward()",
            "optimizer.step()",
            "n += X.shape[0]",
            "if n % 200 == 0:",
            "timer.stop()",
            "animator.add(n/X.shape[0]/len(data_iter),",
            "-                             (d2l.evaluate_loss(net, data_iter, loss)/2,))",
            "+                             (d2l.evaluate_loss(net, data_iter, loss),))",
            "timer.start()",
            "print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1321065)",
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=1321066)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1321067)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1321068)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1321069)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1321070)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=call), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=l), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'mean'), position=2, insert_id=1321071)",
            "Delete(target_node=ASTNode(type=/, text=/))",
            "Delete(target_node=ASTNode(type=integer, text=2))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=/, text=/))",
            "Delete(target_node=ASTNode(type=integer, text=2))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 9199,
        "neg_line": [
            "-l = loss(out, y)/2",
            "-l.backward()",
            "-(d2l.evaluate_loss(net, data_iter, loss)/2,))"
        ],
        "pos_line": [
            "+l = loss(out, y)",
            "+l.mean().backward()",
            "+(d2l.evaluate_loss(net, data_iter, loss),))"
        ],
        "core_change": "-l = loss(out, y)/2 -l.backward() +l = loss(out, y) +l.mean().backward() -(d2l.evaluate_loss(net, data_iter, loss)/2,)) +(d2l.evaluate_loss(net, data_iter, loss),))",
        "core_API": "zero_grad"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "45132d96..20e4e0e3 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SequenceLabelField(Field[torch.Tensor]):",
            "@overrides",
            "def as_tensor(self,",
            "padding_lengths: Dict[str, int],",
            "-                  cuda_device: int = -1,",
            "-                  for_training: bool = True) -> torch.Tensor:",
            "+                  cuda_device: int = -1) -> torch.Tensor:",
            "desired_num_tokens = padding_lengths['num_tokens']",
            "padded_tags = pad_sequence_to_length(self._indexed_labels, desired_num_tokens)",
            "-        tensor = Variable(torch.LongTensor(padded_tags), volatile=not for_training)",
            "+        tensor = torch.LongTensor(padded_tags)",
            "return tensor if cuda_device == -1 else tensor.cuda(cuda_device)",
            "",
            "@overrides"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=for_training))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=bool))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=typed_default_parameter))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=volatile))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=not, text=not))",
            "Delete(target_node=ASTNode(type=identifier, text=for_training))",
            "Delete(target_node=ASTNode(type=not_operator))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 9204,
        "neg_line": [
            "-cuda_device: int = -1,",
            "-for_training: bool = True) -> torch.Tensor:",
            "-tensor = Variable(torch.LongTensor(padded_tags), volatile=not for_training)"
        ],
        "pos_line": [
            "+cuda_device: int = -1) -> torch.Tensor:",
            "+tensor = torch.LongTensor(padded_tags)"
        ],
        "core_change": "-cuda_device: int = -1, -for_training: bool = True) -> torch.Tensor: +cuda_device: int = -1) -> torch.Tensor: -tensor = Variable(torch.LongTensor(padded_tags), volatile=not for_training) +tensor = torch.LongTensor(padded_tags)",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "01e5b1e06c683dd0eb2dc8c0d90c9749b90b7324",
        "index": "01d83ddc4e..7b31bcc279 100644",
        "commit_message": "fixed lint errors.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def multinomial(",
            ")",
            "/ population_size",
            ")",
            "-    return torch.multinomial(probs.float(),",
            "-                             num_samples,",
            "-                             replace).to(default_device(device))",
            "+    return torch.multinomial(probs.float(), num_samples, replace).to(",
            "+        default_device(device)",
            "+    )",
            "",
            "",
            "def randint("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 9216,
        "neg_line": [
            "-return torch.multinomial(probs.float(),",
            "-num_samples,",
            "-replace).to(default_device(device))"
        ],
        "pos_line": [
            "+return torch.multinomial(probs.float(), num_samples, replace).to(",
            "+default_device(device)",
            "+)"
        ],
        "core_change": "-return torch.multinomial(probs.float(), -num_samples, -replace).to(default_device(device)) +return torch.multinomial(probs.float(), num_samples, replace).to( +default_device(device) +)",
        "core_API": "multinomial"
    },
    {
        "commit_hash": "2eb5a2d31f40b4a466eb6666e284de4255858d54",
        "index": "7c140b41f1..6a6fa13982 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "matrix_rank.support_native_out = True",
            "",
            "",
            "def matrix_transpose(",
            "-    x: torch.Tensor,",
            "-    /,",
            "-    *,",
            "-    conjugate: bool = False,",
            "-    out: Optional[torch.Tensor] = None",
            "+    x: torch.Tensor, /, *, conjugate: bool = False, out: Optional[torch.Tensor] = None",
            ") -> torch.Tensor:",
            "if conjugate:",
            "torch.conj(x)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 5,
        "AST_diff_line": 17,
        "number": 9221,
        "neg_line": [
            "-x: torch.Tensor,",
            "-/,",
            "-*,",
            "-conjugate: bool = False,",
            "-out: Optional[torch.Tensor] = None"
        ],
        "pos_line": [
            "+x: torch.Tensor, /, *, conjugate: bool = False, out: Optional[torch.Tensor] = None"
        ],
        "core_change": "-x: torch.Tensor, -/, -*, -conjugate: bool = False, -out: Optional[torch.Tensor] = None +x: torch.Tensor, /, *, conjugate: bool = False, out: Optional[torch.Tensor] = None",
        "core_API": "conj"
    },
    {
        "commit_hash": "a0ed4151c036d9d860e1ce8b9f5eee4f04092c4b",
        "index": "882f72d..57524f4 100644",
        "commit_message": "[transformer] Format & Test Refactoring (#1325)\n\n* try PyTorch custom TestCase class\n\n* revert\n\n* initial working example\n\n* update\n\n* data utils\n\n* fix imports\n\n* hardcode backend to nccl\n\n* fix signature\n\n* fix typo\n\n* mapping\n\n* set device\n\n* init\n\n* refactor x entropy\n\n* remove unused import & destroy model parallel\n\n* refactor random\n\n* fix test\n\n* remove migrated tests\n\n* refactor\n\n* init\n\n* separate affine weight init\n\n* init model parallel\n\n* split more\n\n* weight init fix part 1\n\n* use cpu init for consistency btwn native and tensor parallel\n\n* black\n\n* add col parallel\n\n* use a 3D tensor of square matrix for column parallel linear\n\n* skip the failing cases\n\n* migrate layers test\n\n* pipeline parallel forward/backward\n\n* fix typo\n\n* fix typo\n\n* fix\n\n* fix pipeline world size\n\n* black\n\n* rm `run_pipeline_parallel_test` in favor of test_pipeline_parallel_fwd_bwd.py\n\n* stop logging\n\n* set log level\n\n* black\n\n* license and format\n\n* fix\n\n* skip tf32 as matrices are small\n\n* remove potentially inappropriate license\n\n* Apply suggestions from code review\n\n* remove `TODO` comment\n\n* `torch.testing.assert_allclose` -> `torch.testing.assert_close`\n\n* remove comment-outs\n\n* remote unused import\n\n* minor fix\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _gather(input_):",
            "",
            "tensor_list = [torch.empty_like(input_) for _ in range(world_size)]",
            "tensor_list[rank] = input_",
            "-    torch.distributed.all_gather(tensor_list, input_, group=get_tensor_model_parallel_group())",
            "+    torch.distributed.all_gather(",
            "+        tensor_list, input_, group=get_tensor_model_parallel_group()",
            "+    )",
            "",
            "# Note: torch.cat already creates a contiguous tensor.",
            "output = torch.cat(tensor_list, dim=last_dim).contiguous()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 9223,
        "neg_line": [
            "-torch.distributed.all_gather(tensor_list, input_, group=get_tensor_model_parallel_group())"
        ],
        "pos_line": [
            "+torch.distributed.all_gather(",
            "+tensor_list, input_, group=get_tensor_model_parallel_group()",
            "+)"
        ],
        "core_change": "-torch.distributed.all_gather(tensor_list, input_, group=get_tensor_model_parallel_group()) +torch.distributed.all_gather( +tensor_list, input_, group=get_tensor_model_parallel_group() +)",
        "core_API": "empty_like"
    },
    {
        "commit_hash": "7282f554886b1885357e7e0e73d2b4000c60642b",
        "index": "79eb9ac..e05fcbb 100644",
        "commit_message": "Fix weight init in FastRCNNPredictor (#60)\n\n\n",
        "file": "maskrcnn-benchmark.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class FastRCNNPredictor(nn.Module):",
            "self.bbox_pred = nn.Linear(num_inputs, num_classes * 4)",
            "",
            "nn.init.normal_(self.cls_score.weight, mean=0, std=0.01)",
            "-        nn.init.constant_(self.cls_score.weight, 0)",
            "+        nn.init.constant_(self.cls_score.bias, 0)",
            "",
            "nn.init.normal_(self.bbox_pred.weight, mean=0, std=0.001)",
            "-        nn.init.constant_(self.bbox_pred.weight, 0)",
            "+        nn.init.constant_(self.bbox_pred.bias, 0)",
            "",
            "def forward(self, x):",
            "x = self.avgpool(x)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1421204)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1421205)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1421206)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'constant_'), position=2, insert_id=1421207)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1421208)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1421209)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'init'), position=2, insert_id=1421210)",
            "Update(target_node=ASTNode(type=identifier, text=weight), value='bias')",
            "Update(target_node=ASTNode(type=identifier, text=weight), value='bias')",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=init))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=constant_))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 9224,
        "neg_line": [
            "-nn.init.constant_(self.cls_score.weight, 0)",
            "-nn.init.constant_(self.bbox_pred.weight, 0)"
        ],
        "pos_line": [
            "+nn.init.constant_(self.cls_score.bias, 0)",
            "+nn.init.constant_(self.bbox_pred.bias, 0)"
        ],
        "core_change": "-nn.init.constant_(self.cls_score.weight, 0) +nn.init.constant_(self.cls_score.bias, 0) -nn.init.constant_(self.bbox_pred.weight, 0) +nn.init.constant_(self.bbox_pred.bias, 0)",
        "core_API": "Linear"
    },
    {
        "commit_hash": "dcc1129afa3470e81169b1a28a8816d4b1d2c44c",
        "index": "d3d3381952..d89d7d981c 100644",
        "commit_message": "small bug fix for pytorch stop_gradient method, which was showing new errors after version upgrade.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def adam_update(ws, dcdws, lr, mw, vw, step, beta1=0.9, beta2=0.999, epsilon=1e-",
            "",
            "",
            "def stop_gradient(x):",
            "-    return x.detach()",
            "+    # ToDo: work out why _torch.tensor() wrapping is necessary in certain cases, presumably .detach() should be enough.",
            "+    return _torch.tensor(x.detach())"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=381885)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=381886)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_torch'), position=0, insert_id=381887)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=381888)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=381889)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=381890)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=381891)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 9225,
        "neg_line": [
            "-return x.detach()"
        ],
        "pos_line": [
            "+# ToDo: work out why _torch.tensor() wrapping is necessary in certain cases, presumably .detach() should be enough.",
            "+return _torch.tensor(x.detach())"
        ],
        "core_change": "-return x.detach() +# ToDo: work out why _torch.tensor() wrapping is necessary in certain cases, presumably .detach() should be enough. +return _torch.tensor(x.detach())",
        "core_API": "detach"
    },
    {
        "commit_hash": "a0c603355bb7daab9c9738753dfeeb322c4d3ae4",
        "index": "405ae929..6e36b97f 100755",
        "commit_message": "fixed continuous action explorations\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class OrnsteinUhlenbeckProcess(Exploration):",
            "",
            "def tf_explore(self, episode, timestep, action_shape):",
            "normal_sample = tf.random_normal(shape=action_shape.shape, mean=0.0, stddev=1.0)",
            "-        state = tf.get_variable(name='ornstein_uhlenbeck', dtype=util.tf_dtype('float'), shape=action_shape.shape,",
            "-                                initializer=tf.constant_initializer(self.mu))",
            "+        state = tf.get_variable(",
            "+            name='ornstein_uhlenbeck',",
            "+            dtype=util.tf_dtype('float'),",
            "+            shape=action_shape.shape,",
            "+            initializer=tf.constant_initializer(self.mu)",
            "+        )",
            "return tf.assign_add(ref=state, value=(self.theta * (self.mu - state) + self.sigma * normal_sample))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 6,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 9226,
        "neg_line": [
            "-state = tf.get_variable(name='ornstein_uhlenbeck', dtype=util.tf_dtype('float'), shape=action_shape.shape,",
            "-initializer=tf.constant_initializer(self.mu))"
        ],
        "pos_line": [
            "+state = tf.get_variable(",
            "+name='ornstein_uhlenbeck',",
            "+dtype=util.tf_dtype('float'),",
            "+shape=action_shape.shape,",
            "+initializer=tf.constant_initializer(self.mu)",
            "+)"
        ],
        "core_change": "-state = tf.get_variable(name='ornstein_uhlenbeck', dtype=util.tf_dtype('float'), shape=action_shape.shape, -initializer=tf.constant_initializer(self.mu)) +state = tf.get_variable( +name='ornstein_uhlenbeck', +dtype=util.tf_dtype('float'), +shape=action_shape.shape, +initializer=tf.constant_initializer(self.mu) +)",
        "core_API": "random_normal"
    },
    {
        "commit_hash": "990f1e51df9707b5886d3fcd4a9e92692515eac0",
        "index": "4ed6c0e403..e95516611b 100644",
        "commit_message": "small fix to median torch backend\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def median(",
            "keepdims: Optional[bool] = False,",
            "out: Optional[torch.tensor] = None,",
            ") -> torch.tensor:",
            "+    temp = input",
            "if hasattr(axis, \"__iter__\"):",
            "for dim in axis:",
            "-            input = torch.median(",
            "-                input,",
            "+            temp = torch.median(",
            "+                temp,",
            "dim=dim,",
            "keepdim=keepdims,",
            ")[0]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('block', None), position=6, insert_id=265930)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=265931)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=265932)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'temp'), position=0, insert_id=265933)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=265934)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'input'), position=2, insert_id=265935)",
            "Update(target_node=ASTNode(type=identifier, text=input), value='temp')",
            "Update(target_node=ASTNode(type=identifier, text=input), value='temp')",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 9236,
        "neg_line": [
            "-input = torch.median(",
            "-input,"
        ],
        "pos_line": [
            "+temp = input",
            "+temp = torch.median(",
            "+temp,"
        ],
        "core_change": "+temp = input -input = torch.median( -input, +temp = torch.median( +temp,",
        "core_API": "median"
    },
    {
        "commit_hash": "02b176c4ce14340d26d42825523f406959c6c202",
        "index": "29a12e7df..a3e11eb60 100644",
        "commit_message": "Fix torch version comparisons (#18460)\n\nComparisons like\nversion.parse(torch.__version__) > version.parse(\"1.6\")\nare True for torch==1.6.0+cu101 or torch==1.6.0+cpu\n\nversion.parse(version.parse(torch.__version__).base_version) are preferred (and available in pytorch_utils.py\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Pipeline(_ScikitCompat):",
            "",
            "def get_inference_context(self):",
            "inference_context = (",
            "-            torch.inference_mode if version.parse(torch.__version__) >= version.parse(\"1.9.0\") else torch.no_grad",
            "+            torch.inference_mode",
            "+            if version.parse(version.parse(torch.__version__).base_version) >= version.parse(\"1.9.0\")",
            "+            else torch.no_grad",
            ")",
            "return inference_context"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=1193594)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1193595)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'base_version'), position=2, insert_id=1193596)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1193597)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1193598)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'version'), position=0, insert_id=1193599)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1193600)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'parse'), position=2, insert_id=1193601)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1193602)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1193603)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 9240,
        "neg_line": [
            "-torch.inference_mode if version.parse(torch.__version__) >= version.parse(\"1.9.0\") else torch.no_grad"
        ],
        "pos_line": [
            "+torch.inference_mode",
            "+if version.parse(version.parse(torch.__version__).base_version) >= version.parse(\"1.9.0\")",
            "+else torch.no_grad"
        ],
        "core_change": "-torch.inference_mode if version.parse(torch.__version__) >= version.parse(\"1.9.0\") else torch.no_grad +torch.inference_mode +if version.parse(version.parse(torch.__version__).base_version) >= version.parse(\"1.9.0\") +else torch.no_grad",
        "core_API": "parse"
    },
    {
        "commit_hash": "1d9c57dd21f22ac1343ad360e49293ffd2ee867b",
        "index": "a25b7289bb..fb5f1c5a44 100644",
        "commit_message": "small fix to torch argsort\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def argsort(",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "if out is not None:",
            "-        out = tuple([torch.zeros(out.shape, dtype=out.dtype), out])",
            "+        out = tuple([torch.zeros(x.shape, dtype=x.dtype), out.long()])",
            "_, sorted_indices = torch.sort(",
            "x, dim=axis, descending=descending, stable=stable, out=out",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('call', None), position=3, insert_id=326956)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=326957)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=326958)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=out), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=326959)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'long'), position=2, insert_id=326960)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=326961)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=326962)",
            "Update(target_node=ASTNode(type=identifier, text=out), value='x')",
            "Update(target_node=ASTNode(type=identifier, text=out), value='x')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 9241,
        "neg_line": [
            "-out = tuple([torch.zeros(out.shape, dtype=out.dtype), out])"
        ],
        "pos_line": [
            "+out = tuple([torch.zeros(x.shape, dtype=x.dtype), out.long()])"
        ],
        "core_change": "-out = tuple([torch.zeros(out.shape, dtype=out.dtype), out]) +out = tuple([torch.zeros(x.shape, dtype=x.dtype), out.long()])",
        "core_API": "zeros"
    },
    {
        "commit_hash": "98291f0d33b704d1e4d14197278f2996ac8f4b4a",
        "index": "3ab4fb3f..68be2f77 100644",
        "commit_message": "[WIP] - Documentation Cleaning and Coding Style (#516)\n\n* Activation Cleaning Docstring Test\n\n* Requirements Pinned with range to insure tested versions are used. Range are used to prevent updating requirements all the time.\n\n* setup.cfg file added with PEP8 configuration\n\n* activation.py refactored\n\n* docstring fixed - ready for documentation unittest\n\n* Yapf correction for max_line_length: 120\n\n* test yapf refactored\n\n* Requirements conflict solved\n\n* Yapf Style modified and merged in file \"setup.cfg\"\n\n* Yapf Confiuguration Updated\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* tl.layers.pooling YAPF reformat\n\n* yapf updated\n\n* gitignore updated\n\n* YAPF Style Fixing Attempt\n\n* Space Error Fix\n\n* Style Correction\n\n* Assertion Codacy Errors Corrected\n\n* Error Fix\n\n* Assertion Refactored\n\n* YAPF Style Applied to Master\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SpatialTransformer2dAffineLayer(Layer):",
            "if out_size is None:",
            "out_size = [40, 40]",
            "",
            "-        logging.info(\"SpatialTransformer2dAffineLayer %s: in_size:%s out_size:%s\" % (name, self.inputs.get_shape().as_list(), out_size))",
            "+        logging.info(",
            "+            \"SpatialTransformer2dAffineLayer %s: in_size:%s out_size:%s\" %",
            "+            (name, self.inputs.get_shape().as_list(), out_size)",
            "+        )",
            "",
            "with tf.variable_scope(name) as vs:",
            "# 1. make the localisation network to [batch, 6] via Flatten and Dense."
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 9243,
        "neg_line": [
            "-logging.info(\"SpatialTransformer2dAffineLayer %s: in_size:%s out_size:%s\" % (name, self.inputs.get_shape().as_list(), out_size))"
        ],
        "pos_line": [
            "+logging.info(",
            "+\"SpatialTransformer2dAffineLayer %s: in_size:%s out_size:%s\" %",
            "+(name, self.inputs.get_shape().as_list(), out_size)",
            "+)"
        ],
        "core_change": "-logging.info(\"SpatialTransformer2dAffineLayer %s: in_size:%s out_size:%s\" % (name, self.inputs.get_shape().as_list(), out_size)) +logging.info( +\"SpatialTransformer2dAffineLayer %s: in_size:%s out_size:%s\" % +(name, self.inputs.get_shape().as_list(), out_size) +)",
        "core_API": "info"
    },
    {
        "commit_hash": "74737373d443bae676c7c872f7724f2a000b7beb",
        "index": "5746a8c3..5ff7c967 100644",
        "commit_message": "add diff command (#5109)\n\n* add diff command\n\n* fix docs\n\n* no silly geese\n\n* update CHANGELOG\n\n* move 'load_state_dict' to nn.util\n\n* normalize by size\n\n* handle different checkpoint types\n\n* add integration tests\n\n* add 'scale' and 'threshold' params\n\n* HuggingFace Hub support\n\n* support '_/' as well, add test\n\n* revert some changes\n\n* fix\n\n* Update CHANGELOG.md\n\n* Update codecov.yml\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Model(torch.nn.Module, Registrable):",
            "",
            "# Load state dict. We pass `strict=False` so PyTorch doesn't raise a RuntimeError",
            "# if the state dict is missing keys because we handle this case below.",
            "-        model_state = torch.load(weights_file, map_location=util.device_mapping(cuda_device))",
            "+        model_state = util.load_state_dict(weights_file, cuda_device=cuda_device)",
            "missing_keys, unexpected_keys = model.load_state_dict(model_state, strict=False)",
            "",
            "# Modules might define a class variable called `authorized_missing_keys`,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=util), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=load), value='load_state_dict')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Update(target_node=ASTNode(type=identifier, text=map_location), value='cuda_device')",
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=identifier, text=cuda_device), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device_mapping))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 9245,
        "neg_line": [
            "-model_state = torch.load(weights_file, map_location=util.device_mapping(cuda_device))"
        ],
        "pos_line": [
            "+model_state = util.load_state_dict(weights_file, cuda_device=cuda_device)"
        ],
        "core_change": "-model_state = torch.load(weights_file, map_location=util.device_mapping(cuda_device)) +model_state = util.load_state_dict(weights_file, cuda_device=cuda_device)",
        "core_API": "load"
    },
    {
        "commit_hash": "d87f5d88cf187bae24aa18e07ac5aad5660bba93",
        "index": "de2d46d1..d10e3b19 100644",
        "commit_message": "fixed dropout, changed parameter dtype handling\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Dropout(Layer):",
            "",
            "skip_dropout = tf.math.logical_not(x=Module.retrieve_tensor(name='update'))",
            "zero = tf.constant(value=0.0, dtype=util.tf_dtype(dtype='float'))",
            "-        skip_dropout = tf.math.logical_or(x=apply_dropout, y=tf.math.equal(x=rate, y=zero))",
            "+        skip_dropout = tf.math.logical_or(x=skip_dropout, y=tf.math.equal(x=rate, y=zero))",
            "return self.cond(pred=skip_dropout, true_fn=no_dropout, false_fn=apply_dropout)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=apply_dropout), value='skip_dropout')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9246,
        "neg_line": [
            "-skip_dropout = tf.math.logical_or(x=apply_dropout, y=tf.math.equal(x=rate, y=zero))"
        ],
        "pos_line": [
            "+skip_dropout = tf.math.logical_or(x=skip_dropout, y=tf.math.equal(x=rate, y=zero))"
        ],
        "core_change": "-skip_dropout = tf.math.logical_or(x=apply_dropout, y=tf.math.equal(x=rate, y=zero)) +skip_dropout = tf.math.logical_or(x=skip_dropout, y=tf.math.equal(x=rate, y=zero))",
        "core_API": "logical_not"
    },
    {
        "commit_hash": "edcd5d32cf6af90d83e686935fadb965fe2a7481",
        "index": "dff3da8f5..f0c36c806 100644",
        "commit_message": "fixes\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_dnn_beamformer(use_frontend, use_beamformer, bnmask, num_spkrs, m_str):",
            "ws2 = b.get_mvdr_vector(psd_speech2, psd_speech1 + psd_noise, u2)",
            "",
            "enhanced1 = b.apply_beamforming_vector(ws1, feat).transpose(-1, -2)",
            "-    enhanced2 = b.apply_beamforming_vector(ws1, feat).transpose(-1, -2)",
            "+    enhanced2 = b.apply_beamforming_vector(ws2, feat).transpose(-1, -2)",
            "",
            "assert torch.equal(enhanced1.real, enhanced[0].real)",
            "assert torch.equal(enhanced2.real, enhanced[1].real)",
            "assert torch.equal(enhanced1.imag, enhanced[0].imag)",
            "-    assert torch.equal(enhanced2.imag, enhanced[1].imag)",
            "\\ No newline at end of file",
            "+    assert torch.equal(enhanced2.imag, enhanced[1].imag)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assert_statement), node=('ERROR', None), position=1, insert_id=167194)",
            "Insert(target_node=ASTNode(type=assert_statement), node=('identifier', 'file'), position=2, insert_id=167195)",
            "Move(target_node=ASTNode(type=assert_statement), node=ASTNode(type=call), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=ERROR), node=('ERROR', ' '), position=1, insert_id=167196)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'No'), position=2, insert_id=167197)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'newline'), position=3, insert_id=167198)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'at'), position=4, insert_id=167199)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'end'), position=5, insert_id=167200)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'of'), position=6, insert_id=167201)",
            "Update(target_node=ASTNode(type=identifier, text=ws1), value='ws2')",
            "Delete(target_node=ASTNode(type=ERROR, text= ))",
            "Delete(target_node=ASTNode(type=identifier, text=No))",
            "Delete(target_node=ASTNode(type=identifier, text=newline))",
            "Delete(target_node=ASTNode(type=identifier, text=at))",
            "Delete(target_node=ASTNode(type=identifier, text=end))",
            "Delete(target_node=ASTNode(type=identifier, text=of))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=file))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 9255,
        "neg_line": [
            "-enhanced2 = b.apply_beamforming_vector(ws1, feat).transpose(-1, -2)",
            "-assert torch.equal(enhanced2.imag, enhanced[1].imag)"
        ],
        "pos_line": [
            "+enhanced2 = b.apply_beamforming_vector(ws2, feat).transpose(-1, -2)",
            "+assert torch.equal(enhanced2.imag, enhanced[1].imag)"
        ],
        "core_change": "-enhanced2 = b.apply_beamforming_vector(ws1, feat).transpose(-1, -2) +enhanced2 = b.apply_beamforming_vector(ws2, feat).transpose(-1, -2) -assert torch.equal(enhanced2.imag, enhanced[1].imag) +assert torch.equal(enhanced2.imag, enhanced[1].imag)",
        "core_API": "get_mvdr_vector"
    },
    {
        "commit_hash": "ce8f42b12ecd522e3ef7251e0d5f5175075a3fb4",
        "index": "f2d6c1c5..54de0547 100644",
        "commit_message": "Fixes to enable dev branch to move to pytorch-1.2 (#2001)\n\n* Fixes to enable dev branch to move to pytorch-1.2\n\n* change name for published cpu wheels\n\n* fix failing unit tests\n\n* change docs conf\n\n* change tracking example\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _sample_tree_approx(edge_logits):",
            "# the complete graph. The id of an edge (v1,v2) is k = v1+v2*(v2-1)/2.",
            "edge_ids = torch.empty((E,), dtype=torch.long)",
            "# This maps each vertex to whether it is a member of the cumulative tree.",
            "-    components = torch.zeros(V, dtype=torch.uint8)",
            "+    components = torch.zeros(V, dtype=torch.bool)",
            "",
            "# Sample the first edge at random.",
            "probs = (edge_logits - edge_logits.max()).exp()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=uint8), value='bool')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9256,
        "neg_line": [
            "-components = torch.zeros(V, dtype=torch.uint8)"
        ],
        "pos_line": [
            "+components = torch.zeros(V, dtype=torch.bool)"
        ],
        "core_change": "-components = torch.zeros(V, dtype=torch.uint8) +components = torch.zeros(V, dtype=torch.bool)",
        "core_API": "empty"
    },
    {
        "commit_hash": "3392215ef69b483bff82bf7b0270c2349b33a73c",
        "index": "be666f3aa..d03986631 100644",
        "commit_message": "Fix broken `test_cpu_amp_precision_context_manager` (#9809)\n\n* @RunIf(min_gpus=1)\n\n* dtype -> fast_dtype\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_cpu_amp_precision_context_manager(tmpdir):",
            "assert not hasattr(plugin, \"scaler\")",
            "context_manager = plugin.autocast_context_manager()",
            "assert isinstance(context_manager, torch.cpu.amp.autocast)",
            "-    assert context_manager.dtype == torch.bfloat16",
            "+    assert context_manager.fast_dtype == torch.bfloat16",
            "",
            "",
            "@pytest.mark.skipif(not _TORCH_CPU_AMP_AVAILABLE, reason=\"Torch CPU AMP is not available.\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=dtype), value='fast_dtype')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9260,
        "neg_line": [
            "-assert context_manager.dtype == torch.bfloat16"
        ],
        "pos_line": [
            "+assert context_manager.fast_dtype == torch.bfloat16"
        ],
        "core_change": "-assert context_manager.dtype == torch.bfloat16 +assert context_manager.fast_dtype == torch.bfloat16",
        "core_API": "autocast_context_manager"
    },
    {
        "commit_hash": "23efadd9a394d9d69e10e8539a7332665b118df0",
        "index": "8d348f2b..44f02ae7 100644",
        "commit_message": "upgrade to pytorch 1.2 (#3182)\n\n* first attempt at pytorch 1.2\n\n* explicit is better than implicit\n\n* more explicit\n\n* attempt to fix flaky tests\n\n* pylint\n\n* no disable dropout\n\n* disable dropout by default\n\n* restore dropout, don't deepcopy\n\n* change batch size for biaffine_dependency_parser_multilang_test, maybe that will make it pass? :(\n\n* try batch size 10\n\n* ignore bad gradient parameter\n\n* cleanup\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LanguageModel(Model):",
            "mask: torch.Tensor,",
            "direction: int) -> torch.Tensor:",
            "# Need to shift the mask in the correct direction",
            "-        zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()",
            "+        zero_col = token_embeddings.new_zeros(mask.size(0), 1).to(dtype=torch.bool)",
            "if direction == 0:",
            "# forward direction, get token to right",
            "shifted_mask = torch.cat([zero_col, mask[:, 0:-1]], dim=1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=byte), value='to')",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=24334)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=24335)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=24336)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=24337)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=24338)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=24339)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=24340)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 9265,
        "neg_line": [
            "-zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte()"
        ],
        "pos_line": [
            "+zero_col = token_embeddings.new_zeros(mask.size(0), 1).to(dtype=torch.bool)"
        ],
        "core_change": "-zero_col = token_embeddings.new_zeros(mask.size(0), 1).byte() +zero_col = token_embeddings.new_zeros(mask.size(0), 1).to(dtype=torch.bool)",
        "core_API": "new_zeros"
    },
    {
        "commit_hash": "e94abf66783f5e24ab37adc754ac902e827a4b4d",
        "index": "b99eb656..1b48c501 100755",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "zc = tf.one_hot(ids, 10, name='zc_train')",
            "zc = tf.placeholder_with_default(zc, [None, 10], name='zc')",
            "",
            "-        z = tf.random_uniform(tf.pack([tf.shape(zc)[0], 90]), -1, 1, name='z_train')",
            "+        z = tf.random_uniform(tf.stack([tf.shape(zc)[0], 90]), -1, 1, name='z_train')",
            "z = tf.placeholder_with_default(z, [None, 90], name='z')",
            "-        z = tf.concat(1, [zc, z], name='fullz')",
            "+        z = tf.concat_v2([zc, z], 1, name='fullz')",
            "",
            "with argscope([Conv2D, Deconv2D, FullyConnected],",
            "W_init=tf.truncated_normal_initializer(stddev=0.02)):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=concat), value='concat_v2')",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '1'), position=4, insert_id=2308264)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=5, insert_id=2308265)",
            "Update(target_node=ASTNode(type=identifier, text=pack), value='stack')",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 9270,
        "neg_line": [
            "-z = tf.random_uniform(tf.pack([tf.shape(zc)[0], 90]), -1, 1, name='z_train')",
            "-z = tf.concat(1, [zc, z], name='fullz')"
        ],
        "pos_line": [
            "+z = tf.random_uniform(tf.stack([tf.shape(zc)[0], 90]), -1, 1, name='z_train')",
            "+z = tf.concat_v2([zc, z], 1, name='fullz')"
        ],
        "core_change": "-z = tf.random_uniform(tf.pack([tf.shape(zc)[0], 90]), -1, 1, name='z_train') +z = tf.random_uniform(tf.stack([tf.shape(zc)[0], 90]), -1, 1, name='z_train') -z = tf.concat(1, [zc, z], name='fullz') +z = tf.concat_v2([zc, z], 1, name='fullz')",
        "core_API": "one_hot"
    },
    {
        "commit_hash": "ccb38b4049e3cb82ce109f23d3c7117e55bf8b26",
        "index": "204108a50..1dd746634 100644",
        "commit_message": "Update text_sentiment_ngrams_tutorial.py (#2246)\n\nFixes #2245\n",
        "file": "tutorials.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TextClassificationModel(nn.Module):",
            "",
            "def __init__(self, vocab_size, embed_dim, num_class):",
            "super(TextClassificationModel, self).__init__()",
            "-        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)",
            "+        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)",
            "self.fc = nn.Linear(embed_dim, num_class)",
            "self.init_weights()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('false', 'False'), position=2, insert_id=1559720)",
            "Delete(target_node=ASTNode(type=true, text=True))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 9276,
        "neg_line": [
            "-self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)"
        ],
        "pos_line": [
            "+self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)"
        ],
        "core_change": "-self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True) +self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)",
        "core_API": "EmbeddingBag"
    },
    {
        "commit_hash": "27ff6a1807757d86584817b629a496b1908f44fa",
        "index": "59fec015..4f77eb2e 100755",
        "commit_message": "fix example about dropout\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def get_model(inputs, is_training):",
            "cost: the cost to minimize. scalar variable",
            "\"\"\"",
            "is_training = bool(is_training)",
            "-    keep_prob = tf.constant(0.5 if is_training else 1.0)",
            "+    keep_prob = tf.constant(0.5 if is_training else 0.0)",
            "",
            "image, label = inputs",
            "image = tf.expand_dims(image, 3)    # add a single channel"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=1.0), value='0.0')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9277,
        "neg_line": [
            "-keep_prob = tf.constant(0.5 if is_training else 1.0)"
        ],
        "pos_line": [
            "+keep_prob = tf.constant(0.5 if is_training else 0.0)"
        ],
        "core_change": "-keep_prob = tf.constant(0.5 if is_training else 1.0) +keep_prob = tf.constant(0.5 if is_training else 0.0)",
        "core_API": "constant"
    },
    {
        "commit_hash": "17d87731062691f4510c75f12f2ce63b5dde0a43",
        "index": "7932483ff..540604250 100644",
        "commit_message": "New modular metric interface (#2528)\n\n* new base structure\n\n* missing packages\n\n* updated interface\n\n* revert some changes\n\n* fixes\n\n* add changelog\n\n* fix bug\n\n* added description\n\n* test for pickable\n\n* fixing test\n\n* fixing test\n\n* fix pickle issue\n\n* reduceop typehints back\n\n* remove redundant module arg\n\n* add save/load test\n\n* add aggregate method\n\n* text clarification\n\n* fix doctest\n\n* Apply suggestions from code review\n\n* change test to results obj\n\n* fix docs\n\n* formatting\n\nCo-authored-by: Rohit Gupta <rohitgr1998@gmail.com>\n\n* formatting\n\n* pep\n\n* Update CHANGELOG.md\n\n* suggestions\n\n* fix tests\n\n* fix pep8\n\n* fix tests\n\nCo-authored-by: Nicki Skafte <nugginea@gmail.com>\nCo-authored-by: Adrian WÃ¤lchli <aedu.waelchli@gmail.com>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Rohit Gupta <rohitgr1998@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def confusion_matrix(",
            "\"\"\"",
            "num_classes = get_num_classes(pred, target, None)",
            "",
            "-    unique_labels = target.view(-1) * num_classes + pred.view(-1)",
            "+    unique_labels = (target.view(-1) * num_classes + pred.view(-1)).to(torch.int)",
            "",
            "bins = torch.bincount(unique_labels, minlength=num_classes ** 2)",
            "cm = bins.reshape(num_classes, num_classes).squeeze().float()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=570441)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=570442)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=570443)",
            "Insert(target_node=IN(type=attribute), node=('parenthesized_expression', None), position=0, insert_id=570444)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=570445)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=570446)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=570447)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=570448)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=570449)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=570450)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=binary_operator), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=570451)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=570452)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=570453)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int'), position=2, insert_id=570454)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 9280,
        "neg_line": [
            "-unique_labels = target.view(-1) * num_classes + pred.view(-1)"
        ],
        "pos_line": [
            "+unique_labels = (target.view(-1) * num_classes + pred.view(-1)).to(torch.int)"
        ],
        "core_change": "-unique_labels = target.view(-1) * num_classes + pred.view(-1) +unique_labels = (target.view(-1) * num_classes + pred.view(-1)).to(torch.int)",
        "core_API": "view"
    },
    {
        "commit_hash": "24a43d5060adb1b523a018eeee17d5ca75b2b23a",
        "index": "adf7a69a..58bc6833 100644",
        "commit_message": "fix async test (#2820)\n\n* fix async test\n\n* minor fix\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_ga_anchor_head_loss():",
            "gt_bboxes = [",
            "torch.Tensor([[23.6667, 23.8757, 238.6326, 151.8874]]).cuda(),",
            "]",
            "-        gt_labels = [torch.LongTensor([2])]",
            "+        gt_labels = [torch.LongTensor([2]).cuda()]",
            "one_gt_losses = head.loss(cls_scores, bbox_preds, shape_preds,",
            "loc_preds, gt_bboxes, gt_labels, img_metas,",
            "gt_bboxes_ignore)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=637632)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=637633)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=637634)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=637635)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=637636)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=637637)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 9284,
        "neg_line": [
            "-gt_labels = [torch.LongTensor([2])]"
        ],
        "pos_line": [
            "+gt_labels = [torch.LongTensor([2]).cuda()]"
        ],
        "core_change": "-gt_labels = [torch.LongTensor([2])] +gt_labels = [torch.LongTensor([2]).cuda()]",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "c7c7b008d92a7d22cf7d994c1ca0bb39fa696826",
        "index": "a3c03995b..20865bc81 100644",
        "commit_message": "Squashed commit of the following:\n\ncommit 047d0c474c18a87c205e566948410be16787e477\nMerge: 9396ed37d bfe7bca3a\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu May 19 09:50:02 2022 -0400\n\n    Merge pull request #4378 from akreal/fix-check_short_utt\n\n    Fix minimum input length for Conv2dSubsampling2 in check_short_utt\n\ncommit bfe7bca3a98da52714e1c45906cf826704464b7c\nAuthor: Pavel Denisov <pavel.denisov@ims.uni-stuttgart.de>\nDate:   Thu May 19 13:41:59 2022 +0200\n\n    Fix minimum input length for Conv2dSubsampling2 in check_short_utt\n\ncommit 9396ed37deb8b101fd064d46c85975ad9047bf87\nMerge: c54b585c1 e047156ec\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Thu May 19 14:50:56 2022 +0900\n\n    Merge pull request #4376 from kamo-naoyuki/libsndfile\n\n    Remove the restriction for libsndfile version\n\ncommit c54b585c1ca6693ae7ba7e299a48af762eda6adf\nMerge: 9ca49caed 88465607c\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Thu May 19 12:29:02 2022 +0900\n\n    Merge pull request #4374 from YosukeHiguchi/master\n\n    Minor fixes for the intermediate loss usage and Mask-CTC decoding\n\ncommit e047156ec8df3266259aed03742ac798e365f648\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 19 10:11:08 2022 +0900\n\n    remove version restiction for libsndfile\n\ncommit 9ca49caed98410cd7d2c71e4781819a1e92b35d9\nMerge: b008ac7d5 2952c3bca\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Thu May 19 09:38:33 2022 +0900\n\n    Merge pull request #4375 from espnet/kamo-naoyuki-patch-1\n\n    Update .mergify.yml\n\ncommit 88465607cf5e899b8ce1b93c5c9fe09b69a2ab83\nAuthor: Yosuke Higuchi <wasapon.dev@gmail.com>\nDate:   Thu May 19 07:05:29 2022 +0900\n\n    fix for test\n\ncommit 2952c3bca26a70723094d5a160387b7936f71769\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Thu May 19 06:59:02 2022 +0900\n\n    Update .mergify.yml\n\ncommit b008ac7d58e9ced1a9f8c89cc85ee69d9e9461ab\nMerge: 3c96908ed 4203c9c9c\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Thu May 19 06:32:44 2022 +0900\n\n    Merge pull request #4372 from kamo-naoyuki/isort\n\n    Add isort checking to the CI tests\n\ncommit 4de7aa562f74c596e5b616fd8278a50a707d0198\nAuthor: Yosuke Higuchi <wasapon.dev@gmail.com>\nDate:   Thu May 19 06:19:20 2022 +0900\n\n    fix for test\n\ncommit 9c83ddb46404334914764a8e4356ea8a4c3c806c\nAuthor: Yosuke Higuchi <wasapon.dev@gmail.com>\nDate:   Thu May 19 05:05:01 2022 +0900\n\n    support gpu decoding for mask-ctc\n\ncommit 49100e4f1b3fc389c5672dc2ca17973525c4bf02\nAuthor: Yosuke Higuchi <wasapon.dev@gmail.com>\nDate:   Thu May 19 05:03:29 2022 +0900\n\n    fix bug for returning intermediate states\n\ncommit 4203c9c9c9d5a68cd13d464290cead3738ed003d\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Wed May 18 17:47:22 2022 +0900\n\n    apply isort\n\ncommit d0f2eac70a5521adf59618ba3ce6603e2863f0c5\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Wed May 18 17:46:47 2022 +0900\n\n    modified for isort options\n\ncommit 8f73b73d23d34bf5f3e8ed2f625dca1916ea8683\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Wed May 18 16:38:34 2022 +0900\n\n    apply black\n\ncommit 6974dd4efc11e465d4a3d1a34190c7ed782dacee\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Wed May 18 16:35:15 2022 +0900\n\n    Add descriptions for isort\n\ncommit 24c3676a8d4c2e60d2726e9bcd9bdbed740610e0\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Wed May 18 16:16:53 2022 +0900\n\n    Apply isort\n\ncommit 3c96908edc5c592c9c99bba0640428613dc7c3cb\nMerge: c173c3093 aa5d6ffff\nAuthor: Jiatong <728307998@qq.com>\nDate:   Tue May 17 18:00:40 2022 -0700\n\n    Merge pull request #4341 from chintu619/st_bugfix\n\n    bug fixes in ST recipes\n\ncommit c173c30930631731e6836c274a591ad571749741\nMerge: e0e0620ac d38188cc3\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Tue May 17 15:20:31 2022 +0900\n\n    Merge pull request #4371 from espnet/kamo-naoyuki-patch-1\n\n    Update .mergify.yml\n\ncommit d38188cc30af6cffc4ad0233e7e705e93511c11d\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Tue May 17 13:43:40 2022 +0900\n\n    Update .mergify.yml\n\ncommit e0e0620acca0df345cf317a13c839d7d4d5c773f\nMerge: df053b8c1 2cfbbd337\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 13:01:02 2022 +0900\n\n    Merge pull request #4369 from kan-bayashi/minor_fix_jets\n\ncommit 2cfbbd337d64f68e1f937e37feeb544d972c4e0b\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 11:06:00 2022 +0900\n\n    updated jets test\n\ncommit 17ab7747fe7e0d4d6885847f2c738253a859dedf\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 11:05:52 2022 +0900\n\n    updated README\n\ncommit 6ec8c27815c6fded4c13b01b8d2707016e9e8e95\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 09:25:41 2022 +0900\n\n    updated README\n\ncommit b1e6c752b0d94f3209593e0cdbd5b43d79e8076d\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 09:19:54 2022 +0900\n\n    shorten jets test\n\ncommit df053b8c13c26fe289fc882751801fd781e9d43e\nMerge: afa8f8ec5 5aa543a9f\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 08:13:36 2022 +0900\n\n    Merge pull request #4364 from imdanboy/master\n\n    add e2e tts model: JETS\n\ncommit 5aa543a9ff6c329f5fc601f3aa053ffd4afb19ba\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Mon May 16 21:13:30 2022 +0900\n\n    minor fix of docstrings and comments\n\ncommit a82e78d18aca9c00bcf8f378c42e78a0de24940e\nAuthor: imdanboy <imdanboy@gmail.com>\nDate:   Fri May 13 22:28:31 2022 +0900\n\n    JETS; e2e tts model\n\ncommit afa8f8ec5b8ec77deb1a3c1531915ebbee7b80e6\nMerge: fffb3444f cd77501a8\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Fri May 13 17:36:30 2022 -0400\n\n    Merge pull request #4349 from pyf98/quantization\n\n    Add quantization in ESPnet2 for asr inference\n\ncommit fffb3444fe4d8ef2630a22dd145d6f1fb0caab46\nMerge: f840b8114 5331890e6\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 20:36:39 2022 +0900\n\n    Merge pull request #4361 from espnet/kamo-naoyuki-patch-1\n\n    Update README.md\n\ncommit aa5d6ffff67079f2cbe6a7e1eba852e459f0f6a4\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 13 05:15:32 2022 -0400\n\n    fix lm tag names\n\ncommit 3cac7bb7f732a694f4b87007271d394a9ee3838e\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 13 05:07:55 2022 -0400\n\n    resolve conflicts and fix lm_train filenames\n\ncommit ea44663e8a24ebfcaa03f3bba149e561e970fdf3\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 13 04:43:18 2022 -0400\n\n    review suggested changes\n\ncommit 650c733437da32627f88fe369555ce1955536087\nMerge: 6d1bd3a8e f840b8114\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 13 03:18:08 2022 -0400\n\n    Merge branch 'espnet_master' into st_bugfix\n\ncommit 5331890e6a6a61a3006e5e2c13d47172f5587a29\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 13:15:40 2022 +0900\n\n    Update README.md\n\ncommit f840b8114452b4803b8fb25c1f22a93da146e9ba\nMerge: 1b1241040 9cfd6af64\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 13:13:34 2022 +0900\n\n    Merge pull request #4348 from kamo-naoyuki/1.11.0\n\n    Add pytorch=1.10.2 and 1.11.0 to ci configurations\n\ncommit 9cfd6af64a28237019196cd495fbd2943790ce21\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 09:58:04 2022 +0900\n\n    fix\n\ncommit 2625be71a722e7eb030dff4f71d8dc9599a33844\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 03:46:24 2022 +0900\n\n    remove warning\n\ncommit 9a2001fac56dddf5ba1c2eaec092cb420f83f7c9\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 03:44:11 2022 +0900\n\n    fix for pytorch1.11 (+= became inplace op)\n\ncommit 5518b6ba0af0bba9e9d59d6c47607656f49c9988\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 22:04:42 2022 +0900\n\n    fix import order\n\ncommit 98689a5f0bfd88efffdbbcdd5d924e186d563a91\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 21:17:35 2022 +0900\n\n    change to show the error logs when jobs are failed\n\ncommit bb0d0aaa9e9f9076ac88aad425ad2f2caef369a7\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 20:40:39 2022 +0900\n\n    fix code style\n\ncommit 934b161f1f714637c3d7d47c14f8c810a9df6fe2\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 20:33:58 2022 +0900\n\n    change to show the error logs when jobs are failed\n\ncommit 5c474b96c543c3d26e95b432355bcfd2bf8dc116\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 20:20:18 2022 +0900\n\n    remove verbosity options\n\ncommit 005aad11b37acf388c6b70143ab40a5231bc7a39\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 20:04:57 2022 +0900\n\n    fix\n\ncommit 5c4b966a957062e4de298bcb69fe8cf6f1365fd1\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 19:36:11 2022 +0900\n\n    remove tests for python=3.10.0 temporary\n\ncommit 809ac3741814b7d9ebdd351b9e0e9343e236977c\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 19:27:20 2022 +0900\n\n    fix\n\ncommit 86186b744fb2bfc259909c49cc906fb0856d15bf\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 19:10:18 2022 +0900\n\n    add installation for packaging\n\ncommit 8fbac77268906075043cbecfb3e1c5625b145fce\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:59:17 2022 +0900\n\n    fix\n\ncommit b0050d97da3d0545b62a5d21b029ddd016ce6ca1\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:56:52 2022 +0900\n\n    fix\n\ncommit 6e9035d42eea31cad87a7c8b87fc79635a6df7c2\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:32:33 2022 +0900\n\n    fix\n\ncommit 1c344a95ceb83b4b44675aee5326afeb9284d8e8\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:25:35 2022 +0900\n\n    change LooseVersion to parse\n\ncommit f899a05768436cc38fb432d6f002ab667983abbd\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:09:33 2022 +0900\n\n    fix\n\ncommit 7d5242212403e740c4d5b8ebd9a346a991ea50a9\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:09:15 2022 +0900\n\n    fix\n\ncommit b7cfdd9a70559271e45de103e242228f94e837ff\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:05:41 2022 +0900\n\n    Change LooseVersion to parse\n\ncommit d234b9ab30bbc2bb6fd42d6335421a6f8a9ed637\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 17:10:40 2022 +0900\n\n    fix\n\ncommit 1b1241040e1e30e575a182b6be8b8e4602badeb8\nMerge: 39bae01e4 52c238d02\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Wed May 11 13:00:13 2022 -0400\n\n    Merge pull request #4352 from espnetUser/master\n\n    Add unit test to streaming ASR inference\n\ncommit 52c238d02d50fcfb2c4e2a5058c743c7db913eec\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Wed May 11 16:10:04 2022 +0200\n\n    Applied black formating to test_asr_inference.py for PR\n\ncommit 87c7573874aeec096dd1e902478d3dd6e2c83ad2\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Wed May 11 15:43:01 2022 +0200\n\n    Update asr_inference_streaming.py\n\n    Fix CI error on mismatch in Tensor dtypes\n\ncommit 39bae01e4a132da69b9b0d025da8c579a5f38b77\nMerge: dd24d7d41 71f3c8813\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 17:53:04 2022 +0900\n\n    Merge pull request #4355 from kan-bayashi/fix_lid_in_gan_tts\n\ncommit dd24d7d41517202b308afb186f466c8006ae4c14\nMerge: 2dde7734b f7b390582\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 17:52:09 2022 +0900\n\n    Merge pull request #4206 from WeiGodHorse/master\n\ncommit 2dde7734bade874d4f8cfe7df4be069e64259fd5\nMerge: beb336027 ec7e2b07b\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 16:27:55 2022 +0900\n\n    Merge pull request #4356 from kan-bayashi/fix_mixed_precision_vits\n\n    fix loss = NaN in VITS with mixed precision\n\ncommit 7a590ccd0da4897ef283486776f134eabe865ce0\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Wed May 11 09:25:03 2022 +0200\n\n    Applied black formating to test_asr_inference.py for PR\n\ncommit ec7e2b07bfa85c8a2292de7a2edbf1c2cd956d99\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 14:48:36 2022 +0900\n\n    fixed black\n\ncommit 2be9ddc5a2c0a7c4aad2b155fa1450222ca0c7a3\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 14:28:05 2022 +0900\n\n    fixed mixed_precision NaN (#4236)\n\ncommit 71f3c88133c7a29db54baa7eaa3b4fdf329cbdf5\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 13:39:59 2022 +0900\n\n    fixed optional data names for TTS\n\ncommit ee57ff94dfa2c3ced30c1b103076b4ae18fa9199\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Tue May 10 22:37:18 2022 +0200\n\n    Update asr_inference_streaming.py\n\n    Fix dtype CI error\n\ncommit 272d5d015f89f1520c82c31bd309fdce89d88f50\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Tue May 10 21:52:21 2022 +0200\n\n    Update test_asr_inference.py\n\n    Remove streaming=true parameter\n\ncommit c96e0d7f79e6e94e568b22156eb61004d5d8cf8c\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Tue May 10 21:25:57 2022 +0200\n\n    Aplied black formating to test_asr_inference.py for PR\n\ncommit cd77501a8f09b5b11bf5422b0e24b8316820af77\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Tue May 10 12:02:07 2022 -0400\n\n    fix error for rnn encoders flatten_parameters\n\ncommit 3aafdb9d92c8c61d62be72f0907da957d177aa8c\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Tue May 10 17:05:48 2022 +0200\n\n    Update asr_inference_streaming.py\n\n    Bugfix in streaming inference #4216\n\ncommit 61b50138b7e8828506a18067cc2f482e745e83d7\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Tue May 10 16:58:14 2022 +0200\n\n    Update test_asr_inference.py\n\n    Added edge test case for streaming asr unit test and increased execution time out\n\ncommit 052dd603900362048675f65058b7a6f4bd94bc7d\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Mon May 9 23:27:41 2022 -0400\n\n    fix ci\n\ncommit 06e2a7a16a06cda326035d03c84734d18c852cd3\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Mon May 9 23:10:14 2022 -0400\n\n    apply black\n\ncommit a48423fda5ab75d1205396ca5f744dc8ca98df00\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Mon May 9 22:59:57 2022 -0400\n\n    add test for espnet2 quantization\n\ncommit acb24c886f47fec7a00063cb66423e7bd52ea0bc\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Mon May 9 22:59:39 2022 -0400\n\n    add quantization to asr_inference\n\ncommit b98fc861939310b73b50f959bc45176da10ef493\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Tue May 10 11:52:27 2022 +0900\n\n    fix\n\ncommit 3428f032d58c73902b5e6fe80307eb08cfc64ff6\nMerge: 4ff2ce124 beb336027\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Tue May 10 11:42:23 2022 +0900\n\n    Merge branch 'master' into 1.11.0\n\ncommit 4ff2ce1244e0af72439deaa59226eba434a70618\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Tue May 10 11:34:31 2022 +0900\n\n    add pytorch=1.10.1, 1.11.0 to ci configurations\n\ncommit beb3360276aa9ff65fe84f4c5e99c0c063c2a6be\nMerge: 537f9b6c1 79cda74ba\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Mon May 9 16:27:37 2022 -0400\n\n    Merge pull request #4347 from YosukeHiguchi/espnet2_maskctc2\n\n    Minor fix for Mask-CTC forward function\n\ncommit 79cda74ba20f0b795251e23a9cb9fd624e2be02d\nAuthor: Yosuke Higuchi <wasapon.dev@gmail.com>\nDate:   Mon May 9 22:43:29 2022 +0900\n\n    add kwargs in forward argument\n\ncommit 537f9b6c14ab195cdcd21c404656c8534295f15d\nMerge: 793b999a5 9e8e75315\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Sun May 8 17:34:55 2022 -0400\n\n    Merge pull request #4343 from Emrys365/complex_support\n\n    Fix a bug in stats aggregation when PITSolver is used\n\ncommit 9e8e753154f5f71c9cb26217483427adb278759c\nAuthor: Wangyou Zhang <C0me_On@163.com>\nDate:   Sat May 7 13:16:35 2022 +0800\n\n    Apply black\n\ncommit 5ea4e087a311ab7c798950e68ae92e10b1bb41d8\nAuthor: Wangyou Zhang <C0me_On@163.com>\nDate:   Sat May 7 12:05:49 2022 +0800\n\n    Fix a bug in stats aggregation when PITSolver is used\n\ncommit 6d1bd3a8ef695a75358d019cc1b33100817c0dad\nMerge: eb6dc2d55 793b999a5\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 6 10:51:14 2022 -0400\n\n    Merge branch 'espnet:master' into st_bugfix\n\ncommit eb6dc2d55faac7e62742d0b7791d8f3a991e91d1\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 6 10:08:19 2022 -0400\n\n    typo fix\n\ncommit 8c56ee817867358f2a8130372fd914c136bd7a5b\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 6 08:59:26 2022 -0400\n\n    bug fixes in ST recipes\n\n    * Change sampling frequency in `fbank.conf` and `pitch.conf` in Covost2 recipe\n    * In `run.sh`, if language is low resource, then have more speed perturbations. Fix typos for test sets\n    * In `st.sh`\n      * fix directory naming issues to avoid replacement for different language pairs\n      * Replace `>>` with `>` to replace previous inference results\n      * Fix removing of empty text in stage 4\n      * When removing utterance-ID in `ref.trn.org` or `hyp.trn.org`, the current implementation removes all words in parenthesis instead of removing just the utterance-ID from the end of each line. Fixed this by changing `perl -pe 's/\\([^\\)]+\\)//g;'` to `perl -pe 's/\\([^\\)]+\\)$//g;'`\n\ncommit f7b390582d2d77b113a92a5e52f907d5832d6f04\nAuthor: é­å®ªè±ª <weixianhao@bytedance.com>\nDate:   Fri May 6 20:18:05 2022 +0800\n\n    change a test file to conform new pypinyin package\n\ncommit b83128fafc913e775a49d37a5cad24a893718020\nAuthor: é­å®ªè±ª <weixianhao@bytedance.com>\nDate:   Fri May 6 17:54:20 2022 +0800\n\n    Fix missing punctuation\n\ncommit 931fd226babe69b35c6e3a6a288e5e0c901736a1\nAuthor: é­å®ªè±ª <weixianhao@bytedance.com>\nDate:   Fri May 6 16:54:31 2022 +0800\n\n    reformat\n\ncommit 793b999a50af484a5eaf6227ef7556b48514ef15\nMerge: 4f41a1a06 6d0672882\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu May 5 21:54:27 2022 -0400\n\n    Merge pull request #4330 from pyf98/show_translation_result\n\n    Update show_translation_result.sh to show all decoding results under the given exp directory\n\ncommit 4f41a1a06ecd96af567bc73d1d6734531dd3cb44\nMerge: a49cc60cd f0d7cc2bf\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu May 5 21:53:10 2022 -0400\n\n    Merge pull request #4329 from roshansh-cmu/wandb\n\n    Wandb Minor Fix for Model Resume\n\ncommit a49cc60cda690e448d925c3e2bfdc5a85b3f5cd3\nMerge: de624ed58 21fba33c6\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu May 5 21:51:43 2022 -0400\n\n    Merge pull request #4338 from espnet/ftshijt-patch-1\n\n    Fix typo\n\ncommit 21fba33c69d9199c6897ffc6da8433ab94b7051d\nAuthor: Jiatong <728307998@qq.com>\nDate:   Thu May 5 21:25:10 2022 -0400\n\n    Fix typo\n\ncommit de624ed58953d17907fb241c5cb6514f27510162\nMerge: b757b89d4 fe288000d\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu May 5 16:10:44 2022 -0400\n\n    Merge pull request #4332 from simpleoier/chime6\n\n    add chime6 recipe\n\ncommit c504336661fa3cefa60b2214da39fbf0118fce49\nMerge: 50269e8b4 b757b89d4\nAuthor: é­å®ªè±ª <weixianhao@bytedance.com>\nDate:   Wed May 4 21:58:43 2022 +0800\n\n    Merge remote-tracking branch 'upstream/master'\n\ncommit fe288000dbde339b4c386408af488af4bac423b6\nAuthor: simpleoier <netnetchangxk@gmail.com>\nDate:   Tue May 3 17:51:36 2022 -0400\n\n    add egs2/chime6/asr1 recipe\n\ncommit 6d06728820576ed96a729b3477a29ccab12542f1\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Sat Apr 30 20:53:52 2022 -0400\n\n    fix ci\n\ncommit 72333a892d16ef913633111120f159008812795e\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Sat Apr 30 20:34:06 2022 -0400\n\n    fix ci\n\ncommit f15e6adaafaca380ea152cf2b38d604eea3603d3\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Sat Apr 30 18:54:37 2022 -0400\n\n    quote expansion\n\ncommit f6731cd97565bf4108f1064a83f1fffea4ca351b\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Sat Apr 30 18:43:49 2022 -0400\n\n    update mt.sh\n\ncommit 552060a1d5670d0fd838bd8e10fc9e47a1122346\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Sat Apr 30 18:41:41 2022 -0400\n\n    update show translation result\n\ncommit f0d7cc2bfbc8f68c42820262a8ca6e4906f3818b\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Fri Apr 29 20:57:18 2022 -0400\n\n    Delete resnet.py\n\ncommit 79c071e9ecd268a1963e8ca3863a2f5eaf34a525\nAuthor: roshansh-cmu <roshansh@andrew.cmu.edu>\nDate:   Fri Apr 29 20:54:37 2022 -0400\n\n    Wandb minor fix for model resume\n\ncommit ffe7c58ac8a255769f6952b8c7225a5158a00068\nMerge: 835033c70 b757b89d4\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Fri Apr 29 20:45:47 2022 -0400\n\n    Merge branch 'espnet:master' into master\n\ncommit b757b89d45d5574cebf44e225cbe32e3e9e4f522\nMerge: 930b380de 664414c8f\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Fri Apr 29 16:11:56 2022 +0900\n\n    Merge pull request #4320 from cadia-lvl/add-progress-bar\n\ncommit 930b380de02b31f8d2da4144d471e60ed41d70fc\nMerge: 2a48371b8 de81cf979\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu Apr 28 16:30:34 2022 -0400\n\n    Merge pull request #4316 from simpleoier/enh_s2t\n\n    add egs2/chime4/enh_asr1 recipe and results\n\ncommit de81cf979fd61ab13e0ab0fe0432fbbaa4776be3\nAuthor: simpleoier <netnetchangxk@gmail.com>\nDate:   Thu Apr 28 11:54:10 2022 -0400\n\n    update egs2/chime4/enh_asr1/README.md and related enh1, asr1 configs.\n\ncommit 664414c8f27d5148377ffa733c7f8369eaf7ebd4\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Thu Apr 28 21:31:45 2022 +0900\n\n    fixed flake8\n\ncommit 2a48371b8ceffd4899dc08f2fc5df092ed1d8a93\nMerge: 72c1d8f2b 5a9178236\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu Apr 28 07:40:31 2022 -0400\n\n    Merge pull request #4243 from D-Keqi/master\n\n    Add streaming ST/SLU\n\ncommit 72c1d8f2bde996febde895c603722dba1634cf20\nMerge: b7f0a5a6f 406656cdc\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu Apr 28 07:37:23 2022 -0400\n\n    Merge pull request #4110 from earthmanylf/dpclanddan\n\n    Merge Deep Clustering and Deep Attractor Network to enh separator\n\ncommit b7f0a5a6fc227049c1b8735d8ac4362c27333022\nMerge: 44971ff96 2d950f962\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu Apr 28 07:33:11 2022 -0400\n\n    Merge pull request #4328 from Emrys365/egs2_aishell4\n\n    Rename egs2/clarity21/enh_2021 to egs2/clarity21/enh1\n\ncommit 2d950f96223fd4823203b6a4e9afdc86b2357e7e\nAuthor: Wangyou Zhang <C0me_On@163.com>\nDate:   Thu Apr 28 16:58:26 2022 +0800\n\n    Rename egs2/clarity21/enh_2021/\n\ncommit 2b663318cd1773fb8685b1e03295b6bc6889c283\nAuthor: simpleoier <netnetchangxk@gmail.com>\nDate:   Thu Apr 28 00:59:22 2022 -0400\n\n    fix small bugs and add CHiME4 enh_asr1 recipe & results\n\ncommit 406656cdcb668a77910074b4382b557b6f845c54\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Thu Apr 28 11:10:11 2022 +0800\n\n    Add custom name in __init__ in tf_domain.py; Merge test_dpcl_loss.py to test_tf_domain.py\n\ncommit 5a9178236bc1a7a4a5db82ad84773d9c43199c81\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 28 10:31:29 2022 +0800\n\n    use the another st_inference\n\ncommit 9e4bb7fa88e8c63e69712e77c5b783c64181fbc2\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 28 10:13:59 2022 +0800\n\n    fix conflict\n\ncommit 21d2ac6331ec0779b8ec2d3265ccdfabfaacbd61\nMerge: b801ddc96 44971ff96\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 28 10:12:15 2022 +0800\n\n    Merge pull request #17 from espnet/master\n\n    merge the latest espnet\n\ncommit b801ddc96aedd2a9b4e63d2e3612c3cf7417799a\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 28 10:11:11 2022 +0800\n\n    Add files via upload\n\ncommit 316cf02340a627548b71317ba04afac457f68101\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 28 10:04:29 2022 +0800\n\n    fix conflict\n\ncommit 9b33b791d7c7b509f514b7540a8ec5dd7fff9d0b\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 27 23:22:22 2022 +0800\n\n    Fix format\n\ncommit 346a42467881e5bbd9414200dd3c915935eb56dd\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 27 22:37:22 2022 +0800\n\n    Fix format\n\ncommit 44971ff962aae30c962226f1ba3d87de057ac00e\nMerge: 0ae377389 c4b93e8fd\nAuthor: Jiatong <728307998@qq.com>\nDate:   Wed Apr 27 10:13:03 2022 -0400\n\n    Merge pull request #4324 from ftshijt/master\n\n    Add Test Functions for ST Train and Inference\n\ncommit 0d3be31602306650fee44c367cbc788e0b0462db\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 27 22:09:12 2022 +0800\n\n    Fix format\n\ncommit b24d108b0d7d501b2faa1971feca5a281198d351\nMerge: 4c679c061 f1312a8b2\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 27 21:29:33 2022 +0800\n\n    Fix conflict\n\ncommit 4c679c061c1a0be411f613bdbdeb7849af19edf4\nMerge: a90e2ecef 0ae377389\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 27 21:15:33 2022 +0800\n\n    Fix conflict\n\ncommit 10e6c7ea2e5783442631213dfc20dd7b9543839d\nAuthor: Gunnar Thor <ornolfsson@gmail.com>\nDate:   Wed Apr 27 09:30:47 2022 +0000\n\n    split docstring to conform with linter\n\ncommit c4b93e8fd870954ec2649abc3fc6172d78d92166\nAuthor: ftshijt <728307998@qq.com>\nDate:   Wed Apr 27 01:49:00 2022 -0400\n\n    apply black\n\ncommit 04d0cd84878701a0ff5e09933581c98ef7e0adac\nMerge: 72b6b21d5 4a12ab320\nAuthor: ftshijt <728307998@qq.com>\nDate:   Wed Apr 27 01:27:36 2022 -0400\n\n    Merge branch 'master' of https://github.com/ftshijt/espnet\n\ncommit 72b6b21d509a26d30a454525811c3530ee6b297b\nAuthor: ftshijt <728307998@qq.com>\nDate:   Wed Apr 27 01:27:09 2022 -0400\n\n    add st unit test\n\ncommit d1e8ac3d8717f8717fb645592c25ee8cafc4060c\nAuthor: ftshijt <728307998@qq.com>\nDate:   Wed Apr 27 01:15:18 2022 -0400\n\n    update test\n\ncommit 5fb7dd619293dcd1cc02c6371c4079c22a40a23b\nAuthor: ftshijt <728307998@qq.com>\nDate:   Wed Apr 27 00:53:46 2022 -0400\n\n    remove requirement for src_token_list\n\ncommit 4118b1b21f25fc7d8aa56658cd7ff691684884be\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 27 10:31:42 2022 +0800\n\n    fix conflict\n\ncommit 5436784241eaa4f60e0990627758a841e7927651\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 27 10:06:19 2022 +0800\n\n    Update test_integration_espnet2.sh\n\ncommit 469168b4451b4922306b3393598d199a514acd50\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 27 10:04:56 2022 +0800\n\n    fix issue\n\ncommit 06ddfe19a346f1ea8b620e4eb5bf61bfdcfc3309\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 27 10:01:38 2022 +0800\n\n    fix conflict\n\ncommit 5a81f91ce6734745272e6d960261797cfcb3dd41\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 27 09:57:18 2022 +0800\n\n    fix conflict\n\ncommit 91d48d920c229af3902fc05c361ba1b5f1636c67\nAuthor: Gunnar Thor <ornolfsson@gmail.com>\nDate:   Tue Apr 26 22:21:13 2022 +0000\n\n    applied black\n\ncommit ec518ccc74b85e3b50304ab70ae5a1f069df0038\nAuthor: Gunnar Thor <ornolfsson@gmail.com>\nDate:   Wed Feb 23 11:31:56 2022 +0000\n\n    Add progress bar to phonemization\n\ncommit f1312a8b2eeecf57f740b963b832dc4a806ac5f8\nAuthor: earthmanylf <43513215+earthmanylf@users.noreply.github.com>\nDate:   Mon Apr 25 10:37:19 2022 +0800\n\n    Update README.md\n\n    Co-authored-by: Wangyou Zhang <C0me_On@163.com>\n\ncommit a90e2ecef4854884dc525345a466f33fce79bd0a\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Sun Apr 24 22:55:54 2022 +0800\n\n    Fix format problems\n\ncommit be0112bf99c7caf787feba50c7dbc47a1879dbfb\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Sun Apr 24 22:06:45 2022 +0800\n\n    Fix format problems\n\ncommit 16acdadb6dba56d0f91a3132b540a01c9bd25c89\nMerge: feb28baf9 f6a2522ad\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Sun Apr 24 21:14:02 2022 +0800\n\n    Fix conflict\n\ncommit 95be28ab0e48415922677a92639833d648f3844c\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 14:47:11 2022 +0800\n\n    Fix CI\n\ncommit a0966f61701041228c96924359b8e6678960a31a\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 14:46:10 2022 +0800\n\n    Fix CI\n\ncommit 1daecd4570f477da905e4365ff30e4c0be53ca44\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 14:44:21 2022 +0800\n\n    fix CI\n\ncommit 7261735b82173ae5ac377844fad2f3b9289e08ec\nMerge: 809106e2a f6a2522ad\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 14:21:06 2022 +0800\n\n    Merge pull request #15 from espnet/master\n\n    Merging the latest ESPnet\n\ncommit 809106e2a512990b30fd1afcf2c7bf897d185d58\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 12:33:18 2022 +0800\n\n    show the log result\n\ncommit 65b53563cac0fdc09d653112f85dd735313cb650\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 11:10:41 2022 +0800\n\n    show the error report in the log\n\ncommit 36bdfcbfd0731e543db130b6fb756e140f9f2cb2\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 21 15:21:07 2022 +0800\n\n    fix ci\n\ncommit c8e05efd90ea4c9f775b149916d05f0f74092157\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 21 11:30:54 2022 +0800\n\n    fix ci\n\ncommit 4831a6671728e52f0b2a0766a7c4cb60dd3d470f\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 20 20:34:26 2022 +0800\n\n    fix CI\n\ncommit 26fc7e1b41c57dc5c6a6882fe20a8847ee5a055c\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 20 16:37:29 2022 +0800\n\n    Add files via upload\n\ncommit b7c7bf13f9df6d9c09888c21c5c071c15f1023bc\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 20 15:19:37 2022 +0800\n\n    fix ci\n\ncommit 2b1b6bbef15553a11862a9c74352bed95412337d\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 20 11:33:40 2022 +0800\n\n    fix fbank_pitch issue\n\ncommit 0d5736fc393332465ae49a620392735a22312c97\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 20 11:33:21 2022 +0800\n\n    fix fbank_pitch issue\n\ncommit 835033c70cb2821340481b6e3f695d3afe6cbcd0\nMerge: fcf13c412 42eb3108a\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Tue Apr 19 07:36:09 2022 -0400\n\n    Merge branch 'espnet:master' into master\n\ncommit 70c1980b7c8d396bd5d05d8eba50bf90a84bff55\nAuthor: D-Keqi <462975470@qq.com>\nDate:   Tue Apr 19 19:01:41 2022 +0800\n\n    fix CI\n\ncommit fabb3a1fd17b10cbcf252240e0c40243a8c2f971\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:39:39 2022 +0800\n\n    update the test_integration_espnet2\n\ncommit c08e023e429ad90399f3722d825ccaa33c84b291\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:36:09 2022 +0800\n\n    Update and rename tmp to path.sh\n\ncommit 838d2ecfa767585a3df0161388f5dd5de426695a\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:35:08 2022 +0800\n\n    Add files via upload\n\ncommit 62162ae8938d71f0f9040ee1e27eb40c83882808\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:33:31 2022 +0800\n\n    Create tmp\n\ncommit 9a5585e282b68d44921879385f5a3796bacd1fdb\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:33:00 2022 +0800\n\n    Delete t\n\ncommit 349f4ab3498bc296d46ad4b42a77fda25d5e2286\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:31:43 2022 +0800\n\n    add conf\n\ncommit e3486d24210cb53491518d913df2268a2f03eded\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:28:12 2022 +0800\n\n    Create t\n\ncommit 652cf1774dd442d55082652713bbadbc4b6946a6\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:27:47 2022 +0800\n\n    Delete tmp\n\ncommit 48fcab7a8d8b0ad1a97798fa823d315aa7708d3d\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:27:12 2022 +0800\n\n    add st1 of mini_an4\n\ncommit 1800b0be298111842ab2a3cf5f39a9ac79c3a86f\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:25:21 2022 +0800\n\n    Create tmp\n\ncommit 0a1d05b61d611ca8a7b7ca1815ae089781cbdfde\nMerge: 73ca6e4e4 952a70a70\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 13 10:20:46 2022 +0800\n\n    Merge pull request #14 from espnet/master\n\n    Merge the latest ESPnet\n\ncommit 73ca6e4e4baddd5f3fb6075788ed3e902021b9c8\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:59:52 2022 +0800\n\n    fix ci\n\n    fix ci\n\ncommit acd3e0acdc4d4c6eadfa531711906aa29ffb01a0\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:58:34 2022 +0800\n\n    fix CI\n\n    fix CI\n\ncommit e6da9baea12c6383282bdb716745060be5011a08\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:16:45 2022 +0800\n\n    Add files via upload\n\ncommit fc45fa368bc55b92f94e9ae6f9a6953728f3c894\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:13:53 2022 +0800\n\n    Delete README.md\n\ncommit 5b8c0b567f6b172e2112c5460c45e44b934478a6\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:13:11 2022 +0800\n\n    Delete egs2/chime4/asr1/exp/asr_train_asr_streaming_transformer_raw_en_char/decode_asr_streamindt05_real_beamformit_2micsg_lm_lm_train_lm_en_char_valid.loss.ave_asr_model_valid.acc.ave directory\n\ncommit 87ac110aaf70e2c339bac6ed7c5b60a856acc535\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:10:14 2022 +0800\n\n    streaming slu\n\ncommit 7b7fde9752cd9cd4905d642996215a158bf8d026\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:09:27 2022 +0800\n\n    streaming slu\n\ncommit fcd129620bbbc063dd918b83961d568ad694e45a\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:08:55 2022 +0800\n\n    streaming st\n\ncommit 17fe79ca89b496e4f9b6b4caaa2497816d4855b3\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:07:28 2022 +0800\n\n    streaming st\n\ncommit 812a527bb836a2fbd12ceb6d3bcabcc728d88427\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:06:31 2022 +0800\n\n    streaming st\n\ncommit e69a6d8efcd1ae57aca6315d70a20e484d360f7f\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:05:25 2022 +0800\n\n    streaming st\n\ncommit e488037b8d9b3e46476874f62b095ae5b7323e19\nMerge: 9fb445053 189e1593d\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 15:32:57 2022 +0800\n\n    Merge pull request #13 from espnet/master\n\n    Update lastest espnet\n\ncommit fcf13c412842d57cf48580dd89ff0d1fc5e6c3e0\nMerge: 39700a054 c4aba12f9\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Wed Apr 6 13:35:13 2022 -0400\n\n    Merge branch 'espnet:master' into master\n\ncommit feb28baf9dd6af564fe30920c1c6e70c2258e0de\nMerge: 3e6167c51 c4aba12f9\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 6 19:24:06 2022 +0800\n\n    Add deep clustering end-to-end training method\n\ncommit 50269e8b4dd0696d02e5da9f70c2d7952a26f392\nAuthor: WeiGodHorse <weigodhorse@gmail.com>\nDate:   Fri Mar 25 22:58:41 2022 +0800\n\n    fix a bug in Mandarin pypinyin_g2p_phone\n\ncommit 39700a054ac5ed718a1eb74cef9b64b2144b727c\nMerge: aa706c512 14c635069\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Thu Mar 24 17:42:11 2022 -0400\n\n    Merge branch 'espnet:master' into master\n\ncommit aa706c5122391feee57d4db121a403dfd8ea0ab0\nMerge: ab2fa25af 350af365f\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Wed Mar 23 23:34:17 2022 -0400\n\n    Merge branch 'espnet:master' into master\n\ncommit ab2fa25af6dffce3ecdf3e92adaa171d3d156d50\nMerge: de5e7139b cb8181a99\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Tue Mar 8 16:03:38 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit de5e7139b65549adfcac58cb0ee23c32c50634ea\nMerge: 5ef36bcae 1bac0f080\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Tue Mar 8 15:09:20 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 5ef36bcae3fac1792ccc2aae6b7dbab715f094fe\nMerge: 597cd7bd8 0c246e23c\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Tue Mar 8 13:35:27 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 597cd7bd8a0efbe82733d19774297ab90f5c659f\nMerge: 6625f9056 f16e579e2\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Mon Mar 7 21:54:06 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 6625f9056b5087aeb13a2214c770d586c067f5e3\nMerge: 5f237866b 5e070668e\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Mon Mar 7 13:35:03 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 3e6167c51df23b7629d7830e81e8cf4ea52032fc\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Mon Mar 7 20:03:31 2022 +0800\n\n    Fixed format in some files\n\ncommit 294373a121cf0766efe623dc56b12d0990a77c93\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Mon Mar 7 18:26:49 2022 +0800\n\n    Update code and add comments in separator\n\ncommit 5f86c1104cbce4275043e11050b69191834ddbc0\nMerge: 7aa90b584 6f429608b\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Mon Mar 7 18:06:10 2022 +0800\n\n    Add experiment result in egs2/wsj0_2mix/enh1/README.md; Update code in some files\n\ncommit 5f237866b360028676c7b9e903d15839cdaa0113\nMerge: 66c1a798d 6f429608b\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Sun Mar 6 19:26:35 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 66c1a798d15f531b4c4b4c1e02cfd1eda6813f92\nMerge: 5c5eb0292 a04a98c98\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Thu Mar 3 18:14:47 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 7aa90b5844ba1d0050cfd737b2a2fabe9abd5d62\nMerge: 5f7e2e714 b274c4ea6\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Thu Mar 3 16:20:25 2022 +0800\n\n    Merge branch 'master' of github.com:espnet/espnet into dpclanddan\n\ncommit 5c5eb0292e28c19345fc71d456348f6353f2e2a4\nMerge: bd8e400fa 9863980d2\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Wed Mar 2 12:13:35 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit bd8e400fa37ebc1b77f7a938ae9275bb18de6fe5\nMerge: 58aec432d 7999009d5\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Mon Feb 28 20:37:32 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 5f7e2e7140cc7204acecda90a6ff1d5379967da6\nMerge: d3acdcc3b 637d8c333\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Sun Feb 27 13:19:45 2022 +0800\n\n    Merge branch 'master' of github.com:espnet/espnet into dpclanddan\n\ncommit d3acdcc3bd537cf3f50c8d5c4642dfc488daa656\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Fri Feb 25 18:32:30 2022 +0800\n\n    fix bugs of test_dan_separator.py\n\ncommit c54d9a4087106b56ab5ce4ec9758aeb74bca0b4c\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Fri Feb 25 16:00:30 2022 +0800\n\n    add subs to the abs_separator.py\n\ncommit c1d9be5f4f9eb32bc75fb7a8b2fe406aa997946c\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Fri Feb 25 15:30:46 2022 +0800\n\n    update for dpcl and dan\n\ncommit 58aec432d97300ec12494676a19900a08a950827\nMerge: 23a537e2a 9c24b3add\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Wed Feb 23 16:17:09 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 23a537e2ad1ee9af7e8016054208d5ce1cc572fd\nAuthor: roshansh-cmu <roshansh@andrew.cmu.edu>\nDate:   Tue Feb 22 06:50:03 2022 -0500\n\n    black fix\n\ncommit 8572a57af47ef72e9f010601483b31eb96baf03f\nMerge: 969b333d9 650472b45\nAuthor: roshansh-cmu <roshansh@andrew.cmu.edu>\nDate:   Mon Feb 21 22:35:49 2022 -0500\n\n    Mergefix\n\ncommit ee20e18a5f0eef55c8b0709e1e6b9bcddf10e4e6\nMerge: 63f88c02b a3e1543e9\nAuthor: earthmanylf <43513215+earthmanylf@users.noreply.github.com>\nDate:   Wed Feb 16 14:29:36 2022 +0800\n\n    Merge pull request #1 from espnet/master\n\n    Merge from upstream\n\ncommit 9fb445053f999b64350e5e7a56a1699a727ed125\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Sep 15 00:30:05 2021 +0800\n\n    Update README.md\n\ncommit 8c6d3e1614a247b78f1b17ff2c6ef3b3725b166a\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Sep 15 00:29:31 2021 +0800\n\n    Update README.md\n\ncommit 2411dbb82b08aee182df0738a47d7f6f44bdcea8\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Mon Sep 13 13:08:52 2021 +0800\n\n    Update README.md\n\ncommit 3edc1a6d816428b3e4e099271dc51c117b9c8d3b\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Mon Sep 13 13:08:25 2021 +0800\n\n    Update README.md\n\ncommit d4d4b7e450992867bc0ee91ffb467ec38ad6981c\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Sep 11 23:11:39 2021 +0800\n\n    Update README.md\n\ncommit 885ab0552dc26076b0b581eb88813f426179fdcb\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Sep 4 10:48:05 2021 +0800\n\n    add results\n\ncommit dfba960da5e60cd9d78c439b7fa0e400332fbe46\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Sep 4 10:43:36 2021 +0800\n\n    create exp\n\ncommit 391d7c78f310313ca78abc1b3341183a15336579\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Sep 4 10:40:23 2021 +0800\n\n    steaming results\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Reporter:",
            "seconds=time.perf_counter() - sub_reporter.start_time",
            ")",
            "stats[\"total_count\"] = sub_reporter.total_count",
            "-        if LooseVersion(torch.__version__) >= LooseVersion(\"1.4.0\"):",
            "+        if V(torch.__version__) >= V(\"1.4.0\"):",
            "if torch.cuda.is_initialized():",
            "stats[\"gpu_max_cached_mem_GB\"] = (",
            "torch.cuda.max_memory_reserved() / 2**30"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=LooseVersion), value='V')",
            "Update(target_node=ASTNode(type=identifier, text=LooseVersion), value='V')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 9287,
        "neg_line": [
            "-if LooseVersion(torch.__version__) >= LooseVersion(\"1.4.0\"):"
        ],
        "pos_line": [
            "+if V(torch.__version__) >= V(\"1.4.0\"):"
        ],
        "core_change": "-if LooseVersion(torch.__version__) >= LooseVersion(\"1.4.0\"): +if V(torch.__version__) >= V(\"1.4.0\"):",
        "core_API": "perf_counter"
    },
    {
        "commit_hash": "47d5c244706ab01859a3931f66066407a9d5efbb",
        "index": "b9a8de4c..354b199c 100755",
        "commit_message": "Summaries completely changed, distributed mode incompletely changed, various fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFOptimizer(Optimizer):",
            "loss = fn_loss()",
            "",
            "with tf.control_dependencies(control_inputs=(loss,)):",
            "-            vars_before = [tf.add(x=var, y=0.0) for var in variables]",
            "+            vars_before = [var + 0.0 for var in variables]",
            "",
            "with tf.control_dependencies(control_inputs=vars_before):",
            "applied = self.optimizer.minimize(loss=loss, var_list=variables)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list_comprehension), node=('binary_operator', None), position=1, insert_id=2242685)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=var), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=2242686)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=float, text=0.0), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=add))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=y))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 9289,
        "neg_line": [
            "-vars_before = [tf.add(x=var, y=0.0) for var in variables]"
        ],
        "pos_line": [
            "+vars_before = [var + 0.0 for var in variables]"
        ],
        "core_change": "-vars_before = [tf.add(x=var, y=0.0) for var in variables] +vars_before = [var + 0.0 for var in variables]",
        "core_API": "control_dependencies"
    },
    {
        "commit_hash": "3dda5ac9fcf390d8b83eb855249360711707c11c",
        "index": "41769b0a..c82779a8 100644",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from allennlp.modules.attention.cosine_attention import CosineAttention",
            "",
            "",
            "class TestCosineAttention(AllenNlpTestCase):",
            "-",
            "def test_can_init_cosine(self):",
            "legacy_attention = Attention.from_params(Params({\"type\": \"cosine\"}))",
            "isinstance(legacy_attention, CosineAttention)",
            "",
            "def test_cosine_similarity(self):",
            "linear = CosineAttention(normalize=False)",
            "-        output = linear(torch.FloatTensor([[0, 0, 0], [1, 1, 1]]),",
            "-                        torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]))",
            "+        output = linear(",
            "+            torch.FloatTensor([[0, 0, 0], [1, 1, 1]]),",
            "+            torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]),",
            "+        )",
            "",
            "assert_almost_equal(output.numpy(), numpy.array([[0.0, 0.0], [0.9948, 0.9973]]), decimal=2)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=23822)"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 9290,
        "neg_line": [
            "-",
            "-output = linear(torch.FloatTensor([[0, 0, 0], [1, 1, 1]]),",
            "-torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]))"
        ],
        "pos_line": [
            "+output = linear(",
            "+torch.FloatTensor([[0, 0, 0], [1, 1, 1]]),",
            "+torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]),",
            "+)"
        ],
        "core_change": "- -output = linear(torch.FloatTensor([[0, 0, 0], [1, 1, 1]]), -torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])) +output = linear( +torch.FloatTensor([[0, 0, 0], [1, 1, 1]]), +torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]), +)",
        "core_API": "from_params"
    },
    {
        "commit_hash": "64236e92e99fe64467dda8e05d3e8dd63ec7d274",
        "index": "ac130364..05d4e067 100644",
        "commit_message": "fix all tests\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ShaDowKHopSampler(torch.utils.data.DataLoader):",
            "sparse_sizes=(n_id.numel(), n_id.numel()),",
            "is_sorted=True)",
            "",
            "-        batch = Batch(torch.ops.torch_sparse.ptr2ind(ptr, n_id.numel()), ptr)",
            "+        batch = Batch(batch=torch.ops.torch_sparse.ptr2ind(ptr, n_id.numel()),",
            "+                      ptr=ptr)",
            "batch.root_n_id = root_n_id",
            "",
            "if self.is_sparse_tensor:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=1005637)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=4, insert_id=1005638)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'batch'), position=0, insert_id=1005639)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1005640)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=call), position=2)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=ptr), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1005641)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'ptr'), position=2, insert_id=1005642)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 9301,
        "neg_line": [
            "-batch = Batch(torch.ops.torch_sparse.ptr2ind(ptr, n_id.numel()), ptr)"
        ],
        "pos_line": [
            "+batch = Batch(batch=torch.ops.torch_sparse.ptr2ind(ptr, n_id.numel()),",
            "+ptr=ptr)"
        ],
        "core_change": "-batch = Batch(torch.ops.torch_sparse.ptr2ind(ptr, n_id.numel()), ptr) +batch = Batch(batch=torch.ops.torch_sparse.ptr2ind(ptr, n_id.numel()), +ptr=ptr)",
        "core_API": "numel"
    },
    {
        "commit_hash": "a8da3205f52527c363db5783283f25dd1721addd",
        "index": "46b4bdb3..fc7507a7 100644",
        "commit_message": "formatting, fixes, etc\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Clip(Preprocessor):",
            "Clip by min/max.",
            "\"\"\"",
            "",
            "-    def __init__(self, min, max, scope='clip', summary_labels=()):",
            "-        self.min = min",
            "-        self.max = max",
            "-        super(Clip, self).__init__(scope, summary_labels)",
            "+    def __init__(self, min_value, max_value, scope='clip', summary_labels=()):",
            "+        self.min_value = min_value",
            "+        self.max_value = max_value",
            "+        super(Clip, self).__init__(scope=scope, summary_labels=summary_labels)",
            "",
            "def tf_process(self, tensor):",
            "-        return tf.clip_by_value(t=tensor, clip_value_min=self.min, clip_value_max=self.max)",
            "+        return tf.clip_by_value(t=tensor, clip_value_min=self.min_value, clip_value_max=self.max_value)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=min), value='min_value')",
            "Update(target_node=ASTNode(type=identifier, text=max), value='max_value')",
            "Update(target_node=ASTNode(type=identifier, text=min), value='min_value')",
            "Update(target_node=ASTNode(type=identifier, text=max), value='max_value')",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2237993)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=4, insert_id=2237994)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=scope), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2237995)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'scope'), position=2, insert_id=2237996)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=summary_labels), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2237997)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'summary_labels'), position=2, insert_id=2237998)",
            "Update(target_node=ASTNode(type=identifier, text=min), value='min_value')",
            "Update(target_node=ASTNode(type=identifier, text=max), value='max_value')",
            "Update(target_node=ASTNode(type=identifier, text=min), value='min_value')",
            "Update(target_node=ASTNode(type=identifier, text=max), value='max_value')"
        ],
        "plus_line": 5,
        "minus_line": 5,
        "AST_diff_line": 16,
        "number": 9302,
        "neg_line": [
            "-def __init__(self, min, max, scope='clip', summary_labels=()):",
            "-self.min = min",
            "-self.max = max",
            "-super(Clip, self).__init__(scope, summary_labels)",
            "-return tf.clip_by_value(t=tensor, clip_value_min=self.min, clip_value_max=self.max)"
        ],
        "pos_line": [
            "+def __init__(self, min_value, max_value, scope='clip', summary_labels=()):",
            "+self.min_value = min_value",
            "+self.max_value = max_value",
            "+super(Clip, self).__init__(scope=scope, summary_labels=summary_labels)",
            "+return tf.clip_by_value(t=tensor, clip_value_min=self.min_value, clip_value_max=self.max_value)"
        ],
        "core_change": "-def __init__(self, min, max, scope='clip', summary_labels=()): -self.min = min -self.max = max -super(Clip, self).__init__(scope, summary_labels) +def __init__(self, min_value, max_value, scope='clip', summary_labels=()): +self.min_value = min_value +self.max_value = max_value +super(Clip, self).__init__(scope=scope, summary_labels=summary_labels) -return tf.clip_by_value(t=tensor, clip_value_min=self.min, clip_value_max=self.max) +return tf.clip_by_value(t=tensor, clip_value_min=self.min_value, clip_value_max=self.max_value)",
        "core_API": "clip_by_value"
    },
    {
        "commit_hash": "f00c9b490b893a0f5b5913d57b67f9c0c1da0d56",
        "index": "ca773683..563f03ca 100644",
        "commit_message": "fixed dtype\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DeepQNetwork(ValueFunction):",
            "",
            "with tf.name_scope(\"update\"):",
            "self.q_targets = tf.placeholder(tf.float32, [None], name='q_targets')",
            "-            self.actions = tf.placeholder(tf.float32, [None, self.action_count], name='actions')",
            "+            self.actions = tf.placeholder(tf.int32, [None, self.action_count], name='actions')",
            "",
            "# Q values for actions taken in batch",
            "+            print(self.actions)",
            "actions_one_hot = tf.one_hot(self.actions, self.action_count, 1.0, 0.0, name='action_one_hot')",
            "q_values_actions_taken = tf.reduce_sum(self.training_output * actions_one_hot, reduction_indices=1,",
            "name='q_acted')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=2248872)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2248873)",
            "Insert(target_node=ASTNode(type=assignment), node=('attribute', None), position=0, insert_id=2248874)",
            "Insert(target_node=IN(type=call), node=('identifier', 'print'), position=0, insert_id=2248875)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2248876)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2248877)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2248878)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'actions'), position=2, insert_id=2248879)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2248880)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2248881)",
            "Update(target_node=ASTNode(type=identifier, text=float32), value='int32')"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 9327,
        "neg_line": [
            "-self.actions = tf.placeholder(tf.float32, [None, self.action_count], name='actions')"
        ],
        "pos_line": [
            "+self.actions = tf.placeholder(tf.int32, [None, self.action_count], name='actions')",
            "+print(self.actions)"
        ],
        "core_change": "-self.actions = tf.placeholder(tf.float32, [None, self.action_count], name='actions') +self.actions = tf.placeholder(tf.int32, [None, self.action_count], name='actions') +print(self.actions)",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "4df34712a555f94b433cfaa5265556358ebbfcf4",
        "index": "cb83441f..8792b2c5 100644",
        "commit_message": "updated tune script, other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SignatureDict(NestedDict):",
            "assert isinstance(arg, TensorDict)",
            "args.append(spec.kwargs_to_args(kwargs=arg))",
            "else:",
            "-                assert isinstance(arg, (tf.IndexedSlices, tf.Tensor, tf.Variable))",
            "+                assert isinstance(arg, (tf.IndexedSlices, tf.Tensor, tf.Variable)), (name, spec, arg)",
            "args.append(arg)",
            "return args"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('ERROR', None), position=2, insert_id=2224312)",
            "Insert(target_node=ASTNode(type=assignment), node=('type', None), position=3, insert_id=2224313)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=assert), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=ERROR), node=(',', ','), position=2, insert_id=2224314)",
            "Insert(target_node=IN(type=type), node=('tuple', None), position=0, insert_id=2224315)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2224316)",
            "Insert(target_node=IN(type=tuple), node=('identifier', 'name'), position=1, insert_id=2224317)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2224318)",
            "Insert(target_node=IN(type=tuple), node=('identifier', 'spec'), position=3, insert_id=2224319)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=4, insert_id=2224320)",
            "Insert(target_node=IN(type=tuple), node=('identifier', 'arg'), position=5, insert_id=2224321)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=6, insert_id=2224322)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=type))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 9328,
        "neg_line": [
            "-assert isinstance(arg, (tf.IndexedSlices, tf.Tensor, tf.Variable))"
        ],
        "pos_line": [
            "+assert isinstance(arg, (tf.IndexedSlices, tf.Tensor, tf.Variable)), (name, spec, arg)"
        ],
        "core_change": "-assert isinstance(arg, (tf.IndexedSlices, tf.Tensor, tf.Variable)) +assert isinstance(arg, (tf.IndexedSlices, tf.Tensor, tf.Variable)), (name, spec, arg)",
        "core_API": "append"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "a83cdc61..7a8dcc0d 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestRGBShift:",
            "",
            "def test_rgb_shift(self, device, dtype):",
            "r_shift, g_shift, b_shift = 0.1, 0.2, -0.3",
            "-        image = torch.tensor([[[[0.2, 0.]], [[0.3, 0.5]], [[0.4, 0.7]]]], device=device, dtype=dtype)",
            "+        image = torch.tensor([[[[0.2, 0.0]], [[0.3, 0.5]], [[0.4, 0.7]]]], device=device, dtype=dtype)",
            "shifted = kornia.enhance.shift_rgb(image, r_shift, g_shift, b_shift)",
            "expected = torch.tensor([[[[0.3, 0.1]], [[0.5, 0.7]], [[0.1, 0.4]]]], device=device, dtype=dtype)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=0.), value='0.0')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9335,
        "neg_line": [
            "-image = torch.tensor([[[[0.2, 0.]], [[0.3, 0.5]], [[0.4, 0.7]]]], device=device, dtype=dtype)"
        ],
        "pos_line": [
            "+image = torch.tensor([[[[0.2, 0.0]], [[0.3, 0.5]], [[0.4, 0.7]]]], device=device, dtype=dtype)"
        ],
        "core_change": "-image = torch.tensor([[[[0.2, 0.]], [[0.3, 0.5]], [[0.4, 0.7]]]], device=device, dtype=dtype) +image = torch.tensor([[[[0.2, 0.0]], [[0.3, 0.5]], [[0.4, 0.7]]]], device=device, dtype=dtype)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "2ac9a0d24a091989f869af55f9f6411b37ff5188",
        "index": "d8b1fa579..4513e1c65 100644",
        "commit_message": "Fix typo in the comment in _info function (#1149)\n\n\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class NewDataset(datasets.GeneratorBasedBuilder):",
            "DEFAULT_CONFIG_NAME = \"first_domain\"  # It's not mandatory to have a default configuration. Just use one if it make sense.",
            "",
            "def _info(self):",
            "-        # TODO: This method pecifies the datasets.DatasetInfo object which contains informations and typings for the dataset",
            "+        # TODO: This method specifies the datasets.DatasetInfo object which contains informations and typings for the dataset",
            "if self.config.name == \"first_domain\":  # This is the name of the configuration selected in BUILDER_CONFIGS above",
            "features = datasets.Features(",
            "{"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 9346,
        "neg_line": [
            "-# TODO: This method pecifies the datasets.DatasetInfo object which contains informations and typings for the dataset"
        ],
        "pos_line": [
            "+# TODO: This method specifies the datasets.DatasetInfo object which contains informations and typings for the dataset"
        ],
        "core_change": "-# TODO: This method pecifies the datasets.DatasetInfo object which contains informations and typings for the dataset +# TODO: This method specifies the datasets.DatasetInfo object which contains informations and typings for the dataset",
        "core_API": "Features"
    },
    {
        "commit_hash": "bcc00049550d920ddf4e66809c2f2c1aac7c31cb",
        "index": "2ba97d7d9..2cc609877 100644",
        "commit_message": "Add before_batch_transfer and after_batch_transfer hooks (#3671)\n\n* add hooks\n\n* comment\n\n* docs\n\n* add tests\n\n* make it private\n\n* fix tests\n\n* docs\n\n* chlog\n\n* testcode\n\n* codefactor\n\n* fix doctest\n\n* fix doctest\n\n* suggestions\n\n* is always overriden\n\n* pep and BoringModel\n\n* BoringModel\n\n* docs\n\n* docs\n\n* docs\n\n* fix\n\n* rebase\n\n* rebase\n\n* suggestions\n\n* docs\n\n* suggestions\n\n* try fix docs\n\n* docs\n\n* update name\n\n* yapf\n\n* docs\n\n* rebase\n\n* yapf\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Accelerator(object):",
            ":paramref:`~pytorch_lightning.trainer.trainer.Trainer.truncated_bptt_steps` > 0.",
            "",
            "\"\"\"",
            "-        batch = self.to_device(args[0])",
            "-",
            "-        args[0] = batch",
            "+        args[0] = self.to_device(args[0])",
            "",
            "with self.precision_plugin.train_step_context(), self.training_type_plugin.train_step_context():",
            "return self.training_type_plugin.training_step(*args)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=subscript), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=batch))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=batch))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 9352,
        "neg_line": [
            "-batch = self.to_device(args[0])",
            "-",
            "-args[0] = batch"
        ],
        "pos_line": [
            "+args[0] = self.to_device(args[0])"
        ],
        "core_change": "-batch = self.to_device(args[0]) - -args[0] = batch +args[0] = self.to_device(args[0])",
        "core_API": "to_device"
    },
    {
        "commit_hash": "03ab49feb075149c0df65d47cdb91d563b8980e2",
        "index": "446e9bdb..d50ca472 100644",
        "commit_message": "update pytorch ci matrix 1.10.2 and 1.11.0 (#1771)\n\n* update pytorch ci matrix 1.10.2 and 1.11.0\n\n* Update tests_cpu.yml\n\n* fix mypy\n\n* Update tests_cpu.yml\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ZCAWhitening(nn.Module):",
            "T, mean, T_inv = zca_mean(x, self.dim, self.unbiased, self.eps, self.compute_inv)",
            "",
            "self.mean_vector = mean",
            "-        self.transform_matrix: torch.Tensor = T",
            "+        self.transform_matrix = T",
            "if T_inv is None:",
            "-            self.transform_inv: Optional[torch.Tensor] = torch.empty([0])",
            "+            self.transform_inv = torch.empty([0])",
            "else:",
            "self.transform_inv = T_inv"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=Optional))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=type))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 9355,
        "neg_line": [
            "-self.transform_matrix: torch.Tensor = T",
            "-self.transform_inv: Optional[torch.Tensor] = torch.empty([0])"
        ],
        "pos_line": [
            "+self.transform_matrix = T",
            "+self.transform_inv = torch.empty([0])"
        ],
        "core_change": "-self.transform_matrix: torch.Tensor = T +self.transform_matrix = T -self.transform_inv: Optional[torch.Tensor] = torch.empty([0]) +self.transform_inv = torch.empty([0])",
        "core_API": "empty"
    },
    {
        "commit_hash": "b7a41d5d715a9792b9f23d68090b047e480cd750",
        "index": "44eb3ca1dd..530a86c516 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def quantile(",
            "# backends the quantile has to be in range [0, 1].",
            "q = q * 100",
            "",
            "-    # The quantile instance method in other backends is equivalent of",
            "+    # The quantile instance method in other backends is equivalent of",
            "# percentile instance method in tensorflow_probability",
            "result = tfp.stats.percentile(",
            "-        a,",
            "-        q,",
            "-        axis=axis,",
            "-        interpolation=interpolation,",
            "-        keepdims=keepdims",
            "+        a, q, axis=axis, interpolation=interpolation, keepdims=keepdims",
            ")",
            "return result"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 6,
        "AST_diff_line": 17,
        "number": 9359,
        "neg_line": [
            "-# The quantile instance method in other backends is equivalent of",
            "-a,",
            "-q,",
            "-axis=axis,",
            "-interpolation=interpolation,",
            "-keepdims=keepdims"
        ],
        "pos_line": [
            "+# The quantile instance method in other backends is equivalent of",
            "+a, q, axis=axis, interpolation=interpolation, keepdims=keepdims"
        ],
        "core_change": "-# The quantile instance method in other backends is equivalent of +# The quantile instance method in other backends is equivalent of -a, -q, -axis=axis, -interpolation=interpolation, -keepdims=keepdims +a, q, axis=axis, interpolation=interpolation, keepdims=keepdims",
        "core_API": "percentile"
    },
    {
        "commit_hash": "2e6db6fba3cf1fb13aa31b1f7a77f9ccac9c4e74",
        "index": "3a50eb430..55c548cbf 100644",
        "commit_message": "style: code style fixes (#1046)\n\n* fix: automatic code style correction\n\n* fix: squad_iterator\n\n* fix: delete document_bert_ner_iterator\n\n* fix: revert json files to dev version\n\n* fix: removed from registry\n\n* refactor: fix merge mistakes\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SquadModel(LRScheduledTFModel):",
            "self.cc_ph = tf.placeholder(shape=(None, None, self.char_limit), dtype=tf.int32, name='cc_ph')",
            "self.q_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name='q_ph')",
            "self.qc_ph = tf.placeholder(shape=(None, None, self.char_limit), dtype=tf.int32, name='qc_ph')",
            "-        self.y1_ph = tf.placeholder(shape=(None, ), dtype=tf.int32, name='y1_ph')",
            "-        self.y2_ph = tf.placeholder(shape=(None, ), dtype=tf.int32, name='y2_ph')",
            "+        self.y1_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name='y1_ph')",
            "+        self.y2_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name='y2_ph')",
            "",
            "self.lear_rate_ph = tf.placeholder_with_default(0.0, shape=[], name='learning_rate')",
            "self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=[], name='keep_prob_ph')"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 9361,
        "neg_line": [
            "-self.y1_ph = tf.placeholder(shape=(None, ), dtype=tf.int32, name='y1_ph')",
            "-self.y2_ph = tf.placeholder(shape=(None, ), dtype=tf.int32, name='y2_ph')"
        ],
        "pos_line": [
            "+self.y1_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name='y1_ph')",
            "+self.y2_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name='y2_ph')"
        ],
        "core_change": "-self.y1_ph = tf.placeholder(shape=(None, ), dtype=tf.int32, name='y1_ph') -self.y2_ph = tf.placeholder(shape=(None, ), dtype=tf.int32, name='y2_ph') +self.y1_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name='y1_ph') +self.y2_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name='y2_ph')",
        "core_API": "placeholder"
    },
    {
        "commit_hash": "de4159a318f9a4f2f474733306c00ac326e643a7",
        "index": "48c26a138..ea379a039 100644",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFTapasPreTrainedModel(TFPreTrainedModel):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "\"attention_mask\": tf.TensorSpec((None, None), tf.float32, name=\"attention_mask\"),",
            "-                \"token_type_ids\": tf.TensorSpec((None, None, None), tf.int64, name=\"token_type_ids\"),",
            "+                \"token_type_ids\": tf.TensorSpec((None, None, None), tf.int32, name=\"token_type_ids\"),",
            "}",
            "]",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2358201)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2358202)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2358203)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'TensorSpec'), position=2, insert_id=2358204)",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='int32')",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='int32')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=TensorSpec))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 9362,
        "neg_line": [
            "-\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "-\"token_type_ids\": tf.TensorSpec((None, None, None), tf.int64, name=\"token_type_ids\"),"
        ],
        "pos_line": [
            "+\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "+\"token_type_ids\": tf.TensorSpec((None, None, None), tf.int32, name=\"token_type_ids\"),"
        ],
        "core_change": "-\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"), +\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"), -\"token_type_ids\": tf.TensorSpec((None, None, None), tf.int64, name=\"token_type_ids\"), +\"token_type_ids\": tf.TensorSpec((None, None, None), tf.int32, name=\"token_type_ids\"),",
        "core_API": "function"
    },
    {
        "commit_hash": "7fdfc7075f7144df94db09eabbb7a3927c325881",
        "index": "bc84ff7b24..01f4b90558 100644",
        "commit_message": "fix torch einsum\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def einsum(",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "dtype = _get_promoted_type_of_operands(operands)",
            "-    operands = (ivy.astype(operand, torch.float32, copy=False) for operand in operands)",
            "+    operands = (",
            "+        ivy.astype(operand, torch.float32, copy=False).to_native()",
            "+        for operand in operands",
            "+    )",
            "return ivy.astype(torch.einsum(equation, *operands), dtype, copy=False)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=305654)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=305655)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=305656)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to_native'), position=2, insert_id=305657)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=305658)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=305659)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 9363,
        "neg_line": [
            "-operands = (ivy.astype(operand, torch.float32, copy=False) for operand in operands)"
        ],
        "pos_line": [
            "+operands = (",
            "+ivy.astype(operand, torch.float32, copy=False).to_native()",
            "+for operand in operands",
            "+)"
        ],
        "core_change": "-operands = (ivy.astype(operand, torch.float32, copy=False) for operand in operands) +operands = ( +ivy.astype(operand, torch.float32, copy=False).to_native() +for operand in operands +)",
        "core_API": "astype"
    },
    {
        "commit_hash": "1ffc4838f60f4ff2480873d3ba2428f6195abfea",
        "index": "26beeb5..1c0dc06 100644",
        "commit_message": "fix memory issue of exporter for bi-transformer\n\nSummary: Before word_feat for bi-transformer is a required input feature, which makes a lot of stuff inefficient for bi-transformer fine-tuning model. So that we met memory issues when exporting hate speech models, eg f105928516. This diff is to fix the inefficiencies for bi-transformer fine-tuning model.\n\nDifferential Revision: D14694641\n\nfbshipit-source-id: 85d183033b0490720bfb248756c4a3ae8395bc79\n\n",
        "file": "pytext.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PureDocAttention(RepresentationBase):",
            "self.representation_dim = self.dense.out_dim",
            "",
            "def forward(",
            "-        self, embedded_tokens: torch.Tensor, seq_lengths: torch.Tensor, *args",
            "+        self, embedded_tokens: torch.Tensor, seq_lengths: torch.Tensor = None, *args",
            ") -> Any:",
            "rep = self.dropout(embedded_tokens)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('typed_default_parameter', None), position=5, insert_id=886378)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=identifier, text=seq_lengths), position=0)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=:, text=:), position=1)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=type), position=2)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('=', '='), position=3, insert_id=886379)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('none', 'None'), position=4, insert_id=886380)",
            "Delete(target_node=ASTNode(type=typed_parameter))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 9367,
        "neg_line": [
            "-self, embedded_tokens: torch.Tensor, seq_lengths: torch.Tensor, *args"
        ],
        "pos_line": [
            "+self, embedded_tokens: torch.Tensor, seq_lengths: torch.Tensor = None, *args"
        ],
        "core_change": "-self, embedded_tokens: torch.Tensor, seq_lengths: torch.Tensor, *args +self, embedded_tokens: torch.Tensor, seq_lengths: torch.Tensor = None, *args",
        "core_API": "dropout"
    },
    {
        "commit_hash": "427ac2ccf34be9427bbadb56f6760830de72a191",
        "index": "5735c9f4de..5bf4f86400 100644",
        "commit_message": "[RLlib] fix preprocessor test (#33719)\n\nSigned-off-by: Artur Niederfahrenhorst <artur@anyscale.com>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TestPreprocessors(unittest.TestCase):",
            "if __name__ == \"__main__\":",
            "# Call this on startup to prevet TF from complaining further down the line about",
            "# not calling in on startup.",
            "-    tf.enable_eager_execution()",
            "import pytest",
            "import sys"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', ''), position=0, insert_id=2131234)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=enable_eager_execution))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 9370,
        "neg_line": [
            "-tf.enable_eager_execution()"
        ],
        "pos_line": [],
        "core_change": "-tf.enable_eager_execution()",
        "core_API": "enable_eager_execution"
    },
    {
        "commit_hash": "37e98945728f9961fd33d598a954358d79988c28",
        "index": "48655fdd..ffc46fa7 100644",
        "commit_message": "fix flake8 style in tensorpack/\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class QueueInputTrainer(SimpleFeedfreeTrainer):",
            "\"\"\"",
            "config.data = QueueInput(config.dataset, input_queue)",
            "if predict_tower is not None:",
            "-            logger.warn(\"[Deprecated] Argument `predict_tower` is deprecated for trainer. Use TrainConfig.predict_tower instead!\")",
            "+            logger.warn(\"[Deprecated] Argument `predict_tower` is deprecated for trainer. \"",
            "+                        \"Use TrainConfig.predict_tower instead!\")",
            "config.predict_tower = predict_tower",
            "assert len(config.tower) == 1, \\",
            "\"QueueInputTrainer doesn't support multigpu! Use Sync/AsyncMultiGPUTrainer instead.\""
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 9373,
        "neg_line": [
            "-logger.warn(\"[Deprecated] Argument `predict_tower` is deprecated for trainer. Use TrainConfig.predict_tower instead!\")"
        ],
        "pos_line": [
            "+logger.warn(\"[Deprecated] Argument `predict_tower` is deprecated for trainer. \"",
            "+\"Use TrainConfig.predict_tower instead!\")"
        ],
        "core_change": "-logger.warn(\"[Deprecated] Argument `predict_tower` is deprecated for trainer. Use TrainConfig.predict_tower instead!\") +logger.warn(\"[Deprecated] Argument `predict_tower` is deprecated for trainer. \" +\"Use TrainConfig.predict_tower instead!\")",
        "core_API": "warn"
    },
    {
        "commit_hash": "63004976de2486d77f9e1d6544e65800f8f82d15",
        "index": "92e49877..19b39857 100644",
        "commit_message": "simplify WGAN. fix bug in viz.\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PostProcessOptimizer(ProxyOptimizer):",
            "for _, var in grads_and_vars:",
            "with self._maybe_colocate(var):",
            "op = self._func(var)",
            "-                    assert isinstance(op, tf.Operation), op",
            "if op is not None:",
            "+                        assert isinstance(op, tf.Operation), op",
            "ops.append(op)",
            "update_op = tf.group(update_op, *ops, name=name)",
            "return update_op"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=2305195)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2305196)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=comparison_operator), position=1)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2305197)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=2305198)",
            "Move(target_node=IN(type=block), node=ASTNode(type=assert_statement), position=0)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block, text=))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 9375,
        "neg_line": [
            "-assert isinstance(op, tf.Operation), op"
        ],
        "pos_line": [
            "+assert isinstance(op, tf.Operation), op"
        ],
        "core_change": "-assert isinstance(op, tf.Operation), op +assert isinstance(op, tf.Operation), op",
        "core_API": "_maybe_colocate"
    },
    {
        "commit_hash": "400c6a98ef943d9896e5f8343b7c92ef55d1f8da",
        "index": "4ba85646..9ab35d11 100644",
        "commit_message": "Upgrade to latest pytorch master (#890)\n\n* Upgrade to latest pytorch master\n\n* fix to double tensor for profiling\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from pyro.distributions import (Bernoulli, Beta, Categorical, Cauchy, Dirichlet,",
            "",
            "",
            "def T(arr):",
            "-    return Variable(torch.Tensor(arr))",
            "+    return Variable(torch.DoubleTensor(arr))",
            "",
            "",
            "TOOL = 'timeit'"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='DoubleTensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9381,
        "neg_line": [
            "-return Variable(torch.Tensor(arr))"
        ],
        "pos_line": [
            "+return Variable(torch.DoubleTensor(arr))"
        ],
        "core_change": "-return Variable(torch.Tensor(arr)) +return Variable(torch.DoubleTensor(arr))",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "1ef8ce27..7a4258a7 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CrfTagger(Model):",
            "class_probabilities[i, j, tag_id] = 1",
            "",
            "for metric in self.metrics.values():",
            "-                metric(class_probabilities, tags, mask.float())",
            "+                metric(class_probabilities, tags, mask)",
            "if self.calculate_span_f1:",
            "-                self._f1_metric(class_probabilities, tags, mask.float())",
            "+                self._f1_metric(class_probabilities, tags, mask)",
            "if metadata is not None:",
            "output[\"words\"] = [x[\"words\"] for x in metadata]",
            "return output"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=mask), position=5)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=6)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=mask), position=5)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=6)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 9387,
        "neg_line": [
            "-metric(class_probabilities, tags, mask.float())",
            "-self._f1_metric(class_probabilities, tags, mask.float())"
        ],
        "pos_line": [
            "+metric(class_probabilities, tags, mask)",
            "+self._f1_metric(class_probabilities, tags, mask)"
        ],
        "core_change": "-metric(class_probabilities, tags, mask.float()) +metric(class_probabilities, tags, mask) -self._f1_metric(class_probabilities, tags, mask.float()) +self._f1_metric(class_probabilities, tags, mask)",
        "core_API": "values"
    },
    {
        "commit_hash": "991731edf0fecf017de8e64d92345252a7a5bc0b",
        "index": "9dbd3e3e..a21a1e9a 100644",
        "commit_message": "Fix failing CUDA tests on dev (#1277)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_dynamic_lr(scheduler, num_steps):",
            "assert opt.state_dict()['param_groups'][0]['lr'] == 0.02",
            "assert opt.state_dict()['param_groups'][0]['initial_lr'] == 0.01",
            "assert abs(pyro.param('loc').item()) > 1e-5",
            "-            assert abs(pyro.param('scale').item()) - 0.5 > 1e-5",
            "+            assert abs(pyro.param('scale').item() - 0.5) > 1e-5",
            "",
            "",
            "@pytest.mark.parametrize('factory', [optim.Adam, optim.ClippedAdam, optim.RMSprop, optim.SGD])"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Move(target_node=ASTNode(type=comparison_operator), node=ASTNode(type=call), position=0)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=binary_operator), position=1)",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=call), position=0)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 9389,
        "neg_line": [
            "-assert abs(pyro.param('scale').item()) - 0.5 > 1e-5"
        ],
        "pos_line": [
            "+assert abs(pyro.param('scale').item() - 0.5) > 1e-5"
        ],
        "core_change": "-assert abs(pyro.param('scale').item()) - 0.5 > 1e-5 +assert abs(pyro.param('scale').item() - 0.5) > 1e-5",
        "core_API": "state_dict"
    },
    {
        "commit_hash": "e855844e40060383d25746626f1e5927b85ba71a",
        "index": "5b7422f3a..16fee0f46 100644",
        "commit_message": "Adapt image datasets (#3362)\n\n* Initial commit\n\n* Add basic decoding\n\n* Replace features.Audio with Audio\n\n* Add Image to package reference\n\n* Use np.array\n\n* Update error msg\n\n* Add mode and channel decoding\n\n* Fix return value\n\n* Finish decoding\n\n* Make CI happy\n\n* Some more fixes\n\n* Minor doc fix\n\n* Remove animated option\n\n* Pin version\n\n* Remove unused imports in setup.py\n\n* Add vision requirements to setup.py\n\n* Add initial tests\n\n* Delete other formats\n\n* Make Image feature hashable\n\n* Add more tests\n\n* Support numpy array in alter data check in TypedSequence\n\n* Fix TypedSequence converion\n\n* Finish tests\n\n* Update Image - add ImageExtensionType and supporting functions\n\n* Update encoding functions\n\n* Add support in TypedSequence for ImageExtensionType\n\n* Add tests\n\n* Remove unused import\n\n* Fix doc and style\n\n* Fix doc indentation\n\n* Improve comment\n\n* Return single image instead of dict\n\n* Return PIL Image and not dict\n\n* Encode dict\n\n* Update tests\n\n* Style\n\n* np.ndarray encoding/decoding\n\n* Minor improvements\n\n* PIL Image support in cast_to_python_objects\n\n* Test cast\n\n* Doc fix\n\n* Extension type fixes\n\n* Style\n\n* Use types_mapper in Dataset.to_pandas\n\n* Add pandas extension array for image type\n\n* Update tests\n\n* image type inference\n\n* Remvoe cast_to_python test after Quentin's change\n\n* Improve tests\n\n* Add storage type\n\n* Improve tests\n\n* Test map that returns np.ndarray\n\n* Rename functions\n\n* Add streaming test\n\n* Use image struct in all situations\n\n* Update src/datasets/features/image.py - encode_example type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Update src/datasets/features/image.py -list_image_compression_formats type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Use Image feature in ImageClassification task\n\n* Adapt cats_vs_dogs\n\n* Adapt beans\n\n* Adapt cifar10\n\n* Adapt cifar100\n\n* Add task templates to cifar10 and cifar100\n\n* Adapt FashionMNIST\n\n* Adapt food101\n\n* Adapt mnist datasets\n\n* Adapt head_qa\n\n* Update example in head_qa readme\n\n* Update head_qa dummy data\n\n* Fix streaming in beans and cats_vs_dogs\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Food101(datasets.GeneratorBasedBuilder):",
            "if file_path[len(_IMAGES_DIR) : -len(\".jpg\")] in files_to_keep:",
            "label = file_path.split(\"/\")[2]",
            "yield file_path, {",
            "-                        \"image\": {\"filename\": file_path.split(\"/\")[-1], \"data\": file_obj.read()},",
            "+                        \"image\": {\"path\": file_path, \"bytes\": file_obj.read()},",
            "\"label\": label,",
            "}"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"filename\"), value='\"path\"')",
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=identifier, text=file_path), position=2)",
            "Update(target_node=ASTNode(type=string, text=\"data\"), value='\"bytes\"')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=split))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"/\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 9406,
        "neg_line": [
            "-\"image\": {\"filename\": file_path.split(\"/\")[-1], \"data\": file_obj.read()},"
        ],
        "pos_line": [
            "+\"image\": {\"path\": file_path, \"bytes\": file_obj.read()},"
        ],
        "core_change": "-\"image\": {\"filename\": file_path.split(\"/\")[-1], \"data\": file_obj.read()}, +\"image\": {\"path\": file_path, \"bytes\": file_obj.read()},",
        "core_API": "split"
    },
    {
        "commit_hash": "4185a222dc5c4166ce84a9403337b326eeae4b91",
        "index": "45c0ef3f..a6b0486c 100644",
        "commit_message": "fix some errors\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ParamRestore(SessionInit):",
            "self.prms = param_dict",
            "",
            "def init(self, sess):",
            "+        sess.run(tf.initialize_all_variables())",
            "variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)",
            "var_dict = dict([v.name, v] for v in variables)",
            "for name, value in self.prms.iteritems():"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=2321857)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2321858)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2321859)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2321860)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2321861)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sess'), position=0, insert_id=2321862)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2321863)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'run'), position=2, insert_id=2321864)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2321865)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=2321866)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2321867)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2321868)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2321869)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2321870)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2321871)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'initialize_all_variables'), position=2, insert_id=2321872)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2321873)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2321874)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 19,
        "number": 9408,
        "neg_line": [],
        "pos_line": [
            "+sess.run(tf.initialize_all_variables())"
        ],
        "core_change": "+sess.run(tf.initialize_all_variables())",
        "core_API": "run"
    },
    {
        "commit_hash": "c74dc58f8b38bb947b903d3f92efbd8d1cd675ce",
        "index": "067b40d4e..7a0a3cec1 100644",
        "commit_message": "[RLlib] Fix `use_lstm` flag for ModelV2 (w/o ModelV1 wrapping) and add it for PyTorch. (#8734)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def build_vtrace_loss(policy, model, dist_class, train_batch):",
            "values = model.value_function()",
            "",
            "if policy.is_recurrent():",
            "-        max_seq_len = torch.max(train_batch[\"seq_lens\"]) - 1",
            "-        mask = sequence_mask(train_batch[\"seq_lens\"], max_seq_len)",
            "-        mask = torch.reshape(mask, [-1])",
            "+        max_seq_len = torch.max(train_batch[\"seq_lens\"])",
            "+        mask_orig = sequence_mask(train_batch[\"seq_lens\"], max_seq_len)",
            "+        mask = torch.reshape(mask_orig, [-1])",
            "else:",
            "mask = torch.ones_like(rewards)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=mask), value='mask_orig')",
            "Update(target_node=ASTNode(type=identifier, text=mask), value='mask_orig')",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 6,
        "number": 9409,
        "neg_line": [
            "-max_seq_len = torch.max(train_batch[\"seq_lens\"]) - 1",
            "-mask = sequence_mask(train_batch[\"seq_lens\"], max_seq_len)",
            "-mask = torch.reshape(mask, [-1])"
        ],
        "pos_line": [
            "+max_seq_len = torch.max(train_batch[\"seq_lens\"])",
            "+mask_orig = sequence_mask(train_batch[\"seq_lens\"], max_seq_len)",
            "+mask = torch.reshape(mask_orig, [-1])"
        ],
        "core_change": "-max_seq_len = torch.max(train_batch[\"seq_lens\"]) - 1 -mask = sequence_mask(train_batch[\"seq_lens\"], max_seq_len) -mask = torch.reshape(mask, [-1]) +max_seq_len = torch.max(train_batch[\"seq_lens\"]) +mask_orig = sequence_mask(train_batch[\"seq_lens\"], max_seq_len) +mask = torch.reshape(mask_orig, [-1])",
        "core_API": "value_function"
    },
    {
        "commit_hash": "8d6487f3cbe89bb6e32f82fc9f04df6ce001ef24",
        "index": "464d5ef3..6b23aca3 100644",
        "commit_message": "Fix some failing tests (#1041)\n\n* up\n\n* up\n\n* up\n\n* Update src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py\n\n* Apply suggestions from code review\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class StableDiffusionPipelineIntegrationTests(unittest.TestCase):",
            "pipeline_id = \"CompVis/stable-diffusion-v1-4\"",
            "prompt = \"Andromeda galaxy in a bottle\"",
            "",
            "-        pipeline = StableDiffusionPipeline.from_pretrained(",
            "-            pipeline_id, revision=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\"",
            "-        )",
            "+        pipeline = StableDiffusionPipeline.from_pretrained(pipeline_id, revision=\"fp16\", torch_dtype=torch.float16)",
            "pipeline.enable_attention_slicing(1)",
            "pipeline.enable_sequential_cpu_offload()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=device_map))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"auto\"))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 5,
        "number": 9411,
        "neg_line": [
            "-pipeline = StableDiffusionPipeline.from_pretrained(",
            "-pipeline_id, revision=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\"",
            "-)"
        ],
        "pos_line": [
            "+pipeline = StableDiffusionPipeline.from_pretrained(pipeline_id, revision=\"fp16\", torch_dtype=torch.float16)"
        ],
        "core_change": "-pipeline = StableDiffusionPipeline.from_pretrained( -pipeline_id, revision=\"fp16\", torch_dtype=torch.float16, device_map=\"auto\" -) +pipeline = StableDiffusionPipeline.from_pretrained(pipeline_id, revision=\"fp16\", torch_dtype=torch.float16)",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "eb66e0d01ecded59160ab8d5c0e8e302eb1836a1",
        "index": "7bd71a87..8832efea 100644",
        "commit_message": "add pat change (#3414)\n\n* add pat change\n\n* fix grid roi head\n\n* fix comments\n\n* clean\n\n* revert change\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def collect_env():",
            "env_info['GCC'] = gcc",
            "",
            "env_info['PyTorch'] = torch.__version__",
            "-    env_info['PyTorch compiling details'] = torch.__config__.show()",
            "+    env_info['PyTorch compiling details'] = get_build_config()",
            "",
            "env_info['TorchVision'] = torchvision.__version__"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='get_build_config')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=__config__))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=show))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 9412,
        "neg_line": [
            "-env_info['PyTorch compiling details'] = torch.__config__.show()"
        ],
        "pos_line": [
            "+env_info['PyTorch compiling details'] = get_build_config()"
        ],
        "core_change": "-env_info['PyTorch compiling details'] = torch.__config__.show() +env_info['PyTorch compiling details'] = get_build_config()",
        "core_API": "show"
    },
    {
        "commit_hash": "3bb4b8677181ea4bb37e108e70d5d5ac70c26591",
        "index": "9c4213a6..d3b09ca4 100644",
        "commit_message": "refactor: fix import (#1769)\n\n\n",
        "file": "BentoML.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "class KerasModel(ModelArtifact):",
            "",
            "if os.path.isfile(cls.__get_model_json__path(path)):",
            "# load keras model via json and weights since json file are in path",
            "-            with cls.sess.as_default():",
            "+            with cls.sess.as_default():  # pylint: disable=not-context-manager",
            "with open(cls.__get_model_json__path(path), 'r') as json_file:",
            "model_json = json_file.read()",
            "obj = tfk.models.model_from_json("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 9426,
        "neg_line": [
            "-with cls.sess.as_default():"
        ],
        "pos_line": [
            "+with cls.sess.as_default():  # pylint: disable=not-context-manager"
        ],
        "core_change": "-with cls.sess.as_default(): +with cls.sess.as_default():  # pylint: disable=not-context-manager",
        "core_API": "isfile"
    },
    {
        "commit_hash": "18a09cb2a331fadca01d3215eca1f3173a482224",
        "index": "8577f34ec..2981bec94 100644",
        "commit_message": "add pytest fixture as workaround for set_grad_enabled\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_lm():",
            "# numpy.testing.assert_equal(rnnlm_ch.predictor.lo.W.data, rnnlm_th.predictor.lo.weight.data.numpy())",
            "",
            "# test prediction equality",
            "-    if torch.__version__.startswith(\"0.3.\"):",
            "+    if torch_is_old:",
            "x = torch.autograd.Variable(",
            "torch.from_numpy(numpy.random.randint(n_vocab, size=(batchsize))),",
            "volatile=True).long()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='torch_is_old')",
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=identifier, text=torch), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=__version__))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=startswith))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"0.3.\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 9428,
        "neg_line": [
            "-if torch.__version__.startswith(\"0.3.\"):"
        ],
        "pos_line": [
            "+if torch_is_old:"
        ],
        "core_change": "-if torch.__version__.startswith(\"0.3.\"): +if torch_is_old:",
        "core_API": "assert_equal"
    },
    {
        "commit_hash": "8e68b6b75f2cdf7936786b31fb426086c61338ae",
        "index": "355a3a6..2002ada 100644",
        "commit_message": "Fix: Shape must not have repeated dimensions\n\nUse differently named singletons instead\n\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def conv1d(x, scope, nf, *, w_init_stdev=0.02, params=None, scale=False):",
            "c = mtf.layers.conv1d(x, nf, name=scope, filter_size=1, stride=1,",
            "filter_initializer=tf.random_normal_initializer(stddev=w_init_stdev, dtype=dt))",
            "with tf.variable_scope(scope):",
            "-        singleton = mtf.Dimension('singleton', 1)",
            "+        singletona = mtf.Dimension('singletona', 1)",
            "+        singletonb = mtf.Dimension('singletonb', 1)",
            "",
            "b = mtf.get_variable(x.mesh, 'b', [nf], initializer=tf.constant_initializer(0, dtype=tf.bfloat16), dtype=dt)",
            "# NWC",
            "-        b = mtf.reshape(b, [singleton, singleton, nf])",
            "+        b = mtf.reshape(b, [singletona, singletonb, nf])",
            "",
            "c += b",
            "return c"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=1, insert_id=1942951)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1942952)",
            "Update(target_node=ASTNode(type=identifier, text=singleton), value='singletona')",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'singletonb'), position=0, insert_id=1942953)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1942954)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1942955)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1942956)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1942957)",
            "Update(target_node=ASTNode(type=identifier, text=singleton), value='singletona')",
            "Update(target_node=ASTNode(type=identifier, text=singleton), value='singletonb')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'mtf'), position=0, insert_id=1942958)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1942959)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Dimension'), position=2, insert_id=1942960)",
            "Update(target_node=ASTNode(type=string, text='singleton'), value=\"'singletona'\")",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1942961)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'singletonb'\"), position=1, insert_id=1942962)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1942963)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=3, insert_id=1942964)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1942965)"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 9430,
        "neg_line": [
            "-singleton = mtf.Dimension('singleton', 1)",
            "-b = mtf.reshape(b, [singleton, singleton, nf])"
        ],
        "pos_line": [
            "+singletona = mtf.Dimension('singletona', 1)",
            "+singletonb = mtf.Dimension('singletonb', 1)",
            "+b = mtf.reshape(b, [singletona, singletonb, nf])"
        ],
        "core_change": "-singleton = mtf.Dimension('singleton', 1) +singletona = mtf.Dimension('singletona', 1) +singletonb = mtf.Dimension('singletonb', 1) -b = mtf.reshape(b, [singleton, singleton, nf]) +b = mtf.reshape(b, [singletona, singletonb, nf])",
        "core_API": "conv1d"
    },
    {
        "commit_hash": "01a169fc9b71bfbfe597071425f1f71148cb13a2",
        "index": "6ba7d538..e0045e4a 100644",
        "commit_message": "Fix jit compilation of HMM examples (#1596)\n\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Dice(object):",
            "prob = probs[key]",
            "prob._pyro_dims = queries[key]._pyro_dims",
            "mask = prob > 0",
            "-                    if not mask.all():",
            "+                    if torch._C._get_tracing_state() or not mask.all():",
            "mask._pyro_dims = prob._pyro_dims",
            "cost, prob, mask = packed.broadcast_all(cost, prob, mask)",
            "prob = prob[mask]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=727230)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=727231)",
            "Insert(target_node=IN(type=boolean_operator), node=('or', 'or'), position=1, insert_id=727232)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=not_operator), position=2)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=727233)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=727234)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=727235)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=727236)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_get_tracing_state'), position=2, insert_id=727237)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=727238)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=727239)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=727240)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=727241)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_C'), position=2, insert_id=727242)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 9438,
        "neg_line": [
            "-if not mask.all():"
        ],
        "pos_line": [
            "+if torch._C._get_tracing_state() or not mask.all():"
        ],
        "core_change": "-if not mask.all(): +if torch._C._get_tracing_state() or not mask.all():",
        "core_API": "all"
    },
    {
        "commit_hash": "bfe3db7678ccbb8d670cd7b3755fbb78f4e49118",
        "index": "90a37dd0d1..8f2b3d84a3 100644",
        "commit_message": "modified logspace to also include the dtype argument, fixing the failing unit test.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def logspace(",
            "base=10.0,",
            "axis=None,",
            "*,",
            "+    dtype: tf.DType,",
            "device: str,",
            "out: Union[tf.Tensor, tf.Variable] = None",
            "):",
            "-    power_seq = linspace(start, stop, num, axis, dtype=None, device=device)",
            "+    power_seq = ivy.linspace(start, stop, num, axis, dtype=dtype, device=device)",
            "return base**power_seq"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('typed_parameter', None), position=7, insert_id=2005465)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=2005466)",
            "Insert(target_node=IN(type=typed_parameter), node=('identifier', 'dtype'), position=0, insert_id=2005467)",
            "Insert(target_node=IN(type=typed_parameter), node=(':', ':'), position=1, insert_id=2005468)",
            "Insert(target_node=IN(type=typed_parameter), node=('type', None), position=2, insert_id=2005469)",
            "Insert(target_node=IN(type=type), node=('attribute', None), position=0, insert_id=2005470)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2005471)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2005472)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'DType'), position=2, insert_id=2005473)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2005474)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ivy'), position=0, insert_id=2005475)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2005476)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=linspace), position=2)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'dtype'), position=2, insert_id=2005477)",
            "Delete(target_node=ASTNode(type=none, text=None))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 9444,
        "neg_line": [
            "-power_seq = linspace(start, stop, num, axis, dtype=None, device=device)"
        ],
        "pos_line": [
            "+dtype: tf.DType,",
            "+power_seq = ivy.linspace(start, stop, num, axis, dtype=dtype, device=device)"
        ],
        "core_change": "+dtype: tf.DType, -power_seq = linspace(start, stop, num, axis, dtype=None, device=device) +power_seq = ivy.linspace(start, stop, num, axis, dtype=dtype, device=device)",
        "core_API": "linspace"
    },
    {
        "commit_hash": "7ce7fb01e035a7ba8ca9cb35784cd75cca3d99fd",
        "index": "fb3c4d33..92d8bcdd 100644",
        "commit_message": "fix for live progress breaking lowvram and medvram optimizations\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def check_progress_call():",
            "else:",
            "preview_visibility = gr_show(True)",
            "",
            "-    shared.state.current_progress_index += 1",
            "-",
            "return f\"<span style='display: none'>{time.time()}</span><p>{progressbar}</p>\", preview_visibility, image"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=shared))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=state))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=current_progress_index))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=+=, text=+=))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=augmented_assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 9445,
        "neg_line": [
            "-shared.state.current_progress_index += 1",
            "-"
        ],
        "pos_line": [],
        "core_change": "-shared.state.current_progress_index += 1 -",
        "core_API": "time"
    },
    {
        "commit_hash": "76fe1b6be751068ba31df371c9e874498629f38a",
        "index": "830f4aa9..e6bc3405 100755",
        "commit_message": "update cifar number & fix multigpu restore bug\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def get_config():",
            "optimizer=tf.train.MomentumOptimizer(lr, 0.9),",
            "callbacks=Callbacks([",
            "StatPrinter(),",
            "-            PeriodicSaver(),",
            "-            ValidationError(dataset_test, prefix='test'),",
            "+            ModelSaver(),",
            "+            ClassificationError(dataset_test, prefix='validation'),",
            "ScheduledHyperParamSetter('learning_rate',",
            "[(1, 0.1), (20, 0.01), (33, 0.001), (60, 0.0001)])",
            "]),"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=PeriodicSaver), value='ModelSaver')",
            "Update(target_node=ASTNode(type=identifier, text=ValidationError), value='ClassificationError')",
            "Update(target_node=ASTNode(type=string, text='test'), value=\"'validation'\")"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 9448,
        "neg_line": [
            "-PeriodicSaver(),",
            "-ValidationError(dataset_test, prefix='test'),"
        ],
        "pos_line": [
            "+ModelSaver(),",
            "+ClassificationError(dataset_test, prefix='validation'),"
        ],
        "core_change": "-PeriodicSaver(), -ValidationError(dataset_test, prefix='test'), +ModelSaver(), +ClassificationError(dataset_test, prefix='validation'),",
        "core_API": "MomentumOptimizer"
    },
    {
        "commit_hash": "9a49b3c473fbbef6df1e0f245f8e99e36f6fec6a",
        "index": "26ba5d57..21cc3a1f 100644",
        "commit_message": "[FIX] Soft-argmax test fixes, renaming and enables jit (#553)\n\n* Rename spatial_softargmax_2d to spatial_expectation_2d\n\n* Add JIT to some core DSNT operations\n\n* Add jit.script support to render_gaussian_2d\n\n* Rename DSNT functions from *_2d to *2d\n\n* Fix jit.script for meshgrid functions by removing incorrect usage of Optional\n\n* Add check_is_tensor function\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def spatial_soft_argmax2d(",
            ">>> coords = kornia.spatial_soft_argmax2d(input, False)",
            "tensor([[[1.0000, 1.0000]]])",
            "\"\"\"",
            "-    input_soft: torch.Tensor = dsnt.spatial_softmax_2d(input, temperature)",
            "-    output: torch.Tensor = dsnt.spatial_softargmax_2d(input_soft,",
            "-                                                      normalized_coordinates)",
            "+    input_soft: torch.Tensor = dsnt.spatial_softmax2d(input, temperature)",
            "+    output: torch.Tensor = dsnt.spatial_expectation2d(input_soft, normalized_coordinates)",
            "return output"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=ERROR), position=8)",
            "Insert(target_node=ASTNode(type=ERROR), node=('attribute', None), position=4, insert_id=441558)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=441559)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=441560)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=441561)",
            "Update(target_node=ASTNode(type=identifier, text=spatial_softmax_2d), value='spatial_softmax2d')",
            "Update(target_node=ASTNode(type=identifier, text=spatial_softargmax_2d), value='spatial_expectation2d')",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 11,
        "number": 9449,
        "neg_line": [
            "-input_soft: torch.Tensor = dsnt.spatial_softmax_2d(input, temperature)",
            "-output: torch.Tensor = dsnt.spatial_softargmax_2d(input_soft,",
            "-normalized_coordinates)"
        ],
        "pos_line": [
            "+input_soft: torch.Tensor = dsnt.spatial_softmax2d(input, temperature)",
            "+output: torch.Tensor = dsnt.spatial_expectation2d(input_soft, normalized_coordinates)"
        ],
        "core_change": "-input_soft: torch.Tensor = dsnt.spatial_softmax_2d(input, temperature) -output: torch.Tensor = dsnt.spatial_softargmax_2d(input_soft, -normalized_coordinates) +input_soft: torch.Tensor = dsnt.spatial_softmax2d(input, temperature) +output: torch.Tensor = dsnt.spatial_expectation2d(input_soft, normalized_coordinates)",
        "core_API": "spatial_soft_argmax2d"
    },
    {
        "commit_hash": "93dbc1ce18613747fa981918d277f5bb843f752b",
        "index": "4f5579ea04..bc6a364a60 100644",
        "commit_message": "fix dtype, device in `sum`, `prod`, `to_dev` (#1358)\n\n* fix dtype, device in `sum`, `prod`, `to_dev`\n\n* make `device` have `None` as default\n\n* add `dtype = ivy.as_native_dtype(dtype)` and make `copy` positional\n\n* `to_dev` conform to array API\n\n* `astype` fixes to signature\n\n* black\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def unique_values(x: torch.Tensor) -> torch.Tensor:",
            "return ret",
            "",
            "",
            "-def unique_counts(",
            "-    x: torch.Tensor",
            "-) -> Tuple[torch.Tensor, torch.Tensor]:",
            "+def unique_counts(x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:",
            "v, c = torch.unique(torch.reshape(x, [-1]), return_counts=True)",
            "nan_idx = torch.where(torch.isnan(v))",
            "c[nan_idx] = 1"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 9467,
        "neg_line": [
            "-def unique_counts(",
            "-x: torch.Tensor",
            "-) -> Tuple[torch.Tensor, torch.Tensor]:"
        ],
        "pos_line": [
            "+def unique_counts(x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:"
        ],
        "core_change": "-def unique_counts( -x: torch.Tensor -) -> Tuple[torch.Tensor, torch.Tensor]: +def unique_counts(x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:",
        "core_API": "unique"
    },
    {
        "commit_hash": "706014843879fc7792c9743de1cac75668c16268",
        "index": "65439e4cc..9dee3ac68 100644",
        "commit_message": "fix\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Stft(torch.nn.Module, InversibleInterface):",
            "pad = self.n_fft // 2",
            "ilens = ilens + 2 * pad",
            "",
            "-            olens = (",
            "-                torch.div((ilens - self.n_fft), self.hop_length, rounding_mode=\"floor\")",
            "-                + 1",
            "-            )",
            "+            olens = (ilens - self.n_fft) // self.hop_length + 1",
            "output.masked_fill_(make_pad_mask(olens, output, 1), 0.0)",
            "else:",
            "olens = None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('binary_operator', None), position=2, insert_id=119825)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=binary_operator), position=0)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=+, text=+), position=1)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=integer, text=1), position=2)",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=parenthesized_expression), position=0)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('//', '//'), position=1, insert_id=119826)",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=div))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=rounding_mode))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"floor\"))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))"
        ],
        "plus_line": 1,
        "minus_line": 4,
        "AST_diff_line": 24,
        "number": 9472,
        "neg_line": [
            "-olens = (",
            "-torch.div((ilens - self.n_fft), self.hop_length, rounding_mode=\"floor\")",
            "-+ 1",
            "-)"
        ],
        "pos_line": [
            "+olens = (ilens - self.n_fft) // self.hop_length + 1"
        ],
        "core_change": "-olens = ( -torch.div((ilens - self.n_fft), self.hop_length, rounding_mode=\"floor\") -+ 1 -) +olens = (ilens - self.n_fft) // self.hop_length + 1",
        "core_API": "div"
    },
    {
        "commit_hash": "8831c6880390e84494b34fc14f938c8a1c9654eb",
        "index": "6962481ad..b2f8432d3 100755",
        "commit_message": "fixing various parts of model conversion, loading and weights sharing\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def convert_transfo_xl_checkpoint_to_pytorch(tf_checkpoint_path,",
            "model = TransfoXLModel(config)",
            "",
            "# Build TF to PyTorch weights loading map",
            "-        tf_to_pt_map = build_tf_to_pytorch_map(model.transformer, config)",
            "+        tf_to_pt_map = build_tf_to_pytorch_map(model, config)",
            "",
            "# Load weights from TF model",
            "init_vars = tf.train.list_variables(tf_path)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=model), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=transformer))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 9473,
        "neg_line": [
            "-tf_to_pt_map = build_tf_to_pytorch_map(model.transformer, config)"
        ],
        "pos_line": [
            "+tf_to_pt_map = build_tf_to_pytorch_map(model, config)"
        ],
        "core_change": "-tf_to_pt_map = build_tf_to_pytorch_map(model.transformer, config) +tf_to_pt_map = build_tf_to_pytorch_map(model, config)",
        "core_API": "list_variables"
    },
    {
        "commit_hash": "7f1f768d528c87c45e777fe90041b5a2f312c8b4",
        "index": "4de73ec19b..1715c51657 100644",
        "commit_message": "fix lint failure\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def inv(",
            "return ret",
            "else:",
            "cofactor = tf.transpose(tf.linalg.inv(x)) * tf.linalg.det(x)",
            "-            inverse = tf.math.multiply(tf.math.divide(",
            "-                1, tf.linalg.det(x)), tf.transpose(cofactor))",
            "+            inverse = tf.math.multiply(",
            "+                tf.math.divide(1, tf.linalg.det(x)), tf.transpose(cofactor)",
            "+            )",
            "ret = inverse",
            "return ret"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 9475,
        "neg_line": [
            "-inverse = tf.math.multiply(tf.math.divide(",
            "-1, tf.linalg.det(x)), tf.transpose(cofactor))"
        ],
        "pos_line": [
            "+inverse = tf.math.multiply(",
            "+tf.math.divide(1, tf.linalg.det(x)), tf.transpose(cofactor)",
            "+)"
        ],
        "core_change": "-inverse = tf.math.multiply(tf.math.divide( -1, tf.linalg.det(x)), tf.transpose(cofactor)) +inverse = tf.math.multiply( +tf.math.divide(1, tf.linalg.det(x)), tf.transpose(cofactor) +)",
        "core_API": "transpose"
    },
    {
        "commit_hash": "d08bc09495a289519a774e881f399cf864178f9b",
        "index": "29a23dca..7e55352c 100644",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def triangulate_points(",
            "# 1. Solve the system Ax=0 with smallest eigenvalue",
            "# 2. Return homogeneous coordinates",
            "",
            "-    U, S, V = torch.svd(X)",
            "+    _, _, V = torch.svd(X)",
            "",
            "points3d_h = V[..., -1]",
            "points3d: torch.Tensor = kornia.convert_points_from_homogeneous(points3d_h)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=U), value='_')",
            "Update(target_node=ASTNode(type=identifier, text=S), value='_')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 9477,
        "neg_line": [
            "-U, S, V = torch.svd(X)"
        ],
        "pos_line": [
            "+_, _, V = torch.svd(X)"
        ],
        "core_change": "-U, S, V = torch.svd(X) +_, _, V = torch.svd(X)",
        "core_API": "svd"
    },
    {
        "commit_hash": "6ba874b017f47949f5b4107b70c12bdd96b410b9",
        "index": "4295ad2d..e3898eef 100644",
        "commit_message": "fix gpu test errors\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def match_snn(desc1: torch.Tensor, desc2: torch.Tensor,",
            "match_dists = ratio[mask]",
            "idxs_in1 = torch.arange(0, idxs_in_2.size(0), device=dm.device)[mask]",
            "idxs_in_2 = idxs_in_2[:, 0][mask]",
            "-    matches_idxs = torch.cat([idxs_in1.view(-1, 1), idxs_in_2.cpu().view(-1, 1)], dim=1)",
            "+    matches_idxs = torch.cat([idxs_in1.view(-1, 1), idxs_in_2.view(-1, 1)], dim=1)",
            "return match_dists.view(-1, 1), matches_idxs.view(-1, 2)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=attribute), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=cpu))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 9480,
        "neg_line": [
            "-matches_idxs = torch.cat([idxs_in1.view(-1, 1), idxs_in_2.cpu().view(-1, 1)], dim=1)"
        ],
        "pos_line": [
            "+matches_idxs = torch.cat([idxs_in1.view(-1, 1), idxs_in_2.view(-1, 1)], dim=1)"
        ],
        "core_change": "-matches_idxs = torch.cat([idxs_in1.view(-1, 1), idxs_in_2.cpu().view(-1, 1)], dim=1) +matches_idxs = torch.cat([idxs_in1.view(-1, 1), idxs_in_2.view(-1, 1)], dim=1)",
        "core_API": "arange"
    },
    {
        "commit_hash": "458c56026fd5d2d1a943c3e8d55b9102894820de",
        "index": "72a68781bc..90946175c1 100644",
        "commit_message": "small fix for solve with torch backend.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "slogdet.support_native_out = True",
            "",
            "",
            "def solve(",
            "-    x1: torch.Tensor, x2: torch.Tensor, *, out: Optional[torch.Tensor]",
            "+    x1: torch.Tensor,",
            "+    x2: torch.Tensor,",
            "+    *,",
            "+    out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "if x1.dtype != torch.float:",
            "x1 = x1.type(torch.float)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('typed_default_parameter', None), position=7, insert_id=348103)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=348104)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=identifier, text=out), position=0)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=:, text=:), position=1)",
            "Move(target_node=IN(type=typed_default_parameter), node=ASTNode(type=type), position=2)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('=', '='), position=3, insert_id=348105)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('none', 'None'), position=4, insert_id=348106)",
            "Delete(target_node=ASTNode(type=typed_parameter))"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 9482,
        "neg_line": [
            "-x1: torch.Tensor, x2: torch.Tensor, *, out: Optional[torch.Tensor]"
        ],
        "pos_line": [
            "+x1: torch.Tensor,",
            "+x2: torch.Tensor,",
            "+*,",
            "+out: Optional[torch.Tensor] = None,"
        ],
        "core_change": "-x1: torch.Tensor, x2: torch.Tensor, *, out: Optional[torch.Tensor] +x1: torch.Tensor, +x2: torch.Tensor, +*, +out: Optional[torch.Tensor] = None,",
        "core_API": "type"
    },
    {
        "commit_hash": "6ba2231d7227825dfb76e9df161824d04234e69f",
        "index": "4c8bef0e..a5518442 100644",
        "commit_message": "Reproducibility 3/3 (#1924)\n\n* make tests deterministic\n\n* run slow tests\n\n* prepare for testing\n\n* finish\n\n* refactor\n\n* add print statements\n\n* finish more\n\n* correct some test failures\n\n* more fixes\n\n* set up to correct tests\n\n* more corrections\n\n* up\n\n* fix more\n\n* more prints\n\n* add\n\n* up\n\n* up\n\n* up\n\n* uP\n\n* uP\n\n* more fixes\n\n* uP\n\n* up\n\n* up\n\n* up\n\n* up\n\n* fix more\n\n* up\n\n* up\n\n* clean tests\n\n* up\n\n* up\n\n* up\n\n* more fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* make\n\n* correct\n\n* finish\n\n* finish\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class VersatileDiffusionImageVariationPipelineIntegrationTests(unittest.TestCase",
            "image_slice = image[0, 253:256, 253:256, -1]",
            "",
            "assert image.shape == (1, 512, 512, 3)",
            "-        expected_slice = np.array([0.1205, 0.1914, 0.2289, 0.0883, 0.1595, 0.1683, 0.0703, 0.1493, 0.1298])",
            "+        expected_slice = np.array([0.0441, 0.0469, 0.0507, 0.0575, 0.0632, 0.0650, 0.0865, 0.0909, 0.0945])",
            "+",
            "assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=0.1205), value='0.0441')",
            "Update(target_node=ASTNode(type=float, text=0.1914), value='0.0469')",
            "Update(target_node=ASTNode(type=float, text=0.2289), value='0.0507')",
            "Update(target_node=ASTNode(type=float, text=0.0883), value='0.0575')",
            "Update(target_node=ASTNode(type=float, text=0.1595), value='0.0632')",
            "Update(target_node=ASTNode(type=float, text=0.1683), value='0.0650')",
            "Update(target_node=ASTNode(type=float, text=0.0703), value='0.0865')",
            "Update(target_node=ASTNode(type=float, text=0.1493), value='0.0909')",
            "Update(target_node=ASTNode(type=float, text=0.1298), value='0.0945')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 9486,
        "neg_line": [
            "-expected_slice = np.array([0.1205, 0.1914, 0.2289, 0.0883, 0.1595, 0.1683, 0.0703, 0.1493, 0.1298])"
        ],
        "pos_line": [
            "+expected_slice = np.array([0.0441, 0.0469, 0.0507, 0.0575, 0.0632, 0.0650, 0.0865, 0.0909, 0.0945])",
            "+"
        ],
        "core_change": "-expected_slice = np.array([0.1205, 0.1914, 0.2289, 0.0883, 0.1595, 0.1683, 0.0703, 0.1493, 0.1298]) +expected_slice = np.array([0.0441, 0.0469, 0.0507, 0.0575, 0.0632, 0.0650, 0.0865, 0.0909, 0.0945]) +",
        "core_API": "array"
    },
    {
        "commit_hash": "e21f89f64c0683f111572b8b9fa38ffff64885f1",
        "index": "31110f3b6..c20377f70 100644",
        "commit_message": "fix nan in full-fp16 label_smoothing eval (#10815)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LabelSmoother:",
            "# will ignore them in any case.",
            "labels.clamp_min_(0)",
            "nll_loss = log_probs.gather(dim=-1, index=labels)",
            "-        smoothed_loss = log_probs.sum(dim=-1, keepdim=True)",
            "+        # works for fp16 input tensor too, by internally upcasting it to fp32",
            "+        smoothed_loss = log_probs.sum(dim=-1, keepdim=True, dtype=torch.float32)",
            "",
            "nll_loss.masked_fill_(padding_mask, 0.0)",
            "smoothed_loss.masked_fill_(padding_mask, 0.0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1217858)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1217859)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1217860)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1217861)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1217862)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1217863)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1217864)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=1217865)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 9490,
        "neg_line": [
            "-smoothed_loss = log_probs.sum(dim=-1, keepdim=True)"
        ],
        "pos_line": [
            "+# works for fp16 input tensor too, by internally upcasting it to fp32",
            "+smoothed_loss = log_probs.sum(dim=-1, keepdim=True, dtype=torch.float32)"
        ],
        "core_change": "-smoothed_loss = log_probs.sum(dim=-1, keepdim=True) +# works for fp16 input tensor too, by internally upcasting it to fp32 +smoothed_loss = log_probs.sum(dim=-1, keepdim=True, dtype=torch.float32)",
        "core_API": "clamp_min_"
    },
    {
        "commit_hash": "7b37b7447091f449cfb895a07445f0a3227ac0b2",
        "index": "61b7dabc28..4ba35a176a 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def fmod(",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "result = tf.math.floormod(x1, x2, name=None)",
            "temp = [result, x1]",
            "-    return tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp, fn_output_signature=result.dtype)",
            "+    return tf.map_fn(",
            "+        lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]),",
            "+        temp,",
            "+        fn_output_signature=result.dtype,",
            "+    )",
            "",
            "",
            "def fmax("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=1974028)"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9497,
        "neg_line": [
            "-return tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp, fn_output_signature=result.dtype)"
        ],
        "pos_line": [
            "+return tf.map_fn(",
            "+lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]),",
            "+temp,",
            "+fn_output_signature=result.dtype,",
            "+)"
        ],
        "core_change": "-return tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp, fn_output_signature=result.dtype) +return tf.map_fn( +lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), +temp, +fn_output_signature=result.dtype, +)",
        "core_API": "floormod"
    },
    {
        "commit_hash": "5f0cccaf8e00fd24a093b40db09cb9aef4266e06",
        "index": "3ae9fcf9..e9ed26b9 100644",
        "commit_message": "fixed tests\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_gae():",
            "loss = model.loss(z, data.train_pos_edge_index, data.train_neg_adj_mask)",
            "assert loss.item() > 0",
            "",
            "-    auc, ap = model.evaluate(z, data.val_pos_edge_index,",
            "-                             data.val_neg_edge_index)",
            "+    auc, ap = model.test(z, data.val_pos_edge_index, data.val_neg_edge_index)",
            "assert auc >= 0 and auc <= 1 and ap >= 0 and ap <= 1"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=evaluate), value='test')"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 9500,
        "neg_line": [
            "-auc, ap = model.evaluate(z, data.val_pos_edge_index,",
            "-data.val_neg_edge_index)"
        ],
        "pos_line": [
            "+auc, ap = model.test(z, data.val_pos_edge_index, data.val_neg_edge_index)"
        ],
        "core_change": "-auc, ap = model.evaluate(z, data.val_pos_edge_index, -data.val_neg_edge_index) +auc, ap = model.test(z, data.val_pos_edge_index, data.val_neg_edge_index)",
        "core_API": "loss"
    },
    {
        "commit_hash": "c7c71d0c42d8952445a7cc423cfd55cfd40193d5",
        "index": "bfb37eba..59317903 100644",
        "commit_message": "Fix typing errors (#2012)\n\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def one_hot(",
            "raise ValueError(\"The number of classes must be bigger than one.\" \" Got: {}\".format(num_classes))",
            "",
            "shape = labels.shape",
            "-    one_hot = torch.zeros((shape[0], num_classes) + shape[1:], device=device, dtype=dtype)",
            "+    one_hot = zeros((shape[0], num_classes) + shape[1:], device=device, dtype=dtype)",
            "",
            "return one_hot.scatter_(1, labels.unsqueeze(1), 1.0) + eps"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=ERROR), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='zeros')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=zeros))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 9505,
        "neg_line": [
            "-one_hot = torch.zeros((shape[0], num_classes) + shape[1:], device=device, dtype=dtype)"
        ],
        "pos_line": [
            "+one_hot = zeros((shape[0], num_classes) + shape[1:], device=device, dtype=dtype)"
        ],
        "core_change": "-one_hot = torch.zeros((shape[0], num_classes) + shape[1:], device=device, dtype=dtype) +one_hot = zeros((shape[0], num_classes) + shape[1:], device=device, dtype=dtype)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "da9c11472a4f1bee91291a6d41bd397f6b38571d",
        "index": "458d394dc..d9a39f997 100644",
        "commit_message": "fix: bug with loading model from scratch\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFModel(Trainable, Inferable, metaclass=TfModelMeta):",
            "print('\\n:: Model saved to {} \\n'.format(self.model_path_.as_posix()))",
            "",
            "def get_checkpoint_state(self):",
            "-        return tf.train.get_checkpoint_state(self.model_path_.as_posix())",
            "+        return tf.train.get_checkpoint_state(self.model_path_.parent)",
            "",
            "@check_path_exists()",
            "def load(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=as_posix), value='parent')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 9513,
        "neg_line": [
            "-return tf.train.get_checkpoint_state(self.model_path_.as_posix())"
        ],
        "pos_line": [
            "+return tf.train.get_checkpoint_state(self.model_path_.parent)"
        ],
        "core_change": "-return tf.train.get_checkpoint_state(self.model_path_.as_posix()) +return tf.train.get_checkpoint_state(self.model_path_.parent)",
        "core_API": "as_posix"
    },
    {
        "commit_hash": "94c02e23dc20399c36295579b94d898a1ac0ca83",
        "index": "f6b0a7d5..e4edbbd5 100644",
        "commit_message": "Fix Torchscript for exclusively binary feature inputs (#2103)\n\n* Fix torchscript binary feature only inputs\n\n* change binary preproc output to float32 dtype\n\n* cleanup after merge\n\n* cleanup after merge\n\n* confirm tests still pass\n\n* revert changes to torchscript\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class _BinaryPreprocessing(torch.nn.Module):",
            "v = torch.stack(v)",
            "",
            "if torch.jit.isinstance(v, torch.Tensor):",
            "-            return v.to(dtype=torch.bool)",
            "+            return v.to(dtype=torch.float32)",
            "",
            "v = [s.strip() for s in v]",
            "if self.should_lower:",
            "v = [s.lower() for s in v]",
            "indices = [self.str2bool.get(s, False) for s in v]",
            "-        return torch.tensor(indices, dtype=torch.bool)",
            "+        return torch.tensor(indices, dtype=torch.float32)",
            "",
            "",
            "class _BinaryPostprocessing(torch.nn.Module):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=bool), value='float32')",
            "Update(target_node=ASTNode(type=identifier, text=bool), value='float32')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 9518,
        "neg_line": [
            "-return v.to(dtype=torch.bool)",
            "-return torch.tensor(indices, dtype=torch.bool)"
        ],
        "pos_line": [
            "+return v.to(dtype=torch.float32)",
            "+return torch.tensor(indices, dtype=torch.float32)"
        ],
        "core_change": "-return v.to(dtype=torch.bool) +return v.to(dtype=torch.float32) -return torch.tensor(indices, dtype=torch.bool) +return torch.tensor(indices, dtype=torch.float32)",
        "core_API": "stack"
    },
    {
        "commit_hash": "52d5066e925d345fbd54ddf98b7cadf027b69d99",
        "index": "24ea7a4..2e59098 100644",
        "commit_message": "Fixed deprecated squeeze_dims argument\n\nPiperOrigin-RevId: 194048599\n\n",
        "file": "hub.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def add_input_distortions(flip_left_right, random_crop, random_scale,",
            "precrop_shape_as_int = tf.cast(precrop_shape, dtype=tf.int32)",
            "precropped_image = tf.image.resize_bilinear(decoded_image_4d,",
            "precrop_shape_as_int)",
            "-  precropped_image_3d = tf.squeeze(precropped_image, squeeze_dims=[0])",
            "+  precropped_image_3d = tf.squeeze(precropped_image, axis=[0])",
            "cropped_image = tf.random_crop(precropped_image_3d,",
            "[input_height, input_width, input_depth])",
            "if flip_left_right:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=squeeze_dims), value='axis')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9524,
        "neg_line": [
            "-precropped_image_3d = tf.squeeze(precropped_image, squeeze_dims=[0])"
        ],
        "pos_line": [
            "+precropped_image_3d = tf.squeeze(precropped_image, axis=[0])"
        ],
        "core_change": "-precropped_image_3d = tf.squeeze(precropped_image, squeeze_dims=[0]) +precropped_image_3d = tf.squeeze(precropped_image, axis=[0])",
        "core_API": "cast"
    },
    {
        "commit_hash": "f162fd00152cf65dd7d99c3de6be90f3ce3f6563",
        "index": "2f1babd2..ac130364 100644",
        "commit_message": "Merge PyG master (#48)\n\n* avoid the 'inf'\n\n* Create GATv2Conv\n\n* Update gatv2_conv.py\n\n* Update gatv2_conv.py\n\n* Update gatv2_conv.py\n\n* More doc\n\n* Update gatv2_conv.py\n\n* Update gatv2_conv.py\n\n* Update README.md\n\n* Update README.md\n\n* Create test_gatv2_conv.py\n\n* Update test_gatv2_conv.py\n\n* fixed gatv2 test\n\n* Fixed types\n\n* Update gatv2_conv.py\n\n* fix types\n\n* remove script folder\n\n* update test CI\n\n* fixed comments\n\n* lint + type\n\n* lint\n\n* Update test_gatv2_conv.py\n\n* Update test_gatv2_conv.py\n\n* fixed gatv2 test+ types\n\n* Update gatv2_conv.py\n\n* Update gatv2_conv.py\n\n* pytorch 1.9.0 support\n\n* typo\n\n* The dataset was introduced in the MUSAE paper\n\nDear Matthias,\n\nThese datasets were introduced in the Multi-scale Attributed Node Embedding paper.\n\nhttps://arxiv.org/abs/1909.13021\n\nBenedek\n\n* Github Dataset\n\n* Github Dataset\n\n* Github Dataset\n\n* Revert \"Merge branch 'master' into master\"\n\nThis reverts commit ef38f142465f736692c7c251a315ada287d7f104, reversing\nchanges made to d86de00a98173653a6158fc40238d34d0fb57cc1.\n\n* clean up\n\n* fix doc\n\n* fix gnn explainer\n\n* remove OGB-LSC\n\nCo-authored-by: Ethanzjp <13810907+Ethanzjp@users.noreply.github.com>\nCo-authored-by: shakedbr <shakedbr@campus.technion.ac.il>\nCo-authored-by: Uri Alon <urialon1@gmail.com>\nCo-authored-by: Shaked Brody <shakedbr@gmail.com>\nCo-authored-by: Benedek Rozemberczki <benedek.rozemberczki@gmail.com>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ShaDowKHopSampler(torch.utils.data.DataLoader):",
            "batch.edge_index = torch.stack([row, col], dim=0)",
            "",
            "for k, v in self.data:",
            "-            if k in ['edge_index', 'adj_t', 'num_nodes']:",
            "+            if k in ['edge_index', 'adj_t']:",
            "continue",
            "if k == 'y' and v.size(0) == self.data.num_nodes:",
            "batch[k] = v[n_id][root_n_id]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=string, text='num_nodes'))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 9531,
        "neg_line": [
            "-if k in ['edge_index', 'adj_t', 'num_nodes']:"
        ],
        "pos_line": [
            "+if k in ['edge_index', 'adj_t']:"
        ],
        "core_change": "-if k in ['edge_index', 'adj_t', 'num_nodes']: +if k in ['edge_index', 'adj_t']:",
        "core_API": "stack"
    },
    {
        "commit_hash": "5a57c54e872d48da575797c4eb9863f1f3dd0659",
        "index": "dcdf60f3a8..f9237aba75 100644",
        "commit_message": "fix tensorflow prod\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def prod(",
            "dtype = tf.uint64",
            "if ivy.exists(out):",
            "return ivy.inplace_update(",
            "-            out, tf.experimental.numpy.prod(x, axis=axis, keepdims=keepdims)",
            "+            out,",
            "+            tf.experimental.numpy.prod(x, axis=axis, dtype=dtype, keepdims=keepdims),",
            ")",
            "else:",
            "return tf.experimental.numpy.prod(x, axis, dtype, keepdims)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2009043)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2009044)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=2009045)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2009046)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2009047)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=2, insert_id=2009048)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 9535,
        "neg_line": [
            "-out, tf.experimental.numpy.prod(x, axis=axis, keepdims=keepdims)"
        ],
        "pos_line": [
            "+out,",
            "+tf.experimental.numpy.prod(x, axis=axis, dtype=dtype, keepdims=keepdims),"
        ],
        "core_change": "-out, tf.experimental.numpy.prod(x, axis=axis, keepdims=keepdims) +out, +tf.experimental.numpy.prod(x, axis=axis, dtype=dtype, keepdims=keepdims),",
        "core_API": "exists"
    },
    {
        "commit_hash": "fe8d55ae1870b6c7c3725f7ac7cc6b1d1ac0d39e",
        "index": "3e108439..1a987aca 100755",
        "commit_message": "Added general model parameter 'variable_noise', plus bug fix in q_model\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PGProbRatioModel(PGModel):",
            "log_probs.append(log_prob)",
            "log_prob = tf.reduce_mean(input_tensor=tf.concat(values=log_probs, axis=1), axis=1)",
            "prob_ratio = tf.exp(x=(log_prob - reference))",
            "-        return tf.reduce_mean(input_tensor=(-prob_ratio * reward), axis=0)",
            "+        return tf.reduce_mean(input_tensor=(prob_ratio * reward), axis=0)",
            "",
            "def get_optimizer_kwargs(self, states, actions, terminal, reward, internals):",
            "kwargs = super(PGProbRatioModel, self).get_optimizer_kwargs("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=identifier, text=prob_ratio), position=0)",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=unary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 9550,
        "neg_line": [
            "-return tf.reduce_mean(input_tensor=(-prob_ratio * reward), axis=0)"
        ],
        "pos_line": [
            "+return tf.reduce_mean(input_tensor=(prob_ratio * reward), axis=0)"
        ],
        "core_change": "-return tf.reduce_mean(input_tensor=(-prob_ratio * reward), axis=0) +return tf.reduce_mean(input_tensor=(prob_ratio * reward), axis=0)",
        "core_API": "append"
    },
    {
        "commit_hash": "3b2cf7b0bc152d826f74a90f5f6b922a8b9f7b21",
        "index": "9524eac..021d5a1 100644",
        "commit_message": "fix for self conditioning in diffusion prior network https://github.com/lucidrains/DALLE2-pytorch/issues/273\n\n",
        "file": "DALLE2-pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DiffusionPriorNetwork(nn.Module):",
            "learned_queries = repeat(self.learned_query, 'd -> b 1 d', b = batch)",
            "",
            "if self.self_cond:",
            "-            learned_queries = torch.cat((image_embed, self_cond), dim = -2)",
            "+            learned_queries = torch.cat((self_cond, learned_queries), dim = -2)",
            "",
            "tokens = torch.cat((",
            "text_encodings,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=tuple), position=3)",
            "Insert(target_node=ASTNode(type=tuple), node=('identifier', 'learned_queries'), position=3, insert_id=62870)",
            "Delete(target_node=ASTNode(type=identifier, text=image_embed))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 9555,
        "neg_line": [
            "-learned_queries = torch.cat((image_embed, self_cond), dim = -2)"
        ],
        "pos_line": [
            "+learned_queries = torch.cat((self_cond, learned_queries), dim = -2)"
        ],
        "core_change": "-learned_queries = torch.cat((image_embed, self_cond), dim = -2) +learned_queries = torch.cat((self_cond, learned_queries), dim = -2)",
        "core_API": "cat"
    },
    {
        "commit_hash": "957e78ed9ea05a024c8249a88e0f8562be79420e",
        "index": "90ae88cf..789dbd0e 100644",
        "commit_message": "Upgrade `pydoc-markdown` & refactor GitHub Actions (#2117)\n\n* Upgrade pydoc-markdown and fix the YAMLs to work with it\n\n* Pin pydoc-markdown to major version\n\n* Generalize pydoc-markdown workflow\n\n* Make a single Action to perform all tasks that require committing into the local branch\n\n* Merge the code updates and the docs in the Linux CI to prevent the bot from always show the pipeline as green\n\n* Installing Jupyter deps for Black\n\n* Build cache before running generation tasks\n\n* Add check not to run the code generation on master\n\n* Simplify push action\n\n* Add more test deps in setup.cfg and remove from GH Action workflow\n\n* Remove forced upgrades on pip install\n\nCo-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>\n",
        "file": "haystack.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TriAdaptiveModel(nn.Module):",
            ":param global_step: number of current training step.",
            ":param kwargs: Placeholder for passing generic parameters.",
            "Note: Contains the batch (as dict of tensors), when called from Trainer.train().",
            "-        :return loss: torch.Tensor that is the per sample loss (len: batch_size)",
            "+        :return: loss: torch.Tensor that is the per sample loss (len: batch_size)",
            "\"\"\"",
            "all_losses = self.logits_to_loss_per_head(logits, **kwargs)",
            "# This aggregates the loss per sample across multiple prediction heads"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=(':', ':'), position=2, insert_id=245720)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9558,
        "neg_line": [
            "-:return loss: torch.Tensor that is the per sample loss (len: batch_size)"
        ],
        "pos_line": [
            "+:return: loss: torch.Tensor that is the per sample loss (len: batch_size)"
        ],
        "core_change": "-:return loss: torch.Tensor that is the per sample loss (len: batch_size) +:return: loss: torch.Tensor that is the per sample loss (len: batch_size)",
        "core_API": "train"
    },
    {
        "commit_hash": "9c371931ac20d3d026e6024c0ff8ebd7209295f4",
        "index": "14052ab..38555dc 100644",
        "commit_message": "fix nms bug (sorted)\n\n",
        "file": "darkflow.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def loss(self, net_out):",
            "loss = tf.multiply(loss, wght)",
            "loss = tf.reshape(loss, [-1, H*W*B*(4 + 1 + C)])",
            "loss = tf.reduce_sum(loss, 1)",
            "-    self.loss = .5 * tf.reduce_mean(loss)",
            "-",
            "-",
            "+    self.loss = .5 * tf.reduce_mean(loss)",
            "\\ No newline at end of file"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 9559,
        "neg_line": [
            "-self.loss = .5 * tf.reduce_mean(loss)",
            "-",
            "-"
        ],
        "pos_line": [
            "+self.loss = .5 * tf.reduce_mean(loss)"
        ],
        "core_change": "-self.loss = .5 * tf.reduce_mean(loss) - - +self.loss = .5 * tf.reduce_mean(loss)",
        "core_API": "multiply"
    },
    {
        "commit_hash": "aaef09bc4e29adb9b0313e0550eded78887e0574",
        "index": "e5ebbf8b..9098d38d 100644",
        "commit_message": "bugfix in ori\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestLAFOrienter:",
            "def test_toy(self, device):",
            "ori = LAFOrienter(32).to(device)",
            "inp = torch.zeros(1, 1, 19, 19, device=device)",
            "-        inp[:, :, :10, :] = 1",
            "+        inp[:, :, :, :10] = 1",
            "laf = torch.tensor([[[[0, 5., 8.], [5.0, 0., 8.]]]], device=device)",
            "new_laf = ori(laf, inp)",
            "-        expected = torch.tensor([[[[5., 0., 8.], [0., 5., 8.]]]], device=device)",
            "+        expected = torch.tensor([[[[0., 5., 8.], [-5.0, 0, 8.]]]], device=device)",
            "assert_allclose(new_laf, expected)",
            "",
            "def test_gradcheck(self, device):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=slice), node=ASTNode(type=subscript), position=7)",
            "Move(target_node=ASTNode(type=slice), node=ASTNode(type=subscript), position=6)",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=list), position=1)",
            "Insert(target_node=ASTNode(type=list), node=('list', None), position=4, insert_id=429587)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=429588)",
            "Insert(target_node=IN(type=list), node=('unary_operator', None), position=1, insert_id=429589)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=429590)",
            "Insert(target_node=IN(type=list), node=('integer', '0'), position=3, insert_id=429591)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=4, insert_id=429592)",
            "Insert(target_node=IN(type=list), node=('float', '8.'), position=5, insert_id=429593)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=6, insert_id=429594)",
            "Insert(target_node=IN(type=unary_operator), node=('-', '-'), position=0, insert_id=429595)",
            "Insert(target_node=IN(type=unary_operator), node=('float', '5.0'), position=1, insert_id=429596)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=float, text=5.))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=float, text=0.))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=float, text=8.))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 9561,
        "neg_line": [
            "-inp[:, :, :10, :] = 1",
            "-expected = torch.tensor([[[[5., 0., 8.], [0., 5., 8.]]]], device=device)"
        ],
        "pos_line": [
            "+inp[:, :, :, :10] = 1",
            "+expected = torch.tensor([[[[0., 5., 8.], [-5.0, 0, 8.]]]], device=device)"
        ],
        "core_change": "-inp[:, :, :10, :] = 1 +inp[:, :, :, :10] = 1 -expected = torch.tensor([[[[5., 0., 8.], [0., 5., 8.]]]], device=device) +expected = torch.tensor([[[[0., 5., 8.], [-5.0, 0, 8.]]]], device=device)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "e75e576b40fadc3317205e5d0a37c482bb67b501",
        "index": "4b76a124..de6ca1fd 100644",
        "commit_message": "kl loss fix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ARGVA(ARGA):",
            "def loss(self, mu, logvar, pos_edge_index, neg_adj_mask):",
            "z = self.sample(mu, logvar)",
            "recon_loss = self.reconstruction_loss(z, pos_edge_index, neg_adj_mask)",
            "-        kl_loss = self.kl_loss(z, mu, logvar)",
            "+        kl_loss = self.kl_loss(mu, logvar)",
            "d_loss = self.discriminator_loss(*self.discriminate(z))",
            "return recon_loss + kl_loss + d_loss"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=argument_list), position=3)",
            "Delete(target_node=ASTNode(type=identifier, text=z))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 9574,
        "neg_line": [
            "-kl_loss = self.kl_loss(z, mu, logvar)"
        ],
        "pos_line": [
            "+kl_loss = self.kl_loss(mu, logvar)"
        ],
        "core_change": "-kl_loss = self.kl_loss(z, mu, logvar) +kl_loss = self.kl_loss(mu, logvar)",
        "core_API": "sample"
    },
    {
        "commit_hash": "fafd2546782c8d2c249c9d7546f74b416e803af7",
        "index": "c30692d0a..1b9f43137 100644",
        "commit_message": "Fix device parser logic to avoid creating CUDA context (#14319)\n\n* let environment disable forking\n\n* add helper function and error messages\n\n* tests\n\n* changelog\n\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def is_cuda_available() -> bool:",
            "Unlike :func:`torch.cuda.is_available`, this function will do its best not to create a CUDA context for fork",
            "support, if the platform allows it.",
            "\"\"\"",
            "-    if \"fork\" not in torch.multiprocessing.get_all_start_methods():",
            "+    if \"fork\" not in torch.multiprocessing.get_all_start_methods() or _is_forking_disabled():",
            "return torch.cuda.is_available()",
            "with multiprocessing.get_context(\"fork\").Pool(1) as pool:",
            "return pool.apply(torch.cuda.is_available)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 9575,
        "neg_line": [
            "-if \"fork\" not in torch.multiprocessing.get_all_start_methods():"
        ],
        "pos_line": [
            "+if \"fork\" not in torch.multiprocessing.get_all_start_methods() or _is_forking_disabled():"
        ],
        "core_change": "-if \"fork\" not in torch.multiprocessing.get_all_start_methods(): +if \"fork\" not in torch.multiprocessing.get_all_start_methods() or _is_forking_disabled():",
        "core_API": "get_all_start_methods"
    },
    {
        "commit_hash": "753525641935890f9af92ae528f061dbb7a42a35",
        "index": "ed713f9b..b8408147 100644",
        "commit_message": "fixed scale\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from .random_scale import RandomScale",
            "",
            "class RandomScaleTest(TestCase):",
            "def test_random_scale(self):",
            "-        position = [[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7]]",
            "-        position = torch.FloatTensor(position)",
            "+        position = [[0, 0, 0], [1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]]",
            "+        position = torch.FloatTensor(position) - 2",
            "data = Data(None, None, position, None)",
            "-        data = RandomScale(1)(data)",
            "+        data = RandomScale(2)(data)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('binary_operator', None), position=2, insert_id=1087448)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=1087449)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '2'), position=2, insert_id=1087450)",
            "Update(target_node=ASTNode(type=integer, text=1), value='2')",
            "Update(target_node=ASTNode(type=integer, text=1), value='0')",
            "Update(target_node=ASTNode(type=integer, text=2), value='0')",
            "Update(target_node=ASTNode(type=integer, text=3), value='0')",
            "Update(target_node=ASTNode(type=integer, text=2), value='1')",
            "Update(target_node=ASTNode(type=integer, text=3), value='1')",
            "Update(target_node=ASTNode(type=integer, text=4), value='1')",
            "Update(target_node=ASTNode(type=integer, text=3), value='2')",
            "Update(target_node=ASTNode(type=integer, text=4), value='2')",
            "Update(target_node=ASTNode(type=integer, text=5), value='2')",
            "Update(target_node=ASTNode(type=integer, text=4), value='3')",
            "Update(target_node=ASTNode(type=integer, text=5), value='3')",
            "Update(target_node=ASTNode(type=integer, text=6), value='3')",
            "Update(target_node=ASTNode(type=integer, text=5), value='4')",
            "Update(target_node=ASTNode(type=integer, text=6), value='4')",
            "Update(target_node=ASTNode(type=integer, text=7), value='4')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 20,
        "number": 9579,
        "neg_line": [
            "-position = [[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7]]",
            "-position = torch.FloatTensor(position)",
            "-data = RandomScale(1)(data)"
        ],
        "pos_line": [
            "+position = [[0, 0, 0], [1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]]",
            "+position = torch.FloatTensor(position) - 2",
            "+data = RandomScale(2)(data)"
        ],
        "core_change": "-position = [[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7]] -position = torch.FloatTensor(position) +position = [[0, 0, 0], [1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]] +position = torch.FloatTensor(position) - 2 -data = RandomScale(1)(data) +data = RandomScale(2)(data)",
        "core_API": "FloatTensor"
    },
    {
        "commit_hash": "c5a94a6100afdd550fb3ea445d8bddc6b9769fcc",
        "index": "b29e72155..f1df6f668 100644",
        "commit_message": "fix function that defines masks in XLM\n\nthe definition of `get_masks` would blow with the proper combination of\narguments. It was just a matter of moving a definition outside of a\ncontrol structure.\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_masks(slen, lengths, causal, padding_mask=None):",
            "\"\"\"",
            "Generate hidden states mask, and optionally an attention mask.",
            "\"\"\"",
            "-    bs = lengths.size(0)",
            "+    alen = torch.arange(slen, dtype=torch.long, device=lengths.device)",
            "if padding_mask is not None:",
            "mask = padding_mask",
            "else:",
            "assert lengths.max().item() <= slen",
            "-        alen = torch.arange(slen, dtype=torch.long, device=lengths.device)",
            "mask = alen < lengths[:, None]",
            "",
            "# attention mask is the same as mask, or triangular inferior attention (causal)",
            "if causal:",
            "+        bs = lengths.size(0)",
            "attn_mask = alen[None, None, :].repeat(bs, slen, 1) <= alen[None, :, None]",
            "else:",
            "attn_mask = mask"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=2)",
            "Insert(target_node=ASTNode(type=if_statement), node=('block', None), position=3, insert_id=1245803)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 9584,
        "neg_line": [
            "-bs = lengths.size(0)",
            "-alen = torch.arange(slen, dtype=torch.long, device=lengths.device)"
        ],
        "pos_line": [
            "+alen = torch.arange(slen, dtype=torch.long, device=lengths.device)",
            "+bs = lengths.size(0)"
        ],
        "core_change": "-bs = lengths.size(0) +alen = torch.arange(slen, dtype=torch.long, device=lengths.device) -alen = torch.arange(slen, dtype=torch.long, device=lengths.device) +bs = lengths.size(0)",
        "core_API": "size"
    },
    {
        "commit_hash": "455c6390938a5c737fa63e78396cedae41e4e87e",
        "index": "e901e8d70..0916aa1d5 100644",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "from .modeling_tf_utils import TFPreTrainedModel, TFSharedEmbeddings, shape_list",
            "logger = logging.getLogger(__name__)",
            "",
            "TF_T5_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"t5-small\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-tf_model.h5\",",
            "-    \"t5-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-tf_model.h5\",",
            "-    \"t5-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-large-tf_model.h5\",",
            "-    \"t5-3b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-3b-tf_model.h5\",",
            "-    \"t5-11b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-11b-tf_model.h5\",",
            "+    \"t5-small\": \"https://cdn.huggingface.co/t5-small-tf_model.h5\",",
            "+    \"t5-base\": \"https://cdn.huggingface.co/t5-base-tf_model.h5\",",
            "+    \"t5-large\": \"https://cdn.huggingface.co/t5-large-tf_model.h5\",",
            "+    \"t5-3b\": \"https://cdn.huggingface.co/t5-3b-tf_model.h5\",",
            "+    \"t5-11b\": \"https://cdn.huggingface.co/t5-11b-tf_model.h5\",",
            "}",
            "",
            "####################################################"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=dictionary), position=9)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=2, insert_id=2689797)",
            "Insert(target_node=ASTNode(type=pair), node=(':', ':'), position=1, insert_id=2689798)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-tf_model.h5\"), value='\"https://cdn.huggingface.co/t5-small-tf_model.h5\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-tf_model.h5\"), value='\"https://cdn.huggingface.co/t5-base-tf_model.h5\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/t5-large-tf_model.h5\"), value='\"https://cdn.huggingface.co/t5-large-tf_model.h5\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/t5-3b-tf_model.h5\"), value='\"https://cdn.huggingface.co/t5-3b-tf_model.h5\"')",
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=:, text=:), position=1)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/t5-11b-tf_model.h5\"), value='\"https://cdn.huggingface.co/t5-11b-tf_model.h5\"')",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 5,
        "minus_line": 5,
        "AST_diff_line": 11,
        "number": 9588,
        "neg_line": [
            "-\"t5-small\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-tf_model.h5\",",
            "-\"t5-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-tf_model.h5\",",
            "-\"t5-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-large-tf_model.h5\",",
            "-\"t5-3b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-3b-tf_model.h5\",",
            "-\"t5-11b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-11b-tf_model.h5\","
        ],
        "pos_line": [
            "+\"t5-small\": \"https://cdn.huggingface.co/t5-small-tf_model.h5\",",
            "+\"t5-base\": \"https://cdn.huggingface.co/t5-base-tf_model.h5\",",
            "+\"t5-large\": \"https://cdn.huggingface.co/t5-large-tf_model.h5\",",
            "+\"t5-3b\": \"https://cdn.huggingface.co/t5-3b-tf_model.h5\",",
            "+\"t5-11b\": \"https://cdn.huggingface.co/t5-11b-tf_model.h5\","
        ],
        "core_change": "-\"t5-small\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-tf_model.h5\", -\"t5-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-tf_model.h5\", -\"t5-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-large-tf_model.h5\", -\"t5-3b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-3b-tf_model.h5\", -\"t5-11b\": \"https://s3.amazonaws.com/models.huggingface.co/bert/t5-11b-tf_model.h5\", +\"t5-small\": \"https://cdn.huggingface.co/t5-small-tf_model.h5\", +\"t5-base\": \"https://cdn.huggingface.co/t5-base-tf_model.h5\", +\"t5-large\": \"https://cdn.huggingface.co/t5-large-tf_model.h5\", +\"t5-3b\": \"https://cdn.huggingface.co/t5-3b-tf_model.h5\", +\"t5-11b\": \"https://cdn.huggingface.co/t5-11b-tf_model.h5\",",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "ce8f42b12ecd522e3ef7251e0d5f5175075a3fb4",
        "index": "10572b22..a7611061 100644",
        "commit_message": "Fixes to enable dev branch to move to pytorch-1.2 (#2001)\n\n* Fixes to enable dev branch to move to pytorch-1.2\n\n* change name for published cpu wheels\n\n* fix failing unit tests\n\n* change docs conf\n\n* change tracking example\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def setup(app):",
            "",
            "# @jpchen's hack to get rtd builder to install latest pytorch",
            "if 'READTHEDOCS' in os.environ:",
            "-    os.system('pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp36-cp36m-linux_x86_64.whl')",
            "+    os.system('pip install torch==1.2.0+cpu -f https://download.pytorch.org/whl/torch_stable.html')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp36-cp36m-linux_x86_64.whl'), value=\"'pip install torch==1.2.0+cpu -f https://download.pytorch.org/whl/torch_stable.html'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9589,
        "neg_line": [
            "-os.system('pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp36-cp36m-linux_x86_64.whl')"
        ],
        "pos_line": [
            "+os.system('pip install torch==1.2.0+cpu -f https://download.pytorch.org/whl/torch_stable.html')"
        ],
        "core_change": "-os.system('pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp36-cp36m-linux_x86_64.whl') +os.system('pip install torch==1.2.0+cpu -f https://download.pytorch.org/whl/torch_stable.html')",
        "core_API": "system"
    },
    {
        "commit_hash": "66ee15a2787c95f71522f2d36337a6c20beec484",
        "index": "1db97f1d..cfafe390 100644",
        "commit_message": "fix issue of image_histogram2d (#1295)\n\n* fix https://github.com/kornia/kornia/issues/1294\n\n* update test for image_histogram2d\n\n* update test_histogram.py\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def image_histogram2d(",
            "",
            "hist = torch.sum(kernel_values, dim=(-2, -1)).permute(1, 2, 0)",
            "if return_pdf:",
            "-        normalization = torch.sum(hist, dim=-1).unsqueeze(0) + eps",
            "+        normalization = torch.sum(hist, dim=-1, keepdim=True) + eps",
            "pdf = hist / normalization",
            "if image.dim() == 2:",
            "hist = hist.squeeze()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=418089)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=418090)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'keepdim'), position=0, insert_id=418091)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=418092)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=418093)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=unsqueeze))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 9599,
        "neg_line": [
            "-normalization = torch.sum(hist, dim=-1).unsqueeze(0) + eps"
        ],
        "pos_line": [
            "+normalization = torch.sum(hist, dim=-1, keepdim=True) + eps"
        ],
        "core_change": "-normalization = torch.sum(hist, dim=-1).unsqueeze(0) + eps +normalization = torch.sum(hist, dim=-1, keepdim=True) + eps",
        "core_API": "sum"
    },
    {
        "commit_hash": "cec92098641d3f4c395cd51d84ba93b691d1cdf3",
        "index": "c500fd2a..88e82975 100644",
        "commit_message": "Several micro optimizations (#4833)\n\n* benchmark transfers\n\n* create tensors directl on device when possible\n\n* fix\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SampledSoftmaxLoss(torch.nn.Module):",
            "",
            "if embeddings.shape[0] == 0:",
            "# empty batch",
            "-            return torch.tensor(0.0).to(embeddings.device)",
            "+            return torch.tensor(0.0, device=embeddings.device)",
            "",
            "if not self.training:",
            "return self._forward_eval(embeddings, targets)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=5332)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=float, text=0.0), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=5333)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=5334)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=5335)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=5336)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 9607,
        "neg_line": [
            "-return torch.tensor(0.0).to(embeddings.device)"
        ],
        "pos_line": [
            "+return torch.tensor(0.0, device=embeddings.device)"
        ],
        "core_change": "-return torch.tensor(0.0).to(embeddings.device) +return torch.tensor(0.0, device=embeddings.device)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "32aa71fcf58d6f8b25baa85f8021a1571285a2f2",
        "index": "01d4a6be..27c62135 100644",
        "commit_message": ":lady_beetle: Fix rgb_to_hsv for onnx (#1329)\n\n* add test_onnx for RgbToHsv\n\n* add new rgb2hsv implementation\n\n* fix issues\n\n* add version checking\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add support function _compute_max_argmax to fix jit issues\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestRgbToHsv(BaseTester):",
            "assert_close(kornia.color.rgb_to_hsv(data), expected)",
            "",
            "def test_nan_rgb_to_hsv(self, device, dtype):",
            "-        data = torch.zeros(1, 5, 5, device=device, dtype=dtype)  # 3x5x5",
            "-        data = data.repeat(3, 1, 1)  # 2x3x5x5",
            "-",
            "+        data = torch.zeros(3, 5, 5, device=device, dtype=dtype)  # 3x5x5",
            "expected = torch.zeros_like(data)  # 3x5x5",
            "assert_close(kornia.color.rgb_to_hsv(data), expected)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=1), value='3')",
            "Delete(target_node=ASTNode(type=identifier, text=data))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=data))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=repeat))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=3))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 9615,
        "neg_line": [
            "-data = torch.zeros(1, 5, 5, device=device, dtype=dtype)  # 3x5x5",
            "-data = data.repeat(3, 1, 1)  # 2x3x5x5",
            "-"
        ],
        "pos_line": [
            "+data = torch.zeros(3, 5, 5, device=device, dtype=dtype)  # 3x5x5"
        ],
        "core_change": "-data = torch.zeros(1, 5, 5, device=device, dtype=dtype)  # 3x5x5 -data = data.repeat(3, 1, 1)  # 2x3x5x5 - +data = torch.zeros(3, 5, 5, device=device, dtype=dtype)  # 3x5x5",
        "core_API": "rgb_to_hsv"
    },
    {
        "commit_hash": "edca96353fc8799debccb4d221b78e402d1a1cba",
        "index": "225db9cdc..17a9af883 100644",
        "commit_message": "[RLlib] Curiosity Bug Fix. (#24880)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Curiosity(Exploration):",
            "{",
            "SampleBatch.OBS: torch.cat(",
            "[",
            "-                        torch.from_numpy(sample_batch[SampleBatch.OBS]),",
            "-                        torch.from_numpy(sample_batch[SampleBatch.NEXT_OBS]),",
            "+                        torch.from_numpy(sample_batch[SampleBatch.OBS]).to(",
            "+                            policy.device",
            "+                        ),",
            "+                        torch.from_numpy(sample_batch[SampleBatch.NEXT_OBS]).to(",
            "+                            policy.device",
            "+                        ),",
            "]",
            ")",
            "}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('call', None), position=1, insert_id=1111504)",
            "Insert(target_node=ASTNode(type=list), node=('call', None), position=4, insert_id=1111505)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1111506)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1111507)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1111508)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1111509)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1111510)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1111511)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1111512)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1111513)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1111514)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1111515)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1111516)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1111517)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1111518)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1111519)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'policy'), position=0, insert_id=1111520)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1111521)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1111522)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'policy'), position=0, insert_id=1111523)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1111524)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1111525)"
        ],
        "plus_line": 6,
        "minus_line": 2,
        "AST_diff_line": 24,
        "number": 9616,
        "neg_line": [
            "-torch.from_numpy(sample_batch[SampleBatch.OBS]),",
            "-torch.from_numpy(sample_batch[SampleBatch.NEXT_OBS]),"
        ],
        "pos_line": [
            "+torch.from_numpy(sample_batch[SampleBatch.OBS]).to(",
            "+policy.device",
            "+),",
            "+torch.from_numpy(sample_batch[SampleBatch.NEXT_OBS]).to(",
            "+policy.device",
            "+),"
        ],
        "core_change": "-torch.from_numpy(sample_batch[SampleBatch.OBS]), -torch.from_numpy(sample_batch[SampleBatch.NEXT_OBS]), +torch.from_numpy(sample_batch[SampleBatch.OBS]).to( +policy.device +), +torch.from_numpy(sample_batch[SampleBatch.NEXT_OBS]).to( +policy.device +),",
        "core_API": "cat"
    },
    {
        "commit_hash": "448895e4aaf6a9a9eaca40f5e127268fb67825f8",
        "index": "9484eacc..f3553535 100644",
        "commit_message": "fix moving summar name scope\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def add_moving_summary(*args, **kwargs):",
            "ema_var, c, decay,",
            "zero_debias=True, name=name + '_EMA_apply')",
            "ema_ops.append(ema_op)",
            "-        # cannot add it into colocate group -- will force everything to cpus",
            "-        tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary",
            "+        with tf.name_scope(None):",
            "+            # cannot add it into colocate group -- will force everything to cpus",
            "+            tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary",
            "if coll is not None:",
            "for op in ema_ops:",
            "# TODO a new collection to summary every step?"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('with_statement', None), position=2, insert_id=2295692)",
            "Insert(target_node=IN(type=with_statement), node=('with', 'with'), position=0, insert_id=2295693)",
            "Insert(target_node=IN(type=with_statement), node=('with_clause', None), position=1, insert_id=2295694)",
            "Insert(target_node=IN(type=with_statement), node=(':', ':'), position=2, insert_id=2295695)",
            "Insert(target_node=IN(type=with_statement), node=('block', None), position=3, insert_id=2295696)",
            "Insert(target_node=IN(type=with_clause), node=('with_item', None), position=0, insert_id=2295697)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=with_item), node=('call', None), position=0, insert_id=2295698)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2295699)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2295700)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2295701)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2295702)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'name_scope'), position=2, insert_id=2295703)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2295704)",
            "Insert(target_node=IN(type=argument_list), node=('none', 'None'), position=1, insert_id=2295705)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2295706)"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 9620,
        "neg_line": [
            "-# cannot add it into colocate group -- will force everything to cpus",
            "-tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary"
        ],
        "pos_line": [
            "+with tf.name_scope(None):",
            "+# cannot add it into colocate group -- will force everything to cpus",
            "+tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary"
        ],
        "core_change": "-# cannot add it into colocate group -- will force everything to cpus -tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary +with tf.name_scope(None): +# cannot add it into colocate group -- will force everything to cpus +tf.summary.scalar(name + '-summary', ema_op)    # write the EMA value as a summary",
        "core_API": "append"
    },
    {
        "commit_hash": "116fd6b011346ccfaf6de8b114f5b02c279643ae",
        "index": "b46ca4fd..4aee4395 100644",
        "commit_message": "Minor testing related fixes (#1410)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train_test_split(pd_dataframe):",
            "Training data - 45 initial at-bats and hits for each player.",
            "Validation data - Full season at-bats and hits for each player.",
            "\"\"\"",
            "-    train_data = torch.tensor(pd_dataframe.as_matrix([\"At-Bats\", \"Hits\"]), dtype=torch.float)",
            "-    test_data = torch.tensor(pd_dataframe.as_matrix([\"SeasonAt-Bats\", \"SeasonHits\"]), dtype=torch.float)",
            "+    train_data = torch.tensor(pd_dataframe[[\"At-Bats\", \"Hits\"]].values, dtype=torch.float)",
            "+    test_data = torch.tensor(pd_dataframe[[\"SeasonAt-Bats\", \"SeasonHits\"]].values, dtype=torch.float)",
            "first_name = pd_dataframe[\"FirstName\"].values",
            "last_name = pd_dataframe[\"LastName\"].values",
            "player_names = [\" \".join([first, last]) for first, last in zip(first_name, last_name)]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n    train_data = torch.tensor(pd_dataframe.as_matrix([\"At-Bats\", \"Hits\"]), dtype=torch.float)\n    test_data = torch.tensor(pd_dataframe.as_matrix([\"SeasonAt-Bats\", \"SeasonHits\"]), dtype=torch.float)\nfirst_name = pd_dataframe[\"FirstName\"].values\nlast_name = pd_dataframe[\"LastName\"].values\nplayer_names = [\" \"), value='\"\"\"\\n    train_data = torch.tensor(pd_dataframe[[\"At-Bats\", \"Hits\"]].values, dtype=torch.float)\\n    test_data = torch.tensor(pd_dataframe[[\"SeasonAt-Bats\", \"SeasonHits\"]].values, dtype=torch.float)\\nfirst_name = pd_dataframe[\"FirstName\"].values\\nlast_name = pd_dataframe[\"LastName\"].values\\nplayer_names = [\" \"')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 9625,
        "neg_line": [
            "-train_data = torch.tensor(pd_dataframe.as_matrix([\"At-Bats\", \"Hits\"]), dtype=torch.float)",
            "-test_data = torch.tensor(pd_dataframe.as_matrix([\"SeasonAt-Bats\", \"SeasonHits\"]), dtype=torch.float)"
        ],
        "pos_line": [
            "+train_data = torch.tensor(pd_dataframe[[\"At-Bats\", \"Hits\"]].values, dtype=torch.float)",
            "+test_data = torch.tensor(pd_dataframe[[\"SeasonAt-Bats\", \"SeasonHits\"]].values, dtype=torch.float)"
        ],
        "core_change": "-train_data = torch.tensor(pd_dataframe.as_matrix([\"At-Bats\", \"Hits\"]), dtype=torch.float) -test_data = torch.tensor(pd_dataframe.as_matrix([\"SeasonAt-Bats\", \"SeasonHits\"]), dtype=torch.float) +train_data = torch.tensor(pd_dataframe[[\"At-Bats\", \"Hits\"]].values, dtype=torch.float) +test_data = torch.tensor(pd_dataframe[[\"SeasonAt-Bats\", \"SeasonHits\"]].values, dtype=torch.float)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "78fd8f4b0..8b215c4ce 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario Å aÅ¡ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class CovidQaUcsd(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure the file is present in the directory specified in the data_dir specified in the input {} `datasets.load_dataset('covid_qa_ucsd', 'en', data_dir=...)`. Manual download instructions: {})\".format(",
            "-                    path_to_manual_file, dl_manager.manual_dir, self.manual_download_instructions",
            "-                )",
            "+                f\"{path_to_manual_file} does not exist. Make sure the file is present in the directory specified in the data_dir specified in the input {dl_manager.manual_dir} `datasets.load_dataset('covid_qa_ucsd', 'en', data_dir=...)`. Manual download instructions: {self.manual_download_instructions})\"",
            ")",
            "",
            "return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepath\": path_to_manual_file})]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{path_to_manual_file} does not exist. Make sure the file is present in the directory specified in the data_dir specified in the input {dl_manager.manual_dir} `datasets.load_dataset(\\'covid_qa_ucsd\\', \\'en\\', data_dir=...)`. Manual download instructions: {self.manual_download_instructions})\"'), position=1, insert_id=1781582)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure the file is present in the directory specified in the data_dir specified in the input {} `datasets.load_dataset('covid_qa_ucsd', 'en', data_dir=...)`. Manual download instructions: {})\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=path_to_manual_file))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=dl_manager))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_dir))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 9630,
        "neg_line": [
            "-\"{} does not exist. Make sure the file is present in the directory specified in the data_dir specified in the input {} `datasets.load_dataset('covid_qa_ucsd', 'en', data_dir=...)`. Manual download instructions: {})\".format(",
            "-path_to_manual_file, dl_manager.manual_dir, self.manual_download_instructions",
            "-)"
        ],
        "pos_line": [
            "+f\"{path_to_manual_file} does not exist. Make sure the file is present in the directory specified in the data_dir specified in the input {dl_manager.manual_dir} `datasets.load_dataset('covid_qa_ucsd', 'en', data_dir=...)`. Manual download instructions: {self.manual_download_instructions})\""
        ],
        "core_change": "-\"{} does not exist. Make sure the file is present in the directory specified in the data_dir specified in the input {} `datasets.load_dataset('covid_qa_ucsd', 'en', data_dir=...)`. Manual download instructions: {})\".format( -path_to_manual_file, dl_manager.manual_dir, self.manual_download_instructions -) +f\"{path_to_manual_file} does not exist. Make sure the file is present in the directory specified in the data_dir specified in the input {dl_manager.manual_dir} `datasets.load_dataset('covid_qa_ucsd', 'en', data_dir=...)`. Manual download instructions: {self.manual_download_instructions})\"",
        "core_API": "exists"
    },
    {
        "commit_hash": "8503cc755050c6ed5bc771e3244c29b71be1841e",
        "index": "b2d518600..667caa384 100644",
        "commit_message": "Fix torch device issues (#20304)\n\n* fix device issue\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ConditionalDetrModelIntegrationTests(unittest.TestCase):",
            "results = feature_extractor.post_process_object_detection(",
            "outputs, threshold=0.3, target_sizes=[image.size[::-1]]",
            ")[0]",
            "-        expected_scores = torch.tensor([0.8330, 0.8313, 0.8039, 0.6829, 0.5355])",
            "+        expected_scores = torch.tensor([0.8330, 0.8313, 0.8039, 0.6829, 0.5355]).to(torch_device)",
            "expected_labels = [75, 17, 17, 75, 63]",
            "-        expected_slice_boxes = torch.tensor([38.3089, 72.1022, 177.6293, 118.4512])",
            "+        expected_slice_boxes = torch.tensor([38.3089, 72.1022, 177.6293, 118.4512]).to(torch_device)",
            "",
            "self.assertEqual(len(results[\"scores\"]), 5)",
            "self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-4))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=1184228)",
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=1184229)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1184230)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1184231)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1184232)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1184233)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1184234)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1184235)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1184236)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'torch_device'), position=1, insert_id=1184237)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1184238)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1184239)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1184240)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1184241)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'torch_device'), position=1, insert_id=1184242)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1184243)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 9631,
        "neg_line": [
            "-expected_scores = torch.tensor([0.8330, 0.8313, 0.8039, 0.6829, 0.5355])",
            "-expected_slice_boxes = torch.tensor([38.3089, 72.1022, 177.6293, 118.4512])"
        ],
        "pos_line": [
            "+expected_scores = torch.tensor([0.8330, 0.8313, 0.8039, 0.6829, 0.5355]).to(torch_device)",
            "+expected_slice_boxes = torch.tensor([38.3089, 72.1022, 177.6293, 118.4512]).to(torch_device)"
        ],
        "core_change": "-expected_scores = torch.tensor([0.8330, 0.8313, 0.8039, 0.6829, 0.5355]) +expected_scores = torch.tensor([0.8330, 0.8313, 0.8039, 0.6829, 0.5355]).to(torch_device) -expected_slice_boxes = torch.tensor([38.3089, 72.1022, 177.6293, 118.4512]) +expected_slice_boxes = torch.tensor([38.3089, 72.1022, 177.6293, 118.4512]).to(torch_device)",
        "core_API": "post_process_object_detection"
    },
    {
        "commit_hash": "37e98945728f9961fd33d598a954358d79988c28",
        "index": "af69c660..0bb592a3 100644",
        "commit_message": "fix flake8 style in tensorpack/\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class EnqueueThread(threading.Thread):",
            "feed = dict(zip(self.placehdrs, dp))",
            "# print 'qsize:', self.sess.run([self.op, self.size_op], feed_dict=feed)[1]",
            "self.op.run(feed_dict=feed)",
            "-            except tf.errors.CancelledError as e:",
            "+            except tf.errors.CancelledError:",
            "pass",
            "except Exception:",
            "logger.exception(\"Exception in EnqueueThread:\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=2308353)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=4, insert_id=2308354)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=5, insert_id=2308355)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2308356)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'except'), position=0, insert_id=2308357)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2308358)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=:, text=:), position=1)",
            "Insert(target_node=IN(type=assignment), node=('type', None), position=2, insert_id=2308359)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=identifier, text=Exception), position=0)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=:, text=:), position=1)",
            "Insert(target_node=IN(type=assignment), node=('type', None), position=2, insert_id=2308360)",
            "Insert(target_node=IN(type=type), node=('identifier', 'pass'), position=0, insert_id=2308361)",
            "Move(target_node=IN(type=type), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=as, text=as))",
            "Delete(target_node=ASTNode(type=identifier, text=e))",
            "Delete(target_node=ASTNode(type=pass, text=pass))",
            "Delete(target_node=ASTNode(type=except, text=except))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=as_pattern_target))",
            "Delete(target_node=ASTNode(type=as_pattern))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 9632,
        "neg_line": [
            "-except tf.errors.CancelledError as e:"
        ],
        "pos_line": [
            "+except tf.errors.CancelledError:"
        ],
        "core_change": "-except tf.errors.CancelledError as e: +except tf.errors.CancelledError:",
        "core_API": "run"
    },
    {
        "commit_hash": "6dc042782f398149c62d473e2b98c7a65d132f84",
        "index": "9eb41424..a4fecf07 100644",
        "commit_message": "fix BN again and mute some compatibility noise\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class GraphVarParam(HyperParam):",
            "self._readable_name, self.var_name = get_op_var_name(name)",
            "",
            "def setup_graph(self):",
            "-        all_vars = tf.all_variables()",
            "+        try:",
            "+            all_vars = tf.global_variables()",
            "+        except:",
            "+            # TODO",
            "+            all_vars = tf.all_variables()",
            "+",
            "for v in all_vars:",
            "if v.name == self.var_name:",
            "self.var = v"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('try_statement', None), position=0, insert_id=2309780)",
            "Insert(target_node=IN(type=try_statement), node=('try', 'try'), position=0, insert_id=2309781)",
            "Insert(target_node=IN(type=try_statement), node=(':', ':'), position=1, insert_id=2309782)",
            "Insert(target_node=IN(type=try_statement), node=('block', None), position=2, insert_id=2309783)",
            "Insert(target_node=IN(type=try_statement), node=('except_clause', None), position=3, insert_id=2309784)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2309785)",
            "Insert(target_node=IN(type=except_clause), node=('except', 'except'), position=0, insert_id=2309786)",
            "Insert(target_node=IN(type=except_clause), node=(':', ':'), position=1, insert_id=2309787)",
            "Move(target_node=IN(type=except_clause), node=ASTNode(type=block), position=2)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2309788)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'all_vars'), position=0, insert_id=2309789)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2309790)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2309791)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2309792)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2309793)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2309794)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2309795)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'global_variables'), position=2, insert_id=2309796)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2309797)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2309798)"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 9633,
        "neg_line": [
            "-all_vars = tf.all_variables()"
        ],
        "pos_line": [
            "+try:",
            "+all_vars = tf.global_variables()",
            "+except:",
            "+# TODO",
            "+all_vars = tf.all_variables()",
            "+"
        ],
        "core_change": "-all_vars = tf.all_variables() +try: +all_vars = tf.global_variables() +except: +# TODO +all_vars = tf.all_variables() +",
        "core_API": "all_variables"
    },
    {
        "commit_hash": "c9402c4746b0e4d7b610145226713ab8daeef747",
        "index": "= self.index[:, i]",
        "commit_message": "bugfixes\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FAUST(Dataset):",
            "index = self.index[:, i]",
            "weight = torch.FloatTensor(index.size(1)).fill_(1)",
            "input = torch.FloatTensor(position.size(0)).fill_(1)",
            "-        adj = torch.sparse.FloatTensor(index, weight, torch.Size([75, 75]))",
            "+        adj = torch.sparse.FloatTensor(index, weight, torch.Size([6890, 6890]))",
            "data = (input, adj, position)",
            "",
            "if self.correspondence:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=75), value='6890')",
            "Update(target_node=ASTNode(type=integer, text=75), value='6890')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 9636,
        "neg_line": [
            "-adj = torch.sparse.FloatTensor(index, weight, torch.Size([75, 75]))"
        ],
        "pos_line": [
            "+adj = torch.sparse.FloatTensor(index, weight, torch.Size([6890, 6890]))"
        ],
        "core_change": "-adj = torch.sparse.FloatTensor(index, weight, torch.Size([75, 75])) +adj = torch.sparse.FloatTensor(index, weight, torch.Size([6890, 6890]))",
        "core_API": "FloatTensor"
    },
    {
        "commit_hash": "b5e2b183af5e40e33a4dc7659e697d137259d56e",
        "index": "235351279..9c7ccccc8 100755",
        "commit_message": "Doc styler examples (#14953)\n\n* Fix bad examples\n\n* Add black formatting to style_doc\n\n* Use first nonempty line\n\n* Put it at the right place\n\n* Don't add spaces to empty lines\n\n* Better templates\n\n* Deal with triple quotes in docstrings\n\n* Result of style_doc\n\n* Enable mdx treatment and fix code examples in MDXs\n\n* Result of doc styler on doc source files\n\n* Last fixes\n\n* Break copy from\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AlbertForPreTraining(AlbertPreTrainedModel):",
            ">>> from transformers import AlbertTokenizer, AlbertForPreTraining",
            ">>> import torch",
            "",
            "-        >>> tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')",
            "-        >>> model = AlbertForPreTraining.from_pretrained('albert-base-v2')",
            "+        >>> tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")",
            "+        >>> model = AlbertForPreTraining.from_pretrained(\"albert-base-v2\")",
            "",
            "-        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1",
            "+        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(",
            "+        ...     0",
            "+        >>> )  # Batch size 1",
            ">>> outputs = model(input_ids)",
            "",
            ">>> prediction_logits = outputs.prediction_logits"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=module), position=12)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=1, insert_id=1208507)",
            "Insert(target_node=IN(type=ERROR), node=('>>', '>>'), position=0, insert_id=1208508)",
            "Insert(target_node=IN(type=ERROR), node=('>', '>'), position=1, insert_id=1208509)",
            "Update(target_node=ASTNode(type=string, text='albert-base-v2'), value='\"albert-base-v2\"')",
            "Update(target_node=ASTNode(type=string, text='albert-base-v2'), value='\"albert-base-v2\"')",
            "Insert(target_node=ASTNode(type=argument_list), node=('ellipsis', '...'), position=1, insert_id=1208510)",
            "Insert(target_node=ASTNode(type=argument_list), node=('ERROR', None), position=2, insert_id=1208511)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=integer, text=0), position=0)",
            "Insert(target_node=IN(type=ERROR), node=('>>', '>>'), position=1, insert_id=1208512)",
            "Insert(target_node=IN(type=ERROR), node=('>', '>'), position=2, insert_id=1208513)",
            "Delete(target_node=ASTNode(type=>>, text=>>))",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 5,
        "minus_line": 3,
        "AST_diff_line": 14,
        "number": 9642,
        "neg_line": [
            "->>> tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')",
            "->>> model = AlbertForPreTraining.from_pretrained('albert-base-v2')",
            "->>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1"
        ],
        "pos_line": [
            "+>>> tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")",
            "+>>> model = AlbertForPreTraining.from_pretrained(\"albert-base-v2\")",
            "+>>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(",
            "+...     0",
            "+>>> )  # Batch size 1"
        ],
        "core_change": "->>> tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2') ->>> model = AlbertForPreTraining.from_pretrained('albert-base-v2') +>>> tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\") +>>> model = AlbertForPreTraining.from_pretrained(\"albert-base-v2\") ->>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1 +>>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze( +...     0 +>>> )  # Batch size 1",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "c26e81a22038a84a054c630a4435bcdb995c9e21",
        "index": "b2d1a1a8..4ab5e218 100644",
        "commit_message": "enh: Implements `InferenceModule` as a pipelined module with separate preprocessor, predictor, and postprocessor modules (#2105)\n\n* Adding inference pipeline with seperate pre-processing, predict and post-processing modules\n\n* Update to flatten outputs from predict consistent to support triton\n\n* inference module refactor\n\n* add back InferenceLudwigModel\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* unify modules into inference.py\n\n* cleaned up inaccurate documentation\n\n* clean up\n\n* clean up type hints and update InferenceLudwigModel\n\n* clean up type hint; passes test_torchscript.py\n\n* added typing to inference module for clarity\n\n* remove inference_module_file_name constant\n\n* unified predict module with postproc\n\n* removed InferencePredictor entirely\n\n* add back the old inference module\n\n* add back training set metadata\n\n* revert change to predict module, move feature filtering to postproc\n\n* cleanup inference_module_v0\n\n* cleanup\n\n* adds device placement to InferenceLudwigModel\n\n* adds ability to save/load torchscript on particular devices\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* allows saving torchscript with dict of devices from api.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* correct device inputs\n\n* refactor to expose inference stages (prep for triton refactor)\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove magic 'cpu' string\n\n* remove extraneous constants\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add from_directory classmethod for e2e users\n\n* merge\n\n* merge InferenceModule and InferenceLudwigModel\n\n* add comment\n\n* revert small change\n\n* cleanup\n\n* add to_torchscript functionality\n\n* cleanup\n\n* pushes device logic down into inference stages\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* move device placement upstream to inference module to ensure stage modules are performant\n\n* adds logs for device placement experiments\n\n* removes logs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove stage_to_dict\n\n* clean up how we get input device in predictor_forward\n\n* first commit\n\n* wip\n\n* updated interfaces\n\n* postproc GPU\n\n* add intelligent device placement\n\n* clean up device api\n\n* revert flatten op in inference_module_v0\n\n* remove dtype workaround\n\n* benchmarking code\n\n* add DEVICE constant as good default for loading/saving\n\n* added helpful logging and style\n\n* cleanup\n\n* cleanup, adding docstrings\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* docstring\n\nCo-authored-by: Geoffrey Angus <geoffrey@predibase.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AudioFeatureMixin(BaseFeatureMixin):",
            "",
            "feature_length = audio_feature.shape[0]",
            "broadcast_feature_length = min(feature_length, max_length)",
            "-        audio_feature_padded = torch.full((max_length, feature_dim), padding_value, dtype=torch.float32)",
            "+        audio_feature_padded = torch.full(",
            "+            (max_length, feature_dim), padding_value, dtype=torch.float32, device=audio_feature.device",
            "+        )",
            "audio_feature_padded[:broadcast_feature_length, :] = audio_feature[:max_length, :]",
            "",
            "return audio_feature_padded"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=601294)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=601295)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=601296)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=601297)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=601298)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'audio_feature'), position=0, insert_id=601299)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=601300)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=601301)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 9643,
        "neg_line": [
            "-audio_feature_padded = torch.full((max_length, feature_dim), padding_value, dtype=torch.float32)"
        ],
        "pos_line": [
            "+audio_feature_padded = torch.full(",
            "+(max_length, feature_dim), padding_value, dtype=torch.float32, device=audio_feature.device",
            "+)"
        ],
        "core_change": "-audio_feature_padded = torch.full((max_length, feature_dim), padding_value, dtype=torch.float32) +audio_feature_padded = torch.full( +(max_length, feature_dim), padding_value, dtype=torch.float32, device=audio_feature.device +)",
        "core_API": "full"
    },
    {
        "commit_hash": "969859d5f67c7106de4d1098c4891c9b03694bbe",
        "index": "d8ee191a0..20837a938 100755",
        "commit_message": "Fix doc errors and typos across the board (#8139)\n\n* Fix doc errors and typos across the board\n\n* Fix a typo\n\n* Fix the CI\n\n* Fix more typos\n\n* Fix CI\n\n* More fixes\n\n* Fix CI\n\n* More fixes\n\n* More fixes\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Transformer(nn.Module):",
            "attn_mask: torch.tensor(bs, seq_length) Attention mask on the sequence.",
            "",
            "Returns:",
            "-            hidden_state: torch.tensor(bs, seq_length, dim) Sequence of hiddens states in the last (top)",
            "+            hidden_state: torch.tensor(bs, seq_length, dim) Sequence of hidden states in the last (top)",
            "layer all_hidden_states: Tuple[torch.tensor(bs, seq_length, dim)]",
            "Tuple of length n_layers with the hidden states from each layer.",
            "Optional: only if output_hidden_states=True"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=hiddens), value='hidden')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9648,
        "neg_line": [
            "-hidden_state: torch.tensor(bs, seq_length, dim) Sequence of hiddens states in the last (top)"
        ],
        "pos_line": [
            "+hidden_state: torch.tensor(bs, seq_length, dim) Sequence of hidden states in the last (top)"
        ],
        "core_change": "-hidden_state: torch.tensor(bs, seq_length, dim) Sequence of hiddens states in the last (top) +hidden_state: torch.tensor(bs, seq_length, dim) Sequence of hidden states in the last (top)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "f38aaa5be05104ddfc3452249eeb521f139ded99",
        "index": "848927fe..cc2a1a30 100644",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class LineSearch(Iterative):",
            "# Trivial operation to enforce control dependency",
            "return tf.less(x=value, y=value)  # == False",
            "",
            "-        improved = tf.cond(",
            "+        improved = self.cond(",
            "pred=(improvement > last_improvement),",
            "true_fn=(lambda: True),",
            "false_fn=undo_deltas"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='self')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9650,
        "neg_line": [
            "-improved = tf.cond("
        ],
        "pos_line": [
            "+improved = self.cond("
        ],
        "core_change": "-improved = tf.cond( +improved = self.cond(",
        "core_API": "less"
    },
    {
        "commit_hash": "f38aaa5be05104ddfc3452249eeb521f139ded99",
        "index": "ffe7c6b3..69b2812d 100644",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PrioritizedReplay(Memory):",
            "value=tf.zeros(shape=tf.shape(self.batch_indices), dtype=tf.int32)",
            ")",
            "with tf.control_dependencies(control_inputs=(assignment,)):",
            "-            priority_indices = tf.cond(",
            "+            priority_indices = self.cond(",
            "pred=num_priority_elements > 0,",
            "true_fn=sampling_fn,",
            "false_fn=lambda: tf.zeros(shape=(num_priority_elements,), dtype=tf.int32)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='self')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9655,
        "neg_line": [
            "-priority_indices = tf.cond("
        ],
        "pos_line": [
            "+priority_indices = self.cond("
        ],
        "core_change": "-priority_indices = tf.cond( +priority_indices = self.cond(",
        "core_API": "zeros"
    },
    {
        "commit_hash": "5866646cc8347e45c17ca1204f3e770712365f99",
        "index": "7fce122..ce561b7 100755",
        "commit_message": "Fix float zeros format (#5491)\n\n* Fix float zeros format\n\n* 255 to integer\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LoadImagesAndLabels(Dataset):",
            "n = len(shapes) // 4",
            "img4, label4, path4, shapes4 = [], [], path[:n], shapes[:n]",
            "",
            "-        ho = torch.tensor([[0., 0, 0, 1, 0, 0]])",
            "-        wo = torch.tensor([[0., 0, 1, 0, 0, 0]])",
            "-        s = torch.tensor([[1, 1, .5, .5, .5, .5]])  # scale",
            "+        ho = torch.tensor([[0.0, 0, 0, 1, 0, 0]])",
            "+        wo = torch.tensor([[0.0, 0, 1, 0, 0, 0]])",
            "+        s = torch.tensor([[1, 1, 0.5, 0.5, 0.5, 0.5]])  # scale",
            "for i in range(n):  # zidane torch.zeros(16,3,720,1280)  # BCHW",
            "i *= 4",
            "if random.random() < 0.5:",
            "-                im = F.interpolate(img[i].unsqueeze(0).float(), scale_factor=2., mode='bilinear', align_corners=False)[",
            "+                im = F.interpolate(img[i].unsqueeze(0).float(), scale_factor=2.0, mode='bilinear', align_corners=False)[",
            "0].type(img[i].type())",
            "l = label[i]",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1296326)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1296327)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1296328)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=1296329)",
            "Update(target_node=ASTNode(type=float, text=2.), value='2.0')",
            "Update(target_node=ASTNode(type=float, text=0.), value='0.0')",
            "Update(target_node=ASTNode(type=float, text=0.), value='0.0')",
            "Update(target_node=ASTNode(type=float, text=.5), value='0.5')",
            "Update(target_node=ASTNode(type=float, text=.5), value='0.5')",
            "Update(target_node=ASTNode(type=float, text=.5), value='0.5')",
            "Update(target_node=ASTNode(type=float, text=.5), value='0.5')",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 16,
        "number": 9664,
        "neg_line": [
            "-ho = torch.tensor([[0., 0, 0, 1, 0, 0]])",
            "-wo = torch.tensor([[0., 0, 1, 0, 0, 0]])",
            "-s = torch.tensor([[1, 1, .5, .5, .5, .5]])  # scale",
            "-im = F.interpolate(img[i].unsqueeze(0).float(), scale_factor=2., mode='bilinear', align_corners=False)["
        ],
        "pos_line": [
            "+ho = torch.tensor([[0.0, 0, 0, 1, 0, 0]])",
            "+wo = torch.tensor([[0.0, 0, 1, 0, 0, 0]])",
            "+s = torch.tensor([[1, 1, 0.5, 0.5, 0.5, 0.5]])  # scale",
            "+im = F.interpolate(img[i].unsqueeze(0).float(), scale_factor=2.0, mode='bilinear', align_corners=False)["
        ],
        "core_change": "-ho = torch.tensor([[0., 0, 0, 1, 0, 0]]) -wo = torch.tensor([[0., 0, 1, 0, 0, 0]]) -s = torch.tensor([[1, 1, .5, .5, .5, .5]])  # scale +ho = torch.tensor([[0.0, 0, 0, 1, 0, 0]]) +wo = torch.tensor([[0.0, 0, 1, 0, 0, 0]]) +s = torch.tensor([[1, 1, 0.5, 0.5, 0.5, 0.5]])  # scale -im = F.interpolate(img[i].unsqueeze(0).float(), scale_factor=2., mode='bilinear', align_corners=False)[ +im = F.interpolate(img[i].unsqueeze(0).float(), scale_factor=2.0, mode='bilinear', align_corners=False)[",
        "core_API": "tensor"
    },
    {
        "commit_hash": "db5f46327aaa6207b2beb3a138fb819e67d63d2f",
        "index": "852b175c4..33ef4fb9c 100644",
        "commit_message": "fixed wrong sum axis\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Energy(AbsFeatsExtract):",
            "",
            "# input_stft: (..., F, 2) -> (..., F)",
            "input_power = input_stft[..., 0] ** 2 + input_stft[..., 1] ** 2",
            "-        energy = torch.sqrt(torch.clamp(input_power.sum(dim=1), min=1.0e-10))",
            "+        # sum over frequency (B, N, F) -> (B, N)",
            "+        energy = torch.sqrt(torch.clamp(input_power.sum(dim=2), min=1.0e-10))",
            "",
            "# (Optional): Adjust length to match with the mel-spectrogram",
            "if feats_lengths is not None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=1), value='2')"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9665,
        "neg_line": [
            "-energy = torch.sqrt(torch.clamp(input_power.sum(dim=1), min=1.0e-10))"
        ],
        "pos_line": [
            "+# sum over frequency (B, N, F) -> (B, N)",
            "+energy = torch.sqrt(torch.clamp(input_power.sum(dim=2), min=1.0e-10))"
        ],
        "core_change": "-energy = torch.sqrt(torch.clamp(input_power.sum(dim=1), min=1.0e-10)) +# sum over frequency (B, N, F) -> (B, N) +energy = torch.sqrt(torch.clamp(input_power.sum(dim=2), min=1.0e-10))",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "c9ecdcd195890bbefcafd13403ec82bf1fd3c662",
        "index": "3d6bfce..9cc7be1 100644",
        "commit_message": "Fix the wrong calculation of pred_ctr_y\n\n",
        "file": "tf-faster-rcnn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def bbox_transform_inv_tf(boxes, deltas):",
            "dh = deltas[:, 3]",
            "",
            "pred_ctr_x = tf.add(tf.multiply(dx, widths), ctr_x)",
            "-  pred_ctr_y = tf.add(tf.multiply(dy, widths), ctr_y)",
            "+  pred_ctr_y = tf.add(tf.multiply(dy, heights), ctr_y)",
            "pred_w = tf.multiply(tf.exp(dw), widths)",
            "pred_h = tf.multiply(tf.exp(dh), heights)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=widths), value='heights')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9671,
        "neg_line": [
            "-pred_ctr_y = tf.add(tf.multiply(dy, widths), ctr_y)"
        ],
        "pos_line": [
            "+pred_ctr_y = tf.add(tf.multiply(dy, heights), ctr_y)"
        ],
        "core_change": "-pred_ctr_y = tf.add(tf.multiply(dy, widths), ctr_y) +pred_ctr_y = tf.add(tf.multiply(dy, heights), ctr_y)",
        "core_API": "add"
    },
    {
        "commit_hash": "6422ee88a9a3bdfd93ec98d2def1e9f2b02eacab",
        "index": "21a66892..51c2ffa3 100644",
        "commit_message": "Replace Distribution+TorchDistribution with a thin Distribution mixin (#769)\n\n* Sketch Distribution class as mixin\n\n* Remove TorchDistribution class\n\n* Simplify TransformedDistribution\n\n* Update torch wrappers for most distributions\n\n* Fix docs\n\n* Use dist.Reshape() to set extra_event_dims\n\n* Fix bugs in Reshape distribution\n\n* Fix rejector tests\n\n* Update _Subsample distribution\n\n* Use .reshape() method for extra_event_dims and sample_shape\n\n* Refactor Distribution -> TorchDistribution class hierarchy\n\n* Update docs\n\n* Fix json error in air.ipynb\n\n* Fix bugs in air.ipynb and abstract_infer.py\n\n* Fix distributions docs\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class NaiveBeta(Beta):",
            "alpha_beta = torch.stack([alpha, beta], -1)",
            "self._gamma = Gamma(alpha_beta, torch.ones_like(alpha_beta))",
            "",
            "-    def sample(self, sample_shape=torch.Size()):",
            "-        gammas = self._gamma.sample(sample_shape)",
            "+    def rsample(self, sample_shape=torch.Size()):",
            "+        gammas = self._gamma.rsample(sample_shape)",
            "probs = gammas / gammas.sum(-1, True)",
            "return probs[..., 0]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=sample), value='rsample')",
            "Update(target_node=ASTNode(type=identifier, text=sample), value='rsample')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 9675,
        "neg_line": [
            "-def sample(self, sample_shape=torch.Size()):",
            "-gammas = self._gamma.sample(sample_shape)"
        ],
        "pos_line": [
            "+def rsample(self, sample_shape=torch.Size()):",
            "+gammas = self._gamma.rsample(sample_shape)"
        ],
        "core_change": "-def sample(self, sample_shape=torch.Size()): -gammas = self._gamma.sample(sample_shape) +def rsample(self, sample_shape=torch.Size()): +gammas = self._gamma.rsample(sample_shape)",
        "core_API": "stack"
    },
    {
        "commit_hash": "9a14f04322c93de45f5903476c8d64e1634880b7",
        "index": "ed6e8a14b..2a77abc98 100644",
        "commit_message": "Fix mypy typing errors in optimizer loop (#9317)\n\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LightningModule(",
            "def __to_tensor(self, value: numbers.Number) -> torch.Tensor:",
            "return torch.tensor(value, device=self.device)",
            "",
            "-    def log_grad_norm(self, grad_norm_dict: Dict[str, torch.Tensor]) -> None:",
            "+    def log_grad_norm(self, grad_norm_dict: Dict[str, float]) -> None:",
            "\"\"\"Override this method to change the default behaviour of ``log_grad_norm``.",
            "",
            "Args:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='float')",
            "Move(target_node=ASTNode(type=subscript), node=ASTNode(type=identifier, text=torch), position=4)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Tensor))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 9676,
        "neg_line": [
            "-def log_grad_norm(self, grad_norm_dict: Dict[str, torch.Tensor]) -> None:"
        ],
        "pos_line": [
            "+def log_grad_norm(self, grad_norm_dict: Dict[str, float]) -> None:"
        ],
        "core_change": "-def log_grad_norm(self, grad_norm_dict: Dict[str, torch.Tensor]) -> None: +def log_grad_norm(self, grad_norm_dict: Dict[str, float]) -> None:",
        "core_API": "tensor"
    },
    {
        "commit_hash": "b481cfbd019e1508e90fef39a0eeefc1b2759291",
        "index": "6b9efc9..59f43fe 100644",
        "commit_message": "Correct shape for default grid_sizes\n\nSummary: Small fix for omitting this argument.\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D29548610\n\nfbshipit-source-id: f25032fab3faa2f09006f5fcf8628138555f2f20\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def add_points_features_to_volume_densities_features(",
            "",
            "# init the volumetric grid sizes if uninitialized",
            "if grid_sizes is None:",
            "-        grid_sizes = torch.LongTensor(list(volume_densities.shape[2:])).to(",
            "-            volume_densities",
            "+        # grid sizes shape (minibatch, 3)",
            "+        grid_sizes = (",
            "+            torch.LongTensor(list(volume_densities.shape[2:]))",
            "+            .to(volume_densities)",
            "+            .expand(volume_densities.shape[0], 3)",
            ")",
            "",
            "# flatten densities and features"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=typed_default_parameter), node=('parenthesized_expression', None), position=5, insert_id=918253)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=918254)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('call', None), position=1, insert_id=918255)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=918256)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=918257)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=918258)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=918259)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'expand'), position=2, insert_id=918260)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=918261)",
            "Insert(target_node=IN(type=argument_list), node=('subscript', None), position=1, insert_id=918262)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=918263)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '3'), position=3, insert_id=918264)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=918265)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=0, insert_id=918266)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=918267)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=918268)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=918269)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'volume_densities'), position=0, insert_id=918270)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=918271)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shape'), position=2, insert_id=918272)"
        ],
        "plus_line": 5,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 9683,
        "neg_line": [
            "-grid_sizes = torch.LongTensor(list(volume_densities.shape[2:])).to(",
            "-volume_densities"
        ],
        "pos_line": [
            "+# grid sizes shape (minibatch, 3)",
            "+grid_sizes = (",
            "+torch.LongTensor(list(volume_densities.shape[2:]))",
            "+.to(volume_densities)",
            "+.expand(volume_densities.shape[0], 3)"
        ],
        "core_change": "-grid_sizes = torch.LongTensor(list(volume_densities.shape[2:])).to( -volume_densities +# grid sizes shape (minibatch, 3) +grid_sizes = ( +torch.LongTensor(list(volume_densities.shape[2:])) +.to(volume_densities) +.expand(volume_densities.shape[0], 3)",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "f38aaa5be05104ddfc3452249eeb521f139ded99",
        "index": "e3d3779e..ddb07f83 100644",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class RunningStandardize(Preprocessor):",
            "# Standardize tensor",
            "return (tensor - mean_estimate) / tf.maximum(x=tf.sqrt(x=variance_estimate), y=util.epsilon)",
            "",
            "-            return tf.cond(pred=(count > 1.0), true_fn=later_run, false_fn=first_run)",
            "+            return self.cond(pred=(count > 1.0), true_fn=later_run, false_fn=first_run)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='self')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9686,
        "neg_line": [
            "-return tf.cond(pred=(count > 1.0), true_fn=later_run, false_fn=first_run)"
        ],
        "pos_line": [
            "+return self.cond(pred=(count > 1.0), true_fn=later_run, false_fn=first_run)"
        ],
        "core_change": "-return tf.cond(pred=(count > 1.0), true_fn=later_run, false_fn=first_run) +return self.cond(pred=(count > 1.0), true_fn=later_run, false_fn=first_run)",
        "core_API": "maximum"
    },
    {
        "commit_hash": "ac65fcfb64df510764f8c4e55a63c7c7129856dd",
        "index": "62eb7f7f..abe4be03 100644",
        "commit_message": "Reinforcement learning fix opt (#999)\n\n* change readme\n\n* Add files via upload\n\n* fix opt and make format\n\n* readme\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PPO(object):",
            "advantage = tfdc_r - self.critic(s)",
            "closs = tf.reduce_mean(tf.square(advantage))",
            "grad = tape.gradient(closs, self.critic.trainable_weights)",
            "-        tf.optimizers.Adam(C_LR).apply_gradients(zip(grad, self.critic.trainable_weights))",
            "+        self.critic_opt.apply_gradients(zip(grad, self.critic.trainable_weights))",
            "",
            "def cal_adv(self, tfs, tfdc_r):",
            "'''"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='self')",
            "Update(target_node=ASTNode(type=identifier, text=optimizers), value='critic_opt')",
            "Delete(target_node=ASTNode(type=identifier, text=Adam))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=C_LR))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 9689,
        "neg_line": [
            "-tf.optimizers.Adam(C_LR).apply_gradients(zip(grad, self.critic.trainable_weights))"
        ],
        "pos_line": [
            "+self.critic_opt.apply_gradients(zip(grad, self.critic.trainable_weights))"
        ],
        "core_change": "-tf.optimizers.Adam(C_LR).apply_gradients(zip(grad, self.critic.trainable_weights)) +self.critic_opt.apply_gradients(zip(grad, self.critic.trainable_weights))",
        "core_API": "critic"
    },
    {
        "commit_hash": "c2c9dddf6d99bfa14b8ffb65b507a6be50b0ad6e",
        "index": "919d0d50..e31ba360 100755",
        "commit_message": "Fixed general problems with variable handling and various related changes, removed DDQN\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class NaturalGradient(Optimizer):",
            "# Solve the following system for delta' via the conjugate gradient solver.",
            "# [delta' * F] * delta' = -grad(loss)",
            "# --> delta'  (= lambda * delta)",
            "-        deltas = self.solver.solve(fn_x=fisher_matrix_product, x_init=None, b=[-grad for grad in loss_gradients], f_args=())",
            "+        deltas = self.solver.solve(fn_x=fisher_matrix_product, x_init=None, b=[-grad for grad in loss_gradients])",
            "",
            "# delta' * F",
            "delta_fisher_matrix_product = fisher_matrix_product(deltas=deltas)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=6)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=f_args))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=tuple))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 9692,
        "neg_line": [
            "-deltas = self.solver.solve(fn_x=fisher_matrix_product, x_init=None, b=[-grad for grad in loss_gradients], f_args=())"
        ],
        "pos_line": [
            "+deltas = self.solver.solve(fn_x=fisher_matrix_product, x_init=None, b=[-grad for grad in loss_gradients])"
        ],
        "core_change": "-deltas = self.solver.solve(fn_x=fisher_matrix_product, x_init=None, b=[-grad for grad in loss_gradients], f_args=()) +deltas = self.solver.solve(fn_x=fisher_matrix_product, x_init=None, b=[-grad for grad in loss_gradients])",
        "core_API": "solve"
    },
    {
        "commit_hash": "ef4684adbbf0b999466ba6a3c68cf84ba35f68a2",
        "index": "96cf5d8a..098d1f83 100644",
        "commit_message": "Fix typos discovered by codespell (#1214)\n\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def random_crop_size_generator(",
            "# Element-wise w, h condition",
            "cond = ((0 < w) * (w < size[0]) * (0 < h) * (h < size[1])).int()",
            "",
            "-    # torch.argmax is not reproducible accross devices: https://github.com/pytorch/pytorch/issues/17738",
            "-    # Here, we will select the first occurance of the duplicated elements.",
            "+    # torch.argmax is not reproducible across devices: https://github.com/pytorch/pytorch/issues/17738",
            "+    # Here, we will select the first occurrence of the duplicated elements.",
            "cond_bool, argmax_dim1 = ((cond.cumsum(1) == 1) & cond.bool()).max(1)",
            "h_out = w[torch.arange(0, batch_size, device=device, dtype=torch.long), argmax_dim1]",
            "w_out = h[torch.arange(0, batch_size, device=device, dtype=torch.long), argmax_dim1]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 9696,
        "neg_line": [
            "-# torch.argmax is not reproducible accross devices: https://github.com/pytorch/pytorch/issues/17738",
            "-# Here, we will select the first occurance of the duplicated elements."
        ],
        "pos_line": [
            "+# torch.argmax is not reproducible across devices: https://github.com/pytorch/pytorch/issues/17738",
            "+# Here, we will select the first occurrence of the duplicated elements."
        ],
        "core_change": "-# torch.argmax is not reproducible accross devices: https://github.com/pytorch/pytorch/issues/17738 -# Here, we will select the first occurance of the duplicated elements. +# torch.argmax is not reproducible across devices: https://github.com/pytorch/pytorch/issues/17738 +# Here, we will select the first occurrence of the duplicated elements.",
        "core_API": "cumsum"
    },
    {
        "commit_hash": "806731155aa059ce4f6297d75f9dbfb2f0dad7ad",
        "index": "18222c2a..022a095f 100644",
        "commit_message": "Removed arrays_to_variables (#580)\n\n* Removed arrays_to_variables\n\n* Fix merged tests\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestElmo(AllenNlpTestCase):",
            "dataset = Dataset(instances)",
            "vocab = Vocabulary()",
            "dataset.index_instances(vocab)",
            "-        character_ids = dataset.as_array_dict()['elmo']['character_ids']",
            "+        character_ids = dataset.as_tensor_dict()['elmo']['character_ids']",
            "",
            "-        output = elmo(Variable(torch.from_numpy(character_ids)))",
            "+        output = elmo(character_ids)",
            "elmo_representations = output['elmo_representations']",
            "mask = output['mask']"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='elmo')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=as_array_dict), value='as_tensor_dict')",
            "Delete(target_node=ASTNode(type=identifier, text=elmo))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=from_numpy))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 9698,
        "neg_line": [
            "-character_ids = dataset.as_array_dict()['elmo']['character_ids']",
            "-output = elmo(Variable(torch.from_numpy(character_ids)))"
        ],
        "pos_line": [
            "+character_ids = dataset.as_tensor_dict()['elmo']['character_ids']",
            "+output = elmo(character_ids)"
        ],
        "core_change": "-character_ids = dataset.as_array_dict()['elmo']['character_ids'] +character_ids = dataset.as_tensor_dict()['elmo']['character_ids'] -output = elmo(Variable(torch.from_numpy(character_ids))) +output = elmo(character_ids)",
        "core_API": "index_instances"
    },
    {
        "commit_hash": "62e3d1dbdc88af9ed8afb37f7e8ecc011f5486a3",
        "index": "f9852840..5da200fb 100644",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CrfTagger(Model):",
            "\"\"\"",
            "# Parameters",
            "",
            "-        tokens : ``Dict[str, torch.LongTensor]``, required",
            "+        tokens : ``TextFieldTensors``, required",
            "The output of ``TextField.as_array()``, which should typically be passed directly to a",
            "``TextFieldEmbedder``. This output is a dictionary mapping keys to ``TokenIndexer``",
            "tensors.  At its most basic, using a ``SingleIdTokenIndexer`` this is : ``{\"tokens\":"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 9700,
        "neg_line": [
            "-tokens : ``Dict[str, torch.LongTensor]``, required"
        ],
        "pos_line": [
            "+tokens : ``TextFieldTensors``, required"
        ],
        "core_change": "-tokens : ``Dict[str, torch.LongTensor]``, required +tokens : ``TextFieldTensors``, required",
        "core_API": "as_array"
    },
    {
        "commit_hash": "a95e654ce7588e41e1d26706f60c25cf646ed164",
        "index": "2014fb34..4a4c31e6 100644",
        "commit_message": "some tutorial improvements (#667)\n\n* some tutorial improvements\n\n* minor fix\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "\"\\n\",",
            "\"```python\\n\",",
            "\"data = Variable(torch.zeros(10, 1))\\n\",",
            "-    \"data[0:6, 0].data = torch.ones(6)  # 6 heads and 4 tails\\n\",",
            "+    \"data.data[0:6, 0] = torch.ones(6)  # 6 heads and 4 tails\\n\",",
            "\"```\\n\",",
            "\"\\n\",",
            "\"Then we have:\\n\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"data[0:6, 0].data = torch.ones(6)  # 6 heads and 4 tails\\n\"), value='\"data.data[0:6, 0] = torch.ones(6)  # 6 heads and 4 tails\\\\n\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9704,
        "neg_line": [
            "-\"data[0:6, 0].data = torch.ones(6)  # 6 heads and 4 tails\\n\","
        ],
        "pos_line": [
            "+\"data.data[0:6, 0] = torch.ones(6)  # 6 heads and 4 tails\\n\","
        ],
        "core_change": "-\"data[0:6, 0].data = torch.ones(6)  # 6 heads and 4 tails\\n\", +\"data.data[0:6, 0] = torch.ones(6)  # 6 heads and 4 tails\\n\",",
        "core_API": "zeros"
    },
    {
        "commit_hash": "1b98fe5860ee00f0062d899a4c7b6db9588bef70",
        "index": "a652709d..f4b9face 100644",
        "commit_message": "Improve compatibility to tensorflow 2.3 (#1487)\n\n* fix gfile not found error occurring with tensorflow 2.3\n\n* fix as_list() not found error occurring with tensorflow 2.3\n\nCo-authored-by: Philipp Werner <pw_post@gmx.de>\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class AutoResumeTrainConfig(TrainConfig):",
            "if not dir:",
            "return None",
            "path = os.path.join(dir, 'checkpoint')",
            "-        if not tf.gfile.Exists(path):",
            "+        if not tfv1.gfile.Exists(path):",
            "return None",
            "return SaverRestore(path)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tfv1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9705,
        "neg_line": [
            "-if not tf.gfile.Exists(path):"
        ],
        "pos_line": [
            "+if not tfv1.gfile.Exists(path):"
        ],
        "core_change": "-if not tf.gfile.Exists(path): +if not tfv1.gfile.Exists(path):",
        "core_API": "join"
    },
    {
        "commit_hash": "f358b9b14dbc1414c588f308b35f55705d777873",
        "index": "99f3852..aca2658 100644",
        "commit_message": "Fix squared distance for CPU impl. (#83)\n\nSummary:\n`PointLineDistanceForward()` should return squared distance. However, it seems that it returned non-squared distance when `v0` was near by `v1` in CPU implementation.\nPull Request resolved: https://github.com/facebookresearch/pytorch3d/pull/83\n\nReviewed By: bottler\n\nDifferential Revision: D20097181\n\nPulled By: nikhilaravi\n\nfbshipit-source-id: 7ea851c0837ab89364e42d283c999df21ff5ff02\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def point_line_distance(p, v0, v1):",
            "",
            "v1v0 = v1 - v0",
            "l2 = v1v0.dot(v1v0)  # |v1 - v0|^2",
            "-    if l2 == 0.0:",
            "-        return torch.sqrt((p - v1).dot(p - v1))  # v0 == v1",
            "+    if l2 <= kEpsilon:",
            "+        return (p - v1).dot(p - v1)  # v0 == v1",
            "",
            "t = (v1v0).dot(p - v0) / l2",
            "t = torch.clamp(t, min=0.0, max=1.0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('<=', '<='), position=1, insert_id=929134)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('identifier', 'kEpsilon'), position=2, insert_id=929135)",
            "Move(target_node=ASTNode(type=return_statement), node=ASTNode(type=call), position=1)",
            "Delete(target_node=ASTNode(type===, text===))",
            "Delete(target_node=ASTNode(type=float, text=0.0))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=sqrt))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 13,
        "number": 9707,
        "neg_line": [
            "-if l2 == 0.0:",
            "-return torch.sqrt((p - v1).dot(p - v1))  # v0 == v1"
        ],
        "pos_line": [
            "+if l2 <= kEpsilon:",
            "+return (p - v1).dot(p - v1)  # v0 == v1"
        ],
        "core_change": "-if l2 == 0.0: -return torch.sqrt((p - v1).dot(p - v1))  # v0 == v1 +if l2 <= kEpsilon: +return (p - v1).dot(p - v1)  # v0 == v1",
        "core_API": "dot"
    },
    {
        "commit_hash": "fcfdd95f0b40e52c1b50f5b23eafb2423f1664ba",
        "index": "e85e2386..992b4ca2 100644",
        "commit_message": "Fix/Enable all schedulers for in-painting (#1331)\n\n* inpaint fix k lms\n\n* onnox as well\n\n* up\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class StableDiffusionInpaintPipeline(DiffusionPipeline):",
            "latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents",
            "",
            "# concat latents, mask, masked_image_latents in the channel dimension",
            "-            latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1)",
            "-",
            "latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)",
            "+            latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1)",
            "",
            "# predict the noise residual",
            "noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 9709,
        "neg_line": [
            "-latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1)",
            "-"
        ],
        "pos_line": [
            "+latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1)"
        ],
        "core_change": "-latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1) - +latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1)",
        "core_API": "cat"
    }
]