{"number": 5, "change": "class AlbertEmbeddings(nn.Module):\n# position_ids (1, len position emb) is contiguous in memory and exported when serialized\nself.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\nself.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n-        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):\n+        if is_torch_greater_than_1_6:\nself.register_buffer(\n\"token_type_ids\",\ntorch.zeros(self.position_ids.size(), dtype=torch.long),\n", "fix_pattern": "<condition>: The condition is to check if the version of torch is greater than \"1.6.0\".\n<pattern>: The pattern is to use the `version.parse()` function to compare the torch version.\n<code_one>: The code that is removed is the condition `if version.parse(torch.__version__) > version.parse(\"1.6.0\"):`.\n<code_two>: The code that is added is the condition `if is_torch_greater_than_1_6:`.\nFix_pattern: In the condition of checking the torch version, if the pattern of comparing the version with `version.parse()` is detected, then remove the code that directly compares the version and instead use a boolean variable (`is_torch_greater_than_1_6`) to check if the torch version is greater than \"1.6.0\" in order to fix the API misuse."}
{"number": 7, "change": "def test_quantile():\n\n\ndef test_pi():\n-    x = torch.empty(1000).log_normal_(0, 1)\n+    x = torch.randn(1000).exp()\nassert_equal(pi(x, prob=0.8), quantile(x, probs=[0.1, 0.9]))\n", "fix_pattern": "<condition>: The condition is when the function `pi` is used with the argument `prob=0.8` and `quantile` function is used with the argument `probs=[0.1, 0.9]`.\n\n<pattern>: The pattern detected is that the code is generating a log-normal distribution using `torch.empty(1000).log_normal_(0, 1)`.\n\n<code_one>: The code `torch.empty(1000).log_normal_(0, 1)` is being removed.\n\n<code_two>: The code `torch.randn(1000).exp()` is being added.\n\nFix_pattern: In the condition of `test_pi`, if the pattern of generating a log-normal distribution using `torch.empty(1000).log_normal_(0, 1)` is detected, then remove the code `torch.empty(1000).log_normal_(0, 1)` and replace it with `torch.randn(1000).exp()` to fix the API misuse."}
{"number": 8, "change": "class TPUAccelerator(Accelerator):\nReturn:\nA tensor of shape (world_size, batch, ...)\n\"\"\"\n-        return xm.all_gather(tensor, group=group, sync_grads=sync_grads)\n+        # todo: Add support for backward with all_gather\n+        if torch.distributed.is_initialized():\n+            return xm.all_gather(tensor, group=group, sync_grads=sync_grads)\n+        return tensor\n", "fix_pattern": "<condition>: The code is used in a distributed computing environment where torch.distributed is initialized.\n<pattern>: The API `xm.all_gather()` is used with an optional argument `group=group` and `sync_grads=sync_grads`.\n<code_one>: `return xm.all_gather(tensor, group=group, sync_grads=sync_grads)`\n<code_two>: \n'''\n        if torch.distributed.is_initialized():\n            return xm.all_gather(tensor, group=group, sync_grads=sync_grads)\n        return tensor\n'''\nFix_pattern: In the condition of distributed computing environment initialization, if `xm.all_gather()` is used with the optional arguments `group=group` and `sync_grads=sync_grads`, then replace the code `return xm.all_gather(tensor, group=group, sync_grads=sync_grads)` with the following code: \n'''\n        if torch.distributed.is_initialized():\n            return xm.all_gather(tensor, group=group, sync_grads=sync_grads)\n        return tensor\n'''"}
{"number": 12, "change": "def get_rotation_matrix2d(\n\n# create output tensor\nbatch_size: int = center.shape[0]\n-    one = torch.tensor(1.).to(center.device)\n+    one = torch.tensor(1., device=center.device, dtype=center.dtype)\nM: torch.Tensor = torch.zeros(\nbatch_size, 2, 3, device=center.device, dtype=center.dtype)\nM[..., 0:2, 0:2] = scaled_rotation\n", "fix_pattern": "<condition>: No pre condition is needed.\n<pattern>: Code one is using an incorrect device and dtype specification for the tensor.\n<code_one>: one = torch.tensor(1.).to(center.device)\n<code_two>: one = torch.tensor(1., device=center.device, dtype=center.dtype)\nFix_pattern: In the condition of no pre condition, if code one is detected, then change the code one to code two to fix the API misuse."}
{"number": 15, "change": "class Trainer(TrainerBase):\nbreak\nsys.stdout.flush()\n\n-        model.load_state_dict(torch.load(best_model_path))\n+        if rank == 0:\n+            model.load_state_dict(torch.load(best_model_path))\nreturn model, best_metric\n\ndef _run_epoch(\n", "fix_pattern": "<condition>: In the condition of rank being equal to 0.\n<pattern>: Loading of model state dictionary using torch.load function.\n<code_one>: model.load_state_dict(torch.load(best_model_path))\n<code_two>: if rank == 0:\n                model.load_state_dict(torch.load(best_model_path))\nFix_pattern: In the condition of rank being equal to 0, if loading of the model state dictionary using torch.load function is detected, then change code_one to code_two to fix the API misuse."}
{"number": 23, "change": "class TacotronTrainTest(unittest.TestCase):\noptimizer = optim.Adam(model.parameters(), lr=c.lr)\nfor i in range(5):\nmel_out, linear_out, align, stop_tokens = model.forward(\n-                input, input_lengths, mel_spec)\n+                input, input_lengths, mel_spec, speaker_ids)\noptimizer.zero_grad()\nloss = criterion(mel_out, mel_spec, mel_lengths)\nstop_loss = criterion_st(stop_tokens, stop_targets)\n", "fix_pattern": "<condition>: The code is within a for loop.\n<pattern>: The code within the for loop is missing an argument.\n<code_one>: input, input_lengths, mel_spec\n<code_two>: input, input_lengths, mel_spec, speaker_ids\nFix_pattern: In the condition of the for loop, if the code is missing an argument, then add speaker_ids to fix the API misuse."}
{"number": 24, "change": "def evaluate(args, model, tokenizer, prefix=\"\", test=False):\neval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n# multi-gpu evaluate\n-        if args.n_gpu > 1:\n+        if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\nmodel = torch.nn.DataParallel(model)\n\n# Eval!\n", "fix_pattern": "<condition>: The condition is that the number of GPUs (args.n_gpu) is greater than 1.\n<pattern>: The pattern that is detected is checking if the model is not already an instance of torch.nn.DataParallel.\n<code_one>: The code that is removed is: \"if args.n_gpu > 1\".\n<code_two>: The code that is added is: \"if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel)\".\nFix_pattern: In the condition of checking if the number of GPUs is greater than 1, if the model is not already an instance of torch.nn.DataParallel, then change the code \"if args.n_gpu > 1\" to \"if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel)\" to fix the API misuse."}
{"number": 25, "change": "class TestMotionBlur:\n) -> torch.Tensor:\nreturn kornia.filters.motion_blur(input, ksize, angle, direction)\n\n-        img = torch.rand(2, 3, 4, 5)\n+        img = torch.rand(2, 3, 4, 5).to(device)\nksize = 5\nangle = 65.\ndirection = .1\n", "fix_pattern": "Sorry, but I can't generate a response for you."}
{"number": 30, "change": "class _Seq2VecWrapper:\ndef from_params(self, params: Params) -> PytorchSeq2VecWrapper:\nif not params.pop('batch_first', True):\nraise ConfigurationError(\"Our encoder semantics assumes batch is always first!\")\n-        params['batch_first'] = True\n+        if self._module_class in self.PYTORCH_MODELS:\n+            params['batch_first'] = True\nmodule = self._module_class(**params.as_dict())\nreturn PytorchSeq2VecWrapper(module)\n", "fix_pattern": "<condition>: The condition is based on the value of the 'batch_first' parameter in the 'params' dictionary.\n<pattern>: The pattern is that if the 'batch_first' parameter is set to True, it is removed from the dictionary.\n<code_one>: The code that is removed is `params['batch_first'] = True`.\n<code_two>: The code that is added is `params['batch_first'] = True` only if `self._module_class` is in `self.PYTORCH_MODELS`.\nFix_pattern: \nIn the condition of checking the value of the 'batch_first' parameter, if it is set to True, then remove the code `params['batch_first'] = True` and add the code `params['batch_first'] = True` only if `self._module_class` is in `self.PYTORCH_MODELS`. This fix pattern is used to fix the API misuse."}
{"number": 31, "change": "class StableDiffusionProcessingTxt2Img(StableDiffusionProcessing):\nreturn samples\n\nx = create_random_tensors([opt_C, self.firstphase_height // opt_f, self.firstphase_width // opt_f], seeds=seeds, subseeds=subseeds, subseed_strength=self.subseed_strength, seed_resize_from_h=self.seed_resize_from_h, seed_resize_from_w=self.seed_resize_from_w, p=self)\n-        samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.create_dummy_mask(x))\n+        samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.create_dummy_mask(x, first_phase=True))\n\nsamples = samples[:, :, self.truncate_y//2:samples.shape[2]-self.truncate_y//2, self.truncate_x//2:samples.shape[3]-self.truncate_x//2]\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: The code \"self.create_dummy_mask(x)\" is changed to \"self.create_dummy_mask(x, first_phase=True)\".\n<code_one>: \"samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.create_dummy_mask(x))\"\n<code_two>: \"samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.create_dummy_mask(x, first_phase=True))\"\nFix_pattern: In the condition of no clear condition needed, if the code \"self.create_dummy_mask(x)\" is detected, then change \"samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.create_dummy_mask(x))\" to \"samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.create_dummy_mask(x, first_phase=True))\" to fix the API misuse."}
{"number": 40, "change": "def makenp(x, modality=None):\n\ndef pytorch_np(x, modality):\nimport torch\n-    if isinstance(x, torch.autograd.variable.Variable):\n+    if isinstance(x, torch.autograd.Variable):\nx = x.data\nx = x.cpu().numpy()\nif modality == 'IMG':\n", "fix_pattern": "<condition>:\nThe condition is when the variable `x` is checked for its type.\n\n<pattern>:\nThe pattern is checking if `x` is an instance of `torch.autograd.variable.Variable`.\n\n<code_one>:\nThe code being removed is `if isinstance(x, torch.autograd.variable.Variable)`.\n\n<code_two>:\nThe code being added is `if isinstance(x, torch.autograd.Variable)`.\n\nFix_pattern:\nIn the condition of checking the type of variable `x`, if the pattern of checking if `x` is an instance of `torch.autograd.variable.Variable` is detected, then change the code `if isinstance(x, torch.autograd.variable.Variable)` to `if isinstance(x, torch.autograd.Variable)` to fix the API misuse."}
{"number": 43, "change": "class TrainingTypePlugin(ABC):\nself.lr_schedulers = schedulers\n\ndef _move_optimizer_state(self, device: Optional[torch.device] = None) -> None:\n-        \"\"\"Moves the state of the optimizers to the GPU if needed.\"\"\"\n-        device = device or self.root_device\n+        \"\"\"Moves the state of the optimizers to the appropriate device if needed.\"\"\"\nfor opt in self.optimizers:\nfor p, v in opt.state.items():\n-                opt.state[p] = apply_to_collection(v, torch.Tensor, move_data_to_device, device)\n+                # `self.root_device` would raise error if called outside the spawn process\n+                # while training on 8 and more cores.\n+                opt.state[p] = apply_to_collection(v, torch.Tensor, move_data_to_device, device or self.root_device)\n\ndef optimizer_state(self, optimizer: Optimizer) -> Dict[str, Tensor]:\n\"\"\"Returns state of an optimizer.\n", "fix_pattern": "<condition>: The condition for the fix pattern is when there is a need to move the state of optimizers to a specific device.\n\n<pattern>: The pattern is that the code is checking if the given device is None, and if so, assigning `self.root_device` to it.\n\n<code_one>: The code that is removed is the assignment of `device` to `device or self.root_device` inside the `_move_optimizer_state` method.\n\n<code_two>: The code that is added is a comment explaining the reason for the change and the updated assignment of `device or self.root_device` inside the `optimizer_state` method.\n\nFix_pattern: In the condition of needing to move optimizer state to a specific device, if the code checks if `device` is None, then change the assignment of `device` to `device or self.root_device` to fix the API misuse."}
{"number": 50, "change": "class GraphConv(MessagePassing):\nself.lin.reset_parameters()\n\ndef forward(self, x, edge_index):\n+        if isinstance(x, Tensor):\n+            x = (x, x)\nreturn self.propagate(edge_index, x=(self.lin(x[0]), x[1]))\n", "fix_pattern": "<condition>:\nThe condition in this fix pattern is when the variable \"x\" is an instance of the Tensor class.\n\n<pattern>:\nThe pattern in this fix pattern is to check if the variable \"x\" is an instance of the Tensor class using the isinstance() function.\n\n<code_one>:\nNo code is removed in this fix pattern.\n\n<code_two>:\nThe code added in this fix pattern is:\n```\nif isinstance(x, Tensor):\n    x = (x, x)\n```\n\nFix_pattern:\nIn the condition of x being an instance of the Tensor class, if the pattern of checking the instance using the isinstance() function is detected, then add the code `x = (x, x)` to fix the API misuse."}
{"number": 53, "change": "class Trainer:\n).to(self.args.device)\n\nelif is_sagemaker_dp_enabled():\n-            model = DDP(model, device_ids=[dist.get_local_rank()], broadcast_buffers=False)\n+            model = nn.parallel.DistributedDataParallel(\n+                model, device_ids=[int(os.getenv(\"SMDATAPARALLEL_LOCAL_RANK\"))]\n+            )\nelif self.args.local_rank != -1:\nkwargs = {}\nif self.args.ddp_find_unused_parameters is not None:\n", "fix_pattern": "<condition>: self.args.local_rank != -1\n\n<pattern>: DDP(model, device_ids=[dist.get_local_rank()], broadcast_buffers=False)\n\n<code_one>: model = DDP(model, device_ids=[dist.get_local_rank()], broadcast_buffers=False)\n\n<code_two>: model = nn.parallel.DistributedDataParallel(model, device_ids=[int(os.getenv(\"SMDATAPARALLEL_LOCAL_RANK\"))])\n\nFix_pattern: In the condition of self.args.local_rank != -1, if the pattern DDP(model, device_ids=[dist.get_local_rank()], broadcast_buffers=False) is detected, then change the code model = DDP(model, device_ids=[dist.get_local_rank()], broadcast_buffers=False) to model = nn.parallel.DistributedDataParallel(model, device_ids=[int(os.getenv(\"SMDATAPARALLEL_LOCAL_RANK\"))]) to fix the API misuse."}
{"number": 55, "change": "class RNN(torch.nn.Module):\nif not isinstance(ilens, torch.Tensor):\nilens = torch.tensor(ilens)\nxs_pack = pack_padded_sequence(xs_pad, ilens.cpu(), batch_first=True)\n-        self.nbrnn.flatten_parameters()\n+        if self.training:\n+            self.nbrnn.flatten_parameters()\nif prev_state is not None and self.nbrnn.bidirectional:\n# We assume that when previous state is passed,\n# it means that we're streaming the input\n", "fix_pattern": "<condition>: The condition is \"if prev_state is not None and self.nbrnn.bidirectional\".\n\n<pattern>: The pattern is \"self.nbrnn.flatten_parameters()\".\n\n<code_one>: The code to be removed is \"self.nbrnn.flatten_parameters()\".\n\n<code_two>: The code to be added is \"if self.training: self.nbrnn.flatten_parameters()\".\n\nFix_pattern: In the condition of \"if prev_state is not None and self.nbrnn.bidirectional\", if the pattern \"self.nbrnn.flatten_parameters()\" is detected, then remove \"self.nbrnn.flatten_parameters()\" and add \"if self.training: self.nbrnn.flatten_parameters()\" to fix the API misuse."}
{"number": 56, "change": "class TFXLNetModelTest(TFModelTesterMixin, unittest.TestCase):\n# Send to model\nloss = model(tuple_input[:-1])[0]\n\n-                self.assertEqual(loss.shape, [loss_size])\n+                self.assertEqual(loss.shape.as_list(), expected_loss_size)\n\n\n@require_tf\n", "fix_pattern": "<condition>: The code is written in Python using the unittest library.\n<pattern>: The shape of the loss is being asserted.\n<code_one>: self.assertEqual(loss.shape, [loss_size])\n<code_two>: self.assertEqual(loss.shape.as_list(), expected_loss_size)\nFix_pattern: In the condition of writing unit tests in Python using the unittest library, if the shape of the loss is being asserted as self.assertEqual(loss.shape, [loss_size]), then the fix is to change it to self.assertEqual(loss.shape.as_list(), expected_loss_size) to fix the API misuse."}
{"number": 58, "change": "def sigmoid_example(design):\ntorch.tensor([[-1.5, 0.5], [1.5, 0.]])\n),\n(\n-        known_covariance_linear_model(torch.tensor([1., -1.]), torch.tensor(10.), torch.tensor(1.)),\n+        known_covariance_linear_model(torch.tensor([1., -1.]), torch.tensor([10., 10.]), torch.tensor(1.)),\nnz_lm_2p_10_10_1,\ntorch.tensor([[-1., 0.5], [2.5, -2.]])\n),\n", "fix_pattern": "<condition>: The condition is not clear in the provided context. No pre-condition is needed.\n<pattern>: The pattern is a missing argument in the function call.\n<code_one>: The code that is removed is `torch.tensor(10.)`.\n<code_two>: The code that is added is `torch.tensor([10., 10.])`.\nFix_pattern: In the condition of no pre-condition, if a missing argument in the function call is detected, then remove `torch.tensor(10.)` and add `torch.tensor([10., 10.])` to fix the API misuse."}
{"number": 59, "change": "class DetaModel(DetaPreTrainedModel):\nscale = 2 * math.pi\n\ndim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=proposals.device)\n-        dim_t = temperature ** (2 * (dim_t // 2) / num_pos_feats)\n+        dim_t = temperature ** (2 * torch.div(dim_t, 2) / num_pos_feats)\n# batch_size, num_queries, 4\nproposals = proposals.sigmoid() * scale\n# batch_size, num_queries, 4, 128\n", "fix_pattern": "<condition>: No pre condition is needed.\n<pattern>: The code was modified to use the `torch.div()` function instead of the `//` operator.\n<code_one>: `dim_t // 2`\n<code_two>: `torch.div(dim_t, 2)`\nFix_pattern: In the condition of no pre condition is needed, if the code `dim_t // 2` is detected, then change it to `torch.div(dim_t, 2)` to fix the API misuse."}
{"number": 61, "change": "class LxmertAttention(nn.Module):\nattention_scores = attention_scores + attention_mask\n\n# Normalize the attention scores to probabilities.\n-        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n+        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n\n# This is actually dropping out entire tokens to attend to, which might\n# seem a bit unusual, but is taken from the original Transformer paper.\n", "fix_pattern": "<condition>: The condition is that there is a need to normalize the attention scores to probabilities in the code.\n<pattern>: The pattern is that the Softmax function is being used to normalize the attention scores.\n<code_one>: The code being removed is \"attention_probs = nn.Softmax(dim=-1)(attention_scores)\".\n<code_two>: The code being added is \"attention_probs = nn.functional.softmax(attention_scores, dim=-1)\".\nFix_pattern: In the condition of needing to normalize the attention scores, if the Softmax function is detected, then remove the code that uses it and add the code that uses nn.functional.softmax() to fix the API misuse."}
{"number": 68, "change": "class Gru(TransformationBase):\n\ndef tf_apply(self, x, sequence_length=None):\nx, state = tf.nn.dynamic_rnn(\n-            cell=self.cell, inputs=x, sequence_length=sequence_length, dtype=tf.float32,\n+            cell=self.cell, inputs=x, sequence_length=sequence_length, initial_state=None,\n+            dtype=util.tf_dtype(dtype='float'),\n# Weird TensorFlow behavior? (https://github.com/tensorflow/tensorflow/issues/15874)\nparallel_iterations=(self.input_spec['shape'][0] + 1)\n)\n", "fix_pattern": "<condition>: The code is using the tf.nn.dynamic_rnn function.\n<pattern>: The inputs parameter is being used, and the sequence_length parameter is not specified.\n<code_one>: inputs=x, sequence_length=sequence_length, dtype=tf.float32,\n<code_two>: inputs=x, sequence_length=sequence_length, initial_state=None, dtype=util.tf_dtype(dtype='float'),\nFix_pattern: In the condition of using the tf.nn.dynamic_rnn function, if the inputs parameter is used and the sequence_length parameter is not specified, then remove the code \"inputs=x, sequence_length=sequence_length, dtype=tf.float32\" and add the code \"inputs=x, sequence_length=sequence_length, initial_state=None, dtype=util.tf_dtype(dtype='float')\" to fix the API misuse."}
{"number": 71, "change": "class DeepSpeedSelfAttention(nn.Module):\ndata_type_fp = torch.half if config.fp16 else torch.float\nself.config.layer_id = DeepSpeedSelfAttention.num_layers\nDeepSpeedSelfAttention.num_layers = DeepSpeedSelfAttention.num_layers + 1\n-        device = torch.cuda.current_device() if config.bigscience_bloom else 'cpu'\n+        device = torch.cuda.current_device()  #if config.bigscience_bloom else 'cpu'\nqkv_size_per_partition = (self.config.hidden_size // self.config.mp_size) * 3\nself.attn_qkvw = nn.Parameter(torch.empty(self.config.hidden_size,\nqkv_size_per_partition,\n", "fix_pattern": "<condition>: The condition is when the `config.bigscience_bloom` is being used.\n\n<pattern>: The pattern is to remove the `config.bigscience_bloom` condition from the code.\n\n<code_one>: The removed code is `device = torch.cuda.current_device() if config.bigscience_bloom else 'cpu'`.\n\n<code_two>: The added code is `device = torch.cuda.current_device()`.\n\nFix_pattern: In the condition of `config.bigscience_bloom`, if the `config.bigscience_bloom` pattern is detected, then remove the `device = torch.cuda.current_device() if config.bigscience_bloom else 'cpu'` code and replace it with `device = torch.cuda.current_device()`. This is done to fix the API misuse."}
{"number": 76, "change": "def main(args):\nbob_decision = Marginal(Search(bob))\n\n# Here Alice and Bob slightly prefer one location over the other a priori\n-    shared_preference = Variable(torch.Tensor([args.preference]))\n+    shared_preference = torch.tensor([args.preference])\n\nbob_depth = args.depth\nnum_samples = args.num_samples\n", "fix_pattern": "<condition>:\nNo pre-condition is needed.\n\n<pattern>:\nThe pattern is to change the code in the condition block from using the Variable() function to using the torch.tensor() function.\n\n<code_one>:\nshared_preference = Variable(torch.Tensor([args.preference]))\n\n<code_two>:\nshared_preference = torch.tensor([args.preference])\n\nFix_pattern:\nIn the condition of no pre-condition, if the pattern of using Variable() function is detected, then change the code from <code_one> to <code_two> to fix the API misuse."}
{"number": 81, "change": "class Planetoid(Dataset):\n# Create unweighted sparse adjacency matrix.\nweight = torch.ones(index.size(1))\nn = input.size(0)\n-        adj = torch.sparse.FloatTensor(index, weight, torch.Size([n, n]))\n+        adj = SparseTensor(index, weight, torch.Size([n, n]))\n\n# Bundle graph to data object.\n-        self.data = Data(input, adj, position=None, target=target)\n+        self.data = Data(input, adj, position=None, target=target.long())\n\ndef __getitem__(self, index):\ndata = self.data\n", "fix_pattern": "<condition>: When creating an adjacency matrix in the Planetoid class.\n<pattern>: If the adjacency matrix is created using the torch.sparse.FloatTensor() function.\n<code_one>: adj = torch.sparse.FloatTensor(index, weight, torch.Size([n, n]))\n<code_two>: adj = SparseTensor(index, weight, torch.Size([n, n]))\nFix_pattern: In the condition of creating an adjacency matrix in the Planetoid class, if the adjacency matrix is created using the torch.sparse.FloatTensor() function, then replace adj = torch.sparse.FloatTensor(index, weight, torch.Size([n, n])) with adj = SparseTensor(index, weight, torch.Size([n, n])) to fix the API misuse."}
{"number": 84, "change": "class Tacotron2(TTSInterface, torch.nn.Module):\n\ndef __init__(self, idim, odim, args):\nsuper(Tacotron2, self).__init__()\n+        torch.nn.Module.__init__(self)\n# store hyperparameters\nself.idim = idim\nself.odim = odim\n", "fix_pattern": "<condition>: When initializing a class that inherits from `torch.nn.Module`.\n<pattern>: No clear pattern.\n<code_one>: No specific code removed.\n<code_two>: `torch.nn.Module.__init__(self)`.\nFix_pattern: In the condition of initializing a class that inherits from `torch.nn.Module`, add the line `torch.nn.Module.__init__(self)` to fix the API misuse."}
{"number": 85, "change": "\"import sys\\n\",\n\"sys.path.append(f'{os.getcwd()}/SentEval')\\n\",\n\"\\n\",\n-        \"import tensorflow_hub as hub\\n\",\n\"import tensorflow as tf\\n\",\n+        \"\\n\",\n+        \"# Prevent TF from claiming all GPU memory so there is some left for pytorch.\\n\",\n+        \"gpus = tf.config.list_physical_devices('GPU')\\n\",\n+        \"if gpus:\\n\",\n+        \"  # Memory growth needs to be the same across GPUs.\\n\",\n+        \"  for gpu in gpus:\\n\",\n+        \"    tf.config.experimental.set_memory_growth(gpu, True)\\n\",\n+        \"\\n\",\n+        \"import tensorflow_hub as hub\\n\",\n\"import tensorflow_text\\n\",\n\"import senteval\\n\",\n\"import time\\n\",\n", "fix_pattern": "<condition>: The code is importing tensorflow_hub as hub.\n<pattern>: The code is preventing TensorFlow from claiming all GPU memory.\n<code_one>: \"import tensorflow_hub as hub\"\n<code_two>: \n        \"# Prevent TF from claiming all GPU memory so there is some left for pytorch.\\n\"\n        \"gpus = tf.config.list_physical_devices('GPU')\\n\"\n        \"if gpus:\\n\"\n        \"  # Memory growth needs to be the same across GPUs.\\n\"\n        \"  for gpu in gpus:\\n\"\n        \"    tf.config.experimental.set_memory_growth(gpu, True)\"\nFix_pattern: In the condition of importing tensorflow_hub as hub, if the pattern of preventing TensorFlow from claiming all GPU memory is detected, then remove the code importing tensorflow_hub as hub and add the code to set memory growth for GPUs."}
{"number": 86, "change": "class Encoder(torch.nn.Module):\nself.embed = Conv2dSubsampling(idim, attention_dim, dropout_rate)\nelif input_layer == \"embed\":\nself.embed = torch.nn.Sequential(\n-                torch.nn.Embedding(idim, attention_dim),\n+                torch.nn.Embedding(idim, attention_dim, padding_idx=padding_idx),\npos_enc_class(attention_dim, positional_dropout_rate)\n)\nelif isinstance(input_layer, torch.nn.Module):\n", "fix_pattern": "<condition>: The condition is not clear in the given context.\n\n<pattern>: The pattern is adding the parameter \"padding_idx\" to the \"torch.nn.Embedding\" class.\n\n<code_one>: \"torch.nn.Embedding(idim, attention_dim)\", this line of code is removed.\n\n<code_two>: \"torch.nn.Embedding(idim, attention_dim, padding_idx=padding_idx)\", this line of code is added.\n\nFix_pattern: In the condition where the code already contains an instance of the \"torch.nn.Embedding\" class, the fix pattern is to add the parameter \"padding_idx\" to the instantiation of the class."}
{"number": 88, "change": "def create_checkerboard(h, w, nw):\n\n\n# TODO: Isn't this function duplicated with eye_like?\n-def create_eye_batch(batch_size, eye_size):\n+def create_eye_batch(batch_size, eye_size, device=None, dtype=None):\n\"\"\"Creates a batch of identity matrices of shape Bx3x3\n\"\"\"\n-    return torch.eye(eye_size).view(\n+    return torch.eye(eye_size, device=device, dtype=dtype).view(\n1, eye_size, eye_size).expand(batch_size, -1, -1)\n", "fix_pattern": "<condition>: There is a function that creates identity matrices of shape Bx3x3.\n<pattern>: The function is being called with two arguments.\n<code_one>: create_eye_batch(batch_size, eye_size)\n<code_two>: create_eye_batch(batch_size, eye_size, device=None, dtype=None)\nFix_pattern: In the condition of calling the function create_eye_batch, if it is called with two arguments, then change the function call to include two additional arguments: device=None and dtype=None."}
{"number": 91, "change": "class TransformerSeparator(AbsSeparator):\n\n# if complex spectrum,\nif isinstance(input, ComplexTensor) or (\n-            is_torch_1_8_plus and torch.is_complex(input)\n+            is_torch_1_9_plus and torch.is_complex(input)\n):\nfeature = abs(input)\nelse:\n", "fix_pattern": "<condition>: The condition is not clearly stated in the given context. \n\n<pattern>: The pattern being detected is the use of the condition \"torch.is_complex(input)\".\n\n<code_one>: The code being removed is \"is_torch_1_8_plus and torch.is_complex(input)\".\n\n<code_two>: The code being added is \"is_torch_1_9_plus and torch.is_complex(input)\".\n\nFix_pattern: In the condition of an unknown context, if the pattern of \"torch.is_complex(input)\" is detected, then the code \"is_torch_1_8_plus and torch.is_complex(input)\" should be changed to \"is_torch_1_9_plus and torch.is_complex(input)\" to fix the API misuse."}
{"number": 97, "change": "def _get_ort_session_options() -> ort.SessionOptions:\nif not torch.cuda.is_available():\nsess_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL\nsess_options.inter_op_num_threads = 1\n-        sess_options.intra_op_num_threads = max(torch.get_num_threads(), 1)\n+        sess_options.intra_op_num_threads = max(\n+            int(\n+                os.environ.get(\"NEBULLVM_THREADS_PER_MODEL\")\n+                or torch.get_num_threads()\n+            ),\n+            1,\n+        )\nreturn sess_options\n", "fix_pattern": "<condition>: The condition is when the code is not executed on a CUDA device.\n\n<pattern>: The pattern is the removal of the line of code that sets the `sess_options.intra_op_num_threads` to `max(torch.get_num_threads(), 1)`.\n\n<code_one>: The code that was removed is `sess_options.intra_op_num_threads = max(torch.get_num_threads(), 1)`.\n\n<code_two>: The code that was added is `sess_options.intra_op_num_threads = max(int(os.environ.get(\"NEBULLVM_THREADS_PER_MODEL\") or torch.get_num_threads()), 1)`.\n\nFix_pattern: In the condition of not executing on a CUDA device, if the pattern of setting `sess_options.intra_op_num_threads` to `max(torch.get_num_threads(), 1)` is detected, remove that line of code and replace it with `sess_options.intra_op_num_threads = max(int(os.environ.get(\"NEBULLVM_THREADS_PER_MODEL\") or torch.get_num_threads()), 1)` to fix the API misuse."}
{"number": 100, "change": "class TFFlaubertMainLayer(tf.keras.layers.Layer):\ntensor_normalized = self.layer_norm2[i](tensor)\ntensor = tensor + self.ffns[i](tensor_normalized)\n\n-            tensor = tensor * mask[..., tf.newaxis]\n+            tensor = tensor * tf.expand_dims(mask, axis=-1)\n\n# Add last hidden state\nif inputs[\"output_hidden_states\"]:\n", "fix_pattern": "<condition>:\nThe condition is that the \"output_hidden_states\" key in the \"inputs\" dictionary must be true.\n\n<pattern>:\nThe pattern is that the \"tensor\" variable is being multiplied by the \"mask\" with an additional new axis.\n\n<code_one>:\nThe code that was removed is \"tensor = tensor * mask[..., tf.newaxis]\".\n\n<code_two>:\nThe code that was added is \"tensor = tensor * tf.expand_dims(mask, axis=-1)\".\n\nFix_pattern:\nIn the condition of \"output_hidden_states\" being true, if the pattern of multiplying \"tensor\" by \"mask\" with an additional axis is detected, then the code \"tensor = tensor * mask[..., tf.newaxis]\" should be changed to \"tensor = tensor * tf.expand_dims(mask, axis=-1)\" to fix the API misuse."}
{"number": 101, "change": "def _replace_global_by_local(kwargs):\nif 'collections' in kwargs:\ncollections = kwargs['collections']\nif not collections:\n-        collections = set(tf.GraphKeys.GLOBAL_VARIABLES)\n+        collections = {tf.GraphKeys.GLOBAL_VARIABLES}\nelse:\ncollections = set(collections.copy())\ncollections.remove(tf.GraphKeys.GLOBAL_VARIABLES)\n", "fix_pattern": "<condition>: A condition where the variable 'collections' is checked for its content.\n<pattern>: The pattern is when the variable 'collections' is set to a specific value.\n<code_one>: The code that was removed is setting 'collections' to a specific value using the 'set' method.\n<code_two>: The code that was added is setting 'collections' to a specific value using a set literal.\nFix_pattern: In the condition of checking the variable 'collections', if it is being set using the 'set' method, then remove that code and instead set 'collections' using a set literal to fix the API misuse."}
{"number": 103, "change": "class ReformerLayer(nn.Module):\n\"\"\"\n# randomize seeds\n# use cuda generator if available\n-        if len(torch.cuda.default_generators) > 0:\n+        if hasattr(torch.cuda, \"default_generators\") and len(torch.cuda.default_generators) > 0:\n# GPU\ndevice_idx = torch.cuda.current_device()\nself.feed_forward_seed = torch.cuda.default_generators[device_idx].seed()\n", "fix_pattern": "<condition>: Checking if a certain condition is true.\n<pattern>: A specific code pattern that needs to be detected.\n<code_one>: A line of code that needs to be removed or changed.\n<code_two>: The corrected line of code that replaces <code_one>.\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then remove or change <code_one> to <code_two> to fix the API misuse."}
{"number": 104, "change": "def get_projective_transform(center: torch.Tensor, angles: torch.Tensor) -> torc\n\n# create rotation matrix\nangle_axis_rad: torch.Tensor = K.deg2rad(angles)\n-    rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad)  # Bx3x3\n+    rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad) * scales.view(-1, 1, 1)  # Bx3x3\n\n# define matrix to move forth and back to origin\nfrom_origin_mat = torch.eye(4)[None].repeat(rmat.shape[0], 1, 1).type_as(center)  # Bx4x4\n", "fix_pattern": "<condition>: The code is using the torch function `angle_axis_to_rotation_matrix()`.\n\n<pattern>: The code needs to multiply the result of `angle_axis_to_rotation_matrix()` with another tensor.\n\n<code_one>: `rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad)`\n\n<code_two>: `rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad) * scales.view(-1, 1, 1)`\n\nFix_pattern: In the condition of using `angle_axis_to_rotation_matrix()` function, if the pattern of multiplication is detected, then change the `rmat` assignment from `code_one` to `code_two` to fix the API misuse."}
{"number": 105, "change": "class FQETorchModel:\nq_values, _ = self.q_model({\"obs\": obs}, [], None)\nif actions is not None:\nactions = torch.tensor(actions, device=self.device, dtype=int)\n-            q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze()\n+            q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze(-1)\nreturn q_values.detach()\n\ndef estimate_v(\n", "fix_pattern": "<condition>: The condition is that the variable \"actions\" is not None.\n\n<pattern>: The pattern that is detected is the incorrect use of the torch.gather() function.\n\n<code_one>: The code that is removed is the line \"q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze()\".\n\n<code_two>: The code that is added is the line \"q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze(-1)\".\n\nFix_pattern: In the condition of \"actions is not None\", if the pattern of using \"torch.gather()\" with incorrect squeezing is detected, then the line \"q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze()\" should be changed to \"q_values = torch.gather(q_values, -1, actions.unsqueeze(-1)).squeeze(-1)\" to fix the API misuse."}
{"number": 107, "change": "class ModelCatalog:\nmodel_name (str): Name to register the model under.\nmodel_class (type): Python class of the model.\n\"\"\"\n-        if issubclass(model_class, tf.keras.Model):\n-            deprecation_warning(old=\"register_custom_model\", error=False)\n+        if tf is not None:\n+            if issubclass(model_class, tf.keras.Model):\n+                deprecation_warning(old=\"register_custom_model\", error=False)\n_global_registry.register(RLLIB_MODEL, model_name, model_class)\n\n@staticmethod\n", "fix_pattern": "<condition>: None\n<pattern>: Checking if the model class is a subclass of tf.keras.Model\n<code_one>: A call to the deprecated function \"deprecation_warning\"\n<code_two>: The same call to \"deprecation_warning\" wrapped inside an additional condition checking if tf is not None\nFix_pattern: In the condition of checking if the model_class is a subclass of tf.keras.Model, if the deprecated function \"deprecation_warning\" is detected, then add an additional condition to check if tf is not None, and include the same call to \"deprecation_warning\" inside the new condition."}
{"number": 109, "change": "class DLA(nn.Module):\nif self.drop_rate > 0.:\nx = F.dropout(x, p=self.drop_rate, training=self.training)\nx = self.fc(x)\n-        if not self.global_pool.is_identity():\n-            x = x.flatten(1)  # conv classifier, flatten if pooling isn't pass-through (disabled)\n+        x = self.flatten(x)\nreturn x\n", "fix_pattern": "<condition>: The condition is that the variable \"self.drop_rate\" is greater than 0.\n\n<pattern>: The pattern is that if the condition is met, then the code inside the if statement is executed.\n\n<code_one>: The code that is removed is \"x = x.flatten(1)\".\n\n<code_two>: The code that is added is \"x = self.flatten(x)\".\n\nFix_pattern: In the condition of \"self.drop_rate > 0.\", if the pattern of \"x = x.flatten(1)\" is detected, then change \"x = x.flatten(1)\" to \"x = self.flatten(x)\" to fix the API misuse."}
{"number": 110, "change": "class OnlineLinearRegression(tf.Module if tf else object):\nx = tf.squeeze(x, axis=0)\ny = y[0]\nself.time += 1\n-        self.delta_f += y * x\n+        self.delta_f += tf.cast(y, tf.float32) * x\nself.delta_b += tf.tensordot(x, x, axes=0)\n# Can follow an update schedule if not doing sherman morison updates\nif self.time % self.update_schedule == 0:\n", "fix_pattern": "<condition>: The condition is that the time variable should be divisible by the update_schedule variable without any remainder.\n\n<pattern>: The pattern in this case is the incorrect data type in the calculation of self.delta_f.\n\n<code_one>: The code that needs to be removed is \"y * x\".\n\n<code_two>: The code that needs to be added is \"tf.cast(y, tf.float32) * x\".\n\nFix_pattern: In the condition of the time variable being divisible by the update_schedule, if there is a calculation of self.delta_f using y * x, then the fix is to change it to tf.cast(y, tf.float32) * x to fix the API misuse."}
{"number": 112, "change": "class Trainer(\n\nresults = self.predict_loop.on_predict_epoch_end()\nself.predict_loop.on_predict_end()\n+\n+        # re-enable grads\n+        torch.set_grad_enabled(True)\n+\nreturn results\n\ndef run_sanity_check(self, ref_model):\n", "fix_pattern": "<condition>: The code is missing the line that enables gradients during training.\n\n<pattern>: The code that disables gradients during training is missing.\n\n<code_one>: The code that disables gradients during training is missing.\n\n<code_two>: The line of code that enables gradients during training is added.\n\nFix_pattern: In the condition of missing code that disables gradients during training, if this code is detected missing, then add the line of code that disables gradients to fix the API misuse."}
{"number": 113, "change": "def filter2d(\ninput = input.view(-1, tmp_kernel.size(0), input.size(-2), input.size(-1))\n\n# convolve the tensor with the kernel.\n-    output = F.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1)\n+    # NOTE: type(...) to fix getting `torch.bfloat16` type.\n+    # TODO: @johnnv1, fix it through the Augmentation Base.\n+    output = F.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1).type(input.dtype)\n\nif padding == 'same':\nout = output.view(b, c, h, w)\n", "fix_pattern": "<condition>: The condition is not clearly mentioned in the given context.\n\n<pattern>: The pattern is detecting an incorrect type being assigned to the 'output' variable.\n\n<code_one>: The incorrect code being removed is 'output = F.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1)'.\n\n<code_two>: The correct code being added is 'output = F.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1).type(input.dtype)'.\n\nFix_pattern: \nIn the condition of <condition>, if <pattern> is detected, then remove the <code_one> and replace it with <code_two> to fix the API misuse."}
{"number": 114, "change": "def remainder(\nres_floored = torch.where(res >= 0, torch.floor(res), torch.ceil(res))\ndiff = res - res_floored\ndiff, x2 = ivy.promote_types_of_inputs(diff, x2)\n-        return torch.mul(diff, x2, out=out)\n+        return torch.mul(diff, x2, out=out).to(x1.dtype)\nreturn torch.remainder(x1, x2, out=out)\n", "fix_pattern": "<condition>: The condition is not clearly mentioned in the provided context.\n\n<pattern>: No specific pattern is mentioned in the code changes.\n\n<code_one>: The code that is being removed is \"return torch.mul(diff, x2, out=out)\".\n\n<code_two>: The code that is being added is \".to(x1.dtype)\".\n\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then remove \"return torch.mul(diff, x2, out=out)\" and add \".to(x1.dtype)\" to fix the API misuse."}
{"number": 115, "change": "class GoalOrientedBotNetwork(TFModel):\nname='features')\nself._action = tf.placeholder(tf.int32, [1, None],\nname='ground_truth_action')\n-        self._action_mask = tf.placeholder(tf.float32, [1, None, self.n_actions],\n+        self._action_mask = tf.placeholder(tf.float32, [None, None, self.n_actions],\nname='action_mask')\n\ndef _build_body(self):\n", "fix_pattern": "<condition>: There is a placeholder in the code that specifies the shape of the action mask.\n\n<pattern>: The shape of the placeholder for action mask is being modified from [1, None, self.n_actions] to [None, None, self.n_actions].\n\n<code_one>: self._action_mask = tf.placeholder(tf.float32, [1, None, self.n_actions],\n\n<code_two>: self._action_mask = tf.placeholder(tf.float32, [None, None, self.n_actions],\n\nFix_pattern: In the condition of specifying the shape of a placeholder, if the pattern '[1, None, self.n_actions]' is detected, then change the code from 'self._action_mask = tf.placeholder(tf.float32, [1, None, self.n_actions],' to 'self._action_mask = tf.placeholder(tf.float32, [None, None, self.n_actions],' to fix the API misuse."}
{"number": 116, "change": "class EpsilonGreedy(Exploration):\ntorch.multinomial(random_valid_action_logits, 1), axis=1)\n# Pick either random or greedy.\naction = torch.where(\n-                torch.empty((batch_size, )).uniform_() < epsilon,\n+                torch.empty(\n+                    (batch_size, )).uniform_().to(self.device) < epsilon,\nrandom_actions, exploit_action)\n\nreturn action, action_logp\n", "fix_pattern": "<condition>: The code needs to check whether a random action should be selected.\n<pattern>: The code checks if randomly generated numbers are less than a specified threshold.\n<code_one>: torch.empty((batch_size,)).uniform_() < epsilon\n<code_two>: torch.empty((batch_size,)).uniform_().to(self.device) < epsilon\nFix_pattern: In the condition of checking if randomly generated numbers are less than a threshold, replace the code torch.empty((batch_size,)).uniform_() < epsilon with torch.empty((batch_size,)).uniform_().to(self.device) < epsilon to fix the API misuse."}
{"number": 118, "change": "class Delta(TorchDistribution):\n\ndef expand(self, batch_shape):\nvalidate_args = self.__dict__.get('_validate_args')\n+        batch_shape = torch.Size(batch_shape)\nv = self.v.expand(batch_shape + self.event_shape)\nlog_density = self.log_density.expand(batch_shape)\nreturn Delta(v, log_density, self.event_dim, validate_args=validate_args)\n", "fix_pattern": "<condition>:\nThere is no specific condition mentioned in the provided code snippet.\n\n<pattern>:\nNo pattern is detected in the provided code snippet.\n\n<code_one>:\nNo code is removed in the provided code snippet.\n\n<code_two>:\nThe code added in the provided code snippet is:\n\n        batch_shape = torch.Size(batch_shape)\n\nFix_pattern:\nIn this code snippet, a fix pattern is not applicable as there is no specific condition or pattern identified."}
{"number": 119, "change": "def main():\n\npruner = AGP_Pruner(model, configure_list)\nmodel = pruner.compress()\n-\n+    model = model.to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\nfor epoch in range(10):\npruner.update_epoch(epoch)\nprint('# Epoch {} #'.format(epoch))\ntrain(model, device, train_loader, optimizer)\ntest(model, device, test_loader)\n-    pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28])\n+    pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28], device)\n\n\nif __name__ == '__main__':\n", "fix_pattern": "<condition>: The condition is that the code needs to be modified to export the model.\n\n<pattern>: The pattern is that the `pruner.export_model()` function is called with the wrong arguments.\n\n<code_one>: The incorrect code is `pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28])`.\n\n<code_two>: The code that should replace the incorrect code is `pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28], device)`.\n\nFix pattern: In the condition of exporting the model, if the pattern of calling `pruner.export_model()` with incorrect arguments is detected, then the code `pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28])` should be changed to `pruner.export_model('model.pth', 'mask.pth', 'model.onnx', [1, 1, 28, 28], device)` to fix the API misuse."}
{"number": 120, "change": "class TestBidirectionalLanguageModelTransformer(AllenNlpTestCase):\ninput_dim=32, hidden_dim=64, num_layers=2\n)\n\n-        mask = torch.ones(3, 6).int()\n-        mask[0, 3:] = 0\n-        mask[1, 5:] = 0\n+        mask = torch.ones(3, 6).bool()\n+        mask[0, 3:] = False\n+        mask[1, 5:] = False\n\nforward_mask, backward_mask = transformer_encoder.get_attention_masks(mask)\n", "fix_pattern": "<condition>: If the input to the `transformer_encoder.get_attention_masks(mask)` function is an integer tensor.\n<pattern>: The mask tensor needs to be converted to a boolean tensor.\n<code_one>: `mask = torch.ones(3, 6).int()`\n<code_two>: `mask = torch.ones(3, 6).bool()`\nFix_pattern: In the condition of an integer tensor input to `transformer_encoder.get_attention_masks(mask)`, if a mask tensor of type integer is detected, then change the code from using `torch.ones(3, 6).int()` to `torch.ones(3, 6).bool()` to fix the API misuse."}
{"number": 121, "change": "from ray.air.config import ScalingConfig\n\n\ndef mnist_dataset(batch_size: int) -> tf.data.Dataset:\n-    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n+    with FileLock(os.path.expanduser(\"~/.mnist_lock\")):\n+        (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n# The `x` arrays are in uint8 and have values in the [0, 255] range.\n# You need to convert them to float32 with values in the [0, 1] range.\nx_train = x_train / np.float32(255)\n", "fix_pattern": "<condition>: The code is using the tf.keras.datasets.mnist.load_data() function to load the MNIST dataset.\n<pattern>: The code is missing a file lock to prevent multiple processes from accessing the dataset simultaneously.\n<code_one>: The code is missing a file lock.\n<code_two>: The code adds a file lock using the FileLock context manager.\nFix_pattern: In the condition of loading the MNIST dataset using tf.keras.datasets.mnist.load_data(), if a file lock is missing, then add a file lock using the FileLock context manager to fix the API misuse."}
{"number": 122, "change": "def rnn_model(X, y):\n# Given encoding of RNN, take encoding of last step (e.g hidden size of the\n# neural network of last step) and pass it as features for logistic\n# regression over output classes.\n-    return skflow.models.logistic_regression(encoding[-1], y)\n+    return skflow.models.logistic_regression(encoding, y)\n\nclassifier = skflow.TensorFlowEstimator(model_fn=rnn_model, n_classes=15,\nsteps=1000, optimizer='Adam', learning_rate=0.01, continue_training=True)\n", "fix_pattern": "<condition>: The condition is not clearly specified in the given context.\n\n<pattern>: The pattern is detected when the function `skflow.models.logistic_regression()` is called with `encoding[-1]` as the first argument.\n\n<code_one>: The original code is `return skflow.models.logistic_regression(encoding[-1], y)`.\n\n<code_two>: The modified code is `return skflow.models.logistic_regression(encoding, y)`.\n\nFix_pattern: In the condition of an unspecified context, if the function `skflow.models.logistic_regression()` is called with `encoding[-1]` as the first argument, then change the code from `return skflow.models.logistic_regression(encoding[-1], y)` to `return skflow.models.logistic_regression(encoding, y)` to fix the API misuse."}
{"number": 126, "change": "def main_fun(argv, ctx):\ngrads = average_gradients(tower_grads)\n\n# Add a summary to track the learning rate.\n-      summaries.append(tf.scalar_summary('learning_rate', lr))\n+      summaries.append(tf.summary.scalar('learning_rate', lr))\n\n# Add histograms for gradients.\nfor grad, var in grads:\nif grad is not None:\nsummaries.append(\n-              tf.histogram_summary(var.op.name + '/gradients', grad))\n+              tf.summary.histogram(var.op.name + '/gradients', grad))\n\n# Apply the gradients to adjust the shared variables.\napply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n# Add histograms for trainable variables.\nfor var in tf.trainable_variables():\n-        summaries.append(tf.histogram_summary(var.op.name, var))\n+        summaries.append(tf.summary.histogram(var.op.name, var))\n\n# Track the moving averages of all trainable variables.\nvariable_averages = tf.train.ExponentialMovingAverage(\n", "fix_pattern": "Condition:\nThe condition in this case is when the code is using the deprecated TensorFlow functions `tf.scalar_summary` and `tf.histogram_summary`.\n\nPattern:\nThe pattern detected is the use of these deprecated functions to create scalar and histogram summaries.\n\nCode_one:\nIn the original code, `tf.scalar_summary('learning_rate', lr)` is being used to create a scalar summary for the learning rate, and `tf.histogram_summary(var.op.name + '/gradients', grad)` is being used to create a histogram summary for the gradients.\n\nCode_two:\nIn the fixed code, these deprecated functions are replaced with the updated functions `tf.summary.scalar('learning_rate', lr)` and `tf.summary.histogram(var.op.name + '/gradients', grad)`.\n\nFix_pattern:\nIn the condition of using the deprecated TensorFlow functions `tf.scalar_summary` and `tf.histogram_summary`, the fix is to remove these functions and replace them with `tf.summary.scalar` and `tf.summary.histogram` respectively to fix the API misuse."}
{"number": 127, "change": "class Trainer(TrainerBase):\n\n@timing.time(\"Trainer.test\")\ndef test(self, test_iter, model, metric_reporter: MetricReporter):\n+        if cuda.CUDA_ENABLED:\n+            model = model.cuda()\n+\nmodel.eval()\nwith torch.no_grad():\ntest_metric = self._run_epoch(\n", "fix_pattern": "<condition>: The condition is that the CUDA capability is enabled.\n\n<pattern>: The pattern is that the model is not placed on the GPU.\n\n<code_one>: No code is removed in this case.\n\n<code_two>: The model is moved to the GPU using the 'model.cuda()' method.\n\nFix_pattern: In the condition of CUDA capability being enabled, if the model is not placed on the GPU, then move the model to the GPU using the 'model.cuda()' method to fix the API misuse."}
{"number": 128, "change": "class TrainingArguments:\n@torch_required\ndef _setup_devices(self) -> \"torch.device\":\nlogger.info(\"PyTorch: setting up devices\")\n-        if torch.distributed.is_initialized() and self.local_rank == -1:\n+        if torch.distributed.is_available() and torch.distributed.is_initialized() and self.local_rank == -1:\nlogger.warning(\n\"torch.distributed process group is initialized, but local_rank == -1. \"\n\"In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\"\n", "fix_pattern": "<condition>: \nIn a code section that deals with setting up devices for PyTorch.\n\n<pattern>: \nThe pattern detected is checking if the distributed processing group is initialized and the local rank is -1.\n\n<code_one>: \nThe code being removed is \"if torch.distributed.is_initialized() and self.local_rank == -1\".\n\n<code_two>: \nThe code being added is \"if torch.distributed.is_available() and torch.distributed.is_initialized() and self.local_rank == -1\".\n\nFix_pattern: \nIn the condition of setting up devices for PyTorch, if the distributed processing group is initialized, then check if it is available and continue with the code execution."}
{"number": 131, "change": "with tf.Graph().as_default():\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"model\")\nif not os.path.exists(checkpoint_dir):\nos.makedirs(checkpoint_dir)\n-        saver = tf.train.Saver(tf.all_variables())\n+        saver = tf.train.Saver(tf.global_variables())\n\n# Write vocabulary\nvocab_processor.save(os.path.join(out_dir, \"vocab\"))\n\n# Initialize all variables\n-        sess.run(tf.initialize_all_variables())\n+        sess.run(tf.global_variables_initializer())\n\ndef train_step(x_batch, y_batch):\n\"\"\"\n", "fix_pattern": "<condition>: When using TensorFlow for saving and initializing variables in a model.\n<pattern>: Using deprecated functions/arguments.\n<code_one>: tf.train.Saver(tf.all_variables()) and sess.run(tf.initialize_all_variables())\n<code_two>: tf.train.Saver(tf.global_variables()) and sess.run(tf.global_variables_initializer())\nFix_pattern: In the condition of using TensorFlow for saving and initializing variables in a model, if deprecated functions/arguments are detected, then remove tf.all_variables() from tf.train.Saver() and change tf.initialize_all_variables() to tf.global_variables_initializer() to fix the API misuse."}
{"number": 132, "change": "class CLIPTextTransformer(nn.Module):\nattentions=encoder_outputs.attentions,\n)\n\n-    def _build_causal_attention_mask(self, bsz, seq_len):\n+    def _build_causal_attention_mask(self, bsz, seq_len, dtype):\n# lazily create causal attention mask, with full attention between the vision tokens\n# pytorch uses additive attention mask; fill with -inf\n-        mask = torch.empty(bsz, seq_len, seq_len)\n-        mask.fill_(torch.tensor(float(\"-inf\")))\n+        mask = torch.empty(bsz, seq_len, seq_len, dtype=dtype)\n+        mask.fill_(torch.tensor(torch.finfo(dtype).min))\nmask.triu_(1)  # zero out the lower diagonal\nmask = mask.unsqueeze(1)  # expand mask\nreturn mask\n", "fix_pattern": "<condition>: The code is using a function with missing or incorrect arguments.\n\n<pattern>: The code is missing the \"dtype\" argument in the function \"_build_causal_attention_mask\".\n\n<code_one>: def _build_causal_attention_mask(self, bsz, seq_len):\n\n<code_two>: def _build_causal_attention_mask(self, bsz, seq_len, dtype):\n\nFix_pattern: \nIn the condition of missing/incorrect argument, if the missing argument \"dtype\" is detected, then add the \"dtype\" argument to the function \"_build_causal_attention_mask\" to fix the API misuse."}
{"number": 135, "change": "def get_mini_batch(mini_batch_indices, sequences, seq_lengths, cuda=False):\n# get mask for mini-batch\nmini_batch_mask = get_mini_batch_mask(mini_batch, sorted_seq_lengths)\n\n-    # wrap in PyTorch Variables\n-    mini_batch = Variable(torch.Tensor(mini_batch))\n-    mini_batch_reversed = Variable(torch.Tensor(mini_batch_reversed))\n-    mini_batch_mask = Variable(torch.Tensor(mini_batch_mask))\n+    # wrap in PyTorch Tensors\n+    mini_batch = torch.tensor(mini_batch)\n+    mini_batch_reversed = torch.tensor(mini_batch_reversed)\n+    mini_batch_mask = torch.tensor(mini_batch_mask)\n\n# cuda() here because need to cuda() before packing\nif cuda:\n", "fix_pattern": "<condition>: The condition is when the variable \"cuda\" is True.\n\n<pattern>: The pattern detected is the conversion of PyTorch Variables to PyTorch Tensors.\n\n<code_one>: The code that is removed is the wrapping of the variables \"mini_batch\", \"mini_batch_reversed\", and \"mini_batch_mask\" in PyTorch Variables.\n\n<code_two>: The code that is added is the wrapping of the variables \"mini_batch\", \"mini_batch_reversed\", and \"mini_batch_mask\" in PyTorch Tensors.\n\nFix_pattern: In the condition of \"cuda\" is True, remove the code of wrapping the variables in PyTorch Variables and add the code of wrapping the variables in PyTorch Tensors to fix the API misuse."}
{"number": 137, "change": "class FlopsProfiler(object):\nstart_time_hook)\n\ndef end_time_hook(module, input, output):\n-                torch.cuda.synchronize()\n+                get_accelerator().synchronize()\nmodule.__duration__ += time.time() - module.__start_time__\n\nif not hasattr(module, \"__end_time_hook_handle__\"):\n", "fix_pattern": "<condition>: If the module does not have the attribute \"__end_time_hook_handle__\"\n<pattern>: Removing the line \"torch.cuda.synchronize()\"\n<code_one>: torch.cuda.synchronize()\n<code_two>: get_accelerator().synchronize()\nFix_pattern: In the condition of the module not having the attribute \"__end_time_hook_handle__\", remove the line \"torch.cuda.synchronize()\" and replace it with \"get_accelerator().synchronize()\" to fix the API misuse."}
{"number": 151, "change": "def main(args):\n# CNN will transform a high dimension image into a low dimension 2D tensors for RBF kernel.\n# This kernel accepts inputs are inputs of CNN and gives outputs are covariance matrix of RBF on\n# outputs of CNN.\n-    kernel = gp.kernels.RBF(input_dim=10, lengthscale=torch.ones(10)).warp(iwarping_fn=cnn_fn)\n+    kernel = gp.kernels.Warp(gp.kernels.RBF(input_dim=10, lengthscale=torch.ones(10)),\n+                             iwarping_fn=cnn_fn)\n\n# init inducing points (taken randomly from dataset)\nXu = next(iter(train_loader))[0][:args.num_inducing]\n", "fix_pattern": "<condition>: The code involves the use of a Gaussian Process (GP) for which a kernel is being defined.\n<pattern>: The RBF kernel is being used with a certain configuration.\n<code_one>: The RBF kernel is being used with the `warp` function and a certain argument `iwarping_fn`.\n<code_two>: The RBF kernel is being wrapped with the `Warp` function, which is then passed the `iwarping_fn`.\n\nFix_pattern:\nIn the condition of using a GP kernel, if the RBF kernel is being used with the `warp` function and the `iwarping_fn` argument, then remove the direct usage of the `warp` function and instead wrap the RBF kernel with the `Warp` function, passing the `iwarping_fn` as an argument."}
{"number": 153, "change": "def linspace_helper(start, stop, num, axis=None, *, device):\nelse:\nres = [linspace_method(start, stp, num, device=device) for stp in stop]\nelse:\n-        return linspace_method(start, stop, num, device=device)\n+        return linspace_method(start, stop, num, dtype=torch.float64, device=device)\nres = torch.cat(res, -1).reshape(sos_shape + [num])\nif axis is not None:\nres = torch.transpose(res, axis, -1)\n", "fix_pattern": "<condition>: The condition is \"if axis is not None\"\n\n<pattern>: The pattern to be detected is the API misuse of not passing the \"dtype\" argument when calling the \"linspace_method\" function.\n\n<code_one>: The code to be removed is \"return linspace_method(start, stop, num, device=device)\"\n\n<code_two>: The code to be added is \"return linspace_method(start, stop, num, dtype=torch.float64, device=device)\"\n\nFix_pattern: In the condition of \"if axis is not None\", if the API misuse of not passing the \"dtype\" argument when calling the \"linspace_method\" function is detected, then remove the code \"return linspace_method(start, stop, num, device=device)\" and add the code \"return linspace_method(start, stop, num, dtype=torch.float64, device=device)\" to fix the API misuse."}
{"number": 156, "change": "class UnittestBase(object):\ndatetime.now().strftime('%H:%M:%S'), self.__class__.__name__[4:], name\n))\nsys.stdout.flush()\n+        tf.compat.v1.reset_default_graph()\n\ndef finished_test(self, assertion=None):\n\"\"\"\n", "fix_pattern": "<condition>: The code is using an outdated method or API.\n<pattern>: The code is using the method or API in a way that is no longer supported or has been deprecated.\n<code_one>: The outdated method or API that needs to be replaced.\n<code_two>: The updated method or API that should be used instead.\nFix_pattern: In the condition of using an outdated method or API, if the deprecated usage is detected, then replace the outdated method or API with the updated method or API to fix the API misuse."}
{"number": 158, "change": "class GPTNeoXModel(GPTNeoXPreTrainedModel):\n# Since we are adding it to the raw scores before the softmax, this is\n# effectively the same as removing these entirely.\nattention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n-            attention_mask = (1.0 - attention_mask) * -10000.0\n+            attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min\n\n# Prepare head mask if needed\n# 1.0 in head_mask indicate we keep the head\n", "fix_pattern": "<condition>: Adding attention mask to raw scores before softmax  \n<pattern>: Incorrect calculation of attention mask  \n<code_one>: attention_mask = (1.0 - attention_mask) * -10000.0  \n<code_two>: attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min  \nFix_pattern: In the condition of adding attention mask to raw scores before softmax, if an incorrect calculation of attention mask is detected, then change the code of attention_mask calculation from \"(1.0 - attention_mask) * -10000.0\" to \"(1.0 - attention_mask) * torch.finfo(self.dtype).min\" to fix the API misuse."}
{"number": 160, "change": "def compute_tf_latency(\nwith tf.device(device):\nfor _ in range(steps):\nstarting_time = time.time()\n-            _ = model(x)\n+            _ = model(*xs)\nlatencies.append(time.time() - starting_time)\nlatency = sum(latencies) / steps\nreturn latency, latencies\n", "fix_pattern": "<condition>: The code is using an API in an incorrect way.\n<pattern>: The code is calling a model with a single input parameter.\n<code_one>: '_ = model(x)'\n<code_two>: '_ = model(*xs)'\nFix_pattern: In the condition of API misuse, if a call to a model with a single input parameter is detected, then change the code from '_ = model(x)' to '_ = model(*xs)' to fix the API misuse."}
{"number": 163, "change": "def non_max_suppression(prediction, conf_thres=0.1, iou_thres=0.6, classes=None,\nmerge = False  # use merge-NMS\n\nt = time.time()\n-    output = [torch.zeros(0, 6)] * prediction.shape[0]\n+    output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\nfor xi, x in enumerate(prediction):  # image index, image inference\n# Apply constraints\n# x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n", "fix_pattern": "<condition>: N/A (no pre condition is needed)\n<pattern>: The pattern is to change the initialization of the \"output\" variable.\n<code_one>: [torch.zeros(0, 6)] * prediction.shape[0]\n<code_two>: [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\nFix_pattern: In the condition of \"N/A\", if the pattern of initializing \"output\" as [torch.zeros(0, 6)] * prediction.shape[0] is detected in the code, then change it to [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0] to fix the API misuse."}
{"number": 166, "change": "class DartsTrainer(BaseOneShotTrainer):\np += e * d\n\n_, loss = self._logits_and_loss(trn_X, trn_y)\n-            dalphas.append(torch.autograd.grad(loss, [c.alpha for c in self.nas_modules]))\n+            dalphas.append(torch.autograd.grad(loss, [c.alpha for _, c in self.nas_modules]))\n\ndalpha_pos, dalpha_neg = dalphas  # dalpha { L_trn(w+) }, # dalpha { L_trn(w-) }\nhessian = [(p - n) / (2. * eps) for p, n in zip(dalpha_pos, dalpha_neg)]\n", "fix_pattern": "<condition>:\nThe condition here is that `dalphas` is a list.\n\n<pattern>:\nThe pattern here is that the `torch.autograd.grad(loss, [c.alpha for c in self.nas_modules])` function call is inside a loop and the resulting gradients are being appended to the `dalphas` list.\n\n<code_one>:\nThe code that is being removed is `dalphas.append(torch.autograd.grad(loss, [c.alpha for c in self.nas_modules]))`.\n\n<code_two>:\nThe code that is being added is `dalphas.append(torch.autograd.grad(loss, [c.alpha for _, c in self.nas_modules]))`. The only change here is that the loop variable `_` is added to the list comprehension.\n\nFix_pattern:\nIn the condition that `dalphas` is a list, if the pattern of appending gradients to `dalphas` inside a loop is detected, then change the line `dalphas.append(torch.autograd.grad(loss, [c.alpha for c in self.nas_modules]))` to `dalphas.append(torch.autograd.grad(loss, [c.alpha for _, c in self.nas_modules]))` by adding the loop variable `_` to the list comprehension."}
{"number": 167, "change": "def subtract(\nout: Optional[Union[tf.Tensor, tf.Variable]] = None,\n) -> Union[tf.Tensor, tf.Variable]:\nx1, x2 = ivy.promote_types_of_inputs(x1, x2)\n-    return tf.subtract(x1, x2)\n+    return tf.experimental.numpy.subtract(x1, x2)\n\n\ndef tan(\n", "fix_pattern": "<condition>: When using the TensorFlow subtract function\n<pattern>: When the tf.subtract() function is used\n<code_one>: return tf.subtract(x1, x2)\n<code_two>: return tf.experimental.numpy.subtract(x1, x2)\nFix_pattern: In the condition of using the tf.subtract() function, if the tf.subtract() pattern is detected, then change the code return tf.subtract(x1, x2) to return tf.experimental.numpy.subtract(x1, x2) to fix the API misuse."}
{"number": 169, "change": "class XDropout(torch.autograd.Function):\n# Once https://github.com/pytorch/pytorch/issues/78391 is fixed, do something like:\n# if opset_version < 12:\n#   return torch.onnx.symbolic_opset9.dropout(g, input, dropout_p, train)\n-        return torch.onnx.symbolic_opset12.dropout(g, input, dropout_p, train)\n+        return symbolic_opset12.dropout(g, input, dropout_p, train)\n\n\n# Copied from transformers.models.deberta.modeling_deberta.StableDropout\n", "fix_pattern": "<condition>: The condition is when the PyTorch opset version is not yet at version 12.\n<pattern>: The pattern is the incorrect usage of the `torch.onnx.symbolic_opset12.dropout` function.\n<code_one>: The code that needs to be removed is `torch.onnx.symbolic_opset12.dropout(g, input, dropout_p, train)`.\n<code_two>: The code that needs to be added is `symbolic_opset12.dropout(g, input, dropout_p, train)`.\nFix_pattern: In the condition of the PyTorch opset version being less than 12, if the incorrect usage of `torch.onnx.symbolic_opset12.dropout` is detected, then remove the code and replace it with `symbolic_opset12.dropout` to fix the API misuse."}
{"number": 170, "change": "class Parquet(datasets.ArrowBasedBuilder):\nBUILDER_CONFIG_CLASS = ParquetConfig\n\ndef _info(self):\n-        if version.parse(pa.__version__) < version.parse(\"3.0.0\"):\n+        if datasets.config.PYARROW_VERSION.major < 3:\nraise ImportError(\n\"PyArrow >= 3.0.0 is required to used the Parquet dataset builder: pip install --upgrade pyarrow\"\n)\n", "fix_pattern": "<condition>: The version of PyArrow is less than 3.0.0.\n<pattern>: API misuse.\n<code_one>: `version.parse(pa.__version__) < version.parse(\"3.0.0\")`\n<code_two>: `datasets.config.PYARROW_VERSION.major < 3`\nFix_pattern: In the condition of the PyArrow version being less than 3.0.0, if the API misuse pattern `version.parse(pa.__version__) < version.parse(\"3.0.0\")` is detected, then change the code from `version.parse(pa.__version__) < version.parse(\"3.0.0\")` to `datasets.config.PYARROW_VERSION.major < 3` to fix the API misuse."}
{"number": 171, "change": "class RPCPlugin(DDPPlugin):\nworld_size: int) -> None:\nos.environ['MASTER_PORT'] = os.getenv('RPC_MASTER_PORT', '15000')\nrpc.init_rpc(f\"worker{global_rank}\", rank=global_rank, world_size=world_size)\n+        rpc._set_rpc_timeout(self.rpc_timeout_sec)\nself.rpc_initialized = True\n\ndef rpc_save_model(self,\n", "fix_pattern": "<condition>: The code is written in Python and uses the `rpc` library.\n<pattern>: No clear pattern is identified.\n<code_one>: No code is removed.\n<code_two>: The line `rpc._set_rpc_timeout(self.rpc_timeout_sec)` is added.\nFix_pattern: In the condition of using the `rpc` library, the line `rpc._set_rpc_timeout(self.rpc_timeout_sec)` is added to fix the API misuse."}
{"number": 172, "change": "class SimpleSeq2SeqTest(ModelTestCase):\nstate = self.model._init_decoder_state(state)\nbatch_size = state[\"source_mask\"].size()[0]\nstart_predictions = state[\"source_mask\"].new_full(\n-            (batch_size,), fill_value=self.model._start_index\n+            (batch_size,), fill_value=self.model._start_index, dtype=torch.long\n)\nall_top_k_predictions, _ = beam_search.search(\nstart_predictions, state, self.model.take_step\n", "fix_pattern": "<condition>: API misuse in initializing a tensor with a fill value.\n<pattern>: Initializing a tensor with a fill value using the `new_full` method without specifying the data type.\n<code_one>: `(batch_size,), fill_value=self.model._start_index`\n<code_two>: `(batch_size,), fill_value=self.model._start_index, dtype=torch.long`\nFix_pattern: In the condition of API misuse in initializing a tensor with a fill value, if using the `new_full` method without specifying the data type is detected, then change the code from `(batch_size,), fill_value=self.model._start_index` to `(batch_size,), fill_value=self.model._start_index, dtype=torch.long` to fix the API misuse."}
{"number": 174, "change": "class StableDiffusionDepth2ImgPipeline(DiffusionPipeline):\n`pipeline.enable_sequential_cpu_offload()` the execution device can only be inferred from Accelerate's module\nhooks.\n\"\"\"\n-        if self.device != torch.device(\"meta\") or not hasattr(self.unet, \"_hf_hook\"):\n+        if not hasattr(self.unet, \"_hf_hook\"):\nreturn self.device\nfor module in self.unet.modules():\nif (\n", "fix_pattern": "<condition>: The execution device can only be inferred from Accelerate's module hooks.\n<pattern>: If the device is not \"meta\" or the unet does not have the attribute \"_hf_hook\".\n<code_one>: if self.device != torch.device(\"meta\") or not hasattr(self.unet, \"_hf_hook\"):\n<code_two>: if not hasattr(self.unet, \"_hf_hook\"):\nFix_pattern: In the condition of the execution device can only be inferred from Accelerate's module hooks, if the device is not \"meta\" or the unet does not have the attribute \"_hf_hook\", remove the code block containing the condition and add the updated condition to fix the API misuse."}
{"number": 177, "change": "class IvyModule(ivy.Module):\nif ivy.array_mode():\na, kw = ivy.args_to_native(*a, **kw)\n# noinspection PyUnresolvedReferences\n-        params_hk = self._native_module.init(ivy.functional.core.random.RNG, *a, **kw)\n+        params_hk = self._native_module.init(ivy.random.RNG, *a, **kw)\nparams_dict = _hk_flat_map_to_dict(params_hk)\nself._hk_params = ivy.Container(params_dict)\nparam_iterator = self._hk_params.to_iterator()\n", "fix_pattern": "<condition>: There is a check for the array mode being enabled. \n<pattern>: The code was using the wrong import for the RNG module. \n<code_one>: self._native_module.init(ivy.functional.core.random.RNG, *a, **kw)\n<code_two>: self._native_module.init(ivy.random.RNG, *a, **kw)\nFix_pattern: In the condition of array mode being enabled, if the wrong import for the RNG module is detected, then change the code from \"ivy.functional.core.random.RNG\" to \"ivy.random.RNG\" to fix the API misuse."}
{"number": 179, "change": "class DataParallel(torch.nn.DataParallel):\nBatch.from_data_list(data_list[split[i]:split[i + 1]],\nfollow_batch=self.follow_batch,\nexclude_keys=self.exclude_keys).to(\n-                                     torch.device('cuda:{}'.format(\n-                                         device_ids[i])))\n+                                     torch.device(f'cuda:{device_ids[i]}'))\nfor i in range(len(split) - 1)\n]\n", "fix_pattern": "<condition>: The condition where an incorrect API usage is detected.\n<pattern>: The specific pattern or mistake that is identified.\n<code_one>: The code that needs to be removed or changed.\n<code_two>: The corrected code that should replace <code_one>.\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then remove/change <code_one> to <code_two> to fix the API misuse."}
{"number": 180, "change": "class ProjectedAdaptiveLogSoftmax(nn.Module):\nd_emb_i = d_embed // (div_val ** i)\n\nself.out_projs.append(\n-                    nn.Parameter(torch.Tensor(d_proj, d_emb_i))\n+                    nn.Parameter(torch.FloatTensor(d_proj, d_emb_i))\n)\n\nself.out_layers.append(nn.Linear(d_emb_i, r_idx-l_idx))\n", "fix_pattern": "<condition>: The code is using nn.Parameter to initialize a parameter tensor.\n<pattern>: The code is using torch.Tensor for initializing a parameter tensor.\n<code_one>: nn.Parameter(torch.Tensor(d_proj, d_emb_i))\n<code_two>: nn.Parameter(torch.FloatTensor(d_proj, d_emb_i))\nFix_pattern: In the condition of using nn.Parameter to initialize a parameter tensor, if the code is using torch.Tensor for initialization, then change it to nn.Parameter(torch.FloatTensor(d_proj, d_emb_i)) to fix the API misuse."}
{"number": 181, "change": "class TensorFlowEstimator(BaseEstimator):\nraise NotFittedError()\npredict_data_feeder = setup_predict_data_feeder(X)\npreds = []\n-        dropouts = tf.get_collection(DROPOUTS)\n-        feed_dict = {prob: 0.0 for prob in dropouts}\n+        dropouts = self._graph.get_collection(DROPOUTS)\n+        feed_dict = {prob: 1.0 for prob in dropouts}\nfor data in predict_data_feeder:\nfeed_dict[self._inp] = data\npreds.append(self._session.run(\n", "fix_pattern": "<condition>: The condition is when using TensorFlowEstimator in a software project.\n<pattern>: The pattern is detecting the use of tf.get_collection(DROPOUTS) to fetch dropouts and creating a feed_dict with {prob: 0.0 for prob in dropouts}.\n<code_one>: The code being removed is: dropouts = tf.get_collection(DROPOUTS) and feed_dict = {prob: 0.0 for prob in dropouts}.\n<code_two>: The code being added is: dropouts = self._graph.get_collection(DROPOUTS) and feed_dict = {prob: 1.0 for prob in dropouts}.\nFix_pattern: In the condition of using TensorFlowEstimator, if the pattern of fetching dropouts with tf.get_collection(DROPOUTS) and creating a feed_dict with {prob: 0.0 for prob in dropouts} is detected, then remove the code_one and change it to code_two to fix the API misuse."}
{"number": 187, "change": "class Pix2PixModel(BaseModel):\ndef backward_D(self):\n# Fake\n# stop backprop to the generator by detaching fake_B\n-        fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1))\n+        fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1).data)\npred_fake = self.netD.forward(fake_AB.detach())\nself.loss_D_fake = self.criterionGAN(pred_fake, False)\n\n# Real\nreal_AB = torch.cat((self.real_A, self.real_B), 1)\npred_real = self.netD.forward(real_AB)\n-        self.loss_D_real = self.criterionGAN(self.pred_real, True)\n+        self.loss_D_real = self.criterionGAN(pred_real, True)\n\n# Combined loss\nself.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n", "fix_pattern": "<condition>: In the backward_D method of the Pix2PixModel class.\n<pattern>: The variable pred_real is used before it is defined.\n<code_one>: self.loss_D_real = self.criterionGAN(self.pred_real, True)\n<code_two>: self.loss_D_real = self.criterionGAN(pred_real, True)\nFix_pattern: In the condition of backward_D, if the pattern of using the pred_real variable before it is defined is detected, then change the code_one to code_two to fix the API misuse."}
{"number": 189, "change": "class Trainer:\ntransformer_cls_to_wrap = get_module_class_from_name(\nmodel, self.args.fsdp_transformer_layer_cls_to_wrap\n)\n+                    if transformer_cls_to_wrap is None:\n+                        raise Exception(\"Could not find the transformer layer class to wrap in the model.\")\nauto_wrap_policy = functools.partial(\ntransformer_auto_wrap_policy,\n# Transformer layer class to wrap\n", "fix_pattern": "<condition>: In the condition of the variable \"transformer_cls_to_wrap\" being None.\n<pattern>: The pattern is to check if \"transformer_cls_to_wrap\" is None and raise an exception if it is.\n<code_one>: No code is removed.\n<code_two>: The code added is \"raise Exception(\"Could not find the transformer layer class to wrap in the model.\")\".\nFix Pattern: In the condition of \"transformer_cls_to_wrap\" being None, add code to raise an exception stating the inability to find the transformer layer class to wrap in the model."}
{"number": 193, "change": "class tensorflow_extractor(base_extractor):\nwriter.close()\nsess.run(init)\nsaver = tf.train.Saver()\n+            tf.train.export_meta_graph(\"kit.meta\", as_text=True)\nsaver.restore(sess, path + cls.architecture_map[architecture]['filename'])\nsave_path = saver.save(sess, path + \"imagenet_{}.ckpt\".format(architecture))\nprint(\"Model saved in file: %s\" % save_path)\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: N/A\n<code_one>: N/A\n<code_two>: N/A\nFix_pattern: No fix pattern is needed."}
{"number": 194, "change": "def test_auto_diagonal_gaussians(auto_class, Elbo):\nguide = auto_class(model, rank=1)\nelse:\nguide = auto_class(model)\n-    adam = optim.Adam({\"lr\": .001, \"betas\": (0.95, 0.999)})\n+    adam = optim.ClippedAdam({\"lr\": .01, \"betas\": (0.95, 0.999),\n+                              \"lrd\": 0.1 ** (1 / n_steps)})\nsvi = SVI(model, guide, adam, loss=Elbo())\n\nfor k in range(n_steps):\n", "fix_pattern": "<condition>: n_steps is greater than 1\n<pattern>: optim.Adam is being used as the optimizer\n<code_one>: adam = optim.Adam({\"lr\": .001, \"betas\": (0.95, 0.999)})\n<code_two>: adam = optim.ClippedAdam({\"lr\": .01, \"betas\": (0.95, 0.999), \"lrd\": 0.1 ** (1 / n_steps)})\nFix_pattern: In the condition of n_steps being greater than 1, if the optimizer used is optim.Adam, then replace the code `{adam = optim.Adam({\"lr\": .001, \"betas\": (0.95, 0.999)})}` with `{adam = optim.ClippedAdam({\"lr\": .01, \"betas\": (0.95, 0.999), \"lrd\": 0.1 ** (1 / n_steps)})}` to fix the API misuse."}
{"number": 195, "change": "def test_frontend_backward_multi_channel(train, use_wpe, use_beamformer):\nfrontend.train()\nelse:\nfrontend.eval()\n+    torch.random.manual_seed(14)\nx = torch.randn(2, 1000, 2, requires_grad=True)\nx_lengths = torch.LongTensor([1000, 980])\ny, y_lengths = frontend(x, x_lengths)\n", "fix_pattern": "<condition>: The condition is not specified in the given context.\n\n<pattern>: No clear pattern is detected in the given context.\n\n<code_one>: No code is removed in the given context.\n\n<code_two>: The code added is \"torch.random.manual_seed(14)\".\n\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then add the code \"torch.random.manual_seed(14)\" to fix the API misuse."}
{"number": 199, "change": "class EulerSamplingTest(tf.test.TestCase, parameterized.TestCase):\ntimes=times,\nnum_samples=num_samples,\ninitial_state=x0,\n-            random_type=tff.math.random.RandomType.SOBOL,\n+            random_type=tff.math.random.RandomType.HALTON,\ntime_step=0.01,\n-            seed=12134))\n+            seed=12134,\n+            skip=100,\n+            dtype=tf.float32))\n\n-    self.assertAllClose(paths.shape, (num_samples, 5, 2), atol=0)\n+    self.assertAllClose(paths.shape, (num_samples, 3, 2), atol=0)\nmeans = np.mean(paths, axis=0)\ntimes = np.reshape(times, [-1, 1])\nexpected_means = x0 + (2.0 / 3.0) * mu * np.power(times, 1.5)\n", "fix_pattern": "<condition>: The code is using the wrong random type for sampling.\n\n<pattern>: The pattern detected is that the random_type needs to be changed from SOBOL to HALTON.\n\n<code_one>: random_type=tff.math.random.RandomType.SOBOL\n\n<code_two>: random_type=tff.math.random.RandomType.HALTON\n\nFix_pattern: In the condition of using random sampling, if the pattern of using the wrong random_type (SOBOL) is detected, then change the random_type to HALTON to fix the API misuse."}
{"number": 203, "change": "class BatchNorm(TransformModule):\nif self.training:\nmean, var = y.mean(0), y.var(0)\n\n-            # NOTE: The momentum variable agrees with the definition in e.g. `torch.nn.BatchNorm1d`\n-            self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum)\n-            self.moving_variance.mul_(1 - self.momentum).add_(var * self.momentum)\n+            with torch.no_grad():\n+                # NOTE: The momentum variable agrees with the definition in e.g. `torch.nn.BatchNorm1d`\n+                self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum)\n+                self.moving_variance.mul_(1 - self.momentum).add_(var * self.momentum)\n\n# During test time, use smoothed averages rather than the sample ones\nelse:\n", "fix_pattern": "<condition>: The code is checking whether the model is in training mode or not.\n<pattern>: The pattern is detecting the misuse of the API by not using torch.no_grad() during test time.\n<code_one>: The code that needs to be removed is self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum) and self.moving_variance.mul_(1 - self.momentum).add_(var * self.momentum).\n<code_two>: The code that needs to be added is with torch.no_grad(): followed by the same code that was removed.\nFix_pattern: In the condition of checking training mode, if the misuse pattern is detected, then remove the incorrect code and add the correct code with torch.no_grad() to fix the API misuse."}
{"number": 205, "change": "class RNNLM(nn.Module):\n\ndef forward(self, state, x):\nh0 = self.embed(x)\n-        h1, c1 = self.l1(F.dropout(h0), (state['h1'], state['c1']))\n-        h2, c2 = self.l2(F.dropout(h1), (state['h2'], state['c2']))\n-        y = self.lo(F.dropout(h2))\n+        h1, c1 = self.l1(self.d0(h0), (state['h1'], state['c1']))\n+        h2, c2 = self.l2(self.d1(h1), (state['h2'], state['c2']))\n+        y = self.lo(self.d2(h2))\nstate = {'c1': c1, 'h1': h1, 'c2': c2, 'h2': h2}\nreturn state, y\n", "fix_pattern": "<condition>: There is an API misuse in the forward method of the RNNLM module.\n<pattern>: The API `F.dropout` is being called on the input `h0` before passing it to the LSTM layers, but it should be called after applying the dropout on `h0`.\n<code_one>: `F.dropout(h0)`\n<code_two>: `self.d0(h0)`\nFix_pattern: In the condition of the input being passed to the LSTM layers, if the `F.dropout(h0)` pattern is detected, then change the `F.dropout(h0)` to `self.d0(h0)` to fix the API misuse."}
{"number": 208, "change": "def reportScore(name, scoreTotal, wordsTotal):\ndef main():\nopt = parser.parse_args()\nopt.cuda = opt.gpu > -1\n-    torch.cuda.set_device(opt.gpu)\n+    if opt.cuda:\n+        torch.cuda.set_device(opt.gpu)\n\ntranslator = onmt.Translator(opt)\n", "fix_pattern": "<condition>: If the `opt.cuda` value is greater than -1 (indicating GPU usage).\n<pattern>: Detect whether `torch.cuda.set_device(opt.gpu)` is present in the code.\n<code_one>: `torch.cuda.set_device(opt.gpu)`\n<code_two>: Code is added to check if `opt.cuda` is True and then set the device.\nFix_pattern: In the condition of `opt.cuda` being True, change the `torch.cuda.set_device(opt.gpu)` to a conditional statement that checks if `opt.cuda` is True before setting the device."}
{"number": 209, "change": "class Conv2dStaticSamePadding(nn.Conv2d):\npad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\npad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\nif pad_h > 0 or pad_w > 0:\n-            self.static_padding = nn.ZeroPad2d((pad_w - pad_w // 2, pad_w - pad_w // 2,\n-                                                pad_h - pad_h // 2, pad_h - pad_h // 2))\n+            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2,\n+                                                pad_h // 2, pad_h - pad_h // 2))\nelse:\nself.static_padding = nn.Identity()\n", "fix_pattern": "<condition>: If the condition `pad_h > 0 or pad_w > 0` is met.\n<pattern>: The code was using an incorrect padding calculation.\n<code_one>: `self.static_padding = nn.ZeroPad2d((pad_w - pad_w // 2, pad_w - pad_w // 2, pad_h - pad_h // 2, pad_h - pad_h // 2))`\n<code_two>: `self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))`\nFix_pattern: In the condition of `pad_h > 0 or pad_w > 0`, if the padding calculation is incorrect, then change the padding calculation from `code_one` to `code_two` to fix the API misuse."}
{"number": 213, "change": "class SpeedySpeech(BaseTTS):\noutputs = {\"model_outputs\": o_de.transpose(1, 2), \"durations_log\": o_dr_log.squeeze(1), \"alignments\": attn}\nreturn outputs\n\n+    @torch.no_grad()\ndef inference(self, x, aux_input={\"d_vectors\": None, \"speaker_ids\": None}):  # pylint: disable=unused-argument\n\"\"\"\nShapes:\n", "fix_pattern": "<condition>: The condition is when there is a need to fix an API misuse.\n<pattern>: The pattern is the detection of a certain code or code snippet that is being misused.\n<code_one>: The code that is causing the API misuse.\n<code_two>: The corrected code that should replace the misused code.\nFix_pattern: In the condition of API misuse, if the pattern is detected, then remove/change the code_one to code_two to fix the API misuse."}
{"number": 214, "change": "class KerasBackend(AbstractBackend):\nreturn keras\n\ndef einsum(self, pattern, *x):\n-        return self.tf.einsum(pattern, *x)\n+        return self.tf.vectorized_map(\n+            functools.partial(self.tf.einsum, pattern),\n+            *x\n+        )\n\n\nclass OneFlowBackend(AbstractBackend):\n", "fix_pattern": "<condition>: Using the TensorFlow backend in Keras.\n<pattern>: Calling the `einsum` function.\n<code_one>: `self.tf.einsum(pattern, *x)`\n<code_two>: `self.tf.vectorized_map(functools.partial(self.tf.einsum, pattern), *x)`\nFix_pattern: In the condition of using the TensorFlow backend in Keras, if the `einsum` function is called, then change `self.tf.einsum(pattern, *x)` to `self.tf.vectorized_map(functools.partial(self.tf.einsum, pattern), *x)` to fix the API misuse."}
{"number": 215, "change": "def run_modelmerger(primary_model_name, secondary_model_name, teritary_model_nam\nt2 = theta_2.get(key, torch.zeros_like(theta_1[key]))\ntheta_1[key] = theta_func1(theta_1[key], t2)\nelse:\n-                    theta_1[key] = 0\n+                    theta_1[key] = torch.zeros_like(theta_1[key])\ndel theta_2, teritary_model\n\nfor key in tqdm.tqdm(theta_0.keys()):\n", "fix_pattern": "<condition>: The condition is not clear in the given context.\n\n<pattern>: The pattern is to replace the assignment statement of \"theta_1[key] = 0\" with \"theta_1[key] = torch.zeros_like(theta_1[key])\".\n\n<code_one>: The code \"theta_1[key] = 0\" is removed.\n\n<code_two>: The code \"theta_1[key] = torch.zeros_like(theta_1[key])\" is added.\n\nFix_pattern: In the condition of <no pre condition is needed>, if the pattern of assigning 0 to \"theta_1[key]\" is detected, then change the code \"theta_1[key] = 0\" to \"theta_1[key] = torch.zeros_like(theta_1[key])\" to fix the API misuse."}
{"number": 216, "change": "class DefaultClassifier(Classifier):\n\ndef _calculate_loss(self, scores, labels):\n\n-        if len(labels) == 0: return torch.tensor(0., requires_grad=True), 1\n+        if len(labels) == 0: return torch.tensor(0., requires_grad=True, device=flair.device), 1\n\nif self.multi_label:\nlabels = torch.tensor([[1 if l in all_labels_for_point else 0 for l in self.label_dictionary.get_items()]\n", "fix_pattern": "<condition>: The condition is if the length of the labels is 0.\n<pattern>: The pattern is that the code checks if the length of labels is 0 and returns a tensor and a value.\n<code_one>: The code being removed is \"return torch.tensor(0., requires_grad=True), 1\".\n<code_two>: The code being added is \"return torch.tensor(0., requires_grad=True, device=flair.device), 1\".\nFix_pattern: In the condition of checking if the length of the labels is 0, the code is modified to include the device parameter in the torch.tensor() function call."}
{"number": 217, "change": "class EmbeddingLayer(nn.Module):\ntorch.empty(weight_shape[0],\nweight_shape[1],\ndtype=dtype,\n-                        device=torch.cuda.current_device()))\n+                        device=get_accelerator().current_device_name()))\n\ndef forward(self, input):\nreturn F.embedding(input, self.weight)\n", "fix_pattern": "<condition>: The code is using torch.cuda.current_device() to get the current device.\n<pattern>: The code is replaced with get_accelerator().current_device_name() to get the current device.\n<code_one>: device=torch.cuda.current_device()\n<code_two>: device=get_accelerator().current_device_name()\nFix_pattern: In the condition of the code using torch.cuda.current_device(), if the pattern device=torch.cuda.current_device() is detected, then change device=torch.cuda.current_device() to device=get_accelerator().current_device_name() to fix the API misuse."}
{"number": 218, "change": "class MultiActionDistribution(ActionDistribution):\n\ndef logp(self, x):\n\"\"\"The log-likelihood of the action distribution.\"\"\"\n-        split_list = self.reshaper.split_tensor(x)\n+        split_list = tf.split(x, len(self.input_lens), axis=1)\nfor i, distribution in enumerate(self.child_distributions):\n# Remove extra categorical dimension\nif isinstance(distribution, Categorical):\n", "fix_pattern": "<condition>: The condition is that the distribution variable is an instance of the Categorical class.\n<pattern>: The pattern is that the reshaper's split_tensor() method is being used to split the tensor x.\n<code_one>: The code being removed is \"split_list = self.reshaper.split_tensor(x)\".\n<code_two>: The code being added is \"split_list = tf.split(x, len(self.input_lens), axis=1)\".\nFix_pattern: In the condition of checking if the distribution is an instance of Categorical, if the split_tensor() method is being used, then replace it with tf.split() method using the specified arguments to fix the API misuse."}
{"number": 219, "change": "class CategoricalOneHotPolicy(StochasticPolicy):\ndef __init__(self, network, session, state, random, action_count=1, scope='policy'):\nwith tf.variable_scope(scope):\naction_layer = linear(layer_input=network.output, config={'num_outputs': action_count}, scope='outputs')\n+            action_layer = tf.reshape(action_layer, [-1, action_count])\n+\ndistribution = tf.nn.softmax(action_layer)\nsample = tf.multinomial(distribution, 1)\n", "fix_pattern": "<condition>: The condition of the API misuse fix pattern is the presence of an action layer in a class or function. \n\n<pattern>: The pattern in the fix is the incorrect shape or dimensions of the action_layer.\n\n<code_one>: The code that needs to be removed in order to fix the API misuse is the code that does not properly reshape the action_layer.\n\n<code_two>: The code that needs to be added in order to fix the API misuse is the code that correctly reshapes the action_layer.\n\nFix_pattern: In the condition of having an action layer, if the incorrect shape or dimensions of the action_layer are detected, then the code that does not properly reshape the action_layer should be removed and the code that correctly reshapes the action_layer should be added to fix the API misuse."}
{"number": 222, "change": "def test(data,\nelse:  # called by train.py\ntraining = True\ndevice = next(model.parameters()).device  # get model device\n-        half = device.type != 'cpu'  # half precision only supported on CUDA\n+        half = device.type != 'cpu' and torch.cuda.device_count() == 1  # half precision only supported on single-GPU\nif half:\nmodel.half()  # to FP16\n", "fix_pattern": "<condition>: Function called by train.py\n<pattern>: Detection of device type\n<code_one>: `half = device.type != 'cpu'`\n<code_two>: `half = device.type != 'cpu' and torch.cuda.device_count() == 1`\nFix_pattern: In the condition of being called by train.py, if the device type is not 'cpu', then change `half = device.type != 'cpu'` to `half = device.type != 'cpu' and torch.cuda.device_count() == 1` to fix the API misuse."}
{"number": 223, "change": "class MobileNetV3LargeEncoder(MobileNetV3):\n)\n\nif pretrained:\n-            self.load_state_dict(load_state_dict_from_url(\n+            self.load_state_dict(torch.hub.load_state_dict_from_url(\n'https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth'))\n\ndel self.avgpool\n", "fix_pattern": "<condition>: In the MobileNetV3LargeEncoder class.\n<pattern>: The use of the load_state_dict_from_url() function.\n<code_one>: self.load_state_dict(load_state_dict_from_url(\n<code_two>: self.load_state_dict(torch.hub.load_state_dict_from_url(\nFix_pattern: In the condition of MobileNetV3LargeEncoder class, if the load_state_dict_from_url() function is detected, then change the code from self.load_state_dict(load_state_dict_from_url( to self.load_state_dict(torch.hub.load_state_dict_from_url( to fix the API misuse."}
{"number": 224, "change": "def make_non_pad_mask(lengths):\n\"\"\"\nbs = int(len(lengths))\nmaxlen = int(max(lengths))\n-    mask = torch.zeros(bs, maxlen).byte()\n+    mask = torch.zeros(bs, maxlen, dtype=torch.uint8)\nfor i, l in enumerate(lengths):\nmask[i, :l] = 1\n", "fix_pattern": "<condition>: The code is creating a mask with torch.zeros().\n\n<pattern>: The code is using torch.zeros() with a byte data type.\n\n<code_one>: mask = torch.zeros(bs, maxlen).byte()\n\n<code_two>: mask = torch.zeros(bs, maxlen, dtype=torch.uint8)\n\nFix Pattern:\n\nIn the condition of creating a mask with torch.zeros(), if the code is using torch.zeros() with a byte data type, then change the code from mask = torch.zeros(bs, maxlen).byte() to mask = torch.zeros(bs, maxlen, dtype=torch.uint8) to fix the API misuse."}
{"number": 229, "change": "class TFXGLMPreTrainedModel(TFPreTrainedModel):\n@tf.function(\ninput_signature=[\n{\n-                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),\n-                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),\n+                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),\n+                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),\n}\n]\n)\n", "fix_pattern": "<condition>: The condition is that the code is using the `@tf.function` decorator with the `input_signature` argument.\n\n<pattern>: The pattern is to update the `tf.TensorSpec` data types from `tf.int32` to `tf.int64`.\n\n<code_one>: The code that needs to be modified is:\n\n```\n\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\")\n\"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\")\n```\n\n<code_two>: The code that needs to be added is:\n\n```\n\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\")\n\"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\")\n```\n\nFix_pattern: In the condition of using `@tf.function` with `input_signature`, if the data type of `input_ids` and `attention_mask` is `tf.int32`, then update it to `tf.int64` to fix the API misuse."}
{"number": 233, "change": "class TacotronTrainTest(unittest.TestCase):\noptimizer = optim.Adam(model.parameters(), lr=c.lr)\nfor i in range(5):\nmel_out, linear_out, align, stop_tokens = model.forward(\n-                input, mel_spec)\n+                input, input_lengths, mel_spec)\nassert stop_tokens.data.max() <= 1.0\nassert stop_tokens.data.min() >= 0.0\noptimizer.zero_grad()\n", "fix_pattern": "<condition>: The condition is that the value of `stop_tokens` should not exceed 1.0 and should not be less than 0.0.\n<pattern>: The pattern is that the wrong input argument `input` is passed to the `model.forward()` function.\n<code_one>: The wrong code that is removed is `input, mel_spec`.\n<code_two>: The correct code that is added is `input, input_lengths, mel_spec`.\nFix_pattern: In the condition of the `stop_tokens` value check, if the wrong input argument `input` is detected, then change the `input` argument to `input, input_lengths` to fix the API misuse."}
{"number": 234, "change": "class EulerAncestralDiscreteScheduler(SchedulerMixin, ConfigMixin):\n\nprev_sample = sample + derivative * dt\n\n-        device = model_output.device if torch.is_tensor(model_output) else torch.device(\"cpu\")\n+        device = model_output.device\nif device.type == \"mps\":\n# randn does not work reproducibly on mps\nnoise = torch.randn(model_output.shape, dtype=model_output.dtype, device=\"cpu\", generator=generator).to(\n", "fix_pattern": "<condition>: Check if `device.type` is equal to \"mps\".\n<pattern>: The code is trying to create noise using `torch.randn()`, but it is not working reproducibly on the \"mps\" device.\n<code_one>: `device = model_output.device if torch.is_tensor(model_output) else torch.device(\"cpu\")`\n<code_two>: `device = model_output.device`\nFix_pattern: In the condition of `device.type == \"mps\"`, if the pattern of `torch.is_tensor(model_output)` is detected, then remove the code `device = model_output.device if torch.is_tensor(model_output) else torch.device(\"cpu\")` and replace it with `device = model_output.device` to fix the API misuse."}
{"number": 244, "change": "def test_runway_sd_1_5_negative_prompt(sd_device, strategy, sampler):\ndef test_cv2(strategy, cv2_flag, cv2_radius):\nmodel = ModelManager(\nname=\"cv2\",\n-        device=device,\n+        device=torch.device(device),\n)\ncfg = get_config(strategy, cv2_flag=cv2_flag, cv2_radius=cv2_radius)\nassert_equal(\n", "fix_pattern": "<condition>: When using the torch library for deep learning tasks.\n<pattern>: Using the argument \"device\" in the incorrect format.\n<code_one>: device=device,\n<code_two>: device=torch.device(device),\nFix_pattern: In the condition of using the torch library, if the pattern of incorrect \"device=device\" usage is detected, then change the code from \"device=device\" to \"device=torch.device(device)\" to fix the API misuse."}
{"number": 250, "change": "def train_cnn_and_compute_accuracy(params, steps, train_images, train_labels, va\n# Do the training and evaluation.\nwith tf.Session() as sess:\n# Initialize the network weights.\n-    sess.run(tf.initialize_all_variables())\n+    sess.run(tf.global_variables_initializer())\nfor i in range(1, steps + 1):\n# Fetch the next batch of data.\nimage_batch = get_batch(train_images, i, batch_size)\n", "fix_pattern": "<condition>: In the context of initializing network weights in TensorFlow.\n<pattern>: Usage of deprecated function `tf.initialize_all_variables()`.\n<code_one>: `sess.run(tf.initialize_all_variables())`.\n<code_two>: `sess.run(tf.global_variables_initializer())`.\nFix_pattern: In the condition of initializing network weights in TensorFlow, if usage of deprecated function `tf.initialize_all_variables()` is detected, then change `sess.run(tf.initialize_all_variables())` to `sess.run(tf.global_variables_initializer())` to fix the API misuse."}
{"number": 251, "change": "def actor_critic_loss(policy, model, dist_class, train_batch):\nvalues = model.value_function()\ndist = dist_class(logits, model)\nlog_probs = dist.logp(train_batch[SampleBatch.ACTIONS])\n-    policy.entropy = dist.entropy().mean()\n+    policy.entropy = dist.entropy().sum()\npolicy.pi_err = -train_batch[Postprocessing.ADVANTAGES].dot(\nlog_probs.reshape(-1))\n-    policy.value_err = nn.functional.mse_loss(\n-        values.reshape(-1), train_batch[Postprocessing.VALUE_TARGETS])\n+    policy.value_err = torch.sum(\n+        torch.pow(\n+            values.reshape(-1) - train_batch[Postprocessing.VALUE_TARGETS],\n+            2.0))\noverall_err = sum([\npolicy.pi_err,\npolicy.config[\"vf_loss_coeff\"] * policy.value_err,\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: If the `policy.entropy` and `policy.value_err` calculations are not correct.\n<code_one>: `policy.entropy = dist.entropy().mean()` and `policy.value_err = nn.functional.mse_loss(values.reshape(-1), train_batch[Postprocessing.VALUE_TARGETS])`\n<code_two>: `policy.entropy = dist.entropy().sum()` and `policy.value_err = torch.sum(torch.pow(values.reshape(-1) - train_batch[Postprocessing.VALUE_TARGETS], 2.0))`\nFix_pattern: In the condition of no clear condition, if the `policy.entropy` and `policy.value_err` calculations are not correct, then change the code from `policy.entropy = dist.entropy().mean()` to `policy.entropy = dist.entropy().sum()` and from `policy.value_err = nn.functional.mse_loss(values.reshape(-1), train_batch[Postprocessing.VALUE_TARGETS])` to `policy.value_err = torch.sum(torch.pow(values.reshape(-1) - train_batch[Postprocessing.VALUE_TARGETS], 2.0))` to fix the API misuse."}
{"number": 252, "change": "class DeepSpeedDataLoader(object):\nelse:\nif data_sampler is None:\ndata_sampler = RandomSampler(dataset)\n-                device_count = torch.cuda.device_count()\n+                device_count = get_accelerator().device_count()\nbatch_size *= device_count\n\nif num_local_io_workers is None:\n", "fix_pattern": "<condition>: num_local_io_workers is None\n<pattern>: device_count = torch.cuda.device_count()\n<code_one>: device_count = torch.cuda.device_count()\n<code_two>: device_count = get_accelerator().device_count()\nFix_pattern: In the condition where num_local_io_workers is None, if the pattern of device_count = torch.cuda.device_count() is detected, then change the code from device_count = torch.cuda.device_count() to device_count = get_accelerator().device_count() to fix the API misuse."}
{"number": 258, "change": "class StopwatchMeter(Meter):\nif self.start_time is not None:\ndelta = time.perf_counter() - self.start_time\nself.sum = self.sum + delta\n-            self.n = self.n + n\n+            self.n = type_as(self.n, n) + n\n\ndef reset(self):\nself.sum = 0  # cumulative time during which stopwatch was active\n", "fix_pattern": "<condition>: The condition is that the \"start_time\" attribute of the StopwatchMeter class is not None.\n<pattern>: The pattern is detecting the change made to the \"self.n\" attribute.\n<code_one>: The original code was \"self.n = self.n + n\".\n<code_two>: The fixed code is \"self.n = type_as(self.n, n) + n\".\nFix_pattern: In the condition of \"start_time\" not being None, if the \"self.n\" attribute is detected, then the code should be changed from \"self.n = self.n + n\" to \"self.n = type_as(self.n, n) + n\" to fix the API misuse."}
{"number": 259, "change": "class RagTokenForGeneration(RagPreTrainedModel):\nn_docs = n_docs if n_docs is not None else self.config.n_docs\n\n# RAG-token marginalization\n-        seq_logprobs = torch.nn.functional.log_softmax(seq_logits, dim=-1).view(\n+        seq_logprobs = nn.functional.log_softmax(seq_logits, dim=-1).view(\nseq_logits.shape[0] // n_docs, n_docs, -1, seq_logits.size(-1)\n)\ndoc_logprobs = torch.log_softmax(doc_scores, dim=1)\n", "fix_pattern": "<condition>: The code is trying to perform a softmax operation on a tensor.\n\n<pattern>: The use of torch.nn.functional.log_softmax()\n\n<code_one>: seq_logits = torch.nn.functional.log_softmax(seq_logits, dim=-1).view(\n\n<code_two>: seq_logits = nn.functional.log_softmax(seq_logits, dim=-1).view(\n\nFix_pattern: In the condition of performing softmax on a tensor, if torch.nn.functional.log_softmax() is detected, then remove \"torch.\" from \"torch.nn.functional.log_softmax()\" and replace it with \"nn.\" to fix the API misuse."}
{"number": 265, "change": "class HorovodTrainer(DataParallelTrainer):\n),\n)\ntrain_dataset = ray.data.from_items([{\"x\": x, \"y\": x + 1} for x in range(32)])\n-        scaling_config = ScalingConfig(num_workers=3)\n-        # If using GPUs, use the below scaling config instead.\n-        # scaling_config = ScalingConfig(num_workers=3, use_gpu=True)\n+        scaling_config = ScalingConfig(num_workers=3, use_gpu=use_gpu)\ntrainer = HorovodTrainer(\ntrain_loop_per_worker=train_loop_per_worker,\nscaling_config=scaling_config,\n", "fix_pattern": "<condition>: The condition is when using HorovodTrainer.\n<pattern>: The pattern is the use of scaling_config with a fixed number of workers and optionally using GPUs.\n<code_one>: The original code is setting scaling_config to ScalingConfig(num_workers=3), which assumes 3 workers and no GPU usage.\n<code_two>: The fixed code is setting scaling_config to ScalingConfig(num_workers=3, use_gpu=use_gpu), which allows the number of workers and GPU usage to be controlled by the use_gpu variable.\nFix_pattern: In the condition of using HorovodTrainer, if the fixed number of workers and GPU usage need to be controlled, then replace the code where scaling_config is set to ScalingConfig(num_workers=3) with ScalingConfig(num_workers=3, use_gpu=use_gpu)."}
{"number": 267, "change": "def qr(A: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:  # pragma: no cove\n\"\"\"\nLike torch.linalg.qr.\n\"\"\"\n-    if hasattr(torch.linalg, \"qr\"):\n+    if hasattr(torch, \"linalg\") and hasattr(torch.linalg, \"qr\"):\n# PyTorch version >= 1.9\nreturn torch.linalg.qr(A)\nreturn torch.qr(A)\n", "fix_pattern": "<condition>: The condition is to check if the torch.linalg.qr function is available.\n\n<pattern>: The pattern is to update the condition check to include the existence of the 'linalg' module in torch.\n\n<code_one>: The code that is removed is the check for the existence of the 'qr' function in the torch.linalg module.\n\n<code_two>: The code that is added is the check for the existence of both the 'linalg' module and the 'qr' function in the torch module.\n\nFix_pattern:\nIn the condition of checking if the torch.linalg.qr function is available, if the 'qr' function is detected, then remove the check for the existence of the 'qr' function in the torch.linalg module and add the check for the existence of both the 'linalg' module and the 'qr' function in the torch module to fix the API misuse."}
{"number": 268, "change": "def prepare_bart_inputs_dict(\nif decoder_attention_mask is None:\ndecoder_attention_mask = decoder_input_ids.ne(config.pad_token_id)\nif head_mask is None:\n-        head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads)\n+        head_mask = torch.ones(config.encoder_layers, config.encoder_attention_heads, device=torch_device)\nif decoder_head_mask is None:\n-        decoder_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads)\n+        decoder_head_mask = torch.ones(config.decoder_layers, config.decoder_attention_heads, device=torch_device)\nreturn {\n\"input_ids\": input_ids,\n\"decoder_input_ids\": decoder_input_ids,\n", "fix_pattern": "<condition>: If the variables \"head_mask\" and \"decoder_head_mask\" are None.\r\n<pattern>: The pattern is the initialization of \"head_mask\" and \"decoder_head_mask\" tensors with ones.\r\n<code_one>: The code for initializing \"head_mask\" and \"decoder_head_mask\" tensors with ones.\r\n<code_two>: The code for initializing \"head_mask\" and \"decoder_head_mask\" tensors with ones and specifying the device.\r\nFix_pattern: In the condition of \"head_mask\" and \"decoder_head_mask\" being None, the fix pattern is to initialize the tensors with ones and specify the device."}
{"number": 271, "change": "class Plan(Serializable):\n# prevent circular dependency\n# syft relative\nfrom ...core.node.vm.vm import VirtualMachine  # noqa: F401\n+        if self.local_executor is not None:\n+            # this is necessary for syfts nn.module, because the plan contains state from the module\n+            # in order to use this state, we first need to send the model, and then execute te plan\n+            return self.local_executor(**kwargs)\n\nalice = VirtualMachine(name=\"plan_executor\")\nalice_client: client.Client = alice.get_client()\n", "fix_pattern": "<condition>: The code is inside a class or function that has a variable named \"local_executor\"\n<pattern>: The code checks if the \"local_executor\" variable is not None before executing some code\n<code_one>: None\n<code_two>: \"return self.local_executor(**kwargs)\"\nFix_pattern: In the condition of having a variable named \"local_executor\", if the \"local_executor\" variable is not None, then return the result of executing the \"local_executor\" function with the given keyword arguments to fix the API misuse."}
{"number": 272, "change": "class GradTTS(DiffusionPipeline):\nmu_y = mu_y.transpose(1, 2)\n\n# Sample latent representation from terminal distribution N(mu_y, I)\n-        z = mu_y + torch.randn_like(mu_y, device=mu_y.device) / temperature\n+        z = mu_y + torch.randn(mu_y.shape, device=mu_y.device, generator=generator) / temperature\n\nxt = z * y_mask\nh = 1.0 / num_inference_steps\n", "fix_pattern": "<condition>: The code is using the `torch.randn_like()` function.\n\n<pattern>: The code is adding noise to a tensor using the `torch.randn_like()` function.\n\n<code_one>: `z = mu_y + torch.randn_like(mu_y, device=mu_y.device) / temperature`\n\n<code_two>: `z = mu_y + torch.randn(mu_y.shape, device=mu_y.device, generator=generator) / temperature`\n\nFix_pattern: In the condition of using the `torch.randn_like()` function, if adding noise to a tensor is detected, then change the `torch.randn_like()` function to the `torch.randn()` function with the specified shape and generator to fix the API misuse."}
{"number": 273, "change": "class NanDetector:\ngradients = {}\nfor name, param in self.named_parameters:\nif param.grad is not None:\n-                grad_norm = torch.norm(param.grad.data, p=2, dtype=torch.float32)\n+                grad_norm = torch.norm(param.grad.data.float(), p=2)\nnorm[name] = grad_norm.item()\nif torch.isnan(grad_norm).any() or torch.isinf(grad_norm).any():\ngradients[name] = param.grad.data\n", "fix_pattern": "<condition>: Checking for NaN or Inf values in the gradient norm.\n<pattern>: Using torch.isnan() or torch.isinf() to check for NaN or Inf values.\n<code_one>: Calculating the gradient norm using torch.norm(param.grad.data, p=2, dtype=torch.float32).\n<code_two>: Changing the calculation of gradient norm to torch.norm(param.grad.data.float(), p=2).\nFix_pattern: In the condition of checking for NaN or Inf values in the gradient norm, if NaN or Inf values are detected, then change the calculation of the gradient norm from using torch.norm(param.grad.data, p=2, dtype=torch.float32) to torch.norm(param.grad.data.float(), p=2) to fix the API misuse."}
{"number": 274, "change": "def create_loader(\n# of samples per-process, will slightly alter validation results\nsampler = OrderedDistributedSampler(dataset)\n\n+    if collate_fn is None:\n+        collate_fn = fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate\n+\nloader = torch.utils.data.DataLoader(\ndataset,\nbatch_size=batch_size,\nshuffle=sampler is None and is_training,\nnum_workers=num_workers,\nsampler=sampler,\n-        collate_fn=fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate,\n+        collate_fn=collate_fn,\ndrop_last=is_training,\n)\nif use_prefetcher:\n", "fix_pattern": "<condition>: If the use_prefetcher flag is True.\n<pattern>: The collate_fn attribute is set to fast_collate if use_prefetcher is True, or default_collate otherwise.\n<code_one>: collate_fn=fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate,\n<code_two>: collate_fn = fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate\nFix_pattern: In the condition of the use_prefetcher flag being True, if the collate_fn attribute is set to fast_collate if use_prefetcher is True, or default_collate otherwise, then remove collate_fn=fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate and add collate_fn = fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate."}
{"number": 278, "change": "def mesh_laplacian_smoothing(meshes, method: str = \"uniform\"):\nelif method == \"cot\":\nloss = L.mm(verts_packed) * norm_w - verts_packed\nelif method == \"cotcurv\":\n-        loss = (L.mm(verts_packed) - verts_packed) * norm_w\n+        loss = (L.mm(verts_packed) - L_sum * verts_packed) * norm_w\nloss = loss.norm(dim=1)\n\nloss = loss * weights\n", "fix_pattern": "<condition>:\nmethod == \"cotcurv\"\n\n<pattern>:\nRemove the line of code calculating \"loss\" without applying any transformation to it.\n\n<code_one>:\nloss = loss.norm(dim=1)\n\n<code_two>:\nloss = loss * weights\n\nFix_pattern:\nIn the condition of method == \"cotcurv\", if the pattern of calculating \"loss\" without any transformation is detected, then remove the line of code calculating \"loss\" and replace it with the line of code multiplying \"loss\" with \"weights\" to fix the API misuse."}
{"number": 280, "change": "class AsyncMultiGPUTrainer(MultiGPUTrainer,\n\nself._setup_predictor_factory(predict_tower)\nself._average_gradient = average_gradient\n+        assert tf.test.is_gpu_available()\n\ndef _setup(self):\nsuper(AsyncMultiGPUTrainer, self)._setup()\n", "fix_pattern": "<condition>:\nThere is no pre condition needed for this fix pattern.\n\n<pattern>:\nThe pattern is to add an assert statement to check if a GPU is available.\n\n<code_one>:\nNo code needs to be removed for this fix pattern.\n\n<code_two>:\nThe code that needs to be added is:\nassert tf.test.is_gpu_available()\n\nFix_pattern:\nIn the condition of no pre condition is needed, if a GPU is not available, then add the assert statement \"assert tf.test.is_gpu_available()\" to fix the API misuse."}
{"number": 282, "change": "class GCNConv(MessagePassing):\nx = torch.matmul(x, self.weight)\n\nif not self.cached or self.cached_result is None:\n-            edge_index, norm = GCNConv.norm(edge_index,\n-                                            x.size(0), edge_weight,\n+            edge_index, norm = GCNConv.norm(edge_index, x.size(0), edge_weight,\nself.improved, x.dtype)\nself.cached_result = edge_index, norm\n", "fix_pattern": "<condition>: The condition is \"if not self.cached or self.cached_result is None\".\n\n<pattern>: The pattern is \"GCNConv.norm(edge_index, x.size(0), edge_weight,\".\n\n<code_one>: The code that was removed is \"edge_index, norm = GCNConv.norm(edge_index, x.size(0), edge_weight,\".\n\n<code_two>: The code that was added is \"edge_index, norm = GCNConv.norm(edge_index, x.size(0), edge_weight,\".\n\nFix_pattern: In the condition of \"if not self.cached or self.cached_result is None\", if the pattern \"GCNConv.norm(edge_index, x.size(0), edge_weight,\" is detected, then remove the code \"edge_index, norm = GCNConv.norm(edge_index, x.size(0), edge_weight,\" and add the code \"edge_index, norm = GCNConv.norm(edge_index, x.size(0), edge_weight,\" to fix the API misuse."}
{"number": 283, "change": "class CTC(torch.nn.Module):\nif self.ctc_type == \"builtin\":\nolens = to_device(ys_hat, torch.LongTensor([len(s) for s in ys]))\nhlens = hlens.long()\n+            ys_pad = torch.cat(ys)  # without this the code breaks for asr_mix\nself.loss = self.loss_fn(ys_hat, ys_pad, hlens, olens)\nelse:\nself.loss = None\n", "fix_pattern": "<condition>: The condition is that if the value of self.ctc_type is not \"builtin\".\n<pattern>: The pattern is that without the line \"ys_pad = torch.cat(ys)\" the code breaks for asr_mix.\n<code_one>: The code that is removed is empty.\n<code_two>: The code that is added is \"ys_pad = torch.cat(ys)\".\nFix_pattern: In the condition of self.ctc_type not being \"builtin\", add the line \"ys_pad = torch.cat(ys)\" to fix the API misuse."}
{"number": 296, "change": "class DeformableDetrModelIntegrationTests(unittest.TestCase):\nresults = feature_extractor.post_process_object_detection(\noutputs, threshold=0.3, target_sizes=[image.size[::-1]]\n)[0]\n-        expected_scores = torch.tensor([0.7999, 0.7894, 0.6331, 0.4720, 0.4382])\n+        expected_scores = torch.tensor([0.7999, 0.7894, 0.6331, 0.4720, 0.4382]).to(torch_device)\nexpected_labels = [17, 17, 75, 75, 63]\n-        expected_slice_boxes = torch.tensor([16.5028, 52.8390, 318.2544, 470.7841])\n+        expected_slice_boxes = torch.tensor([16.5028, 52.8390, 318.2544, 470.7841]).to(torch_device)\n\nself.assertEqual(len(results[\"scores\"]), 5)\nself.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-4))\n", "fix_pattern": "<condition>:\nNo pre condition is needed\n\n<pattern>:\nThe pattern detected is that the tensors \"expected_scores\" and \"expected_slice_boxes\" were not properly converted to the correct torch device.\n\n<code_one>:\nIn the code removed section, the original \"expected_scores\" and \"expected_slice_boxes\" tensors were defined without the \".to(torch_device)\" method.\n\n<code_two>:\nIn the code added section, the \"expected_scores\" and \"expected_slice_boxes\" tensors were defined with the \".to(torch_device)\" method to ensure they are on the correct torch device.\n\nFix_pattern:\nIn the condition of no pre condition is needed, if the pattern of not properly converting tensors to the correct torch device is detected, then add \".to(torch_device)\" method to the tensors in order to fix the API misuse."}
{"number": 297, "change": "class Critic(object):\nn = InputLayer(self.s, name='in')\nn = DenseLayer(n, n_units=30, act=tf.nn.relu6, W_init=tf.random_uniform_initializer(0, 0.01), name='hidden')\n# n = DenseLayer(n, n_units=5, act=tf.nn.relu, W_init=tf.random_uniform_initializer(0, 0.01), name='hidden2')\n-            n = DenseLayer(n, n_units=1, act=tf.identity, name='V')\n+            n = DenseLayer(n, n_units=1, act=None, name='V')\nself.v = n.outputs\n\nwith tf.variable_scope('squared_TD_error'):\n", "fix_pattern": "<condition>: In the code context, there is no specific condition identified.\n\n<pattern>: The pattern that is detected is the replacement of the activation function `tf.identity` with `None`.\n\n<code_one>: The code that is removed is `n = DenseLayer(n, n_units=1, act=tf.identity, name='V')`.\n\n<code_two>: The code that is added is `n = DenseLayer(n, n_units=1, act=None, name='V')`.\n\nFix_pattern: In the condition of no specific condition, if the pattern of replacing the activation function `tf.identity` with `None` is detected, then remove `n = DenseLayer(n, n_units=1, act=tf.identity, name='V')` and add `n = DenseLayer(n, n_units=1, act=None, name='V')` to fix the API misuse."}
{"number": 298, "change": "class TensorflowBackendInferenceLearner(TensorflowBaseInferenceLearner):\nmetadata = LearnerMetadata.read(path)\nnetwork_parameters = ModelParams(**metadata.network_parameters)\ninput_tfms = metadata.input_tfms\n-        model = nebullvm.operations.inference_learners.utils.load_model(\n+        model = tf.keras.models.load_model(\npath / TENSORFLOW_BACKEND_FILENAMES[\"tf_model\"]\n)\ndevice = Device(metadata.device)\n", "fix_pattern": "<condition>: There is an attempt to load a model using an API from the package 'nebullvm.operations.inference_learners.utils'.\n<pattern>: The code uses the incorrect API to load a model.\n<code_one>: model = nebullvm.operations.inference_learners.utils.load_model(\n<code_two>: model = tf.keras.models.load_model(\nFix_pattern: In the condition of attempting to load a model using an API from 'nebullvm.operations.inference_learners.utils', if the pattern of 'model = nebullvm.operations.inference_learners.utils.load_model(' is detected, then change the code to 'model = tf.keras.models.load_model(' to fix the API misuse."}
{"number": 300, "change": "FileType = Any\n# Represents the result dict returned by Trainer.train().\nResultDict = dict\n\n+# A tf or torch local optimizer object.\n+LocalOptimizer = Union[\"tf.keras.optimizers.Optimizer\",\n+                       \"torch.optim.Optimizer\"]\n+\n# Dict of tensors returned by compute gradients on the policy, e.g.,\n# {\"td_error\": [...], \"learner_stats\": {\"vf_loss\": ..., ...}}, for multi-agent,\n# {\"policy1\": {\"learner_stats\": ..., }, \"policy2\": ...}.\n", "fix_pattern": "<condition>:\nThe condition is when there is a requirement for a tf or torch local optimizer object in the code.\n\n<pattern>:\nThe pattern is the addition of the LocalOptimizer type hint to the code.\n\n<code_one>:\nThere is no code removal in this fix pattern.\n\n<code_two>:\nThe code added is:\n\n# A tf or torch local optimizer object.\nLocalOptimizer = Union[\"tf.keras.optimizers.Optimizer\",\n                       \"torch.optim.Optimizer\"]\n\nFix_pattern:\nIn the condition of requiring a tf or torch local optimizer object, adding the LocalOptimizer type hint to the code fixes the API misuse."}
{"number": 305, "change": "def ones_like(x, name=None):\n[ 1.,  1.,  1.]], dtype=float32)\n```\n\"\"\"\n-    return tf.ones_like(x, name=name)\n+    return tf.ones_like(x, dtype=dtype, name=name)\n\n\ndef random_uniform_variable(shape, low, high, dtype=None,\n", "fix_pattern": "<condition>: The function is using the `ones_like` function from a TensorFlow library.\n<pattern>: The `dtype` parameter is missing in the `ones_like` function call.\n<code_one>: `tf.ones_like(x, name=name)`\n<code_two>: `tf.ones_like(x, dtype=dtype, name=name)`\nFix_pattern: In the condition of using the `ones_like` function, if the `dtype` parameter is missing, then change the `code_one` to `code_two` to fix the API misuse."}
{"number": 307, "change": "from allennlp.common.params import Params\n\nclass TestStackedBidirectionalLstm(AllenNlpTestCase):\ndef test_stacked_bidirectional_lstm_completes_forward_pass(self):\n-        input_tensor = torch.autograd.Variable(torch.rand(4, 5, 3))\n+        input_tensor = torch.rand(4, 5, 3)\ninput_tensor[1, 4:, :] = 0.\ninput_tensor[2, 2:, :] = 0.\ninput_tensor[3, 1:, :] = 0.\n", "fix_pattern": "<condition>: No pre condition is needed.\n<pattern>: No pattern detected.\n<code_one>: No code removed.\n<code_two>: No code added.\nFix_pattern: No fix pattern detected."}
{"number": 309, "change": "class TFCTRLMainLayer(tf.keras.layers.Layer):\ntoken_type_embeds = 0\nposition_ids = tf.reshape(position_ids, [-1, shape_list(position_ids)[-1]])\n\n-        inputs_embeds = self.w(input_ids)\n+        inputs_embeds = self.w(input_ids, mode='embedding')\n# x = embedded.unsqueeze(0) if len(input_ids.shape)<2 else embedded\nseq_len = input_shape[-1]\nmask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n", "fix_pattern": "<condition>: The condition is that the variable \"input_ids\" is being used as an argument for the function \"self.w\" in the code.\n<pattern>: The pattern is that the API misuse is due to not specifying the mode parameter when calling the function \"self.w\".\n<code_one>: The code being removed is \"inputs_embeds = self.w(input_ids)\".\n<code_two>: The code being added is \"inputs_embeds = self.w(input_ids, mode='embedding')\".\nFix_pattern: In the condition of \"input_ids\" being used as an argument for \"self.w\", if the pattern of not specifying the mode is detected, then change \"inputs_embeds = self.w(input_ids)\" to \"inputs_embeds = self.w(input_ids, mode='embedding')\" to fix the API misuse."}
{"number": 315, "change": "class TFWav2Vec2MainLayer(tf.keras.layers.Layer):\nif inputs[\"attention_mask\"] is not None:\n# compute real output lengths according to convolution formula\noutput_lengths = self._get_feat_extract_output_lengths(tf.reduce_sum(inputs[\"attention_mask\"], -1))\n-            attention_mask = tf.sequence_mask(output_lengths, dtype=hidden_states.dtype)\n+\n+            attention_mask = tf.sequence_mask(\n+                output_lengths, maxlen=shape_list(hidden_states)[1], dtype=hidden_states.dtype\n+            )\n\nhidden_states = self.feature_projection(hidden_states, training=inputs[\"training\"])\n", "fix_pattern": "<condition>: The condition is when the \"attention_mask\" input is not None.\n<pattern>: The pattern is the usage of tf.sequence_mask with only the \"output_lengths\" argument.\n<code_one>: The code removed is the line \"attention_mask = tf.sequence_mask(output_lengths, dtype=hidden_states.dtype)\".\n<code_two>: The code added is the line \"attention_mask = tf.sequence_mask(output_lengths, maxlen=shape_list(hidden_states)[1], dtype=hidden_states.dtype)\".\nFix_pattern: In the condition of \"attention_mask\" not being None, if the code uses tf.sequence_mask with only the \"output_lengths\" argument, then the code needs to be changed from \"attention_mask = tf.sequence_mask(output_lengths, dtype=hidden_states.dtype)\" to \"attention_mask = tf.sequence_mask(output_lengths, maxlen=shape_list(hidden_states)[1], dtype=hidden_states.dtype)\" to fix the API misuse."}
{"number": 323, "change": "class BaseModel():\nsave_filename = '%s_net_%s.pth' % (which_epoch, name)\nsave_path = os.path.join(self.save_dir, save_filename)\nnet = getattr(self, 'net' + name)\n-                net.load_state_dict(torch.load(save_path))\n+                net.module.load_state_dict(torch.load(save_path))\n\n# print network information\ndef print_networks(self, verbose):\n", "fix_pattern": "<condition>: The code is trying to load the state dictionary of a neural network.\n<pattern>: The code is using the `net.load_state_dict()` method.\n<code_one>: `net.load_state_dict(torch.load(save_path))`\n<code_two>: `net.module.load_state_dict(torch.load(save_path))`\nFix_pattern: In the condition of loading the state dictionary of a neural network, if `net.load_state_dict()` is detected, then change `net.load_state_dict(torch.load(save_path))` to `net.module.load_state_dict(torch.load(save_path))` to fix the API misuse."}
{"number": 324, "change": "class SpeedsterRootOp(Operation):\n) -> List[BaseInferenceLearner]:\nif self.orig_latency_measure_op.get_result() is not None:\nmodel_outputs = self.orig_latency_measure_op.get_result()[0]\n-            if isinstance(model, Module):\n+            if isinstance(model, torch.nn.Module):\noptimization_op = self.torch_optimization_op\nelif isinstance(model, tf.Module) and model is not None:\noptimization_op = self.tensorflow_optimization_op\n", "fix_pattern": "<condition>: The condition is that the variable \"model\" is being checked for its type.\n\n<pattern>: The pattern being detected is the use of \"isinstance(model, Module)\" to check if \"model\" is an instance of the \"Module\" class.\n\n<code_one>: The code being removed is the line \"if isinstance(model, Module):\".\n\n<code_two>: The code being added is the line \"if isinstance(model, torch.nn.Module):\".\n\nFix_pattern: In the condition of checking the type of \"model\", if the pattern of using \"isinstance(model, Module)\" is detected, then change the line of code from \"if isinstance(model, Module):\" to \"if isinstance(model, torch.nn.Module):\" to fix the API misuse."}
{"number": 328, "change": "class UNet2DConditionModel(ModelMixin, ConfigMixin):\nif not torch.is_tensor(timesteps):\ntimesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\nelif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n-            timesteps = timesteps[None].to(sample.device)\n+            timesteps = timesteps.to(dtype=torch.float32)\n+            timesteps = timesteps[None].to(device=sample.device)\n\n# broadcast to batch dimension in a way that's compatible with ONNX/Core ML\ntimesteps = timesteps.expand(sample.shape[0])\n", "fix_pattern": "<condition>: The condition is that the variable \"timesteps\" is checked to see if it is not a tensor or if it is a tensor with a shape of 0.\n<pattern>: The pattern is that if the condition is satisfied, the variable \"timesteps\" is modified.\n<code_one>: The code that is removed is \"timesteps = timesteps[None].to(sample.device)\".\n<code_two>: The code that is added is \"timesteps = timesteps.to(dtype=torch.float32)\\n            timesteps = timesteps[None].to(device=sample.device)\".\nFix_pattern: In the condition of checking the \"timesteps\" variable, if it is not a tensor or if it is a tensor with a shape of 0, the \"timesteps\" variable is modified by first converting it to `torch.float32` dtype and then adding a dimension to it using `None`. Additionally, the device is set to `sample.device`."}
{"number": 333, "change": "class BilinearSimilarity(SimilarityFunction):\nself.reset_parameters()\n\ndef reset_parameters(self):\n-        torch.nn.init.xavier_uniform(self._weight_matrix)\n+        torch.nn.init.xavier_uniform_(self._weight_matrix)\nself._bias.data.fill_(0)\n\n@overrides\n", "fix_pattern": "<condition>: The condition is when the `reset_parameters` method is called in the `BilinearSimilarity` class.\n\n<pattern>: The pattern is the API misuse of the `torch.nn.init.xavier_uniform` method.\n\n<code_one>: The code that is removed is `torch.nn.init.xavier_uniform(self._weight_matrix)`.\n\n<code_two>: The code that is added is `torch.nn.init.xavier_uniform_(self._weight_matrix)`.\n\nFix_pattern: In the condition of calling the `reset_parameters` method in the `BilinearSimilarity` class, if the API misuse pattern of `torch.nn.init.xavier_uniform` is detected, then change the code from `torch.nn.init.xavier_uniform(self._weight_matrix)` to `torch.nn.init.xavier_uniform_(self._weight_matrix)` to fix the API misuse."}
{"number": 335, "change": "class AdaptiveEmbedding(nn.Module):\n\ninp_i = inp_flat.index_select(0, indices_i) - l_idx\nemb_i = self.emb_layers[i](inp_i)\n-                emb_i = F.linear(emb_i, self.emb_projs[i])\n+                emb_i = nn.functional.linear(emb_i, self.emb_projs[i])\n\nemb_flat.index_copy_(0, indices_i, emb_i)\n", "fix_pattern": "<condition>: The code is using the `F` module from the `nn` package.\n<pattern>: The pattern is that `F.linear()` is being used to perform a linear transformation.\n<code_one>: `F.linear(emb_i, self.emb_projs[i])`\n<code_two>: `nn.functional.linear(emb_i, self.emb_projs[i])`\nFix_pattern: In the condition of using the `F` module, if `F.linear()` is detected, then change the `F.linear()` to `nn.functional.linear()` to fix the API misuse."}
{"number": 337, "change": "class Residual(tf.keras.Model):  #@save\nif self.conv3 is not None:\nX = self.conv3(X)\nY += X\n-        return tf.keras.activations.relu(Y + X)\n+        return tf.keras.activations.relu(Y)\n", "fix_pattern": "<condition>: \"self.conv3 is not None\"\n<pattern>: \"Y += X\" is used to add X to Y.\n<code_one>: \"return tf.keras.activations.relu(Y + X)\"\n<code_two>: \"return tf.keras.activations.relu(Y)\"\nFix_pattern: In the condition of \"self.conv3 is not None\", if the pattern \"Y += X\" is detected, then change the code \"return tf.keras.activations.relu(Y + X)\" to \"return tf.keras.activations.relu(Y)\" to fix the API misuse."}
{"number": 341, "change": "for m in model_list:\ndata_root=os.environ.get('IMAGENET_DIR', './imagenet')\n)\n\n+    torch.cuda.empty_cache()\n+\n", "fix_pattern": "<condition>:\nMissing call to empty_cache() after the extraction of features from a model using the GPU.\n\n<pattern>:\nA check is added after the feature extraction code to determine if the code is running on a GPU. If it is, then the empty_cache() function is called to clear the GPU cache.\n\n<code_one>:\nNo specific code is removed.\n\n<code_two>:\n```\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n```\n\nFix_pattern:\nIn the condition of checking if CUDA is available, if it is, then the empty_cache() function is called to clear the GPU cache, to fix the API misuse."}
{"number": 342, "change": "class TFMLP(tf.keras.layers.Layer):\nnx = config.n_embd\nself.c_fc = TFConv1D(n_state, nx, initializer_range=config.initializer_range, name=\"c_fc\")\nself.c_proj = TFConv1D(nx, n_state, initializer_range=config.initializer_range, name=\"c_proj\")\n-        self.act = gelu\n+        self.act = get_tf_activation(\"gelu\")\nself.dropout = tf.keras.layers.Dropout(config.resid_pdrop)\n\ndef call(self, x, training=False):\n", "fix_pattern": "<condition>: This condition applies to the class `TFMLP` in the given code snippet.\n<pattern>: The pattern being detected is a specific activation function assignment, `self.act = gelu`.\n<code_one>: The code `self.act = gelu` was removed.\n<code_two>: The code `self.act = get_tf_activation(\"gelu\")` was added.\nFix_pattern: In the condition of the class `TFMLP`, if the pattern of assigning `self.act = gelu` is detected, then change the code to `self.act = get_tf_activation(\"gelu\")` to fix the API misuse."}
{"number": 347, "change": "class TacotronAbstract(ABC, nn.Module):\ndef _backward_pass(self, mel_specs, encoder_outputs, mask):\n\"\"\" Run backwards decoder \"\"\"\ndecoder_outputs_b, alignments_b, _ = self.decoder_backward(\n-            encoder_outputs, torch.flip(mel_specs, dims=(1,)), mask,\n-            self.speaker_embeddings_projected)\n+            encoder_outputs, torch.flip(mel_specs, dims=(1,)), mask)\ndecoder_outputs_b = decoder_outputs_b.transpose(1, 2).contiguous()\nreturn decoder_outputs_b, alignments_b\n", "fix_pattern": "<condition>: The condition is not specified in the given code snippet.\n<pattern>: The pattern is not specified in the given code snippet.\n<code_one>: The code `self.speaker_embeddings_projected` is removed.\n<code_two>: No code is added in the given code snippet.\nFix_pattern: In the condition of unspecified condition, if the code `self.speaker_embeddings_projected` is detected, then remove it to fix the API misuse."}
{"number": 348, "change": "def main():\nmodel = MMDataParallel(model, device_ids=[0])\noutputs = single_gpu_test(model, data_loader, args.show)\nelse:\n-        model = MMDistributedDataParallel(model.cuda())\n+        model = MMDistributedDataParallel(\n+            model.cuda(),\n+            device_ids=[torch.cuda.current_device()],\n+            broadcast_buffers=False)\noutputs = multi_gpu_test(model, data_loader, args.tmpdir,\nargs.gpu_collect)\n", "fix_pattern": "<condition>: There is a need to use MMDistributedDataParallel for multi-GPU testing.\n<pattern>: The previous code was using MMDataParallel instead of MMDistributedDataParallel for multi-GPU testing.\n<code_one>: model = MMDistributedDataParallel(model.cuda())\n<code_two>: model = MMDistributedDataParallel(model.cuda(), device_ids=[torch.cuda.current_device()], broadcast_buffers=False)\nFix_pattern: In the condition of performing multi-GPU testing, if the previous code is using MMDataParallel, then change it to MMDistributedDataParallel with the specified device_ids and broadcast_buffers settings to fix the API misuse."}
{"number": 357, "change": "class Tester(unittest.TestCase):\n# generate input data\nbatch_size = 1\ncenter = torch.zeros(batch_size, 2)\n-        angle = torch.ones(batch_size, 1)\n-        scale = torch.ones(batch_size, 1)\n+        angle = torch.ones(batch_size)\n+        scale = torch.ones(batch_size)\n\ncenter = utils.tensor_to_gradcheck_var(center)  # to var\nangle = utils.tensor_to_gradcheck_var(angle)  # to var\n", "fix_pattern": "<condition>: When converting a tensor to a variable using the function `utils.tensor_to_gradcheck_var`.\n<pattern>: The code removes the dimension of size 1 for both `angle` and `scale` tensors.\n<code_one>: `angle = torch.ones(batch_size, 1)`\n<code_two>: `angle = torch.ones(batch_size)`\nFix_pattern: In the condition of converting a tensor to a variable using `utils.tensor_to_gradcheck_var`, if there is a tensor `angle` with a dimension of size 1, then remove the dimension of size 1 from `angle` to fix the API misuse."}
{"number": 360, "change": "class Trainer:\nreturn type(data)(self._prepare_input(v) for v in data)\nelif isinstance(data, torch.Tensor):\nkwargs = {\"device\": self.args.device}\n-            if self.deepspeed and data.dtype != torch.int64:\n-                # NLP models inputs are int64 and those get adjusted to the right dtype of the\n+            if self.deepspeed and (torch.is_floating_point(data) or torch.is_complex(data)):\n+                # NLP models inputs are int/uint and those get adjusted to the right dtype of the\n# embedding. Other models such as wav2vec2's inputs are already float and thus\n# may need special handling to match the dtypes of the model\nkwargs.update({\"dtype\": self.args.hf_deepspeed_config.dtype()})\n", "fix_pattern": "<condition>: The condition is that self.deepspeed is True and data's dtype is not torch.int64.\n<pattern>: The pattern is that if data's dtype is not torch.int64, it needs to be adjusted to the right dtype for NLP models.\n<code_one>: The code that was removed is \"if self.deepspeed and data.dtype != torch.int64\".\n<code_two>: The code that was added is \"if self.deepspeed and (torch.is_floating_point(data) or torch.is_complex(data))\".\nFix_pattern: In the condition of self.deepspeed being True and data's dtype not being torch.int64, the fix pattern is to check if data is of type torch.is_floating_point or torch.is_complex, and adjust it to the right dtype for NLP models."}
{"number": 362, "change": "class DeformableDetrImageProcessor(BaseImageProcessor):\nimg_w = torch.Tensor([i[1] for i in target_sizes])\nelse:\nimg_h, img_w = target_sizes.unbind(1)\n-        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n+        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\nboxes = boxes * scale_fct[:, None, :]\n\nresults = []\n", "fix_pattern": "<condition>: The code is checking for the dimensions of the target image.\n<pattern>: The pattern that is detected is that the tensor `scale_fct` needs to be moved to the same device as the `boxes` tensor.\n<code_one>: The code that is removed is `scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)`.\n<code_two>: The code that is added is `scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)`.\nFix_pattern: In the condition of checking the target image dimensions, if the pattern of `scale_fct` not being on the same device as `boxes` is detected, then the code is changed from `scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)` to `scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)` to fix the API misuse."}
{"number": 363, "change": "class SpanBasedF1Test(AllenNlpTestCase):\ngold_indices = [self.vocab.get_token_index(x, \"tags\") for x in bio_tags]\ngold_tensor = torch.tensor([gold_indices], device=device)\nprediction_tensor = torch.rand([1, 6, self.vocab.get_vocab_size(\"tags\")], device=device)\n-        mask = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device=device)\n+        mask = torch.BoolTensor(\n+            [[True, True, True, True, True, True, True, True, True]], device=device\n+        )\n\n# Make prediction so that it is exactly correct.\nfor i, tag_index in enumerate(gold_indices):\n", "fix_pattern": "<condition>: When converting a tensor to a boolean tensor.\n<pattern>: Incorrect syntax for creating a boolean tensor.\n<code_one>: `mask = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device=device)`\n<code_two>: `mask = torch.BoolTensor([[True, True, True, True, True, True, True, True, True]], device=device)`\nFix_pattern: In the condition of converting a tensor to a boolean tensor, if the incorrect syntax for creating a boolean tensor is detected, then replace the code `mask = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device=device)` with `mask = torch.BoolTensor([[True, True, True, True, True, True, True, True, True]], device=device)` to fix the API misuse."}
{"number": 364, "change": "class TowerContext(object):\nself._ctxs = []\nif len(self._name):\nif self.has_own_variables:\n-                # open new variable scopes\n-                self._ctxs.append(tf.variable_scope(self._name))\n+                if self.vs_name:\n+                    self._ctxs.append(tf.variable_scope(self.vs_name))\nelse:\n# use existing variable scope\nreuse = self.index > 0 or (not self.is_training)\n", "fix_pattern": "<condition>: Checking if a certain condition is met\n<pattern>: Identifying the usage of an incorrect API\n<code_one>: Incorrect API usage\n<code_two>: Corrected API usage\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then change <code_one> to <code_two> to fix the API misuse."}
{"number": 367, "change": "class _BinaryPostprocessing(torch.nn.Module):\npredictions = [self.bool2str.get(pred, self.bool2str[0]) for pred in predictions]\n\nprobs = preds[self.probabilities_key]\n-        probs = torch.dstack(1 - probs, probs)\n+        probs = torch.stack([1 - probs, probs], dim=-1)\n\nreturn {\nself.predictions_key: predictions,\n", "fix_pattern": "<condition>: The condition is when there is a need to stack tensors along a specified dimension.\n\n<pattern>: The pattern is that the tensor stacking is done using `torch.dstack`.\n\n<code_one>: The code to be removed is `probs = torch.dstack(1 - probs, probs)`.\n\n<code_two>: The code to be added is `probs = torch.stack([1 - probs, probs], dim=-1)`.\n\nFix_pattern: In the condition of needing to stack tensors, if the tensor stacking is done using `torch.dstack`, then remove `torch.dstack` and replace it with `torch.stack([1 - probs, probs], dim=-1)` to fix the API misuse."}
{"number": 368, "change": "if __name__ == \"__main__\":\nexp = get_exp(args.exp_file, args.name)\nexp.merge(args.opts)\n\n-    num_gpu = get_num_devices() if args.devices is None else args.devices\n-    assert num_gpu <= get_num_devices()\n+    num_gpu = torch.cuda.device_count() if args.devices is None else args.devices\n+    assert num_gpu <= torch.cuda.device_count()\n\ndist_url = \"auto\" if args.dist_url is None else args.dist_url\nlaunch(\n", "fix_pattern": "<condition>: The code is checking if the number of GPUs is specified by the user or not.\n<pattern>: The pattern being detected is the inconsistency in accessing the number of available GPUs.\n<code_one>: The code is using the `get_num_devices()` function to determine the number of GPUs.\n<code_two>: The code is using the `torch.cuda.device_count()` function to determine the number of GPUs.\nFix_pattern: In the condition of checking if the number of GPUs is specified, if the inconsistency in accessing the number of available GPUs is detected, then replace the usage of `get_num_devices()` with `torch.cuda.device_count()` to fix the API misuse."}
{"number": 370, "change": "def test_log_prob_eta1(d):\nassert (lps_less_ladj - lps_less_ladj.min()).abs().sum() < 1e-4\n\n\n-@pytest.mark.parametrize(\"eta\", [.1, .5, 1, 2, 5])\n+@pytest.mark.parametrize(\"eta\", [.1, .5, 1., 2., 5.])\ndef test_log_prob_d2(eta):\n-    dist = LKJCorrCholesky(2, torch.DoubleTensor([eta]))\n+    dist = LKJCorrCholesky(2, torch.tensor([eta]))\ntest_dist = TransformedDistribution(Beta(eta, eta), AffineTransform(loc=-1., scale=2.0))\n\nsamples = dist.sample(torch.Size([100]))\n", "fix_pattern": "<condition>:\nIn the context section, the condition is not clearly stated.\n\n<pattern>:\nThe pattern is not clearly stated in the code removed section.\n\n<code_one>:\nThe code removed section does not provide any code to be removed.\n\n<code_two>:\nThe code added section includes the addition of a dot (.) after the number 1 in the list of values for the parameter \"eta\" in the \"parametrize\" function. Also, the DoubleTensor type is changed to the tensor type.\n\nFix_pattern:\nIn the condition of unknown condition, if an unknown pattern is detected, then change the code_one to code_two to fix the API misuse."}
{"number": 375, "change": "def corr2d(X, K):  #@save\n\n# Defined in file: ./chapter_convolutional-neural-networks/lenet.md\ndef evaluate_accuracy_gpu(net, data_iter, device=None): #@save\n+    net.eval()  # Set the model to evaluation mode\nif not device:\ndevice = next(iter(net.parameters())).device\nmetric = d2l.Accumulator(2)  # num_corrected_examples, num_examples\n", "fix_pattern": "<condition>: The condition is that the device is not specified.\n<pattern>: The pattern is that the model is not set to evaluation mode.\n<code_one>: No code is removed.\n<code_two>: The code added is \"net.eval()\".\nFix_pattern: In the condition of the device not being specified, if the model is not set to evaluation mode, then add \"net.eval()\" to fix the API misuse."}
{"number": 376, "change": "class VideoSequential(ImageSequential):\n# Size of T\nframe_num = input.size(self._temporal_channel)\n# Got param generation shape to (B, C, H, W). Ignoring T.\n-        batch_shape = self.__infer_channel_exclusive_batch_shape__(input)\n+        batch_shape = self.__infer_channel_exclusive_batch_shape__(input, self._temporal_channel)\ninput = self._input_shape_convert_in(input)\ninput = input.reshape(-1, *batch_shape[1:])\nif not self.same_on_frame:\n", "fix_pattern": "<condition>: The code is trying to infer the batch shape from the input.\n<pattern>: The code is missing an argument in the function call.\n<code_one>: `input` is passed as an argument to the function `self.__infer_channel_exclusive_batch_shape__()`.\n<code_two>: `input` and `self._temporal_channel` are passed as arguments to the function `self.__infer_channel_exclusive_batch_shape__()`.\nFix_pattern: In the condition of inferring the batch shape from the input, if the function call to `self.__infer_channel_exclusive_batch_shape__()` is missing an argument, then add `self._temporal_channel` as an argument to the function call to fix the API misuse."}
{"number": 377, "change": "def create_dataloader(path, imgsz, batch_size, stride, single_cls=False, hyp=Non\nprefix=prefix)\n\nbatch_size = min(batch_size, len(dataset))\n-    nw = min([os.cpu_count() // DEVICE_COUNT, batch_size if batch_size > 1 else 0, workers])  # number of workers\n+    nd = torch.cuda.device_count()  # number of CUDA devices\n+    nw = min([os.cpu_count() // max(nd, 1), batch_size if batch_size > 1 else 0, workers])  # number of workers\nsampler = None if rank == -1 else distributed.DistributedSampler(dataset, shuffle=shuffle)\nloader = DataLoader if image_weights else InfiniteDataLoader  # only DataLoader allows for attribute updates\nreturn loader(dataset,\n", "fix_pattern": "<condition>: The condition can be identified as the need to determine the number of workers to use in the data loader.\n\n<pattern>: The pattern that is detected is that the number of workers is calculated using the `os.cpu_count()` function and the `DEVICE_COUNT` variable.\n\n<code_one>: The code that is removed is `nw = min([os.cpu_count() // DEVICE_COUNT, batch_size if batch_size > 1 else 0, workers])`.\n\n<code_two>: The code that is added is `nd = torch.cuda.device_count()` and `nw = min([os.cpu_count() // max(nd, 1), batch_size if batch_size > 1 else 0, workers])`.\n\nFix_pattern: In the condition of determining the number of workers for the data loader, if the pattern of calculating the number of workers using `os.cpu_count()` and `DEVICE_COUNT` is detected, then remove the code `nw = min([os.cpu_count() // DEVICE_COUNT, batch_size if batch_size > 1 else 0, workers])` and replace it with `nd = torch.cuda.device_count()` and `nw = min([os.cpu_count() // max(nd, 1), batch_size if batch_size > 1 else 0, workers])` to fix the API misuse."}
{"number": 384, "change": "def array(object_in, dtype: Optional[str] = None, dev: Optional[str] = None):\ndev = default_device(dev)\ndtype = dtype_from_str(default_dtype(dtype, object_in))\nif isinstance(object_in, np.ndarray):\n-        return _torch.Tensor(object_in).to(dev_from_str(dev))\n+        return torch.Tensor(object_in).to(dev_from_str(dev))\nif dtype is not None:\n-        return _torch.tensor(object_in, dtype=dtype, device=dev_from_str(dev))\n-    elif isinstance(object_in, _torch.Tensor):\n+        return torch.tensor(object_in, dtype=dtype, device=dev_from_str(dev))\n+    elif isinstance(object_in, torch.Tensor):\nreturn object_in.to(dev_from_str(dev))\nelse:\n-        return _torch.tensor(object_in, device=dev_from_str(dev))\n+        return torch.tensor(object_in, device=dev_from_str(dev))\n\nasarray = array\n", "fix_pattern": "<condition>: The condition is that the object_in parameter is of type np.ndarray or _torch.Tensor.\n\n<pattern>: The pattern is that if the object_in is an np.ndarray and dtype is not None, then the code returns object_in.to(dev_from_str(dev)). If the object_in is an _torch.Tensor, then the code returns _torch.tensor(object_in, device=dev_from_str(dev)).\n\n<code_one>: The code that is removed is the following:\n- return _torch.Tensor(object_in).to(dev_from_str(dev))\n- return _torch.tensor(object_in, dtype=dtype, device=dev_from_str(dev))\n- return _torch.tensor(object_in, device=dev_from_str(dev))\n\n<code_two>: The code that is added is the following:\n- return torch.Tensor(object_in).to(dev_from_str(dev))\n- return torch.tensor(object_in, dtype=dtype, device=dev_from_str(dev))\n- return torch.tensor(object_in, device=dev_from_str(dev))\n\nFix_pattern: In the condition of checking the type of object_in, if the object_in is an np.ndarray and dtype is not None, then replace the code with return object_in.to(dev_from_str(dev)). If the object_in is an _torch.Tensor, then replace the code with return torch.tensor(object_in, device=dev_from_str(dev))."}
{"number": 387, "change": "class Trainer(\nself.gpus = gpus\nself.data_parallel_device_ids = parse_gpu_ids(self.gpus)\nself.root_gpu = determine_root_gpu_device(self.data_parallel_device_ids)\n+        self.root_device = torch.device(\"cpu\")\n\n# tpu state flags\nself.use_tpu = False\n", "fix_pattern": "<condition>: The condition is when the variable \"use_tpu\" is set to False. \n<pattern>: The pattern is the incorrect assignment of the \"root_gpu\" variable. \n<code_one>: The code that needs to be removed is the assignment of \"root_gpu\" variable. \n<code_two>: The code that needs to be added is the correct assignment of \"root_device\" variable to \"torch.device(\"cpu\")\". \nFix_pattern: In the condition where \"use_tpu\" is set to False, remove the assignment of \"root_gpu\" and add the assignment of \"root_device\" to \"torch.device(\"cpu\")\" to fix the API misuse."}
{"number": 390, "change": "def _calculate_expected_result(\naggregation_op_only_probs = gumbel_dist.sample()\nelse:\n# <float32>[batch_size, num_aggregation_labels - 1]\n-        aggregation_op_only_probs = torch.nn.functional.softmax(\n+        aggregation_op_only_probs = nn.functional.softmax(\nlogits_aggregation[:, 1:] / config.aggregation_temperature, dim=-1\n)\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: The pattern detected is the change of torch.nn.functional to nn.functional.\n<code_one>: The code removed is \"torch.nn.functional.softmax(\".\n<code_two>: The code added is \"nn.functional.softmax(\".\nFix_pattern: In the condition of no clear condition, if the pattern of changing \"torch.nn.functional.softmax(\" to \"nn.functional.softmax(\" is detected, then remove \"torch.nn.functional.softmax(\" and add \"nn.functional.softmax(\" to fix the API misuse."}
{"number": 394, "change": "if torch_available and torch.cuda.is_available():\nif rocm_major <= 4:\ncupy = f\"cupy-rocm-{rocm_major}-{rocm_minor}\"\nelse:\n-        cupy = f\"cupy-cuda{torch.version.cuda.replace('.','')[:3]}\"\n+        cupy = f\"cupy-cuda{''.join(map(str,installed_cuda_version()))}\"\nif cupy:\nextras_require['1bit'].append(cupy)\nextras_require['1bit_mpi'].append(cupy)\n", "fix_pattern": "<condition>:\nThe condition is if `torch_available` is True and `torch.cuda.is_available()` is also True.\n\n<pattern>:\nThe pattern is the incorrect assignment of `cupy` using string interpolation to specify the CUDA version.\n\n<code_one>:\nThe code that was removed is `cupy = f\"cupy-cuda{torch.version.cuda.replace('.','')[:3]}\"`.\n\n<code_two>:\nThe code that was added is `cupy = f\"cupy-cuda{''.join(map(str,installed_cuda_version()))}\"`.\n\nFix_pattern:\nIn the condition of `if torch_available and torch.cuda.is_available():`, if the incorrect pattern of assigning `cupy` using string interpolation to specify the CUDA version is detected, then remove the code `cupy = f\"cupy-cuda{torch.version.cuda.replace('.','')[:3]}\"` and replace it with `cupy = f\"cupy-cuda{''.join(map(str,installed_cuda_version()))}\"` to fix the API misuse."}
{"number": 397, "change": "def spatial_soft_argmax2d(\n>>> coords = kornia.spatial_soft_argmax2d(input, False)\ntensor([[[1.0000, 1.0000]]])\n\"\"\"\n-    input_soft: torch.Tensor = dsnt.spatial_softmax_2d(input, temperature)\n-    output: torch.Tensor = dsnt.spatial_softargmax_2d(input_soft,\n-                                                      normalized_coordinates)\n+    input_soft: torch.Tensor = spatial_softmax_2d(input, temperature)\n+    output: torch.Tensor = spatial_softargmax_2d(input_soft,\n+                                                 normalized_coordinates)\nreturn output\n", "fix_pattern": "<condition>: The code is calling a function from the \"dsnt\" module.\n<pattern>: The code is using the functions \"spatial_softmax_2d\" and \"spatial_softargmax_2d\" from the \"dsnt\" module.\n<code_one>: \"dsnt.spatial_softmax_2d(input, temperature)\" and \"dsnt.spatial_softargmax_2d(input_soft, normalized_coordinates)\"\n<code_two>: \"spatial_softmax_2d(input, temperature)\" and \"spatial_softargmax_2d(input_soft, normalized_coordinates)\"\nFix_pattern: In the condition of calling functions from the \"dsnt\" module, if using \"spatial_softmax_2d\" and \"spatial_softargmax_2d\" functions, remove \"dsnt.\" from the function calls and replace it with the module name as \"spatial_softmax_2d\" and \"spatial_softargmax_2d\" to fix the API misuse."}
{"number": 401, "change": "def test_torch_instance_to(\nfrontend,\n):\ninput_dtype, x, method_num_positional_args, method_all_as_kwargs_np = args_kwargs\n+    method_flags.num_positional_args = method_num_positional_args\nhelpers.test_frontend_method(\ninit_input_dtypes=input_dtype,\ninit_all_as_kwargs_np={\n", "fix_pattern": "<condition>: The condition is not mentioned in the given context.\n<pattern>: The pattern is not mentioned in the given context.\n<code_one>: The code that was removed is not mentioned in the given context.\n<code_two>: The code that was added is mentioned as \"method_flags.num_positional_args = method_num_positional_args\".\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then change the <code_one> to <code_two> to fix the API misuse."}
{"number": 407, "change": "class UnigramRecall(Metric):\nA tensor of predictions of shape (batch_size, k, sequence_length).\ngold_labels : `torch.Tensor`, required.\nA tensor of integer class label of shape (batch_size, sequence_length).\n-        mask : `torch.Tensor`, optional (default = None).\n+        mask : `torch.BoolTensor`, optional (default = None).\nA masking tensor the same size as `gold_labels`.\n\"\"\"\npredictions, gold_labels, mask = self.detach_tensors(predictions, gold_labels, mask)\n", "fix_pattern": "<condition>: Class UnigramRecall in the context section.\n<pattern>: The mask parameter is changed from `torch.Tensor` to `torch.BoolTensor`.\n<code_one>: `mask : `torch.Tensor`, optional (default = None).`\n<code_two>: `mask : `torch.BoolTensor`, optional (default = None).`\nFix_pattern: In the condition of the UnigramRecall class, if the `torch.Tensor` pattern is detected in the `mask` parameter, then change the `mask` parameter from `torch.Tensor` to `torch.BoolTensor` to fix the API misuse."}
{"number": 408, "change": "\"        # compute the gating function and one minus the gating function\\n\",\n\"        gate_intermediate = self.relu(self.lin_gate_z_to_hidden(z_t_1))\\n\",\n\"        gate = self.sigmoid(self.lin_gate_hidden_to_z(gate_intermediate))\\n\",\n-    \"        one_minus_gate = ng_ones(gate.size()).type_as(gate) - gate\\n\",\n+    \"        one_minus_gate = torch.ones(gate.size()).type_as(gate) - gate\\n\",\n\"        # compute the 'proposed mean'\\n\",\n\"        proposed_mean_intermediate = self.relu(self.lin_proposed_mean_z_to_hidden(z_t_1))\\n\",\n\"        proposed_mean = self.lin_proposed_mean_hidden_to_z(proposed_mean_intermediate)\\n\",\n", "fix_pattern": "<condition>: The condition is not specified in the context provided.\n\n<pattern>: The pattern is the replacement of a specific line of code.\n\n<code_one>: The code that needs to be removed as part of the fix.\n\n<code_two>: The code that needs to be added/changed as part of the fix.\n\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then remove <code_one> and add/change <code_two> to fix the API misuse."}
{"number": 410, "change": "def rotation_matrix_to_quaternion(\nreturn torch.cat([qx, qy, qz, qw], dim=-1)\n\ndef cond_3():\n-        sq = torch.sqrt(trace + 1.0 + m00 - m11 - m22) * 2.  # sq = 4 * qw.\n+        sq = torch.sqrt(1.0 + m00 - m11 - m22 + eps) * 2.  # sq = 4 * qw.\nqw = safe_zero_division(m10 - m01, sq)\nqx = safe_zero_division(m02 - m20, sq)\nqy = safe_zero_division(m12 - m21, sq)\n", "fix_pattern": "<condition>: The condition is not clear from the given code snippet.\n<pattern>: The pattern is detecting the calculation of `sq` using the formula `torch.sqrt(trace + 1.0 + m00 - m11 - m22) * 2.`.\n<code_one>: The code being removed is `sq = torch.sqrt(trace + 1.0 + m00 - m11 - m22) * 2.`.\n<code_two>: The code being added is `sq = torch.sqrt(1.0 + m00 - m11 - m22 + eps) * 2.`.\nFix_pattern: In the condition where the calculation of `sq` is detected using the formula `torch.sqrt(trace + 1.0 + m00 - m11 - m22) * 2.`, the fix is to remove the code `sq = torch.sqrt(trace + 1.0 + m00 - m11 - m22) * 2.` and add the code `sq = torch.sqrt(1.0 + m00 - m11 - m22 + eps) * 2.` to fix the API misuse."}
{"number": 417, "change": "class Model(ModelDesc):\nif get_current_tower_context().is_training:\nwd_w = tf.train.exponential_decay(2e-4, get_global_step_var(),\n80000, 0.7, True)\n-            wd_cost = tf.mul(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')\n+            wd_cost = tf.multiply(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')\ncosts.append(wd_cost)\n\nadd_param_summary(('.*/W', ['histogram']))   # monitor W\n", "fix_pattern": "<condition>: `is_training` is True\n<pattern>: The code is multiplying two tensors using the `mul` function.\n<code_one>: `tf.mul(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')`\n<code_two>: `tf.multiply(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')`\nFix_pattern: In the condition of `is_training` being True, if the code is using `tf.mul` to multiply two tensors, then replace it with `tf.multiply` to fix the API misuse."}
{"number": 418, "change": "if __name__ == '__main__':\nloss_values.clear()\naccuracies.clear()\nif step % 100 == 0:\n-            vis.draw_projections(embeds.detach().cpu().numpy(), utterances_per_speaker, step)\n+            vis.draw_projections(embeds.detach().numpy(), utterances_per_speaker, step)\n", "fix_pattern": "<condition>:\nNone\n\n<pattern>:\nUsing a method or function that is not compatible with the API being used.\n\n<code_one>:\nCode that uses the incompatible method or function.\n\n<code_two>:\nCode that replaces the incompatible method or function with a compatible one.\n\nFix_pattern:\nIn the condition of no pre condition is needed, if a pattern of using an incompatible method or function is detected, then change the code that uses the incompatible method or function to use a compatible one to fix the API misuse."}
{"number": 419, "change": "class ScoreSdeVpScheduler(SchedulerMixin, ConfigMixin):\nx_mean = x + drift * dt\n\n# add noise\n-        noise = torch.randn(x.shape, layout=x.layout, generator=generator).to(x.device)\n+        noise = randn_tensor(x.shape, layout=x.layout, generator=generator, device=x.device, dtype=x.dtype)\nx = x_mean + diffusion * math.sqrt(-dt) * noise\n\nreturn x, x_mean\n", "fix_pattern": "<condition>: No pre condition is needed.\n<pattern>: Changing the torch.randn() function call to a custom randn_tensor() function call.\n<code_one>: `noise = torch.randn(x.shape, layout=x.layout, generator=generator).to(x.device)`\n<code_two>: `noise = randn_tensor(x.shape, layout=x.layout, generator=generator, device=x.device, dtype=x.dtype)`\nFix_pattern: In the condition of not having a pre condition, if the pattern of using torch.randn() is detected, then change the code from `noise = torch.randn(x.shape, layout=x.layout, generator=generator).to(x.device)` to `noise = randn_tensor(x.shape, layout=x.layout, generator=generator, device=x.device, dtype=x.dtype)` to fix the API misuse."}
{"number": 422, "change": "def get_keras_model():\nM.add(KL.Conv2D(32, 3, padding='same', activation='relu'))\nM.add(KL.Flatten())\nM.add(KL.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-5)))\n-        M.add(KL.Dropout(0.5))\n+        M.add(KL.Dropout(rate=0.5))\nM.add(KL.Dense(10, activation=None, kernel_regularizer=keras.regularizers.l2(1e-5)))\nreturn M\n", "fix_pattern": "<condition>: Using the Keras library to create a neural network model.\n<pattern>: Incorrect usage of the Dropout layer in the model.\n<code_one>: M.add(KL.Dropout(0.5))\n<code_two>: M.add(KL.Dropout(rate=0.5))\nFix_pattern: In the condition of using the Keras library to create a neural network model, if the pattern of M.add(KL.Dropout(0.5)) is detected, then change the code to M.add(KL.Dropout(rate=0.5)) to fix the API misuse."}
{"number": 424, "change": "def main(parsed_args):\n\ndef cli_main():\nparser = options.get_eval_lm_parser()\n+    add_distributed_training_args(parser)\nargs = options.parse_args_and_arch(parser)\n-    main(args)\n+    distributed_utils.call_main(args, main)\n\n\nif __name__ == '__main__':\n", "fix_pattern": "<condition>: The code is checking if the current module is being executed directly.\n\n<pattern>: The pattern is to replace the function call to `main(args)` with a call to `distributed_utils.call_main(args, main)`.\n\n<code_one>: `main(args)`\n\n<code_two>: `distributed_utils.call_main(args, main)`\n\nFix_pattern: In the condition of checking if the current module is being executed directly, if a call to `main(args)` is detected, then replace it with a call to `distributed_utils.call_main(args, main)` to fix the API misuse."}
{"number": 425, "change": "def degree(index, num_nodes=None, dtype=None, device=None):\ntensor([3., 1., 1.])\n\"\"\"\nnum_nodes = maybe_num_nodes(index, num_nodes)\n-    out = torch.zeros((num_nodes), dtype=dtype, device=device)\n+    out = torch.zeros((num_nodes), dtype=dtype, device=index.device)\nreturn out.scatter_add_(0, index, out.new_ones((index.size(0))))\n", "fix_pattern": "<condition>: Indexing operation on a tensor or array\n<pattern>: Creating a new tensor with the specified size and data type\n<code_one>: out = torch.zeros((num_nodes), dtype=dtype, device=device)\n<code_two>: out = torch.zeros((num_nodes), dtype=dtype, device=index.device)\nFix_pattern: In the condition of indexing operation on a tensor or array, if a new tensor is created using torch.zeros() with the specified size, data type, and device, then replace the original device with the device attribute of the index tensor to fix the API misuse."}
{"number": 426, "change": "class TransducerTasks(torch.nn.Module):\nif ctc_loss:\nself.ctc_lin = torch.nn.Linear(encoder_dim, output_dim)\n\n-            if LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\"):\n+            if LooseVersion(torch.__version__) > LooseVersion(\"1.0.1\"):\nself.ctc_loss = torch.nn.CTCLoss(\nblank=blank_id,\nreduction=\"sum\",\n", "fix_pattern": "<condition>: `LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\")`\n<pattern>: If the version of `torch` is lower than \"1.7.0\"\n<code_one>: The condition check `if LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\"):`\n<code_two>: The version check `if LooseVersion(torch.__version__) > LooseVersion(\"1.0.1\"):`\nFix_pattern: In the condition of `LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\")`, if the version of `torch` is lower than \"1.7.0\", then change the condition check from `if LooseVersion(torch.__version__) < LooseVersion(\"1.7.0\"):`, to `if LooseVersion(torch.__version__) > LooseVersion(\"1.0.1\"):` to fix the API misuse."}
{"number": 429, "change": "class RandomThinPlateSpline(AugmentationBase2D):\n\ndef generate_parameters(self, shape: torch.Size) -> Dict[str, Tensor]:\nB, _, _, _ = shape\n-        src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2\n+        src = tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2\ndst = src + self.dist.rsample(src.shape)\nreturn dict(src=src, dst=dst)\n", "fix_pattern": "<condition>: No pre condition is needed\n<pattern>: The code was using the torch module to create a tensor\n<code_one>: src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2\n<code_two>: src = tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2\nFix_pattern: In the condition of no pre condition needed, if the pattern of using the torch module to create a tensor is detected, then remove/replace the code using torch module and use the tensor module instead."}
{"number": 433, "change": "def HomographyRegressionApp():\n[-1, 1],  # top-right\n]]).to(dst_homo_src.device)\n# transform points\n-            pts_dst = dgm.transform_points(dgm.inverse(dst_homo_src), pts_src)\n+            pts_dst = dgm.transform_points(torch.inverse(dst_homo_src), pts_src)\n\ndef compute_factor(size):\nreturn 1.0 * size / 2\n", "fix_pattern": "<condition>: There is an API misuse in the code.\n<pattern>: The code is using the method \"dgm.transform_points\" with the incorrect argument \"dgm.inverse(dst_homo_src)\".\n<code_one>: pts_dst = dgm.transform_points(dgm.inverse(dst_homo_src), pts_src)\n<code_two>: pts_dst = dgm.transform_points(torch.inverse(dst_homo_src), pts_src)\nFix_pattern: In the condition of API misuse, if the pattern \"dgm.inverse(dst_homo_src)\" is detected, then change the code \"pts_dst = dgm.transform_points(dgm.inverse(dst_homo_src), pts_src)\" to \"pts_dst = dgm.transform_points(torch.inverse(dst_homo_src), pts_src)\" to fix the API misuse."}
{"number": 436, "change": "class Highway(torch.nn.Module):\n# above, too.\nnonlinear_part, gate = projected_input.chunk(2, dim=-1)\nnonlinear_part = self._activation(nonlinear_part)\n-            gate = torch.nn.functional.sigmoid(gate)\n+            gate = torch.sigmoid(gate)\ncurrent_input = gate * linear_part + (1 - gate) * nonlinear_part\nreturn current_input\n", "fix_pattern": "<condition>: There is a usage of the torch.nn.functional.sigmoid() function.\n<pattern>: The torch.nn.functional.sigmoid() function is replaced with the torch.sigmoid() function.\n<code_one>: gate = torch.nn.functional.sigmoid(gate)\n<code_two>: gate = torch.sigmoid(gate)\nFix_pattern: In the condition of using the torch.nn.functional.sigmoid() function, if detected, then change the code from gate = torch.nn.functional.sigmoid(gate) to gate = torch.sigmoid(gate) to fix the API misuse."}
{"number": 437, "change": "class Model(object):\n\"It should be either Tensor or a list of Tensor.\"\n)\nfor idx in range(len(check_argu)):\n-                        if not isinstance(check_argu[idx], tf_ops._TensorLike) or not tf_ops.is_dense_tensor_like(\n+                        if not isinstance(check_argu[idx], [tf.Tensor, tf.SparseTensor, tf.Variable]) or not tf_ops.is_dense_tensor_like(\ncheck_argu[idx]):\nraise TypeError(\n\"The argument `%s` should be either Tensor or a list of Tensor \" % (check_order[co]) +\n", "fix_pattern": "<condition>: The condition is that the argument 'check_argu' should be either of type 'Tensor' or a list of 'Tensor'.\n<pattern>: The pattern detected is that the check for 'Tensor' type using 'isinstance' function and 'is_dense_tensor_like' function is being performed.\n<code_one>: The code being removed is: \n'''\nif not isinstance(check_argu[idx], tf_ops._TensorLike) or not tf_ops.is_dense_tensor_like(\n'''\n<code_two>: The code being added is:\n'''\nif not isinstance(check_argu[idx], [tf.Tensor, tf.SparseTensor, tf.Variable]) or not tf_ops.is_dense_tensor_like(\n'''\nFix_pattern: In the condition of 'check_argu' being either 'Tensor' or a list of 'Tensor', if the pattern of checking for 'Tensor' type and 'is_dense_tensor_like' function is detected, then the fix is to remove the code checking for 'Tensor' type using 'isinstance' function and replace it with code checking for 'Tensor', 'SparseTensor', or 'Variable' types using 'isinstance' function, along with the 'is_dense_tensor_like' function."}
{"number": 446, "change": "class Csv(datasets.ArrowBasedBuilder):\nif schema is not None\nelse None\n)\n-        for file_idx, file in enumerate(files):\n+        for file_idx, file in enumerate(itertools.chain.from_iterable(files)):\ncsv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)\ntry:\nfor batch_idx, df in enumerate(csv_file_reader):\n", "fix_pattern": "<condition>: The condition is when the code needs to iterate over a list or iterator.\n<pattern>: The pattern is when the code uses a for loop with an enumerate function to iterate over a list.\n<code_one>: The code that needs to be removed is \"for file_idx, file in enumerate(files):\"\n<code_two>: The code that needs to be added is \"for file_idx, file in enumerate(itertools.chain.from_iterable(files)):\"\nFix_pattern: In the condition of needing to iterate over a list, if the pattern of using a for loop with an enumerate function is detected, then remove the code \"for file_idx, file in enumerate(files):\" and replace it with \"for file_idx, file in enumerate(itertools.chain.from_iterable(files))\" to fix the API misuse."}
{"number": 448, "change": "class ARMAConv(MessagePassing):\nif self.bias is not None:\nout += self.bias[0 if self.shared_weights else t]\n\n-            if t < self.num_layers - 1:\n+            if self.act is not None and t < self.num_layers - 1:\nout = self.act(out)\n\nreturn out.mean(dim=-3)\n", "fix_pattern": "<condition>: `self.bias is not None`\n\n<pattern>: `if t < self.num_layers - 1:`\n\n<code_one>: `if t < self.num_layers - 1:` (removed from the code)\n\n<code_two>: `if self.act is not None and t < self.num_layers - 1:` (added to the code)\n\nFix_pattern: In the condition of `self.bias is not None`, if `t < self.num_layers - 1` is detected, then remove the `if t < self.num_layers - 1:` condition and add `if self.act is not None and t < self.num_layers - 1:` to fix the API misuse."}
{"number": 449, "change": "class DependencyParser(flair.nn.Model):\nsentence_tensor = self.word_dropout(sentence_tensor)\n\nif self.use_rnn:\n-            sentence_tensor = pack_padded_sequence(sentence_tensor, lengths, True, False)\n+            sentence_sequence = pack_padded_sequence(sentence_tensor, torch.IntTensor(lengths), True, False)\n\n-            sentence_tensor, _ = self.lstm(sentence_tensor)\n-            sentence_tensor, _ = pad_packed_sequence(sentence_tensor, True, total_length=seq_len)\n+            sentence_sequence, _ = self.lstm(sentence_sequence)\n+            sentence_tensor, _ = pad_packed_sequence(sentence_sequence, True, total_length=seq_len)\n\n# apply MLPs for arc and relations to the BiLSTM output states\narc_h = self.mlp_arc_h(sentence_tensor)\n", "fix_pattern": "<condition>: self.use_rnn is True\n<pattern>: sentence_tensor is being packed, passed through an LSTM, and then unpacked\n<code_one>: \n```\nsentence_tensor = pack_padded_sequence(sentence_tensor, lengths, True, False)\nsentence_tensor, _ = self.lstm(sentence_tensor)\nsentence_tensor, _ = pad_packed_sequence(sentence_tensor, True, total_length=seq_len)\n```\n<code_two>: \n```\nsentence_sequence = pack_padded_sequence(sentence_tensor, torch.IntTensor(lengths), True, False)\nsentence_sequence, _ = self.lstm(sentence_sequence)\nsentence_tensor, _ = pad_packed_sequence(sentence_sequence, True, total_length=seq_len)\n```\nFix_pattern: In the condition of self.use_rnn being True, if the pattern of packing, passing through an LSTM, and unpacking sentence_tensor is detected, then change the code block to use sentence_sequence instead of sentence_tensor in order to fix the API misuse."}
{"number": 451, "change": "def testtanh():\n\nPtensor = PolynomialTensor()\n\n-    x = torch.linspace(-3, 3, steps=10)\n+    x = torch.tensor(np.linspace(-3, 3, 10))\nexpected = torch.tensor(\n[\n-3.3883e02,\n", "fix_pattern": "<condition>: API misuse\n<pattern>: <code_one> is replaced by <code_two>\n<code_one>: x = torch.linspace(-3, 3, steps=10)\n<code_two>: x = torch.tensor(np.linspace(-3, 3, 10))\nFix_pattern: In the condition of API misuse, if the code with <code_one> is detected, then replace <code_one> with <code_two> to fix the API misuse."}
{"number": 452, "change": "class BartTranslationTests(unittest.TestCase):\nwith torch.no_grad():\nlogits, *other_stuff = model(**self.net_input)\n\n-        expected_slice = torch.tensor([9.0078, 10.1113, 14.4787])\n+        expected_slice = torch.tensor([9.0078, 10.1113, 14.4787], device=torch_device)\nresult_slice = logits[0][0][:3]\nself.assertTrue(torch.allclose(expected_slice, result_slice, atol=TOLERANCE))\n", "fix_pattern": "<condition>: The code is using torch.tensor without specifying the device.\n<pattern>: The code needs to specify the device for the torch.tensor object.\n<code_one>: The original code did not specify the device for the torch.tensor object.\n<code_two>: The fixed code added the device specification for the torch.tensor object.\nFix_pattern: In the condition of not specifying the device for a torch.tensor object, if <pattern> is detected, then change the <code_one> to <code_two> to fix the API misuse."}
{"number": 453, "change": "def test_dc_crn_separator_invalid_type():\ndef test_dc_crn_separator_output():\nreal = torch.rand(2, 10, 17)\nimag = torch.rand(2, 10, 17)\n-    x = ComplexTensor(real, imag) if is_torch_1_9_plus else torch.complex(real, imag)\n+    x = torch.complex(real, imag) if is_torch_1_9_plus else ComplexTensor(real, imag)\nx_lens = torch.tensor([10, 8], dtype=torch.long)\n\nfor num_spk in range(1, 3):\n", "fix_pattern": "<condition>: No pre condition is needed.\n<pattern>: API misuse of creating a complex tensor.\n<code_one>: ComplexTensor(real, imag) if is_torch_1_9_plus else torch.complex(real, imag)\n<code_two>: torch.complex(real, imag) if is_torch_1_9_plus else ComplexTensor(real, imag)\nFix_pattern: In the condition of API misuse of creating a complex tensor, if ComplexTensor is detected, then remove ComplexTensor(real, imag) and add torch.complex(real, imag) instead to fix the API misuse."}
{"number": 457, "change": "def multilevel_roi_align(features, rcnn_boxes, resolution):\nall_rois = tf.concat(all_rois, axis=0)  # NCHW\n# Unshuffle to the original order, to match the original samples\nlevel_id_perm = tf.concat(level_ids, axis=0)  # A permutation of 1~N\n-    level_id_invert_perm = tf.invert_permutation(level_id_perm)\n+    level_id_invert_perm = tf.math.invert_permutation(level_id_perm)\nall_rois = tf.gather(all_rois, level_id_invert_perm, name=\"output\")\nreturn all_rois\n", "fix_pattern": "Condition: No clear condition is needed.\nPattern: The pattern is a function call that needs to be changed.\nCode One: `tf.invert_permutation(level_id_perm)`\nCode Two: `tf.math.invert_permutation(level_id_perm)`\nFix Pattern: In the condition, if the pattern `tf.invert_permutation(level_id_perm)` is detected, then change the code from `tf.invert_permutation(level_id_perm)` to `tf.math.invert_permutation(level_id_perm)` to fix the API misuse."}
{"number": 458, "change": "def SoftMax(x, use_temperature=False, temperature_init=1.0):\n:param x: a 2D tensor\n\"\"\"\nif use_temperature:\n-        t = tf.get_variable('temp', [1],\n+        t = tf.get_variable('invtemp', [],\ninitializer=tf.constant_initializer(1.0 / float(temperature_init)))\nx = x * t\nreturn tf.nn.softmax(x, name='output')\n", "fix_pattern": "<condition>: The condition is 'use_temperature' is True.\n<pattern>: The pattern is 'tf.get_variable('temp', [1],'\n<code_one>: The code_one is 't = tf.get_variable('temp', [1],'\n<code_two>: The code_two is 't = tf.get_variable('invtemp', [],'\nFix_pattern: In the condition of 'use_temperature' being True, if the pattern 'tf.get_variable('temp', [1],' is detected, then remove the code 't = tf.get_variable('temp', [1],' and add the code 't = tf.get_variable('invtemp', [],' to fix the API misuse."}
{"number": 459, "change": "def _preprocess_deconv_output_shape(x, shape, dim_ordering):\nshape = (shape[0], shape[2], shape[3], shape[1])\n\nif shape[0] is None:\n-        shape = (tf.shape(x)[0], ) + shape[1:]\n+        shape = (tf.shape(x)[0], ) + tuple(shape[1:])\nreturn shape\n", "fix_pattern": "<condition>: If the value of shape[0] is None.\n<pattern>: Modifying the shape tuple.\n<code_one>: shape = (tf.shape(x)[0], ) + shape[1:]\n<code_two>: shape = (tf.shape(x)[0], ) + tuple(shape[1:])\nFix_pattern: In the condition where shape[0] is None, the fix is to modify the shape tuple by adding the first element and converting it to a tuple."}
{"number": 462, "change": "class RGCNConv(MessagePassing):\nreturn out if edge_norm is None else out * edge_norm.view(-1, 1)\n\ndef update(self, aggr_out, x):\n-        if x.dtype == torch.long:\n+        if x is None:\nout = aggr_out + self.root\nelse:\nout = aggr_out + torch.matmul(x, self.root)\n", "fix_pattern": "<condition>: No pre condition is needed\n<pattern>: Remove condition and add a new condition\n<code_one>: if x.dtype == torch.long\n<code_two>: if x is None\nFix_pattern: In the condition of no pre condition is needed, if the condition of `x.dtype == torch.long` is detected, then remove the condition `x.dtype == torch.long` and add the condition `if x is None` to fix the API misuse."}
{"number": 470, "change": "class DenseGCNConv(torch.nn.Module):\nidx = torch.arange(N, dtype=torch.long, device=adj.device)\nadj[:, idx, idx] = 1 if not self.improved else 2\n\n-        out = self.lin(x)\n+        out = torch.matmul(x, self.weight)\ndeg_inv_sqrt = adj.sum(dim=-1).clamp(min=1).pow(-0.5)\n\nadj = deg_inv_sqrt.unsqueeze(-1) * adj * deg_inv_sqrt.unsqueeze(-2)\n", "fix_pattern": "<condition>: The condition is not clearly given in the context section.\n\n<pattern>: The pattern is not clearly given in the code removed section.\n\n<code_one>: The code that was removed is \"out = self.lin(x)\".\n\n<code_two>: The code that was added is \"out = torch.matmul(x, self.weight)\".\n\nFix_pattern: In the condition of no clear condition, if the code \"out = self.lin(x)\" is detected, then change it to \"out = torch.matmul(x, self.weight)\" to fix the API misuse."}
{"number": 472, "change": "class GradientsTest(tf.test.TestCase):\n\n\nif __name__ == \"__main__\":\n-  tf.test.main()\n+  if tf.__internal__.tf2.enabled():\n+    tf.test.main()\n", "fix_pattern": "<condition>: Execution of tf.test.main() is needed.\n<pattern>: tf.__internal__.tf2.enabled() condition is added to the code.\n<code_one>: tf.test.main()\n<code_two>: if tf.__internal__.tf2.enabled(): tf.test.main()\nFix_pattern: In the condition of needing to execute tf.test.main(), the fix pattern is to add the condition if tf.__internal__.tf2.enabled() and keep the code tf.test.main()."}
{"number": 473, "change": "class Model(ModelDesc):\ncost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\ncost = tf.reduce_mean(cost, name='cross_entropy_loss')\n\n-        acc = tf.to_float(tf.nn.in_top_k(logits, label, 1))\n+        acc = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32)\n\nacc = tf.reduce_mean(acc, name='accuracy')\nsummary.add_moving_summary(acc)\n", "fix_pattern": "<condition>: When calculating the accuracy of a model's prediction using the `in_top_k` function\n<pattern>: The `tf.to_float()` function is used to convert the accuracy calculation result to a float value\n<code_one>: `acc = tf.to_float(tf.nn.in_top_k(logits, label, 1))`\n<code_two>: `acc = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32)`\nFix_pattern: In the condition of calculating accuracy using `in_top_k`, if `tf.to_float()` is detected, then change `tf.to_float()` to `tf.cast(..., tf.float32)` to fix the API misuse."}
{"number": 476, "change": "class TFCoreModelTesterMixin:\n\nself.assertIsNotNone(outputs)\n\n-        tf.keras.mixed_precision.experimental.set_policy(\"float32\")\n+        tf.keras.mixed_precision.set_global_policy(\"float32\")\n\n@slow\ndef test_train_pipeline_custom_model(self):\n", "fix_pattern": "Condition: The code is testing a train pipeline for a custom model.\nPattern: A method or function call related to TensorFlow's mixed precision policy is being used.\nCode one: tf.keras.mixed_precision.experimental.set_policy(\"float32\")\nCode two: tf.keras.mixed_precision.set_global_policy(\"float32\")\nFix pattern: In the condition of testing the train pipeline for a custom model, if a method call to set the mixed precision policy is detected, replace tf.keras.mixed_precision.experimental.set_policy(\"float32\") with tf.keras.mixed_precision.set_global_policy(\"float32\") to fix the API misuse."}
{"number": 479, "change": "class TFMBartForConditionalGeneration(TFMBartPreTrainedModel, TFCausalLanguageMo\nif labels is not None:\nlabels = tf.where(\nlabels == self.config.pad_token_id,\n-                tf.fill(shape_list(labels), -100),\n+                tf.cast(tf.fill(shape_list(labels), -100), labels.dtype),\nlabels,\n)\nuse_cache = False\n", "fix_pattern": "<condition>: Checking if labels is not None.\n<pattern>: Replacing tf.fill(shape_list(labels), -100) with tf.cast(tf.fill(shape_list(labels), -100), labels.dtype)\n<code_one>: tf.fill(shape_list(labels), -100)\n<code_two>: tf.cast(tf.fill(shape_list(labels), -100), labels.dtype)\nFix_pattern: In the condition of labels is not None, if tf.fill(shape_list(labels), -100) is detected, then change the tf.fill(shape_list(labels), -100) to tf.cast(tf.fill(shape_list(labels), -100), labels.dtype) to fix the API misuse."}
{"number": 481, "change": "def main():\n# Setup CUDA, GPU & distributed training\nif args.local_rank == -1 or args.no_cuda:\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n-        args.n_gpu = torch.cuda.device_count()\n+        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\nelse:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\ntorch.cuda.set_device(args.local_rank)\ndevice = torch.device(\"cuda\", args.local_rank)\n", "fix_pattern": "<condition>: args.local_rank == -1 or args.no_cuda\n<pattern>: args.n_gpu = torch.cuda.device_count()\n<code_one>: args.n_gpu = torch.cuda.device_count()\n<code_two>: args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\nFix_pattern: in the condition of args.local_rank == -1 or args.no_cuda, if args.n_gpu = torch.cuda.device_count() is detected, then change the args.n_gpu to args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count() to fix the API misuse."}
{"number": 482, "change": "class Pandas(datasets.ArrowBasedBuilder):\nreturn pa_table\n\ndef _generate_tables(self, files):\n-        for i, file in enumerate(files):\n+        for i, file in enumerate(itertools.chain.from_iterable(files)):\nwith open(file, \"rb\") as f:\npa_table = pa.Table.from_pandas(pd.read_pickle(f))\nyield i, self._cast_table(pa_table)\n", "fix_pattern": "<condition>: In the given context, there is no clear condition mentioned.\n<pattern>: The pattern detected is a change in the iteration logic from `for i, file in enumerate(files):` to `for i, file in enumerate(itertools.chain.from_iterable(files)):`.\n<code_one>: The code to be removed is `for i, file in enumerate(files):`.\n<code_two>: The code to be added is `for i, file in enumerate(itertools.chain.from_iterable(files)):`.\n\nFix_pattern: In the condition of iterating over `files`, if the pattern of iterating over the flattened `files` using `itertools.chain.from_iterable()` is detected, then remove the code `for i, file in enumerate(files):` and add the code `for i, file in enumerate(itertools.chain.from_iterable(files)):`. This fix is done to fix the API misuse."}
{"number": 483, "change": "class DiceLoss(nn.Module):\ncardinality = torch.sum(input_soft + target_one_hot, dims)\n\ndice_score = 2. * intersection / (cardinality + self.eps)\n-        return torch.mean(1. - dice_score)\n+        return torch.mean(torch.tensor(1.) - dice_score)\n\n\n######################\n", "fix_pattern": "<condition>: The condition is that the variable `dice_score` is being computed using certain calculations.\n<pattern>: The pattern is that the value of `dice_score` is being used in an incorrect way.\n<code_one>: The code that was removed is `return torch.mean(1. - dice_score)`.\n<code_two>: The code that was added is `return torch.mean(torch.tensor(1.) - dice_score)`.\nFix_pattern: In the condition of computing `dice_score`, if the incorrect usage of `dice_score` is detected, then change the code from `return torch.mean(1. - dice_score)` to `return torch.mean(torch.tensor(1.) - dice_score)` to fix the API misuse."}
{"number": 488, "change": "class SageMakerTrainingArguments(TrainingArguments):\n# Here, we'll use torch.distributed.\n# Initializes the distributed backend which will take care of synchronizing nodes/GPUs\nif not torch.distributed.is_initialized():\n-                torch.distributed.init_process_group(backend=\"nccl\")\n+                torch.distributed.init_process_group(backend=\"nccl\", timeout=self.ddp_timeout_delta)\ndevice = torch.device(\"cuda\", self.local_rank)\nself._n_gpu = 1\n", "fix_pattern": "<condition>: The condition is that torch.distributed is not initialized.\n<pattern>: The pattern is that torch.distributed is manually initialized with the \"nccl\" backend.\n<code_one>: The code that is removed is torch.distributed.init_process_group(backend=\"nccl\").\n<code_two>: The code that is added is torch.distributed.init_process_group(backend=\"nccl\", timeout=self.ddp_timeout_delta).\nFix_pattern: In the condition of torch.distributed not being initialized, if the pattern of manually initializing torch.distributed with the \"nccl\" backend is detected, then remove the code for initializing torch.distributed and add the code with the additional timeout argument to fix the API misuse."}
{"number": 489, "change": "def main(_):\n# net = tl.layers.ReshapeLayer(net,\n#       shape=[-1, int(net.outputs._shape[-1])], name='reshape')\nnet = tl.layers.DropoutLayer(net, keep=keep_prob, is_fix=True, is_train=is_training, name='drop3')\n-            net = tl.layers.DenseLayer(net, vocab_size, W_init=init, b_init=init, act=tf.identity, name='output')\n+            net = tl.layers.DenseLayer(net, vocab_size, W_init=init, b_init=init, act=None, name='output')\nreturn net, lstm1, lstm2\n\n# Inference for Training\n", "fix_pattern": "<condition>: The condition is not clearly mentioned in the provided code.\n\n<pattern>: In the given code, a specific pattern is not mentioned.\n\n<code_one>: The code that was removed is:\n\n```\nnet = tl.layers.DenseLayer(net, vocab_size, W_init=init, b_init=init, act=tf.identity, name='output')\n```\n\n<code_two>: The code that was added is:\n\n```\nnet = tl.layers.DenseLayer(net, vocab_size, W_init=init, b_init=init, act=None, name='output')\n```\n\nFix_pattern: When refactoring the code, the fix involves removing `act=tf.identity` from the `DenseLayer` initialization and using `act=None` instead."}
{"number": 494, "change": "class DecoderLayer(nn.Module):\nself.sublayer = nn_util.clone(SublayerConnection(size, dropout), 3)\n\ndef forward(\n-        self, x: torch.Tensor, memory: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor\n+        self,\n+        x: torch.Tensor,\n+        memory: torch.Tensor,\n+        src_mask: torch.BoolTensor,\n+        tgt_mask: torch.BoolTensor,\n) -> torch.Tensor:\n# Follow Figure 1 (right) for connections.\nx = self.sublayer[0](x, lambda y: self.self_attn(y, y, y, tgt_mask))\n", "fix_pattern": "<condition>: When defining the forward method in the DecoderLayer class.\n<pattern>: The arguments x, memory, src_mask, and tgt_mask are missing type annotations.\n<code_one>: self, x: torch.Tensor, memory: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor\n<code_two>: self, x: torch.Tensor, memory: torch.Tensor, src_mask: torch.BoolTensor, tgt_mask: torch.BoolTensor\nFix_pattern: In the condition of defining the forward method in the DecoderLayer class, if the arguments x, memory, src_mask, and tgt_mask are missing type annotations, then remove the code \"self, x: torch.Tensor, memory: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor\" and add the code \"self, x: torch.Tensor, memory: torch.Tensor, src_mask: torch.BoolTensor, tgt_mask: torch.BoolTensor\" to fix the API misuse."}
{"number": 497, "change": "class EvalbBracketingScorer(Metric):\nshutil.rmtree(tempdir)\n\nif is_distributed():\n-            # Setting the device to CPU since this metric is not expected to run on GPUs.\n-            device = torch.device(\"cpu\")\n+            device = torch.device(\"cuda\" if dist.get_backend() == \"nccl\" else \"cpu\")\ncorrect_predicted_brackets = torch.tensor(_correct_predicted_brackets).to(device)\npredicted_brackets = torch.tensor(_predicted_brackets).to(device)\ngold_brackets = torch.tensor(_gold_brackets).to(device)\n", "fix_pattern": "<condition>: Code is setting the device to CPU.\n<pattern>: Code needs to determine if the distributed backend is \"nccl\" and set the device accordingly.\n<code_one>: device = torch.device(\"cpu\")\n<code_two>: device = torch.device(\"cuda\" if dist.get_backend() == \"nccl\" else \"cpu\")\nFix_pattern: In the condition of setting the device to CPU, if the distributed backend is \"nccl\", then change the code to set the device to CUDA, otherwise set it to CPU to fix the API misuse."}
{"number": 501, "change": "class E2E(torch.nn.Module):\n# Neither CPUTensor nor float/int value can be used\n# because NCCL communicates between GPU devices.\ndevice = next(self.parameters()).device\n-        acc = torch.tensor([acc], device=device)\n+\n+        acc = torch.tensor([acc], device=device) if acc is not None else None\ncer = torch.tensor([cer], device=device)\nwer = torch.tensor([wer], device=device)\nreturn self.loss, loss_ctc, loss_att, acc, cer, wer\n", "fix_pattern": "<condition>: The condition is that acc can be None.\n<pattern>: The pattern is that acc is assigned a tensor value without checking if it is None.\n<code_one>: The code that is being removed is acc = torch.tensor([acc], device=device)\n<code_two>: The code that is being added is acc = torch.tensor([acc], device=device) if acc is not None else None\nFix_pattern: In the condition of checking if acc is None, if acc is not None, then change the code assignment of acc to acc = torch.tensor([acc], device=device) to fix the API misuse."}
{"number": 502, "change": "class DeepQNetwork(ValueFunction):\n\"\"\"\n\n# Compute estimated future value\n-        float_terminals = tf.to_float(batch['terminals'])\n+        float_terminals = batch['terminals'].astype(float)\nq_targets = batch['rewards'] + (1. - float_terminals) \\\n* self.gamma * self.get_target_values(batch['next_states'])\n", "fix_pattern": "<condition>: The condition is when there is a need to convert a variable from one type to another.\n\n<pattern>: The pattern is using the `tf.to_float()` method to convert a variable to a float type.\n\n<code_one>: The code that is being removed is `float_terminals = tf.to_float(batch['terminals'])`.\n\n<code_two>: The code that is being added is `float_terminals = batch['terminals'].astype(float)`.\n\nFix_pattern: In the condition of needing to convert a variable from one type to another, if the pattern of using `tf.to_float()` is detected, then remove the code `float_terminals = tf.to_float(batch['terminals'])` and add the code `float_terminals = batch['terminals'].astype(float)` to fix the API misuse."}
{"number": 508, "change": "class DistributedFusedAdam(torch.optim.Optimizer):\ngrp = torch.distributed.new_group(ranks=ranks)\nif torch.distributed.get_rank() in ranks:\nself._rs_pg.append(grp)\n-            if self._compute_L2_grad_norm and torch.distributed.get_rank() in ranks:\n-                self._l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)\n-                torch.distributed.all_reduce(self._overflow_buf,group=self._l2_grad_norm_pg)\n+            if self._compute_L2_grad_norm:\n+                l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)\n+                if torch.distributed.get_rank() in ranks:\n+                    self._l2_grad_norm_pg = l2_grad_norm_pg\n+                    torch.distributed.all_reduce(self._overflow_buf,group=self._l2_grad_norm_pg)\nself._rs_st = [torch.cuda.Stream() for _ in range(self._num_rs_pg)]\nfor rs_pg in self._rs_pg:\ntorch.distributed.all_reduce(self._overflow_buf,group=rs_pg)\n", "fix_pattern": "<condition>: self._compute_L2_grad_norm is True\n<pattern>: Checking if torch.distributed.get_rank() is in ranks\n<code_one>: self._l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)\n<code_two>: l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks), \n            self._l2_grad_norm_pg = l2_grad_norm_pg\nFix_pattern: \n\nIn the condition of self._compute_L2_grad_norm is True, if torch.distributed.get_rank() is in ranks, then remove the code \"self._l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)\" and add the code \"l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks), self._l2_grad_norm_pg = l2_grad_norm_pg\" to fix the API misuse."}
{"number": 510, "change": "def cartesian_product_of_parameters(**possible_parameters):\n\n\ndef default_with_one_parameter_changed(*, default={}, **possible_parameters):\n-    assert isinstance(default, dict), f\"default should be a dict not a {type(default)}\"\n+    if not isinstance(default, dict):\n+        raise AssertionError(f\"default should be a dict not a {type(default)}\")\n\nfor parameter_name, possible_values in possible_parameters.items():\nfor v in possible_values:\n", "fix_pattern": "<condition>: The condition is that the variable \"default\" should be a dictionary.\n\n<pattern>: The pattern is that the assert statement is used to check if \"default\" is a dictionary.\n\n<code_one>: The code that is removed is the assert statement.\n\n<code_two>: The code that is added is an if statement to raise an AssertionError if \"default\" is not a dictionary.\n\nFix_pattern: In the condition of the variable \"default\" should be a dictionary. If the variable is not a dictionary, then change the assert statement to an if statement to raise an AssertionError."}
{"number": 516, "change": "class Speech2Text2SinusoidalPositionalEmbedding(nn.Module):\nemb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)\nif padding_idx is not None:\nemb[padding_idx, :] = 0\n-        return emb\n+        return emb.to(torch.get_default_dtype())\n\n@torch.no_grad()\ndef forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0):\n", "fix_pattern": "<condition>: When calling the `forward` method of the `Speech2Text2SinusoidalPositionalEmbedding` class.\n<pattern>: Returning the `emb` variable.\n<code_one>: `return emb`\n<code_two>: `return emb.to(torch.get_default_dtype())`\nFix_pattern: In the condition of calling the `forward` method of the `Speech2Text2SinusoidalPositionalEmbedding` class, if returning the `emb` variable is detected, then change the code from `return emb` to `return emb.to(torch.get_default_dtype())` to fix the API misuse."}
{"number": 518, "change": "class EulerAncestralDiscreteScheduler(SchedulerMixin, ConfigMixin):\n\nprev_sample = sample + derivative * dt\n\n-        device = model_output.device if torch.is_tensor(model_output) else \"cpu\"\n-        if str(device) == \"mps\":\n+        device = model_output.device if torch.is_tensor(model_output) else torch.device(\"cpu\")\n+        if device.type == \"mps\":\n# randn does not work reproducibly on mps\nnoise = torch.randn(model_output.shape, dtype=model_output.dtype, device=\"cpu\", generator=generator).to(\ndevice\n", "fix_pattern": "<condition>: If the device type is \"mps\".\n<pattern>: The condition to check the device type.\n<code_one>: `device = model_output.device if torch.is_tensor(model_output) else \"cpu\"`\n<code_two>: `device = model_output.device if torch.is_tensor(model_output) else torch.device(\"cpu\")`\nFix_pattern: In the condition of checking the device type, if the device type is detected as \"mps\", then change the assignment of `device` from `device = model_output.device if torch.is_tensor(model_output) else \"cpu\"` to `device = model_output.device if torch.is_tensor(model_output) else torch.device(\"cpu\")` to fix the API misuse."}
{"number": 519, "change": "class AutoRegressiveNN(nn.Module):\n\nif permutation is None:\n# By default set a random permutation of variables, which is important for performance with multiple steps\n-            self.permutation = torch.randperm(input_dim)\n+            self.permutation = torch.randperm(input_dim, device='cpu').to(torch.Tensor().device)\nelse:\n# The permutation is chosen by the user\nself.permutation = permutation.type(dtype=torch.int64)\n", "fix_pattern": "<condition>: If permutation is None.\n<pattern>: When permutation is None, the code assigns a random permutation of variables to the \"self.permutation\" variable.\n<code_one>: self.permutation = torch.randperm(input_dim)\n<code_two>: self.permutation = torch.randperm(input_dim, device='cpu').to(torch.Tensor().device)\nFix_pattern: In the condition of permutation being None, the code assigns a random permutation of variables to the \"self.permutation\" variable."}
{"number": 522, "change": "class StableDiffusionInpaintPipeline(DiffusionPipeline):\nelse:\nraise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n-        device = torch.device(\"cuda\")\n+        device = torch.device(f\"cuda:{gpu_id}\")\n\nfor cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.safety_checker]:\nif cpu_offloaded_model is not None:\n", "fix_pattern": "<condition>: The code is checking if a GPU is available.\n<pattern>: The pattern is that the code is using a generic \"cuda\" device.\n<code_one>: The code is setting the device to \"cuda\".\n<code_two>: The code is setting the device to a specific GPU using its ID.\nFix_pattern: In the condition of checking if a GPU is available, if the pattern of using a generic \"cuda\" device is detected, then change the code from setting the device to \"cuda\" to setting the device to a specific GPU using its ID to fix the API misuse."}
{"number": 524, "change": "class TwoStageDetector(BaseDetector, RPNTestMixin, BBoxTestMixin,\nif self.with_rpn:\nrpn_outs = self.rpn_head(x)\nouts = outs + (rpn_outs, )\n-        proposals = torch.randn(1000, 4).cuda()\n+        proposals = torch.randn(1000, 4).to(device=img.device)\n# bbox head\nrois = bbox2roi([proposals])\nif self.with_bbox:\n", "fix_pattern": "<condition>: The code is checking if the object of the class has a bbox/head.\n\n<pattern>: The pattern is detecting if the proposals tensor is being assigned with random values using the cuda() method.\n\n<code_one>: The original code contains the line \"proposals = torch.randn(1000, 4).cuda()\".\n\n<code_two>: The fixed code replaces the \".cuda()\" method with \".to(device=img.device)\".\n\nFix_pattern: In the condition of checking if the object has a bbox/head, if the pattern of assigning random values to the proposals tensor using the cuda() method is detected, then the code is changed by replacing \".cuda()\" with \".to(device=img.device)\" to fix the API misuse."}
{"number": 526, "change": "class PGModel(Model):\nactions = np.concatenate([path['actions'] for path in batch])\nbatch_advantage = np.concatenate([path[\"advantage\"] for path in batch])\nbatch_advantage = zero_mean_unit_variance(batch_advantage)\n+        batch_advantage = np.expand_dims(batch_advantage, axis=1)\nstates = np.concatenate([path['states'] for path in batch])\n\nreturn action_log_stds, action_means, actions, batch_advantage, states\n", "fix_pattern": "<condition>: The condition in the context section is not clearly defined.\n<pattern>: No pattern is detected in the code.\n<code_one>: No code is removed.\n<code_two>: np.expand_dims(batch_advantage, axis=1)\nFix_pattern: in the condition of <condition>, if <pattern> is detected, then add np.expand_dims(batch_advantage, axis=1) to fix the API misuse."}
{"number": 529, "change": "class Categorical(Distribution):\nelif one_hot:\nboolean_mask = x\nelse:\n-                boolean_mask = torch.zeros(ps.size()).scatter_(-1, x.data.long(), 1)\n+                boolean_mask = torch_zeros_like(ps.data).scatter_(-1, x.data.long(), 1)\n# apply log function to masked probability tensor\nreturn torch.log(ps.masked_select(boolean_mask.byte()).contiguous().view(*batch_pdf_size))\n", "fix_pattern": "<condition>: The condition is when the variable \"one_hot\" is true.\n<pattern>: The pattern is that the code initializes the \"boolean_mask\" variable with a tensor of zeros and then scatters ones at specific indices.\n<code_one>: The code that is removed is \"boolean_mask = torch.zeros(ps.size()).scatter_(-1, x.data.long(), 1)\".\n<code_two>: The code that is added is \"boolean_mask = torch_zeros_like(ps.data).scatter_(-1, x.data.long(), 1)\".\nFix_pattern: In the condition of \"one_hot\" being true, the fix pattern is to change the initialization of \"boolean_mask\" from \"torch.zeros(ps.size())\" to \"torch_zeros_like(ps.data)\". This fixes the API misuse in the code."}
{"number": 530, "change": "class ViTMAEModelIntegrationTest(unittest.TestCase):\n\n# forward pass\nwith torch.no_grad():\n-            outputs = model(**inputs, noise=torch.from_numpy(noise))\n+            outputs = model(**inputs, noise=torch.from_numpy(noise).to(device=torch_device))\n\n# verify the logits\nexpected_shape = torch.Size((1, 196, 768))\n", "fix_pattern": "<condition>: None\n<pattern>: The missing 'to' method in 'torch.from_numpy(noise)'.\n<code_one>: 'noise=torch.from_numpy(noise)'\n<code_two>: 'noise=torch.from_numpy(noise).to(device=torch_device)'\nFix_pattern: In the condition of no clear pre condition is needed, if the pattern of missing 'to' method in 'torch.from_numpy(noise)' is detected, then change the code 'noise=torch.from_numpy(noise)' to 'noise=torch.from_numpy(noise).to(device=torch_device)' to fix the API misuse."}
{"number": 532, "change": "def initialize_vocabulary(vocabulary_path):\nrev_vocab = []\nwith gfile.GFile(vocabulary_path, mode=\"rb\") as f:\nrev_vocab.extend(f.readlines())\n-    rev_vocab = [line.strip() for line in rev_vocab]\n+    rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]\nvocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\nreturn vocab, rev_vocab\nelse:\n", "fix_pattern": "<condition>: The code is trying to read and process data from a file.\n<pattern>: The code is removing leading and trailing whitespaces from each line of data.\n<code_one>: rev_vocab = [line.strip() for line in rev_vocab]\n<code_two>: rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]\nFix_pattern: In the condition of reading and processing data from a file, if there is data processing that involves removing leading and trailing whitespaces from each line, then replace the code that removes the whitespaces with code that converts the line to bytes while also removing the whitespaces."}
{"number": 534, "change": "class ConformerSeparator(AbsSeparator):\n\"\"\"\n\n# if complex spectrum,\n-        if isinstance(input, ComplexTensor):\n+        if isinstance(input, ComplexTensor) or (\n+            is_torch_1_8_plus and torch.is_complex(input)\n+        ):\nfeature = abs(input)\nelse:\nfeature = input\n", "fix_pattern": "<condition>: The condition is when the input is an instance of ComplexTensor.\n<pattern>: The pattern is to check the type of the input using isinstance.\n<code_one>: The code that is removed is the line \"if isinstance(input, ComplexTensor):\".\n<code_two>: The code that is added is the line \"if isinstance(input, ComplexTensor) or (is_torch_1_8_plus and torch.is_complex(input)): \".\nFix_pattern: In the condition of when the input is an instance of ComplexTensor, if the pattern of checking the type of the input using isinstance is detected, then remove the line \"if isinstance(input, ComplexTensor):\" and add the line \"if isinstance(input, ComplexTensor) or (is_torch_1_8_plus and torch.is_complex(input)): \" to fix the API misuse."}
{"number": 537, "change": "def batch_flatten(x):\n'''Turn a n-D tensor into a 2D tensor where\nthe first dimension is conserved.\n'''\n-    x = tf.reshape(x, [-1, prod(shape(x)[1:])])\n+    x = tf.reshape(x, tf.pack([-1, prod(shape(x)[1:])]))\nreturn x\n", "fix_pattern": "<condition>: The code is using the deprecated `tf.pack` function.\n\n<pattern>: The pattern is to replace the deprecated `tf.pack` function with `tf.stack` function.\n\n<code_one>: `tf.pack([-1, prod(shape(x)[1:])])`\n\n<code_two>: `tf.stack([-1, prod(shape(x)[1:])])`\n\nFix_pattern: In the condition of using the deprecated `tf.pack` function, replace `tf.pack` with `tf.stack` to fix the API misuse."}
{"number": 538, "change": "def distributed_train(local_rank: int, main_address: str, main_port: int, num_no\n# 2. PREPARE DISTRIBUTED MODEL\nmodel = torch.nn.Linear(32, 2)\ndevice = torch.device(f\"cuda:{local_rank}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n-    model = DistributedDataParallel(model, device_ids=[local_rank]).to(device)\n+    model = DistributedDataParallel(model, device_ids=[local_rank] if torch.cuda.is_available() else None).to(device)\n\n# 3. SETUP LOSS AND OPTIMIZER\ncriterion = torch.nn.MSELoss()\n", "fix_pattern": "<condition>: If the code is running on a distributed environment.\n<pattern>: The model is wrapped with `DistributedDataParallel` using the `device_ids` argument.\n<code_one>: `model = DistributedDataParallel(model, device_ids=[local_rank]).to(device)`\n<code_two>: `model = DistributedDataParallel(model, device_ids=[local_rank] if torch.cuda.is_available() else None).to(device)`\nFix_pattern: In the condition of running in a distributed environment, if the code is wrapped with `DistributedDataParallel` using the `device_ids` argument, then change the `device_ids` argument to `[local_rank] if torch.cuda.is_available() else None` to fix the API misuse."}
{"number": 545, "change": "if __name__ == '__main__':\n\n# dataset = roiLoader(roidb, imdb.num_classes)\ndataset = roibatchLoader(roidb, imdb.num_classes)\n-  dataloader = torch.utils.data.DataLoader(dataset, batch_size=cfg.TRAIN.IMS_PER_BATCH,\n+  dataloader = torch.utils.data.DataLoader(dataset, batch_size=8,\nshuffle=False, num_workers=5)\n\n# initilize the tensor holder here.\n", "fix_pattern": "<condition>: The code is being executed when the file is run directly.\n\n<pattern>: The dataset is being loaded using the `roiLoader` function.\n\n<code_one>: `roiLoader(roidb, imdb.num_classes)`\n\n<code_two>: `roibatchLoader(roidb, imdb.num_classes)`\n\nFix_pattern: In the condition of `if __name__ == '__main__':`, if `dataset = roiLoader(roidb, imdb.num_classes)` is detected, then change `dataset = roiLoader(roidb, imdb.num_classes)` to `dataset = roibatchLoader(roidb, imdb.num_classes)` to fix the API misuse."}
{"number": 548, "change": "class VisionTransformer(nn.Module):\n\ndef forward(self, x):\nx = self.forward_features(x)\n-        if isinstance(x, tuple):\n-            x, x_dist = self.head(x[0]), self.head_dist(x[1])\n+        if self.head_dist is not None:\n+            x, x_dist = self.head(x[0]), self.head_dist(x[1])  # x must be a tuple\nif self.training and not torch.jit.is_scripting():\n# during inference, return the average of both classifier predictions\nreturn x, x_dist\n", "fix_pattern": "<context>\nclass VisionTransformer(nn.Module):\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        if self.training and not torch.jit.is_scripting():\n            # during inference, return the average of both classifier predictions\n            return x, x_dist\n\n<code_removed>\nif isinstance(x, tuple):\n    x, x_dist = self.head(x[0]), self.head_dist(x[1])\n\n<code_added>\nif self.head_dist is not None:\n    x, x_dist = self.head(x[0]), self.head_dist(x[1])  # x must be a tuple\n\n<condition>: \nself.head_dist is not None\n\n<pattern>:\nCheck if variable `[x]` is a tuple.\nIf not, `x` is unpacked by `[self.head(x[0]), self.head_dist(x[1])]`.\n\n<code_one>: \nif isinstance(x, tuple):\n\n<code_two>: \nx must be a tuple\n\nFix_pattern:\nIn the condition of `self.head_dist is not None`, if `[x]` is not a tuple, then change `[self.head(x[0]), self.head_dist(x[1])]` to `x must be a tuple` to fix the API misuse."}
{"number": 549, "change": "class TFKerasUtil(object):\n\ndataset = dataset.batch(batch_size).map(prep_data_tf_keras)\nreturn dataset\n-        return fn\n+        return tf.autograph.experimental.do_not_convert(fn) if _HAS_AUTOGRAPH else fn\n\n@staticmethod\ndef get_horovod():\n", "fix_pattern": "<condition>: The code is using the TensorFlow API `tf.autograph.experimental.do_not_convert()`.\n<pattern>: The code is returning the value of `fn` without any modification.\n<code_one>: `return fn`\n<code_two>: `return tf.autograph.experimental.do_not_convert(fn) if _HAS_AUTOGRAPH else fn`\nFix_pattern: In the condition of using the TensorFlow API `tf.autograph.experimental.do_not_convert()`, if the code is returning the value of `fn`, then change it to `return tf.autograph.experimental.do_not_convert(fn) if _HAS_AUTOGRAPH else fn` to fix the API misuse."}
{"number": 554, "change": "with tf.device('/cpu:0'):\nnet = FlattenLayer(net, name='flatten')\nnet = DenseLayer(net, 384, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d1relu')\nnet = DenseLayer(net, 192, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d2relu')\n-            net = DenseLayer(net, n_units=10, act=tf.identity, W_init=W_init2, name='output')\n+            net = DenseLayer(net, n_units=10, act=None, W_init=W_init2, name='output')\ny = net.outputs\n\nce = tl.cost.cross_entropy(y, y_, name='cost')\n", "fix_pattern": "<condition>: The condition is not explicitly mentioned in the context.\n\n<pattern>: If the layer with name 'output' is created with the argument 'act=tf.identity', it should be changed to 'act=None'.\n\n<code_one>: `net = DenseLayer(net, n_units=10, act=tf.identity, W_init=W_init2, name='output')`\n\n<code_two>: `net = DenseLayer(net, n_units=10, act=None, W_init=W_init2, name='output')`\n\nFix_pattern: In the condition of not explicitly mentioned, if the layer with name 'output' is created with the argument 'act=tf.identity', then change the line `net = DenseLayer(net, n_units=10, act=tf.identity, W_init=W_init2, name='output')` to `net = DenseLayer(net, n_units=10, act=None, W_init=W_init2, name='output')` to fix the API misuse."}
{"number": 557, "change": "class up(nn.Module):\nif bilinear:\nself.up = nn.UpsamplingBilinear2d(scale_factor=2)\nelse:\n-            self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\n+            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n\nself.conv = double_conv(in_ch, out_ch)\n", "fix_pattern": "<condition>: The condition is that the variable \"bilinear\" is checked.\n<pattern>: The pattern is that a nn.ConvTranspose2d layer is used for upsampling.\n<code_one>: The code that is removed is \"self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\".\n<code_two>: The code that is added is \"self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\".\nFix_pattern: In the condition of checking the variable \"bilinear\", if the pattern of using nn.ConvTranspose2d for upsampling is detected, then the code \"self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\" should be changed to \"self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\" to fix the API misuse."}
{"number": 559, "change": "class Metric(nn.Module, ABC):\nAutomatically calls ``update()``. Returns the metric value over inputs if ``compute_on_step`` is True.\n\"\"\"\n# add current step\n-        self.update(*args, **kwargs)\n+        with torch.no_grad():\n+            self.update(*args, **kwargs)\nself._forward_cache = None\n\nif self.compute_on_step:\n", "fix_pattern": "<condition>:\nThe condition is when the `compute_on_step` flag is True.\n\n<pattern>:\nThe pattern is the omission of the `with torch.no_grad():` context manager.\n\n<code_one>:\nThe code that is removed is `self.update(*args, **kwargs)`.\n\n<code_two>:\nThe code that is added is `with torch.no_grad(): self.update(*args, **kwargs)`.\n\nFix_pattern:\nIn the condition of `compute_on_step` is True, if the pattern of omitting `with torch.no_grad():` is detected, then add `with torch.no_grad():` before `self.update(*args, **kwargs)` to fix the API misuse."}
{"number": 560, "change": "temperature = max(args.temperature, 1e-3)\nwith open(args.outf, 'w') as outf:\nfor i in range(args.nwords):\n\n-        output, hidden = model(Variable(input, requires_grad=False), hidden)\n-        gen = torch.multinomial(output[0].data.cpu().div(temperature).exp(), 1)[0][0] # FIXME: no multinomial on GPU?\n+        output, hidden = model(Variable(input, volatile=True), hidden)\n+        gen = torch.multinomial(output[0].data.div(temperature).exp().cpu(), 1)[0][0] # FIXME: multinomial is only for CPU\ninput.fill_(gen)\nword = corpus.dic.idx2word[gen]\noutf.write(word)\n", "fix_pattern": "<condition>:\nThe condition is when using the `torch.multinomial` function.\n\n<pattern>:\nThe pattern is detecting the use of `torch.multinomial` on the GPU.\n\n<code_one>:\nThe code that needs to be changed is `torch.multinomial(output[0].data.cpu().div(temperature).exp(), 1)[0][0]`\n\n<code_two>:\nThe code that should replace the original code is `torch.multinomial(output[0].data.div(temperature).exp().cpu(), 1)[0][0]`\n\nFix_pattern:\nIn the condition of using `torch.multinomial`, if the code includes `torch.multinomial` on the GPU, then the fix is to change the code from `<code_one>` to `<code_two>` in order to fix the API misuse."}
{"number": 561, "change": "class TFFunnelForMultipleChoice(TFFunnelPreTrainedModel, TFMultipleChoiceLoss):\n@tf.function(\ninput_signature=[\n{\n-                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),\n+                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),\n\"attention_mask\": tf.TensorSpec((None, None), tf.float32, name=\"attention_mask\"),\n-                \"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),\n+                \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),\n}\n]\n)\n", "fix_pattern": "<condition>: The condition is when defining the input signature for a TensorFlow function.\n\n<pattern>: The pattern is the presence of \"input_ids\" and \"token_type_ids\" in the input signature.\n\n<code_one>: The code that is removed is the input signature for \"input_ids\" and \"token_type_ids\" with the tf.int64 data type.\n\n<code_two>: The code that is added is the input signature for \"input_ids\" and \"token_type_ids\" with the tf.int32 data type.\n\nFix_pattern: In the condition of defining the input signature for a TensorFlow function, if the pattern of having \"input_ids\" and \"token_type_ids\" with the tf.int64 data type is detected, then change the code to have \"input_ids\" and \"token_type_ids\" with the tf.int32 data type to fix the API misuse."}
{"number": 566, "change": "class ModelSaver(Callback):\nself.var_collections = var_collections\nif checkpoint_dir is None:\ncheckpoint_dir = logger.get_logger_dir()\n-        assert checkpoint_dir is not None\n-        if not tf.gfile.IsDirectory(checkpoint_dir):\n-            tf.gfile.MakeDirs(checkpoint_dir)\n+        if checkpoint_dir is not None:\n+            if not tf.gfile.IsDirectory(checkpoint_dir):\n+                tf.gfile.MakeDirs(checkpoint_dir)\nself.checkpoint_dir = checkpoint_dir\n\ndef _setup_graph(self):\n+        assert self.checkpoint_dir is not None, \\\n+            \"ModelSaver() doesn't have a valid checkpoint directory.\"\nvars = []\nfor key in self.var_collections:\nvars.extend(tf.get_collection(key))\n", "fix_pattern": "<condition>: The condition is that the variable \"checkpoint_dir\" is not None.\n<pattern>: The pattern detected is that the code is checking if the variable \"checkpoint_dir\" is not None and then creating the directory if it doesn't exist.\n<code_one>: The code that was removed is the assertion that checks if \"checkpoint_dir\" is not None and then creates the directory using \"tf.gfile.MakeDirs(checkpoint_dir)\".\n<code_two>: The code that was added is a conditional check if \"checkpoint_dir\" is not None and then creates the directory using \"tf.gfile.MakeDirs(checkpoint_dir)\". Additionally, an assertion is added to check that \"self.checkpoint_dir\" is not None.\nFix_pattern: In the condition of \"checkpoint_dir\" not being None, if the pattern of checking and creating the directory is detected, then remove the code that checks if \"checkpoint_dir\" is not None and creates the directory using \"tf.gfile.MakeDirs(checkpoint_dir)\", and instead add a conditional check for \"checkpoint_dir\" not being None and create the directory. Also, add an assertion to check that \"self.checkpoint_dir\" is not None."}
{"number": 571, "change": "class ModelCheckpoint(Callback):\nself.best_k_models.pop(del_filepath)\n\n# do not save nan, replace with +/- inf\n-        if torch.isnan(current):\n+        if isinstance(current, torch.Tensor) and torch.isnan(current):\ncurrent = torch.tensor(float('inf' if self.mode == \"min\" else '-inf'))\n\nfilepath = self._get_metric_interpolated_filepath_name(ckpt_name_metrics, epoch, step, del_filepath)\n", "fix_pattern": "<condition>: `current` is a scalar value\n<pattern>: `current` is checked for NaN\n<code_one>: `if torch.isnan(current):`\n<code_two>: `if isinstance(current, torch.Tensor) and torch.isnan(current):`\nFix_pattern: In the condition of `current` being a scalar value, if NaN value is detected, then change the code `if torch.isnan(current):` to `if isinstance(current, torch.Tensor) and torch.isnan(current):` to fix the API misuse."}
{"number": 574, "change": "class Graph(kerastuner.HyperModel, serializable.Serializable):\n\ndef build(self, hp):\n\"\"\"Build the HyperModel into a Keras Model.\"\"\"\n-        tf.keras.backend.clear_session()\nself._register_hps(hp)\nself.compile()\nreal_nodes = {}\n", "fix_pattern": "<condition>: The code is part of a class method in a class that inherits from \"kerastuner.HyperModel\" and \"serializable.Serializable\".\n<pattern>: The use of \"tf.keras.backend.clear_session()\" is incorrect or unnecessary.\n<code_one>: tf.keras.backend.clear_session()\n<code_two>: None\nFix_pattern: In the condition of inheriting from \"kerastuner.HyperModel\" and \"serializable.Serializable\", if \"tf.keras.backend.clear_session()\" is detected, then remove the code to fix the API misuse."}
{"number": 575, "change": "class TensorflowONNXTensorRTInferenceLearner(\nelse None\n)\nout_arrays = self._predict_array(cuda_input_arrays, input_shapes)\n-        return tuple(tf.convert_to_tensor(array) for array in out_arrays)\n+        return tuple(tf.convert_to_tensor(array[0]) for array in out_arrays)\n\n\nclass NumpyONNXTensorRTInferenceLearner(\n", "fix_pattern": "<condition>: The condition is not specified in the given information.\n\n<pattern>: The pattern is the use of the `tf.convert_to_tensor` function on an array.\n\n<code_one>: The code that is removed is `return tuple(tf.convert_to_tensor(array) for array in out_arrays)`.\n\n<code_two>: The code that is added is `return tuple(tf.convert_to_tensor(array[0]) for array in out_arrays)`.\n\nFix_pattern: In the condition where the `tf.convert_to_tensor` function is used on an array, the fix is to change the code from `return tuple(tf.convert_to_tensor(array) for array in out_arrays)` to `return tuple(tf.convert_to_tensor(array[0]) for array in out_arrays)` to fix the API misuse."}
{"number": 578, "change": "class GroupViTVisionTransformer(nn.Module):\n\nself.embeddings = GroupViTVisionEmbeddings(config)\nself.encoder = GroupViTVisionEncoder(config)\n-        self.layernorm = nn.LayerNorm(embed_dim)\n+        self.layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n\n@add_start_docstrings_to_model_forward(GROUPVIT_VISION_INPUTS_DOCSTRING)\n@replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=GroupViTVisionConfig)\n", "fix_pattern": "<condition>: The condition is not clear or specified in the given context.\n<pattern>: The pattern is to modify the code by adding an additional argument to the nn.LayerNorm() function call.\n<code_one>: The code that is removed is self.layernorm = nn.LayerNorm(embed_dim).\n<code_two>: The code that is added is self.layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps).\nFix_pattern: In the condition where <pattern> is detected, the fix is to modify the code by adding an additional argument to the nn.LayerNorm() function call. Specifically, <code_one> should be changed to <code_two> to fix the API misuse."}
{"number": 585, "change": "class TestTrainSampleHook(tf.test.TestCase):\npred_dict = {}\npred_dict[\"predicted_tokens\"] = tf.constant([[\"Hello\", \"World\", \"\u7b11w\"]])\npred_dict[\"labels.target_tokens\"] = tf.constant([[\"Hello\", \"World\", \"\u7b11w\"]])\n-    pred_dict[\"labels.target_len\"] = tf.constant([2]),\n+    pred_dict[\"labels.target_len\"] = tf.constant(2),\ngraph_utils.add_dict_to_collection(pred_dict, \"predictions\")\n\ndef tearDown(self):\n", "fix_pattern": "<condition>: When adding a constant value to a dictionary element in Python.\n<pattern>: The code is using an incorrect syntax to assign a constant value to a dictionary element.\n<code_one>: pred_dict[\"labels.target_len\"] = tf.constant([2]),\n<code_two>: pred_dict[\"labels.target_len\"] = tf.constant(2),\nFix_pattern: In the condition of adding a constant value to a dictionary element, if the incorrect syntax of assigning the constant value is detected, then change \"tf.constant([2])\" to \"tf.constant(2)\" to fix the API misuse."}
{"number": 587, "change": "class VonMises(TorchDistribution):\n\"\"\"\nshape = self._extended_shape(sample_shape)\nx = torch.empty(shape, dtype=self.loc.dtype, device=self.loc.device)\n-        done = torch.zeros(shape, dtype=self.loc.dtype, device=self.loc.device).byte()\n+        done = torch.zeros(shape, dtype=self.loc.dtype, device=self.loc.device).bool()\nwhile not done.all():\nu = torch.rand((3,) + shape, dtype=self.loc.dtype, device=self.loc.device)\nu1, u2, u3 = u.unbind()\n", "fix_pattern": "<condition>: The condition where a boolean tensor is being used to check a condition.\n<pattern>: The pattern of using `.byte()` method to create a boolean tensor.\n<code_one>: The code using `.byte()` method to create a boolean tensor.\n<code_two>: The code using `.bool()` method to create a boolean tensor.\nFix_pattern: In the condition of a boolean tensor, if the `.byte()` method is detected, then change it to `.bool()` to fix the API misuse."}
{"number": 591, "change": "class Model(ModelDesc):\nself.last_state = tf.identity(last_state, 'last_state')\n\n# seqlen x (Bxrnnsize)\n-        output = tf.reshape(tf.concat(1, outputs), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize\n+        output = tf.reshape(tf.concat_v2(outputs, 1), [-1, param.rnn_size])  # (Bxseqlen) x rnnsize\nlogits = FullyConnected('fc', output, param.vocab_size, nl=tf.identity)\nself.prob = tf.nn.softmax(logits / param.softmax_temprature)\n", "fix_pattern": "<condition>: The code is using the deprecated function `tf.concat` with `axis=1` in the line `output = tf.reshape(tf.concat(1, outputs), [-1, param.rnn_size])`.\n<pattern>: The usage of `tf.concat` needs to be updated to the new function `tf.concat_v2`.\n<code_one>: `tf.concat(1, outputs)`\n<code_two>: `tf.concat_v2(outputs, 1)`\nFix_pattern: In the condition of using `tf.concat` with `axis=1`, if the usage of `tf.concat(1, outputs)` is detected, then change the code to `tf.concat_v2(outputs, 1)` to fix the API misuse."}
{"number": 595, "change": "def fit_line(points: Tensor, weights: Optional[Tensor] = None) -> ParametrizedLi\nA = A.transpose(-2, -1) @ A\n\n# NOTE: not optimal for 2d points, but for now works for other dimensions\n-    _, _, V = torch.linalg.svd(A)\n+    _, _, V = _torch_svd_cast(A)\n+    V = V.transpose(-2, -1)\n\n# the first left eigenvector is the direction on the fited line\ndirection = V[..., 0, :]  # BxD\n", "fix_pattern": "<condition>: \nNone\n\n<pattern>: \nThe pattern is the usage of the `torch.linalg.svd` function to compute the singular value decomposition of tensor `A`.\n\n<code_one>:\nThe code that is removed is `_, _, V = torch.linalg.svd(A)`.\n\n<code_two>:\nThe code that is added is `_torch_svd_cast(A)` to compute the singular value decomposition of tensor `A`, and then `V = V.transpose(-2, -1)` to transpose the tensor `V`.\n\nFix_pattern:\nIn the condition of API misuse, if the usage of `torch.linalg.svd` is detected to compute the singular value decomposition of tensor `A`, then remove the code `torch.linalg.svd(A)` and replace it with `_torch_svd_cast(A)`. Additionally, transpose the tensor `V` by adding `V = V.transpose(-2, -1)`."}
{"number": 596, "change": "def ndim(x):\n'''Returns the number of axes in a tensor, as an integer.\n'''\nif is_sparse(x):\n-        return int(x.shape.get_shape()[0])\n+        return x._dims\n\ndims = x.get_shape()._dims\nif dims is not None:\n", "fix_pattern": "<condition>: The condition is that the input 'x' is a sparse tensor.\n<pattern>: The pattern is that the original code was using the attribute 'shape' of 'x' to get the shape information, and then accessing the '_dims' attribute of the shape to get the number of dimensions of the sparse tensor.\n<code_one>: The code that was removed is `return int(x.shape.get_shape()[0])`.\n<code_two>: The code that was added is `return x._dims`.\nFix_pattern: In the condition of 'x' being a sparse tensor, if the code is using the attribute 'shape' to get the shape information and then accessing '_dims' to get the number of dimensions, then the fix is to replace it with 'return x._dims'."}
{"number": 599, "change": "def convert_pandas_to_tf_tensor(\n# them. If the columns contain different types (for example, `float32`s\n# and `int32`s), then `tf.concat` raises an error.\ndtype: np.dtype = np.find_common_type(df.dtypes, [])\n+\n+            # if the columns are `ray.data.extensions.tensor_extension.TensorArray`,\n+            # the dtype will be `object`. In this case, we need to set the dtype to\n+            # none, and use the automatic type casting of `tf.convert_to_tensor`.\n+            if isinstance(dtype, object):\n+                dtype = None\n+\nexcept TypeError:\n# `find_common_type` fails if a series has `TensorDtype`. In this case,\n# don't cast any of the series and continue.\n", "fix_pattern": "<condition>: If the columns are `ray.data.extensions.tensor_extension.TensorArray`, the dtype will be `object`.\n<pattern>: Check if the dtype is an object and set it to None.\n<code_one>: None\n<code_two>: dtype = None\nFix_pattern: In the condition of if the columns are `ray.data.extensions.tensor_extension.TensorArray`, check if dtype is an object and set it to None to fix the API misuse."}
{"number": 600, "change": "def initialize(model: torch.nn.Module, init: str):\n\n# reset some modules with default init\nfor m in model.modules():\n-            if isinstance(m, (torch.nn.Embedding, torch.nn.LayerNorm)):\n+            if isinstance(m, (torch.nn.Embedding, torch.nn.LayerNorm, torch.nn.GroupNorm)):\nm.reset_parameters()\nif hasattr(m, \"espnet_initialization_fn\"):\nm.espnet_initialization_fn()\n", "fix_pattern": "<condition>: No specific condition is mentioned in the context section. \n\n<pattern>: The pattern to be detected is the check for the instance type of a module. \n\n<code_one>: The code being removed is the condition check for isinstance on torch.nn.Embedding and torch.nn.LayerNorm. \n\n<code_two>: The code being added is a modified condition check for isinstance on torch.nn.Embedding, torch.nn.LayerNorm, and torch.nn.GroupNorm. \n\nFix_pattern: In the condition of checking the instance type of a module, if the check for isinstance on torch.nn.Embedding and torch.nn.LayerNorm is detected, then change the condition to check for isinstance on torch.nn.Embedding, torch.nn.LayerNorm, and torch.nn.GroupNorm to fix the API misuse."}
{"number": 601, "change": "class Model(ModelDesc):\ninput_list = tf.unstack(input_feature, axis=1)  # seqlen x (Bxrnnsize)\n\n# seqlen is 1 in inference. don't need loop_function\n-        outputs, last_state = tf.nn.rnn(cell, input_list, initial, scope='rnnlm')\n+        outputs, last_state = tf.contrib.rnn.static_rnn(cell, input_list, initial, scope='rnnlm')\nself.last_state = tf.identity(last_state, 'last_state')\n\n# seqlen x (Bxrnnsize)\n", "fix_pattern": "<condition>:\nThere is a call to the tf.nn.rnn function.\n\n<pattern>:\nThe tf.nn.rnn function is replaced with tf.contrib.rnn.static_rnn.\n\n<code_one>:\noutputs, last_state = tf.nn.rnn(cell, input_list, initial, scope='rnnlm')\n\n<code_two>:\noutputs, last_state = tf.contrib.rnn.static_rnn(cell, input_list, initial, scope='rnnlm')\n\nFix_pattern:\nIn the condition of calling tf.nn.rnn, if the tf.nn.rnn function is detected, then the code \"outputs, last_state = tf.nn.rnn(cell, input_list, initial, scope='rnnlm')\" should be changed to \"outputs, last_state = tf.contrib.rnn.static_rnn(cell, input_list, initial, scope='rnnlm')\" to fix the API misuse."}
{"number": 611, "change": "class Attention(nn.Module):\nquery, processed_inputs)\n# apply masking\nif mask is not None:\n-            attention.data.masked_fill_(torch.bitwise_not(mask), self._mask_value)\n+            attention.data.masked_fill_(~mask, self._mask_value)\n# apply windowing - only in eval mode\nif not self.training and self.windowing:\nattention = self.apply_windowing(attention, inputs)\n", "fix_pattern": "<condition>: The condition is that the mask is not None.\n<pattern>: The pattern is that the code is removing the mask using the bitwise_not function.\n<code_one>: The code being removed is \"torch.bitwise_not(mask)\".\n<code_two>: The code being added is \"~mask\".\nFix_pattern: In the condition of the mask not being None, if the pattern of removing the mask using the bitwise_not function is detected, then change the code from \"torch.bitwise_not(mask)\" to \"~mask\" to fix the API misuse."}
{"number": 613, "change": "class EarlyStopping(Callback):\nf\" {self.monitor} = {current} {self.order_dict[self.mode]} {self.divergence_threshold}.\"\n\" Signaling Trainer to stop.\"\n)\n-        elif self.monitor_op(current - self.min_delta, self.best_score):\n+        elif self.monitor_op(current - self.min_delta, self.best_score.to(trainer.lightning_module.device)):\nshould_stop = False\nreason = self._improvement_message(current)\nself.best_score = current\n", "fix_pattern": "<condition>:\nThe condition is when the specified `monitor_op` returns True.\n\n<pattern>:\nThe pattern is to modify the argument passed to the `monitor_op` function.\n\n<code_one>:\nThe original code passed only the `current - self.min_delta` argument to the `monitor_op` function.\n\n<code_two>:\nThe modified code adds the `.to(trainer.lightning_module.device)` method to the `self.best_score` argument.\n\nFix_pattern:\nIn the condition of `elif monitor_op(current - self.min_delta, self.best_score)`, if the `monitor_op` function returns True, then change `self.best_score` to `self.best_score.to(trainer.lightning_module.device)` to fix the API misuse."}
{"number": 614, "change": "class LSTM(Model):\nlast_layer = add_time_dimension(features, self.seq_lens)\n\n# Setup the LSTM cell\n-        lstm = rnn.BasicLSTMCell(cell_size, state_is_tuple=True)\n+        lstm = tf.nn.rnn_cell.LSTMCell(cell_size, state_is_tuple=True)\nself.state_init = [\nnp.zeros(lstm.state_size.c, np.float32),\nnp.zeros(lstm.state_size.h, np.float32)\n", "fix_pattern": "<condition>: Using the deprecated rnn.BasicLSTMCell()\n<pattern>: Using the updated tf.nn.rnn_cell.LSTMCell()\n<code_one>: lstm = rnn.BasicLSTMCell(cell_size, state_is_tuple=True)\n<code_two>: lstm = tf.nn.rnn_cell.LSTMCell(cell_size, state_is_tuple=True)\nFix_pattern: In the condition of using the deprecated rnn.BasicLSTMCell(), if the pattern is detected, then change the code from lstm = rnn.BasicLSTMCell(cell_size, state_is_tuple=True) to lstm = tf.nn.rnn_cell.LSTMCell(cell_size, state_is_tuple=True) to fix the API misuse."}
{"number": 615, "change": "class XGLMModel(XGLMPreTrainedModel):\n\nhidden_states = inputs_embeds + positions\n\n-        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = nn.functional.dropout(hidden_states, p=float(self.dropout), training=self.training)\n\n# decoder layers\nall_hidden_states = () if output_hidden_states else None\n", "fix_pattern": "<condition>: API misuse in the XGLMModel class.\n<pattern>: Incorrect dropout parameter type.\n<code_one>: `hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)`\n<code_two>: `hidden_states = nn.functional.dropout(hidden_states, p=float(self.dropout), training=self.training)`\nFix_pattern: In the condition of XGLMModel class, if the incorrect dropout parameter type is detected, then change the line `hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)` to `hidden_states = nn.functional.dropout(hidden_states, p=float(self.dropout), training=self.training)` to fix the API misuse."}
{"number": 619, "change": "class Imagen(nn.Module):\ntext_embeds, text_masks = t5_encode_text(texts, name = self.text_encoder_name, return_attn_mask = True)\ntext_embeds, text_masks = map(lambda t: t.to(images.device), (text_embeds, text_masks))\n\n-        text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n+        if not self.unconditional:\n+            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n\nassert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into decoder if specified'\nassert not (not self.condition_on_text and exists(text_embeds)), 'decoder specified not to be conditioned on text, yet it is presented'\n", "fix_pattern": "<condition>: The condition is that the API misuse occurs when there is a need to check if the model should be conditioned on text or not.\n\n<pattern>: The pattern is that the code performs an incorrect check for this condition.\n\n<code_one>: The original code incorrectly checks if the model should be conditioned on text by using the `assert` statement.\n\n<code_two>: The fixed code correctly checks if the model should be conditioned on text by using an `if` statement.\n\nFix_pattern: In the condition of checking if the model should be conditioned on text, if the incorrect check is detected using the `assert` statement, then the code should be changed to use an `if` statement to fix the API misuse."}
{"number": 621, "change": "def conditional(\nif f_scale_tril is not None:\npack = torch.cat((pack, f_scale_tril_2D), dim=1)\n\n-        Lffinv_pack = pack.triangular_solve(Lff, upper=False)[0]\n+        Lffinv_pack = torch.linalg.solve_triangular(Lff, pack, upper=False)\n# unpack\nv_2D = Lffinv_pack[:, : f_loc_2D.size(1)]\nW = Lffinv_pack[:, f_loc_2D.size(1) : f_loc_2D.size(1) + M].t()\n", "fix_pattern": "<condition>:\nIf there is a code snippet that uses the `triangular_solve` function from the `torch` library with the `upper` parameter set to False.\n\n<pattern>:\nReplace the `triangular_solve` function with the `torch.linalg.solve_triangular` function.\n\n<code_one>:\n```\nLffinv_pack = pack.triangular_solve(Lff, upper=False)[0]\n```\n\n<code_two>:\n```\nLffinv_pack = torch.linalg.solve_triangular(Lff, pack, upper=False)\n```\n\nFix_pattern:\nIn the condition of using `triangular_solve` function with `upper` set to False, replace the `triangular_solve` function with `torch.linalg.solve_triangular` function by changing the code from <code_one> to <code_two> to fix the API misuse."}
{"number": 623, "change": "class Model(ModelDesc):\nwrong = prediction_incorrect(logits, label, 5, name='wrong-top5')\nadd_moving_summary(tf.reduce_mean(wrong, name='train-error-top5'))\n\n-        wd_cost = tf.mul(1e-4, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss')\n+        wd_cost = regularize_cost('.*/W', l2_regularizer(1e-4), name='l2_regularize_loss')\nadd_moving_summary(loss, wd_cost)\nself.cost = tf.add_n([loss, wd_cost], name='cost')\n", "fix_pattern": "<condition>: The condition in the code is to calculate the weight decay cost.\n<pattern>: The pattern detected is the use of the deprecated function \"tf.mul\".\n<code_one>: The code that was removed is \"tf.mul(1e-4, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss')\".\n<code_two>: The code that was added is \"l2_regularizer(1e-4)\".\nFix_pattern: In the condition of calculating the weight decay cost, if the use of the deprecated function \"tf.mul\" is detected, then change the code from \"tf.mul(1e-4, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss')\" to \"l2_regularizer(1e-4)\" to fix the API misuse."}
{"number": 627, "change": "class Optimizer:\ng = [dev_grads[dev][var_idx][0] for dev in devices]\n\nif np.prod(grad_shape):  # nccl does not support zero-sized tensors\n-                            g = tf.contrib.nccl.all_sum(g)\n+                            g = nccl_ops.all_sum(g)\n\nfor dev, gg in zip(devices, g):\ndev_grads[dev][var_idx] = (gg, dev_grads[dev][var_idx][1])\n", "fix_pattern": "<condition>: `np.prod(grad_shape)`\n<pattern>: Using `tf.contrib.nccl.all_sum` to sum the gradients\n<code_one>: `g = tf.contrib.nccl.all_sum(g)`\n<code_two>: `g = nccl_ops.all_sum(g)`\nFix_pattern: In the condition of `np.prod(grad_shape)`, if the pattern of using `tf.contrib.nccl.all_sum` is detected, then change `g = tf.contrib.nccl.all_sum(g)` to `g = nccl_ops.all_sum(g)` to fix the API misuse."}
{"number": 633, "change": "class Decoder(torch.nn.Module, ScorerInterface):\n\nif self.labeldist is not None:\nif self.vlabeldist is None:\n-                self.vlabeldist = to_device(hs_pad, torch.from_numpy(self.labeldist))\n+                self.vlabeldist = to_device(hs_pad[0], torch.from_numpy(self.labeldist))\nloss_reg = -torch.sum(\n(F.log_softmax(y_all, dim=1) * self.vlabeldist).view(-1), dim=0\n) / len(ys_in)\n", "fix_pattern": "<condition>: self.labeldist is not None and self.vlabeldist is None\n<pattern>: Variable self.vlabeldist is assigned to to_device(hs_pad, torch.from_numpy(self.labeldist))\n<code_one>: self.vlabeldist = to_device(hs_pad, torch.from_numpy(self.labeldist))\n<code_two>: self.vlabeldist = to_device(hs_pad[0], torch.from_numpy(self.labeldist))\nFix_pattern: In the condition of self.labeldist is not None and self.vlabeldist is None, the code self.vlabeldist is assigned to to_device(hs_pad[0], torch.from_numpy(self.labeldist)) to fix the API misuse."}
{"number": 635, "change": "class TransformerModel(nn.Module):\ndef init_weights(self):\ninitrange = 0.1\nnn.init.uniform_(self.encoder.weight, -initrange, initrange)\n-        nn.init.zeros_(self.decoder.weight)\n+        nn.init.zeros_(self.decoder.bias)\nnn.init.uniform_(self.decoder.weight, -initrange, initrange)\n\ndef forward(self, src, has_mask=True):\n", "fix_pattern": "<condition>: The condition is that there is a need to initialize the weights of the decoder.\n\n<pattern>: The pattern is that the code `nn.init.zeros_(self.decoder.weight)` is used to initialize the decoder weights.\n\n<code_one>: The code `nn.init.zeros_(self.decoder.weight)` is being removed.\n\n<code_two>: The code `nn.init.zeros_(self.decoder.bias)` is being added.\n\nFix_pattern: In the condition where there is a need to initialize the weights of the decoder, if the pattern of using `nn.init.zeros_(self.decoder.weight)` is detected, then the code `nn.init.zeros_(self.decoder.weight)` should be removed and replaced with `nn.init.zeros_(self.decoder.bias)` to fix the API misuse."}
{"number": 637, "change": "import syft\ndef model():\nl_in, l_h, l_out = 32, 16, 2\nmodel = crypten.nn.Sequential(\n-        [crypten.nn.Linear(l_in, l_h), crypten.nn.ReLU(), crypten.nn.Linear(l_h, l_out)]\n+        crypten.nn.Linear(l_in, l_h), crypten.nn.ReLU(), crypten.nn.Linear(l_h, l_out)\n)\nreturn model\n", "fix_pattern": "<condition>: When creating a model using the crypten.nn.Sequential() function.\n<pattern>: A list of layers is provided as an argument to the crypten.nn.Sequential() function.\n<code_one>: [crypten.nn.Linear(l_in, l_h), crypten.nn.ReLU(), crypten.nn.Linear(l_h, l_out)]\n<code_two>: crypten.nn.Linear(l_in, l_h), crypten.nn.ReLU(), crypten.nn.Linear(l_h, l_out)\nFix_pattern: In the condition of creating a model using the crypten.nn.Sequential() function, if a list of layers is provided as an argument, then remove the square brackets from the list to fix the API misuse."}
{"number": 639, "change": "class LinearModel(object):\nreturn self.sess.run(self.cross_entropy_grads, feed_dict={self.x: xs, self.y_: ys})\n\ndef net_initialization():\n-  return LinearModel([784,10])\n+  with tf.Graph().as_default():\n+    return LinearModel([784,10])\n\n# By default, when an environment variable is used by a remote function, the\n# initialization code will be rerun at the end of the remote task to ensure\n", "fix_pattern": "<condition>:\nIn a function where TensorFlow operations are used.\n\n<pattern>:\nReturning a TensorFlow object without wrapping it in a new graph.\n\n<code_one>:\nreturn LinearModel([784,10])\n\n<code_two>:\nwith tf.Graph().as_default():\n    return LinearModel([784,10])\n\nFix_pattern:\nIn the condition of using TensorFlow operations, if returning a TensorFlow object without wrapping it in a new graph is detected, then change the code to wrap the object in a new graph using `tf.Graph().as_default()` to fix the API misuse."}
{"number": 641, "change": "class LinearRegression(d2l.Module):\ndef __init__(self, lr):\nsuper().__init__()\nself.save_hyperparameters()\n-        self.net = tf.keras.layers.Dense(1)\n+        initializer = tf.initializers.RandomNormal(stddev=0.01)\n+        self.net = tf.keras.layers.Dense(1, kernel_initializer=initializer)\n\ndef forward(self, X):\n\"\"\"The linear regression model.\n", "fix_pattern": "<condition>: Condition in which the code is being used for linear regression.\n\n<pattern>: The pattern that needs to be detected is the initialization of the Dense layer without specifying a kernel initializer.\n\n<code_one>: The code that needs to be removed is the line that initializes the Dense layer without a kernel initializer.\n\n<code_two>: The code that needs to be added is the line that initializes the Dense layer with a kernel initializer.\n\nFix_pattern: In the condition of linear regression, if the pattern of initializing the Dense layer without a kernel initializer is detected, then remove the line that initializes the Dense layer and add a line to initialize the Dense layer with a kernel initializer."}
{"number": 642, "change": "class TorchCategorical(TorchDistributionWrapper):\n\"\"\"Wrapper class for PyTorch Categorical distribution.\"\"\"\n\n@override(ActionDistribution)\n-    def __init__(self, inputs, model):\n-        super().__init__(inputs, model)\n-        self.dist = torch.distributions.categorical.Categorical(logits=inputs)\n+    def __init__(self, inputs, model=None, temperature=1.0):\n+        assert temperature > 0.0, \"Categorical `temperature` must be > 0.0!\"\n+        super().__init__(inputs / temperature, model)\n+        self.dist = torch.distributions.categorical.Categorical(\n+            logits=self.inputs)\n\n@override(ActionDistribution)\ndef deterministic_sample(self):\n", "fix_pattern": "<condition>: When initializing the TorchCategorical class.\n<pattern>: Using the torch.distributions.categorical.Categorical(logits=inputs) method.\n<code_one>: self.dist = torch.distributions.categorical.Categorical(logits=inputs)\n<code_two>: self.dist = torch.distributions.categorical.Categorical(logits=self.inputs)\nFix_pattern: In the condition of initializing the TorchCategorical class, if the pattern of using the torch.distributions.categorical.Categorical(logits=inputs) method is detected, then change the self.dist = torch.distributions.categorical.Categorical(logits=inputs) to self.dist = torch.distributions.categorical.Categorical(logits=self.inputs) to fix the API misuse."}
{"number": 643, "change": "class TestPretrainedTransformerEmbedder(AllenNlpTestCase):\ndef test_xlnet_token_type_ids(self):\ntoken_embedder = PretrainedTransformerEmbedder(\"xlnet-base-cased\")\ntoken_ids = torch.LongTensor([[1, 2, 3], [2, 3, 4]])\n-        mask = torch.ones_like(token_ids)\n+        mask = torch.ones_like(token_ids).bool()\ntype_ids = torch.zeros_like(token_ids)\ntype_ids[1, 1] = 1\ntoken_embedder(token_ids, mask, type_ids)\n", "fix_pattern": "<condition>:\nNone\n\n<pattern>:\nReplacing the line of code where a mask tensor is created with a ones tensor and a boolean conversion.\n\n<code_one>:\nmask = torch.ones_like(token_ids)\n\n<code_two>:\nmask = torch.ones_like(token_ids).bool()\n\nFix_pattern:\nIn the condition of no specific condition, if a pattern of creating a mask tensor using `torch.ones_like(token_ids)` is detected, then change the code from `mask = torch.ones_like(token_ids)` to `mask = torch.ones_like(token_ids).bool()` to fix the API misuse."}
{"number": 645, "change": "class Block(Layer):\nlayer_counter[layer_type] += 1\n\n# layer_name = self.name + '-' + layer_name\n-            self.layers[n] = self.submodule(\n+            layer = self.submodule(\nname=layer_name, module=layer_spec, modules=tensorforce.core.layer_modules,\ninput_spec=self._input_spec\n)\n-            self._input_spec = self.layers[n].output_spec()\n-\n+            self.layers.append(layer)\n+            self._input_spec = layer.output_spec()\n\nreturn self.layers[0].input_spec.copy()\n", "fix_pattern": "<condition>: When working with a Block class in a neural network\n<pattern>: Replacing the assignment of self.layers[n] with self.submodule() and the assignment of self._input_spec with self.layers[n].output_spec()\n<code_one>: self.layers[n] = self.submodule()\n<code_two>: self._input_spec = layer.output_spec()\nFix_pattern: In the condition of working with a Block class in a neural network, if the code pattern of assigning self.layers[n] with self.submodule() and self._input_spec with self.layers[n].output_spec() is detected, then replace self.layers[n] with self.submodule() and self._input_spec with layer.output_spec()."}
{"number": 648, "change": "def model():\n\nif sd_vae_approx_model is None:\nsd_vae_approx_model = VAEApprox()\n-        sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\")))\n+        sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\"), map_location='cpu' if devices.device.type != 'cuda' else None))\nsd_vae_approx_model.eval()\nsd_vae_approx_model.to(devices.device, devices.dtype)\n", "fix_pattern": "<condition>: `sd_vae_approx_model` being None.\n<pattern>: Loading state dictionary with the `torch.load()` function.\n<code_one>: `sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\")))`\n<code_two>: `sd_vae_approx_model.load_state_dict(torch.load(os.path.join(paths.models_path, \"VAE-approx\", \"model.pt\"), map_location='cpu' if devices.device.type != 'cuda' else None))`\nFix_pattern: In the condition of `sd_vae_approx_model` being None, if the pattern of loading the state dictionary with `torch.load()` is detected, then change the code from `<code_one>` to `<code_two>` to fix the API misuse."}
{"number": 654, "change": "class FP16_DeepSpeedZeroOptimizer(object):\n\"\"\" Perform all reduce within model parallel group, if any.\n\"\"\"\nif self.model_parallel_group is None:\n-            torch.distributed.all_reduce(tensor=tensor, op=op)\n+            pass\nelse:\ntorch.distributed.all_reduce(tensor=tensor,\nop=op,\n", "fix_pattern": "<condition>: The condition is that the \"model_parallel_group\" variable is not None.\n<pattern>: The pattern is that the \"all_reduce\" function is called with \"tensor\" and \"op\" arguments.\n<code_one>: The code that is being removed is the line that calls the \"all_reduce\" function.\n<code_two>: The code that is being added is a pass statement.\nFix_pattern: In the condition of \"model_parallel_group\" not being None, if the pattern of calling the \"all_reduce\" function with \"tensor\" and \"op\" arguments is detected, then remove the line that calls the \"all_reduce\" function and add a pass statement to fix the API misuse."}
{"number": 655, "change": "class {{cookiecutter.camelcase_modelname}}EncoderLayer(nn.Module):\nhidden_states = residual + hidden_states\nhidden_states = self.final_layer_norm(hidden_states)\n\n-        if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\n+        if hidden_states.dtype == torch.float16 and (torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()):\nclamp_value = torch.finfo(hidden_states.dtype).max - 1000\nhidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n", "fix_pattern": "<condition>: The condition is not specified in the given task. \n\n<pattern>: The pattern is detecting if there are any infinite values (torch.isinf) or NaN values (torch.isnan) in the hidden_states variable. \n\n<code_one>: The code that is removed is \"if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\"\n\n<code_two>: The code that is added is \"if hidden_states.dtype == torch.float16 and (torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()):\"\n\nFix_pattern: \nIn the condition of not specified, if the pattern of detecting infinite values or NaN values in the hidden_states variable is detected, then change the code \"if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\" to \"if hidden_states.dtype == torch.float16 and (torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()):\" to fix the API misuse."}
{"number": 656, "change": "class LabelSmoother:\n\ndef __call__(self, model_output, labels):\nlogits = model_output[\"logits\"] if isinstance(model_output, dict) else model_output[0]\n-        log_probs = -torch.nn.functional.log_softmax(logits, dim=-1)\n+        log_probs = -nn.functional.log_softmax(logits, dim=-1)\nif labels.dim() == log_probs.dim() - 1:\nlabels = labels.unsqueeze(-1)\n", "fix_pattern": "<condition>: The condition is that the dimension of the labels is one less than the dimension of the log probabilities (logits).\n\n<pattern>: The pattern is that the log probabilities are computed using the torch.nn.functional.log_softmax function.\n\n<code_one>: The code that is removed is `log_probs = -torch.nn.functional.log_softmax(logits, dim=-1)`.\n\n<code_two>: The code that is added is `log_probs = -nn.functional.log_softmax(logits, dim=-1)`.\n\nFix_pattern: In the condition of when the dimension of the labels is one less than the dimension of the log probabilities, the log probabilities are computed using the `nn.functional.log_softmax` function instead of the `torch.nn.functional.log_softmax` function to fix the API misuse."}
{"number": 657, "change": "class NonMaximaSuppression2d(nn.Module):\ndef forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore\nassert len(x.shape) == 4, x.shape\n# find local maximum values\n-        x_max: torch.Tensor = self.max_pool2d(x)\n+        x_max: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]] = \\\n+            self.max_pool2d(x)\n\n# create mask for maximums in the original map\nx_mask: torch.Tensor = torch.where(\n", "fix_pattern": "<condition>: The condition is that the input tensor `x` should have a shape of 4 dimensions.\n<pattern>: The pattern is that the original code uses the `self.max_pool2d(x)` function without assigning it to a variable.\n<code_one>: The code removed is `x_max: torch.Tensor = self.max_pool2d(x)`.\n<code_two>: The code added is `x_max: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]] = self.max_pool2d(x)`.\nFix_pattern: In the condition of `x` having 4 dimensions, if the `self.max_pool2d(x)` function is called, assign the result to the variable `x_max` with the type annotation `Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]`. This fixes the API misuse."}
{"number": 659, "change": "class ImageFeatureEmbeddings(Embeddings):\n\ndef __init__(self, feature_size: int, embedding_size: int, dropout: float = 0.0):\nimage_embeddings = torch.nn.Linear(feature_size, embedding_size)\n-        location_embeddings = torch.nn.Linear(4, embedding_size)\n+        location_embeddings = torch.nn.Linear(4, embedding_size, bias=False)\nembeddings = torch.nn.ModuleDict(\n{\"image_embeddings\": image_embeddings, \"location_embeddings\": location_embeddings}\n)\n", "fix_pattern": "<condition>: The code is initializing a `ModuleDict` called `embeddings` with two keys (`\"image_embeddings\"` and `\"location_embeddings\"`).\n\n<pattern>: The pattern is to adjust the initialization of `location_embeddings` to include the `bias=False` argument.\n\n<code_one>: The original code set `location_embeddings` using `torch.nn.Linear(4, embedding_size)`.\n\n<code_two>: The fixed code sets `location_embeddings` using `torch.nn.Linear(4, embedding_size, bias=False)`.\n\nFix_pattern: In the condition of initializing a `ModuleDict`, if the initialization of `location_embeddings` is detected as `torch.nn.Linear(4, embedding_size)`, then change it to `torch.nn.Linear(4, embedding_size, bias=False)` to fix the API misuse."}
{"number": 661, "change": "class Model(ModelDesc):\n.apply(fg)\n.BatchNorm('bn5').apply(activate)\n# 5\n-                      .tf.nn.dropout(0.5 if is_training else 1.0)\n+                      .Dropout(rate=0.5 if is_training else 0.0)\n.Conv2D('conv6', 512, 5, padding='VALID')\n.apply(fg).BatchNorm('bn6')\n.apply(nonlin)\n", "fix_pattern": "<condition>: The condition is whether the code is using the tf.nn.dropout function.\n<pattern>: The pattern is that the tf.nn.dropout function is being replaced with the tf.nn.Dropout class.\n<code_one>: The code being removed is \".tf.nn.dropout(0.5 if is_training else 1.0)\".\n<code_two>: The code being added is \".Dropout(rate=0.5 if is_training else 0.0)\".\nFix_pattern: In the condition of using tf.nn.dropout, if the pattern of \".tf.nn.dropout(0.5 if is_training else 1.0)\" is detected, then the code is fixed by changing it to \".Dropout(rate=0.5 if is_training else 0.0)\" to fix the API misuse."}
{"number": 665, "change": "class BLEU(Metric):\nreturn math.exp(1.0 - self._reference_lengths / self._prediction_lengths)\n\ndef _get_valid_tokens_mask(self, tensor: torch.LongTensor) -> torch.ByteTensor:\n-        valid_tokens_mask = torch.ones(tensor.size(), dtype=torch.uint8)\n+        valid_tokens_mask = torch.ones(tensor.size(), dtype=torch.bool)\nfor index in self._exclude_indices:\nvalid_tokens_mask = valid_tokens_mask & (tensor != index)\nreturn valid_tokens_mask\n", "fix_pattern": "<condition>: None\n<pattern>: The code changes the data type of the variable \"valid_tokens_mask\" from torch.uint8 to torch.bool.\n<code_one>: None\n<code_two>: valid_tokens_mask = torch.ones(tensor.size(), dtype=torch.bool)\nFix_pattern: In the condition of no specific condition, if the code changes the data type of a variable from torch.uint8 to torch.bool, then set the variable equal to torch.ones with dtype=torch.bool."}
{"number": 674, "change": "class DecoderLayer(nn.Module):\nif self.normalize_before:\nx = self.norm2(x)\nif self.concate_after:\n-            x_concat = torch.cat(x, self.src_attn(x, memory, memory, memory_mask))\n+            x_concat = torch.cat((x, self.src_attn(x, memory, memory, memory_mask)), dim=-1)\nx = residual + self.concate_linear2(x_concat)\nelse:\nx = residual + self.dropout(self.src_attn(x, memory, memory, memory_mask))\n", "fix_pattern": "<condition>: If the condition \"self.concate_after\" is True.\n<pattern>: The pattern is a missing dimension specification in the torch.cat() function call.\n<code_one>: The code that needs to be removed is \"x_concat = torch.cat(x, self.src_attn(x, memory, memory, memory_mask))\".\n<code_two>: The code that needs to be added is \"x_concat = torch.cat((x, self.src_attn(x, memory, memory, memory_mask)), dim=-1)\".\nFix_pattern: In the condition of \"self.concate_after\" being True, if a missing dimension specification is detected in the torch.cat() function call, then remove the code \"x_concat = torch.cat(x, self.src_attn(x, memory, memory, memory_mask))\" and add the code \"x_concat = torch.cat((x, self.src_attn(x, memory, memory, memory_mask)), dim=-1)\" to fix the API misuse."}
{"number": 676, "change": "class RoBERTaEncoder(Encoder):\n@property\ndef output_shape(self) -> torch.Size:\nif self.reduce_output is None:\n-            return torch.Size([self.max_sequence_length, self.transformer.module.config.hidden_size])\n+            return torch.Size([self.max_sequence_length - 2, self.transformer.module.config.hidden_size])\nreturn torch.Size([self.transformer.module.config.hidden_size])\n\n@property\n", "fix_pattern": "<condition>: The condition is when the \"output_shape\" property is accessed in the RoBERTaEncoder class.\n<pattern>: The pattern is detecting a specific return value of \"torch.Size([self.max_sequence_length, self.transformer.module.config.hidden_size])\".\n<code_one>: The original code being removed is \"return torch.Size([self.max_sequence_length, self.transformer.module.config.hidden_size])\".\n<code_two>: The new code being added is \"return torch.Size([self.max_sequence_length - 2, self.transformer.module.config.hidden_size])\".\nFix_pattern: In the condition of accessing the \"output_shape\" property, if the pattern of \"torch.Size([self.max_sequence_length, self.transformer.module.config.hidden_size])\" is detected, then change the code from \"return torch.Size([self.max_sequence_length, self.transformer.module.config.hidden_size])\" to \"return torch.Size([self.max_sequence_length - 2, self.transformer.module.config.hidden_size])\" to fix the API misuse."}
{"number": 677, "change": "def clip(\n*,\nout: Optional[torch.Tensor] = None,\n) -> torch.Tensor:\n-    assert torch.all(torch.less(x_min, x_max)), \"Min value must be less than max.\"\n+    assert torch.all(\n+        torch.less(torch.tensor(x_min), x_max)\n+    ), \"Min value must be less than max.\"\nif hasattr(x_min, \"dtype\"):\npromoted_type = torch.promote_types(x_min.dtype, x_max.dtype)\npromoted_type = torch.promote_types(promoted_type, x.dtype)\n", "fix_pattern": "<condition>: The condition is that `x_min` must be less than `x_max`.\n\n<pattern>: The pattern is an incorrect assertion in the code that checks if `x_min` is less than `x_max`.\n\n<code_one>: The code that needs to be removed is the incorrect assertion `assert torch.all(torch.less(x_min, x_max)), \"Min value must be less than max.\"`.\n\n<code_two>: The code that needs to be added is the correct assertion `assert torch.all(torch.less(torch.tensor(x_min), x_max)), \"Min value must be less than max.\"`.\n\nFix_pattern: In the condition of `x_min` being less than `x_max`, if the incorrect assertion is detected, then remove the old incorrect assertion code and add the correct assertion code to fix the API misuse."}
{"number": 678, "change": "class TFTokenClassificationLoss:\n)\n# make sure only labels that are not equal to -100\n# are taken into account as loss\n-        if tf.math.reduce_any(labels == -1).numpy() is True:\n+        if tf.math.reduce_any(labels == -1):\nwarnings.warn(\"Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\")\nactive_loss = tf.reshape(labels, (-1,)) != -1\nelse:\n", "fix_pattern": "<condition>: Check if any labels in a TensorFlow tensor are equal to -1.\n<pattern>: Using the `tf.math.reduce_any` function to check if any labels are equal to -1.\n<code_one>: `tf.math.reduce_any(labels == -1).numpy() is True`\n<code_two>: `tf.math.reduce_any(labels == -1)`\nFix_pattern: In the condition of checking if any labels are equal to -1, if the pattern `tf.math.reduce_any(labels == -1).numpy() is True` is detected, then change the code from `tf.math.reduce_any(labels == -1).numpy() is True` to `tf.math.reduce_any(labels == -1)` to fix the API misuse."}
{"number": 681, "change": "class LayoutLMv3Model(LayoutLMv3PreTrainedModel):\nposition_ids = position_ids.expand_as(input_ids)\nfinal_position_ids = position_ids\n\n-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, None, device)\n+        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(\n+            attention_mask, None, device, dtype=embedding_output.dtype\n+        )\n\n# Prepare head mask if needed\n# 1.0 in head_mask indicate we keep the head\n", "fix_pattern": "<condition>:\nThe condition is not clearly specified in the provided context.\n\n<pattern>:\nThe pattern is to modify the argument in the method call to fix the API misuse.\n\n<code_one>:\nThe code that was removed is `self.get_extended_attention_mask(attention_mask, None, device)`.\n\n<code_two>:\nThe code that was added is `self.get_extended_attention_mask(attention_mask, None, device, dtype=embedding_output.dtype)`.\n\nFix_pattern:\nIn the condition of <condition>, if <pattern> is detected, then change the argument in the method call from <code_one> to <code_two> to fix the API misuse."}
{"number": 683, "change": "class StableDiffusionKDiffusionPipeline(DiffusionPipeline):\nreturn_tensors=\"pt\",\n)\ntext_input_ids = text_inputs.input_ids\n-        untruncated_ids = self.tokenizer(prompt, padding=\"max_length\", return_tensors=\"pt\").input_ids\n+        untruncated_ids = self.tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n\n-        if not torch.equal(text_input_ids, untruncated_ids):\n+        if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(text_input_ids, untruncated_ids):\nremoved_text = self.tokenizer.batch_decode(untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1])\nlogger.warning(\n\"The following part of your input was truncated because CLIP can only handle sequences up to\"\n", "fix_pattern": "<condition>: The condition where the input tokens have been truncated due to the maximum sequence length constraint of CLIP model.\n<pattern>: The pattern where the original and truncated input tokens are compared using torch.equal() function.\n<code_one>: The code that checks if the original and truncated input tokens are not equal.\n<code_two>: The code that fixes the API misuse by changing the padding parameter to \"longest\" and performs the same comparison.\nFix_pattern: In the condition of input token truncation, if the original and truncated input tokens are not equal, then change the padding parameter to \"longest\" and perform the same comparison to fix the API misuse."}
{"number": 685, "change": "def to_tf_values(result, path):\n\nclass TFLogger(Logger):\ndef _init(self):\n-        logger.info(\n-            \"Initializing TFLogger instead of TF2Logger. We recommend \"\n-            \"migrating to TF2.0. This class will be removed in the future.\")\n-        self._file_writer = tf.summary.FileWriter(self.logdir)\n+        logger.info(\"Initializing TFLogger instead of TF2Logger.\")\n+        self._file_writer = tf.compat.v1.summary.FileWriter(self.logdir)\n\ndef on_result(self, result):\ntmp = result.copy()\n", "fix_pattern": "<condition>:\nNo clear condition can be identified.\n\n<pattern>:\nThe pattern detected is the initialization of TFLogger instead of TF2Logger.\n\n<code_one>:\nIn the code removed section, the code that is removed is logging an info message recommending the migration to TF2.0 and initializing a tf.summary.FileWriter using self.logdir.\n\n<code_two>:\nIn the code added section, the code that is added is just logging an info message and initializing a tf.compat.v1.summary.FileWriter using self.logdir.\n\nFix_pattern:\nIn the condition of no clear condition, if the pattern of initializing TFLogger instead of TF2Logger is detected, then remove the logging message recommending the migration to TF2.0 and change the initialization of tf.summary.FileWriter to tf.compat.v1.summary.FileWriter to fix the API misuse."}
{"number": 693, "change": "class Ensemble(nn.ModuleList):\n\n\ndef attempt_load(weights, device=None, inplace=True, fuse=True):\n+    # Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a\nfrom models.yolo import Detect, Model\n\n-    # Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a\nmodel = Ensemble()\nfor w in weights if isinstance(weights, list) else [weights]:\n-        ckpt = torch.load(attempt_download(w), map_location=device)\n-        ckpt = (ckpt.get('ema') or ckpt['model']).float()  # FP32 model\n+        ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n+        ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model\nmodel.append(ckpt.fuse().eval() if fuse else ckpt.eval())  # fused or un-fused model in eval mode\n\n# Compatibility updates\n", "fix_pattern": "<condition>: The code is attempting to load a model from weights.\n<pattern>: The code is loading the model weights and converting it to a floating-point 32 (FP32) model.\n<code_one>: ckpt = (ckpt.get('ema') or ckpt['model']).float()  # FP32 model\n<code_two>: ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model\nFix_pattern: In the condition of loading model weights, if the code is converting the loaded model to a floating-point 32 (FP32) model, then change the line of code that converts the model to include 'map_location' and 'to(device)'."}
{"number": 694, "change": "class Conv2dSubsampling6(torch.nn.Module):\ntorch.nn.ReLU(),\n)\nself.out = torch.nn.Sequential(\n-            torch.nn.Linear(odim * (((idim - 1) // 2 - 1) // 3), odim),\n+            torch.nn.Linear(odim * (((idim - 1) // 2 - 2) // 3), odim),\nPositionalEncoding(odim, dropout_rate),\n)\n", "fix_pattern": "<condition>: The condition is not clearly specified in the given code snippet.\n\n<pattern>: The pattern is that the linear layer's input size is changed based on the calculation of `odim * (((idim - 1) // 2 - 1) // 3)`.\n\n<code_one>: The code being removed is `torch.nn.Linear(odim * (((idim - 1) // 2 - 1) // 3), odim)`.\n\n<code_two>: The code being added is `torch.nn.Linear(odim * (((idim - 1) // 2 - 2) // 3), odim)`.\n\nFix_pattern: In the condition where the linear layer's input size needs to be modified based on the calculation `odim * (((idim - 1) // 2 - 1) // 3)`, the fix is to change the linear layer's input size from `odim * (((idim - 1) // 2 - 1) // 3)` to `odim * (((idim - 1) // 2 - 2) // 3)`."}
{"number": 707, "change": "class TFLiteBackendInferenceLearner(TensorflowBaseInferenceLearner):\nself.interpreter.set_tensor(i, input_tensor)\nself.interpreter.invoke()\nreturn tuple(\n-            self.interpreter.get_tensor(output_detail[\"index\"])\n+            tf.convert_to_tensor(\n+                self.interpreter.get_tensor(output_detail[\"index\"])\n+            )\nfor output_detail in output_details\n)\n", "fix_pattern": "<condition>: API misuse in the TensorFlow Lite backend inference learner\n<pattern>: The getter method self.interpreter.get_tensor() is used without converting the output to a TensorFlow tensor\n<code_one>: self.interpreter.get_tensor(output_detail[\"index\"])\n<code_two>: tf.convert_to_tensor(self.interpreter.get_tensor(output_detail[\"index\"]))\nFix_pattern: In the condition of API misuse in the TensorFlow Lite backend inference learner, if the getter method self.interpreter.get_tensor() is detected, then change the code self.interpreter.get_tensor(output_detail[\"index\"]) to tf.convert_to_tensor(self.interpreter.get_tensor(output_detail[\"index\"])) to fix the API misuse."}
{"number": 710, "change": "class Model(ModelDesc):\n.Conv2D('conv3.1', filters=128, padding='VALID') \\\n.Conv2D('conv3.2', filters=128, padding='VALID') \\\n.FullyConnected('fc0', 1024 + 512, activation=tf.nn.relu) \\\n-                .tf.nn.dropout(keep_prob) \\\n+                .Dropout(rate=drop_rate) \\\n.FullyConnected('fc1', 512, activation=tf.nn.relu) \\\n.FullyConnected('linear', out_dim=self.cifar_classnum)()\n", "fix_pattern": "<condition>: The code is using the tf.nn.dropout() function to apply dropout in the model.\n<pattern>: The pattern is that tf.nn.dropout() is replaced with the .Dropout() method.\n<code_one>: .tf.nn.dropout(keep_prob)\n<code_two>: .Dropout(rate=drop_rate)\nFix_pattern: In the condition of using tf.nn.dropout(), if the pattern of .tf.nn.dropout(keep_prob) is detected, then remove it and add the .Dropout(rate=drop_rate) method to fix the API misuse."}
{"number": 720, "change": "class CustomConverter(object):\nxs_pad = pad_list([torch.from_numpy(x).float() for x in xs], 0).to(device, dtype=self.dtype)\n\nilens = torch.from_numpy(ilens).to(device)\n-        # NOTE: this is for multi-task learning (e.g., speech translation)\n-        ys_pad = pad_list([torch.from_numpy(np.array(y[0]) if isinstance(y, tuple) else y).long()\n+        # NOTE: this is for multi-output (e.g., speech translation)\n+        ys_pad = pad_list([torch.from_numpy(np.array(y[0][:]) if isinstance(y, tuple) else y).long()\nfor y in ys], self.ignore_id).to(device)\n\nreturn xs_pad, ilens, ys_pad\n", "fix_pattern": "<condition>: The condition is related to multi-task learning or multi-output.\n\n<pattern>: The pattern is to modify the code that converts the input data to tensors.\n\n<code_one>: The code that needs to be modified is using the `ys` list directly without additional checks.\n\n<code_two>: The fixed code adds checks to handle cases where `ys` may contain tuples.\n\nFix_pattern:\nIn the condition of multi-task learning or multi-output, if the code uses `ys` list directly, then modify the code to handle cases where `ys` may contain tuples by adding additional checks."}
{"number": 725, "change": "def run_benchmark(state):\n\n\ndef on_state_reset():\n-    tf.keras.backend.set_value(model.optimizer.lr, lr * hvd.size())\n+    opt.lr.assign(lr * hvd.size())\n\n\nstate = hvd.elastic.TensorFlowKerasState(model, opt, img_secs=[], iter=0, batch=0, warm=False)\n", "fix_pattern": "<condition>:\nThe condition is when the learning rate of the TensorFlow model optimizer needs to be updated.\n\n<pattern>:\nThe pattern is to use the TensorFlow optimizer's `assign()` method to update the learning rate instead of using the `set_value()` method.\n\n<code_one>:\nThe code to be removed is `tf.keras.backend.set_value(model.optimizer.lr, lr * hvd.size())`.\n\n<code_two>:\nThe code to be added is `opt.lr.assign(lr * hvd.size())`.\n\nFix_pattern:\nIn the condition of needing to update the learning rate of the TensorFlow model optimizer, if the pattern of using `set_value()` to update the learning rate is detected, then change the code to use `assign()` method as `opt.lr.assign(lr * hvd.size())` to fix the API misuse."}
{"number": 727, "change": "def _interpret_blender_cameras(\n\nRpt3, Tpt3 = mtx[:, :3].split([3, 1], dim=0)\n\n-        focal_length_pt3 = torch.FloatTensor([[-focal, focal]])\n-        principal_point_pt3 = torch.FloatTensor([[W / 2, H / 2]])\n+        focal_length_pt3 = torch.FloatTensor([[focal, focal]])\n+        principal_point_pt3 = torch.FloatTensor([[0.0, 0.0]])\n\ncameras = PerspectiveCameras(\nfocal_length=focal_length_pt3,\n", "fix_pattern": "<condition>: The code is interpreting blender cameras.\n<pattern>: A focal length and principal point are being defined using torch.FloatTensor.\n<code_one>: The focal length_pt3 and principal_point_pt3 are defined with specific values.\n<code_two>: The focal length_pt3 and principal_point_pt3 are redefined with different values.\nFix_pattern: In the condition of interpreting blender cameras, if torch.FloatTensor is used to define focal length_pt3 and principal_point_pt3 with specific values, then re-define them with different values to fix the API misuse."}
{"number": 728, "change": "def vector_to_skew_symmetric_matrix(\na2s = vector_expanded[..., 1:2, :]\na3s = vector_expanded[..., 2:3, :]\n# BS x 1 x 1\n-    zs = torch.zeros(batch_shape + [1, 1], device=vector.device)\n+    zs = torch.zeros(batch_shape + [1, 1], device=vector.device, dtype=vector.dtype)\n# BS x 1 x 3\nrow1 = torch.cat((zs, -a3s, a2s), -1)\nrow2 = torch.cat((a3s, zs, -a1s), -1)\n", "fix_pattern": "<condition>: The code is missing a dtype argument in the torch.zeros function.\n<pattern>: The pattern is that the code is missing a dtype argument in the torch.zeros function.\n<code_one>: code_one is `zs = torch.zeros(batch_shape + [1, 1], device=vector.device)`\n<code_two>: code_two is `zs = torch.zeros(batch_shape + [1, 1], device=vector.device, dtype=vector.dtype)`\nFix_pattern: In the condition of missing a dtype argument in the torch.zeros function, if the pattern of missing the dtype argument is detected, then change the code_one `zs = torch.zeros(batch_shape + [1, 1], device=vector.device)` to code_two `zs = torch.zeros(batch_shape + [1, 1], device=vector.device, dtype=vector.dtype)` to fix the API misuse."}
{"number": 730, "change": "from pyro.ops.einsum import contract\ndef _finfo(tensor):\n# This can be replaced with torch.finfo once it is available\n# https://github.com/pytorch/pytorch/issues/10742\n-    return np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype).numpy().dtype)\n+    return np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype, device=\"cpu\").numpy().dtype)\n\n\ndef _check_batch_dims_are_sensible(output_dims, nonoutput_ordinal):\n", "fix_pattern": "<condition>: API misuse in the code\n<pattern>: The code is using the numpy's finfo function with a tensor argument\n<code_one>: return np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype).numpy().dtype)\n<code_two>: return np.finfo(torch.empty(torch.Size(), dtype=tensor.dtype, device=\"cpu\").numpy().dtype)\nFix_pattern: In the condition of API misuse, if the pattern of using np.finfo with a tensor argument is detected, then change the code to use the torch.empty function with the device argument set to \"cpu\"."}
{"number": 733, "change": "try:\nwith torch.cuda.device(x.device):\nreturn super().forward(x)\n\n+\nexcept ImportError:\nhas_fused_layernorm = False\n\n\ndef LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True, export=False):\n-    if torch.jit.is_scripting():\n+    if torch.jit.is_scripting() or torch.jit.is_tracing():\nexport = True\nif not export and torch.cuda.is_available() and has_fused_layernorm:\nreturn FusedLayerNorm(normalized_shape, eps, elementwise_affine)\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: In the condition of checking if the code is being scripted, if <pattern> is detected\n<code_one>: remove the line of code\n<code_two>: add the line of code\nFix_pattern: In the condition of checking if the code is being scripted, if the line \"torch.jit.is_scripting()\" is detected, then remove the line \"if torch.jit.is_scripting()\" to fix the API misuse and add the line \"if torch.jit.is_scripting() or torch.jit.is_tracing()\" to handle both scripting and tracing scenarios."}
{"number": 735, "change": "class TRPOModel(PolicyGradientModel):\n:param batch:\n:return:\n\"\"\"\n+        super(TRPOModel, self).update(batch)\n+\nself.feed_dict = {state: batch['states'][name] for name, state in self.state.items()}\nself.feed_dict.update({action: batch['actions'][name] for name, action in self.action.items()})\nself.feed_dict[self.reward] = batch['rewards']\n", "fix_pattern": "<condition>: When updating the TRPOModel class with a batch of data.\n<pattern>: Assigning values to the self.feed_dict dictionary using dictionary comprehensions.\n<code_one>: Assigning values to self.feed_dict using dictionary comprehensions.\n<code_two>: Calling the update() method of the parent class TRPOModel.\nFix_pattern: In the condition of updating the TRPOModel class with a batch of data, if assigning values to self.feed_dict using dictionary comprehensions is detected, then change the assignment statement to calling the update() method of the parent class TRPOModel to fix the API misuse."}
{"number": 740, "change": "def spline_gcn(\nrow = row.view(-1, 1).expand(row.size(0), output.size(1))\noutput = zero.scatter_add_(0, row, output)\n\n-    # Weighten root node features by multiplying with the meaned weights at the\n-    # origin.\n-    index = torch.arange(0, kernel_size[-1]).long()\n+    # Weighten root node features by multiplying with the meaned weights from\n+    # the origin.\n+    index = torch.arange(0, reduce(lambda x, y: x * y, kernel_size[1:])).long()\nroot_weight = weight[index].mean(0)\noutput += torch.mm(features, root_weight)\n", "fix_pattern": "<condition>: The fix pattern applies when there is a need to weighten root node features by multiplying them with meaned weights at the origin.\n<pattern>: The pattern is detecting the incorrect indexing of the weights array.\n<code_one>: The code that needs to be removed is the line that defines the index variable using the incorrect kernel_size.\n<code_two>: The correct code to be added is the line that defines the index variable using the correct kernel_size.\nFix_pattern: In the condition of needing to weighten root node features, if the incorrect indexing of the weights array is detected, then remove the line that defines the index variable incorrectly and add a new line that defines the index variable correctly based on the kernel_size."}
{"number": 742, "change": "def stats(policy, train_batch):\n\"cur_lr\": tf.cast(policy.cur_lr, tf.float64),\n\"policy_loss\": policy.loss.pi_loss,\n\"entropy\": policy.loss.entropy,\n-        \"var_gnorm\": tf.global_norm(policy.model.trainable_variables()),\n+        \"var_gnorm\": tf.linalg.global_norm(policy.model.trainable_variables()),\n\"vf_loss\": policy.loss.vf_loss,\n\"vf_explained_var\": explained_variance(\ntf.reshape(policy.loss.value_targets, [-1]),\n", "fix_pattern": "<condition>: No clear condition is needed\n<pattern>: tf.global_norm\n<code_one>: \"var_gnorm\": tf.global_norm(policy.model.trainable_variables())\n<code_two>: \"var_gnorm\": tf.linalg.global_norm(policy.model.trainable_variables())\nFix_pattern: In the condition where tf.global_norm is used, change \"var_gnorm\": tf.global_norm(policy.model.trainable_variables()) to \"var_gnorm\": tf.linalg.global_norm(policy.model.trainable_variables()) to fix the API misuse."}
{"number": 743, "change": "def test_tensorrt_torch(\nres_orig = tuple(model(*inputs_example))\nassert all(\n[\n-                    torch.allclose(res_tensor, res_orig_tensor, rtol=1e-01)\n+                    torch.allclose(\n+                        res_tensor.float(), res_orig_tensor, rtol=1e-01\n+                    )\nfor (res_tensor, res_orig_tensor) in zip(res, res_orig)\n]\n)\n", "fix_pattern": "<condition>: The condition of the fix pattern is when comparing two tensors in the code.\n<pattern>: The pattern is that the tensors are being compared using the `torch.allclose()` function.\n<code_one>: The code being removed is `torch.allclose(res_tensor, res_orig_tensor, rtol=1e-01)`.\n<code_two>: The code being added is `torch.allclose(res_tensor.float(), res_orig_tensor, rtol=1e-01)`.\nFix_pattern: In the condition of comparing two tensors using `torch.allclose()`, if the tensors are not of the same type, then change `res_tensor` to `res_tensor.float()` to fix the API misuse."}
{"number": 744, "change": "class Attention(nn.Module):\n# Apply the attention mask\nw = w + attention_mask\n\n-        w = nn.Softmax(dim=-1)(w)\n+        w = nn.functional.softmax(w, dim=-1)\nw = self.attn_dropout(w)\n\n# Mask heads if we want to\n", "fix_pattern": "<condition>: There is a softmax operation being used.\n<pattern>: The softmax operation is being called using the nn.Softmax(dim=-1) function.\n<code_one>: nn.Softmax(dim=-1)(w)\n<code_two>: nn.functional.softmax(w, dim=-1)\nFix_pattern: In the condition of using nn.Softmax(dim=-1) to apply softmax operation, replace nn.Softmax(dim=-1)(w) with nn.functional.softmax(w, dim=-1) to fix the API misuse."}
{"number": 752, "change": "class DistributedGroupSampler(Sampler):\nif size > 0:\nindice = np.where(self.flag == i)[0]\nassert len(indice) == size\n-                indice = indice[list(torch.randperm(int(size),\n-                                                    generator=g))].tolist()\n+                # add .numpy() to avoid bug when selecting indice in parrots.\n+                # TODO: check whether torch.randperm() can be replaced by\n+                # numpy.random.permutation().\n+                indice = indice[list(\n+                    torch.randperm(int(size), generator=g).numpy())].tolist()\nextra = int(\nmath.ceil(\nsize * 1.0 / self.samples_per_gpu / self.num_replicas)\n", "fix_pattern": "<condition>: The condition is \"if size > 0\".\n\n<pattern>: The pattern is \"torch.randperm(int(size), generator=g)\".\n\n<code_one>: The code being removed is \"indice = indice[list(torch.randperm(int(size), generator=g))].tolist()\".\n\n<code_two>: The code being added is \"indice = indice[list(torch.randperm(int(size), generator=g).numpy())].tolist()\".\n\nFix_pattern: In the condition of \"if size > 0\", if the pattern \"torch.randperm(int(size), generator=g)\" is detected, then remove the code \"indice = indice[list(torch.randperm(int(size), generator=g))].tolist()\" and change it to \"indice = indice[list(torch.randperm(int(size), generator=g).numpy())].tolist()\" to fix the API misuse."}
{"number": 759, "change": "def test_pair_norm(scale_individually):\nassert out1.size() == (100, 16)\n\nout2 = norm(torch.cat([x, x], dim=0), torch.cat([batch, batch + 1], dim=0))\n-    assert torch.allclose(out1, out2[:100])\n-    assert torch.allclose(out1, out2[100:])\n+    assert torch.allclose(out1, out2[:100], atol=1e-6)\n+    assert torch.allclose(out1, out2[100:], atol=1e-6)\n", "fix_pattern": "<condition>: The condition is when there is an API misuse in the code.\n<pattern>: The pattern is a specific error or bug in the code that needs to be fixed.\n<code_one>: The code that needs to be removed or changed to fix the API misuse.\n<code_two>: The code that needs to be added or modified to fix the API misuse.\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then <code_one> should be removed or changed to <code_two> to fix the API misuse."}
{"number": 764, "change": "class BlenderbotSmallEncoderLayer(nn.Module):\nhidden_states = residual + hidden_states\nhidden_states = self.final_layer_norm(hidden_states)\n\n-        if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\n+        if hidden_states.dtype == torch.float16 and (\n+            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n+        ):\nclamp_value = torch.finfo(hidden_states.dtype).max - 1000\nhidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n", "fix_pattern": "<condition>:\nNo pre condition is needed.\n\n<pattern>:\nThe pattern is to check if any element in the tensor `hidden_states` is infinity or NaN.\n\n<code_one>:\nThe code that needed to be removed is:\n```python\n        if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\n```\n\n<code_two>:\nThe code that needed to be added is:\n```python\n        if hidden_states.dtype == torch.float16 and (\n            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n        ):\n```\n\nFix_pattern:\nIn the condition of no pre condition, if the pattern of checking if any element in the tensor `hidden_states` is infinity or NaN is detected, then change the code logic from checking `torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()` to `hidden_states.dtype == torch.float16 and (torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any())` to fix the API misuse."}
{"number": 774, "change": "class QuantLinear(nn.Module):\nx_int = x / prev_act_scaling_factor\n\nreturn (\n-            F.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor,\n+            nn.functional.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor,\nbias_scaling_factor,\n)\n", "fix_pattern": "<condition>: The condition is not clearly identified in the provided context.\n\n<pattern>: The pattern is to replace the usage of the F.linear function with nn.functional.linear function.\n\n<code_one>: The code to be removed is \"F.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor.\"\n\n<code_two>: The code to be added is \"nn.functional.linear(x_int, weight=self.weight_integer, bias=self.bias_integer) * bias_scaling_factor.\"\n\nFix_pattern: In the condition where no clear condition is identified, if the pattern of using the F.linear function is detected, then replace the usage of F.linear with nn.functional.linear by removing the code_one and adding code_two to fix the API misuse."}
{"number": 779, "change": "class Importance(TracePosterior):\n\"\"\"\nif self.log_weights:\nlog_w_norm = self.get_normalized_weights(log_scale=True)\n-            ess = torch.exp(-logsumexp(2*log_w_norm, 0))\n+            ess = torch.exp(-torch.logsumexp(2*log_w_norm, 0))\nelse:\nwarnings.warn(\"The log_weights list is empty, effective sample size is zero.\")\ness = 0\n", "fix_pattern": "<condition>:\nThe condition is that the \"log_weights\" list is empty.\n\n<pattern>:\nThe pattern is using the logsumexp function with \"2*log_w_norm\" as input.\n\n<code_one>:\nThe code that needs to be removed is \"ess = torch.exp(-logsumexp(2*log_w_norm, 0))\".\n\n<code_two>:\nThe code that needs to be added is \"ess = torch.exp(-torch.logsumexp(2*log_w_norm, 0))\".\n\nFix_pattern:\nIn the condition of the \"log_weights\" list being empty, if the pattern of using the logsumexp function with \"2*log_w_norm\" is detected, then change the code \"ess = torch.exp(-logsumexp(2*log_w_norm, 0))\" to \"ess = torch.exp(-torch.logsumexp(2*log_w_norm, 0))\" to fix the API misuse."}
{"number": 780, "change": "for epoch in range(opt.niter):\nvutils.save_image(fake.data, 'fake_samples.png')\n\n# do checkpointing\n-    torch.save(netG.parameter_dict(), 'netG_epoch_%d.pth' % epoch)\n-    torch.save(netD.parameter_dict(), 'netD_epoch_%d.pth' % epoch)\n+    torch.save(netG.state_dict(), 'netG_epoch_%d.pth' % epoch)\n+    torch.save(netD.state_dict(), 'netD_epoch_%d.pth' % epoch)\n", "fix_pattern": "<condition>: The code is saving model parameters for both a generator (netG) and discriminator (netD) during each epoch.\n\n<pattern>: The code is saving model parameters using the \"parameter_dict()\" method.\n\n<code_one>:\ntorch.save(netG.parameter_dict(), 'netG_epoch_%d.pth' % epoch)\ntorch.save(netD.parameter_dict(), 'netD_epoch_%d.pth' % epoch)\n\n<code_two>:\ntorch.save(netG.state_dict(), 'netG_epoch_%d.pth' % epoch)\ntorch.save(netD.state_dict(), 'netD_epoch_%d.pth' % epoch)\n\nFix_pattern: In the condition of saving model parameters, if the \"parameter_dict()\" method is used, then it should be changed to the \"state_dict()\" method to fix the API misuse."}
{"number": 782, "change": "def dnn(tensor_in, hidden_units, activation=tf.nn.relu, keep_prob=None):\nwith tf.variable_scope('dnn'):\nfor i, n_units in enumerate(hidden_units):\nwith tf.variable_scope('layer%d' % i):\n-                tensor_in = linear.linear(tensor_in, n_units, True)\n+                tensor_in = linear(tensor_in, n_units, True)\ntensor_in = activation(tensor_in)\nif keep_prob:\ntensor_in = tf.nn.dropout(tensor_in, keep_prob)\n", "fix_pattern": "<condition>: The condition is when a linear transformation is applied to the tensor in the DNN model.\n\n<pattern>: The pattern is to replace the linear.linear() function with the linear() function.\n\n<code_one>: The code that is removed is \"tensor_in = linear.linear(tensor_in, n_units, True)\".\n\n<code_two>: The code that is added is \"tensor_in = linear(tensor_in, n_units, True)\".\n\nFix_pattern: In the condition of applying a linear transformation in the DNN model, if the pattern of using linear.linear() is detected, then remove the code \"tensor_in = linear.linear(tensor_in, n_units, True)\" and replace it with \"tensor_in = linear(tensor_in, n_units, True)\" to fix the API misuse."}
{"number": 783, "change": "class DSClipEncoder(torch.nn.Module):\nseq_len,\nseq_len,\ndtype=dtype,\n-                           device=torch.cuda.current_device())\n+                           device=get_accelerator().current_device_name())\nmask.fill_(torch.tensor(torch.finfo(dtype).min))\nmask.triu_(1)\nmask = mask.unsqueeze(1)\n", "fix_pattern": "<condition>: The code is using a specific compute device for tensor operations.\n<pattern>: The code is calling `torch.cuda.current_device()` to get the current CUDA device.\n<code_one>: `torch.cuda.current_device()`\n<code_two>: `get_accelerator().current_device_name()`\nFix_pattern: In the condition of using a specific compute device, if the pattern `torch.cuda.current_device()` is detected, then change `torch.cuda.current_device()` to `get_accelerator().current_device_name()` to fix the API misuse."}
{"number": 785, "change": "class CTC(torch.nn.Module):\nif self.ctc_type == \"warpctc\":\n# warpctc only supports float32\nys_hat = ys_hat.to(dtype=torch.float32)\n+        else:\n+            # use GPU when using the cuDNN implementation\n+            ys_true = to_device(self, ys_true)\nself.loss = to_device(self, self.loss_fn(ys_hat, ys_true, hlens, olens)).to(dtype=dtype)\nif self.reduce:\n# NOTE: sum() is needed to keep consistency since warpctc return as tensor w/ shape (1,)\n", "fix_pattern": "<condition>: The condition of the fix pattern is not specified in the given context.\n<pattern>: No specific pattern is detected in the provided code.\n<code_one>: No specific code is removed in the given code section.\n<code_two>: In the added code section, the code added is to_device(self, ys_true) with a comment explaining that it uses the GPU when using the cuDNN implementation.\nFix_pattern: Based on the given information, the fix pattern seems to be to add the code to_device(self, ys_true) in order to use the GPU when using the cuDNN implementation."}
{"number": 789, "change": "class AttentionDecoder(DecoderBase):\n])\nelse:\nattention_context = output.attention_context\n-    return tf.concat(1, [next_input, attention_context])\n+    return tf.concat_v2([next_input, attention_context], 1)\n\ndef _pad_att_scores(self, scores):\n\"\"\"Pads attention scores to fixed length. This is a hack because raw_rnn\n", "fix_pattern": "<condition>: \nThe condition when the fix pattern is applied is not explicitly mentioned in the given context.\n\n<pattern>:\nThe pattern that is detected is the use of the `tf.concat` function with the arguments in the wrong order.\n\n<code_one>:\nThe code that is removed is the use of `tf.concat(1, [next_input, attention_context])`.\n\n<code_two>:\nThe code that is added is the use of `tf.concat_v2([next_input, attention_context], 1)`.\n\nFix_pattern:\nIn the condition of <condition>, if the pattern of using `tf.concat` with the arguments in the wrong order is detected, then change the code from <code_one> to <code_two> to fix the API misuse."}
{"number": 792, "change": "class GPTJAttention(nn.Module):\n):\n# compute causal mask from causal mask buffer\nquery_length, key_length = query.size(-2), key.size(-2)\n-        causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].to(torch.bool)\n+        causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n\n# Keep the attention weights computation in fp32 to avoid overflow issues\nquery = query.to(torch.float32)\n", "fix_pattern": "<condition>: The code is inside a class called GPTJAttention.\n<pattern>: There is a line that converts the query tensor to float32.\n<code_one>: causal_mask is defined and converted to bool using to(torch.bool).\n<code_two>: causal_mask is defined without any data type conversion.\nFix_pattern: In the condition of GPTJAttention class, if the query tensor is converted to float32, then remove the conversion of causal_mask to boolean in order to fix the API misuse."}
{"number": 800, "change": "class GeneralizedChannelPermute(ConditionedGeneralizedChannelPermute, TransformM\nself.__delattr__('permutation')\n\n# Sample a random orthogonal matrix\n-        W, _ = torch.qr(torch.randn(channels, channels))\n+        W, _ = torch.linalg.qr(torch.randn(channels, channels))\n\n# Construct the partially pivoted LU-form and the pivots\nLU, pivots = W.lu()\n", "fix_pattern": "<condition>: None\n\n<pattern>: The use of `torch.qr` function to compute the QR decomposition of a matrix.\n\n<code_one>: `W, _ = torch.qr(torch.randn(channels, channels))`\n\n<code_two>: `W, _ = torch.linalg.qr(torch.randn(channels, channels))`\n\nFix_pattern: In the condition of no specific condition, if the use of `torch.qr` function is detected, then change the code `torch.qr` to `torch.linalg.qr` to fix the API misuse."}
{"number": 802, "change": "class TestConfusionMatrix:\nconf_mat = kornia.utils.metrics.confusion_matrix(\npredicted, actual, num_classes)\nconf_mat_real = torch.tensor(\n-            [[[3, 1],\n-              [0, 4]]], dtype=torch.float32)\n+            [\n+                [[3, 1], [0, 4]],\n+                [[3, 1], [0, 4]]\n+            ], dtype=torch.float32)\nassert_allclose(conf_mat, conf_mat_real)\n\ndef test_three_classes(self):\n", "fix_pattern": "<condition>: API misuse in the confusion_matrix function\n<pattern>: A single 2x2 tensor was passed as the input to the confusion_matrix function instead of a list of tensors.\n<code_one>: [[[3, 1], [0, 4]]], dtype=torch.float32\n<code_two>: [[3, 1], [0, 4]], [[3, 1], [0, 4]]\nFix_pattern: In the condition of API misuse in the confusion_matrix function, if a single 2x2 tensor is detected as input, then change the input from [[[3, 1], [0, 4]]] to [[3, 1], [0, 4]], [[3, 1], [0, 4]] to fix the API misuse."}
{"number": 803, "change": "def model(x, is_train, reuse):\n# nt = Conv2d(nin, 16, (3, 3), (2, 2), act=tf.nn.relu, padding='SAME', name='tc1')\n# nt = Conv2d(nt, 8, (3, 3), (2, 2), act=tf.nn.relu, padding='SAME', name='tc2')\n## 2. Spatial transformer module (sampler)\n-        n = tl.layers.SpatialTransformer2dAffineLayer(nin, theta_layer=nt, out_size=[40, 40], name='spatial')\n+        n = tl.layers.SpatialTransformer2dAffineLayer(nin, theta_layer=nt, out_size=(40, 40), name='spatial')\ns = n\n## 3. Classifier\nn = tl.layers.Conv2d(\n", "fix_pattern": "<condition>: The condition is that there is a function call to tl.layers.Conv2d.\n\n<pattern>: The pattern is that there is an incorrect argument provided for the \"out_size\" parameter in the tl.layers.SpatialTransformer2dAffineLayer function call.\n\n<code_one>: The code that is removed is \"out_size=[40, 40]\".\n\n<code_two>: The code that is added is \"out_size=(40, 40)\".\n\nFix_pattern: In the condition of a function call to tl.layers.Conv2d, if an incorrect argument is provided for the \"out_size\" parameter in the tl.layers.SpatialTransformer2dAffineLayer function call, then change the code from \"out_size=[40, 40]\" to \"out_size=(40, 40)\" to fix the API misuse."}
{"number": 804, "change": "class DeiTPreTrainedModel(PreTrainedModel):\ndef _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n\"\"\"Initialize the weights\"\"\"\nif isinstance(module, (nn.Linear, nn.Conv2d)):\n-            # Slightly different from the TF version which uses truncated_normal for initialization\n-            # cf https://github.com/pytorch/pytorch/pull/5617\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.weight.data = nn.init.trunc_normal_(module.weight.data, mean=0.0, std=self.config.initializer_range)\nif module.bias is not None:\nmodule.bias.data.zero_()\nelif isinstance(module, nn.LayerNorm):\n", "fix_pattern": "<condition>:\nThe condition is if the module is an instance of nn.Linear or nn.Conv2d.\n\n<pattern>:\nThe pattern is the initialization of the module's weight data using the normal distribution with a mean of 0.0 and a standard deviation equal to the initializer range specified in the configuration.\n\n<code_one>:\nThe code being removed is the original weight initialization using the normal distribution:\n\nmodule.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n\n<code_two>:\nThe code being added is the fixed weight initialization using the truncated normal distribution from the nn.init module:\n\nmodule.weight.data = nn.init.trunc_normal_(module.weight.data, mean=0.0, std=self.config.initializer_range)\n\nFix_pattern:\nIn the condition of if the module is an instance of nn.Linear or nn.Conv2d, if the weight initialization pattern using the normal distribution is detected, then change the code from module.weight.data.normal_(mean=0.0, std=self.config.initializer_range) to module.weight.data = nn.init.trunc_normal_(module.weight.data, mean=0.0, std=self.config.initializer_range) to fix the API misuse."}
{"number": 808, "change": "class DisentangledSelfAttention(nn.Module):\ndim=-1,\nindex=p2c_pos.squeeze(0).expand([query_layer.size(0), key_layer.size(-2), key_layer.size(-2)]),\n).transpose(-1, -2)\n-            score += p2c_att / scale\n+            score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)\n\nreturn score\n", "fix_pattern": "<condition>:\nThe condition is related to an API misuse in the code.\n\n<pattern>:\nThe pattern is the incorrect usage of an API that involves division operation.\n\n<code_one>:\nThe code that needs to be removed is the division operation without proper casting.\n\n<code_two>:\nThe code that needs to be added is the explicit casting of one operand to the appropriate data type.\n\nFix_pattern:\nIn the condition of an API misuse, if a pattern involving division operation is detected, then remove the code_one and add code_two to fix the API misuse."}
{"number": 812, "change": "def evaluate(model, data_loader, device):\nimage = list(img.to(device) for img in image)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n-        torch.cuda.synchronize(device)\n+        # \u5f53\u4f7f\u7528CPU\u65f6\uff0c\u8df3\u8fc7GPU\u76f8\u5173\u6307\u4ee4\n+        if device != torch.device(\"cpu\"):\n+            torch.cuda.synchronize(device)\n+\nmodel_time = time.time()\noutputs = model(image)\n", "fix_pattern": "<condition>: Using GPU for computation.\n<pattern>: Missing synchronization of GPU and CPU.\n<code_one>: torch.cuda.synchronize(device)\n<code_two>: if device != torch.device(\"cpu\"):\\n\\t\\t    torch.cuda.synchronize(device)\nFix_pattern: In the condition of using GPU for computation, if missing synchronization of GPU and CPU is detected, then add the code 'if device != torch.device(\"cpu\"):\\n\\t\\t    torch.cuda.synchronize(device)' to fix the API misuse."}
{"number": 813, "change": "class Layer_Lambda_Test(CustomTestCase):\nself.dense1 = tl.layers.Dense(in_channels=1, n_units=5)\nself.dense2 = tl.layers.Dense(in_channels=1, n_units=5)\nself.dense3 = tl.layers.Dense(in_channels=1, n_units=5)\n-                self.lambdalayer = tl.layers.ElementwiseLambda(customize_func, fn_weights=[], fn_args={'foo': 1024})\n+                self.lambdalayer = tl.layers.ElementwiseLambda(customize_func, fn_args={'foo': 1024})\n\ndef forward(self, x, bar=None):\nnoise = self.dense1(x)\n", "fix_pattern": "<condition>: When using the ElementwiseLambda function in the TL (TensorLayer) library.\n\n<pattern>: If the `fn_weights` argument is not provided when creating an instance of ElementwiseLambda.\n\n<code_one>: `fn_weights=[]`\n\n<code_two>: `fn_args={'foo': 1024}`\n\nFix_pattern: In the condition of using ElementwiseLambda in the TL library, if the `fn_weights` argument is not provided, then remove the `fn_weights=[]` code in order to fix the API misuse."}
{"number": 821, "change": "def clip_grad_norm_(params, max_norm, aggregate_norm_fn=None) -> torch.Tensor:\nif multi_tensor_l2norm_available:\ntotal_norm = multi_tensor_total_norm(grads)\nelse:\n-            warnings.warn(\n-                \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"\n-                \"you may get better performance by installing NVIDIA's apex library\"\n-            )\n+            if torch.cuda.is_available():\n+                warnings.warn(\n+                    \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"\n+                    \"you may get better performance by installing NVIDIA's apex library\"\n+                )\ntotal_norm = torch.norm(\ntorch.stack([torch.norm(g, p=2, dtype=torch.float32) for g in grads])\n)\n", "fix_pattern": "<condition>: Check if multi_tensor_l2norm is available.\n<pattern>: Display a warning if multi_tensor_l2norm is not available.\n<code_one>: Warn message printing.\n<code_two>: Warn message printing if torch.cuda.is_available() is True.\nFix_pattern: In the condition of checking if multi_tensor_l2norm is available, if it is not available, then change the warn message printing code to only execute if torch.cuda.is_available() is True."}
{"number": 823, "change": "class ModelSpeedupTensorRT(BaseModelSpeedup):\nModel input tensor\n\"\"\"\n# convert pytorch tensor to numpy darray\n+        if test_data.device != torch.device(\"cpu\"):\n+            test_data = test_data.to(\"cpu\")\ntest_data = test_data.numpy()\n# Numpy dtype should be float32\nassert test_data.dtype == np.float32\n", "fix_pattern": "<condition>: The condition is that the test_data should not be on the CPU device.\n<pattern>: The pattern is to check if the test_data is on the CPU device, and if not, move it to the CPU device.\n<code_one>: No code was removed in this fix.\n<code_two>: The code added is \"if test_data.device != torch.device(\"cpu\"): test_data = test_data.to(\"cpu\")\".\nFix_pattern: In the condition of the test_data not being on the CPU device, add the code \"if test_data.device != torch.device(\"cpu\"): test_data = test_data.to(\"cpu\")\" to fix the API misuse."}
{"number": 828, "change": "def main():\nif utils.is_primary(args):\n_logger.info('Using NVIDIA APEX AMP. Training in mixed precision.')\nelif use_amp == 'native':\n-        amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)\n-        if device.type == 'cuda':\n+        try:\n+            amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)\n+        except (AttributeError, TypeError):\n+            # fallback to CUDA only AMP for PyTorch < 1.10\n+            assert device.type == 'cuda'\n+            amp_autocast = torch.cuda.amp.autocast\n+        if device.type == 'cuda' and amp_dtype == torch.float16:\n+            # loss scaler only used for float16 (half) dtype, bfloat16 does not need it\nloss_scaler = NativeScaler()\nif utils.is_primary(args):\n_logger.info('Using native Torch AMP. Training in mixed precision.')\n", "fix_pattern": "<condition>:\nIf the device type is 'cuda' and the variable amp_dtype is torch.float16.\n  \n<pattern>:\nThe code is removed where the variable amp_autocast is assigned to partial(torch.autocast, device_type=device.type, dtype=amp_dtype) if the device type is 'cuda'.\n\n<code_one>:\namp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)\n\n<code_two>:\ntry:\n    amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)\nexcept (AttributeError, TypeError):\n    # fallback to CUDA only AMP for PyTorch < 1.10\n    assert device.type == 'cuda'\n    amp_autocast = torch.cuda.amp.autocast\n\nFix_pattern:\nIn the condition of the device type being 'cuda' and the variable amp_dtype being torch.float16, remove the code where amp_autocast is assigned to partial(torch.autocast, device_type=device.type, dtype=amp_dtype) and replace it with a try-except block that assigns amp_autocast to partial(torch.autocast, device_type=device.type, dtype=amp_dtype) if possible, and to torch.cuda.amp.autocast if an AttributeError or TypeError occurs."}
{"number": 829, "change": "class AutoOutsideCompilationWithKerasTest(tf.test.TestCase):\n\n\nif __name__ == '__main__':\n-  tf.compat.v1.enable_eager_execution()\ntf.test.main()\n", "fix_pattern": "<condition>: None\n<pattern>: tf.compat.v1.enable_eager_execution()\n<code_one>: tf.compat.v1.enable_eager_execution()\n<code_two>: \nFix_pattern: Remove tf.compat.v1.enable_eager_execution() to fix the API misuse."}
{"number": 837, "change": "class Encoder(torch.nn.Module):\npos_enc_class(attention_dim, positional_dropout_rate),\n)\nelif input_layer is None:\n-            self.embed = pos_enc_class(attention_dim, positional_dropout_rate)\n+            self.embed = torch.nn.Sequential(\n+                pos_enc_class(attention_dim, positional_dropout_rate)\n+            )\nelse:\nraise ValueError(\"unknown input_layer: \" + input_layer)\nself.normalize_before = normalize_before\n", "fix_pattern": "<condition>: When the input_layer is not None.\n<pattern>: The code expects a pos_enc_class instance to be assigned to the self.embed variable.\n<code_one>: self.embed = pos_enc_class(attention_dim, positional_dropout_rate)\n<code_two>: self.embed = torch.nn.Sequential(pos_enc_class(attention_dim, positional_dropout_rate))\nFix_pattern: In the condition where the input_layer is not None, the code is modified to wrap the pos_enc_class instantiation in a torch.nn.Sequential function call to fix the API misuse."}
{"number": 838, "change": "class ChineseCLIPVisionTransformer(nn.Module):\nembed_dim = config.hidden_size\n\nself.embeddings = ChineseCLIPVisionEmbeddings(config)\n-        self.pre_layrnorm = nn.LayerNorm(embed_dim)\n+        self.pre_layrnorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\nself.encoder = ChineseCLIPVisionEncoder(config)\n-        self.post_layernorm = nn.LayerNorm(embed_dim)\n+        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n\n@add_start_docstrings_to_model_forward(CHINESE_CLIP_VISION_INPUTS_DOCSTRING)\n@replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=ChineseCLIPVisionConfig)\n", "fix_pattern": "<condition>: The condition is not specified in the given information. No pre-condition is needed.\n\n<pattern>: The pattern is to change the initialization of `nn.LayerNorm` to include the `eps` argument.\n\n<code_one>: `self.pre_layrnorm = nn.LayerNorm(embed_dim)`, `self.post_layernorm = nn.LayerNorm(embed_dim)`\n\n<code_two>: `self.pre_layrnorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)`, `self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)`\n\nFix_pattern: In the condition of no pre-condition, if the code `self.pre_layrnorm = nn.LayerNorm(embed_dim)` and `self.post_layernorm = nn.LayerNorm(embed_dim)` is detected, then change it to `self.pre_layrnorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)` and `self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)` to fix the API misuse."}
{"number": 840, "change": "class TestSolveCast:\n\nclass TestSolveWithMask:\ndef test_smoke(self, device, dtype):\n+        torch.manual_seed(0)  # issue kornia#2027\nA = torch.randn(2, 3, 1, 4, 4, device=device, dtype=dtype)\nB = torch.randn(2, 3, 1, 4, 6, device=device, dtype=dtype)\n", "fix_pattern": "<condition>: When initializing a torch tensor.\n<pattern>: The shape of the tensor is being defined.\n<code_one>: The original shape of the tensor definition.\n<code_two>: The updated shape of the tensor definition.\nFix_pattern: In the condition of tensor initialization, if the shape of the tensor definition is incorrect, then change the original shape to the updated shape to fix the API misuse."}
{"number": 842, "change": "class TFHubertPreTrainedModel(TFPreTrainedModel):\ninput_signature=[\n{\n\"input_values\": tf.TensorSpec((None, None), tf.float32, name=\"input_values\"),\n-                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),\n-                \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),\n+                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),\n+                \"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),\n}\n]\n)\n", "fix_pattern": "<condition>: The condition is when defining the input signature for a class named TFHubertPreTrainedModel in a TensorFlow project.\n\n<pattern>: The pattern is detecting that \"attention_mask\" and \"token_type_ids\" are being defined in the input_signature dictionary.\n\n<code_one>: The code being removed is:\n'''\n\"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),\n\"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),\n'''\n\n<code_two>: The code being added is:\n'''\n\"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),\n\"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),\n'''\n\nFix_pattern: \nIn the condition of defining the input signature for a class named TFHubertPreTrainedModel in a TensorFlow project, if the pattern of defining \"attention_mask\" and \"token_type_ids\" with tf.TensorSpec((None, None), tf.int32, name=\"...\") is detected, then change the code from tf.int32 to tf.int64 to fix the API misuse."}
{"number": 845, "change": "def test_maskctc(encoder_arch, interctc_layer_idx, interctc_use_conditioning):\ninputs = dict(\nspeech=torch.randn(2, 10, 20, requires_grad=True),\nspeech_lengths=torch.tensor([10, 8], dtype=torch.long),\n-        text=torch.randint(0, vocab_size + 1, [2, 4], dtype=torch.long),\n+        text=torch.randint(2, 4, [2, 4], dtype=torch.long),\ntext_lengths=torch.tensor([4, 3], dtype=torch.long),\n)\nloss, *_ = model(**inputs)\n", "fix_pattern": "<condition>: The condition is not explicitly mentioned in the given context.\n<pattern>: The pattern is detecting the incorrect input range for the `text` argument in the `test_maskctc` function.\n<code_one>: The original code sets the `text` argument using `torch.randint(0, vocab_size + 1, [2, 4], dtype=torch.long)`.\n<code_two>: The fixed code sets the `text` argument using `torch.randint(2, 4, [2, 4], dtype=torch.long)`.\nFix_pattern: In the condition of the `test_maskctc` function, if an incorrect input range for `text` is detected, then change the code `torch.randint(0, vocab_size + 1, [2, 4], dtype=torch.long)` to `torch.randint(2, 4, [2, 4], dtype=torch.long)` to fix the API misuse."}
{"number": 847, "change": "def _add_gradients_summaries(grads_and_vars):\ngrad_values = grad.values\nelse:\ngrad_values = grad\n-      summaries.append(tf.histogram_summary(var.op.name + ':gradient',\n+      summaries.append(tf.summary.histogram(var.op.name + ':gradient',\ngrad_values))\n-      summaries.append(tf.histogram_summary(var.op.name + ':gradient_norm',\n+      summaries.append(tf.summary.histogram(var.op.name + ':gradient_norm',\ntf.global_norm([grad_values])))\nelse:\ntf.logging.info('Var %s has no gradient', var.op.name)\n", "fix_pattern": "<condition>: If the code is checking for the type of the variable.\n\n<pattern>: If the code is using deprecated TensorFlow functions.\n\n<code_one>: The deprecated TensorFlow function being used.\n\n<code_two>: The updated version of the TensorFlow function to be used.\n\nFix_pattern: In the condition of checking the type of the variable, if a deprecated TensorFlow function is detected, then replace it with the updated version of the TensorFlow function to fix the API misuse."}
{"number": 849, "change": "class QM9(InMemoryDataset):\nedge_type += 2 * [self.bonds[bond.GetBondType()]]\n\nedge_index = torch.tensor([row, col], dtype=torch.long)\n-            edge_type = torch.tensor(edge_type)\n-            edge_attr = F.one_hot(torch.tensor(edge_type),\n+            edge_type = torch.tensor(edge_type, dtype=torch.long)\n+            edge_attr = F.one_hot(edge_type,\nnum_classes=len(self.bonds)).to(torch.float)\n\nperm = (edge_index[0] * N + edge_index[1]).argsort()\n", "fix_pattern": "<condition>: The condition is not clearly specified in the context provided.\n<pattern>: Based on the provided context, it appears that the pattern is to convert the data type of the \"edge_type\" variable.\n<code_one>: The code that is removed is the conversion of \"edge_type\" to a tensor with the torch.tensor() function and applying the F.one_hot() function.\n<code_two>: The code that is added is the conversion of \"edge_type\" to a tensor with the dtype=torch.long argument and applying the F.one_hot() function.\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then change the data type of \"edge_type\" from the torch.tensor() function to include dtype=torch.long in order to fix the API misuse."}
{"number": 854, "change": "class MinSaver(Callback):\nnewname = os.path.join(logger.LOG_DIR,\nself.filename or\n('max-' + self.monitor_stat if self.reverse else 'min-' + self.monitor_stat))\n-        files_to_copy = glob.glob(path + '*')\n+        files_to_copy = tf.gfile.Glob(path + '*')\nfor file_to_copy in files_to_copy:\n-            shutil.copy(file_to_copy, file_to_copy.replace(path, newname))\n+            tf.gfile.Copy(file_to_copy, file_to_copy.replace(path, newname), overwrite=True)\nlogger.info(\"Model with {} '{}' saved.\".format(\n'maximum' if self.reverse else 'minimum', self.monitor_stat))\n", "fix_pattern": "<condition>: The code is using the \"glob\" module to copy files.\n<pattern>: The code is using the \"shutil.copy\" function to copy files.\n<code_one>: shutil.copy(file_to_copy, file_to_copy.replace(path, newname))\n<code_two>: tf.gfile.Copy(file_to_copy, file_to_copy.replace(path, newname), overwrite=True)\nFix_pattern: \nIn the condition of using the \"glob\" module to copy files, if using the \"shutil.copy\" function is detected, then remove the code `shutil.copy(file_to_copy, file_to_copy.replace(path, newname))` and add the code `tf.gfile.Copy(file_to_copy, file_to_copy.replace(path, newname), overwrite=True)` to fix the API misuse."}
{"number": 855, "change": "class Evaluator(object):\nThe mean average result per tensor over the entire dataset.\n\n\"\"\"\n+        tflearn.is_training(False, self.session)\ncoord = tf.train.Coordinator()\ninputs = tf.get_collection(tf.GraphKeys.INPUTS)\n# Data Preprocessing\n", "fix_pattern": "<condition>: When the API tflearn.is_training() is used to set the training status of the model.\n<pattern>: If tflearn.is_training() is set to True.\n<code_one>: No code is removed.\n<code_two>: tflearn.is_training(False, self.session)\nFix_pattern: In the condition of tflearn.is_training() being set to True, the fix is to change tflearn.is_training() to tflearn.is_training(False, self.session) to set the training status to False."}
{"number": 857, "change": "class Lamb(Optimizer):\nglobal_grad_norm.add_(grad.pow(2).sum())\n\nglobal_grad_norm = torch.sqrt(global_grad_norm)\n-        max_grad_norm = self.defaults['max_grad_norm']\n+        # FIXME it'd be nice to remove explicit tensor conversion of scalars when torch.where promotes\n+        # scalar types properly https://github.com/pytorch/pytorch/issues/9190\n+        max_grad_norm = torch.tensor(self.defaults['max_grad_norm'], device=device)\nclip_global_grad_norm = torch.where(\nglobal_grad_norm > max_grad_norm,\nglobal_grad_norm / max_grad_norm,\n", "fix_pattern": "<condition>: The condition is that the value of `max_grad_norm` needs to be used in the code.\n<pattern>: The pattern is that the value of `max_grad_norm` needs to be converted to a tensor before being used in the `torch.where` operation.\n<code_one>: The code that is removed is `max_grad_norm = self.defaults['max_grad_norm']`.\n<code_two>: The code that is added is `max_grad_norm = torch.tensor(self.defaults['max_grad_norm'], device=device)`.\nFix_pattern: In the condition of needing to use the value of `max_grad_norm`, if the value is detected without being a tensor, then convert it to a tensor using `max_grad_norm = torch.tensor(self.defaults['max_grad_norm'], device=device)` to fix the API misuse."}
{"number": 860, "change": "class ModelCatalogTest(unittest.TestCase):\ndef testCustomModel(self):\nray.init()\nModelCatalog.register_custom_model(\"foo\", CustomModel)\n-        p1 = ModelCatalog.get_model(1, 5, {\"custom_model\": \"foo\"})\n+        p1 = ModelCatalog.get_model(\n+            tf.constant([1, 2, 3]), 5, {\"custom_model\": \"foo\"})\nself.assertEqual(str(type(p1)), str(CustomModel))\n", "fix_pattern": "<condition>: The condition is that a custom model with the name \"foo\" needs to be registered in the ModelCatalog.\n<pattern>: The pattern is that the arguments passed to the ModelCatalog.get_model() function were incorrect.\n<code_one>: p1 = ModelCatalog.get_model(1, 5, {\"custom_model\": \"foo\"})\n<code_two>: p1 = ModelCatalog.get_model(tf.constant([1, 2, 3]), 5, {\"custom_model\": \"foo\"})\nFix_pattern: In the condition of having the custom model \"foo\" registered in the ModelCatalog, if the incorrect arguments pattern of passing 1 and 5 as arguments to ModelCatalog.get_model() is detected, then change the code to pass tf.constant([1, 2, 3]) and 5 as arguments to ModelCatalog.get_model() to fix the API misuse."}
{"number": 862, "change": "class _netD(nn.Module):\n\ndef forward(self, input):\ngpu_ids = None\n-        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n+        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > =1:\ngpu_ids = range(self.ngpu)\noutput = nn.parallel.data_parallel(self.main, input, gpu_ids)\nreturn output.view(-1, 1)\n", "fix_pattern": "<condition>: The ngpu parameter is present and there is a check for the input data type.\n<pattern>: The pattern is \"isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1\"\n<code_one>: The code being removed is the if statement with the pattern.\n<code_two>: The code being added is the if statement with the same pattern, but with >= instead of >.\nFix_pattern: In the condition of having the ngpu parameter available and checking for the input data type, if the pattern \"isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1\" is detected, then change the code from the pattern to \"isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu >= 1\" to fix the API misuse."}
{"number": 864, "change": "from nebullvm.transformations.base import BaseTransformation\n\n\nclass VerifyContiguity(BaseTransformation):\n-    def _transform(self, _input: torch.Tensor, **kwargs) -> Any:\n+    def _transform(self, _input: Any, **kwargs) -> Any:\n+        if not isinstance(_input, torch.Tensor):\n+            return _input\nif not _input.is_contiguous():\n_input = _input.contiguous()\nreturn _input\n", "fix_pattern": "<condition>: The condition is checking if the input tensor is not contiguous.\n<pattern>: The pattern is checking if the input is not an instance of torch.Tensor.\n<code_one>: The code that is being removed is the type hint annotation for the input parameter, which is of type torch.Tensor.\n<code_two>: The code that is being added is a check to see if the input is not an instance of torch.Tensor, and if so, it returns the input as is.\nFix_pattern: In the condition of checking if the input tensor is not contiguous, if the input is not an instance of torch.Tensor, then remove the type hint annotation for the input parameter and add a check to return the input as is if it is not an instance of torch.Tensor."}
{"number": 865, "change": "from tests import utils\ndef test_image_classifier(tmp_path):\ntrain_x = utils.generate_data(num_instances=320, shape=(32, 32, 3))\ntrain_y = utils.generate_one_hot_labels(num_instances=320, num_classes=10)\n-    clf = ak.ImageClassifier(directory=tmp_path, max_trials=2, seed=utils.SEED)\n+    clf = ak.ImageClassifier(\n+        directory=tmp_path,\n+        max_trials=2,\n+        seed=utils.SEED,\n+        distribution_strategy=tf.distribute.MirroredStrategy(),\n+    )\nclf.fit(train_x, train_y, epochs=1, validation_split=0.2)\nkeras_model = clf.export_model()\nclf.evaluate(train_x, train_y)\n", "fix_pattern": "<condition>:\nThe condition in this case is when training a neural network classifier for images using the AutoKeras library.\n\n<pattern>:\nThe pattern is that the distribution strategy for training the classifier was not specified.\n\n<code_one>:\nThe code that was removed is:\n```\nclf = ak.ImageClassifier(directory=tmp_path, max_trials=2, seed=utils.SEED)\n```\n\n<code_two>:\nThe code that was added is:\n```\nclf = ak.ImageClassifier(\n    directory=tmp_path,\n    max_trials=2,\n    seed=utils.SEED,\n    distribution_strategy=tf.distribute.MirroredStrategy(),\n)\n```\n\nFix_pattern:\nIn the condition of training an image classifier using AutoKeras, if the distribution strategy for training is not specified, then the code `clf = ak.ImageClassifier(directory=tmp_path, max_trials=2, seed=utils.SEED)` should be changed to `clf = ak.ImageClassifier(directory=tmp_path, max_trials=2, seed=utils.SEED, distribution_strategy=tf.distribute.MirroredStrategy())` to fix the API misuse."}
{"number": 867, "change": "def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\noutput_shape[3],\noutput_shape[1])\nif output_shape[0] is None:\n-        output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:])\n-        output_shape = tf.stack(list(output_shape))\n+        output_shape = (shape(x)[0],) + tuple(output_shape[1:])\n+\n+    output_shape = tf.stack(list(output_shape))\n\npadding = _preprocess_padding(padding)\nif tf_data_format == 'NHWC':\n", "fix_pattern": "<condition>: output_shape[0] is None\n<pattern>: Updating the way output_shape is calculated\n<code_one>: output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:])\n<code_two>: output_shape = (shape(x)[0],) + tuple(output_shape[1:])\nFix_pattern: In the condition of output_shape[0] is None, if the pattern output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:]) is detected, then change the code to output_shape = (shape(x)[0],) + tuple(output_shape[1:]) to fix the API misuse."}
{"number": 868, "change": "class TestRasterizeMeshes(TestCaseMixin, unittest.TestCase):\n], dtype=torch.int64, device=device)\n# fmt: on\n\n-        pix_to_face_padded = -torch.ones_like(pix_to_face_frontface)\n+        pix_to_face_padded = -(torch.ones_like(pix_to_face_frontface))\n# Run with and without culling\n# Without culling, for k=0, the front face (i.e. face 2) is\n# rasterized and for k=1, the back face (i.e. face 3) is\n", "fix_pattern": "<condition>: The condition is not mentioned in the context section.\n\n<pattern>: The pattern is detecting the variable 'pix_to_face_padded' and changing its value.\n\n<code_one>: The code that is being removed is \"pix_to_face_padded = -torch.ones_like(pix_to_face_frontface)\".\n\n<code_two>: The code that is being added is \"pix_to_face_padded = -(torch.ones_like(pix_to_face_frontface))\".\n\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then change the value of <code_one> to <code_two> to fix the API misuse."}
{"number": 871, "change": "class PipelineTest(test.SparkTest):\nimport tensorflow as tf\nfrom tensorflowonspark import TFNode\n\n+      tf.compat.v1.disable_eager_execution()\ntf.compat.v1.reset_default_graph()\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n", "fix_pattern": "<condition>: No pre condition is needed.\n<pattern>: No pattern detected.\n<code_one>: No code to be removed.\n<code_two>: tf.compat.v1.disable_eager_execution()\nFix_pattern: No fix pattern identified."}
{"number": 875, "change": "class SparkKerasTests(tf.test.TestCase):\n\ndef test_fit_model_multiclass(self):\nmodel = create_mnist_model()\n-        if version.parse(tf.keras.__version__) < version.parse(\"2.11\"):\n+        if version.parse(tf.keras.__version__.replace(\"-tf\", \"+tf\")) < version.parse(\"2.11\"):\noptimizer = tf.keras.optimizers.Adadelta(1.0)\nelse:\noptimizer = tf.keras.optimizers.legacy.Adadelta(1.0)\n", "fix_pattern": "<condition>: \nNo pre-condition is needed.\n\n<pattern>: \nCheck the version of a library.\n\n<code_one>: \nif version.parse(tf.keras.__version__) < version.parse(\"2.11\"):\n\n<code_two>: \nif version.parse(tf.keras.__version__.replace(\"-tf\", \"+tf\")) < version.parse(\"2.11\"):\n\nFix_pattern: \nIn the condition of checking the version of the library, if the original version check <code_one> is detected, then change it to <code_two> to fix the API misuse."}
{"number": 880, "change": "def create_meshgrid(height, width, normalized_coordinates=True):\nelse:\nxs = torch.linspace(0, width - 1, width)\nys = torch.linspace(0, height - 1, height)\n-    return torch.stack(torch.meshgrid([ys, xs])).view(1, 2, -1)\n+    return torch.stack(torch.meshgrid([ys, xs])).view(1, 2, -1)[:, (1, 0), :]\n\n\nclass HomographyWarper(nn.Module):\n", "fix_pattern": "<condition>: normalized_coordinates parameter is set to True\n<pattern>: The code is returning a stack of meshgrid tensors using torch.stack(torch.meshgrid([ys, xs])) and then reshaping it using view(1, 2, -1). However, it is not transposing the dimensions correctly.\n<code_one>: return torch.stack(torch.meshgrid([ys, xs])).view(1, 2, -1)\n<code_two>: return torch.stack(torch.meshgrid([ys, xs])).view(1, 2, -1)[:, (1, 0), :]\nFix_pattern: In the condition of normalized_coordinates=True, if incorrect transposition of dimensions is detected in the returned stack of meshgrid tensors, then change the code from return torch.stack(torch.meshgrid([ys, xs])).view(1, 2, -1) to return torch.stack(torch.meshgrid([ys, xs])).view(1, 2, -1)[:, (1, 0), :] to fix the API misuse."}
{"number": 882, "change": "def main():\n# download url: https://download.pytorch.org/models/resnet34-333f7ec4.pth\nmodel_weight_path = \"./resnet34-pre.pth\"\nassert os.path.exists(model_weight_path), \"file {} does not exist.\".format(model_weight_path)\n-    net.load_state_dict(torch.load(model_weight_path, map_location=device))\n+    net.load_state_dict(torch.load(model_weight_path, map_location='cpu'))\n# for param in net.parameters():\n#     param.requires_grad = False\n", "fix_pattern": "<condition>: The code is trying to load a state dictionary using the `load_state_dict` function.\n<pattern>: The model_weight_path is not being mapped to the correct device. \n<code_one>: `map_location=device`\n<code_two>: `map_location='cpu'`\nFix_pattern: In the condition of loading a state dictionary using the `load_state_dict` function, if the model_weight_path is not mapped correctly, then change the `map_location` argument from `device` to `'cpu'` to fix the API misuse."}
{"number": 883, "change": "def triangular_solve(x, y, upper=False, transpose=False):\n\n\ndef precision_to_scale_tril(P):\n-    Lf = torch.cholesky(torch.flip(P, (-2, -1)))\n+    Lf = torch.linalg.cholesky(torch.flip(P, (-2, -1)))\nL_inv = torch.transpose(torch.flip(Lf, (-2, -1)), -2, -1)\nL = torch.triangular_solve(torch.eye(P.shape[-1], dtype=P.dtype, device=P.device),\nL_inv, upper=False)[0]\n", "fix_pattern": "<condition>: The condition is not clear from the given context.\n<pattern>: The pattern is to replace the usage of the deprecated function `torch.cholesky` with the recommended function `torch.linalg.cholesky`.\n<code_one>: The code that needs to be removed is `Lf = torch.cholesky(torch.flip(P, (-2, -1)))`.\n<code_two>: The code that needs to be added is `Lf = torch.linalg.cholesky(torch.flip(P, (-2, -1)))`.\nFix_pattern: In the condition where the deprecated function `torch.cholesky` is used, replace `torch.cholesky` with `torch.linalg.cholesky` to fix the API misuse."}
{"number": 887, "change": "def _to_ivy(x: Any) -> Any:\n\n\ndef _to_ivy_array(x: Any) -> ivy.Array:\n-    if isinstance(x, (torch.Tensor, tf.Tensor, jnp.numpy.DeviceArray, numpy.ndarray)):\n+    if isinstance(x, (torch.Tensor, tf.Tensor, jnp.DeviceArray, numpy.ndarray)):\nreturn ivy.array(numpy.array(x))\nreturn x\n", "fix_pattern": "<condition>: The code is checking if a variable is an instance of specific types (torch.Tensor, tf.Tensor, jnp.numpy.DeviceArray, numpy.ndarray).\n<pattern>: The pattern is checking the variable type using the isinstance function.\n<code_one>: The code originally checked if x is an instance of (torch.Tensor, tf.Tensor, jnp.numpy.DeviceArray, numpy.ndarray).\n<code_two>: The code was updated to check if x is an instance of (torch.Tensor, tf.Tensor, jnp.DeviceArray, numpy.ndarray).\nFix_pattern: In the condition of checking variable type using isinstance, if the specific type jnp.numpy.DeviceArray is detected, then remove it and use jnp.DeviceArray instead to fix the API misuse."}
{"number": 888, "change": "def vecdot(\nout: Optional[Union[tf.Tensor, tf.Variable]] = None,\n) -> Union[tf.Tensor, tf.Variable]:\ndtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))\n-    x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)\n+    if dtype != \"float64\":\n+        x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)\nret = tf.cast(tf.tensordot(x1, x2, axes=(axis, axis)), dtype)\nreturn ret\n", "fix_pattern": "<condition>: When the dtype is not equal to \"float64\"\n<pattern>: Cast the variables x1 and x2 to tf.float32\n<code_one>: x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)\n<code_two>: x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)\nFix_pattern: In the condition of dtype not equal to \"float64\", if detected, then change the code x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32) to x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32) to fix the API misuse."}
{"number": 895, "change": "class SinusoidalPositionalEmbedding(nn.Module):\nself.embedding_dim,\nself.padding_idx,\n).type_as(self.weights)\n+        self.weights = self.weights.type_as(self._float_tensor)\nweights = Variable(self.weights)\n\nif incremental_state is not None:\n", "fix_pattern": "<condition>: The code is inside the class SinusoidalPositionalEmbedding within a conditional statement checking if the incremental_state is not None.\n<pattern>: The variable weights is being re-assigned.\n<code_one>: `weights = Variable(self.weights)`\n<code_two>: `self.weights = self.weights.type_as(self._float_tensor)`\nFix_pattern: In the condition of incremental_state is not None, if the variable weights is being re-assigned, then change the code from `weights = Variable(self.weights)` to `self.weights = self.weights.type_as(self._float_tensor)` to fix the API misuse."}
{"number": 901, "change": "class AdditiveSharingTensor(AbstractTensor):\nrandom_shares = [random_type(secret.shape) for _ in range(n_workers - 1)]\n\nfor share in random_shares:\n-            share.random_(-field, field)\n+            share.random_(int(-field/2), int(field/2)-1)\n\nshares = []\nfor i in range(n_workers):\n", "fix_pattern": "<condition>: The code is generating random shares for an additive sharing tensor.\n<pattern>: The random values for the shares are being generated using the `random_` method.\n<code_one>: `share.random_(-field, field)`\n<code_two>: `share.random_(int(-field/2), int(field/2)-1)`\nFix_pattern: In the condition of generating random shares for an additive sharing tensor, if the code is using the `random_` method with the range of `-field` to `field`, then change it to use the `random_` method with the range of `int(-field/2)` to `int(field/2)-1` to fix the API misuse."}
{"number": 902, "change": "class PipelineEngine(DeepSpeedEngine):\nmem_cached = new_cached\nmem_alloced = new_alloced\n\n-        max_alloced = torch.cuda.max_memory_allocated()\n-        max_cached = torch.cuda.max_memory_cached()\n+        max_alloced = get_accelerator().max_memory_allocated()\n+        max_cached = get_accelerator().max_memory_cached()\n\n# convert to GB for printing\nnew_alloced /= 1024**3\n", "fix_pattern": "<condition>: The code is using the torch.cuda module to retrieve information about maximum memory usage.\n<pattern>: The code is accessing the \"max_memory_allocated()\" and \"max_memory_cached()\" methods from the torch.cuda module directly.\n<code_one>: max_alloced = torch.cuda.max_memory_allocated()\n             max_cached = torch.cuda.max_memory_cached()\n<code_two>: max_alloced = get_accelerator().max_memory_allocated()\n             max_cached = get_accelerator().max_memory_cached()\nFix_pattern: In the condition of using the torch.cuda module, if the pattern of directly accessing \"max_memory_allocated()\" and \"max_memory_cached()\" methods is detected, then change the code to use the \"get_accelerator()\" function to retrieve the same information."}
{"number": 903, "change": "def torch_multinomial(input, num_samples, replacement=False):\nDoes not support keyword argument `out`.\n\"\"\"\nif input.is_cuda:\n-        return torch_multinomial(input.cpu(), num_samples, replacement).cuda()\n+        return torch.multinomial(input.cpu(), num_samples, replacement).cuda(input.get_device())\nelse:\nreturn torch.multinomial(input, num_samples, replacement)\n", "fix_pattern": "<condition>: The condition is if the input tensor is on CUDA.\n\n<pattern>: The pattern is to replace the code that moves the input tensor to CPU and then back to CUDA with code that directly moves the input tensor to the appropriate CUDA device.\n\n<code_one>: The code that is removed is \"input.cpu()\".\n\n<code_two>: The code that is added is \"input.cuda(input.get_device())\".\n\nFix_pattern: In the condition of the input tensor being on CUDA, if the code attempts to move the tensor to CPU and then back to CUDA, replace the code \"input.cpu()\" with \"input.cuda(input.get_device())\" to fix the API misuse."}
{"number": 904, "change": "def test_delete_entire_dataset(domain_owner, cleanup_storage):\nassert domain_owner.datasets[0].name == \"Dataset_1\"\nassert domain_owner.datasets[1].name == \"Dataset_2\"\n\n-    domain_owner.datasets.delete(dataset_id=domain_owner.datasets[0].id)\n+    domain_owner.datasets.delete(\n+        dataset_id=domain_owner.datasets[0].id, skip_checks=True\n+    )\n\n# Check if the number of available datasets has been decreased\nassert len(domain_owner.datasets) == 1\n", "fix_pattern": "<condition>: There is a need to delete a dataset owned by the domain owner.\n<pattern>: The method call to delete the dataset is missing a parameter 'skip_checks'.\n<code_one>: domain_owner.datasets.delete(dataset_id=domain_owner.datasets[0].id)\n<code_two>: domain_owner.datasets.delete(dataset_id=domain_owner.datasets[0].id, skip_checks=True)\nFix_pattern: In the condition of needing to delete a dataset owned by the domain owner, if the method call to delete the dataset is missing the 'skip_checks' parameter, then add 'skip_checks=True' to the method call to fix the API misuse."}
{"number": 905, "change": "def logspace(\nbase=10.0,\naxis=None,\n*,\n+    dtype: torch.dtype,\ndevice: torch.device,\nout: Optional[torch.Tensor] = None,\n):\n-    power_seq = linspace(\n-        start, stop, num, axis, dtype=None, device=default_device(device)\n+    power_seq = ivy.linspace(\n+        start, stop, num, axis, dtype=dtype, device=ivy.default_device(device)\n)\nreturn base**power_seq\n", "fix_pattern": "<condition>: The condition is not clear in the given context.\n\n<pattern>: The pattern detected is a code change.\n\n<code_one>: The code that was removed is `power_seq = linspace(start, stop, num, axis, dtype=None, device=default_device(device))`.\n\n<code_two>: The code that was added is `dtype: torch.dtype, power_seq = ivy.linspace(start, stop, num, axis, dtype=dtype, device=ivy.default_device(device))`.\n\nFix_pattern: In the condition of an unclear condition, if a code change is detected where the code `power_seq = linspace(start, stop, num, axis, dtype=None, device=default_device(device))` is removed, then it should be replaced with `dtype: torch.dtype, power_seq = ivy.linspace(start, stop, num, axis, dtype=dtype, device=ivy.default_device(device))` to fix the API misuse."}
{"number": 908, "change": "def pack(\ntry:\nimport torch\n\n-        meta_objs.update(torch=torch.__version__)\n+        meta_objs.update(torch=str(torch.__version__))\nexcept ImportError:\npass\ntry:\n", "fix_pattern": "Condition: ImportError is raised when attempting to import the \"torch\" module.\nPattern: Updating the value of the \"torch\" key in the \"meta_objs\" dictionary.\nCode one: meta_objs.update(torch=torch.__version__)\nCode two: meta_objs.update(torch=str(torch.__version__))\nFix_pattern: In the condition of ImportError, if the pattern of updating the \"torch\" key in the \"meta_objs\" dictionary with the value of \"torch.__version__\" is detected, then change the code one to code two to fix the API misuse."}
{"number": 909, "change": "def main():\n# recog\nlogging.info('backend = ' + args.backend)\nif args.backend == \"chainer\":\n-        from espnet.lmchainer.asr_chainer import recog\n+        from espnet.asr.chainer.asr_chainer import recog\nrecog(args)\nelif args.backend == \"pytorch\":\n-        from espnet.lmpytorch.asr_pytorch import recog\n+        from espnet.asr.pytorch.asr_pytorch import recog\nrecog(args)\nelse:\nraise ValueError(\"Only chainer and pytorch are supported.\")\n", "fix_pattern": "<condition>: Checking the value of the \"args.backend\" variable.\n<pattern>: Importing the \"recog\" function from different modules based on the value of \"args.backend\".\n<code_one>: \n        from espnet.lmchainer.asr_chainer import recog\n        from espnet.lmpytorch.asr_pytorch import recog\n<code_two>:\n        from espnet.asr.chainer.asr_chainer import recog\n        from espnet.asr.pytorch.asr_pytorch import recog\nFix_pattern: In the condition of checking the value of \"args.backend\", if the \"recog\" function is imported from \"espnet.lmchainer.asr_chainer\" or \"espnet.lmpytorch.asr_pytorch\", then it needs to be changed to import from \"espnet.asr.chainer.asr_chainer\" or \"espnet.asr.pytorch.asr_pytorch\" to fix the API misuse."}
{"number": 910, "change": "class Schedule(metaclass=ABCMeta):\nraise NotImplementedError\n\ndef value(self, t):\n-        if self.framework == \"tf\" and tf.executing_eagerly() is False:\n+        if self.framework == \"tf\":\nreturn tf.cast(\n-                tf.py_func(self._value, [t], tf.float64),\n+                tf.py_function(self._value, [t], tf.float64),\ntf.float32,\n-                name=\"schedule-value\")\n+                name=\"schedule_value\")\nreturn self._value(t)\n\ndef __call__(self, t):\n", "fix_pattern": "<condition>: The condition is that the framework variable is equal to \"tf\".\n<pattern>: The pattern is the use of tf.executing_eagerly() to check if eager execution is enabled.\n<code_one>: The code that is being removed is tf.py_func(self._value, [t], tf.float64), with the name \"schedule-value\".\n<code_two>: The code that is being added is tf.py_function(self._value, [t], tf.float64), with the name \"schedule_value\".\nFix_pattern: In the condition of the framework variable being equal to \"tf\", if tf.executing_eagerly() is used, then replace tf.py_func(self._value, [t], tf.float64) with tf.py_function(self._value, [t], tf.float64) and change the name from \"schedule-value\" to \"schedule_value\" to fix the API misuse."}
{"number": 912, "change": "class FeedForwardTransformer(TTSInterface, torch.nn.Module):\n\n# concat speaker embedding\nif self.spk_embed_dim is not None:\n-            spembs = torch.nn.functional.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)\n-            hs = self.projection(torch.cat([hs, spembs], dim=-1))\n+            spembs_ = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)\n+            hs = self.projection(torch.cat([hs, spembs_], dim=-1))\n\n# forward duration predictor and length regulator\nd_masks = make_pad_mask(ilens).to(xs.device)\n", "fix_pattern": "<condition>: The condition is when the `spk_embed_dim` is not None.\n<pattern>: The pattern is to normalize the `spembs` tensor using `torch.nn.functional.normalize()`, unsqueeze it along the first dimension, and expand it to match the shape of `hs`. Then, concatenate `hs` and `spembs` along the last dimension.\n<code_one>: The code that was removed is `spembs = torch.nn.functional.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)\\nhs = self.projection(torch.cat([hs, spembs], dim=-1))`\n<code_two>: The code that was added is `spembs_ = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)\\nhs = self.projection(torch.cat([hs, spembs_], dim=-1))`\nFix_pattern: In the condition of `spk_embed_dim` not being None, if the `spembs` tensor needs to be normalized and concatenated with `hs`, the code needs to be changed from `spembs = torch.nn.functional.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)\\nhs = self.projection(torch.cat([hs, spembs], dim=-1))` to `spembs_ = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)\\nhs = self.projection(torch.cat([hs, spembs_], dim=-1))` to fix the API misuse."}
{"number": 919, "change": "def image_histogram2d(\nhist = hist.squeeze()\nelif image.dim() == 3:\nhist = hist.squeeze(0)\n-    return hist, torch.zeros_like(hist, dtype=hist.dtype, device=device)\n+    return hist, torch.zeros_like(hist, dtype=hist.dtype, device=hist.device)\n", "fix_pattern": "<condition>: The condition is checking the dimension of an image.\n<pattern>: The pattern is removing the squeeze operation on the histogram.\n<code_one>: The code being removed is \"hist = hist.squeeze()\".\n<code_two>: The code being added is \"hist = hist.squeeze(0)\".\nFix_pattern: In the condition of checking the dimension of an image, if the squeeze operation on the histogram is detected, then remove \"hist = hist.squeeze()\" and add \"hist = hist.squeeze(0)\" to fix the API misuse."}
{"number": 920, "change": "class FeedForwardEncoder(Seq2SeqEncoder):\nreturn self._feedforward(inputs)\nelse:\noutputs = self._feedforward(inputs)\n-            return outputs * mask.unsqueeze(dim=-1).float()\n+            return outputs * mask.unsqueeze(dim=-1)\n", "fix_pattern": "<condition>: The code snippet is inside an if-else statement.\n<pattern>: The code snippet multiplies the 'outputs' variable by 'mask.unsqueeze(dim=-1).float()'.\n<code_one>: 'outputs * mask.unsqueeze(dim=-1).float()'\n<code_two>: 'outputs * mask.unsqueeze(dim=-1)'\nFix_pattern: In the condition of an if-else statement, if the code snippet that multiplies 'outputs' by 'mask.unsqueeze(dim=-1).float()' is detected, then remove '.float()' from the code snippet to fix the API misuse."}
{"number": 921, "change": "def elastic_transform2d(\nsigma_t = sigma.to(device=device, dtype=dtype)\n\n# Get Gaussian kernel for 'y' and 'x' displacement\n-    kernel_x: torch.Tensor = get_gaussian_kernel2d_t(kernel_size, sigma_t[0].expand(2).unsqueeze(0))\n-    kernel_y: torch.Tensor = get_gaussian_kernel2d_t(kernel_size, sigma_t[1].expand(2).unsqueeze(0))\n+    kernel_x: torch.Tensor = get_gaussian_kernel2d(kernel_size, sigma_t[0].expand(2).unsqueeze(0))\n+    kernel_y: torch.Tensor = get_gaussian_kernel2d(kernel_size, sigma_t[1].expand(2).unsqueeze(0))\n\n# Convolve over a random displacement matrix and scale them with 'alpha'\ndisp_x: torch.Tensor = noise[:, :1]\n", "fix_pattern": "<condition>: The code is using the function `get_gaussian_kernel2d_t` to get Gaussian kernels for 'x' and 'y' displacement.\n\n<pattern>: The pattern is that the code is using the function `get_gaussian_kernel2d_t` to obtain Gaussian kernels.\n\n<code_one>: The code is using `kernel_x` and `kernel_y` to store the Gaussian kernels obtained using `get_gaussian_kernel2d_t`.\n\n<code_two>: The code replaces `get_gaussian_kernel2d_t` with `get_gaussian_kernel2d` to obtain the Gaussian kernels.\n\nFix_pattern: In the condition of using `get_gaussian_kernel2d_t` to obtain Gaussian kernels, the code is changed to use `get_gaussian_kernel2d` instead to fix the API misuse."}
{"number": 929, "change": "def rand_like_with_shape(shape, ori_t):\nhigher_bound = torch.max(ori_t)\n\nif dtype in [torch.uint8, torch.int16, torch.short, torch.int16, torch.long, torch.bool]:\n-        return torch.randint(lower_bound, higher_bound+1, shape, dtype=dtype, device=device)\n+        return torch.randint(lower_bound.long(), higher_bound.long() + 1, shape, dtype=dtype, device=device)\nelse:\nreturn torch.rand(shape, dtype=dtype, device=device, requires_grad=require_grad)\n", "fix_pattern": "<condition>: If the dtype is not in the list [torch.uint8, torch.int16, torch.short, torch.int16, torch.long, torch.bool]\n<pattern>: Remove the code that uses torch.randint() and replace it with torch.rand()\n<code_one>: return torch.randint(lower_bound, higher_bound+1, shape, dtype=dtype, device=device)\n<code_two>: return torch.randint(lower_bound.long(), higher_bound.long() + 1, shape, dtype=dtype, device=device)\nFix_pattern: In the condition of dtype not being in a specific list, if the code that uses torch.randint() is detected, then replace the code with torch.rand() to fix the API misuse."}
{"number": 933, "change": "class SelfAttnFunc(torch.autograd.Function):\nvalues_grads   = torch.bmm(dropout_results.transpose(1,2), output_lin_grads, out=values_grads.transpose(0,1))\n\n# Mask and Scaling for Dropout (not a publically documented op)\n-        dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, dropout_prob_t[0])\n+        dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, 1.0/(1.0-dropout_prob_t[0]))\n\n# Softmax Grad (not a publically documented op)\nsoftmax_grads = torch._softmax_backward_data(dropout_grads, softmax_results, -1, softmax_results)\n", "fix_pattern": "<condition>: When performing a backward pass in a self-attention mechanism.\n<pattern>: Detecting the use of the torch._masked_scale function with dropout.\n<code_one>: dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, dropout_prob_t[0])\n<code_two>: dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, 1.0/(1.0-dropout_prob_t[0]))\nFix_pattern: In the condition of performing a backward pass in a self-attention mechanism, if the use of torch._masked_scale with dropout is detected, then change the code from dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, dropout_prob_t[0]) to dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, 1.0/(1.0-dropout_prob_t[0])) to fix the API misuse."}
{"number": 934, "change": "class ValidationEpochEndVariations(ABC):\n\nval_acc_mean += val_acc\n\n-        val_loss_mean /= len(outputs)\n-        val_acc_mean /= len(outputs)\n+        if outputs:  # skip zero divisions\n+            val_loss_mean /= len(outputs)\n+            val_acc_mean /= len(outputs)\n\nmetrics_dict = {'val_loss': val_loss_mean.item(), 'val_acc': val_acc_mean.item()}\nresults = {'progress_bar': metrics_dict, 'log': metrics_dict}\n", "fix_pattern": "<condition>: The condition is checking if the variable \"outputs\" is not empty.\n\n<pattern>: The pattern that is detected is a potential division by zero error. \n\n<code_one>: The code that is removed is the division of \"val_loss_mean\" and \"val_acc_mean\" by the length of \"outputs\". \n\n<code_two>: The code that is added is a check to skip the division when \"outputs\" is empty.\n\nFix_pattern: In the condition of checking if \"outputs\" is not empty, if a potential division by zero error is detected, then the division of \"val_loss_mean\" and \"val_acc_mean\" by the length of \"outputs\" is removed and a check to skip the division when \"outputs\" is empty is added to fix the API misuse."}
{"number": 935, "change": "class AutoShape(nn.Module):\n#   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images\n\nt = [time_sync()]\n-        p = next(self.model.parameters()) if self.pt else torch.zeros(1)  # for device and type\n+        p = next(self.model.parameters()) if self.pt else torch.zeros(1, device=self.model.device)  # for device, type\nautocast = self.amp and (p.device.type != 'cpu')  # Automatic Mixed Precision (AMP) inference\nif isinstance(imgs, torch.Tensor):  # torch\nwith amp.autocast(autocast):\n", "fix_pattern": "<condition>:\n\nThe condition is that the API misuse occurs when using the `next()` function on a model's parameters.\n\n<pattern>:\n\nThe pattern is the incorrect usage of `torch.zeros(1)` as the default value for `torch.zeros(1, device=self.model.device)` is not provided.\n\n<code_one>:\n\n`p = next(self.model.parameters()) if self.pt else torch.zeros(1)`\n\n<code_two>:\n\n`p = next(self.model.parameters()) if self.pt else torch.zeros(1, device=self.model.device)`\n\nFix_pattern:\n\nIn the condition of using `next()` on a model's parameters, if the pattern of incorrectly using `torch.zeros(1)` is detected, then change the code from `torch.zeros(1)` to `torch.zeros(1, device=self.model.device)` to fix the API misuse."}
{"number": 936, "change": "class CycleDiffusionPipeline(DiffusionPipeline):\n\ndevice = torch.device(f\"cuda:{gpu_id}\")\n\n-        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.safety_checker]:\n+        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\nif cpu_offloaded_model is not None:\ncpu_offload(cpu_offloaded_model, device)\n\n+        if self.safety_checker is not None:\n+            # TODO(Patrick) - there is currently a bug with cpu offload of nn.Parameter in accelerate\n+            # fix by only offloading self.safety_checker for now\n+            cpu_offload(self.safety_checker.vision_model)\n+\n@property\n# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline._execution_device\ndef _execution_device(self):\n", "fix_pattern": "<condition>: cpu_offloaded_model is not None\n<pattern>: The code is looping through a list of models and offloading them to the CPU using the cpu_offload() function.\n<code_one>: for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae, self.safety_checker]\n<code_two>: for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]\n            if self.safety_checker is not None:\n                cpu_offload(self.safety_checker.vision_model)\nFix_pattern: In the condition of cpu_offloaded_model is not None, if the pattern of looping through the models and offloading them is detected, then change the code from [self.unet, self.text_encoder, self.vae, self.safety_checker] to [self.unet, self.text_encoder, self.vae] and add the if statement to check if self.safety_checker is not None and offload only self.safety_checker.vision_model. This fix the API misuse."}
{"number": 937, "change": "def shift_tokens_right(input_ids: tf.Tensor, pad_token_id: int, decoder_start_to\n\nif tf.executing_eagerly():\n# \"Verify that `labels` has only positive values and -100\"\n-        assert_gte0 = tf.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0))\n+        assert_gte0 = tf.debugging.assert_greater_equal(shifted_input_ids, tf.constant(0, dtype=input_ids.dtype))\n\n# Make sure the assertion op is called by wrapping the result in an identity no-op\nwith tf.control_dependencies([assert_gte0]):\n", "fix_pattern": "<condition>:\nThe condition is when the program is executing eagerly.\n\n<pattern>:\nThe pattern is a missing dtype specification in a constant.\n\n<code_one>:\nThe code that is removed is an assertion statement that checks if `shifted_input_ids` is greater than or equal to 0.\n\n<code_two>:\nThe code that is added is an assertion statement that checks if `shifted_input_ids` is greater than or equal to 0, with the added dtype specification for `input_ids.dtype`.\n\nFix_pattern:\nIn the condition of executing eagerly, if a missing dtype specification in a constant is detected, then change the code by adding the dtype specification for the constant in the assertion statement to fix the API misuse."}
{"number": 940, "change": "def arange(start, stop=None, step=1, dtype=None, dev=None):\nif dtype in [torch.int8, torch.uint8, torch.int16]:\nreturn torch.arange(start, stop, step=step, dtype=torch.int64, device=dev).to(dtype)\nelse:\n-            return torch.range(start, stop, step=step, dtype=dtype, device=dev)\n+            return torch.arange(start, stop, step=step, dtype=dtype, device=dev)\n", "fix_pattern": "<condition>: The condition is that the \"dtype\" variable is checked for specific values [torch.int8, torch.uint8, torch.int16].\n<pattern>: The pattern is that the function \"torch.range\" is being used instead of \"torch.arange\".\n<code_one>: The code being removed is \"return torch.range(start, stop, step=step, dtype=dtype, device=dev)\".\n<code_two>: The code being added is \"return torch.arange(start, stop, step=step, dtype=dtype, device=dev)\".\nFix_pattern: In the condition of checking \"dtype\" for specific values, if the function \"torch.range\" is detected, then change it to \"torch.arange\" to fix the API misuse."}
{"number": 944, "change": "def _apply_affine(input: torch.Tensor,\n\nheight, width = x_data.shape[-2:]\ntransform: torch.Tensor = params['transform'].to(device, dtype)\n-\n-    out_data: torch.Tensor = warp_perspective(x_data, transform, (height, width))\n+    out_data: torch.Tensor = warp_affine(x_data, transform[:, :2, :], (height, width))\n\nif return_transform:\nreturn out_data.view_as(input), transform\n", "fix_pattern": "<condition>: The code is using the function warp_perspective from the torch library.\n<pattern>: The code is replacing the use of the function warp_perspective with the function warp_affine.\n<code_one>: warp_perspective(x_data, transform, (height, width))\n<code_two>: warp_affine(x_data, transform[:, :2, :], (height, width))\nFix_pattern: In the condition of using warp_perspective, replace the code of warp_perspective with warp_affine to fix the API misuse."}
{"number": 946, "change": "class GPTNeoAttentionMixin:\nelse:\nraise ValueError(f\"Input tensor rank should be one of [2, 3], but is: {len(tensor.shape)}\")\n\n-        padded_tensor = F.pad(tensor, padding_side, value=pad_value)\n+        padded_tensor = nn.functional.pad(tensor, padding_side, value=pad_value)\npadded_tensor = padded_tensor.unfold(dimension=1, size=window_size + block_length, step=block_length)\n\nif is_key_value:\n", "fix_pattern": "<condition>: The input tensor rank should be one of [2, 3].\n<pattern>: The code was using the function 'F.pad()' to pad the tensor.\n<code_one>: 'padded_tensor = F.pad(tensor, padding_side, value=pad_value)'\n<code_two>: 'padded_tensor = nn.functional.pad(tensor, padding_side, value=pad_value)'\nFix_pattern: In the condition that the input tensor rank is one of [2, 3], if the code is using 'F.pad()', then change it to 'nn.functional.pad()' to fix the API misuse."}
{"number": 949, "change": "class TFTransfoXLPreTrainedModel(TFPreTrainedModel):\n@tf.function(\ninput_signature=[\n{\n-                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),\n+                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),\n}\n]\n)\n", "fix_pattern": "<condition>:\nThe condition is the usage of the `TFTransfoXLPreTrainedModel` class.\n\n<pattern>:\nThe pattern is the incorrect specification of the input tensor type.\n\n<code_one>:\nThe original code had a `tf.TensorSpec` with data type `tf.int32`, specifically for the `\"input_ids\"`.\n\n<code_two>:\nThe fixed code replaces the data type with `tf.int64` for the `\"input_ids\"`.\n\nFix_pattern:\nIn the condition of using `TFTransfoXLPreTrainedModel` class, if the incorrect data type specification of `tf.int32` for `\"input_ids\"` is detected, then the fix is to replace it with `tf.int64` in order to fix the API misuse."}
{"number": 951, "change": "class BiattentiveClassificationNetwork(Model):\n# Create ELMo embeddings if applicable\nif self._elmo:\nif elmo_tokens is not None:\n-                elmo_representations = self._elmo(elmo_tokens)[\"elmo_representations\"]\n+                elmo_representations = self._elmo(elmo_tokens[\"tokens\"])[\"elmo_representations\"]\n# Pop from the end is more performant with list\nif self._use_integrator_output_elmo:\nintegrator_output_elmo = elmo_representations.pop()\n", "fix_pattern": "<condition>: The condition is that the variable \"elmo_tokens\" is not None.\n<pattern>: The pattern is that the method \"_elmo\" is called with the variable \"elmo_tokens\" as an argument to get \"elmo_representations\".\n<code_one>: The code being removed is \"elmo_representations = self._elmo(elmo_tokens)[\"elmo_representations\"]\".\n<code_two>: The code being added is \"elmo_representations = self._elmo(elmo_tokens[\"tokens\"])[\"elmo_representations\"]\".\nFix_pattern: In the condition of \"elmo_tokens is not None\", if the pattern of calling the method \"_elmo\" with the variable \"elmo_tokens\" to get \"elmo_representations\" is detected, then change the code from \"elmo_representations = self._elmo(elmo_tokens)[\"elmo_representations\"]\" to \"elmo_representations = self._elmo(elmo_tokens[\"tokens\"])[\"elmo_representations\"]\" to fix the API misuse."}
{"number": 957, "change": "class Function(object):\nif is_sparse(tensor):\nsparse_coo = value.tocoo()\nindices = np.concatenate((np.expand_dims(sparse_coo.row, 1), np.expand_dims(sparse_coo.col, 1)), 1)\n-                value = (indices, value.data, value.shape)\n+                value = (indices, sparse_coo.data, sparse_coo.shape)\nfeed_dict[tensor] = value\nsession = get_session()\nupdated = session.run(self.outputs + [self.updates_op], feed_dict=feed_dict)\n", "fix_pattern": "<condition>: The code is checking if a given tensor is sparse or not.\n<pattern>: A value is being modified based on whether it is sparse or not.\n<code_one>: value = (indices, value.data, value.shape)\n<code_two>: value = (indices, sparse_coo.data, sparse_coo.shape)\nFix_pattern: In the condition of checking for sparsity, if the value is sparse, then change code_one to code_two to fix the API misuse."}
{"number": 960, "change": "def test_transformer_trainable_and_decodable(model_dict):\nattn_dict = model.calculate_all_attentions(\nx[0:1], ilens[0:1], y_tgt[0:1], y_src[0:1]\n)\n-    plot.plot_multi_head_attention(data, attn_dict, \"\", savefn=_savefn)\n+    plot.plot_multi_head_attention(data, uttid_list, attn_dict, \"\", savefn=_savefn)\n\n# test CTC plot\nctc_probs = model.calculate_all_ctc_probs(\n", "fix_pattern": "<condition>: The condition is when plotting the multi-head attention using CTC plot in a transformer model.\n\n<pattern>: The pattern is that the function `plot_multi_head_attention` is called with arguments `data`, `attn_dict`, and an empty string as the third argument.\n\n<code_one>: The code `plot.plot_multi_head_attention(data, attn_dict, \"\", savefn=_savefn)` is removed.\n\n<code_two>: The code `plot.plot_multi_head_attention(data, uttid_list, attn_dict, \"\", savefn=_savefn)` is added.\n\nFix_pattern: In the condition of plotting multi-head attention using CTC plot in a transformer model, if the pattern of calling the function `plot_multi_head_attention` with arguments `data`, `attn_dict`, and an empty string is detected, then remove the code `plot.plot_multi_head_attention(data, attn_dict, \"\", savefn=_savefn)` and add the code `plot.plot_multi_head_attention(data, uttid_list, attn_dict, \"\", savefn=_savefn)` to fix the API misuse."}
{"number": 961, "change": "class TFLayoutLMv3PreTrainedModel(TFPreTrainedModel):\n@tf.function(\ninput_signature=[\n{\n-                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),\n-                \"bbox\": tf.TensorSpec((None, None, 4), tf.int32, name=\"bbox\"),\n+                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),\n+                \"bbox\": tf.TensorSpec((None, None, 4), tf.int64, name=\"bbox\"),\n\"pixel_values\": tf.TensorSpec((None, None, None, None), tf.float32, name=\"pixel_values\"),\n-                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),\n+                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),\n}\n]\n)\n", "fix_pattern": "<condition>: TFLayoutLMv3PreTrainedModel class\n<pattern>: API misuse with incorrect data types for tensor specifications\n<code_one>: \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"), \"bbox\": tf.TensorSpec((None, None, 4), tf.int32, name=\"bbox\"), \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\")\n<code_two>: \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"), \"bbox\": tf.TensorSpec((None, None, 4), tf.int64, name=\"bbox\"), \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\")\nFix_pattern: In the condition of TFLayoutLMv3PreTrainedModel class, if API misuse with incorrect data types for tensor specifications is detected, then change the \"input_ids\", \"bbox\", and \"attention_mask\" tensor specifications from tf.int32 to tf.int64 to fix the API misuse."}
{"number": 962, "change": "class DecisionTransformerGPT2Attention(nn.Module):\n# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\nmask_value = torch.full([], mask_value, dtype=attn_weights.dtype).to(attn_weights.device)\n-            attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n+            attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n\nif attention_mask is not None:\n# Apply the attention mask\n", "fix_pattern": "<condition>: The code is checking for the condition of a specific attention mask being not None.\n<pattern>: The pattern being detected is that the attention weights should be converted to a specific data type.\n<code_one>: The code is initially using the `torch.where()` function to apply the attention mask to the attention weights.\n<code_two>: The code is modified to additionally convert the attention weights to the same data type as the mask_value using the `.to()` method.\nFix_pattern: In the condition of checking for a specific attention mask being not None, if the pattern of attention weights not being the same data type as the mask_value is detected, then change the code from using `torch.where()` to `torch.where().to()` to fix the API misuse."}
{"number": 963, "change": "def load_tf_graph(graph_file):\n\"\"\"\n# We load the protobuf file from the disk and parse it to retrieve the\n# unserialized graph_def\n-    with tf.gfile.GFile(graph_file, \"rb\") as f:\n-        graph_def = tf.GraphDef()\n+    with tf.io.gfile.GFile(graph_file, \"rb\") as f:\n+        graph_def = tf.compat.v1.GraphDef()\ngraph_def.ParseFromString(f.read())\n\n# Then, we import the graph_def into a new Graph and returns it\n", "fix_pattern": "<condition>: The code is using an outdated function or module.\n<pattern>: The code is using tf.gfile.GFile to read a file.\n<code_one>: tf.gfile.GFile(graph_file, \"rb\")\n<code_two>: tf.io.gfile.GFile(graph_file, \"rb\")\nFix_pattern: In the condition of using tf.gfile.GFile to read a file, if the pattern of tf.gfile.GFile is detected, then change tf.gfile.GFile(graph_file, \"rb\") to tf.io.gfile.GFile(graph_file, \"rb\") to fix the API misuse."}
{"number": 967, "change": "class TestScalarMix(AllenNlpTestCase):\ntensors = [torch.randn([3, 4, 5]) for _ in range(3)]\nnumpy_mask = numpy.ones((3, 4), dtype=\"int32\")\nnumpy_mask[1, 2:] = 0\n-        mask = torch.from_numpy(numpy_mask)\n+        mask = torch.from_numpy(numpy_mask).bool()\n\nweights = [0.1, 0.2, 0.3]\nfor k in range(3):\n", "fix_pattern": "<condition>: There is a need to convert a numpy array to a PyTorch tensor.\n<pattern>: The existing code uses `torch.from_numpy()` to convert the numpy array to a PyTorch tensor.\n<code_one>: `mask = torch.from_numpy(numpy_mask)`\n<code_two>: `mask = torch.from_numpy(numpy_mask).bool()`\nFix_pattern: In the condition of needing to convert a numpy array to a PyTorch tensor, if a boolean tensor is required, then change `torch.from_numpy(numpy_mask)` to `torch.from_numpy(numpy_mask).bool()` to fix the API misuse."}
{"number": 968, "change": "def get_global_step_var():\nwith tf.variable_scope(scope, reuse=False), \\\ntf.name_scope(None):\nvar = tf.get_variable(GLOBAL_STEP_OP_NAME,\n-                                  initializer=0,\n-                                  trainable=False, dtype=tf.int32)\n+                                  initializer=tf.constant(0, dtype=tf.int64),\n+                                  trainable=False, dtype=tf.int64)\nreturn var\n", "fix_pattern": "<condition>: The condition is when initializing a variable with a specific value.\n\n<pattern>: The pattern is initializing a variable with the wrong data type.\n\n<code_one>: The code that was removed is initializing the variable with the wrong data type, specifically `dtype=tf.int32`.\n\n<code_two>: The code that was added is initializing the variable with the correct data type, which is `tf.constant(0, dtype=tf.int64)`.\n\nFix_pattern: In the condition of initializing a variable, if it is detected that the variable is being initialized with the wrong data type, then remove the incorrect initialization code (`dtype=tf.int32`) and add the correct initialization code (`dtype=tf.int64`)."}
{"number": 969, "change": "class SequenceFeatureExtractionTestMixin(FeatureExtractionSavingTestMixin):\ninput_np = feat_extract.pad(processed_features, padding=\"longest\", return_tensors=\"np\")[input_name]\ninput_tf = feat_extract.pad(processed_features, padding=\"longest\", return_tensors=\"tf\")[input_name]\n\n-        self.assertTrue(abs(input_np.sum() - input_tf.numpy().sum()) < 1e-2)\n+        self.assertTrue(abs(input_np.astype(np.float32).sum() - input_tf.numpy().sum()) < 1e-2)\n\ndef test_attention_mask(self):\nfeat_dict = self.feat_extract_dict\n", "fix_pattern": "<condition>: The condition is not specified in the context provided. \n\n<pattern>: The pattern is not specified in the code provided.\n\n<code_one>: The code removed is `self.assertTrue(abs(input_np.sum() - input_tf.numpy().sum()) < 1e-2)`.\n\n<code_two>: The code added is `self.assertTrue(abs(input_np.astype(np.float32).sum() - input_tf.numpy().sum()) < 1e-2)`.\n\nFix_pattern: In the condition of `<condition>`, if `<pattern>` is detected, then change `<code_one>` to `<code_two>` to fix the API misuse."}
{"number": 972, "change": "class SSIM(nn.Module):\nssim_map = ((2 * mu1_mu2 + self.C1) * (2 * sigma12 + self.C2)) / \\\n((mu1_sq + mu2_sq + self.C1) * (sigma1_sq + sigma2_sq + self.C2))\n\n-        loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2.\n+        loss = torch.clamp(torch.tensor(1.) - ssim_map, min=0, max=1) / 2.\n\nif self.reduction == 'mean':\nloss = torch.mean(loss)\n", "fix_pattern": "<condition>: The code is checking for the value of the \"reduction\" attribute to be equal to \"mean\".\n<pattern>: The pattern is the incorrect use of the \"torch.clamp\" function on the \"ssim_map\" variable.\n<code_one>: The incorrect code is \"loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2.\".\n<code_two>: The correct code is \"loss = torch.clamp(torch.tensor(1.) - ssim_map, min=0, max=1) / 2.\".\nFix_pattern: \nIn the condition of checking the \"reduction\" attribute value, if there is an incorrect use of \"torch.clamp\" function on \"ssim_map\" variable, then change the code from \"loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2.\" to \"loss = torch.clamp(torch.tensor(1.) - ssim_map, min=0, max=1) / 2.\" to fix the API misuse."}
{"number": 974, "change": "def rpn_losses(anchor_labels, anchor_boxes, label_logits, box_logits):\nplaceholder = 1.\nlabel_loss = tf.nn.sigmoid_cross_entropy_with_logits(\nlabels=tf.to_float(valid_anchor_labels), logits=valid_label_logits)\n-    label_loss = label_loss * (1. / config.RPN_BATCH_PER_IM)\n+    label_loss = tf.reduce_sum(label_loss) * (1. / config.RPN_BATCH_PER_IM)\nlabel_loss = tf.where(tf.equal(nr_valid, 0), placeholder, label_loss, name='label_loss')\n\npos_anchor_boxes = tf.boolean_mask(anchor_boxes, pos_mask)\n", "fix_pattern": "<condition>:\nThere is a check for tf.equal(nr_valid, 0).\n\n<pattern>:\nIf the condition tf.equal(nr_valid, 0) is True, then the code_one is multiplied by 1 / config.RPN_BATCH_PER_IM, else no change is made.\n\n<code_one>:\nlabel_loss = label_loss * (1. / config.RPN_BATCH_PER_IM)\n\n<code_two>:\nlabel_loss = tf.reduce_sum(label_loss) * (1. / config.RPN_BATCH_PER_IM)\n\nFix_pattern:\nIn the condition of tf.equal(nr_valid, 0), if the pattern of multiplying label_loss by (1. / config.RPN_BATCH_PER_IM) is detected, then change the code_one to code_two to fix the API misuse."}
{"number": 975, "change": "def test_beamformer_net_wpe_output(ch, num_spk, use_dnn_mask_for_wpe):\ndef test_beamformer_net_bf_output(num_spk):\nch = 3\ninputs = torch.randn(2, 16, ch)\n+    inputs = inputs.float()\nilens = torch.LongTensor([16, 12])\nmodel = BeamformerNet(\nn_fft=8,\n", "fix_pattern": "<condition>:\nThe code is using the `inputs` tensor and attempting to apply a method or operation that is not supported for the data type of `inputs`.\n\n<pattern>:\nThe pattern is detecting the incorrect data type of `inputs`.\n\n<code_one>:\nThe original code is attempting to use the `inputs` tensor directly.\n\n<code_two>:\nThe fix is to convert the `inputs` tensor to a float data type using `.float()`\n\nFix_pattern:\nIn the condition of using the `inputs` tensor, if the incorrect data type of `inputs` is detected, then change the `inputs` tensor to a float data type using `.float()` to fix the API misuse."}
{"number": 976, "change": "from allennlp.common.testing import AllenNlpTestCase\n\nclass TestElmoLstmCell(AllenNlpTestCase):\ndef test_elmo_lstm(self):\n-        input_tensor = Variable(torch.rand(4, 5, 3))\n+        input_tensor = torch.rand(4, 5, 3)\ninput_tensor[1, 4:, :] = 0.\ninput_tensor[2, 2:, :] = 0.\ninput_tensor[3, 1:, :] = 0.\n-        mask = Variable(torch.ones([4, 5]))\n+        mask = torch.ones([4, 5])\nmask[1, 4:] = 0.\nmask[2, 2:] = 0.\nmask[3, 1:] = 0.\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: Variable() is replaced with torch.\n<code_one>: input_tensor = Variable(torch.rand(4, 5, 3))\nmask = Variable(torch.ones([4, 5]))\n<code_two>: input_tensor = torch.rand(4, 5, 3)\nmask = torch.ones([4, 5])\nFix_pattern: In the condition of no clear condition, if Variable() is detected, then the input_tensor and mask are replaced with torch.rand() and torch.ones() respectively to fix the API misuse."}
{"number": 977, "change": "class TpuStrategyTest(tf.test.TestCase):\nserving_fn = create_serving_signature(model)\n\nsaved_model_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n-      tf.saved_model.save(\n-          model, saved_model_dir, signatures={\"serving_default\": serving_fn})\n+      model.save(saved_model_dir, save_format=\"tf\",\n+                 signatures={\"serving_default\": serving_fn})\n\n# Test the saved_model.\nloaded_serving_fn = tf.keras.models.load_model(\n", "fix_pattern": "<condition>: The code is using the `tf.saved_model.save()` function to save a model.\n\n<pattern>: The signature of the `serving_fn` is specified as `signatures={\"serving_default\": serving_fn}`.\n\n<code_one>: `tf.saved_model.save(model, saved_model_dir, signatures={\"serving_default\": serving_fn})`\n\n<code_two>: `model.save(saved_model_dir, save_format=\"tf\", signatures={\"serving_default\": serving_fn})`\n\nFix_pattern: In the condition of using `tf.saved_model.save()` function to save a model and specifying the serving signature, replace the `tf.saved_model.save()` function with `model.save()` and provide the `save_format` parameter as `\"tf\"` to fix the API misuse."}
{"number": 984, "change": "def _preprocess_conv3d_input(x, data_format):\n# Returns\nA tensor.\n\"\"\"\n-    if dtype(x) == 'float64':\n+    # tensorflow doesn't support float64 for conv layer before 1.8.0\n+    if (dtype(x) == 'float64'\n+            and StrictVersion(tf.__version__) < StrictVersion('1.8.0')):\nx = tf.cast(x, 'float32')\ntf_data_format = 'NDHWC'\nif data_format == 'channels_first':\n", "fix_pattern": "<condition>: The code is checking the data format used for convolutional layer.\n\n<pattern>: The code is checking the data type of the input tensor.\n\n<code_one>: The code is checking if the data type of the input tensor is 'float64'.\n\n<code_two>: The code is adding a comparison to also check if the TensorFlow version is less than 1.8.0.\n\nFix_pattern: In the condition of checking the data format, if the data type of the input tensor is 'float64', then the code is updated to also check if the TensorFlow version is less than 1.8.0 to fix the API misuse."}
{"number": 987, "change": "class SingleRoIExtractor(BaseRoIExtractor):\nnum_levels = len(feats)\nroi_feats = feats[0].new_zeros(\nrois.size(0), self.out_channels, *out_size)\n+        # TODO: remove this when parrots supports\n+        if torch.__version__ == 'parrots':\n+            roi_feats.requires_grad = True\n\nif num_levels == 1:\nif len(rois) == 0:\n", "fix_pattern": "<condition>: When the Torch version is 'parrots'.\n<pattern>: The code checks if the Torch version is 'parrots'.\n<code_one>: None\n<code_two>: The code sets the `requires_grad` attribute of `roi_feats` to True.\nFix_pattern: In the condition of `torch.__version__ == 'parrots'`, the code sets the `requires_grad` attribute of `roi_feats` to True to fix the API misuse."}
{"number": 988, "change": "def main(args):\naccelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\noptimizer.step()\nlr_scheduler.step()\n-                optimizer.zero_grad()\n+                optimizer.zero_grad(set_to_none=args.set_grads_to_none)\n\n# Checks if the accelerator has performed an optimization step behind the scenes\nif accelerator.sync_gradients:\n", "fix_pattern": "<condition>:\nThe condition is checking if the accelerator has performed an optimization step behind the scenes.\n\n<pattern>:\nThe pattern detected is the removal of the line that sets the gradients to zero using \"optimizer.zero_grad()\".\n\n<code_one>:\noptimizer.zero_grad()\n\n<code_two>:\noptimizer.zero_grad(set_to_none=args.set_grads_to_none)\n\nFix_pattern:\nIn the condition of checking if the accelerator has performed an optimization step behind the scenes, if the pattern of removing the line of \"optimizer.zero_grad()\" is detected, then the fix is to change it to \"optimizer.zero_grad(set_to_none=args.set_grads_to_none)\" to fix the API misuse."}
{"number": 989, "change": "class Tagger(nn.Module):\n# criterion\nself.crit = nn.CrossEntropyLoss(ignore_index=0) # ignore padding\n\n-        self.drop = Dropout(args['dropout'])\n+        self.drop = nn.Dropout(args['dropout'])\nself.worddrop = WordDropout(args['word_dropout'])\n\ndef forward(self, word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens):\n", "fix_pattern": "<condition>: In the code snippet, the condition is not explicitly stated. However, based on the given context, the condition can be assumed to be an API misuse or issue with using a specific class or method.\n\n<pattern>: The pattern detected is the incorrect usage of the `Dropout` class. \n\n<code_one>: The code that was removed is `self.drop = Dropout(args['dropout'])`. \n\n<code_two>: The code that was added is `self.drop = nn.Dropout(args['dropout'])`.\n\nFix_pattern: In the condition of an API misuse, if the incorrect usage of the `Dropout` class is detected, then change the `self.drop = Dropout(args['dropout'])` to `self.drop = nn.Dropout(args['dropout'])` to fix the API misuse."}
{"number": 991, "change": "def build_linear_positions(index_dims, output_range=(-1.0, 1.0)):\nreturn torch.linspace(start=output_range[0], end=output_range[1], steps=n_xels_per_dim, dtype=torch.float32)\n\ndim_ranges = [_linspace(n_xels_per_dim) for n_xels_per_dim in index_dims]\n-    array_index_grid = torch.meshgrid(*dim_ranges)\n+    array_index_grid = meshgrid(*dim_ranges, indexing=\"ij\")\n\nreturn torch.stack(array_index_grid, dim=-1)\n", "fix_pattern": "<condition>: When building linear positions using \"build_linear_positions\" function.\n\n<pattern>: The pattern detected is the usage of \"torch.meshgrid\" function.\n\n<code_one>: The code that was removed is \"array_index_grid = torch.meshgrid(*dim_ranges)\".\n\n<code_two>: The code that was added is \"array_index_grid = meshgrid(*dim_ranges, indexing=\"ij\")\".\n\nFix_pattern: In the condition of building linear positions using \"build_linear_positions\" function, if the usage of \"torch.meshgrid\" is detected, then remove the code \"array_index_grid = torch.meshgrid(*dim_ranges)\" and add the code \"array_index_grid = meshgrid(*dim_ranges, indexing=\"ij\")\" to fix the API misuse."}
{"number": 1007, "change": "class GPT2Attention(nn.Module):\n# Apply the attention mask\nattn_weights = attn_weights + attention_mask\n\n-        attn_weights = nn.Softmax(dim=-1)(attn_weights)\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n\n# Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op if otherwise\nif attn_weights.dtype != torch.float32:\n", "fix_pattern": "<condition>: The condition is when the data type of `attn_weights` is not `torch.float32`.\n<pattern>: The pattern is to apply a softmax operation to `attn_weights` along the last dimension.\n<code_one>: The code being removed is `attn_weights = nn.Softmax(dim=-1)(attn_weights)`.\n<code_two>: The code being added is `attn_weights = nn.functional.softmax(attn_weights, dim=-1)`.\nFix_pattern: In the condition of `attn_weights` not being of `torch.float32` data type, the fix is to replace the softmax operation with the `nn.functional.softmax` function on `attn_weights` along the last dimension."}
{"number": 1008, "change": "class BidirectionalEndpointSpanExtractor(SpanExtractor):\nsequence_lengths = util.get_lengths_from_binary_sequence_mask(sequence_mask)\nelse:\n# shape (batch_size), filled with the sequence length size of the sequence_tensor.\n-            sequence_lengths = util.ones_like(sequence_tensor[:, 0, 0]).long() * sequence_tensor.size(1)\n+            sequence_lengths = (torch.ones_like(sequence_tensor[:, 0, 0], dtype=torch.long) *\n+                                sequence_tensor.size(1))\n\n# shape (batch_size, num_spans, 1)\nend_sentinel_mask = (exclusive_span_ends == sequence_lengths.unsqueeze(-1)).long().unsqueeze(-1)\n", "fix_pattern": "<condition>: The condition is when an if-else statement is encountered and the else branch is being executed.\n\n<pattern>: The pattern is the incorrect calculation of sequence_lengths.\n\n<code_one>: The incorrect code is \"sequence_lengths = util.ones_like(sequence_tensor[:, 0, 0]).long() * sequence_tensor.size(1)\".\n\n<code_two>: The correct code is \"sequence_lengths = (torch.ones_like(sequence_tensor[:, 0, 0], dtype=torch.long) * sequence_tensor.size(1))\".\n\nFix_pattern: In the condition of an if-else statement where the else branch is being executed, if the incorrect pattern of calculating \"sequence_lengths\" is detected, then the code should be changed from \"<code_one>\" to \"<code_two>\" to fix the API misuse."}
{"number": 1013, "change": "class ARSTFPolicy:\nself.num_params = sum(\nnp.prod(variable.shape.as_list())\nfor _, variable in self.variables.variables.items())\n-        self.sess.run(tf.global_variables_initializer())\n+        self.sess.run(tf1.global_variables_initializer())\n\ndef compute_actions(self,\nobservation,\n", "fix_pattern": "Condition: In the context of the ARSTFPolicy class, when initializing global variables using \"self.sess.run(tf.global_variables_initializer())\".\nPattern: If \"tf.global_variables_initializer()\" is detected.\nCode one: \"self.sess.run(tf.global_variables_initializer())\"\nCode two: \"self.sess.run(tf1.global_variables_initializer())\"\nFix pattern: In the condition of initializing global variables using \"tf.global_variables_initializer()\", change \"self.sess.run(tf.global_variables_initializer())\" to \"self.sess.run(tf1.global_variables_initializer())\" to fix the API misuse."}
{"number": 1016, "change": "class LanguageModel(nn.Module):\n\nfor i in range(number_of_characters):\n\n-                if torch.cuda.is_available():\n-                    input = input.cuda()\n+                input = input.to(flair.device)\n\n# get predicted weights\nprediction, _, hidden = self.forward(input, hidden)\n", "fix_pattern": "<condition>: Checking if CUDA is available\n<pattern>: Changing the device the input tensor is sent to\n<code_one>: if torch.cuda.is_available(): input = input.cuda()\n<code_two>: input = input.to(flair.device)\nFix_pattern: In the condition of checking if CUDA is available, if the pattern of sending the input tensor to CUDA is detected, then change the code from \"input = input.cuda()\" to \"input = input.to(flair.device)\" to fix the API misuse."}
{"number": 1024, "change": "class MultiCategorical(TFActionDistribution):\n\n@override(ActionDistribution)\ndef multi_kl(self, other):\n-        return [cat.kl(oth_cat) for cat, oth_cat in zip(self.cats, other.cats)]\n+        return tf.stack(\n+            [cat.kl(oth_cat) for cat, oth_cat in zip(self.cats, other.cats)],\n+            axis=1)\n\n@override(ActionDistribution)\ndef kl(self, other):\n", "fix_pattern": "<condition>: The code is part of a class that inherits from TFActionDistribution.\n\n<pattern>: The code is using a list comprehension to perform a certain operation on each element of two lists.\n\n<code_one>: `[cat.kl(oth_cat) for cat, oth_cat in zip(self.cats, other.cats)]`\n\n<code_two>: `tf.stack([cat.kl(oth_cat) for cat, oth_cat in zip(self.cats, other.cats)], axis=1)`\n\nFix_pattern: In the condition of inheriting from TFActionDistribution, if a list comprehension is used to perform an operation on each element of two lists, then change the code from `<code_one>` to `<code_two>` to fix the API misuse."}
{"number": 1027, "change": "class Trainer:\nif self.args.past_index >= 0:\ninputs[\"mems\"] = past\n# Our model outputs do not work with DataParallel, so forcing return tuple.\n-            if self.args.n_gpu > 1:\n+            if isinstance(model, nn.DataParallel):\ninputs[\"return_tuple\"] = True\n\nwith torch.no_grad():\n", "fix_pattern": "<condition>:\nself.args.n_gpu > 1\n\n<pattern>:\nCheck if the number of GPUs is greater than 1.\n\n<code_one>:\nif self.args.n_gpu > 1:\n\n<code_two>:\nif isinstance(model, nn.DataParallel):\n\nFix_pattern:\nIn the condition of \"self.args.n_gpu > 1\", if \"self.args.n_gpu\" is detected, then remove the line \"if self.args.n_gpu > 1:\" and add the line \"if isinstance(model, nn.DataParallel):\" to fix the API misuse."}
{"number": 1028, "change": "class PolicyWithValue:\ndef sample(logits, mask_npinf):\nnew_logits = tf.math.add(logits, mask_npinf)\nu = tf.random_uniform(tf.shape(new_logits), dtype=logits.dtype)\n-            return tf.argmax(new_logits - tf.log(-tf.log(u)), axis=-1)\n+            return tf.argmax(new_logits - tf.log(-1*tf.log(u)), axis=-1)\n\ndef neglogp(logits, x):\n# return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=x)\n", "fix_pattern": "<condition>: \nThe condition is not clear from the given context.\n\n<pattern>: \nThe pattern is to change the value of tf.log(-tf.log(u)) to tf.log(-1*tf.log(u)).\n\n<code_one>: \nThe code one is tf.log(-tf.log(u)).\n\n<code_two>: \nThe code two is tf.log(-1*tf.log(u)).\n\nFix_pattern: \nin the condition of <condition>, if <pattern> is detected, then change the <code_one> to <code_two> to fix the API misuse."}
{"number": 1030, "change": "def Conv2DTranspose(\nif get_tf_version_tuple() <= (1, 12):\nkernel_initializer = tf.contrib.layers.variance_scaling_initializer(2.0),\nelse:\n-            kernel_initializer = tf.keras.initializers.VarianceScaling(2.0)\n+            kernel_initializer = tf.keras.initializers.VarianceScaling(2.0, distribution='untruncated_normal')\n\nwith rename_get_variable({'kernel': 'W', 'bias': 'b'}):\nlayer = tf.layers.Conv2DTranspose(\n", "fix_pattern": "<condition>: The condition is that the TensorFlow version should be <= 1.12.\n<pattern>: The pattern is the misuse of the API, specifically the incorrect use of `kernel_initializer`.\n<code_one>: The code that was removed is `kernel_initializer = tf.keras.initializers.VarianceScaling(2.0)`.\n<code_two>: The code that was added is `kernel_initializer = tf.keras.initializers.VarianceScaling(2.0, distribution='untruncated_normal')`.\nFix_pattern: In the condition of TensorFlow version <= 1.12, if the API misuse of `kernel_initializer` is detected, then change the code from `kernel_initializer = tf.keras.initializers.VarianceScaling(2.0)` to `kernel_initializer = tf.keras.initializers.VarianceScaling(2.0, distribution='untruncated_normal')` to fix the API misuse."}
{"number": 1033, "change": "def make_batches(lines, args, task, max_positions, encode_fn):\n).long()\nfor src_str in lines\n]\n-    lengths = torch.LongTensor([t.numel() for t in tokens])\n+    lengths = [t.numel() for t in tokens]\nitr = task.get_batch_iterator(\ndataset=task.build_dataset_for_inference(tokens, lengths),\nmax_tokens=args.max_tokens,\n", "fix_pattern": "<condition>: When creating batches for an API misuse\n<pattern>: The length of the input tokens should be calculated using a list comprehension instead of a torch.LongTensor function call.\n<code_one>: lengths = torch.LongTensor([t.numel() for t in tokens])\n<code_two>: lengths = [t.numel() for t in tokens]\nFix_pattern: In the condition of creating batches for an API misuse, if the pattern of using torch.LongTensor to calculate the length of input tokens is detected, then change the code from lengths = torch.LongTensor([t.numel() for t in tokens]) to lengths = [t.numel() for t in tokens] to fix the API misuse."}
{"number": 1034, "change": "class EarlyStopping(Callback):\n\nif trainer.use_tpu:\nstop = torch.tensor(int(trainer.should_stop), device=pl_module.device, dtype=torch.int32)\n-            stop = xm.mesh_reduce(\"stop_signal\", stop, torch.cat)\n+            stop = xm.mesh_reduce(\"stop_signal\", stop, sum)\ntorch_xla.core.xla_model.rendezvous(\"pl.EarlyStoppingCallback.stop_distributed_training_check\")\ntrainer.should_stop = int(stop.item()) == trainer.world_size\n", "fix_pattern": "<condition>: `trainer.use_tpu` is True.\r\n<pattern>: Use `sum` as the reduction function instead of `torch.cat` in `xm.mesh_reduce` method.\r\n<code_one>: `stop = xm.mesh_reduce(\"stop_signal\", stop, torch.cat)`\r\n<code_two>: `stop = xm.mesh_reduce(\"stop_signal\", stop, sum)`\r\nFix_pattern: In the condition of `trainer.use_tpu` being True, if the pattern of using `torch.cat` as the reduction function in `xm.mesh_reduce` method is detected, then change the code from `stop = xm.mesh_reduce(\"stop_signal\", stop, torch.cat)` to `stop = xm.mesh_reduce(\"stop_signal\", stop, sum)` to fix the API misuse."}
{"number": 1035, "change": "class Entropy(Metric):\nmask : `torch.Tensor`, optional (default = None).\nA masking tensor of shape (batch_size, ...).\n\"\"\"\n-        logits, mask = self.unwrap_to_tensors(logits, mask)\n+        logits, mask = self.detach_tensors(logits, mask)\n\nif mask is None:\n-            mask = torch.ones(logits.size()[:-1])\n+            mask = torch.ones(logits.size()[:-1], device=logits.device)\n\nlog_probs = torch.nn.functional.log_softmax(logits, dim=-1)\nprobabilities = torch.exp(log_probs) * mask.unsqueeze(-1)\n", "fix_pattern": "<condition>: The code is checking if a `mask` variable is None.\n<pattern>: The pattern is that if the `mask` variable is None, the code should perform certain operations.\n<code_one>: The code initially sets the `mask` variable to a tensor of ones if it is None.\n<code_two>: The fixed code sets the `mask` variable to a tensor of ones using the `device` attribute of the `logits` tensor.\nFix_pattern: In the condition of `mask` being None, the code initially sets the `mask` variable to a tensor of ones using `torch.ones` function. The fixed version sets the `mask` variable to a tensor of ones using `torch.ones` function with the `device` attribute of the `logits` tensor."}
{"number": 1039, "change": "class ImageEmbedder(nn.Module):\n\nself.to_patch_embedding = nn.Sequential(\nRearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n+            nn.LayerNorm(patch_dim),\nnn.Linear(patch_dim, dim),\n+            nn.LayerNorm(dim)\n)\n\nself.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n", "fix_pattern": "<condition>:\nThe condition is when the `ImageEmbedder` class is being defined.\n\n<pattern>:\nThe pattern is the absence of any code related to `LayerNorm` in the `to_patch_embedding` section.\n\n<code_one>:\nNo specific code needs to be removed.\n\n<code_two>:\nThe code that needs to be added is `nn.LayerNorm(patch_dim), nn.LayerNorm(dim)`.\n\nFix_pattern:\nIn the condition of defining the `ImageEmbedder` class, if there is no code related to `LayerNorm` in the `to_patch_embedding` section, then add `nn.LayerNorm(patch_dim)` and `nn.LayerNorm(dim)` to fix the API misuse."}
{"number": 1043, "change": "class BertForSequenceClassification(BertPreTrainedModel):\n\nself.bert = BertModel(config)\nself.dropout = nn.Dropout(config.hidden_dropout_prob)\n-        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n+        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\nself.init_weights()\n", "fix_pattern": "<condition>: When implementing a sequence classification model using BERT.\n<pattern>: The number of output labels is wrongly assigned in the linear classifier.\n<code_one>: self.config.num_labels\n<code_two>: config.num_labels\nFix_pattern: In the condition of implementing a sequence classification model using BERT, if the number of output labels is wrongly assigned in the linear classifier, then change self.config.num_labels to config.num_labels to fix the API misuse."}
{"number": 1044, "change": "def fmod(\nout: Optional[Union[tf.Tensor, tf.Variable]] = None,\n) -> Union[tf.Tensor, tf.Variable]:\nresult = tf.math.floormod(x1, x2, name=None)\n-    temp = (result, x1)\n-    return tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp)\n+    temp = [result, x1]\n+    return tf.map_fn(lambda x: x[0] if (x[0] * x[1] >= 0) else (-1 * x[0]), temp, fn_output_signature=result.dtype)\n\n\ndef fmax(\n", "fix_pattern": "<condition>: The condition should check if the product of two variables is greater than or equal to 0.\n\n<pattern>: The pattern to be detected is the usage of `tf.map_fn()` function with a lambda function that applies a condition on the elements of a list.\n\n<code_one>: The code to be removed is `temp = (result, x1)`.\n\n<code_two>: The code to be added is `temp = [result, x1]` and `fn_output_signature=result.dtype`.\n\nFix_pattern:\nIn the condition of checking if the product of two variables is greater than or equal to 0, if the pattern of using `tf.map_fn()` function with a lambda function applying the condition is detected, then remove the code `temp = (result, x1)` and add the code `temp = [result, x1]` with `fn_output_signature=result.dtype` to fix the API misuse."}
{"number": 1046, "change": "class MT5DenseGatedActDense(nn.Module):\n# To make 8bit quantization work for google/flan-t5-xxl, self.wo is kept in float32.\n# See https://github.com/huggingface/transformers/issues/20287\n# we also make sure the weights are not in `int8` in case users will force `_keep_in_fp32_modules` to be `None``\n-        if hidden_states.dtype != self.wo.weight.dtype and self.wo.weight.dtype != torch.int8:\n+        if (\n+            isinstance(self.wo.weight, torch.Tensor)\n+            and hidden_states.dtype != self.wo.weight.dtype\n+            and self.wo.weight.dtype != torch.int8\n+        ):\nhidden_states = hidden_states.to(self.wo.weight.dtype)\n\nhidden_states = self.wo(hidden_states)\n", "fix_pattern": "<condition>: The condition where the code fix is applied.\n<pattern>: The specific issue or bug in the code that is being addressed.\n<code_one>: The code that is being removed or changed.\n<code_two>: The code that is added or modified.\nFix_pattern: A summary of the fix pattern used to address the API misuse."}
{"number": 1048, "change": "def vector_to_skew_symmetric_matrix(vector):\na2s = vector_expanded[..., 1:2, :]\na3s = vector_expanded[..., 2:3, :]\n# BS x 1 x 1\n-    zs = torch.zeros(batch_shape + [1, 1])\n+    zs = torch.zeros(batch_shape + [1, 1], device=vector.device)\n# BS x 1 x 3\nrow1 = torch.cat((zs, -a3s, a2s), -1)\nrow2 = torch.cat((a3s, zs, -a1s), -1)\n", "fix_pattern": "<condition>:\nThe condition of the fix pattern is that the code includes a variable or object that needs to be assigned to a specific device.\n\n<pattern>:\nThe pattern is that a variable or object is created using a certain function or method, and then the device attribute needs to be set or changed.\n\n<code_one>:\nThe code_one is the variable or object that needs to have its device attribute set or changed.\n\n<code_two>:\nThe code_two is the modified version of code_one where the device attribute is set or changed.\n\nFix_pattern:\nIn the condition of the device assignment, if the code includes <code_one>, then change it to <code_two> to fix the API misuse."}
{"number": 1050, "change": "class PointAssigner(BaseAssigner):\n\nif gt_labels is not None:\nassigned_labels = assigned_gt_inds.new_full((num_points, ), -1)\n-            pos_inds = torch.nonzero(assigned_gt_inds > 0).squeeze()\n+            pos_inds = torch.nonzero(\n+                assigned_gt_inds > 0, as_tuple=False).squeeze()\nif pos_inds.numel() > 0:\nassigned_labels[pos_inds] = gt_labels[\nassigned_gt_inds[pos_inds] - 1]\n", "fix_pattern": "<condition>:\nIf gt_labels is not None\n\n<pattern>:\nA non-zero value check on assigned_gt_inds\n\n<code_one>:\npos_inds = torch.nonzero(assigned_gt_inds > 0).squeeze()\n\n<code_two>:\npos_inds = torch.nonzero(assigned_gt_inds > 0, as_tuple=False).squeeze()\n\nFix_pattern:\nIn the condition of \"gt_labels is not None\", if a non-zero value is detected in assigned_gt_inds, then change the code \"pos_inds = torch.nonzero(assigned_gt_inds > 0).squeeze()\" to \"pos_inds = torch.nonzero(assigned_gt_inds > 0, as_tuple=False).squeeze()\" to fix the API misuse."}
{"number": 1052, "change": "def att_to_numpy(att_ws, att):\natt_ws = torch.stack([aw[:, -1] for aw in att_ws], dim=1).cpu().numpy()\nelif isinstance(att, (AttCov, AttCovLoc)):\n# att_ws => list of list of previous attentions\n-        att_ws = torch.stack([aw[-1] for aw in att_ws], dim=1).cpu().numpy()\n+        att_ws = torch.stack([aw[idx] for idx, aw in enumerate(att_ws)], dim=1).cpu().numpy()\nelif isinstance(att, AttLocRec):\n# att_ws => list of tuple of attention and hidden states\natt_ws = torch.stack([aw[0] for aw in att_ws], dim=1).cpu().numpy()\n", "fix_pattern": "<condition>: `isinstance(att, (AttCov, AttCovLoc))` or `isinstance(att, AttLocRec)`.\n<pattern>: `aw[-1]`\n<code_one>: `att_ws`\n<code_two>: `aw[idx]`\nFix_pattern: In the condition of `isinstance(att, (AttCov, AttCovLoc))` or `isinstance(att, AttLocRec)`, if `aw[-1]` is detected, then remove `att_ws` and add `aw[idx]` to fix the API misuse."}
{"number": 1054, "change": "class HestonModel(generic_ito_process.GenericItoProcess):\ndrift = tf.stack([log_spot_drift, var_drift], -1)\nreturn drift\n\n-    super(HestonModel, self).__init__(2, _drift_fn, _vol_fn, dtype, name)\n+    super(HestonModel, self).__init__(2, _drift_fn, _vol_fn, self._dtype, name)\n\ndef sample_paths(self,\ntimes: types.RealTensor,\n", "fix_pattern": "<condition>: When initializing the HestonModel class\n<pattern>: Using the wrong data type for the dtype argument\n<code_one>: dtype\n<code_two>: self._dtype\nFix_pattern: In the condition of initializing the HestonModel class, if the wrong data type is detected in the dtype argument, then change the dtype to self._dtype to fix the API misuse."}
{"number": 1056, "change": "class SimilarityLearner(flair.nn.Model):\nepoch_results_str,\ndetailed_results,\n),\n-            0,\n+            torch.tensor(0),\n)\n\ndef _get_state_dict(self):\n", "fix_pattern": "<condition>: The code contains a condition that checks for a specific criteria.\n<pattern>: A particular pattern is detected within the condition.\n<code_one>: There is a piece of code that needs to be modified or removed.\n<code_two>: The code is replaced with a different piece of code to fix the API misuse.\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then remove/change the <code_one> to <code_two> to fix the API misuse."}
{"number": 1059, "change": "class TFOPTDecoder(tf.keras.layers.Layer):\nif output_attentions:\nall_self_attns += (layer_self_attn,)\n\n+        if self.final_layer_norm is not None:\n+            hidden_states = self.final_layer_norm(hidden_states)\n+\nif self.project_out is not None:\nhidden_states = self.project_out(hidden_states)\n", "fix_pattern": "<condition>: The condition is `self.project_out is not None`.\n<pattern>: The pattern is removing `hidden_states = self.project_out(hidden_states)`.\n<code_one>: The code being removed is `hidden_states = self.project_out(hidden_states)`.\n<code_two>: The code being added is `hidden_states = self.final_layer_norm(hidden_states)`.\nFix_pattern: In the condition of `self.project_out is not None`, if the code `hidden_states = self.project_out(hidden_states)` is detected, then remove it and add the code `hidden_states = self.final_layer_norm(hidden_states)` to fix the API misuse."}
{"number": 1060, "change": "def convert_examples_to_features(examples, seq_length, tokenizer):\nif ex_index < 5:\ntf.logging.info(\"*** Example ***\")\ntf.logging.info(\"unique_id: %s\" % (example.unique_id))\n-      tf.logging.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n+      tf.logging.info(\"tokens: %s\" % \" \".join(\n+          [tokenization.printable_text(x) for x in tokens]))\ntf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\ntf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\ntf.logging.info(\n", "fix_pattern": "<condition>: Checking if `ex_index` is less than 5.\n<pattern>: Calling `tf.logging.info()` with the argument `\"tokens: %s\" % \" \".join([str(x) for x in tokens])`.\n<code_one>: `tf.logging.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))`\n<code_two>: `tf.logging.info(\"tokens: %s\" % \" \".join([tokenization.printable_text(x) for x in tokens]))`\nFix_pattern: In the condition of `ex_index < 5`, if the pattern of calling `tf.logging.info()` with the argument `\"tokens: %s\" % \" \".join([str(x) for x in tokens])` is detected, then change the `code_one` to `code_two` to fix the API misuse."}
{"number": 1064, "change": "def load_model(filepath, custom_optimizers=None, custom_objects=None, compressio\n\"\"\"\ndef wrap_optimizer(cls):\nreturn lambda **kwargs: DistributedOptimizer(cls(**kwargs), compression=compression)\n-    return _impl.load_model(keras, wrap_optimizer, filepath, custom_optimizers, custom_objects)\n+    optimizer_modules = {keras.optimizers.Optimizer.__module__}\n+    return _impl.load_model(keras, wrap_optimizer, optimizer_modules, filepath, custom_optimizers, custom_objects)\n", "fix_pattern": "<condition>:\n\nThe condition is not specified in the given context.\n\n<pattern>:\n\nThe pattern is not specified in the given context.\n\n<code_one>:\n\nThe code one is not specified in the given context.\n\n<code_two>:\n\nThe code two is not specified in the given context.\n\nFix Pattern:\n\nNo fix pattern can be identified without the specific details of the condition, pattern, code one, and code two."}
{"number": 1065, "change": "class ParabolicEquationStepperTest(tf.test.TestCase, parameterized.TestCase):\nsecond_order_coeff_fn=second_order_coeff_fn,\ninner_first_order_coeff_fn=inner_first_order_coeff_fn)[0]\n\n-    true_values = tf.math.exp(final_t + grid[0])\n+    true_values = tf.expand_dims(tf.math.exp(final_t + grid[0]), axis=0)\nself.assertAllClose(\nest_values, true_values, atol=1e-2, rtol=1e-2)\n", "fix_pattern": "<condition>:\nNo clear condition is needed.\n\n<pattern>:\nThe pattern is that the code is modified to add an expansion of the tensor.\n\n<code_one>:\ntrue_values = tf.math.exp(final_t + grid[0])\n\n<code_two>:\ntrue_values = tf.expand_dims(tf.math.exp(final_t + grid[0]), axis=0)\n\nFix_pattern:\nIn the condition of no clear condition needed, if the code \"true_values = tf.math.exp(final_t + grid[0])\" is detected, then the fix is to remove it and add \"true_values = tf.expand_dims(tf.math.exp(final_t + grid[0]), axis=0)\" to fix the API misuse."}
{"number": 1066, "change": "class LocalMultiGPUOptimizer(PolicyOptimizer):\nelse:\nrnn_inputs = []\nself.par_opt = LocalSyncParallelOptimizer(\n-                        tf.train.AdamOptimizer(\n-                            self.sgd_stepsize), self.devices,\n+                        self.policy.optimizer(), self.devices,\n[v for _, v in self.policy.loss_inputs()], rnn_inputs,\nself.per_device_batch_size, self.policy.copy,\nos.getcwd())\n", "fix_pattern": "<condition>: In a class called LocalMultiGPUOptimizer, within an else statement.\n\n<pattern>: Using tf.train.AdamOptimizer with a parameter called sgd_stepsize and a parameter called devices.\n\n<code_one>: tf.train.AdamOptimizer(self.sgd_stepsize), self.devices,\n\n<code_two>: self.policy.optimizer(), self.devices,\n\nFix_pattern: In the condition of an else statement in the LocalMultiGPUOptimizer class, if the pattern of using tf.train.AdamOptimizer with sgd_stepsize and devices is detected, then remove tf.train.AdamOptimizer(self.sgd_stepsize), self.devices, and replace it with self.policy.optimizer(), self.devices, to fix the API misuse."}
{"number": 1067, "change": "class DistributedReplicatedBuilder(DataParallelBuilder):\nreturn grads\n\n# Ngpu * Nvar * 2\n-        grad_list = self.build_on_multi_tower(\n-            get_grads,\n+        grad_list = DataParallelBuilder.build_on_towers(\n+            self.towers, get_grads,\ndevices=self.raw_devices,\nuse_vs=[True] * len(self.towers))  # open vs at each tower\nDataParallelBuilder._check_grad_list(grad_list)\n", "fix_pattern": "<condition>:\nThe code is part of a class method that extends a parent class.\n\n<pattern>:\nThe code is using a method from the parent class that has been deprecated or removed.\n\n<code_one>:\nself.build_on_multi_tower(get_grads)\n\n<code_two>:\nDataParallelBuilder.build_on_towers(self.towers, get_grads)\n\nFix_pattern:\nIn the condition of the class method, if the method build_on_multi_tower is detected, then change the code self.build_on_multi_tower(get_grads) to DataParallelBuilder.build_on_towers(self.towers, get_grads) to fix the API misuse."}
{"number": 1069, "change": "class OPTForSequenceClassification(OPTPreTrainedModel):\nsequence_lengths = -1\nelse:\nif input_ids is not None:\n-                sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1\n+                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)\nelse:\nsequence_lengths = -1\nlogger.warning(\n", "fix_pattern": "<condition>: The condition of the if statement is checking if input_ids is not None.\n<pattern>: The pattern is detecting the use of torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1 to calculate the sequence_lengths.\n<code_one>: The code being removed is sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1.\n<code_two>: The code being added is sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device).\nFix_pattern: In the condition of input_ids not being None, if the pattern of calculating sequence_lengths using torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1 is detected, then remove the code sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1 and add the code sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device) to fix the API misuse."}
{"number": 1072, "change": "def test_discrete_parallel(continuous_class):\n\ndef model(data):\nweights = pyro.sample('weights', dist.Dirichlet(0.5 * torch.ones(K)))\n-        locs = pyro.sample('locs', dist.Normal(0, 10).reshape([K], extra_event_dims=1))\n+        locs = pyro.sample('locs', dist.Normal(0, 10).expand_by([K]).independent(1))\nscale = pyro.sample('scale', dist.LogNormal(0, 1))\n\nwith pyro.iarange('data', len(data)):\n", "fix_pattern": "<condition>: No pre condition is needed\n<pattern>: The code used the `reshape` method to reshape a distribution, but it should use the `expand_by` method instead.\n<code_one>: `locs = pyro.sample('locs', dist.Normal(0, 10).reshape([K], extra_event_dims=1))`\n<code_two>: `locs = pyro.sample('locs', dist.Normal(0, 10).expand_by([K]).independent(1))`\nFix_pattern: In the condition of no pre condition is needed, if the code line `<code_one>` is detected, then change it to `<code_two>` to fix the API misuse."}
{"number": 1077, "change": "class MaskTokensDataset(BaseWrapperDataset):\nif self.mask_whole_words is not None:\nmask = np.repeat(mask, word_lens)\nnew_item = np.full(len(mask), self.pad_idx)\n-                new_item[mask] = item[torch.from_numpy(mask)]\n+                new_item[mask] = item[torch.from_numpy(mask.astype(np.uint8))]\nreturn torch.from_numpy(new_item)\n\n# decide unmasking and random replacement\n", "fix_pattern": "<condition>: The condition is when `self.mask_whole_words` is not `None`.\n<pattern>: The pattern is `new_item[mask] = item[torch.from_numpy(mask)]`.\n<code_one>: The code being removed is `new_item[mask] = item[torch.from_numpy(mask)]`.\n<code_two>: The code being added is `new_item[mask] = item[torch.from_numpy(mask.astype(np.uint8))]`.\nFix_pattern: In the condition of `self.mask_whole_words is not None`, if the pattern `new_item[mask] = item[torch.from_numpy(mask)]` is detected, then change the code from `new_item[mask] = item[torch.from_numpy(mask)]` to `new_item[mask] = item[torch.from_numpy(mask.astype(np.uint8))]` to fix the API misuse."}
{"number": 1079, "change": "def _sample_coalescent_times(leaf_times):\ncoal_times.append(t)\ncoal_times.reverse()\n\n-    return torch.tensor(coal_times)\n+    return proto.new_tensor(coal_times)\n", "fix_pattern": "<condition>: The condition is not specified in the given information. No pre-condition is needed.\n\n<pattern>: The pattern is not specified in the given information. No pattern is identified.\n\n<code_one>: The code that needs to be removed is \"return torch.tensor(coal_times)\".\n\n<code_two>: The code that needs to be added is \"return proto.new_tensor(coal_times)\".\n\nFix_pattern: In the condition where no clear condition is identified, if the code \"return torch.tensor(coal_times)\" is detected, then it should be replaced with \"return proto.new_tensor(coal_times)\" to fix the API misuse."}
{"number": 1083, "change": "class CategoricalAccuracy(Metric):\n# ith entry in gold_labels points to index (0-num_classes) for ith row in max_predictions\n# For each row check if index pointed by gold_label is was 1 or not (among max scored classes)\ncorrect = max_predictions_mask[\n-                torch.arange(gold_labels.numel()).long(), gold_labels\n+                torch.arange(gold_labels.numel(), device=gold_labels.device).long(), gold_labels\n].float()\ntie_counts = max_predictions_mask.sum(-1)\ncorrect /= tie_counts.float()\n", "fix_pattern": "<condition>: The code is checking if a certain condition is met.\n<pattern>: The pattern being detected is using an incorrect range of indices.\n<code_one>: The code is using torch.arange with the number of elements in \"gold_labels\" as the range.\n<code_two>: The correct code should use torch.arange with the number of elements in \"gold_labels\" and the device of \"gold_labels\" as the range.\nFix_pattern: In the condition of checking if a certain condition is met, if the pattern of using an incorrect range of indices is detected, then change the code from using torch.arange with the number of elements in \"gold_labels\" as the range to using torch.arange with the number of elements in \"gold_labels\" and the device of \"gold_labels\" as the range to fix the API misuse."}
{"number": 1091, "change": "class Model(ModelDesc):\n.GlobalAvgPooling('gap')\n.FullyConnected('linear', 1000, nl=tf.identity)())\n\n-        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)\n+        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\nloss = tf.reduce_mean(loss, name='xentropy-loss')\n\nwrong = prediction_incorrect(logits, label, 1, name='wrong-top1')\n", "fix_pattern": "<condition>: The condition is when using the function `tf.nn.sparse_softmax_cross_entropy_with_logits()`.\n\n<pattern>: The pattern is that the arguments `logits` and `label` are not explicitly defined.\n\n<code_one>: The original code does not specify the argument names for `logits` and `label`.\n\n<code_two>: The fixed code specifies the argument names for `logits` and `label` as `logits=logits` and `labels=label`, respectively.\n\nFix_pattern: In the condition of using `tf.nn.sparse_softmax_cross_entropy_with_logits()`, if the pattern of not specifying argument names for `logits` and `label` is detected, then change the code from `<code_one>` to `<code_two>` to fix the API misuse."}
{"number": 1094, "change": "def batchnorm_example(optimizer_fn,\nfor z in range(batch_per_epoch)]).repeat()\n\noptimizer = optimizer_fn()\n-  batchnorm = tf.compat.v1.layers.BatchNormalization(\n+  batchnorm = normalization.BatchNormalization(\nrenorm=renorm, momentum=momentum, fused=False)\n-  layer = tf.compat.v1.layers.Dense(1, use_bias=False)\n+  layer = core.Dense(1, use_bias=False)\n\ndef model_fn(x):\n\"\"\"A model that uses batchnorm.\"\"\"\n", "fix_pattern": "<condition>: The condition is when there is an API misuse related to the use of batch normalization.\n\n<pattern>: The pattern is the incorrect usage of tf.compat.v1.layers.BatchNormalization.\n\n<code_one>: The code that needs to be removed is `batchnorm = tf.compat.v1.layers.BatchNormalization(` and the subsequent line.\n\n<code_two>: The code that needs to be added is `batchnorm = normalization.BatchNormalization(`.\n\nFix_pattern: In the condition of an API misuse related to batch normalization, if the incorrect usage of `tf.compat.v1.layers.BatchNormalization` is detected, then remove the line `batchnorm = tf.compat.v1.layers.BatchNormalization(` and replace it with `batchnorm = normalization.BatchNormalization(` to fix the issue."}
{"number": 1104, "change": "class EpsilonDecay(Exploration):\n\npred = tf.logical_or(x=(timestep < self.start_timestep),\ny=(timestep > self.start_timestep + int(self.timesteps)))\n-        return tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn)\n+        return tf.fill(dims=shape, value=tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))\n", "fix_pattern": "<condition>:\nA condition in which a predicate variable \"pred\" is used.\n\n<pattern>:\nThe pattern that needs to be detected is the usage of the \"tf.cond\" function.\n\n<code_one>:\nThe code that needs to be removed is the usage of the \"tf.cond\" function:\n\nreturn tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn)\n\n<code_two>:\nThe code that needs to be added is using the \"tf.fill\" function with the \"tf.cond\" function:\n\nreturn tf.fill(dims=shape, value=tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))\n\nFix_pattern:\nIn the condition of using the predicate variable \"pred\", if the pattern of using the \"tf.cond\" function is detected, then the code using \"tf.cond\" can be replaced by using \"tf.fill\" with the same \"tf.cond\" function to fix the API misuse."}
{"number": 1114, "change": "class Stft(torch.nn.Module, InversibleInterface):\npad = self.n_fft // 2\nilens = ilens + 2 * pad\n\n-            olens = (\n-                torch.div((ilens - self.n_fft), self.hop_length, rounding_mode=\"floor\")\n-                + 1\n-            )\n+            olens = torch.div((ilens - self.n_fft), self.hop_length) + 1\noutput.masked_fill_(make_pad_mask(olens, output, 1), 0.0)\nelse:\nolens = None\n", "fix_pattern": "<condition>:\nThe condition is not clearly mentioned in the context section.\n\n<pattern>:\nThe pattern is identified as the calculation of `olens` using `torch.div((ilens - self.n_fft), self.hop_length, rounding_mode=\"floor\") + 1`.\n\n<code_one>:\nThe code that was removed is `olens = torch.div((ilens - self.n_fft), self.hop_length, rounding_mode=\"floor\") + 1`.\n\n<code_two>:\nThe code that was added is `olens = torch.div((ilens - self.n_fft), self.hop_length) + 1`.\n\nFix_pattern:\nIn the condition of API misuse, if the calculation of `olens` using `torch.div` with the `rounding_mode=\"floor\"` parameter is detected, then remove the code `torch.div((ilens - self.n_fft), self.hop_length, rounding_mode=\"floor\") + 1` and change it to `torch.div((ilens - self.n_fft), self.hop_length) + 1` to fix the API misuse."}
{"number": 1118, "change": "def _scale_channel(im: torch.Tensor) -> torch.Tensor:\n# and then normalization by step.\nlut = (torch.cumsum(histo, 0) + (step // 2)) // step\n# Shift lut, prepending with 0.\n-        lut = torch.cat([torch.zeros(1, device=lut.device), lut[:-1]])\n+        lut = torch.cat([torch.zeros(1, device=lut.device, dtype=lut.dtype), lut[:-1]])\n# Clip the counts to be in range.  This is done\n# in the C code for image.point.\nreturn torch.clamp(lut, 0, 255)\n", "fix_pattern": "<condition>: When creating a tensor and concatenating it with another tensor.\n<pattern>: The tensor being concatenated is missing the device and dtype attributes.\n<code_one>: `torch.cat([torch.zeros(1, device=lut.device), lut[:-1]])`\n<code_two>: `torch.cat([torch.zeros(1, device=lut.device, dtype=lut.dtype), lut[:-1]])`\nFix_pattern: In the condition of creating a tensor and concatenating it with another tensor, if the tensor being concatenated is missing the device and dtype attributes, then change `torch.cat([torch.zeros(1, device=lut.device), lut[:-1]])` to `torch.cat([torch.zeros(1, device=lut.device, dtype=lut.dtype), lut[:-1]])` to fix the API misuse."}
{"number": 1122, "change": "class DonutSwinLayer(nn.Module):\n# partition windows\nhidden_states_windows = window_partition(shifted_hidden_states, self.window_size)\nhidden_states_windows = hidden_states_windows.view(-1, self.window_size * self.window_size, channels)\n-        attn_mask = self.get_attn_mask(height_pad, width_pad)\n+        attn_mask = self.get_attn_mask(height_pad, width_pad, dtype=hidden_states.dtype)\nif attn_mask is not None:\nattn_mask = attn_mask.to(hidden_states_windows.device)\n", "fix_pattern": "<condition>: check if attn_mask is not None\n<pattern>: change the dtype parameter in the function call to get_attn_mask\n<code_one>: self.get_attn_mask(height_pad, width_pad)\n<code_two>: self.get_attn_mask(height_pad, width_pad, dtype=hidden_states.dtype)\nFix_pattern: In the condition of checking if attn_mask is not None, if the pattern of using the get_attn_mask function without specifying the dtype parameter is detected, then change the code from self.get_attn_mask(height_pad, width_pad) to self.get_attn_mask(height_pad, width_pad, dtype=hidden_states.dtype) to fix the API misuse."}
{"number": 1126, "change": "for it in range(1000000):\nD_reg = D(G_sample_reg)\n\nmse = torch.sum((X - G_sample_reg)**2, 1)\n-    E_loss = torch.mean(lam1 * mse + lam2 * D_reg)\n+    E_loss = torch.mean(lam1 * mse + lam2 * log(D_reg))\n\nE_loss.backward()\nE_solver.step()\n", "fix_pattern": "<condition>: There is a calculation of E_loss using mse and D_reg variables.\n<pattern>: The code is using the incorrect function log(D_reg) instead of D(G_sample_reg) to calculate D_reg.\n<code_one>: E_loss = torch.mean(lam1 * mse + lam2 * D_reg)\n<code_two>: E_loss = torch.mean(lam1 * mse + lam2 * log(D_reg))\nFix_pattern: \n\nIn the condition of calculating the E_loss using mse and D_reg variables, if the pattern of using the incorrect function log(D_reg) is detected, then change the code from E_loss = torch.mean(lam1 * mse + lam2 * D_reg) to E_loss = torch.mean(lam1 * mse + lam2 * log(D_reg)) to fix the API misuse."}
{"number": 1127, "change": "class VisualBertEmbeddings(nn.Module):\ninputs_embeds = self.word_embeddings(input_ids)\n\nif token_type_ids is None:\n-            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.input_embeds.device)\n+            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n\ntoken_type_embeddings = self.token_type_embeddings(token_type_ids)\n", "fix_pattern": "<condition>: token_type_ids is None\n<pattern>: token_type_ids is set to a tensor of zeros\n<code_one>: token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.input_embeds.device)\n<code_two>: token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\nFix_pattern: In the condition of token_type_ids is None, if token_type_ids is set to a tensor of zeros, then change the code token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.input_embeds.device) to token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device) to fix the API misuse."}
{"number": 1128, "change": "class BCELossMasked(nn.Module):\nReturns:\nloss: An average loss value in range [0, 1] masked by the length.\n\"\"\"\n-        # mask: (batch, max_len, 1)\ntarget.requires_grad = False\nif length is not None:\n-            mask = sequence_mask(sequence_length=length, max_len=target.size(1)).float()\n-            x = x * mask\n-            target = target * mask\n+            # mask: (batch, max_len, 1)\n+            mask = sequence_mask(sequence_length=length, max_len=target.size(1))\nnum_items = mask.sum()\n+            loss = functional.binary_cross_entropy_with_logits(x.masked_select(mask), target.masked_select(mask), pos_weight=self.pos_weight, reduction=\"sum\")\nelse:\n+            loss = functional.binary_cross_entropy_with_logits(x, target, pos_weight=self.pos_weight, reduction=\"sum\")\nnum_items = torch.numel(x)\n-        loss = functional.binary_cross_entropy_with_logits(x, target, pos_weight=self.pos_weight, reduction=\"sum\")\nloss = loss / num_items\nreturn loss\n", "fix_pattern": "<condition>: length is not None\n<pattern>: API misuse\n<code_one>: x = x * mask; target = target * mask\n<code_two>: x.masked_select(mask); target.masked_select(mask)\nFix_pattern: in the condition of length is not None, if API misuse is detected, then change the code x = x * mask; target = target * mask to x.masked_select(mask); target.masked_select(mask) to fix the API misuse."}
{"number": 1132, "change": "class TFFastSpeech(tf.keras.Model):\n== config.decoder_self_attention_params.hidden_size,\nname=\"decoder\",\n)\n-        self.mel_dense = tf.keras.layers.Dense(units=config.num_mels, name=\"mel_before\")\n-        self.postnet = TFTacotronPostnet(config=config, name=\"postnet\")\n+        self.mel_dense = tf.keras.layers.Dense(units=config.num_mels, dtype=tf.float32, name=\"mel_before\")\n+        self.postnet = TFTacotronPostnet(config=config, dtype=tf.float32, name=\"postnet\")\n\nself.setup_inference_fn()\n", "fix_pattern": "<condition>:\nThe condition is the presence of a tf.keras.Model class named TFFastSpeech.\n\n<pattern>:\nThe pattern is the modification of the dtype parameter in the initialization of the mel_dense and postnet variables.\n\n<code_one>:\nThe code that was removed is the definition of mel_dense and postnet without the dtype=tf.float32 parameter.\n\n<code_two>:\nThe code that was added is the definition of mel_dense and postnet with the dtype=tf.float32 parameter.\n\nFix_pattern:\nIn the condition of having a TFFastSpeech model, if the code does not have the dtype=tf.float32 parameter in the initialization of mel_dense and postnet, then the dtype=tf.float32 parameter should be added to fix the API misuse."}
{"number": 1140, "change": "class Model(object):\n#     raise TensorForceError(\"Invalid model directory/file.\")\n\nself.saver.restore(sess=self.session, save_path=file)\n+        self.session.run(self.buffer_index_reset_op)\n\ndef get_components(self):\n\"\"\"\n", "fix_pattern": "<condition>: In the condition where a TensorForceError is raised with the message \"Invalid model directory/file.\"\n<pattern>: If a restore method is called on a saver object.\n<code_one>: `self.saver.restore(sess=self.session, save_path=file)`\n<code_two>: `self.session.run(self.buffer_index_reset_op)`\nFix_pattern: In the condition of \"Invalid model directory/file.\", if a restore method is called on a saver object, then change `self.saver.restore(sess=self.session, save_path=file)` to `self.session.run(self.buffer_index_reset_op)` to fix the API misuse."}
{"number": 1142, "change": "class TexturesAtlas(TexturesBase):\n# pyre-fixme[16]: `bool` has no attribute `__getitem__`.\nmask = (pix_to_face < 0)[..., None]\nbary_w01 = torch.where(mask, torch.zeros_like(bary_w01), bary_w01)\n-        w_xy = (bary_w01 * R).to(torch.int64)  # (N, H, W, K, 2)\n+        # If barycentric coordinates are > 1.0 (in the case of\n+        # blur_radius > 0.0), wxy might be > R. We need to clamp this\n+        # index to R-1 to index into the texture atlas.\n+        w_xy = (bary_w01 * R).to(torch.int64).clamp(max=R - 1)  # (N, H, W, K, 2)\n\nbelow_diag = (\nbary_w01.sum(dim=-1) * R - w_xy.float().sum(dim=-1)\n", "fix_pattern": "<condition>: When using barycentric coordinates in an API misuse scenario.\n<pattern>: The variable `w_xy` is calculated using the expression `(bary_w01 * R).to(torch.int64)`.\n<code_one>: `w_xy = (bary_w01 * R).to(torch.int64)`\n<code_two>: `w_xy = (bary_w01 * R).to(torch.int64).clamp(max=R - 1)`\nFix_pattern: In the condition of using barycentric coordinates, if `w_xy` is calculated as `(bary_w01 * R).to(torch.int64)`, then `w_xy` should be clamped using `.clamp(max=R - 1)` to fix the API misuse."}
{"number": 1144, "change": "class VariationalSparseGP(GPModel):\nM = self.Xu.size(0)\nKuu = self.kernel(self.Xu).contiguous()\nKuu.view(-1)[::M + 1] += self.jitter  # add jitter to the diagonal\n-        Luu = Kuu.cholesky()\n+        Luu = torch.linalg.cholesky(Kuu)\n\nzero_loc = self.Xu.new_zeros(self.u_loc.shape)\nif self.whiten:\n", "fix_pattern": "<condition>: API misuse in using the deprecated `torch.cholesky()` function instead of the recommended `torch.linalg.cholesky()` function.\n<pattern>: Detection of the deprecated `torch.cholesky()` function call.\n<code_one>: `Luu = Kuu.cholesky()`\n<code_two>: `Luu = torch.linalg.cholesky(Kuu)`\nFix_pattern: In the condition of API misuse, if the deprecated `torch.cholesky()` function is detected, then change the `Kuu.cholesky()` to `torch.linalg.cholesky(Kuu)` to fix the API misuse."}
{"number": 1148, "change": "def _matvecmul(x, y):\n\n\ndef _cholesky(x):\n-    return x.sqrt() if x.dim() == 1 else x.cholesky()\n+    return x.sqrt() if x.dim() == 1 else torch.linalg.cholesky(x)\n\n\ndef _transpose(x):\n", "fix_pattern": "<condition>: The condition is when the API `x.cholesky()` is called.\n\n<pattern>: The pattern is the misuse of the API `x.cholesky()`.\n\n<code_one>: The code that needs to be removed is `x.cholesky()`.\n\n<code_two>: The code that needs to be added is `torch.linalg.cholesky(x)`.\n\nFix_pattern: In the condition of calling `x.cholesky()` API, remove the code `x.cholesky()` and replace it with `torch.linalg.cholesky(x)` to fix the API misuse."}
{"number": 1152, "change": "def rnn(step_function, inputs, initial_states,\nnew_states = []\n\n# all this circus is to recover the last vector in the sequence.\n-        begin = tf.pack([tf.shape(outputs)[0] - 1] + [0] * (ndim - 1))\n-        size = tf.pack([1] + [-1] * (ndim - 1))\n-        last_output = tf.slice(outputs, begin, size)\n+        slice_begin = tf.pack([tf.shape(outputs)[0] - 1] + [0] * (ndim - 1))\n+        slice_size = tf.pack([1] + [-1] * (ndim - 1))\n+        last_output = tf.slice(outputs, slice_begin, slice_size)\nlast_output = tf.squeeze(last_output, [0])\n\naxes = [1, 0] + list(range(2, len(outputs.get_shape())))\n", "fix_pattern": "<condition>: The condition is not provided in the given context.\n\n<pattern>: The pattern is to change the `begin` and `size` variables to `slice_begin` and `slice_size` respectively.\n\n<code_one>: The code that needed to be changed is `begin = tf.pack([tf.shape(outputs)[0] - 1] + [0] * (ndim - 1))` and `size = tf.pack([1] + [-1] * (ndim - 1))`.\n\n<code_two>: The updated code is `slice_begin = tf.pack([tf.shape(outputs)[0] - 1] + [0] * (ndim - 1))` and `slice_size = tf.pack([1] + [-1] * (ndim - 1))`.\n\nFix_pattern: In the condition of no clear condition provided, if the pattern of changing `begin` and `size` variables to `slice_begin` and `slice_size` respectively is detected, then modify the code by changing `code_one` to `code_two` to fix the API misuse."}
{"number": 1190, "change": "class TorchCheckpointWrapper(CheckpointWrapper):\n#  --> https://github.com/facebookresearch/fairscale/blob/1e4a503cda8571851a68effd6e504a192838ab06/fairscale/nn/checkpoint/checkpoint_activations.py#L145-L153  # noqa: E501\n# We just patch the forward method to avoid having to proxy all the fields and other methods.\n# The use of weakref here is to prevent creating a ref cycle: m -> m.forward -> m.\n+\n+        assert len(kwargs) == 0  # This way of wrapping only works for positional arguments.\n+\nmodule.forward = functools.partial(  # type: ignore[assignment]\n_checkpointed_forward, type(module).forward, weakref.ref(module)\n)\n", "fix_pattern": "<condition>: When using the TorchCheckpointWrapper class from the fairscale library.\n<pattern>: Detecting an assertion that checks if there are any keyword arguments passed to the wrapped function.\n<code_one>: The removed code is not specified.\n<code_two>: Adding the assert statement `assert len(kwargs) == 0`.\nFix_pattern: In the condition of using the TorchCheckpointWrapper class, if there is an assertion checking for any keyword arguments, then add the assert statement `assert len(kwargs) == 0` to fix the API misuse."}
{"number": 1193, "change": "class RNN(torch.nn.Module):\ndef __init__(self, idim, elayers, cdim, hdim, dropout, typ=\"blstm\"):\nsuper(RNN, self).__init__()\nbidir = typ[0] == \"b\"\n-        self.nblstm = torch.nn.LSTM(idim, cdim, elayers, batch_first=True,\n-                                    dropout=dropout, bidirectional=bidir) if \"lstm\" in typ \\\n+        self.nbrnn = torch.nn.LSTM(idim, cdim, elayers, batch_first=True,\n+                                   dropout=dropout, bidirectional=bidir) if \"lstm\" in typ \\\nelse torch.nn.GRU(idim, cdim, elayers, batch_first=True, dropout=dropout,\nbidirectional=bidir)\nif bidir:\n", "fix_pattern": "<condition>: The condition is when the value of the variable \"typ\" contains the substring \"lstm\".\n\n<pattern>: The pattern is that a torch.nn.LSTM module was being used instead of a torch.nn.GRU module.\n\n<code_one>: The code that was removed is \"self.nblstm = torch.nn.LSTM(idim, cdim, elayers, batch_first=True, dropout=dropout, bidirectional=bidir)\".\n\n<code_two>: The code that was added is \"self.nbrnn = torch.nn.LSTM(idim, cdim, elayers, batch_first=True, dropout=dropout, bidirectional=bidir)\".\n\nFix_pattern: In the condition of \"typ\" containing the substring \"lstm\", if a torch.nn.LSTM module is detected, then the code \"self.nblstm = torch.nn.LSTM(idim, cdim, elayers, batch_first=True, dropout=dropout, bidirectional=bidir)\" should be changed to \"self.nbrnn = torch.nn.LSTM(idim, cdim, elayers, batch_first=True, dropout=dropout, bidirectional=bidir)\" to fix the API misuse."}
{"number": 1200, "change": "class TokenCharactersIndexer(TokenIndexer[List[int]]):\n# Removes the \"dummy token\".\npadded_tokens.pop()\n# Truncates all the tokens to the desired length, and return the result.\n-        return {key: [list(token[:desired_token_length]) for token in padded_tokens]}\n+        return {key: torch.LongTensor([list(token[:desired_token_length])\n+                                       for token in padded_tokens])}\n", "fix_pattern": "<condition>:\nThe condition is not clear in the given context. No pre-condition is needed.\n\n<pattern>:\nThe pattern is to convert a list of lists into a torch LongTensor.\n\n<code_one>:\nThe code that needs to be removed is:\nreturn {key: [list(token[:desired_token_length]) for token in padded_tokens]}\n\n<code_two>:\nThe code that needs to be added is:\nreturn {key: torch.LongTensor([list(token[:desired_token_length]) for token in padded_tokens])}\n\nFix_pattern:\nIn the condition of no pre-condition is needed, if the pattern of converting a list of lists into a torch LongTensor is detected, then remove the code that returns a dictionary with a list comprehension and replace it with code that returns a dictionary with the torch LongTensor constructor."}
{"number": 1201, "change": "class LKJCorrCholesky(TorchDistribution):\nKm1 = self._d - 1\n\nlog_diagonals = x.diagonal(offset=0, dim1=-1, dim2=-2)[..., 1:].log()\n+        # TODO: Figure out why the `device` kwarg to torch.linspace seems to not work in certain situations,\n+        # and a seemingly redundant .to(x.device) is needed below.\nvalues = log_diagonals * torch.linspace(start=Km1 - 1, end=0, steps=Km1,\ndtype=x.dtype,\n-                                                device=x.device).expand_as(log_diagonals)\n+                                                device=x.device).expand_as(log_diagonals).to(x.device)\n\nvalues += log_diagonals.mul(eta.mul(2).add(-2.0))\nvalues = values.sum(-1) + lp\n", "fix_pattern": "<condition>: The condition where the code is used to create a tensor that expands to the same shape as log_diagonals with the same device as x.\n<pattern>: The pattern is that the device keyword argument to torch.linspace does not work in certain situations and a seemingly redundant .to(x.device) is needed.\n<code_one>: The code that was removed is device=x.device.\n<code_two>: The code that was added is .to(x.device).\nFix_pattern: In the condition of creating a tensor that expands to the same shape as log_diagonals with the same device as x, if the issue of the device keyword argument to torch.linspace not working is detected, then add .to(x.device) to fix the API misuse."}
{"number": 1215, "change": "class EKFState(object):\nS = H.mm(P).mm(H.transpose(-1, -2)) + R  # innovation cov\n\nK_prefix = self._cov.mm(H.transpose(-1, -2))\n-        dx = K_prefix.mm(torch.gesv(dz, S)[0]).squeeze(1)  # K*dz\n+        dx = K_prefix.mm(torch.gesv(dz.unsqueeze(1), S)[0]).squeeze(1)  # K*dz\nx = self._dynamic_model.geodesic_difference(x, -dx)\n\nI = eye_like(x, self._dynamic_model.dimension)  # noqa: E741\n", "fix_pattern": "<condition>: The code is using torch.gesv() function to solve a linear system.\n<pattern>: The code is not passing the input dz as a 2D tensor to torch.gesv() function.\n<code_one>: dx = K_prefix.mm(torch.gesv(dz, S)[0]).squeeze(1)  # K*dz\n<code_two>: dx = K_prefix.mm(torch.gesv(dz.unsqueeze(1), S)[0]).squeeze(1)  # K*dz\nFix_pattern: In the condition of using torch.gesv() function to solve a linear system, if the input dz is not a 2D tensor, then add .unsqueeze(1) to dz before passing it to torch.gesv() function to fix the API misuse."}
{"number": 1230, "change": "class patch_submodule:\nExamples:\n\n>>> import importlib\n-        >>> from datasets.load import prepare_module\n+        >>> from datasets.load import dataset_module_factory\n>>> from datasets.streaming import patch_submodule, xjoin\n>>>\n-        >>> snli_module_path, _ = prepare_module(\"snli\")\n-        >>> snli_module = importlib.import_module(snli_module_path)\n+        >>> dataset_module = dataset_module_factory(\"snli\")\n+        >>> snli_module = importlib.import_module(dataset_module.module_path)\n>>> patcher = patch_submodule(snli_module, \"os.path.join\", xjoin)\n>>> patcher.start()\n>>> assert snli_module.os.path.join is xjoin\n", "fix_pattern": "<condition>: When importing a module from a specific path and then using that module in the code.\n\n<pattern>: The pattern is to replace the method used for importing the module with a different method.\n\n<code_one>: \"from datasets.load import prepare_module\\nsnli_module_path, _ = prepare_module(\"snli\")\\nsnli_module = importlib.import_module(snli_module_path)\"\n\n<code_two>: \"from datasets.load import dataset_module_factory\\ndataset_module = dataset_module_factory(\"snli\")\\nsnli_module = importlib.import_module(dataset_module.module_path)\"\n\nFix_pattern: In the condition of importing a module from a specific path, if the pattern of calling \"prepare_module\" and \"importlib.import_module\" is detected, then change the code to use \"dataset_module_factory\" and \"importlib.import_module(dataset_module.module_path)\" to fix the API misuse."}
{"number": 1248, "change": "class DeepSpeedZeRoOffload(object):\nself._prefetch_bucket_sz = int(prefetch_bucket_size)\nself._max_reuse_distance_in_numel = int(max_reuse_distance)\nself._max_available_parameters_in_numel = int(max_live_parameters)\n-        self.__allgather_stream = Stream(\n-        ) if overlap_comm else torch.cuda.default_stream()\n+        self.__allgather_stream = get_accelerator().Stream(\n+        ) if overlap_comm else get_accelerator().default_stream()\n\nself.forward_hooks = []\nself.backward_hooks = []\n", "fix_pattern": "<condition>: The code is part of a class called DeepSpeedZeRoOffload.\n\n<pattern>: The code is initializing a stream object.\n\n<code_one>: `self.__allgather_stream = Stream()` or `torch.cuda.default_stream()`\n\n<code_two>: `self.__allgather_stream = get_accelerator().Stream()` or `get_accelerator().default_stream()`\n\nFix_pattern: In the condition of the class DeepSpeedZeRoOffload, if the code initializes a stream object using `Stream()` or `torch.cuda.default_stream()`, then change it to initialize the stream object using `get_accelerator().Stream()` or `get_accelerator().default_stream()` to fix the API misuse."}
{"number": 1257, "change": "class Module(tf.Module):\nelif initializer == 'ones':\ninitializer = tf_util.ones(shape=spec.shape, dtype=spec.type)\nelif initializer == 'constant':\n-            initializer = tf_util.fill(dims=spec.shape, value=self.initialization_scale)\n+            initializer = tf.fill(\n+                dims=spec.shape, value=tf_util.constant(value=initialization_scale, dtype=spec.type)\n+            )\n\n# Variable\nvariable = tf.Variable(\n", "fix_pattern": "<condition>: The condition is when the \"initializer\" variable is set to \"tf_util.fill(dims=spec.shape, value=self.initialization_scale)\".\n<pattern>: The pattern is detected when the initializer is set using the \"tf_util.fill(dims=spec.shape, value=self.initialization_scale)\" method.\n<code_one>: The code that needs to be removed is \"initializer = tf_util.fill(dims=spec.shape, value=self.initialization_scale)\".\n<code_two>: The code that needs to be added is \"initializer = tf.fill(dims=spec.shape, value=tf_util.constant(value=initialization_scale, dtype=spec.type))\".\nFix_pattern: In the condition of the initializer being set with \"tf_util.fill(dims=spec.shape, value=self.initialization_scale)\", the code is changed from \"initializer = tf_util.fill(dims=spec.shape, value=self.initialization_scale)\" to \"initializer = tf.fill(dims=spec.shape, value=tf_util.constant(value=initialization_scale, dtype=spec.type))\" to fix the API misuse."}
{"number": 1266, "change": "PT_SEQUENCE_CLASSIFICATION_SAMPLE = r\"\"\"\n...     \"{checkpoint}\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n-    >>> labels = torch.nn.functional.one_hot(torch.tensor(predicted_class_ids), num_classes=num_labels).to(torch.float)\n+    >>> labels = torch.sum(\n+    ...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n+    ... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```\n\"\"\"\n", "fix_pattern": "<condition>: The code is using torch.nn.functional.one_hot() function to generate one-hot encoded labels for multi-label classification.\n<pattern>: The pattern is that the code is using torch.nn.functional.one_hot() without applying the clone() method to the tensor passed as an argument.\n<code_one>: labels = torch.nn.functional.one_hot(torch.tensor(predicted_class_ids), num_classes=num_labels).to(torch.float)\n<code_two>: labels = torch.sum(torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1).to(torch.float)\nFix_pattern: In the condition of multi-label classification, if the pattern of using torch.nn.functional.one_hot() without applying the clone() method to the tensor is detected, then the <code_one> should be changed to <code_two> to fix the API misuse."}
{"number": 1294, "change": "class SCSEModule(nn.Module):\nnn.Conv2d(in_channels // reduction, in_channels, 1),\nnn.Sigmoid(),\n)\n-        self.sSE = nn.Sequential(nn.Conv2d(in_channels, in_channels, 1), nn.Sigmoid())\n+        self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())\n\ndef forward(self, x):\nreturn x * self.cSE(x) + x * self.sSE(x)\n", "fix_pattern": "<condition>: There is a module called \"SCSEModule\" that inherits from nn.Module, and there is a forward() method in this module.\n<pattern>: The \"sSE\" module is defined as nn.Sequential with nn.Conv2d(in_channels, in_channels, 1) followed by nn.Sigmoid().\n<code_one>: nn.Conv2d(in_channels, in_channels, 1), nn.Sigmoid()\n<code_two>: nn.Conv2d(in_channels, 1, 1), nn.Sigmoid()\nFix_pattern: In the condition of \"SCSEModule\" and in the definition of \"sSE\" module in nn.Sequential, change the first nn.Conv2d's parameters from (in_channels, in_channels, 1) to (in_channels, 1, 1) to fix the API misuse."}
{"number": 1302, "change": "def main(args):  # pylint: disable=redefined-outer-name\npos_weight=torch.tensor(10)) if c.stopnet else None\n\nif args.restore_path:\n-        checkpoint = torch.load(args.restore_path)\n+        checkpoint = torch.load(args.restore_path, map_location='cpu')\ntry:\n# TODO: fix optimizer init, model.cuda() needs to be called before\n# optimizer restore\n", "fix_pattern": "<condition>: The condition is when the variable \"args.restore_path\" is not empty or exists.\n\n<pattern>: The pattern is the addition of a \"map_location\" argument in the \"torch.load\" function.\n\n<code_one>: The code removed is \"checkpoint = torch.load(args.restore_path)\".\n\n<code_two>: The code added is \"checkpoint = torch.load(args.restore_path, map_location='cpu')\".\n\nFix pattern: In the condition of the variable \"args.restore_path\" existing, if the code \"checkpoint = torch.load(args.restore_path)\" is detected, then change it to \"checkpoint = torch.load(args.restore_path, map_location='cpu')\" to fix the API misuse."}
{"number": 1308, "change": "def count_nonzero(\ndef _dtype_count_nonzero(a, axis, dtype):\nif dtype is None:\nreturn torch.count_nonzero(a, dim=axis)\n-        return torch.tensor(torch.count_nonzero(a, dim=axis), dtype=dtype)\n+        return torch.tensor(torch.count_nonzero(a, dim=axis),\n+                            dtype=ivy.as_native_dtype(dtype))\n\nx = _dtype_count_nonzero(a, axis, dtype)\nif not keepdims:\n", "fix_pattern": "<condition>: The condition of the fix pattern is that the variable `dtype` is set to None.\n<pattern>: The pattern to be detected is the direct usage of `torch.count_nonzero(a, dim=axis)` without converting the result to the desired data type.\n<code_one>: The code to be removed is `return torch.tensor(torch.count_nonzero(a, dim=axis), dtype=dtype)`.\n<code_two>: The code to be added is `return torch.tensor(torch.count_nonzero(a, dim=axis), dtype=ivy.as_native_dtype(dtype))`.\nFix_pattern: In the condition of `dtype` being `None`, if the pattern of using `torch.count_nonzero(a, dim=axis)` is detected directly, then change the code from `return torch.tensor(torch.count_nonzero(a, dim=axis), dtype=dtype)` to `return torch.tensor(torch.count_nonzero(a, dim=axis), dtype=ivy.as_native_dtype(dtype))` to fix the API misuse."}
{"number": 1312, "change": "class TorchTensor(AbstractTensor):\n\n\"\"\"\n\n-        assert isinstance(self.child, PointerTensor)\n+        if not isinstance(self.child, PointerTensor):\n+            raise TypeError(\"child should be a PointerTensor\")\n\nps = list(pointers)\nps.append(self)\n", "fix_pattern": "<condition>:\nThe condition is that the variable \"self.child\" should be an instance of the \"PointerTensor\" class.\n\n<pattern>:\nThe pattern is that an assertion check is used to verify if \"self.child\" is an instance of \"PointerTensor\".\n\n<code_one>:\nThe code that was removed is:\n```\nassert isinstance(self.child, PointerTensor)\n```\n\n<code_two>:\nThe code that was added is:\n```\nif not isinstance(self.child, PointerTensor):\n    raise TypeError(\"child should be a PointerTensor\")\n```\n\nFix_pattern:\nIn the condition of \"self.child\" being an instance of \"PointerTensor\", if the pattern of using an assertion check is detected, then the code `assert isinstance(self.child, PointerTensor)` should be removed and replaced with `if not isinstance(self.child, PointerTensor): raise TypeError(\"child should be a PointerTensor\")` to fix the API misuse."}
{"number": 1313, "change": "def unravel_index(\nfor dim in reversed(shape):\noutput.append(temp % dim)\ntemp = temp // dim\n-    return tuple(reversed(output))\n+    return torch.tensor(reversed(output))\n\n\nunravel_index.support_native_out = False\n", "fix_pattern": "<condition>:\nThe condition is when the \"unravel_index\" function is called.\n\n<pattern>:\nThe pattern is the need to reverse the output of the \"unravel_index\" function.\n\n<code_one>:\nThe code being removed is \"return tuple(reversed(output))\".\n\n<code_two>:\nThe code being added is \"return torch.tensor(reversed(output))\".\n\nFix_pattern:\nIn the condition of calling the \"unravel_index\" function, if the pattern of needing to reverse the output is detected, then remove the code \"return tuple(reversed(output))\" and add the code \"return torch.tensor(reversed(output))\" to fix the API misuse."}
{"number": 1315, "change": "class CanineSelfAttention(nn.Module):\n# Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n# masked positions, this operation will create a tensor which is 0.0 for\n# positions we want to attend and -10000.0 for masked positions.\n-                attention_mask = (1.0 - attention_mask.float()) * -10000.0\n+                attention_mask = (1.0 - attention_mask.float()) * torch.finfo(attention_scores.dtype).min\n# Apply the attention mask (precomputed for all layers in CanineModel forward() function)\nattention_scores = attention_scores + attention_mask\n", "fix_pattern": "<condition>: `attention_mask` is a tensor with values of 1.0 and 0.0 indicating positions to attend and masked positions respectively.\n<pattern>: The pattern to detect is when `attention_mask` is multiplied by `(-10000.0)`.\n<code_one>: `attention_mask = (1.0 - attention_mask.float()) * -10000.0`\n<code_two>: `attention_mask = (1.0 - attention_mask.float()) * torch.finfo(attention_scores.dtype).min`\nFix_pattern: In the condition of `attention_mask` being a tensor with values of 1.0 and 0.0, if the pattern of multiplying `attention_mask` by `(-10000.0)` is detected, then change the code `attention_mask = (1.0 - attention_mask.float()) * -10000.0` to `attention_mask = (1.0 - attention_mask.float()) * torch.finfo(attention_scores.dtype).min` to fix the API misuse."}
{"number": 1323, "change": "class TFModel(NNModel, metaclass=TfModelMeta):\nopt_scope = tf.variable_scope(optimizer_scope_name)\nwith opt_scope:\nif learnable_scopes is None:\n-                variables_to_train = tf.trainable_variables()\n+                variables_to_train = tf.global_variables()\nelse:\nvariables_to_train = []\nfor scope_name in learnable_scopes:\n-                    for var in tf.trainable_variables():\n+                    for var in tf.global_variables():\nif scope_name in var.name:\nvariables_to_train.append(var)\n", "fix_pattern": "<condition>:\nThe condition is when there is a need to obtain the list of trainable variables in a TensorFlow model.\n\n<pattern>:\nThe pattern is that the original code was using `tf.trainable_variables()` to obtain the list of trainable variables.\n\n<code_one>:\nThe original code was using `tf.trainable_variables()`.\n\n<code_two>:\nThe fixed code is using `tf.global_variables()`.\n\nFix_pattern:\nIn the condition of needing to obtain the list of trainable variables in a TensorFlow model, if the pattern of using `tf.trainable_variables()` is detected, then change the code from `tf.trainable_variables()` to `tf.global_variables()` to fix the API misuse."}
{"number": 1338, "change": "class EncdecMultiheadAttn(nn.Module):\n\ndef reset_parameters(self):\nnn.init.xavier_uniform_(self.in_proj_weight_q)\n-        nn.init.xavier_uniform_(self.in_proj_weight_kv)\n+        # in_proj_weight_kv has shape [2 * hidden, hidden] but it should be\n+        # initialized like a [hidden, hidden] matrix.\n+        # sqrt(6 / (hidden + hidden)) / sqrt(6 / (2 * hidden + hidden)) = sqrt(1.5)\n+        # therefore xavier_uniform gain should be set to sqrt(1.5).\n+        nn.init.xavier_uniform_(self.in_proj_weight_kv, gain=math.sqrt(1.5))\nnn.init.xavier_uniform_(self.out_proj_weight)\nif self.bias:\nnn.init.constant_(self.in_proj_bias_q, 0.)\n", "fix_pattern": "<condition>: The condition is not provided in the context section. No pre-condition is needed.\n\n<pattern>: The pattern is that the weight matrix \"in_proj_weight_kv\" needs to be initialized differently.\n\n<code_one>: nn.init.xavier_uniform_(self.in_proj_weight_kv)\n\n<code_two>: nn.init.xavier_uniform_(self.in_proj_weight_kv, gain=math.sqrt(1.5))\n\nFix_pattern: In the condition of no pre-condition, if the pattern of initializing \"in_proj_weight_kv\" is detected, then change the code of initializing \"in_proj_weight_kv\" from nn.init.xavier_uniform_(self.in_proj_weight_kv) to nn.init.xavier_uniform_(self.in_proj_weight_kv, gain=math.sqrt(1.5)) to fix the API misuse."}
{"number": 1363, "change": "def get_timestep_embedding(\nassert len(timesteps.shape) == 1, \"Timesteps should be a 1d-array\"\n\nhalf_dim = embedding_dim // 2\n-    exponent = -math.log(max_period) * torch.arange(start=0, end=half_dim, dtype=torch.float32)\n+    exponent = -math.log(max_period) * torch.arange(\n+        start=0, end=half_dim, dtype=torch.float32, device=timesteps.device\n+    )\nexponent = exponent / (half_dim - downscale_freq_shift)\n\n-    emb = torch.exp(exponent).to(device=timesteps.device)\n+    emb = torch.exp(exponent)\nemb = timesteps[:, None].float() * emb[None, :]\n\n# scale embeddings\n", "fix_pattern": "<condition>: The condition is not explicitly mentioned in the given context.\n\n<pattern>: The pattern observed is that the code is being modified to ensure that the device used for computation matches the device of the 'timesteps' input.\n\n<code_one>: The code being modified is the line where the 'emb' tensor is created using 'torch.exp(exponent)'. It does not have the 'device' argument specified.\n\n<code_two>: The modified code adds the 'device' argument to the 'torch.arange' function call when creating the 'exponent' tensor.\n\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then change <code_one> to <code_two> to fix the API misuse."}
{"number": 1373, "change": "def train_func(config):\ntrain_dataset = Subset(train_dataset, list(range(64)))\nvalidation_dataset = Subset(validation_dataset, list(range(64)))\n\n-    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"])\n-    validation_loader = DataLoader(validation_dataset, batch_size=config[\"batch_size\"])\n+    worker_batch_size = config[\"batch_size\"] // train.world_size()\n+\n+    train_loader = DataLoader(train_dataset, batch_size=worker_batch_size)\n+    validation_loader = DataLoader(validation_dataset, batch_size=worker_batch_size)\n\ntrain_loader = train.torch.prepare_data_loader(train_loader)\nvalidation_loader = train.torch.prepare_data_loader(validation_loader)\n", "fix_pattern": "<condition>: The code needs to adjust the batch size of the data loaders based on the number of workers in the training process.\n\n<pattern>: The pattern is to divide the original batch size by the number of workers to calculate the new batch size.\n\n<code_one>: The code that needs to be changed is `batch_size=config[\"batch_size\"]`.\n\n<code_two>: The updated code is `batch_size=worker_batch_size`.\n\nFix_pattern: In the condition of adjusting the batch size based on the number of workers, if `batch_size=config[\"batch_size\"]` is detected, then change it to `batch_size=worker_batch_size` to fix the API misuse."}
{"number": 1397, "change": "def binary_config():\ndef test_binary_input_feature(binary_config: Dict, encoder: str) -> None:\nbinary_config.update({\"encoder\": encoder})\nbinary_input_feature = BinaryInputFeature(binary_config)\n-    binary_tensor = torch.randn([SEQ_SIZE, BINARY_W_SIZE], dtype=torch.float32)\n+    binary_tensor = torch.randn([SEQ_SIZE, BINARY_W_SIZE], dtype=torch.float32).to(DEVICE)\nencoder_output = binary_input_feature(binary_tensor)\nassert encoder_output[\"encoder_output\"].shape[1:] == binary_input_feature.output_shape\n", "fix_pattern": "<condition>: The condition is that the code is performing an API misuse where a tensor is not being moved to a specific device before being used.\n\n<pattern>: The pattern is that the code is missing a call to the \".to(DEVICE)\" method to move the tensor to a specific device.\n\n<code_one>: The code that is removed is \"binary_tensor = torch.randn([SEQ_SIZE, BINARY_W_SIZE], dtype=torch.float32)\".\n\n<code_two>: The code that is added is \"binary_tensor = torch.randn([SEQ_SIZE, BINARY_W_SIZE], dtype=torch.float32).to(DEVICE)\".\n\nFix_pattern: In the condition of API misuse, if the tensor is not moved to the desired device, then add the \".to(DEVICE)\" method to fix the issue."}
{"number": 1399, "change": "class TestEulerFromQuaternion(BaseTester):\ndef test_module(self, device, dtype):\npass\n\n-    def test_jit(self, device, dtype):\n+    def test_dynamo(self, device, dtype, torch_optimizer):\nq = Quaternion.random(batch_size=1)\nq = q.to(device, dtype)\nop = euler_from_quaternion\n-        op_jit = torch.jit.script(op)\n-        assert_close(op(q.w, q.x, q.y, q.z), op_jit(q.w, q.x, q.y, q.z))\n+        op_optimized = torch_optimizer(op)\n+        assert_close(op(q.w, q.x, q.y, q.z), op_optimized(q.w, q.x, q.y, q.z))\n\ndef test_forth_and_back(self, device, dtype):\nq = Quaternion.random(batch_size=2)\n", "fix_pattern": "<condition>: The condition is the existence of the method `test_jit` in the class `TestEulerFromQuaternion`.\n\n<pattern>: The pattern is the usage of `op_jit` to script the `op` method.\n\n<code_one>: The code that was removed is:\n\n```\nop_jit = torch.jit.script(op)\nassert_close(op(q.w, q.x, q.y, q.z), op_jit(q.w, q.x, q.y, q.z))\n```\n\n<code_two>: The code that was added is:\n\n```\nop_optimized = torch_optimizer(op)\nassert_close(op(q.w, q.x, q.y, q.z), op_optimized(q.w, q.x, q.y, q.z))\n```\n\nFix_pattern: In the condition of having the `test_jit` method, if the pattern of using `op_jit` to script the `op` method is detected, then remove the code that scripts the method and add the code that optimizes the `op` method using `torch_optimizer`."}
{"number": 1405, "change": "def run(\nseen, windows, dt = 0, [], (Profile(), Profile(), Profile())\nfor path, im, im0s, vid_cap, s in dataset:\nwith dt[0]:\n-            im = torch.Tensor(im).to(device)\n+            im = torch.Tensor(im).to(model.device)\nim = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\nif len(im.shape) == 3:\nim = im[None]  # expand for batch dim\n", "fix_pattern": "<condition>:\nThe condition is when the variable `im` is a 3-dimensional tensor.\n\n<pattern>:\nThe pattern is using the `torch.Tensor` function to convert `im` to a tensor.\n\n<code_one>:\n`im = torch.Tensor(im).to(device)`\n\n<code_two>:\n`im = torch.Tensor(im).to(model.device)`\n\nFix_pattern:\nIn the condition of `len(im.shape) == 3`, if the pattern of using `torch.Tensor` to convert `im` to a tensor is detected, then change the line `im = torch.Tensor(im).to(device)` to `im = torch.Tensor(im).to(model.device)` to fix the API misuse."}
{"number": 1415, "change": "class SwapBufferManager(object):\nself.count = count\nself.dtype = dtype\nself.all_buffers = [\n-            torch.zeros(num_elems,\n-                        device='cpu',\n-                        dtype=dtype).pin_memory() for _ in range(count)\n+            get_accelerator().pin_memory(\n+                torch.zeros(num_elems,\n+                            device='cpu',\n+                            dtype=dtype)) for _ in range(count)\n]\nself.free_buffer_index = [i for i in range(count)]\nself.used_buffer_index = {}\n", "fix_pattern": "<condition>: When initializing buffers in a SwapBufferManager object.\n<pattern>: Using `torch.zeros` with `pin_memory` in a list comprehension.\n<code_one>: `torch.zeros(num_elems, device='cpu', dtype=dtype).pin_memory() for _ in range(count)`\n<code_two>: `get_accelerator().pin_memory(torch.zeros(num_elems, device='cpu', dtype=dtype)) for _ in range(count)`\nFix_pattern: In the condition of initializing buffers in a SwapBufferManager object, if the pattern of using `torch.zeros` with `pin_memory` in a list comprehension is detected, then change the code of `torch.zeros(num_elems, device='cpu', dtype=dtype).pin_memory() for _ in range(count)` to `get_accelerator().pin_memory(torch.zeros(num_elems, device='cpu', dtype=dtype)) for _ in range(count)` to fix the API misuse."}
{"number": 1427, "change": "def test_gcn2_conv():\n\nt = '(Tensor, Tensor, SparseTensor, OptTensor) -> Tensor'\njit = torch.jit.script(conv.jittable(t))\n-    assert torch.allclose(conv(x, x_0, adj1.t()), out1, atol=1e-6)\n-    assert torch.allclose(conv(x, x_0, adj2.t()), out2, atol=1e-6)\n+    assert torch.allclose(jit(x, x_0, adj1.t()), out1, atol=1e-6)\n+    assert torch.allclose(jit(x, x_0, adj2.t()), out2, atol=1e-6)\n\nconv.cached = True\nconv(x, x_0, edge_index)\n", "fix_pattern": "<condition>: The condition is not clearly stated in the context.\n\n<pattern>: The pattern is that the `conv` function is being replaced with the `jit` function.\n\n<code_one>: `conv(x, x_0, adj1.t()), out1, atol=1e-6` and `conv(x, x_0, adj2.t()), out2, atol=1e-6` are being removed.\n\n<code_two>: `jit(x, x_0, adj1.t()), out1, atol=1e-6` and `jit(x, x_0, adj2.t()), out2, atol=1e-6` are being added.\n\nFix_pattern: In the condition of unknown, if the pattern of replacing calls to `conv` with calls to `jit` is detected, then remove `conv(x, x_0, adj1.t()), out1, atol=1e-6` and `conv(x, x_0, adj2.t()), out2, atol=1e-6` and add `jit(x, x_0, adj1.t()), out1, atol=1e-6` and `jit(x, x_0, adj2.t()), out2, atol=1e-6` to fix the API misuse."}
{"number": 1435, "change": "def test_load_dataset_streaming(dataset_loading_script_dir, data_dir):\ndef test_loading_from_the_datasets_hub():\nwith tempfile.TemporaryDirectory() as tmp_dir:\ndataset = load_dataset(SAMPLE_DATASET_IDENTIFIER, cache_dir=tmp_dir)\n-        assert len(dataset[\"train\"]), 2\n-        assert len(dataset[\"validation\"]), 3\n+        assert len(dataset[\"train\"]) == 2\n+        assert len(dataset[\"validation\"]) == 3\ndel dataset\n", "fix_pattern": "<condition>:\nNo pre condition is needed.\n\n<pattern>:\nThe pattern is a missing equality comparison when asserting the length of a dataset.\n\n<code_one>:\nassert len(dataset[\"train\"]), 2\nassert len(dataset[\"validation\"]), 3\n\n<code_two>:\nassert len(dataset[\"train\"]) == 2\nassert len(dataset[\"validation\"]) == 3\n\nFix_pattern:\nIn the condition of no pre condition is needed, if a missing equality comparison pattern is detected, then change the <code_one> to <code_two> to fix the API misuse."}
{"number": 1450, "change": "class BertForQuestionAnswering(nn.Module):\n\ndef compute_loss(logits, positions):\nmax_position = positions.max().item()\n-                one_hot = torch.FloatTensor(batch_size, max(max_position, seq_length) +1, device=input_ids.device).zero_()\n+                one_hot = torch.FloatTensor(batch_size, max(max_position, seq_length) +1).zero_()\none_hot = one_hot.scatter(1, positions.cpu(), 1) # Second argument need to be LongTensor and not cuda.LongTensor\n-                one_hot = one_hot[:, :seq_length]\n+                one_hot = one_hot[:, :seq_length].to(input_ids.device)\nlog_probs = nn.functional.log_softmax(logits, dim = -1).view(batch_size, seq_length)\nloss = -torch.mean(torch.sum(one_hot*log_probs), dim = -1)\nreturn loss\n", "fix_pattern": "<condition>: The code is trying to create a one-hot tensor and modify it for a specific position.\n<pattern>: The code is creating a one-hot tensor with a specific shape.\n<code_one>: `one_hot = torch.FloatTensor(batch_size, max(max_position, seq_length) +1, device=input_ids.device).zero_()`\n<code_two>: `one_hot = torch.FloatTensor(batch_size, max(max_position, seq_length) +1).zero_()\none_hot = one_hot[:, :seq_length].to(input_ids.device)`\nFix_pattern: In the condition of creating a one-hot tensor, if the code is trying to modify it for a specific position, then remove the device assignment from `torch.FloatTensor()` and add `.to(input_ids.device)` after creating the tensor to fix the API misuse."}
{"number": 1463, "change": "def main():\n\n# Save the result as an audio summary.\ndatestring = str(datetime.now()).replace(' ', 'T')\n-    writer = tf.train.SummaryWriter(logdir)\n-    tf.audio_summary('generated', decode, wavenet_params['sample_rate'])\n-    summaries = tf.merge_all_summaries()\n+    writer = tf.summary.FileWriter(logdir)\n+    tf.summary.audio('generated', decode, wavenet_params['sample_rate'])\n+    summaries = tf.summary.merge_all()\nsummary_out = sess.run(summaries,\nfeed_dict={samples: np.reshape(waveform, [-1, 1])})\nwriter.add_summary(summary_out)\n", "fix_pattern": "<condition>: The code is using deprecated functions for creating and merging summaries.\n<pattern>: The code is using tf.train.SummaryWriter, tf.audio_summary, and tf.merge_all_summaries.\n<code_one>: The code is using tf.train.SummaryWriter(logdir), tf.audio_summary('generated', decode, wavenet_params['sample_rate']), and tf.merge_all_summaries().\n<code_two>: The code should be using tf.summary.FileWriter(logdir), tf.summary.audio('generated', decode, wavenet_params['sample_rate']), and tf.summary.merge_all().\nFix_pattern: In the condition of using deprecated tf.train.SummaryWriter, tf.audio_summary, and tf.merge_all_summaries, remove the code_one and replace it with code_two to fix the API misuse."}
{"number": 1465, "change": "def load_indexes():\n\n@st.cache(allow_output_mutation=True)\ndef load_train_data():\n-    eli5 = nlp.load_dataset(\"eli5\", name=\"LFQA_reddit\")\n+    eli5 = datasets.load_dataset(\"eli5\", name=\"LFQA_reddit\")\neli5_train = eli5[\"train_eli5\"]\neli5_train_q_reps = np.memmap(\n\"eli5_questions_reps.dat\", dtype=\"float32\", mode=\"r\", shape=(eli5_train.num_rows, 128)\n", "fix_pattern": "<condition>: When loading indexes.\n<pattern>: Load the dataset using the wrong function.\n<code_one>: eli5 = nlp.load_dataset(\"eli5\", name=\"LFQA_reddit\")\n<code_two>: eli5 = datasets.load_dataset(\"eli5\", name=\"LFQA_reddit\")\nFix_pattern: In the condition of loading indexes, if the code attempts to load the dataset using the function \"nlp.load_dataset\", then change the code to use the function \"datasets.load_dataset\" to fix the API misuse."}
{"number": 1466, "change": "def depthwise_conv2d(\ndilations: Optional[Union[int, Tuple[int, int]]] = 1,\nout: Optional[torch.Tensor] = None,\n) -> torch.Tensor:\n-    x = torch.tensor(x)\n-    filters = torch.tensor(filters)\n+    x = torch.as_tensor(x)\n+    filters = torch.as_tensor(filters)\nstrides = [strides] * 2 if isinstance(strides, int) else strides\nstrides = [strides[1], strides[2]] if len(strides) == 4 else strides\ndilations = [dilations] * 2 if isinstance(dilations, int) else dilations\n-    filters = ivy.squeeze(filters, 3) if filters.ndim == 4 else filters\n+    filters = ivy.squeeze(filters, 3).to_native() if filters.ndim == 4 else filters\n\nf_w_after_dilation = filters.shape[1] + (\n(dilations[1] - 1) * (filters.shape[1] - 1)\n", "fix_pattern": "<condition>: The condition for this fix pattern is not provided in the given context.\n\n<pattern>: The pattern that needs to be detected is when using `torch.tensor` to create a tensor and `ivy.squeeze` to remove dimensions.\n\n<code_one>: The code that needs to be removed is:\n```\nx = torch.tensor(x)\nfilters = torch.tensor(filters)\nfilters = ivy.squeeze(filters, 3) if filters.ndim == 4 else filters\n```\n\n<code_two>: The code that needs to be added is:\n```\nx = torch.as_tensor(x)\nfilters = torch.as_tensor(filters)\nfilters = ivy.squeeze(filters, 3).to_native() if filters.ndim == 4 else filters\n```\n\nFix_pattern: In the condition of the missing condition, if the pattern of creating tensors using `torch.tensor` and using `ivy.squeeze` to remove dimensions is detected, then replace the removed code with the added code to fix the API misuse."}
{"number": 1477, "change": "class RandomMutator(Mutator):\nresult = dict()\nfor mutable in self.mutables:\nif isinstance(mutable, LayerChoice):\n-                gen_index = torch.randint(high=mutable.length, size=(1, ))\n-                result[mutable.key] = F.one_hot(gen_index, num_classes=mutable.length).view(-1).bool()\n+                gen_index = torch.randint(high=len(mutable), size=(1, ))\n+                result[mutable.key] = F.one_hot(gen_index, num_classes=len(mutable)).view(-1).bool()\nelif isinstance(mutable, InputChoice):\nif mutable.n_chosen is None:\nresult[mutable.key] = torch.randint(high=2, size=(mutable.n_candidates,)).view(-1).bool()\n", "fix_pattern": "<condition>: isinstance(mutable, LayerChoice) \n<pattern>: gen_index = torch.randint(high=mutable.length, size=(1, ))\n<code_one>: result[mutable.key] = F.one_hot(gen_index, num_classes=mutable.length).view(-1).bool()\n<code_two>: gen_index = torch.randint(high=len(mutable), size=(1, ))\n<Fix_pattern>: In the condition of isinstance(mutable, LayerChoice), if gen_index = torch.randint(high=mutable.length, size=(1, )) is detected, then remove the code result[mutable.key] = F.one_hot(gen_index, num_classes=mutable.length).view(-1).bool() and add the code gen_index = torch.randint(high=len(mutable), size=(1, )) to fix the API misuse."}
{"number": 1507, "change": "def indices_where(\ndef shape(\nx: Union[tf.Tensor, tf.Variable],\nas_array: bool = False,\n-) -> Union[tf.Tensor, tf.Variable, TensorShape]:\n+) -> Union[tf.Tensor, ivy.Shape, ivy.Array]:\nif as_array:\n-        return tf.shape(x)\n+        return ivy.array(tf.shape(x))\nelse:\n-        return tuple(x.shape)\n+        return ivy.Shape(x.shape)\n\n\ndef get_num_dims(x, as_tensor=False):\n", "fix_pattern": "<condition>: The condition is if the variable 'as_array' is True.\n\n<pattern>: The pattern is the use of different functions to retrieve the shape of the variable based on the value of 'as_array'.\n\n<code_one>: The code that is removed is the usage of 'tf.shape(x)' and 'tuple(x.shape)'.\n\n<code_two>: The code that is added is the usage of 'ivy.array(tf.shape(x))' and 'ivy.Shape(x.shape)'.\n\nFix_pattern: In the condition of 'as_array' being True, if the code uses 'tf.shape(x)' or 'tuple(x.shape)' to retrieve the shape of the variable, then it should be replaced with 'ivy.array(tf.shape(x))' or 'ivy.Shape(x.shape)' to fix the API misuse."}
{"number": 1532, "change": "def remainder(\nres_floored = torch.where(res >= 0, torch.floor(res), torch.ceil(res))\ndiff = res - res_floored\ndiff, x2 = ivy.promote_types_of_inputs(diff, x2)\n-        return torch.mul(diff, x2, out=out).to(x1.dtype)\n+        return torch.round(torch.mul(diff, x2, out=out), out=out).to(x1.dtype)\nreturn torch.remainder(x1, x2, out=out)\n", "fix_pattern": "<condition>:\nThe condition is that the code is attempting to calculate the remainder of two numbers using the torch.remainder() function.\n\n<pattern>:\nThe pattern is that the code was incorrectly using the torch.mul() function instead of torch.round().\n\n<code_one>:\nThe code_one is \"return torch.mul(diff, x2, out=out).to(x1.dtype)\"\n\n<code_two>:\nThe code_two is \"return torch.round(torch.mul(diff, x2, out=out), out=out).to(x1.dtype)\"\n\nFix_pattern:\nIn the condition of using the torch.remainder() function, if the pattern of using torch.mul() is detected, then change the code_one to code_two in order to fix the API misuse."}
{"number": 1535, "change": "def guide(observed_data):\n\n# do variational inference using KL_QP\nprint(\"doing inference with simulated data\")\n-verbose = False\n+verbose = True\nn_steps = 3001\nkl_optim = KL_QP(model, guide, pyro.optim(optim.Adam, {\"lr\": 0.003, \"betas\": (0.93, 0.993)}))\nfor step in range(n_steps):\nloss = kl_optim.step(observed_data)\nif step % 100 == 0:\nif verbose:\n-            print(\"[epoch %d] mean_mu: %.3f\" % (step, pyro.param(\"mean_mu\").data[0, 0]))\n+            print(\"[epoch %d] mean_mu: %.3f\" % (step, pyro.param(\"mean_mu\").data[0]))\nprint(\"[epoch %d] sigma_mu: %.3f\" % (step,\n-                                                 torch.exp(pyro.param(\"log_sigma_mu\")).data[0, 0]))\n+                                                 torch.exp(pyro.param(\"log_sigma_mu\")).data[0]))\nelse:\nprint(\".\", end='')\nsys.stdout.flush()\n", "fix_pattern": "<condition>: If the verbose flag is set to False.\n<pattern>: A print statement that displays \"[epoch %d] mean_mu: %.3f\" % (step, pyro.param(\"mean_mu\").data[0, 0]) with an additional argument torch.exp(pyro.param(\"log_sigma_mu\")).data[0, 0]).\n<code_one>: verbose = False and print(\"[epoch %d] mean_mu: %.3f\" % (step, pyro.param(\"mean_mu\").data[0, 0]))\\n torch.exp(pyro.param(\"log_sigma_mu\")).data[0, 0]).\n<code_two>: verbose = True and print(\"[epoch %d] mean_mu: %.3f\" % (step, pyro.param(\"mean_mu\").data[0]))\\n torch.exp(pyro.param(\"log_sigma_mu\")).data[0]).\nFix_pattern: In the condition of verbose being set to False, if a print statement with the provided pattern is detected, then change <code_one> to <code_two> to fix the API misuse."}
{"number": 1538, "change": "class UNet2DConditionModel(ModelMixin, ConfigMixin):\n# TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n# This would be a good case for the `match` statement (Python 3.10+)\nis_mps = sample.device.type == \"mps\"\n-            if torch.is_floating_point(timesteps):\n+            if isinstance(timestep, float):\ndtype = torch.float32 if is_mps else torch.float64\nelse:\ndtype = torch.int32 if is_mps else torch.int64\n", "fix_pattern": "<condition>: `timesteps` variable is being checked for its data type.\n<pattern>: The condition `torch.is_floating_point(timesteps)` is used to check if `timesteps` is of floating point data type.\n<code_one>: `torch.is_floating_point(timesteps)`\n<code_two>: `isinstance(timestep, float)`\nFix_pattern: In the condition of `timesteps` being checked for its data type, if `timesteps` is detected to be of floating point data type, then replace `torch.is_floating_point(timesteps)` with `isinstance(timestep, float)` to fix the API misuse."}
{"number": 1543, "change": "def test_utilities(head_size):\nmask[head_size:, head_size:] = 0.\nmask.view(-1)[::size + 1][head_size:] = 1.\narrowhead_full = mask * cov\n-    expected = torch.flip(torch.flip(arrowhead_full, (-2, -1)).cholesky(), (-2, -1))\n+    expected = torch.flip(\n+        torch.linalg.cholesky(torch.flip(arrowhead_full, (-2, -1))), (-2, -1)\n+    )\n# test if those flip ops give expected upper triangular values\nassert_close(expected.triu(), expected)\nassert_close(expected.matmul(expected.t()), arrowhead_full)\n", "fix_pattern": "<condition>: No clear condition is needed\n<pattern>: The pattern is to replace the torch.flip(torch.flip(arrowhead_full, (-2, -1)).cholesky(), (-2, -1)) function call with torch.flip(torch.linalg.cholesky(torch.flip(arrowhead_full, (-2, -1))), (-2, -1))\n<code_one>: torch.flip(torch.flip(arrowhead_full, (-2, -1)).cholesky(), (-2, -1))\n<code_two>: torch.flip(torch.linalg.cholesky(torch.flip(arrowhead_full, (-2, -1))), (-2, -1))\nFix_pattern: In the condition of no clear condition needed, if torch.flip(torch.flip(arrowhead_full, (-2, -1)).cholesky(), (-2, -1)) is detected, then change the code to torch.flip(torch.linalg.cholesky(torch.flip(arrowhead_full, (-2, -1))), (-2, -1)) to fix the API misuse."}
{"number": 1565, "change": "class CharacterEmbeddings(TokenEmbeddings):\nlongest_token_in_sentence = max(chars2_length)\ntokens_mask = torch.zeros((len(tokens_sorted_by_length), longest_token_in_sentence),\ndtype=torch.long, device=flair.device)\n+\nfor i, c in enumerate(tokens_sorted_by_length):\n-                tokens_mask[i, :chars2_length[i]] = c\n+                tokens_mask[i, :chars2_length[i]] = torch.tensor(c, dtype=torch.long, device=flair.device)\n\n# chars for rnn processing\nchars = tokens_mask\n", "fix_pattern": "<condition>: The code is dealing with character embeddings in a token-based model.\n<pattern>: The code is assigning values to the `tokens_mask` tensor based on the length of characters in each token.\n<code_one>: `tokens_mask[i, :chars2_length[i]] = c`\n<code_two>: `tokens_mask[i, :chars2_length[i]] = torch.tensor(c, dtype=torch.long, device=flair.device)`\nFix_pattern: In the condition of dealing with character embeddings in a token-based model, if assigning values to the `tokens_mask` tensor based on the length of characters in each token, then change `code_one` to `code_two` to fix the API misuse."}
{"number": 1576, "change": "def init_seeds(seed=0, deterministic=False):\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)  # for Multi-GPU, exception safe\n-    torch.backends.cudnn.benchmark = True  # for faster training\n+    # torch.backends.cudnn.benchmark = True  # AutoBatch problem https://github.com/ultralytics/yolov5/issues/9287\nif deterministic and check_version(torch.__version__, '1.12.0'):  # https://github.com/ultralytics/yolov5/pull/8213\ntorch.use_deterministic_algorithms(True)\ntorch.backends.cudnn.deterministic = True\n", "fix_pattern": "<condition>: `deterministic` is True and `torch.__version__` is greater than or equal to '1.12.0'\n<pattern>: Setting `torch.backends.cudnn.benchmark` to True\n<code_one>: `torch.backends.cudnn.benchmark = True`\n<code_two>: Commented out the line `torch.backends.cudnn.benchmark = True` and added a comment explaining the reason for the change\nFix_pattern: In the condition of `deterministic` being True and `torch.__version__` being greater than or equal to '1.12.0', if the pattern of setting `torch.backends.cudnn.benchmark` to True is detected, then comment out the line `torch.backends.cudnn.benchmark = True` and add a comment explaining the reason for the change to fix the API misuse."}
{"number": 1579, "change": "def inv(\n*,\nout: Optional[Union[tf.Tensor, tf.Variable]] = None,\n) -> Union[tf.Tensor, tf.Variable]:\n-    if tf.math.reduce_any(tf.linalg.det(x) == 0):\n+    if tf.math.reduce_any(tf.linalg.det(tf.cast(x, dtype=\"float64\")) == 0):\nret = x\nelse:\nret = tf.linalg.inv(x)\n", "fix_pattern": "<condition>: None\n<pattern>: tf.linalg.det(x) == 0\n<code_one>: if tf.math.reduce_any(tf.linalg.det(x) == 0):\n<code_two>: if tf.math.reduce_any(tf.linalg.det(tf.cast(x, dtype=\"float64\")) == 0):\nFix_pattern: In the condition of no specific pre-condition, if the pattern tf.linalg.det(x) == 0 is detected, then change the code if tf.math.reduce_any(tf.linalg.det(x) == 0) to if tf.math.reduce_any(tf.linalg.det(tf.cast(x, dtype=\"float64\")) == 0) to fix the API misuse."}
{"number": 1582, "change": "class Init(InsertPostInitMethodToModuleSubClasses):\n\nsee_memory_usage(f'Before partitioning param {param.ds_id} {param.shape}',\nforce=False)\n-            param.data = torch.ones(1).half().to(param.device)\n+            param.data = torch.ones(partitioned_param_data_shape).half().to(param.device)\nsee_memory_usage(f'After partitioning param {param.ds_id} {param.shape}',\nforce=False)\n", "fix_pattern": "<condition>: The code is attempting to assign a tensor to a parameter in an API misuse scenario.\n<pattern>: The tensor being assigned to the parameter is being modified to have a different shape.\n<code_one>: param.data = torch.ones(1).half().to(param.device)\n<code_two>: param.data = torch.ones(partitioned_param_data_shape).half().to(param.device)\nFix_pattern: In the condition of assigning a tensor to a parameter, if the tensor shape needs to be modified, then change the code_one to code_two to fix the API misuse."}
{"number": 1587, "change": "class TorchService(BaseService):\n\n# FLOAT TENSOR FUNCTIONS\ndef hook_float_tensor___init__(service_self):\n-        def new___init__(self, tensor, owner=service_self, *args, **kwargs):\n-            super(torch.FloatTensor, self).__init__(*args, **kwargs)\n-            self = owner.register_object(self, False)\n+        def new___init__(self, *args):\n+            super(torch.FloatTensor, self).__init__()\n+            self = service_self.register_object(self, False)\n\ntorch.FloatTensor.__init__ = new___init__\n", "fix_pattern": "<condition>: When initializing a float tensor in the TorchService class.\n<pattern>: The code for initializing the float tensor is being modified.\n<code_one>: `def new___init__(self, tensor, owner=service_self, *args, **kwargs): super(torch.FloatTensor, self).__init__(*args, **kwargs) self = owner.register_object(self, False)`\n<code_two>: `def new___init__(self, *args): super(torch.FloatTensor, self).__init__() self = service_self.register_object(self, False)`\nFix_pattern: In the condition of initializing a float tensor in the TorchService class, if the pattern for initializing the tensor is detected, then change the `<code_one>` to `<code_two>` to fix the API misuse."}
{"number": 1591, "change": "def demo_gan(checkpoint_paths):\nimg_list = []\nfixed_noise = torch.randn(64, nz, 1, 1)\nfor path in checkpoint_paths:\n-        netG_path = os.path.join(path, \"checkpoint.pt\")\n+        checkpoint_dict = Checkpoint.from_directory(path).to_dict()\nloadedG = Generator()\n-        loadedG.load_state_dict(torch.load(netG_path)[\"netGmodel\"])\n+        loadedG.load_state_dict(checkpoint_dict[\"netGmodel\"])\nwith torch.no_grad():\nfake = loadedG(fixed_noise).detach().cpu()\nimg_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n", "fix_pattern": "<condition>: Loading a generator model from a checkpoint directory.\n<pattern>: Loading the generator model using the state dictionary from the checkpoint file.\n<code_one>: `loadedG.load_state_dict(torch.load(netG_path)[\"netGmodel\"])`\n<code_two>: `loadedG.load_state_dict(checkpoint_dict[\"netGmodel\"])`\nFix_pattern: In the condition of loading a generator model from a checkpoint directory, if the pattern `loadedG.load_state_dict(torch.load(netG_path)[\"netGmodel\"])` is detected, then change the code to `loadedG.load_state_dict(checkpoint_dict[\"netGmodel\"])` to fix the API misuse."}
{"number": 1592, "change": "class PNAConv(MessagePassing):\nreturn y\n\ndef aggregate(self, inputs, index, dim_size=None):\n-        D = get_degree(inputs, index, self.node_dim, dim_size)\n+        D = get_degree(inputs, index, 0, dim_size)\n\n# aggregators\n-        inputs = torch.cat([aggregator(inputs, index, dim=self.node_dim, dim_size=dim_size)\n+        inputs = torch.cat([aggregator(inputs, index, dim=0, dim_size=dim_size)\nfor aggregator in self.aggregators], dim=-1)\n# scalers\nreturn torch.cat([scaler(inputs, D, self.avg_d) for scaler in self.scalers], dim=-1)\n", "fix_pattern": "<condition>: The condition is when the function `aggregate()` is called.\n\n<pattern>: The pattern is that the `dim` parameter of `aggregator()` and `torch.cat()` is set to `self.node_dim`.\n\n<code_one>: The code that needs to be removed is `dim=self.node_dim`.\n\n<code_two>: The code that needs to be added is `dim=0`.\n\nFix_pattern:\nIn the condition of calling the `aggregate()` function, if the `aggregator()` and `torch.cat()` functions have the `dim` parameter set to `self.node_dim`, then remove `dim=self.node_dim` and add `dim=0` to fix the API misuse."}
{"number": 1596, "change": "class FP16_Optimizer(DeepSpeedOptimizer):\nwill call ``model.load_state_dict()`` before\n``fp16_optimizer_instance.load_state_dict()`` is called.\nExample::\n-            model = torch.nn.Linear(D_in, D_out).cuda().half()\n+            model = torch.nn.Linear(D_in, D_out).to(get_accelerator().device_name()).half()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\noptimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)\n...\n", "fix_pattern": "<condition>: The condition is when using the `FP16_Optimizer` class and calling the `load_state_dict()` method.\n\n<pattern>: The pattern is changing the device assignment in the `model` initialization from `.cuda()` to `.to(get_accelerator().device_name())`.\n\n<code_one>: The code being removed is `model = torch.nn.Linear(D_in, D_out).cuda().half()`.\n\n<code_two>: The code being added is `model = torch.nn.Linear(D_in, D_out).to(get_accelerator().device_name()).half()`.\n\nFix_pattern: In the condition of using the `FP16_Optimizer` class and calling the `load_state_dict()` method, if the pattern of initializing the `model` with `.cuda()` is detected, then change the code to initialize the `model` with `.to(get_accelerator().device_name())` to fix the API misuse."}
{"number": 1604, "change": "def read_state_dict(checkpoint_file, print_global_state=False, map_location=None\nif extension.lower() == \".safetensors\":\ndevice = map_location or shared.weight_load_location\nif device is None:\n-            device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n+            device = devices.get_cuda_device_string() if torch.cuda.is_available() else \"cpu\"\npl_sd = safetensors.torch.load_file(checkpoint_file, device=device)\nelse:\npl_sd = torch.load(checkpoint_file, map_location=map_location or shared.weight_load_location)\n", "fix_pattern": "<condition>: The condition is checking if the extension of a file is \".safetensors\".\n<pattern>: The pattern is setting the device variable to \"cuda:0\" if torch.cuda.is_available() is True, otherwise set it to \"cpu\".\n<code_one>: The code that was removed is \"device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\".\n<code_two>: The code that was added is \"device = devices.get_cuda_device_string() if torch.cuda.is_available() else \"cpu\"\".\nFix_pattern: In the condition of checking the file extension, if the pattern of setting the device as \"cuda:0\" is detected, then change the code to set the device using devices.get_cuda_device_string() to fix the API misuse."}
{"number": 1611, "change": "def RemoteTrainer(estimator, metadata, keras_utils, run_id, dataset_idx):\nif LooseVersion(tf.__version__) < LooseVersion(\"2.0.0\"):\nmodel.load_weights(ckpt_file)\nelse:\n-                        model = k.models.load_model(ckpt_file)\n+                        # needs to be deserialized in the with scope\n+                        with k.utils.custom_object_scope(custom_objects):\n+                            model = k.models.load_model(ckpt_file)\nserialized_model = keras_utils.serialize_model(model)\nelse:\nwith open(ckpt_file, 'rb') as f:\n", "fix_pattern": "<condition>: The condition is checking the version of the TensorFlow library being used.\n\n<pattern>: The pattern being detected is the incorrect loading of a trained model without using the custom object scope.\n\n<code_one>: The code being removed is \"model = k.models.load_model(ckpt_file)\".\n\n<code_two>: The code being added is \"with k.utils.custom_object_scope(custom_objects): model = k.models.load_model(ckpt_file)\".\n\nFix_pattern: In the condition of checking the TensorFlow version, if the pattern of incorrect loading of a trained model is detected, then the \"model = k.models.load_model(ckpt_file)\" should be changed to \"with k.utils.custom_object_scope(custom_objects): model = k.models.load_model(ckpt_file)\" to fix the API misuse."}
{"number": 1629, "change": "class MultiHeadSelfAttention(nn.Module):\nq = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)\nscores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)\nmask = (mask == 0).view(mask_reshp).expand_as(scores)  # (bs, n_heads, q_length, k_length)\n-        scores = scores.masked_fill(mask, -float(\"inf\"))  # (bs, n_heads, q_length, k_length)\n+        scores = scores.masked_fill(mask, torch.tensor(-float(\"inf\")))  # (bs, n_heads, q_length, k_length)\n\nweights = nn.functional.softmax(scores, dim=-1)  # (bs, n_heads, q_length, k_length)\nweights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)\n", "fix_pattern": "<condition>: The condition is when working with multi-head self-attention in a neural network module.\n\n<pattern>: The pattern is that the scores tensor needs to be modified with a masked fill operation.\n\n<code_one>: The original code line is `scores = scores.masked_fill(mask, -float(\"inf\"))`\n\n<code_two>: The fixed code line is `scores = scores.masked_fill(mask, torch.tensor(-float(\"inf\")))`\n\nFix_pattern: In the condition of multi-head self-attention, if the scores tensor needs to be modified with a masked fill operation, then change the code line `scores = scores.masked_fill(mask, -float(\"inf\"))` to `scores = scores.masked_fill(mask, torch.tensor(-float(\"inf\")))` to fix the API misuse."}
{"number": 1649, "change": "def get_extensions():\nextra_compile_args = {\"cxx\": []}\ndefine_macros = []\n\n-    if torch.cuda.is_available() and CUDA_HOME is not None:\n+    if (torch.cuda.is_available() and CUDA_HOME is not None) or os.getenv(\"FORCE_CUDA\", \"0\") == \"1\":\nextension = CUDAExtension\nsources += source_cuda\ndefine_macros += [(\"WITH_CUDA\", None)]\n", "fix_pattern": "<condition>: Checking if CUDA is available and CUDA_HOME is not None\n<pattern>: Adding the option to force CUDA usage by setting an environment variable\n<code_one>: `if torch.cuda.is_available() and CUDA_HOME is not None:`\n<code_two>: `if (torch.cuda.is_available() and CUDA_HOME is not None) or os.getenv(\"FORCE_CUDA\", \"0\") == \"1\":`\nFix_pattern: In the condition of checking if CUDA is available and CUDA_HOME is not None, if the pattern `if torch.cuda.is_available() and CUDA_HOME is not None:` is detected, then change the code to `if (torch.cuda.is_available() and CUDA_HOME is not None) or os.getenv(\"FORCE_CUDA\", \"0\") == \"1\":` to fix the API misuse."}
{"number": 1659, "change": "class AlbertMLMHead(nn.Module):\ndef __init__(self, config):\nsuper().__init__()\n\n-        self.LayerNorm = nn.LayerNorm(config.embedding_size)\n+        self.LayerNorm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)\nself.bias = nn.Parameter(torch.zeros(config.vocab_size))\nself.dense = nn.Linear(config.hidden_size, config.embedding_size)\nself.decoder = nn.Linear(config.embedding_size, config.vocab_size)\n", "fix_pattern": "<condition>: \nThe condition is when the AlbertMLMHead class is being initialized.\n\n<pattern>: \nThe pattern is the absence of the 'eps' argument in the LayerNorm constructor.\n\n<code_one>: \nThe code that is removed is 'self.LayerNorm = nn.LayerNorm(config.embedding_size)'.\n\n<code_two>: \nThe code that is added is 'self.LayerNorm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)'.\n\nFix_pattern: \nIn the condition of initializing the AlbertMLMHead class, if the absence of the 'eps' argument in the LayerNorm constructor is detected, then change the code 'self.LayerNorm = nn.LayerNorm(config.embedding_size)' to 'self.LayerNorm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)' to fix the API misuse."}
{"number": 1668, "change": "def run(\nseen, windows, dt = 0, [], (Profile(), Profile(), Profile())\nfor path, im, im0s, vid_cap, s in dataset:\nwith dt[0]:\n-            im = im.to(device)\n+            im = torch.Tensor(im).to(device)\nim = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\nif len(im.shape) == 3:\nim = im[None]  # expand for batch dim\n", "fix_pattern": "<condition>:\nlen(im.shape) == 3\n\n<pattern>:\nim = im.to(device)\n\n<code_one>:\nim = im.to(device)\n\n<code_two>:\nim = torch.Tensor(im).to(device)\n\nFix_pattern:\nIn the condition of \"len(im.shape) == 3\", if the pattern \"im = im.to(device)\" is detected, then change the code \"im = im.to(device)\" to \"im = torch.Tensor(im).to(device)\" to fix the API misuse."}
{"number": 1680, "change": "class ParameterNoise(Exploration):\nelse:\nfor i in range(len(self.noise)):\nself.noise[i] = torch.normal(\n-                    0.0, self.stddev, size=self.noise[i].size())\n+                    mean=torch.zeros(self.noise[i].size()), std=self.stddev)\n\ndef _tf_sample_new_noise_op(self):\nadded_noises = []\n", "fix_pattern": "<condition>: Within the class ParameterNoise(Exploration), in the else block of the _tf_sample_new_noise_op() method.\n\n<pattern>: The pattern is detecting the usage of torch.normal() method.\n\n<code_one>: Within the for loop, torch.normal() is used to generate noise as `torch.normal(0.0, self.stddev, size=self.noise[i].size())`.\n\n<code_two>: The fix is to change `torch.normal(0.0, self.stddev, size=self.noise[i].size())` to `torch.normal(mean=torch.zeros(self.noise[i].size()), std=self.stddev)`.\n\nFix_pattern: In the condition of `ParameterNoise(Exploration)`, if the pattern of using `torch.normal()` is detected, then change the code from `torch.normal(0.0, self.stddev, size=self.noise[i].size())` to `torch.normal(mean=torch.zeros(self.noise[i].size()), std=self.stddev)` to fix the API misuse."}
{"number": 1685, "change": "class DartsLayerChoice(nn.Module):\nyield name, p\n\ndef export(self):\n-        return torch.argmax(self.alpha).item()\n+        return list(self.op_choices.keys())[torch.argmax(self.alpha).item()]\n\n\nclass DartsInputChoice(nn.Module):\ndef __init__(self, input_choice):\nsuper(DartsInputChoice, self).__init__()\n-        self.name = input_choice.key\n+        self.name = input_choice.label\nself.alpha = nn.Parameter(torch.randn(input_choice.n_candidates) * 1e-3)\nself.n_chosen = input_choice.n_chosen or 1\n", "fix_pattern": "<condition>: There is a class called DartsLayerChoice and a method called export.\n<pattern>: The code is using torch.argmax(self.alpha).item() to return the highest value from a list, but the code is incorrect when accessing self.name and the return statement.\n<code_one>: return torch.argmax(self.alpha).item()  and self.name = input_choice.key\n<code_two>: return list(self.op_choices.keys())[torch.argmax(self.alpha).item()] and self.name = input_choice.label\nFix_pattern: In the condition of class DartsLayerChoice and export method, if the incorrect usage of torch.argmax(self.alpha).item() is detected, then remove the return statement and change the assignment of self.name from input_choice.key to input_choice.label to fix the API misuse."}
{"number": 1697, "change": "class Init(InsertPostInitMethodToModuleSubClasses):\nparam.all_gather()\nreturn param._orig_item()\n\n-        def ds_summary(slf: torch.Tensor) -> dict:\n+        def ds_summary(slf: torch.Tensor, use_debug_name: bool = False) -> dict:\nreturn {\n-                \"id\": slf.ds_id,\n+                \"id\": debug_param2name_id(slf) if use_debug_name else slf.ds_id,\n\"status\": slf.ds_status.name,\n\"numel\": slf.numel(),\n\"ds_numel\": slf.ds_numel,\n", "fix_pattern": "<condition>: None identified\n\n<pattern>: If the \"ds_summary\" function is defined with a parameter \"slf\" of type torch.Tensor and a boolean parameter \"use_debug_name\" is added with a default value of False\n\n<code_one>: \"id\": slf.ds_id,\n\n<code_two>: \"id\": debug_param2name_id(slf) if use_debug_name else slf.ds_id,\n\nFix_pattern: In the condition of \"ds_summary\" function, if a boolean parameter \"use_debug_name\" is detected, then change the line \"id\": slf.ds_id, to \"id\": debug_param2name_id(slf) if use_debug_name else slf.ds_id, to fix the API misuse."}
{"number": 1707, "change": "class ScoreSdeVeScheduler(SchedulerMixin, ConfigMixin):\n\n# For small batch sizes, the paper \"suggest replacing norm(z) with sqrt(d), where d is the dim. of z\"\n# sample noise for correction\n-        noise = torch.randn(sample.shape, layout=sample.layout, generator=generator).to(sample.device)\n+        noise = randn_tensor(sample.shape, layout=sample.layout, generator=generator).to(sample.device)\n\n# compute step size from the model_output, the noise, and the snr\ngrad_norm = torch.norm(model_output.reshape(model_output.shape[0], -1), dim=-1).mean()\n", "fix_pattern": "<condition>: The condition is when there is a need to generate a random noise tensor for correction.\n\n<pattern>: The pattern is the usage of the function \"torch.randn\" to generate the noise tensor.\n\n<code_one>: The code that is being removed is \"torch.randn(sample.shape, layout=sample.layout, generator=generator).to(sample.device)\".\n\n<code_two>: The code that is being added is \"randn_tensor(sample.shape, layout=sample.layout, generator=generator).to(sample.device)\".\n\nFix_pattern: In the condition of needing to generate a random noise tensor for correction, if the pattern of using \"torch.randn\" is detected, then remove the code \"torch.randn(sample.shape, layout=sample.layout, generator=generator).to(sample.device)\" and add the code \"randn_tensor(sample.shape, layout=sample.layout, generator=generator).to(sample.device)\" to fix the API misuse."}
{"number": 1719, "change": "def extract_fbank_features(\nif output_path is not None and output_path.is_file() and not overwrite:\nreturn\n\n-    _waveform = waveform * (2 ** 15)  # Kaldi compliance: 16-bit signed integers\n-    _waveform = _waveform.squeeze().numpy()\n+    _waveform = _convert_to_mono(waveform, sample_rate)\n+    _waveform = _waveform * (2 ** 15)  # Kaldi compliance: 16-bit signed integers\n+    _waveform = _waveform.numpy()\n\nfeatures = _get_kaldi_fbank(_waveform, sample_rate, n_mel_bins)\nif features is None:\n", "fix_pattern": "<condition>:\noutput_path is not None and output_path.is_file() and not overwrite\n\n<pattern>:\n_convert_to_mono(waveform, sample_rate)\n\n<code_one>:\n_waveform = waveform * (2 ** 15)  # Kaldi compliance: 16-bit signed integers\n_waveform = _waveform.squeeze().numpy()\n\n<code_two>:\n_waveform = _convert_to_mono(waveform, sample_rate)\n_waveform = _waveform * (2 ** 15)  # Kaldi compliance: 16-bit signed integers\n_waveform = _waveform.numpy()\n\nFix_pattern:\nIn the condition of output_path not being None, output_path being a file, and overwrite not being True, the code converts the waveform to mono using the _convert_to_mono() function and then scales the waveform by multiplying it with (2 ** 15) to ensure it complies with the \"Kaldi compliance\" of having 16-bit signed integers. The fix pattern involves removing the code that squeezes the waveform and converts it to a numpy array, and instead converting the waveform to mono and then scaling it within the same block of code."}
{"number": 1739, "change": "class DistributionStrategyCheckpointTest(test_utils.TestCase,\n\ndef assertRestoreOnCreateInReplicaContext(self, golden, strategy,\nuse_function):\n+    if self.primary_device == \"GPU\":\n+      self.skipTest(\"Currently not working as expected on multiple devices\")\n+      # TODO(b/134376796) renable this once bug is fixed\nwith strategy.scope():\nmodule = golden.create_module()\n", "fix_pattern": "<condition>: The fix pattern applies when there is a need to skip a test based on the condition of the primary device being \"GPU\".\n<pattern>: The pattern to be detected is the presence of the condition for skipping the test.\n<code_one>: The code to be removed is the existing test skipping code.\n<code_two>: The code to be added is a new test skipping code along with a TODO comment.\nFix_pattern: In the condition of checking the primary device, if the test skipping pattern is detected, then remove the existing test skipping code and add a new test skipping code along with a TODO comment to fix the API misuse."}
{"number": 1742, "change": "def run_modelmerger(primary_model_name, secondary_model_name, teritary_model_nam\nif theta_func1:\nfor key in tqdm.tqdm(theta_1.keys()):\nif 'model' in key:\n-                t2 = theta_2.get(key, torch.zeros_like(theta_1[key]))\n-                theta_1[key] = theta_func1(theta_1[key], t2)\n+                if key in theta_2:\n+                    t2 = theta_2.get(key, torch.zeros_like(theta_1[key]))\n+                    theta_1[key] = theta_func1(theta_1[key], t2)\n+                else:\n+                    theta_1[key] = 0\ndel theta_2, teritary_model\n\nfor key in tqdm.tqdm(theta_0.keys()):\n", "fix_pattern": "<condition>: 'model' in key\n<pattern>: If 'model' is detected in the key, then a code block is added to check if the key is in theta_2. If it is, the code runs as before. If it is not, the value for theta_1[key] is set to 0.\n<code_one>: t2 = theta_2.get(key, torch.zeros_like(theta_1[key]))\n<code_two>: \n```\nif key in theta_2:\n    t2 = theta_2.get(key, torch.zeros_like(theta_1[key]))\n    theta_1[key] = theta_func1(theta_1[key], t2)\nelse:\n    theta_1[key] = 0\n```\nFix_pattern: In the condition of 'model' in key, if the key is in theta_2, then t2 is assigned a value from theta_2, and theta_1[key] is updated using theta_func1. If the key is not in theta_2, theta_1[key] is set to 0."}
{"number": 1744, "change": "def get_lst_from_rank0(lst: List[int]) -> None:\nlst_tensor = torch.tensor(\nlst if dist.get_rank() == 0 else [-1] * len(lst),\ndtype=int,\n-        # device=torch.cuda.current_device(),\n-        device=torch.device('cuda:{}'.format(os.environ[\"LOCAL_RANK\"])),\n+        # device=get_accelerator().current_device_name(),\n+        device=torch.device(get_accelerator().device_name(os.environ[\"LOCAL_RANK\"])),\nrequires_grad=False,\n)\ndist.broadcast(lst_tensor, src=0, async_op=False)\n", "fix_pattern": "<condition>: If the code is running on a specific rank (rank 0 in this case).\n<pattern>: The code is using a specific CUDA device based on the rank.\n<code_one>: # device=torch.cuda.current_device(),\n<code_two>: # device=get_accelerator().current_device_name(),\nFix_pattern: In the condition of running on rank 0, if the code is using the specific CUDA device based on the rank, then change # device=torch.cuda.current_device() to # device=get_accelerator().current_device_name() to fix the API misuse."}
{"number": 1747, "change": "def wrong_module(modelstore, sklearn_onnx_model):\n)\ndef test_onnx_save_load(metadata, save_proc, modelstore, sklearn_onnx_model):\nmodel, data = sklearn_onnx_model\n-    info = save_proc(metadata)\n-    assert info.metadata is not None\n-    assert_have_file_extension(info.path, \".onnx\")\n+    model = save_proc(metadata)\n+    assert model.info.metadata is not None\n+    assert_have_file_extension(model.path, \".onnx\")\n\nopts = ort.SessionOptions()\nopts.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL\nopts.log_verbosity_level = 1\n-    loaded = bentoml.onnx.load(info.tag, model_store=modelstore, session_options=opts)\n+    loaded = bentoml.onnx.load(model.tag, model_store=modelstore, session_options=opts)\nassert predict_arr(loaded, data)[0] == 0\n", "fix_pattern": "Condition: The code is calling the `save_proc` function to save the metadata and then using the resulting `info` object to load the model using `bentoml.onnx.load()`.\n\nPattern: The pattern is using the incorrect object (`info`) to load the model from `bentoml.onnx.load()`.\n\nCode One: `loaded = bentoml.onnx.load(info.tag, model_store=modelstore, session_options=opts)`\n\nCode Two: `loaded = bentoml.onnx.load(model.tag, model_store=modelstore, session_options=opts)`\n\nFix Pattern: In the condition of \"calling the `save_proc` function to save the metadata and using the resulting `info` object to load the model\", if the pattern of using `info` object instead of `model` object is detected, then change the `code_one` to `code_two` to fix the API misuse."}
{"number": 1754, "change": "def _precision_to_scale_tril(P):\n# Ref: https://nbviewer.jupyter.org/gist/fehiepsi/5ef8e09e61604f10607380467eb82006#Precision-to-scale_tril\nLf = torch.linalg.cholesky(torch.flip(P, (-2, -1)))\nL_inv = torch.transpose(torch.flip(Lf, (-2, -1)), -2, -1)\n-    L = torch.triangular_solve(\n-        torch.eye(P.shape[-1], dtype=P.dtype, device=P.device), L_inv, upper=False\n-    )[0]\n+    L = torch.linalg.solve_triangular(\n+        L_inv, torch.eye(P.shape[-1], dtype=P.dtype, device=P.device), upper=False\n+    )\nreturn L\n\n\n+@ignore_torch_deprecation_warnings()\ndef _try_possibly_intractable(fn, *args, **kwargs):\n# Convert ValueError into NotImplementedError.\ntry:\n", "fix_pattern": "<condition>: The condition is when the function `torch.triangular_solve` is used.\n\n<pattern>: The pattern is the use of `torch.triangular_solve` to solve a system of linear equations.\n\n<code_one>: The code being removed is the usage of `torch.triangular_solve` with the arguments `torch.eye(P.shape[-1], dtype=P.dtype, device=P.device), L_inv, upper=False)[0]`.\n\n<code_two>: The code being added is the usage of `torch.linalg.solve_triangular` with the arguments `L_inv, torch.eye(P.shape[-1], dtype=P.dtype, device=P.device), upper=False`.\n\nFix_pattern: In the condition of using `torch.triangular_solve`, if the pattern of using `torch.triangular_solve` is detected, then the code `torch.triangular_solve(torch.eye(P.shape[-1], dtype=P.dtype, device=P.device), L_inv, upper=False)[0]` should be removed and `torch.linalg.solve_triangular(L_inv, torch.eye(P.shape[-1], dtype=P.dtype, device=P.device), upper=False)` should be added to fix the API misuse."}
{"number": 1766, "change": "class RandomThinPlateSpline(GeometricAugmentationBase2D):\n\ndef generate_parameters(self, shape: torch.Size) -> Dict[str, Tensor]:\nB, _, _, _ = shape\n-        src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).repeat(B, 1, 1)  # Bx5x2\n+        src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2\ndst = src + self.dist.rsample(src.shape)\nreturn dict(src=src, dst=dst)\n", "fix_pattern": "<condition>: The code is generating parameters for a thin plate spline transformation.\n<pattern>: The `src` tensor is being repeated using the `repeat()` method.\n<code_one>: `src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).repeat(B, 1, 1)`\n<code_two>: `src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)`\nFix_pattern: In the condition of generating parameters for a thin plate spline transformation, if the `src` tensor is being repeated, then change `repeat(B, 1, 1)` to `expand(B, 5, 2)` to fix the API misuse."}
{"number": 1779, "change": "class DepthWarper(nn.Module):\nfactor_y = (self.height - 1) / 2\nfactor_y = factor_y.to(device)\n\n-        z = 1. / flow[:, 2]  # Nx(H*W)\n+        z = 1. / (flow[:, 2] + self.eps)  # Nx(H*W)\nx = (flow[:, 0] * z - factor_x) / factor_x\ny = (flow[:, 1] * z - factor_y) / factor_y\n", "fix_pattern": "<condition>:\nThe condition is the existence of a variable `flow` and its shape being Nx(H*W).\n\n<pattern>:\nThe pattern is the calculation of `z` as 1. / flow[:, 2].\n\n<code_one>:\nThe code one is `z = 1. / flow[:, 2]`.\n\n<code_two>:\nThe code two is `z = 1. / (flow[:, 2] + self.eps)`.\n\nFix_pattern:\nIn the condition of having the variable `flow` with shape Nx(H*W), if the calculation of `z` as 1. / flow[:, 2] is detected, then change `z = 1. / flow[:, 2]` to `z = 1. / (flow[:, 2] + self.eps)` to fix the API misuse."}
{"number": 1781, "change": "def result_wrapper(result_fn):\n# Wrapping result in identity so that control dependency between\n# update_op from `update_state` and result works in case result\n# returns a tensor.\n-                return tf.identity(result)\n+                return tf.nest.map_structure(tf.identity, result)\n\n# Wrapping result in merge_call. merge_call is used when we want to\n# leave replica mode and compute a value in cross replica mode.\n", "fix_pattern": "<condition>:\nNo clear condition is identified.\n\n<pattern>:\ntf.identity(result) is being replaced with tf.nest.map_structure(tf.identity, result).\n\n<code_one>:\nreturn tf.identity(result)\n\n<code_two>:\nreturn tf.nest.map_structure(tf.identity, result)\n\nFix_pattern:\nIn the condition of no clear condition, if tf.identity(result) is detected, then change the return statement from tf.identity(result) to tf.nest.map_structure(tf.identity, result) to fix the API misuse."}
{"number": 1787, "change": "class AdalamFilter:\n)\nk1, k2, d1, d2, o1, o2, s1, s2 = self.__to_torch(k1, k2, d1, d2, o1, o2, s1, s2)\nif len(d2) <= 1:\n-            return _no_match(d1)\n+            idxs, dists = _no_match(d1)\n+            if return_dist:\n+                return idxs, dists\n+            return idxs\ndistmat = dist_matrix(d1, d2, is_normalized=False)\ndd12, nn12 = torch.topk(distmat, k=2, dim=1, largest=False)  # (n1, 2)\n", "fix_pattern": "<condition>: len(d2) <= 1\n<pattern>: _no_match(d1)\n<code_one>: return _no_match(d1)\n<code_two>: idxs, dists = _no_match(d1)\\nif return_dist:\\n    return idxs, dists\\nreturn idxs\nFix_pattern: In the condition where the length of d2 is less than or equal to 1, the code was using the function _no_match(d1) and returning the result directly. The fix includes assigning the result of _no_match(d1) to idxs and dists variables, and then checking if return_dist is True and return the values accordingly."}
{"number": 1795, "change": "def assert_cov_validity(cov, eigenvalue_lbnd=0., condition_number_ubnd=1e6):\n# Symmetry\nassert (cov.t() == cov).all(), 'Covariance must be symmetric!'\n# Precompute eigenvalues for subsequent tests.\n-    ws, _ = torch.symeig(cov)  # The eigenvalues of cov\n+    ws = torch.linalg.eigvalsh(cov)  # The eigenvalues of cov\nw_min = torch.min(ws)\nw_max = torch.max(ws)\n", "fix_pattern": "<condition>:\nThe code contains a function that checks the validity of a covariance matrix. \n\n<pattern>:\nThe pattern detected is an incorrect usage of the torch.symeig() function. \n\n<code_one>:\nThe code was using \"torch.symeig(cov)\" to compute the eigenvalues of the covariance matrix.\n\n<code_two>:\nThe fix replaces \"torch.symeig(cov)\" with \"torch.linalg.eigvalsh(cov)\".\n\nFix_pattern:\nIn the condition of checking the validity of the covariance matrix, if the incorrect usage of \"torch.symeig(cov)\" is detected, then replace it with \"torch.linalg.eigvalsh(cov)\" to fix the API misuse."}
{"number": 1804, "change": "class TestNnUtil(AllenNlpTestCase):\n\"b\": FakeTensor(),\n\"c\": (1, FakeTensor()),\n}\n-        new_device = 4\n+        new_device = torch.device(4)\nmoved_obj = util.move_to_device(structured_obj, new_device)\nassert moved_obj[\"a\"][0].a == 1\nassert moved_obj[\"a\"][0].b._device == new_device\n", "fix_pattern": "<condition>:\nThe condition is when the variable `new_device` is being assigned a numerical value.\n\n<pattern>:\nThe pattern is to replace the numerical value assigned to `new_device` with `torch.device()`.\n\n<code_one>:\nThe code removed is `new_device = 4`.\n\n<code_two>:\nThe code added is `new_device = torch.device(4)`.\n\nFix_pattern:\nIn the condition of `new_device` being assigned a numerical value, if `new_device` is detected, then change the `new_device` assignment from a numerical value to `torch.device()`."}
{"number": 1825, "change": "def test_gaussian_tensordot(dot_dims,\nnb = dot_dims\nnc = y_dim - dot_dims\ntry:\n-        torch.cholesky(x.precision[..., na:, na:] + y.precision[..., :nb, :nb])\n+        torch.linalg.cholesky(x.precision[..., na:, na:] + y.precision[..., :nb, :nb])\nexcept RuntimeError:\npytest.skip(\"Cannot marginalize the common variables of two Gaussians.\")\n", "fix_pattern": "<condition>: The presence of a RuntimeError in the try-except block.\n\n<pattern>: A specific function call using torch.cholesky().\n\n<code_one>: torch.cholesky(x.precision[..., na:, na:] + y.precision[..., :nb, :nb])\n\n<code_two>: torch.linalg.cholesky(x.precision[..., na:, na:] + y.precision[..., :nb, :nb])\n\nFix_pattern: In the condition of a RuntimeError, if the specific function call using torch.cholesky() is detected, then change the code from torch.cholesky() to torch.linalg.cholesky() to fix the API misuse."}
{"number": 1859, "change": "class MultiHeadSelfAttention(Seq2SeqEncoder):\nkeys_per_head = keys_per_head.view(batch_size * num_heads, timesteps, int(self._attention_dim/num_heads))\n\n# shape (num_heads * batch_size, timesteps, timesteps)\n-        scaled_similarities = torch.bmm(queries_per_head, keys_per_head.transpose(1, 2)) / self._scale\n+        scaled_similarities = torch.bmm(queries_per_head / self._scale, keys_per_head.transpose(1, 2))\n\n# shape (num_heads * batch_size, timesteps, timesteps)\n# Normalise the distributions, using the same mask for all heads.\n-        attention = masked_softmax(scaled_similarities, mask.repeat(1, num_heads).view(batch_size * num_heads, timesteps))\n+        attention = masked_softmax(scaled_similarities,\n+                                   mask.repeat(1, num_heads).view(batch_size * num_heads, timesteps),\n+                                   memory_efficient=True)\nattention = self._attention_dropout(attention)\n\n# Take a weighted sum of the values with respect to the attention\n", "fix_pattern": "<condition>: The code is performing self-attention calculations in a multi-head self-attention layer.\n<pattern>: The code is calculating the scaled similarities between queries and keys, and then applying a softmax operation with a mask.\n<code_one>: The code is using torch.bmm to calculate the scaled similarities and then applying masked_softmax function.\n<code_two>: The code is dividing queries_per_head by self._scale before calculating the scaled similarities and is using masked_softmax function with an additional memory_efficient parameter.\nFix_pattern: In the condition of multi-head self-attention, if calculating scaled similarities, then replace the code for calculating scaled similarities by dividing queries_per_head by self._scale with the mask and apply the masked_softmax function."}
{"number": 1873, "change": "class TestScalarMix(AllenNlpTestCase):\nfor k in range(3):\nmean = numpy.mean(tensors[k].data.numpy()[numpy_mask == 1])\nstd = numpy.std(tensors[k].data.numpy()[numpy_mask == 1])\n-            normed_tensor = (tensors[k].data.numpy() - mean) / (std + 1e-12)\n+            normed_tensor = (tensors[k].data.numpy() - mean) / (\n+                std + util.tiny_value_of_dtype(torch.float)\n+            )\nexpected_result += normed_tensor * normed_weights[k]\nexpected_result *= 0.5\n", "fix_pattern": "<condition>: APIs that involve computations with numerical values.\n<pattern>: Inaccurate computation due to incorrect usage of numerical values.\n<code_one>: Incorrect computation using the mean and standard deviation of the tensors.\n<code_two>: Correct computation using the mean and standard deviation of the tensors along with a small constant value.\nFix_pattern: In the condition of numerical value computations, if incorrect computation is detected, then change the computation code to include a small constant value to fix the API misuse."}
{"number": 1880, "change": "class Finfo:\nreturn float(self._tf_finfo.tiny)\n\n\n-def finfo(datatype_in):\n-    return Finfo(tf.experimental.numpy.finfo(datatype_in))\n+def finfo(type):\n+    return Finfo(tf.experimental.numpy.finfo(dtype_from_str(type)))\n\n\nbackend = 'tensorflow'\n", "fix_pattern": "<condition>: The condition is not specified in the given information.\n\n<pattern>: The pattern detected is a change in the input parameter of the `finfo` function. \n\n<code_one>: The original code has `datatype_in` as the input parameter for the `finfo` function.\n\n<code_two>: The fixed code changes the input parameter to `type` in the `finfo` function.\n\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then change the <code_one> to <code_two> to fix the API misuse."}
{"number": 1895, "change": "class Finfo:\nreturn self._torch_finfo.tiny\n\n\n-def finfo(datatype_in):\n-    return Finfo(_torch.finfo(datatype_in))\n+def finfo(type):\n+    return Finfo(_torch.finfo(dtype_from_str(type)))\n\n\nbackend = 'torch'\n", "fix_pattern": "<condition>:\n\nNo clear condition is needed.\n\n<pattern>:\n\nThe pattern is to change the data type input in the finfo function from using a variable \"datatype_in\" to using a variable \"type\".\n\n<code_one>:\n\nThe original code was:\n\n```python\ndef finfo(datatype_in):\n    return Finfo(_torch.finfo(datatype_in))\n```\n\n<code_two>:\n\nThe fixed code is:\n\n```python\ndef finfo(type):\n    return Finfo(_torch.finfo(dtype_from_str(type)))\n```\n\nFix_pattern:\n\nIn the condition of no specific condition needed, if the pattern of using the variable \"datatype_in\" is detected in the function \"finfo\", then the code should be changed from \"def finfo(datatype_in):\" to \"def finfo(type):\" with the additional conversion of the data type using the \"dtype_from_str\" function."}
{"number": 1915, "change": "class PipelineModule(nn.Module):\nself.tied_weight_attrs = {}\n\n# Offset the random seed by the stage ID.\n-        #newseed = torch.cuda.initial_seed() + self._grid.get_stage_id()\n+        #newseed = get_accelerator().initial_seed() + self._grid.get_stage_id()\n#ds_utils.set_random_seed(newseed)\n\n-        #with torch.random.fork_rng(devices=[torch.cuda.current_device()]):\n+        #with torch.random.fork_rng(devices=[get_accelerator().current_device_name()]):\nself._build()\n-        self.to(f'cuda:{self.local_rank}')\n+        self.to(get_accelerator().device_name(self.local_rank))\n\nself.tied_comms = self._index_tied_modules()\nself._synchronize_tied_weights()\n", "fix_pattern": "<condition>: The code is attempting to offset the random seed by the stage ID in a PipelineModule class.\n<pattern>: The original code is using torch.cuda to set the random seed and fork the RNG, but it needs to be replaced with get_accelerator() to get the initial seed and current device name.\n<code_one>: \n'''\n        #newseed = torch.cuda.initial_seed() + self._grid.get_stage_id()\n        #with torch.random.fork_rng(devices=[torch.cuda.current_device()]):\n        self.to(f'cuda:{self.local_rank}')\n'''\n<code_two>:\n'''\n        #newseed = get_accelerator().initial_seed() + self._grid.get_stage_id()\n        #with torch.random.fork_rng(devices=[get_accelerator().current_device_name()]):\n        self.to(get_accelerator().device_name(self.local_rank))\n'''\nFix_pattern: In the condition of offsetting the random seed by the stage ID in a PipelineModule class, if the code is using torch.cuda, then replace it with get_accelerator() to fix the API misuse."}
{"number": 1932, "change": "class HFGPTJLayerPolicy(DSPolicy):\nkw = self.client_module.attn.k_proj.weight\nvw = self.client_module.attn.v_proj.weight\n\n-        qkvw = torch.cat((qw, kw, vw), dim=0)\n+        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0))\n\nreturn self.linear_layer, \\\nqkvw, \\\n", "fix_pattern": "<condition>: The code is an instance method of a class.\n<pattern>: The concatenation of `qw`, `kw`, and `vw` is assigned to `qkvw`.\n<code_one>: `qkvw = torch.cat((qw, kw, vw), dim=0)`\n<code_two>: `qkvw = Parameter(torch.cat((qw, kw, vw), dim=0))`\nFix_pattern: In the condition of an instance method, if the concatenation of `qw`, `kw`, and `vw` is detected, then change `qkvw` assignment from `torch.cat((qw, kw, vw), dim=0)` to `Parameter(torch.cat((qw, kw, vw), dim=0))` to fix the API misuse."}
{"number": 1967, "change": "class Graph:\nif pre_node[temp_id] == temp_id:\nbreak\ntemp_id = pre_node[temp_id]\n-        assert temp_id == pre_node[temp_id]\n+        if temp_id != pre_node[temp_id]:\n+            raise AssertionError(\"Error: main chain end condition not met.\")\nret.reverse()\nreturn ret\n", "fix_pattern": "<condition>: The condition is that `temp_id` is equal to the value of `pre_node[temp_id]`.\n<pattern>: The pattern is that `temp_id` is being asserted to be equal to `pre_node[temp_id]`.\n<code_one>: The code being removed is `assert temp_id == pre_node[temp_id]`.\n<code_two>: The code being added is `if temp_id != pre_node[temp_id]:\\n    raise AssertionError(\"Error: main chain end condition not met.\")`.\nFix_pattern: In the condition of `temp_id` being equal to `pre_node[temp_id]`, if the pattern of `temp_id` being asserted to be equal to `pre_node[temp_id]` is detected, then remove the code `assert temp_id == pre_node[temp_id]` and add the code `if temp_id != pre_node[temp_id]:\\n    raise AssertionError(\"Error: main chain end condition not met.\")` to fix the API misuse."}
{"number": 1988, "change": "class TransformerEncoderLayerBase(nn.Module):\n# the attention weight (before softmax) for some padded element in query\n# will become -inf, which results in NaN in model parameters\nif attn_mask is not None:\n-            attn_mask = attn_mask.masked_fill(attn_mask.to(torch.bool), -1e8)\n+            attn_mask = attn_mask.masked_fill(\n+                attn_mask.to(torch.bool),\n+                -1e8 if x.dtype == torch.float32 else -1e4\n+            )\n\nresidual = x\nif self.normalize_before:\n", "fix_pattern": "<condition>: The condition is when there is an attention mask present (attn_mask is not None).\n<pattern>: The pattern is that the attn_mask is being masked with -1e8.\n<code_one>: The code being removed is \"attn_mask = attn_mask.masked_fill(attn_mask.to(torch.bool), -1e8)\".\n<code_two>: The code being added is \"attn_mask = attn_mask.masked_fill(attn_mask.to(torch.bool), -1e8 if x.dtype == torch.float32 else -1e4)\".\nFix_pattern: In the condition of having an attention mask, if the attn_mask is being masked with -1e8, then the code is changed to mask the attn_mask with -1e8 if x.dtype is torch.float32 else -1e4 to fix the API misuse."}
{"number": 1989, "change": "class QModel(Model):\n\n# If loss clipping is used, calculate the huber loss\nif config.clip_loss > 0.0:\n-                huber_loss = tf.where(condition=(tf.abs(delta) < config.clip_loss), x=(0.5 * self.loss_per_instance), y=(tf.abs(delta) - 0.5))\n+                huber_loss = tf.where(condition=(tf.abs(delta) < config.clip_loss), x=(0.5 * self.loss_per_instance),\n+                                      y=config.clip_loss * tf.abs(delta) - 0.5 * config.clip_loss ** 2)\nself.q_loss = tf.reduce_mean(input_tensor=huber_loss, axis=0)\nelse:\nself.q_loss = tf.reduce_mean(input_tensor=self.loss_per_instance, axis=0)\n", "fix_pattern": "<condition>: The fix is applied when the condition \"tf.abs(delta) < config.clip_loss\" is true.\n<pattern>: The pattern detected is the calculation of the huber loss using the tf.where function.\n<code_one>: The code being removed is \"huber_loss = tf.where(condition=(tf.abs(delta) < config.clip_loss), x=(0.5 * self.loss_per_instance), y=(tf.abs(delta) - 0.5))\".\n<code_two>: The code being added is \"huber_loss = tf.where(condition=(tf.abs(delta) < config.clip_loss), x=(0.5 * self.loss_per_instance), y=config.clip_loss * tf.abs(delta) - 0.5 * config.clip_loss ** 2)\".\nFix_pattern: In the condition of \"tf.abs(delta) < config.clip_loss\", if the pattern of calculating the huber loss using tf.where is detected, then change the code from \"<code_one>\" to \"<code_two>\" to fix the API misuse."}
{"number": 2003, "change": "class DecoderTrainer(nn.Module):\nindex = unet_number - 1\nunet = self.decoder.unets[index]\n\n-        if exists(self.max_grad_norm):\n-            nn.utils.clip_grad_norm_(unet.parameters(), self.max_grad_norm)\n-\noptimizer = getattr(self, f'optim{index}')\nscaler = getattr(self, f'scaler{index}')\n\n+        if exists(self.max_grad_norm):\n+            scaler.unscale_(optimizer)\n+            nn.utils.clip_grad_norm_(unet.parameters(), self.max_grad_norm)\n+\nscaler.step(optimizer)\nscaler.update()\noptimizer.zero_grad()\n", "fix_pattern": "<condition>: The condition is if the `max_grad_norm` parameter exists.\n\n<pattern>: The pattern is to clip the gradient norm of `unet` parameters using `nn.utils.clip_grad_norm_()`.\n\n<code_one>: The code that was removed is `nn.utils.clip_grad_norm_(unet.parameters(), self.max_grad_norm)`.\n\n<code_two>: The code that was added is `scaler.unscale_(optimizer)` before the clipping of gradient norm.\n\nFix_pattern: In the condition of `if exists(self.max_grad_norm)`, if the `max_grad_norm` parameter is detected, then remove the code `nn.utils.clip_grad_norm_(unet.parameters(), self.max_grad_norm)` and add `scaler.unscale_(optimizer)` before the clipping of gradient norm to fix the API misuse."}
{"number": 2005, "change": "def get_tiny_config_from_class(configuration_class):\n\ntry:\nmodel_slug = model_type.replace(\"-\", \"_\")\n-        module = importlib.import_module(f\".test_modeling_{model_slug}\", package=f\"tests.{model_slug}\")\n+        module = importlib.import_module(f\".test_modeling_{model_slug}\", package=f\"tests.models.{model_slug}\")\nmodel_tester_class = getattr(module, f\"{camel_case_model_name}ModelTester\", None)\nexcept (ImportError, AttributeError):\nlogger.error(f\"No model tester class for {configuration_class.__name__}\")\n", "fix_pattern": "<condition>: When importing a module in Python.\n<pattern>: Import statement uses a wrong package path.\n<code_one>: module = importlib.import_module(f\".test_modeling_{model_slug}\", package=f\"tests.{model_slug}\")\n<code_two>: module = importlib.import_module(f\".test_modeling_{model_slug}\", package=f\"tests.models.{model_slug}\")\nFix_pattern: In the condition of importing a module, if the import statement uses the wrong package path, then change the package path from \"tests.{model_slug}\" to \"tests.models.{model_slug}\" to fix the API misuse."}
{"number": 2009, "change": "from ray.rllib.utils import try_import_torch\n_, nn = try_import_torch()\n\n\n-class VisionNetwork(TorchModelV2):\n+class VisionNetwork(TorchModelV2, nn.Module):\n\"\"\"Generic vision network.\"\"\"\n\ndef __init__(self, obs_space, action_space, num_outputs, model_config,\nname):\nTorchModelV2.__init__(self, obs_space, action_space, num_outputs,\nmodel_config, name)\n+        nn.Module.__init__(self)\n\nactivation = get_activation_fn(\nmodel_config.get(\"conv_activation\"), framework=\"torch\")\n", "fix_pattern": "<condition>: No pre-condition needed\n<pattern>: Adding inheritance from nn.Module to the class VisionNetwork\n<code_one>: class VisionNetwork(TorchModelV2):\n<code_two>: class VisionNetwork(TorchModelV2, nn.Module):\nFix_pattern: In the condition of no pre-condition needed, if inheritance from nn.Module is not detected, then add nn.Module to the class VisionNetwork to fix the API misuse."}
{"number": 2024, "change": "class BiLSTM_CRF(nn.Module):\ndef _get_lstm_features(self, sentence):\nself.hidden = self.init_hidden()\nembeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n-        lstm_out, self.hidden = self.lstm(embeds)\n+        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\nlstm_out = lstm_out.view(len(sentence), self.hidden_dim)\nlstm_feats = self.hidden2tag(lstm_out)\nreturn lstm_feats\n", "fix_pattern": "<condition>:\nThe condition is when utilizing the `lstm` function in the `BiLSTM_CRF` class.\n\n<pattern>:\nThe pattern detected is the absence of passing the `self.hidden` parameter to the `lstm` function.\n\n<code_one>:\nThe code that is removed is `lstm_out, self.hidden = self.lstm(embeds)`.\n\n<code_two>:\nThe code that is added is `lstm_out, self.hidden = self.lstm(embeds, self.hidden)`.\n\nFix_pattern:\nIn the condition of utilizing the `lstm` function in the `BiLSTM_CRF` class, if the absence of passing the `self.hidden` parameter is detected, then the code `lstm_out, self.hidden = self.lstm(embeds)` should be changed to `lstm_out, self.hidden = self.lstm(embeds, self.hidden)` to fix the API misuse."}
{"number": 2033, "change": "def configure_logger(verbose: bool) -> None:\nverbose (bool):\n`True` to use verbose logger, `False` otherwise.\n\"\"\"\n-    tf_logger = tf_logging.get_logger()\n+    tf_logger = tf.get_logger()\ntf_logger.handlers = [handler]\nif verbose:\n-        environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\ntf_logging.set_verbosity(tf_logging.INFO)\nlogger.setLevel(logging.DEBUG)\nelse:\nwarnings.filterwarnings('ignore')\n-        environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\ntf_logging.set_verbosity(tf_logging.ERROR)\n", "fix_pattern": "<condition>: When configuring a logger with a verbosity option.\n<pattern>: The TensorFlow logger is being obtained and the environment variables for the minimum log level are being set.\n<code_one>: tf_logger = tf_logging.get_logger()\n<code_two>: tf_logger = tf.get_logger()\nFix_pattern: In the condition of configuring a logger with a verbosity option, if the pattern of obtaining the TensorFlow logger and setting the environment variables for the minimum log level is detected, then change the code `tf_logger = tf_logging.get_logger()` to `tf_logger = tf.get_logger()` to fix the API misuse."}
{"number": 2036, "change": "def test(epoch):\noutput = model(batch_data)\ntest_loss += criterion(output, batch_targets)\npred = output.data.max(1)[1]\n-        correct += pred.long().eq(batch_targets.data.long()).sum()\n+        correct += pred.long().eq(batch_targets.data.long()).cpu().sum()\n\ntest_loss = test_loss.data[0]\ntest_loss /= (test_data.size(0) / TEST_BATCH_SIZE) # criterion averages over batch size\n", "fix_pattern": "<condition>:\nThe condition is not specified in the given context.\n\n<pattern>:\nThe pattern is not specified in the given context.\n\n<code_one>:\nThe code that was removed is `pred.long().eq(batch_targets.data.long()).sum()`.\n\n<code_two>:\nThe code that was added is `.cpu()`.\n\nFix_pattern:\nIn the condition of <condition>, if <pattern> is detected, then change `<code_one>` to `<code_two>` to fix the API misuse."}
{"number": 2049, "change": "class InstanceNormalization(Layer):\n\nreciprocal_stddev = tf.math.rsqrt(x=tf.maximum(x=variance, y=epsilon))\n\n-        x = (x - mean) * reciprocal_stddev\n+        x = (x - tf.stop_gradient(input=mean)) * tf.stop_gradient(input=reciprocal_stddev)\n\nreturn x\n", "fix_pattern": "<condition>: API misuse in the calculation of InstanceNormalization.\n\n<pattern>: The pattern is to replace the calculation expression for \"x\" in InstanceNormalization with a modified expression.\n\n<code_one>: The original code is \"(x - mean) * reciprocal_stddev\".\n\n<code_two>: The fixed code is \"(x - tf.stop_gradient(input=mean)) * tf.stop_gradient(input=reciprocal_stddev)\".\n\nFix_pattern: In the condition of API misuse in the calculation of InstanceNormalization, if the pattern of \"(x - mean) * reciprocal_stddev\" is detected, then the code should be changed to \"(x - tf.stop_gradient(input=mean)) * tf.stop_gradient(input=reciprocal_stddev)\" to fix the API misuse."}
{"number": 2125, "change": "class ClusterLoader(torch.utils.data.DataLoader):\nnode_idx = torch.cat([torch.arange(s, e) for s, e in zip(start, end)])\n\ndata = copy.copy(self.cluster_data.data)\n-        del data.num_nodes\n+        if hasattr(data, '__num_nodes__'):\n+            del data.__num_nodes__\nadj, data.adj = self.cluster_data.data.adj, None\nadj = cat([adj.narrow(0, s, e - s) for s, e in zip(start, end)], dim=0)\nadj = adj.index_select(1, node_idx)\n", "fix_pattern": "<condition>: The code is using the attribute `data.num_nodes`.\n<pattern>: The attribute `data.num_nodes` is replaced with `data.__num_nodes__`.\n<code_one>: `del data.num_nodes`\n<code_two>: `if hasattr(data, '__num_nodes__'):            del data.__num_nodes__`\nFix_pattern: in the condition of using `data.num_nodes`, if `data.num_nodes` is detected, then remove `del data.num_nodes` and add `if hasattr(data, '__num_nodes__'): del data.__num_nodes__` to fix the API misuse."}
{"number": 2133, "change": "class Decoder(torch.nn.Module):\nelse:\nlocal_scores = functional.log_softmax(self.output(z_list[-1]), dim=1).data\nif lpz is not None:\n-                    local_att_best_scores, local_att_best_ids = torch.topk(local_scores, int(beam*1.5), dim=1)\n+                    local_att_best_scores, local_att_best_ids = torch.topk(local_scores, int(beam * CTC_SCORING_RATIO), dim=1)\nctc_scores, ctc_states = ctc_prefix_score(hyp['yseq'], local_att_best_ids[0], hyp['ctc_prev'])\n-                    joint_scores = (1. - ctc_weight) * (local_att_best_scores[0].numpy() + hyp['score']) + ctc_weight * ctc_scores\n-                    joint_best_ids = np.argsort(joint_scores)[:-beam-1:-1]\n+                    joint_scores = (1. - ctc_weight) * \\\n+                        (local_att_best_scores[0].numpy() + hyp['score']) + ctc_weight * ctc_scores\n+                    joint_best_ids = np.argsort(joint_scores)[:-beam - 1:-1]\nlocal_best_ids = local_att_best_ids.numpy()[:, joint_best_ids]\nlocal_best_scores = local_att_best_scores.numpy()[:, joint_best_ids]\nelse:\n", "fix_pattern": "<condition>: The condition is that the variable \"lpz\" is not None.\n<pattern>: The pattern detected is that the code is using the variable \"beam\" multiplied by a number.\n<code_one>: The code that is removed is:\n```\nlocal_att_best_scores, local_att_best_ids = torch.topk(local_scores, int(beam*1.5), dim=1)\njoint_scores = (1. - ctc_weight) * (local_att_best_scores[0].numpy() + hyp['score']) + ctc_weight * ctc_scores\njoint_best_ids = np.argsort(joint_scores)[:-beam-1:-1]\n```\n<code_two>: The code that is added is:\n```\nlocal_att_best_scores, local_att_best_ids = torch.topk(local_scores, int(beam * CTC_SCORING_RATIO), dim=1)\njoint_scores = (1. - ctc_weight) * \\\n                        (local_att_best_scores[0].numpy() + hyp['score']) + ctc_weight * ctc_scores\njoint_best_ids = np.argsort(joint_scores)[:-beam - 1:-1]\n```\nFix_pattern: \nIn the condition of \"lpz is not None\", if the pattern of multiplying \"beam\" by a number is detected, then change the code from \"<code_one>\" to \"<code_two>\" to fix the API misuse."}
{"number": 2134, "change": "class Random(Exploration):\nif explore:\n# Unsqueeze will be unnecessary, once we support batch/time-aware\n# Spaces.\n-            action = torch.IntTensor(self.action_space.sample()).unsqueeze(0)\n+            action = torch.LongTensor(self.action_space.sample()).unsqueeze(0)\nelse:\n-            action = torch.IntTensor(action_dist.deterministic_sample())\n+            action = torch.LongTensor(action_dist.deterministic_sample())\nlogp = torch.zeros((action.size()[0], ), dtype=torch.float32)\nreturn action, logp\n", "fix_pattern": "<condition>: The code is inside an if-else statement where the \"explore\" variable is the condition.\n<pattern>: The code inside the if statement is using a torch.IntTensor data type.\n<code_one>: action = torch.IntTensor(self.action_space.sample()).unsqueeze(0)\n<code_two>: action = torch.LongTensor(self.action_space.sample()).unsqueeze(0)\nFix_pattern: In the condition of \"explore\", if the code is using a torch.IntTensor, then change it to torch.LongTensor to fix the API misuse."}
{"number": 2142, "change": "class Trainer:\nfor name, param in self._model.named_parameters():\nparam_updates[name].sub_(param.detach().cpu())\nupdate_norm = torch.norm(param_updates[name].view(-1, ))\n-                    param_norm = torch.norm(param.view(-1, ))\n+                    param_norm = torch.norm(param.view(-1, )).cpu()\nself._tensorboard.add_train_scalar(\"gradient_update/\" + name,\nupdate_norm / (param_norm + 1e-7),\nbatch_num_total)\n", "fix_pattern": "<condition>: The code is part of a training loop in a class.\n\n<pattern>: The code is calculating the norm of a parameter.\n\n<code_one>: `param_norm = torch.norm(param.view(-1, ))`\n\n<code_two>: `param_norm = torch.norm(param.view(-1, )).cpu()`\n\nFix_pattern: In the condition of a training loop in a class, if calculating the norm of a parameter is detected, then add `.cpu()` after `torch.norm(param.view(-1, ))` to fix the API misuse."}
{"number": 2163, "change": "def main():\n\nmodel = BertForSequenceClassification(bert_config, len(label_list))\nif args.init_checkpoint is not None:\n-        model.load_state_dict(torch.load(args.init_checkpoint, map_location='cpu'))\n+        model.bert.load_state_dict(torch.load(args.init_checkpoint, map_location='cpu'))\nmodel.to(device)\n\nif args.local_rank != -1:\n", "fix_pattern": "<condition>: The code is checking if the argument \"args.init_checkpoint\" is not None.\n<pattern>: There is a code block that loads the model state dictionary from the given checkpoint file.\n<code_one>: model.load_state_dict(torch.load(args.init_checkpoint, map_location='cpu'))\n<code_two>: model.bert.load_state_dict(torch.load(args.init_checkpoint, map_location='cpu'))\nFix_pattern: In the condition of checking if \"args.init_checkpoint\" is not None, if the code block for loading the model state dictionary is detected, then replace \"model.load_state_dict\" with \"model.bert.load_state_dict\" to fix the API misuse."}
{"number": 2166, "change": "class AdditiveSharingTensor(AbstractTensor):\nmask_pos = x > self.max_value\nmask_neg = x < self.min_value\nif mask_pos.any():\n-                mask_pos = mask_pos.long()\n+                mask_pos = mask_pos.type(self.torch_dtype)\nreturn self.modulo(x - (mask_pos * self.field))\nelif mask_neg.any():\n-                mask_neg = mask_neg.long()\n+                mask_neg = mask_neg.type(self.torch_dtype)\nreturn self.modulo(x + (mask_neg * self.field))\nelse:\nreturn x.type(self.torch_dtype)\n", "fix_pattern": "<condition>: There is a condition where both mask_pos and mask_neg are checked for any True values.\n<pattern>: The data types of mask_pos and mask_neg are being modified.\n<code_one>: mask_pos and mask_neg are being cast to the long data type.\n<code_two>: mask_pos and mask_neg are being cast to the data type of self.torch_dtype.\nFix_pattern: In the condition where mask_pos and mask_neg are checked, the data types of mask_pos and mask_neg need to be cast to self.torch_dtype."}
{"number": 2179, "change": "def test_lite_module_forward_conversion(precision, input_type, expected_type):\nassert precision != 16 or torch.is_autocast_enabled()\nreturn forward_input\n\n-    module = Mock(wraps=torch.nn.Linear(1, 1), side_effect=check_autocast)\n+    module = Mock(wraps=torch.nn.Identity(), side_effect=check_autocast)\nlite_module = _LiteModule(module, lite._precision_plugin).to(device)\n-    out = lite_module(torch.rand(1, dtype=input_type, device=device))\n+    out = lite_module(torch.tensor([1, 2, 3], dtype=input_type, device=device))\nassert module.call_args[0][0].dtype == expected_type\n-    assert out.dtype == torch.get_default_dtype()\n+    assert out.dtype == input_type or out.dtype == torch.get_default_dtype()\n\n\ndef test_lite_dataloader_iterator():\n", "fix_pattern": "<condition>: precision != 16 or torch.is_autocast_enabled()\n<pattern>: module = Mock(wraps=torch.nn.Linear(1, 1), side_effect=check_autocast)\n<code_one>: out.dtype == torch.get_default_dtype()\n<code_two>: module = Mock(wraps=torch.nn.Identity(), side_effect=check_autocast)\nFix_pattern: in the condition of precision not equal to 16 or torch.autocast_enabled is True, if a specific pattern of 'module = Mock(wraps=torch.nn.Linear(1, 1), side_effect=check_autocast)' is detected, then change code 'out.dtype == torch.get_default_dtype()' to 'out.dtype == input_type or out.dtype == torch.get_default_dtype()' to fix the API misuse."}
{"number": 2233, "change": "class TileLayer(Layer):\n\n@deprecated_alias(layer='prev_layer', end_support_version=1.9)  # TODO remove this line for the 1.9 release\ndef __init__(self, prev_layer, multiples=None, name='tile'):\n+\nsuper(TileLayer, self).__init__(prev_layer=prev_layer, name=name)\n\nlogging.info(\"TileLayer  %s: multiples:%s\" % (name, multiples))\n\n-        self.inputs = prev_layer.outputs\n-\nwith tf.variable_scope(name):\nself.outputs = tf.tile(self.inputs, multiples=multiples)\n\n-        self.all_layers.append(self.outputs)\n+        self._add_layers(self.outputs)\n", "fix_pattern": "<condition>: The condition is when a deprecated alias is used in the code.\n<pattern>: The pattern is that the \"inputs\" attribute is assigned the value of \"prev_layer.outputs\".\n<code_one>: The code being removed is \"self.inputs = prev_layer.outputs\".\n<code_two>: The code being added is \"self._add_layers(self.outputs)\".\nFix_pattern: In the condition of using a deprecated alias, if the pattern of assigning \"prev_layer.outputs\" to \"self.inputs\" is detected, then remove the line \"self.inputs = prev_layer.outputs\" and add the line \"self._add_layers(self.outputs)\" to fix the API misuse."}
{"number": 2234, "change": "class Trainer(\nif 'scheduler' not in scheduler:\nraise ValueError(f'Lr scheduler should have key `scheduler`',\n' with item being a lr scheduler')\n-                scheduler['reduce_on_plateau'] = \\\n-                    isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau)\n+                scheduler['reduce_on_plateau'] = isinstance(\n+                    scheduler['scheduler'], optim.lr_scheduler.ReduceLROnPlateau)\n\nlr_schedulers.append({**default_config, **scheduler})\n", "fix_pattern": "<condition>: The code condition is checking if the key \"scheduler\" is present in the \"scheduler\" variable.\n<pattern>: The pattern being detected is whether the value of \"scheduler\" variable is an instance of the class \"optim.lr_scheduler.ReduceLROnPlateau\".\n<code_one>: The code that is being removed is the line that sets the value of \"scheduler['reduce_on_plateau']\" to the result of the isinstance check.\n<code_two>: The code that is being added is a modified line that sets the value of \"scheduler['reduce_on_plateau']\" to the result of the isinstance check applied to \"scheduler['scheduler']\".\nFix_pattern: In the condition of checking if the \"scheduler\" key is present, if the value of \"scheduler\" is an instance of \"optim.lr_scheduler.ReduceLROnPlateau\", then change the code setting the value of \"scheduler['reduce_on_plateau']\" to use \"scheduler['scheduler']\" instead of \"scheduler\"."}
{"number": 2264, "change": "class GenerationMixin:\n\n# First if `inputs_embeds` are given, but no `attention_mask` assume that full attention_mask is used\nif inputs_embeds is not None:\n-            return torch.ones((inputs_embeds.shape[0], inputs_embeds.shape[1]), dtype=torch.long)\n+            return torch.ones((inputs_embeds.shape[0], inputs_embeds.shape[1]), dtype=torch.long, device=self.device)\n\n# Otherwise, use `input_ids`\nis_pad_token_in_inputs_ids = (pad_token_id is not None) and (pad_token_id in input_ids)\n", "fix_pattern": "<condition>:\nThe condition is that the `inputs_embeds` variable is not None.\n\n<pattern>:\nThe pattern is detecting when `inputs_embeds` is given but no `attention_mask` is provided.\n\n<code_one>:\nThe code that needs to be removed is `return torch.ones((inputs_embeds.shape[0], inputs_embeds.shape[1]), dtype=torch.long)`.\n\n<code_two>:\nThe code that needs to be added is `return torch.ones((inputs_embeds.shape[0], inputs_embeds.shape[1]), dtype=torch.long, device=self.device)`.\n\nFix_pattern:\nIn the condition of `inputs_embeds is not None`, if `inputs_embeds` is given but no `attention_mask` is provided, then change the `return torch.ones((inputs_embeds.shape[0], inputs_embeds.shape[1]), dtype=torch.long)` code to `return torch.ones((inputs_embeds.shape[0], inputs_embeds.shape[1]), dtype=torch.long, device=self.device)` to fix the API misuse."}
{"number": 2277, "change": "class MishActivation(nn.Module):\n\ndef __init__(self):\nsuper().__init__()\n-        if version.parse(torch.__version__) < version.parse(\"1.9\"):\n+        if version.parse(version.parse(torch.__version__).base_version) < version.parse(\"1.9\"):\nself.act = self._mish_python\nelse:\nself.act = nn.functional.mish\n", "fix_pattern": "<condition>: Checking the version of a software library\n<pattern>: Comparing the version of a software library using the `torch.__version__` attribute\n<code_one>: `if version.parse(torch.__version__) < version.parse(\"1.9\")`\n<code_two>: `if version.parse(version.parse(torch.__version__).base_version) < version.parse(\"1.9\")`\nFix_pattern: In the condition of checking the version of a software library using `torch.__version__`, if the version is less than \"1.9\", then change the code from `if version.parse(torch.__version__) < version.parse(\"1.9\")` to `if version.parse(version.parse(torch.__version__).base_version) < version.parse(\"1.9\")` to fix the API misuse."}
{"number": 2278, "change": "def Aggregate(dim, dim_out):\nreturn nn.Sequential(\nnn.Conv2d(dim, dim_out, 3, padding = 1),\nChanNorm(dim_out),\n-        nn.MaxPool2d(2)\n+        nn.MaxPool2d(3, stride = 2, padding = 1)\n)\n\nclass Transformer(nn.Module):\n", "fix_pattern": "<condition>:\nThe condition is that there is a MaxPool2d operation in the code.\n\n<pattern>:\nThe pattern is that the MaxPool2d operation needs to be modified.\n\n<code_one>:\nThe code that needs to be modified is nn.MaxPool2d(2).\n\n<code_two>:\nThe modified code is nn.MaxPool2d(3, stride=2, padding=1).\n\nFix_pattern:\nIn the condition of having a MaxPool2d operation, if nn.MaxPool2d(2) is detected, then change it to nn.MaxPool2d(3, stride=2, padding=1) to fix the API misuse."}
{"number": 2286, "change": "def predict():\nif __name__ == \"__main__\":\nparser = argparse.ArgumentParser(description=\"Flask API exposing YOLOv5 model\")\nparser.add_argument(\"--port\", default=5000, type=int, help=\"port number\")\n-    args = parser.parse_args()\n+    opt = parser.parse_args()\n+\n+    # Fix known issue urllib.error.HTTPError 403: rate limit exceeded https://github.com/ultralytics/yolov5/pull/7210\n+    torch.hub._validate_not_a_forked_repo = lambda a, b, c: True\n\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", force_reload=True)  # force_reload to recache\n-    app.run(host=\"0.0.0.0\", port=args.port)  # debug=True causes Restarting with stat\n+    app.run(host=\"0.0.0.0\", port=opt.port)  # debug=True causes Restarting with stat\n", "fix_pattern": "<condition>: When running a Flask API script\n<pattern>: API misuse causing a urllib.error.HTTPError 403: rate limit exceeded\n<code_one>: args = parser.parse_args()\n    app.run(host=\"0.0.0.0\", port=args.port)  # debug=True causes Restarting with stat\n<code_two>: opt = parser.parse_args()\n    torch.hub._validate_not_a_forked_repo = lambda a, b, c: True\n    app.run(host=\"0.0.0.0\", port=opt.port)  # debug=True causes Restarting with stat\nFix_pattern: In the condition of running a Flask API script, if API misuse causing a urllib.error.HTTPError 403: rate limit exceeded is detected, then remove the code for parsing args and replace it with a new code for parsing opt, and add a new code to fix the known issue."}
{"number": 2289, "change": "def synthesis(model,\nstyle_mel = compute_style_mel(style_wav, ap, use_cuda)\n# preprocess the given text\ninputs = text_to_seqvec(text, CONFIG, use_cuda)\n-    speaker_id = speaker_id_var = torch.from_numpy(speaker_id).unsqueeze(0)\n+    speaker_id = np.asarray(speaker_id)\n+    speaker_id = torch.from_numpy(speaker_id).unsqueeze(0)\nif use_cuda:\nspeaker_id.cuda()\n# synthesize voice\ndecoder_output, postnet_output, alignments, stop_tokens = run_model(\n-        model, inputs, CONFIG, truncated, style_mel)\n+        model, inputs, speaker_id, CONFIG, truncated, style_mel)\n# convert outputs to numpy\npostnet_output, decoder_output, alignment = parse_outputs(\npostnet_output, decoder_output, alignments)\n", "fix_pattern": "<condition>: The condition is not clearly mentioned in the context provided.\n<pattern>: The pattern is not clearly mentioned in the context provided.\n<code_one>: The code that was removed is \"speaker_id = speaker_id_var = torch.from_numpy(speaker_id).unsqueeze(0)\"\n<code_two>: The code that was added is \"speaker_id = np.asarray(speaker_id)\\n    speaker_id = torch.from_numpy(speaker_id).unsqueeze(0)\"\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then remove the <code_one> and add the <code_two> to fix the API misuse."}
{"number": 2305, "change": "class AttentionTest(tf.test.TestCase, parameterized.TestCase):\nattention_layer.concat_score_weight = 1\nattention_layer.build(input_shape=([1, 1, 1], [1, 1, 1]))\nattention_layer.scale = 2.\n-    actual = attention_layer._calculate_scores(query=q, key=k)\n+    actual = keras.backend.get_value(\n+            attention_layer._calculate_scores(query=q, key=k))\n\n# Expected tensor of shape [1, 1, 1].\n# expected000 = tanh(2*(1.1+1.6)) = 0.9999592018254402\n", "fix_pattern": "<condition>: The condition is not specified in the given information.\n\n<pattern>: The pattern detected is that the original code was using the `attention_layer._calculate_scores()` method directly without extracting the value using `keras.backend.get_value()`.\n\n<code_one>: The original code was `actual = attention_layer._calculate_scores(query=q, key=k)`.\n\n<code_two>: The fixed code is `actual = keras.backend.get_value(attention_layer._calculate_scores(query=q, key=k))`.\n\nFix_pattern: In the condition where the code is using `attention_layer._calculate_scores()` method, the fix is to replace it with `keras.backend.get_value()` to correctly get the value from the method."}
{"number": 2309, "change": "class TensorFlowEstimator(BaseEstimator):\nif not os.path.exists(saver_filename):\nraise ValueError(\"Restore folder doesn't contain saver defintion.\")\nwith open(saver_filename) as fsaver:\n-                saver_def = tf.python.training.saver_pb2.SaverDef()\n+                saver_def = tf.python.training.saver.saver_pb2.SaverDef()\ntext_format.Merge(fsaver.read(), saver_def)\nself._saver = tf.train.Saver(saver_def=saver_def)\n", "fix_pattern": "<condition>: Check if the file path exists.\n<pattern>: Import statement for TensorFlow's SaverDef protobuf message was changed.\n<code_one>: tf.python.training.saver_pb2.SaverDef()\n<code_two>: tf.python.training.saver.saver_pb2.SaverDef()\nFix_pattern: In the condition of checking if the file path exists, if the import statement for TensorFlow's SaverDef protobuf message is detected as tf.python.training.saver_pb2.SaverDef(), then change it to tf.python.training.saver.saver_pb2.SaverDef() to fix the API misuse."}
{"number": 2312, "change": "class TFPolicy(Policy):\n\n# TODO(rliaw): Can consider exposing these parameters\nself.sess = tf.Session(graph=self.g, config=tf.ConfigProto(\n-            intra_op_parallelism_threads=1, inter_op_parallelism_threads=2))\n+            intra_op_parallelism_threads=1, inter_op_parallelism_threads=2,\n+            gpu_options=tf.GPUOptions(allow_growth=True)))\nself.variables = ray.experimental.TensorFlowVariables(self.loss,\nself.sess)\nself.sess.run(tf.global_variables_initializer())\n", "fix_pattern": "<condition>: When initializing a TensorFlow session and configuring TensorFlowVariables\n\n<pattern>: Missing GPU options configuration\n\n<code_one>: intra_op_parallelism_threads=1, inter_op_parallelism_threads=2\n\n<code_two>: intra_op_parallelism_threads=1, inter_op_parallelism_threads=2, gpu_options=tf.GPUOptions(allow_growth=True)\n\nFix_pattern: In the condition of initializing a TensorFlow session and configuring TensorFlowVariables, if missing GPU options configuration is detected, then add intra_op_parallelism_threads=1, inter_op_parallelism_threads=2, gpu_options=tf.GPUOptions(allow_growth=True) to fix the API misuse."}
{"number": 2341, "change": "def subtract(x1: torch.Tensor,\npromoted_type = torch.promote_types(x1.dtype, x2.dtype)\nx1 = x1.to(promoted_type)\nx2 = x2.to(promoted_type)\n-    return torch.subtract(x1, x2, out=out)\n+        return torch.subtract(x1, x2, out=out)\n+    return torch.subtract(x1, x2)\n\n\ndef remainder(x1: torch.Tensor,\n", "fix_pattern": "<condition>: The condition is not clearly specified in the given context.\n\n<pattern>: The pattern is to remove the code that returns the result directly and add a new block that performs the operation and returns the result.\n\n<code_one>: The code that is removed is `return torch.subtract(x1, x2, out=out)`.\n\n<code_two>: The added code is:\n```\n    return torch.subtract(x1, x2, out=out)\n    return torch.subtract(x1, x2)\n```\n\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then remove the <code_one> and add the <code_two> to fix the API misuse."}
{"number": 2345, "change": "class GPTNeoForSequenceClassification(GPTNeoPreTrainedModel):\nf\"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n)\n\n-        pooled_logits = logits[torch.arange(batch_size), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=self.device), sequence_lengths]\n\nloss = None\nif labels is not None:\n", "fix_pattern": "<condition>: When using padding tokens in conjunction with `inputs_embeds`.\n<pattern>: `logits[torch.arange(batch_size), sequence_lengths]` is being used.\n<code_one>: `logits[torch.arange(batch_size), sequence_lengths]`\n<code_two>: `logits[torch.arange(batch_size, device=self.device), sequence_lengths]`\nFix_pattern: In the condition of using padding tokens in conjunction with `inputs_embeds`, if the pattern `logits[torch.arange(batch_size), sequence_lengths]` is detected, then change the `code_one` to `code_two` to fix the API misuse."}
{"number": 2376, "change": "def attempt_load(weights, map_location=None, inplace=True):\n# Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a\nmodel = Ensemble()\nfor w in weights if isinstance(weights, list) else [weights]:\n-        attempt_download(w)\n-        ckpt = torch.load(w, map_location=map_location)  # load\n+        ckpt = torch.load(attempt_download(w), map_location=map_location)  # load\nmodel.append(ckpt['ema' if ckpt.get('ema') else 'model'].float().fuse().eval())  # FP32 model\n\n# Compatibility updates\n", "fix_pattern": "<condition>: When loading weights from a list or a single model.\n<pattern>: The code was previously attempting to download the weights and then load them using `torch.load(w, map_location=map_location)`.\n<code_one>: `attempt_download(w)`\n<code_two>: `torch.load(attempt_download(w), map_location=map_location)`\nFix_pattern: In the condition of loading weights from a list or a single model, if the code was previously attempting to download the weights and then load them, then change `attempt_download(w)` to `torch.load(attempt_download(w), map_location=map_location)` to fix the API misuse."}
{"number": 2386, "change": "class Wavegrad(nn.Module):\nself.noise_level = self.noise_level.to(y_0)\nif len(y_0.shape) == 3:\ny_0 = y_0.squeeze(1)\n-        s = torch.randint(1, self.num_steps + 1, [y_0.shape[0]])\n-        l_a, l_b = self.noise_level[s-1], self.noise_level[s]\n+        s = torch.randint(0, self.num_steps - 1, [y_0.shape[0]])\n+        l_a, l_b = self.noise_level[s], self.noise_level[s+1]\nnoise_scale = l_a + torch.rand(y_0.shape[0]).to(y_0) * (l_b - l_a)\nnoise_scale = noise_scale.unsqueeze(1)\nnoise = torch.randn_like(y_0)\n", "fix_pattern": "<condition>: If the length of the tensor y_0 is equal to 3.\n<pattern>: Assign the values of self.noise_level indexed by s-1 and s to l_a and l_b respectively.\n<code_one>: s = torch.randint(1, self.num_steps + 1, [y_0.shape[0]])\nl_a, l_b = self.noise_level[s-1], self.noise_level[s]\n<code_two>: s = torch.randint(0, self.num_steps - 1, [y_0.shape[0]])\nl_a, l_b = self.noise_level[s], self.noise_level[s+1]\nFix_pattern: In the condition of the length of y_0 being equal to 3, if the pattern of assigning values to l_a and l_b indexed by s-1 and s respectively is detected, then change the code snippet from s = torch.randint(1, self.num_steps + 1, [y_0.shape[0]])\\nl_a, l_b = self.noise_level[s-1], self.noise_level[s] to s = torch.randint(0, self.num_steps - 1, [y_0.shape[0]])\\nl_a, l_b = self.noise_level[s], self.noise_level[s+1]. This fix corrects the API misuse."}
{"number": 2410, "change": "from copy import deepcopy\n\nimport numpy as np\nimport torch\n-from torch.cuda import amp\n\nfrom utils.general import LOGGER, colorstr\nfrom utils.torch_utils import profile\n\n\n-def check_train_batch_size(model, imgsz=640):\n+def check_train_batch_size(model, imgsz=640, amp=True):\n# Check YOLOv5 training batch size\n-    with amp.autocast():\n+    with torch.cuda.amp.autocast(amp):\nreturn autobatch(deepcopy(model).train(), imgsz)  # compute optimal batch size\n", "fix_pattern": "<condition>: The condition of the fix pattern is when checking the training batch size in the YOLOv5 model.\n<pattern>: The pattern is that the \"amp.autocast()\" function is being used for mixed precision training.\n<code_one>: The code being removed is the import statement for \"amp\" from \"torch.cuda\".\n<code_two>: The code being added is the parameter \"amp=True\" in the function definition and using \"torch.cuda.amp.autocast(amp)\" instead of \"amp.autocast()\".\nFix_pattern: In the condition of checking the training batch size in the YOLOv5 model, if the pattern of using \"amp.autocast()\" is detected, then remove the import statement for \"amp\" and add the parameter \"amp=True\" in the function definition, replacing \"amp.autocast()\" with \"torch.cuda.amp.autocast(amp)\" to fix the API misuse."}
{"number": 2411, "change": "def test_tacotron2_trainable(n_speakers, n_chars, max_input_length, max_mel_leng\npost_mel_preds, \\\nstop_preds, \\\nalignment_history = model(input_ids,\n-                                          tf.constant([max_mel_length, max_mel_length]),\n+                                          tf.constant([max_input_length, max_input_length]),\nspeaker_ids,\nmel_outputs,\n-                                          mel_lengths)\n+                                          mel_lengths,\n+                                          training=True)\nloss_before = tf.keras.losses.MeanSquaredError()(mel_outputs, mel_preds)\nloss_after = tf.keras.losses.MeanSquaredError()(mel_outputs, post_mel_preds)\n", "fix_pattern": "Condition: The condition is not clearly specified in the given context.\n\nPattern: The pattern is not clearly specified in the given code.\n\nCode one: The code to be removed is `tf.constant([max_mel_length, max_mel_length]), mel_lengths)`.\n\nCode two: The code to be added is `tf.constant([max_input_length, max_input_length]), mel_lengths, training=True)`.\n\nFix pattern: In the condition of <condition>, if <pattern> is detected, then remove `tf.constant([max_mel_length, max_mel_length]), mel_lengths)` and add `tf.constant([max_input_length, max_input_length]), mel_lengths, training=True)` to fix the API misuse."}
{"number": 2412, "change": "def apply_grad_clipping(policy, optimizer, loss):\n\n\ndef atanh(x):\n-    return 0.5 * torch.log((1 + x) / (1 - x))\n+    return 0.5 * torch.log(\n+        (1 + x).clamp(min=SMALL_NUMBER) / (1 - x).clamp(min=SMALL_NUMBER))\n\n\ndef convert_to_non_torch_type(stats):\n", "fix_pattern": "<condition>: The condition is not given in the provided context section.\n\n<pattern>: The pattern is to replace the calculation of `return 0.5 * torch.log((1 + x) / (1 - x))` with `return 0.5 * torch.log((1 + x).clamp(min=SMALL_NUMBER) / (1 - x).clamp(min=SMALL_NUMBER))`.\n\n<code_one>: `return 0.5 * torch.log((1 + x) / (1 - x))`\n\n<code_two>: `return 0.5 * torch.log((1 + x).clamp(min=SMALL_NUMBER) / (1 - x).clamp(min=SMALL_NUMBER))`\n\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then change the <code_one> to <code_two> to fix the API misuse."}
{"number": 2415, "change": "class Trainer(object):\n# convert logging_outputs to CPU to avoid unnecessary\n# device-to-host transfers in reduce_metrics\nlogging_outputs = utils.apply_to_sample(\n-                lambda t: t.to(device='cpu', non_blocking=True),\n+                lambda t: t.to(device='cpu', non_blocking=True, dtype=torch.double),\nlogging_outputs\n)\n", "fix_pattern": "<condition>: The code is using the `to()` method of a Torch tensor object.\n<pattern>: The code is calling the `to()` method with the `device` argument set to `'cpu'`.\n<code_one>: `t.to(device='cpu', non_blocking=True)`\n<code_two>: `t.to(device='cpu', non_blocking=True, dtype=torch.double)`\nFix_pattern: In the condition of using the `to()` method on a Torch tensor object with the `device` argument set to `'cpu'`, change the original code `t.to(device='cpu', non_blocking=True)` to `t.to(device='cpu', non_blocking=True, dtype=torch.double)` to fix the API misuse."}
{"number": 2428, "change": "def RegNet(\nin_channels = out_channels\n\nif include_top:\n-        x = Head(num_classes=classes)(x)\nimagenet_utils.validate_activation(classifier_activation, weights)\n+        x = Head(\n+            num_classes=classes,\n+            classifier_activation=classifier_activation,\n+            name=model_name,\n+        )(x)\n\nelse:\nif pooling == \"avg\":\n", "fix_pattern": "<condition>: The condition is when there is an API misuse in the code.\n<pattern>: The pattern is when the code needs to be modified to fix the API misuse.\n<code_one>: The code that needs to be removed.\n<code_two>: The modified code that replaces code_one.\nFix_pattern: In the condition of API misuse, if the pattern is detected, then remove code_one and replace it with code_two to fix the API misuse."}
{"number": 2461, "change": "class ModelPruning(Callback):\ndef _wrap_pruning_fn(pruning_fn: Callable, **kwargs: Any) -> Callable:\nreturn partial(pruning_fn, **kwargs)\n\n-    def make_pruning_permanent(self, pl_module: LightningModule) -> None:\n+    def make_pruning_permanent(self, module: nn.Module) -> None:\n\"\"\"\nRemoves pruning buffers from any pruned modules\n\nAdapted from https://github.com/pytorch/pytorch/blob/1.7.1/torch/nn/utils/prune.py#L1176-L1180\n\"\"\"\n-        for _, module in pl_module.named_modules():\n+        for _, module in module.named_modules():\nfor k in list(module._forward_pre_hooks):\nhook = module._forward_pre_hooks[k]\nif isinstance(hook, pytorch_prune.BasePruningMethod):\n", "fix_pattern": "<condition>: The condition is checking if the hook is an instance of the class \"pytorch_prune.BasePruningMethod\".\n\n<pattern>: The pattern is to change the parameter name \"pl_module\" to \"module\" in the method \"make_pruning_permanent\".\n\n<code_one>: The code that is removed is the line \"def make_pruning_permanent(self, pl_module: LightningModule) -> None:\".\n\n<code_two>: The code that is added is the line \"def make_pruning_permanent(self, module: nn.Module) -> None:\".\n\nFix_pattern: In the condition of checking if the hook is an instance of \"pytorch_prune.BasePruningMethod\", change the parameter name \"pl_module\" to \"module\" in the method \"make_pruning_permanent\" to fix the API misuse."}
{"number": 2466, "change": "class Standardize(Preprocessor):\nelse:\naxes = tuple(range(1, util.rank(tensor)))\n\n-        mean, variance = tf.nn.moments(x=tensor, axes=axes)\n-        return (tensor - mean) / tf.maximum(x=variance, y=util.epsilon)\n+        mean, variance = tf.nn.moments(x=tensor, axes=axes, keep_dims=True)\n+        return (tensor - mean) / tf.maximum(x=tf.sqrt(variance), y=util.epsilon)\n", "fix_pattern": "<condition>:\nThe condition is when using the `tf.nn.moments` function in TensorFlow.\n\n<pattern>:\nThe pattern is that the `keep_dims` argument should be set to `True` when calling the `tf.nn.moments` function.\n\n<code_one>:\nThe original code is `mean, variance = tf.nn.moments(x=tensor, axes=axes)`\n\n<code_two>:\nThe fixed code is `mean, variance = tf.nn.moments(x=tensor, axes=axes, keep_dims=True)`\n\nFix_pattern:\nIn the condition of using the `tf.nn.moments` function, if `keep_dims` argument is not set to `True`, then change `mean, variance = tf.nn.moments(x=tensor, axes=axes)` to `mean, variance = tf.nn.moments(x=tensor, axes=axes, keep_dims=True)` to fix the API misuse."}
{"number": 2511, "change": "def get_perspective_transform(src, dst):\n], dim=1)\n\n# solve the system Ax = b\n-    X, LU = torch.solve(b, A)\n+    X, LU = _torch_solve_cast(b, A)\n\n# create variable to return\nbatch_size = src.shape[0]\n", "fix_pattern": "<condition>: None\n\n<pattern>: The code <code_one> was changed to <code_two>.\n\n<code_one>: `torch.solve(b, A)`\n\n<code_two>: `_torch_solve_cast(b, A)`\n\nFix_pattern: In the condition of no clear condition, if the code `torch.solve(b, A)` is detected, then replace it with `_torch_solve_cast(b, A)` to fix the API misuse."}
{"number": 2523, "change": "class Estimator(CircularBuffer):\nx=tf.zeros_like(tensor=discounts, dtype=util.tf_dtype(dtype='float')),\ny=discounts\n)\n-            reward = reward + discounts * horizon_estimate\n+            reward = reward + discounts * tf.stop_gradient(input=horizon_estimate)\n# TODO: stop gradients?\n\nreturn reward\n", "fix_pattern": "<condition>: The condition in the code where the fix is applied  \n<pattern>: The pattern that is detected in the condition  \n<code_one>: The specific code that is being removed or changed  \n<code_two>: The specific code that is being added or modified  \nFix_pattern: The overall pattern or solution to fix the API misuse  \n\n<condition>: Unknown\n<pattern>: The code is not correctly utilizing the API and may be missing a required feature or using it in an incorrect manner\n<code_one>: The code is using a certain operation or function incorrectly\n<code_two>: The code is modified to use the operation or function correctly\nFix_pattern: In the condition of API misuse, if incorrect usage of a certain operation or function is detected, then the code is modified to use the operation or function correctly."}
{"number": 2542, "change": "th = TorchHijackForUnet()\n\n# Below are monkey patches to enable upcasting a float16 UNet for float32 sampling\ndef apply_model(orig_func, self, x_noisy, t, cond, **kwargs):\n-    for y in cond.keys():\n-        cond[y] = [x.to(devices.dtype_unet) if isinstance(x, torch.Tensor) else x for x in cond[y]]\n+\n+    if isinstance(cond, dict):\n+        for y in cond.keys():\n+            cond[y] = [x.to(devices.dtype_unet) if isinstance(x, torch.Tensor) else x for x in cond[y]]\n+\nwith devices.autocast():\nreturn orig_func(self, x_noisy.to(devices.dtype_unet), t.to(devices.dtype_unet), cond, **kwargs).float()\n", "fix_pattern": "<condition>: The condition is that the variable \"cond\" must be a dictionary.\n<pattern>: The pattern is that each value in the \"cond\" dictionary is checked if it is a torch.Tensor. If it is, then it is converted to devices.dtype_unet. If not, it remains unchanged.\n<code_one>: The code removed is the for loop that iterates over the items in the \"cond\" dictionary and converts them if necessary.\n<code_two>: The code added checks if \"cond\" is a dictionary and then performs the same conversion for each value in \"cond\" if it is a torch.Tensor.\nFix_pattern: In the condition of \"cond\" being a dictionary, if any of the values in \"cond\" are torch.Tensors, then they should be converted to devices.dtype_unet to fix the API misuse."}
{"number": 2562, "change": "class RENet(torch.nn.Module):\n_, perm = logits.sort(dim=1, descending=True)\nmask = (y.view(-1, 1) == perm)\n\n-        mrr = (1 / (mask.nonzero()[:, -1] + 1).to(torch.float)).mean().item()\n+        nnz = mask.nonzero(as_tuple=False)\n+        mrr = (1 / (nnz[:, -1] + 1).to(torch.float)).mean().item()\nhits1 = mask[:, :1].sum().item() / y.size(0)\nhits3 = mask[:, :3].sum().item() / y.size(0)\nhits10 = mask[:, :10].sum().item() / y.size(0)\n", "fix_pattern": "<condition>: API misuse in calculating the mean reciprocal rank (MRR) using a non-zero tensor.\n\n<pattern>: Detection of a non-zero tensor in the mask using `(mask.nonzero()[:, -1] + 1)`. \n\n<code_one>: `mrr = (1 / (mask.nonzero()[:, -1] + 1).to(torch.float)).mean().item()`\n\n<code_two>: `nnz = mask.nonzero(as_tuple=False)\nmrr = (1 / (nnz[:, -1] + 1).to(torch.float)).mean().item()`\n\nFix_pattern: In the condition of API misuse where a non-zero tensor is detected, the fix is to change the code from `mrr = (1 / (mask.nonzero()[:, -1] + 1).to(torch.float)).mean().item()` to `nnz = mask.nonzero(as_tuple=False)\nmrr = (1 / (nnz[:, -1] + 1).to(torch.float)).mean().item()` to accurately calculate the MRR."}
{"number": 2563, "change": "class GaussianChainTests(TestCase):\n(self.N, reparameterized, n_repa_nodes, self.N))\nif self.N < 0:\ndef array_to_string(y):\n-                    return str(map(lambda x: \"%.3f\" % x.data.numpy()[0], y))\n+                    return str(map(lambda x: \"%.3f\" % x.data.cpu().numpy()[0], y))\n\nprint(\"lambdas: \" + array_to_string(self.lambdas))\nprint(\"target_mus: \" + array_to_string(self.target_mus[1:]))\n", "fix_pattern": "<condition>: None\n<pattern>: Using `data.numpy()` to access the data of a tensor\n<code_one>: `x.data.numpy()`\n<code_two>: `x.data.cpu().numpy()`\nFix_pattern: In the condition of `None`, if the pattern of using `data.numpy()` is detected, then change `x.data.numpy()` to `x.data.cpu().numpy()` to fix the API misuse."}
{"number": 2565, "change": "class PANConv(MessagePassing):\n\ntmp = SparseTensor.eye(adj_t.size(0), adj_t.size(1), has_value=True,\ndtype=dtype, device=adj_t.device())\n-        tmp = tmp.mul_nnz(self.weight[0])\n+        tmp = tmp.mul_nnz(self.weight[0], layout='coo')\n\nouts = [tmp]\nfor i in range(1, self.filter_size + 1):\ntmp = tmp @ adj_t\n-            tmp = tmp.mul_nnz(self.weight[i])\n+            tmp = tmp.mul_nnz(self.weight[i], layout='coo')\nouts += [tmp]\n\nrow = torch.cat([out.storage.row() for out in outs], dim=0)\n", "fix_pattern": "<condition>: The condition is not clearly stated in the given context.\n\n<pattern>: The pattern is to modify the multiplication operation on the 'tmp' tensor.\n\n<code_one>: The code 'tmp = tmp.mul_nnz(self.weight[0])' and 'tmp = tmp.mul_nnz(self.weight[i])' are being removed.\n\n<code_two>: The code 'tmp = tmp.mul_nnz(self.weight[0], layout='coo')' and 'tmp = tmp.mul_nnz(self.weight[i], layout='coo')' are being added.\n\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then change the <code_one> to <code_two> to fix the API misuse."}
{"number": 2581, "change": "class PyramidVisionTransformerV2(nn.Module):\ncur += depths[i]\n\n# classification head\n-        self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity()\n+        self.num_features = embed_dims[-1]\n+        self.head = nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity()\n\nself.apply(self._init_weights)\n", "fix_pattern": "<condition>: The condition is not clearly specified in the given context.\n\n<pattern>: The pattern is replacing the code that defines the class `self.head` based on the value of `embed_dims[3]` with a new code that defines `self.head` based on the value of `embed_dims[-1]`.\n\n<code_one>: `self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity()`\n\n<code_two>: `self.head = nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity()`\n\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then remove the <code_one> and replace it with <code_two> to fix the API misuse."}
{"number": 2592, "change": "class SequenceGenerator(nn.Module):\ncum_unfin.append(prev)\ncum_fin_tensor = torch.tensor(cum_unfin, dtype=torch.int).to(bbsz_idx)\n\n-        unfin_idx = bbsz_idx // beam_size\n+        unfin_idx = torch.div(bbsz_idx, beam_size, rounding_mode='trunc')\nsent = unfin_idx + torch.index_select(cum_fin_tensor, 0, unfin_idx)\n\n# Create a set of \"{sent}{unfin_idx}\", where\n", "fix_pattern": "<condition>: When dividing two numbers\n<pattern>: Division using the // operator\n<code_one>: unfin_idx = bbsz_idx // beam_size\n<code_two>: unfin_idx = torch.div(bbsz_idx, beam_size, rounding_mode='trunc')\nFix_pattern: In the condition of division, if the pattern of using the // operator is detected, then replace <code_one> with <code_two> to fix the API misuse."}
{"number": 2606, "change": "def test_conditional_whiten(Xnew, X, kernel, f_loc, f_scale_tril, loc, cov):\nloc0, cov0 = conditional(Xnew, X, kernel, f_loc, f_scale_tril, full_cov=True,\nwhiten=False)\nKff = kernel(X) + torch.eye(3) * 1e-6\n-    Lff = Kff.cholesky()\n+    Lff = torch.linalg.cholesky(Kff)\nwhiten_f_loc = Lff.inverse().matmul(f_loc)\nwhiten_f_scale_tril = Lff.inverse().matmul(f_scale_tril)\nloc1, cov1 = conditional(Xnew, X, kernel, whiten_f_loc, whiten_f_scale_tril,\n", "fix_pattern": "<condition>: `Kff` is a torch tensor.\n<pattern>: `Kff.cholesky()` is used to compute the Cholesky decomposition of `Kff`.\n<code_one>: `Kff.cholesky()`\n<code_two>: `torch.linalg.cholesky(Kff)`\nFix_pattern: In the condition of `Kff` being a torch tensor, if `Kff.cholesky()` is detected, then change `Kff.cholesky()` to `torch.linalg.cholesky(Kff)` to fix the API misuse."}
{"number": 2627, "change": "def accuracy(pr, gt, threshold=0.5, ignore_channels=None):\npr = _threshold(pr, threshold=threshold)\npr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n\n-    tp = torch.sum(gt == pr)\n+    tp = torch.sum(gt == pr, dtype=pr.dtype)\nscore = tp / gt.view(-1).shape[0]\nreturn score\n", "fix_pattern": "<condition>:\nThe condition is not clearly stated in the provided code snippet. Therefore, no pre-condition is needed.\n\n<pattern>:\nThe pattern is detecting an API misuse related to the `torch.sum()` function call.\n\n<code_one>:\n`torch.sum(gt == pr)`\n\n<code_two>:\n`torch.sum(gt == pr, dtype=pr.dtype)`\n\nFix_pattern:\nIn the condition of an API misuse related to the `torch.sum()` function call, if `torch.sum(gt == pr)` is detected, then the code should be changed to `torch.sum(gt == pr, dtype=pr.dtype)` to fix the API misuse."}
{"number": 2636, "change": "def shape(\nas_array: bool = False,\n) -> Union[tf.Tensor, ivy.Shape, ivy.Array]:\nif as_array:\n-        return ivy.array(tf.shape(x))\n+        return ivy.array(tf.shape(x), dtype=ivy.default_int_dtype())\nelse:\nreturn ivy.Shape(x.shape)\n", "fix_pattern": "<condition>: The condition is when the variable \"as_array\" is set to True.\n<pattern>: The pattern is to change the return statement from \"return ivy.Shape(x.shape)\" to \"return ivy.array(tf.shape(x), dtype=ivy.default_int_dtype())\".\n<code_one>: The code being removed is \"return ivy.array(tf.shape(x))\".\n<code_two>: The code being added is \"return ivy.array(tf.shape(x), dtype=ivy.default_int_dtype())\".\nFix_pattern: In the condition of \"as_array\" being True, change the return statement from \"return ivy.array(tf.shape(x))\" to \"return ivy.array(tf.shape(x), dtype=ivy.default_int_dtype())\" to fix the API misuse."}
{"number": 2654, "change": "class Entropy(Metric):\ndef __call__(\nself,  # type: ignore\nlogits: torch.Tensor,\n-        mask: Optional[torch.Tensor] = None,\n+        mask: Optional[torch.BoolTensor] = None,\n):\n\"\"\"\n# Parameters\n\nlogits : `torch.Tensor`, required.\nA tensor of unnormalized log probabilities of shape (batch_size, ..., num_classes).\n-        mask : `torch.Tensor`, optional (default = None).\n+        mask : `torch.BoolTensor`, optional (default = None).\nA masking tensor of shape (batch_size, ...).\n\"\"\"\nlogits, mask = self.detach_tensors(logits, mask)\n\nif mask is None:\n-            mask = torch.ones(logits.size()[:-1], device=logits.device)\n+            mask = torch.ones(logits.size()[:-1], device=logits.device).bool()\n\nlog_probs = torch.nn.functional.log_softmax(logits, dim=-1)\nprobabilities = torch.exp(log_probs) * mask.unsqueeze(-1)\n", "fix_pattern": "<condition>: The condition is `mask` being None.\n\n<pattern>: The pattern is the incorrect data type annotation for the `mask` parameter.\n\n<code_one>: The code that needs to be removed is `mask: Optional[torch.Tensor] = None`.\n\n<code_two>: The code that needs to be added is `mask: Optional[torch.BoolTensor] = None`.\n\nFix_pattern: In the condition of `mask` being None, if the incorrect data type annotation `Optional[torch.Tensor]` is detected, then remove the code `mask: Optional[torch.Tensor] = None` and add the code `mask: Optional[torch.BoolTensor] = None` to fix the API misuse."}
{"number": 2662, "change": "def get_transducer_task_io(\nencoder_out_lens = list(map(int, encoder_out_lens))\n\nt_len = torch.IntTensor(encoder_out_lens).to(device)\n-    u_len = torch.IntTensor([y.size(0) for y in ys]).to(device)\n+    u_len = torch.IntTensor([y.size(0) for y in labels_unpad]).to(device)\n\n-    return target, t_len, u_len\n+    return decoder_in, target, t_len, u_len\n", "fix_pattern": "<condition>: The code is a function that takes a list of encoder output lengths as an argument.\n<pattern>: The code is getting the lengths of the encoder outputs by applying the `size(0)` method on each element of the `ys` list.\n<code_one>: `u_len = torch.IntTensor([y.size(0) for y in ys]).to(device)`\n<code_two>: `u_len = torch.IntTensor([y.size(0) for y in labels_unpad]).to(device)`\nFix_pattern: In the condition of the function, if the pattern of using `size(0)` on each element of a list is detected, then the `code_one` should be changed to `code_two` to fix the API misuse."}
{"number": 2663, "change": "def lower_modules_to_accelerator(\nbackend = \"NNPI\"\nbackend_qualifier = \"\"\n\n-        if throughput_optimize:\n+        if throughput_optimize and gelu_clip:\n+            backend_qualifier = \":throughput_optimized_gelu_clip\"\n+        elif throughput_optimize:\nbackend_qualifier = \":throughput_optimized\"\n\nmodules_to_lower = accelerator.get_modules(model, backend + backend_qualifier)\n", "fix_pattern": "<condition>: Throughput optimization is enabled.\n<pattern>: The code checks for the presence of a flag named \"throughput_optimize\".\n<code_one>: `if throughput_optimize:`\n<code_two>: `if throughput_optimize and gelu_clip: backend_qualifier = \":throughput_optimized_gelu_clip\" elif throughput_optimize:`\nFix_pattern: In the condition of throughput optimization being enabled, if the \"throughput_optimize\" flag is detected, then change the code from `if throughput_optimize:` to `if throughput_optimize and gelu_clip: backend_qualifier = \":throughput_optimized_gelu_clip\" elif throughput_optimize:` to fix the API misuse."}
{"number": 2685, "change": "class GPTJForSequenceClassification(GPTJPreTrainedModel):\nf\"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n)\n\n-        pooled_logits = logits[range(batch_size), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=self.device), sequence_lengths]\n\nloss = None\nif labels is not None:\n", "fix_pattern": "<condition>: The code is using padding tokens in conjunction with `inputs_embeds`.\n<pattern>: The code is accessing logits using indexing.\n<code_one>: `logits[range(batch_size), sequence_lengths]`\n<code_two>: `logits[torch.arange(batch_size, device=self.device), sequence_lengths]`\nFix_pattern: In the condition of using padding tokens in conjunction with `inputs_embeds`, if the pattern of accessing logits using indexing is detected, then change `logits[range(batch_size), sequence_lengths]` to `logits[torch.arange(batch_size, device=self.device), sequence_lengths]` to fix the API misuse."}
{"number": 2727, "change": "class DomainClient(Client):\n\nreturn response\n\n-    def apply_to_network(self, target: str, reason: str):\n+    def apply_to_network(self,\n+            target: str,\n+            reason: str,\n+            route_index: int = 0):\nself.association.create(\ntarget=target,\n-            sender=self.conn.base_url.replace(\"/api/v1\", \"\"),\n+            sender=self.routes[route_index].connection.base_url.replace(\"/api/v1\", \"\"),\nreason=reason,\nnode_name=self.name,\n)\n", "fix_pattern": "<condition>: The condition is that the method \"apply_to_network\" needs to be modified.\n\n<pattern>: The pattern is that the variable \"sender\" needs to be updated.\n\n<code_one>: The original code was \"sender=self.conn.base_url.replace(\"/api/v1\", \"\")\".\n\n<code_two>: The updated code is \"sender=self.routes[route_index].connection.base_url.replace(\"/api/v1\", \"\")\".\n\nFix_pattern: In the condition of modifying the \"apply_to_network\" method, if the pattern of updating the \"sender\" variable is detected, then change the code from \"sender=self.conn.base_url.replace(\"/api/v1\", \"\")\" to \"sender=self.routes[route_index].connection.base_url.replace(\"/api/v1\", \"\")\" to fix the API misuse."}
{"number": 2783, "change": "class CNNLayerVisualization():\nself.conv_output = x[0, self.selected_filter]\n# Loss function is the mean of the output of the selected layer/filter\n# We try to minimize the mean of the output of that specific filter\n-            loss = torch.mean(self.conv_output)\n-            print('Iteration:', str(i), 'Loss:', \"{0:.2f}\".format(loss.data.numpy()[0]))\n+            loss = -torch.mean(self.conv_output)\n+            print('Iteration:', str(i), 'Loss:', \"{0:.2f}\".format(loss.data.numpy()))\n# Backward\nloss.backward()\n# Update image\n", "fix_pattern": "<condition>: The code is trying to minimize the mean output of a specific filter in a convolutional neural network layer.\n<pattern>: The code is computing the mean of \"self.conv_output\" and printing the loss value.\n<code_one>: \"loss = torch.mean(self.conv_output)\"\n<code_two>: \"loss = -torch.mean(self.conv_output)\"\nFix_pattern: In the condition of trying to minimize the mean output of a specific filter in a convolutional neural network layer, if the code is computing the mean of \"self.conv_output\" and printing the loss value, then change \"loss = torch.mean(self.conv_output)\" to \"loss = -torch.mean(self.conv_output)\" to fix the API misuse."}
{"number": 2784, "change": "class BitEncoder(nn.Module):\ndilation = 1\n\nlayer_dropouts = [\n-            x.tolist() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths)).split(config.depths)\n+            x.tolist()\n+            for x in torch.Tensor(np.linspace(0, config.drop_path_rate, sum(config.depths))).split(config.depths)\n]\n\nfor stage_idx, (current_depth, current_hidden_size, layer_dropout) in enumerate(\n", "fix_pattern": "<condition>: In the code, there is a for loop iterating over the values in a specific range.\n<pattern>: The code is converting the values in the range to a list using the `.tolist()` method.\n<code_one>: `x.tolist()`\n<code_two>: `x.tolist()` is removed and replaced with a new code snippet.\nFix_pattern: In the condition of a for loop iterating over a specific range, if the code is converting the range values to a list using the `.tolist()` method, then remove the `x.tolist()` code snippet and replace it with the new code snippet."}
{"number": 2786, "change": "def absolute_path(path):\nThis implementation avoids calling os.path.abspath(path) if 'path' already\nrepresents an absolute Tensorflow filesystem location (e.g. <fs type>://).\n\"\"\"\n-  return path if \"://\" in str(path) else os.path.abspath(path)\n+  return path if b\"://\" in tf.compat.as_bytes(path) else os.path.abspath(path)\n\n\ndef fc2_implements_resources():\n", "fix_pattern": "<condition>: The code is checking if a given path represents an absolute Tensorflow filesystem location.\n\n<pattern>: The pattern being detected is the presence of \"://\" in the given path.\n\n<code_one>: The original code is returning the path as is if \"://\" is found in the path.\n\n<code_two>: The fixed code is using tf.compat.as_bytes() to convert the path to bytes and checking if b\"://\" is found in the bytes.\n\nFix_pattern: In the condition of checking for an absolute Tensorflow filesystem location, if \"://\" is detected in the path, then the code is modified to use tf.compat.as_bytes() and check for b\"://\" in the bytes to fix the API misuse."}
{"number": 2791, "change": "def main():\nif requires_preprocessing:\nprepare_input = PREPROCESSING_FUNCTIONS.get(args.model_type)\nprompt_text, model_kwargs = prepare_input(args, model, tokenizer, prompt_text)\n-    encoded_prompt = torch.tensor(tokenizer.encode(prompt_text, add_special_tokens=False)).unsqueeze(0)\n+    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors='pt')\n\noutput_sequences = model.generate(\n-        intput_ids=encoded_prompt,\n-        length=args.length,\n+        input_ids=encoded_prompt,\n+        max_length=args.length,\ntemperature=args.temperature,\ntop_k=args.k,\ntop_p=args.p,\n", "fix_pattern": "<condition>: requires_preprocessing is True\n<pattern>: The code is using torch.tensor() to convert the prompt_text into a tensor, but it is missing some arguments.\n<code_one>: encoded_prompt = torch.tensor(tokenizer.encode(prompt_text, add_special_tokens=False)).unsqueeze(0)\n<code_two>: encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors='pt')\nFix_pattern: In the condition of requires_preprocessing being True, if the code pattern of converting prompt_text to a tensor using torch.tensor() is detected, then replace the code_one with code_two to fix the API misuse."}
{"number": 2798, "change": "class PlanTranslatorTorchscript(AbstractPlanTranslator):\ntranslation_plan = self.plan.copy()\ntranslation_plan.forward = None\n\n-        args_shape = translation_plan.get_args_shape()\n-        args = PlaceHolder.create_placeholders(args_shape)\n+        args = translation_plan.create_dummy_args()\n\n-        # To avoid storing Plan state tensors in torchscript, they will be send as parameters\n+        # jit.trace clones input args and can change their type, so we have to skip types check\n+        # TODO see if type check can be made less strict,\n+        #  e.g. tensor/custom tensor/nn.Parameter could be considered same type\n+        translation_plan.validate_input_types = False\n+\n+        # To avoid storing Plan state tensors in torchscript, they will be sent as parameters\n# we trace wrapper func, which accepts state parameters as last arg\n# and sets them into the Plan before executing the Plan\ndef wrap_stateful_plan(*args):\n", "fix_pattern": "<condition>:\nThe condition of this fix pattern is that there is a need to avoid storing Plan state tensors in torchscript.\n\n<pattern>:\nThe pattern is to remove the code that creates placeholders for the Plan state tensors and replace it with code that creates dummy arguments.\n\n<code_one>:\nThe code that is removed is:\n```\nargs_shape = translation_plan.get_args_shape()\nargs = PlaceHolder.create_placeholders(args_shape)\n```\n\n<code_two>:\nThe code that is added is:\n```\nargs = translation_plan.create_dummy_args()\ntranslation_plan.validate_input_types = False\n```\n\nFix_pattern:\nIn the condition of avoiding storing Plan state tensors in torchscript, if the pattern of creating placeholders for the Plan state tensors is detected, then the code that creates placeholders should be replaced with code that creates dummy arguments. Additionally, the `validate_input_types` property of the Plan should be set to `False`."}
{"number": 2803, "change": "class TFEmbedding(tf.keras.layers.Embedding):\nsuper().__init__(*args, **kwargs)\n\ndef call(self, inputs):\n-        inputs = tf.cast(tf.expand_dims(inputs, -1), tf.int32)\n-        outputs = tf.gather_nd(self.embeddings, inputs)\n+        inputs = tf.cast(inputs, tf.int32)\n+        outputs = tf.gather(self.embeddings, inputs)\nreturn outputs\n", "fix_pattern": "<condition>: When using the `tf.keras.layers.Embedding` class in TensorFlow.\n<pattern>: The pattern is to remove the `tf.expand_dims` function call from the `inputs` variable and change the `tf.gather_nd` function to `tf.gather` function.\n<code_one>: `inputs = tf.cast(tf.expand_dims(inputs, -1), tf.int32)`\n<code_two>: `inputs = tf.cast(inputs, tf.int32)`\nFix_pattern: In the condition of using the `tf.keras.layers.Embedding` class, if the pattern (removing `tf.expand_dims` and changing `tf.gather_nd` to `tf.gather`) is detected, then replace `code_one` with `code_two` to fix the API misuse."}
{"number": 2819, "change": "class Model(object):\nself.deterministic_mode = config.get('deterministic_mode', False)\nself.episode_length = tf.placeholder(tf.int32, (None,), name='episode_length')\n\n-        self.alpha = config.get('alpha', 0.001)\n+        self.learning_rate = config.get('learning_rate', 0.001)\n\noptimizer = config.get('optimizer')\nif not optimizer:\n-            self.optimizer = tf.train.AdamOptimizer(self.alpha)\n+            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\nelse:\nargs = config.get('optimizer_args', [])\nkwargs = config.get('optimizer_kwargs', {})\noptimizer_cls = get_function(optimizer)\n-            self.optimizer = optimizer_cls(self.alpha, *args, **kwargs)\n+            self.optimizer = optimizer_cls(self.learning_rate, *args, **kwargs)\n\nexploration = config.get('exploration')\nif not exploration:\n", "fix_pattern": "<condition>: optimizer is not configured\n<pattern>: self.alpha = config.get('alpha', 0.001)\n<code_one>: self.optimizer = tf.train.AdamOptimizer(self.alpha)\n<code_two>: self.learning_rate = config.get('learning_rate', 0.001)\nFix_pattern: In the condition of optimizer not configured, if the pattern self.alpha = config.get('alpha', 0.001) is detected, then remove self.optimizer = tf.train.AdamOptimizer(self.alpha) and add self.learning_rate = config.get('learning_rate', 0.001) to fix the API misuse."}
{"number": 2827, "change": "class Metric(Registrable):\nraise NotImplementedError\n\n@staticmethod\n-    def unwrap_to_tensors(*tensors: torch.Tensor):\n+    def detach_tensors(*tensors: torch.Tensor) -> Iterable[torch.Tensor]:\n\"\"\"\nIf you actually passed gradient-tracking Tensors to a Metric, there will be\na huge memory leak, because it will prevent garbage collection for the computation\n-        graph. This method ensures that you're using tensors directly and that they are on\n-        the CPU.\n+        graph. This method ensures the tensors are detached.\n\"\"\"\n-        return (x.detach().cpu() if isinstance(x, torch.Tensor) else x for x in tensors)\n+        # Check if it's actually a tensor in case something else was passed.\n+        return (x.detach() if isinstance(x, torch.Tensor) else x for x in tensors)\n", "fix_pattern": "<condition>: API misuse in the Metric class where gradient-tracking Tensors are passed, causing a memory leak.\n<pattern>: The pattern is the use of a method called \"unwrap_to_tensors\" that ensures the usage of tensors directly and ensures they are on the CPU.\n<code_one>: The code being removed is the \"unwrap_to_tensors\" method.\n<code_two>: The code being added is the \"detach_tensors\" method.\nFix_pattern: In the condition of API misuse in the Metric class, if the pattern of using the \"unwrap_to_tensors\" method is detected, then remove the \"unwrap_to_tensors\" method and add the \"detach_tensors\" method to fix the API misuse."}
{"number": 2841, "change": "class Synthesizer(object):\nsample_rate=self.ap.sample_rate,\n).cuda()\n\n-        check = torch.load(model_file)\n-        self.wavernn.load_state_dict(check['model'], map_location=\"cpu\")\n+        check = torch.load(model_file, map_location=\"cpu\")\n+        self.wavernn.load_state_dict(check['model'])\nif use_cuda:\nself.wavernn.cuda()\nself.wavernn.eval()\n", "fix_pattern": "<condition>: `use_cuda` is True.\n<pattern>: The `load_state_dict` method is called with the `map_location` argument set to \"cpu\" instead of the default argument.\n<code_one>: `check = torch.load(model_file)`\n<code_two>: `check = torch.load(model_file, map_location=\"cpu\")`\nFix_pattern: In the condition of `use_cuda` is True, if the `load_state_dict` method is called, then change the `check = torch.load(model_file)` to `check = torch.load(model_file, map_location=\"cpu\")` to fix the API misuse."}
{"number": 2845, "change": "def floor_divide(\nif (not np.all(x2)) or (np.any(x2) == -0):  # check for division by zero\nret = np.floor_divide(x1, x2)\nelse:\n-        ret = tf.math.floordiv(x1, x2)\n+        ret = tf.experimental.numpy.floor_divide(x1, x2)\n\nif (any(isinf(x1)) and any(isfinite(x2))) or (any(isfinite(x1)) and any(isinf(x2))):\nreturn ivy.full_like(ret, floor(divide(x1, x2)), dtype=ret.dtype)\n", "fix_pattern": "<condition>:\nThe condition is when there is a check for division by zero, indicated by the presence of \"np.all(x2)\" and \"np.any(x2) == -0\" in the condition.\n\n<pattern>:\nThe pattern is the usage of the \"floor_divide\" function, indicated by the presence of \"np.floor_divide(x1, x2)\".\n\n<code_one>:\nThe code that is removed is \"ret = tf.math.floordiv(x1, x2)\".\n\n<code_two>:\nThe code that is added is \"ret = tf.experimental.numpy.floor_divide(x1, x2)\".\n\nFix_pattern:\nIn the condition of checking for division by zero, if the usage of the \"floor_divide\" function is detected, then the code \"ret = tf.math.floordiv(x1, x2)\" should be changed to \"ret = tf.experimental.numpy.floor_divide(x1, x2)\" to fix the API misuse."}
{"number": 2859, "change": "class DDIMScheduler(SchedulerMixin, ConfigMixin):\nprev_sample = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction\n\nif eta > 0:\n+            # randn_like does not support generator https://github.com/pytorch/pytorch/issues/27072\ndevice = model_output.device if torch.is_tensor(model_output) else \"cpu\"\n-            noise = torch.randn(model_output.shape, generator=generator).to(device)\n+            noise = torch.randn(model_output.shape, dtype=model_output.dtype, generator=generator).to(device)\nvariance = self._get_variance(timestep, prev_timestep) ** (0.5) * eta * noise\n\nprev_sample = prev_sample + variance\n", "fix_pattern": "<condition>: The fix pattern applies when there is a conditional statement checking if \"eta > 0\".\n<pattern>: The pattern is to update the code to change how the \"noise\" tensor is generated.\n<code_one>: The original code used torch.randn() function without specifying the dtype.\n<code_two>: The fixed code uses torch.randn() with the additional dtype=model_output.dtype argument.\nFix_pattern: In the condition of \"eta > 0\", if the code using torch.randn() without specifying dtype is detected, then the code should be updated to include dtype=model_output.dtype to fix the API misuse."}
{"number": 2883, "change": "class Fixture(object):\n\ndef _convert_logits_to_ps(self, dist_params):\nif 'logits' in dist_params:\n-            logits = torch.Tensor(dist_params.pop('logits'))\n+            logits = Variable(torch.Tensor(dist_params.pop('logits')))\nis_multidimensional = self.get_test_distribution_name() != 'Bernoulli'\nps, _ = get_probs_and_logits(logits=logits, is_multidimensional=is_multidimensional)\ndist_params['ps'] = list(ps.data.cpu().numpy())\n", "fix_pattern": "<condition>: 'logits' in dist_params\n<pattern>: retrieving 'logits' from dist_params and assigning it to the variable 'logits'\n<code_one>: logits = torch.Tensor(dist_params.pop('logits'))\n<code_two>: logits = Variable(torch.Tensor(dist_params.pop('logits')))\nFix_pattern: In the condition of 'logits' in dist_params, if the pattern of retrieving 'logits' from dist_params and assigning it to the variable 'logits' is detected, then remove the code line logits = torch.Tensor(dist_params.pop('logits')) and add the code line logits = Variable(torch.Tensor(dist_params.pop('logits'))) to fix the API misuse."}
{"number": 2884, "change": "class GEDDataset(InMemoryDataset):\nxs += [assoc[x]]\nys += [assoc[y]]\ngs += [g]\n-            x, y, g = torch.tensor(xs), torch.tensor(ys), torch.tensor(gs)\n+            x, y = torch.tensor(xs), torch.tensor(ys)\n+            g = torch.tensor(gs, dtype=torch.float)\nmat[x, y], mat[y, x] = g, g\n\npath = osp.join(self.processed_dir, '{}_ged.pt'.format(self.name))\n", "fix_pattern": "<condition>:\nThere is a code segment that involves creating tensors x, y, and g using lists xs, ys, and gs.\n\n<pattern>:\nThe pattern is to create tensors x and y separately from xs and ys, and create tensor g separately from gs. Additionally, the data type of g should be explicitly set to torch.float.\n\n<code_one>:\nx, y, g = torch.tensor(xs), torch.tensor(ys), torch.tensor(gs)\n\n<code_two>:\nx, y = torch.tensor(xs), torch.tensor(ys)\ng = torch.tensor(gs, dtype=torch.float)\n\nFix_pattern:\nIn the condition of creating tensors using lists, if the pattern of creating all the tensors at once is detected, then split the creation into separate steps. Create tensors x and y from xs and ys separately, and create tensor g from gs separately. Ensure that the data type of g is explicitly set to torch.float."}
{"number": 2888, "change": "def extract_info_from_torch_data(\ninput_types = ifnone(\ninput_types,\n[\n-            \"int\" if isinstance(x.cpu(), torch.LongTensor) else \"float\"\n+            \"int64\"\n+            if isinstance(x.cpu(), torch.LongTensor)\n+            else \"int32\"\n+            if isinstance(x.cpu(), torch.IntTensor)\n+            else \"float32\"\nfor x in input_row\n],\n)\n", "fix_pattern": "<condition>: The condition is not clearly specified in the given context.\n<pattern>: The pattern is to change the code for determining input_types from \"int\" if isinstance(x.cpu(), torch.LongTensor) else \"float\" to \"int64\" if isinstance(x.cpu(), torch.LongTensor) else \"int32\" if isinstance(x.cpu(), torch.IntTensor) else \"float32\".\n<code_one>: The code that was removed is \"int\" if isinstance(x.cpu(), torch.LongTensor) else \"float\".\n<code_two>: The code that was added is \"int64\" if isinstance(x.cpu(), torch.LongTensor) else \"int32\" if isinstance(x.cpu(), torch.IntTensor) else \"float32\".\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then change the <code_one> to <code_two> to fix the API misuse."}
{"number": 2891, "change": "def block(params, scope, past, append_dim, train=False):\ndef model(features, labels, params, mesh, past=None):\n\"\"\"A GPT style model implemented in mesh tensorlfow.\"\"\"\nresults = {}\n+    sequence_dim = mtf.Dimension('sequence', params[\"n_ctx\"])\nif params[\"num_microbatches\"] > 1:\nx = features[\"inputs\"]\nlabels = features[\"labels\"]\nbatch_dim = x.shape[0]\n-\n-\nelse:\n+      batch_dim = mtf.Dimension('batch', params[\"train_batch_size\"])\nx = mtf.import_tf_tensor(mesh, features, mtf.Shape([batch_dim, sequence_dim]))\n# In this case, labels are simply input shifted one token to the right\n# this op is done in the input_fn\n# define mtf dims\n-      batch_dim = mtf.Dimension('batch', params[\"train_batch_size\"])\nlabels = mtf.import_tf_tensor(mesh, labels, mtf.Shape([batch_dim, sequence_dim]))\n\n-    sequence_dim = mtf.Dimension('sequence', params[\"n_ctx\"])\n\n# we need this because gathering when both the args have the same dimension in them it breaks stuff.\n# this dim is specifically for the weights\n", "fix_pattern": "<condition>: params[\"num_microbatches\"] > 1\n<pattern>: batch_dim = mtf.Dimension('batch', params[\"train_batch_size\"])\n<code_one>: batch_dim = mtf.Dimension('batch', params[\"train_batch_size'])\n<code_two>: batch_dim = mtf.Dimension('batch', params[\"train_batch_size\"])\nFix_pattern: In the condition of params[\"num_microbatches\"] > 1, if batch_dim = mtf.Dimension('batch', params[\"train_batch_size\"]) is detected, then remove/keep the batch_dim = mtf.Dimension('batch', params[\"train_batch_size\"]) to fix the API misuse."}
{"number": 2926, "change": "class CrossAttention(nn.Module):\nkey_slice = key_slice.float()\n\nattn_slice = torch.baddbmm(\n-                torch.empty(slice_size, query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),\n-                query[start_idx:end_idx],\n-                key[start_idx:end_idx].transpose(-1, -2),\n+                torch.empty(slice_size, query.shape[1], key.shape[1], dtype=query_slice.dtype, device=query.device),\n+                query_slice,\n+                key_slice.transpose(-1, -2),\nbeta=0,\nalpha=self.scale,\n)\n", "fix_pattern": "<condition>: When a specific slice of the query and key tensors are used in the CrossAttention class.\n<pattern>: Passing the incorrect dtype of the query tensor to torch.empty() function.\n<code_one>: torch.empty(slice_size, query.shape[1], key.shape[1], dtype=query.dtype, device=query.device)\n<code_two>: torch.empty(slice_size, query.shape[1], key.shape[1], dtype=query_slice.dtype, device=query.device)\nFix_pattern: In the condition of using a specific slice of query and key tensors, if the incorrect dtype of the query tensor is detected, then change the code from torch.empty(..., dtype=query.dtype, ...) to torch.empty(..., dtype=query_slice.dtype, ...) to fix the API misuse."}
{"number": 2948, "change": "class InMemoryDataset(Dataset):\nfor key in keys:\nitem = data_list[0][key]\nif torch.is_tensor(item):\n-                data[key] = torch.cat(\n-                    data[key], dim=data.__cat_dim__(key, data_list[0][key]))\n+                data[key] = torch.cat(data[key],\n+                                      dim=data.__cat_dim__(key, item))\nelif isinstance(item, int) or isinstance(item, float):\ndata[key] = torch.tensor(data[key])\n", "fix_pattern": "<condition>:\nA condition where a key is present in the \"data_list\" dictionary.\n\n<pattern>:\nThe pattern is where the \"data[key]\" is assigned the result of concatenating \"data[key]\" with a value from \"data_list[0][key]\", using the torch.cat() function.\n\n<code_one>:\nThe code being removed is:\n```\ndata[key] = torch.cat(\n                data[key], dim=data.__cat_dim__(key, data_list[0][key]))\n```\n\n<code_two>:\nThe code being added is:\n```\ndata[key] = torch.cat(data[key],\n                      dim=data.__cat_dim__(key, item))\n```\n\nFix_pattern:\nIn the condition where a key is present in the \"data_list\" dictionary, if the \"data[key]\" is being concatenated using torch.cat(), then remove the original code and replace it with the new code shown above to fix the API misuse."}
{"number": 2956, "change": "class FloatVectorField(Field):\n)\nself.dim_error_check = dim_error_check  # dims in data should match config\nself.dummy_model_input = torch.tensor(\n-            [[1.0] * dim], dtype=torch.float, device=\"cpu\"\n+            [[1.0] * dim, [1.0] * dim], dtype=torch.float, device=\"cpu\"\n)\n\ndef _parse_vector(self, s):\n", "fix_pattern": "<condition>: There is a need to parse a vector.\n<pattern>: The input vector should have a specific shape.\n<code_one>: [[1.0] * dim], dtype=torch.float, device=\"cpu\"\n<code_two>: [[1.0] * dim, [1.0] * dim], dtype=torch.float, device=\"cpu\"\nFix_pattern: In the condition of parsing a vector, if the input vector does not have the required shape, then change the code from [[1.0] * dim] to [[1.0] * dim, [1.0] * dim] to fix the API misuse."}
{"number": 2969, "change": "class WGAN_GP(object):\nalpha = tf.random_uniform(shape=self.inputs.get_shape(), minval=0.,maxval=1.)\ndifferences = G - self.inputs # This is different from MAGAN\ninterpolates = self.inputs + (alpha * differences)\n-        D_inter,_,_=self.discriminator(interpolates, is_training=True, reuse=True)\n+        _,D_inter,_=self.discriminator(interpolates, is_training=True, reuse=True)\ngradients = tf.gradients(D_inter, [interpolates])[0]\nslopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\ngradient_penalty = tf.reduce_mean((slopes - 1.) ** 2)\n", "fix_pattern": "<condition>: The condition of the fix pattern is that the code is within a class called \"WGAN_GP\" and there is a method called \"discriminator\" being called with the \"interpolates\" variable as one of the arguments.\n\n<pattern>: The pattern is that the third output of the \"self.discriminator\" method is being assigned to \"D_inter\".\n\n<code_one>: The code being removed is \"D_inter,_,_=self.discriminator(interpolates, is_training=True, reuse=True)\".\n\n<code_two>: The code being added is \"_,D_inter,_=self.discriminator(interpolates, is_training=True, reuse=True)\".\n\nFix_pattern: In the condition of a class called \"WGAN_GP\", if the third output of the \"self.discriminator\" method is being assigned to \"D_inter\", then remove the code \"D_inter,_,_=self.discriminator(interpolates, is_training=True, reuse=True)\" and add the code \"_,D_inter,_=self.discriminator(interpolates, is_training=True, reuse=True)\" to fix the API misuse."}
{"number": 2971, "change": "class TransferLearningModel(pl.LightningModule):\n# 1. Forward pass:\nx, y = batch\ny_logits = self.forward(x)\n+        y_scores = torch.sigmoid(y_logits)\ny_true = y.view((-1, 1)).type_as(x)\n\n# 2. Compute loss\nself.log(\"val_loss\", self.loss(y_logits, y_true), prog_bar=True)\n\n# 3. Compute accuracy:\n-        self.log(\"val_acc\", self.valid_acc(y_logits, y_true.int()), prog_bar=True)\n+        self.log(\"val_acc\", self.valid_acc(y_scores, y_true.int()), prog_bar=True)\n\ndef configure_optimizers(self):\nparameters = list(self.parameters())\n", "fix_pattern": "<condition>: The condition is when computing the accuracy of a model.\n<pattern>: The pattern is that the model is using a function called \"valid_acc\" to calculate the accuracy.\n<code_one>: The code that was removed is \"self.valid_acc(y_logits, y_true.int())\".\n<code_two>: The code that was added is \"y_scores = torch.sigmoid(y_logits)\".\nFix_pattern: In the condition of computing the accuracy, if the model is using the \"valid_acc\" function, then remove the code \"self.valid_acc(y_logits, y_true.int())\" and add the code \"y_scores = torch.sigmoid(y_logits)\" to fix the API misuse."}
{"number": 2975, "change": "class Decoder(nn.Module):\nself.attention = inputs.data.new(B, T).zero_()\nself.attention_cum = inputs.data.new(B, T).zero_()\n\n-    def _parse_outputs(self, outputs, stop_tokens, attentions):\n+    def _parse_outputs(self, outputs, attentions, stop_tokens):\n# Back to batch first\nattentions = torch.stack(attentions).transpose(0, 1)\noutputs = torch.stack(outputs).transpose(0, 1).contiguous()\n-        stop_tokens = torch.stack(stop_tokens).transpose(0, 1)\n-        return outputs, stop_tokens, attentions\n+        stop_tokens = torch.stack(stop_tokens).transpose(0, 1).squeeze(-1)\n+        return outputs, attentions, stop_tokens\n\ndef decode(self,\ninputs,\n", "fix_pattern": "<condition>: In the given code, there is a function called `_parse_outputs` that takes three parameters `outputs`, `stop_tokens`, and `attentions`.\n<pattern>: The pattern is that the order of parameters in the function signature is incorrect.\n<code_one>: The code that needs to be changed is `def _parse_outputs(self, outputs, stop_tokens, attentions):`.\n<code_two>: The correct code should be `def _parse_outputs(self, outputs, attentions, stop_tokens):`.\nFix_pattern: In the condition of `_parse_outputs`, if the pattern of the incorrect order of parameters is detected, then change the code from `def _parse_outputs(self, outputs, stop_tokens, attentions):` to `def _parse_outputs(self, outputs, attentions, stop_tokens):` to fix the API misuse."}
{"number": 2978, "change": "class GAE(torch.nn.Module):\ndata.val_pos_edge_index = torch.stack([r, c], dim=0)\nr, c = row[n_v:n_v + n_t], col[n_v:n_v + n_t]\ndata.test_pos_edge_index = torch.stack([r, c], dim=0)\n+\nr, c = row[n_v + n_t:], col[n_v + n_t:]\n-        data.train_pos_edge_index = torch.stack([r, c], dim=0)\n+        edge_index = torch.stack([r, c], dim=0)\n+        data.train_pos_edge_index = to_undirected(edge_index)\n\n# Negative edges.\nnum_nodes = data.num_nodes\n", "fix_pattern": "<condition>: There is a `train_pos_edge_index` attribute in the `data` object.\n<pattern>: The `train_pos_edge_index` attribute is being created by stacking `r` and `c` along the 0th dimension.\n<code_one>: `data.train_pos_edge_index = torch.stack([r, c], dim=0)`\n<code_two>: `edge_index = torch.stack([r, c], dim=0) \\n data.train_pos_edge_index = to_undirected(edge_index)`\nFix_pattern: In the condition of having a `train_pos_edge_index` attribute in the `data` object, if the `train_pos_edge_index` is being created by stacking `r` and `c` along the 0th dimension, then replace the code `data.train_pos_edge_index = torch.stack([r, c], dim=0)` with `edge_index = torch.stack([r, c], dim=0) \\n data.train_pos_edge_index = to_undirected(edge_index)` to fix the API misuse."}
{"number": 2980, "change": "class LatentDiffusion(DiffusionPipeline):\nnum_trained_timesteps = self.noise_scheduler.timesteps\ninference_step_times = range(0, num_trained_timesteps, num_trained_timesteps // num_inference_steps)\n\n-        image = self.noise_scheduler.sample_noise(\n+        image = torch.randn(\n(batch_size, self.unet.in_channels, self.unet.image_size, self.unet.image_size),\ndevice=torch_device,\ngenerator=generator,\n", "fix_pattern": "<condition>: The code is inside a class method of a class that is a subclass of DiffusionPipeline.\n<pattern>: The method is accessing the noise_scheduler attribute of the class instance.\n<code_one>: The code is calling the sample_noise() method on the noise_scheduler attribute.\n<code_two>: The code is calling the torch.randn() function.\nFix_pattern: In the condition of being a subclass of DiffusionPipeline, if the sample_noise() method is called on the noise_scheduler attribute, then it should be replaced with calling the torch.randn() function."}
{"number": 2995, "change": "class EmpiricalMarginal(Empirical):\nin ``[0, num_chains - 1]``, and there must be equal number\nof samples per chain.\n\"\"\"\n-        weight_type = value.new_empty(1).float().type() if value.dtype in (torch.int32, torch.int64) \\\n-            else value.type()\n# Apply default weight of 1.0.\nif log_weight is None:\n-            log_weight = torch.tensor(0.0).type(weight_type)\n-        if isinstance(log_weight, numbers.Number):\n-            log_weight = torch.tensor(log_weight).type(weight_type)\n-        if self._validate_args and log_weight.dim() > 0:\n+            log_weight = 0.0\n+        if self._validate_args and not isinstance(log_weight, numbers.Number) and log_weight.dim() > 0:\nraise ValueError(\"``weight.dim() > 0``, but weight should be a scalar.\")\n\n# Append to the buffer list\n", "fix_pattern": "<condition>: log_weight.dim() > 0\n\n<pattern>: If log_weight is None or not a number, then set log_weight to 0.0.\n\n<code_one>: if log_weight is None or isinstance(log_weight, numbers.Number)\n\n<code_two>: log_weight = 0.0\n\nFix_pattern: In the condition of log_weight.dim() > 0, if log_weight is None or not a number, then set log_weight to 0.0 to fix the API misuse."}
{"number": 3012, "change": "def _convert_to_tf(x):\nreturn x\n\nif x is not None:\n-        x = tf.nest.map_structure(tf.convert_to_tensor, x)\n+        x = tf.nest.map_structure(\n+            lambda f: tf.convert_to_tensor(f) if f is not None else None, x)\nreturn x\n", "fix_pattern": "<condition>: The condition is when a variable \"x\" is not None.\n\n<pattern>: The pattern is using the tf.nest.map_structure() function to convert each element of \"x\" to a tensor.\n\n<code_one>: The code removed is \"x = tf.nest.map_structure(tf.convert_to_tensor, x)\".\n\n<code_two>: The code added is \"x = tf.nest.map_structure(lambda f: tf.convert_to_tensor(f) if f is not None else None, x)\".\n\nFix_pattern: In the condition of \"x is not None\", if the pattern of using tf.nest.map_structure() with tf.convert_to_tensor() is detected, then remove the code \"x = tf.nest.map_structure(tf.convert_to_tensor, x)\" and replace it with \"x = tf.nest.map_structure(lambda f: tf.convert_to_tensor(f) if f is not None else None, x)\" to fix the API misuse."}
{"number": 3017, "change": "class CTCPrefixScoreTH(object):\nr_prev, s_prev, f_min_prev, f_max_prev = state\n\n# select input dimensions for scoring\n-        if self.scoring_num > 0 and prep_scores is not None:\n-            scoring_ids = torch.topk(prep_scores, self.scoring_num, 1)[1]\n+        if self.scoring_num > 0 and pre_scores is not None:\n+            pre_scores[:, self.blank] = self.logzero  # ignore blank from pre-selection\n+            scoring_ids = torch.topk(pre_scores, self.scoring_num, 1)[1]\nscoring_idmap = torch.full((self.n_bb, self.odim), -1, dtype=torch.long, device=self.device)\nsnum = scoring_ids.size(1)\nscoring_idmap[self.bb_idx, scoring_ids] = torch.arange(snum, device=self.device)\n", "fix_pattern": "<condition>: self.scoring_num > 0 and prep_scores is not None\n<pattern>: Detecting the presence of a condition and a variable\n<code_one>: torch.topk(prep_scores, self.scoring_num, 1)[1]\n<code_two>: pre_scores[:, self.blank] = self.logzero  # ignore blank from pre-selection\\n        scoring_ids = torch.topk(pre_scores, self.scoring_num, 1)[1]\nFix_pattern: In the condition of self.scoring_num > 0 and prep_scores is not None, if the code torch.topk(prep_scores, self.scoring_num, 1)[1] is detected, then change it to pre_scores[:, self.blank] = self.logzero  # ignore blank from pre-selection\\n        scoring_ids = torch.topk(pre_scores, self.scoring_num, 1)[1] to fix the API misuse."}
{"number": 3021, "change": "class MultiHeadedAttention(nn.Module):\n\nscores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)  # (batch, head, time1, time2)\nif mask is not None:\n-            mask.unsqueeze_(1).eq_(0)  # (batch, 1, time1, time2)\n+            mask = mask.unsqueeze(1).eq(0)  # (batch, 1, time1, time2)\nscores = scores.masked_fill(mask, MIN_VALUE)\nself.attn = torch.softmax(scores, dim=-1).masked_fill(mask, 0.0)  # (batch, head, time1, time2)\nelse:\n-            self.attn = torch.softmax(scores, dim=-1)\n+            self.attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)\n\np_attn = self.dropout(self.attn)\nx = torch.matmul(p_attn, v)  # (batch, head, time1, d_k)\n", "fix_pattern": "<condition>: There is a condition check for the variable 'mask'.\n<pattern>: The pattern is to transform the variable 'mask' using the 'unsqueeze_' method and apply the 'eq_' method.\n<code_one>: The code that was removed is \"mask.unsqueeze_(1).eq_(0)\".\n<code_two>: The code that was added is \"mask = mask.unsqueeze(1).eq(0)\".\nFix_pattern: In the condition of checking the 'mask' variable, if the pattern of transforming 'mask' using the 'unsqueeze_' method and applying the 'eq_' method is detected, then remove the code \"mask.unsqueeze_(1).eq_(0)\" and add the code \"mask = mask.unsqueeze(1).eq(0)\" to fix the API misuse."}
{"number": 3048, "change": "class TopKPooling(torch.nn.Module):\n\nweight = F.normalize(self.weight, p=2, dim=-1)\nscore = (x * weight).sum(dim=-1)\n-        perm = self.topk(score, self.k, batch)\n-\n-        x = x[perm] * self.tanh(score[perm])\n+        perm = self.topk(score, self.ratio, batch)\n+        x = x[perm] * torch.tanh(score[perm]).view(-1, 1)\nbatch = batch[perm]\nedge_index, edge_attr = self.filter_adj(\n-            edge_index, edge_attr, perm, num_nodes=x.size(0))\n+            edge_index, edge_attr, perm, num_nodes=score.size(0))\n\n-        return x, edge_index, edge_attr, batch\n+        return x, edge_index, edge_attr, batch, perm\n\ndef __repr__(self):\nreturn '{}({})'.format(self.__class__.__name__, self.ratio)\n", "fix_pattern": "<condition>: The code is inside the class \"TopKPooling\" and is a part of the \"__repr__\" method.\n\n<pattern>: The pattern detected is that the code is using a variable \"self.k\" instead of \"self.ratio\" when calling the \"self.topk\" function.\n\n<code_one>: The code that was removed is \"perm = self.topk(score, self.k, batch)\" and \"x = x[perm] * self.tanh(score[perm])\".\n\n<code_two>: The code that was added is \"perm = self.topk(score, self.ratio, batch)\" and \"x = x[perm] * torch.tanh(score[perm]).view(-1, 1)\".\n\nFix_pattern: In the condition of the \"__repr__\" method of the class \"TopKPooling\", if the pattern of using \"self.k\" instead of \"self.ratio\" is detected, then remove the code \"perm = self.topk(score, self.k, batch)\" and \"x = x[perm] * self.tanh(score[perm])\" and replace it with \"perm = self.topk(score, self.ratio, batch)\" and \"x = x[perm] * torch.tanh(score[perm]).view(-1, 1)\" to fix the API misuse."}
{"number": 3050, "change": "class HaloAttn(nn.Module):\n\nkv = self.kv(x)\n# FIXME I 'think' this unfold does what I want it to, but I should investigate\n-        k = F.unfold(kv, kernel_size=self.win_size, stride=self.block_size, padding=self.halo_size)\n-        k = k.reshape(\n+        kv = F.unfold(kv, kernel_size=self.win_size, stride=self.block_size, padding=self.halo_size)\n+        kv = kv.reshape(\nB * self.num_heads, self.dim_head + (self.dim_v // self.num_heads), -1, num_blocks).transpose(1, 3)\n-        k, v = torch.split(k, [self.dim_head, self.dim_v // self.num_heads], dim=-1)\n+        k, v = torch.split(kv, [self.dim_head, self.dim_v // self.num_heads], dim=-1)\n\nattn_logits = (q @ k.transpose(-1, -2)) * self.scale  # FIXME should usual attn scale be applied?\nattn_logits = attn_logits + self.pos_embed(q)  # B * num_heads, block_size ** 2, win_size ** 2\n", "fix_pattern": "<condition>: The code is attempting to split a variable into two parts.\n\n<pattern>: The code is using the wrong variable to split.\n\n<code_one>: The variable `kv` is being used to split into `k` and `v`.\n\n<code_two>: The variable `kv` should be used instead to split into `k` and `v`.\n\nFix_pattern: In the condition of attempting to split a variable into two parts, if using the wrong variable is detected, then change the variable `kv` to `kv` to fix the API misuse."}
{"number": 3092, "change": "class XLNetRelativeAttention(nn.Module):\n\n# Mask heads if we want to\nif head_mask is not None:\n-            attn_prob = attn_prob * head_mask\n+            attn_prob = attn_prob * torch.einsum('ijbn->bnij', head_mask)\n\n# attention output\nattn_vec = torch.einsum('bnij,jbnd->ibnd', attn_prob, v_head_h)\n", "fix_pattern": "<condition>: The condition is that the code is inside a class or function where the variable \"head_mask\" is being checked for being None or not.\n\n<pattern>: The pattern is that the variable \"attn_prob\" is being multiplied by \"head_mask\".\n\n<code_one>: The code being removed is \"attn_prob = attn_prob * head_mask\".\n\n<code_two>: The code being added is \"attn_prob = attn_prob * torch.einsum('ijbn->bnij', head_mask)\".\n\nFix_pattern: In the condition of checking \"head_mask\" for being None or not, if the pattern of multiplying \"attn_prob\" with \"head_mask\" is detected, then change the code from \"attn_prob = attn_prob * head_mask\" to \"attn_prob = attn_prob * torch.einsum('ijbn->bnij', head_mask)\" to fix the API misuse."}
{"number": 3161, "change": "class LAFOrienter(nn.Module):\nself.patch_size,\nself.patch_size)\nangles_radians: torch.Tensor = self.angle_detector(patches).view(B, N)\n-        laf_out: torch.Tensor = set_laf_orientation(laf, rad2deg(angles_radians))\n+        prev_angle = get_laf_orientation(laf).view_as(angles_radians)\n+        laf_out: torch.Tensor = set_laf_orientation(laf, rad2deg(angles_radians) + prev_angle)\nreturn laf_out\n", "fix_pattern": "<condition>:\nIn the given context, there is no clear condition needed for the fix pattern.\n\n<pattern>:\nThe pattern is to modify the calculation of `laf_out` by adding the previous angle value to the current angle value.\n\n<code_one>:\nThe code that is removed is:\n`laf_out: torch.Tensor = set_laf_orientation(laf, rad2deg(angles_radians))`\n\n<code_two>:\nThe code that is added is:\n```\nprev_angle = get_laf_orientation(laf).view_as(angles_radians)\nlaf_out: torch.Tensor = set_laf_orientation(laf, rad2deg(angles_radians) + prev_angle)\n```\n\nFix_pattern:\nIn the condition of no specific condition, if the pattern of adding the previous angle to the current angle is detected, then change the line `laf_out: torch.Tensor = set_laf_orientation(laf, rad2deg(angles_radians))` to `prev_angle = get_laf_orientation(laf).view_as(angles_radians)` and `laf_out: torch.Tensor = set_laf_orientation(laf, rad2deg(angles_radians) + prev_angle)` to fix the API misuse."}
{"number": 3177, "change": "def regularize_cost(regex, func, name='regularize_cost'):\nfor p in params:\npara_name = p.name\n# in replicated mode, only regularize variables inside this tower\n-        if ctx.has_own_variables and (not para_name.startswith(ctx.name)):\n+        if ctx.has_own_variables and (not para_name.startswith(ctx.vs_name)):\ncontinue\nif re.search(regex, para_name):\ncosts.append(func(p))\n_log_regularizer(para_name)\nif not costs:\n-        return 0\n+        return tf.constant(0, dtype=tf.float32, name='empty_regularize_cost')\nreturn tf.add_n(costs, name=name)\n", "fix_pattern": "<condition>: The condition is checking if the context has its own variables and if the parameter name does not start with a certain name.\n\n<pattern>: The pattern that is being detected is if the condition in the code removed section is true.\n\n<code_one>: The code in the code removed section is checking if the context has its own variables and if the parameter name does not start with the context name.\n\n<code_two>: The code in the code added section is checking if the context has its own variables and if the parameter name does not start with the context variable scope name. If the condition is true, it returns a constant value of 0.\n\nFix_pattern: In the condition of checking if the context has its own variables and if the parameter name does not start with a certain name, if the pattern of the condition from the code removed section is detected, then the code checking for the context name in the code removed section should be changed to checking for the context variable scope name, and return a constant value of 0. This fixes the API misuse."}
{"number": 3187, "change": "class TFConvBertEmbeddings(tf.keras.layers.Layer):\ntoken_type_ids = tf.fill(dims=input_shape, value=0)\n\nif position_ids is None:\n-            position_ids = tf.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)\n+            position_ids = tf.expand_dims(\n+                tf.range(start=past_key_values_length, limit=input_shape[1] + past_key_values_length), axis=0\n+            )\n\nposition_embeds = tf.gather(params=self.position_embeddings, indices=position_ids)\nposition_embeds = tf.tile(input=position_embeds, multiples=(input_shape[0], 1, 1))\n", "fix_pattern": "<condition>: position_ids is None\n<pattern>: position_ids is replaced with a new value\n<code_one>: position_ids = tf.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)\n<code_two>: position_ids = tf.expand_dims(tf.range(start=past_key_values_length, limit=input_shape[1] + past_key_values_length), axis=0)\nFix_pattern: In the condition of position_ids is None, if position_ids is detected, then remove the code_one and replace it with code_two to fix the API misuse."}
{"number": 3205, "change": "class Pipeline(pps_module.Preprocessor):\ntransformed.append(data)\nif len(transformed) == 1:\nreturn transformed[0]\n-        return tuple(transformed)\n+        return tf.data.Dataset.zip(tuple(transformed))\n\ndef save(self, filepath):\nio_utils.save_json(filepath, self.get_config())\n", "fix_pattern": "<condition>: len(transformed) == 1\n<pattern>: Returning a tuple instead of a single value\n<code_one>: return tuple(transformed)\n<code_two>: return transformed[0]\nFix_pattern: In the condition of len(transformed) == 1, if returning a tuple is detected, then remove the return tuple(transformed) and change it to return transformed[0] to fix the API misuse."}
{"number": 3220, "change": "class Iterative(Solver):\nnext_step = self.next_step(*args)\nstep = (lambda: self.step(*args))\ndo_nothing = (lambda: args)\n-                args = tf.cond(pred=next_step, true_fn=step, false_fn=do_nothing)\n+                args = self.cond(pred=next_step, true_fn=step, false_fn=do_nothing)\n\nelse:\n# TensorFlow while loop\n-            args = tf.while_loop(\n+            args = self.while_loop(\ncond=self.next_step, body=self.step, loop_vars=args,\nmaximum_iterations=self.max_iterations\n)\n", "fix_pattern": "<condition>: The condition is not provided in the given context.\n\n<pattern>: The pattern is when the TensorFlow while loop is used to conditionally execute code.\n\n<code_one>: The code that is removed is \"args = tf.while_loop(\".\n\n<code_two>: The code that is added is \"args = self.while_loop(\".\n\nFix_pattern: In the condition of no pre condition is needed, if the pattern of using \"tf.while_loop\" is detected, then change the \"tf.while_loop\" to \"self.while_loop\" to fix the API misuse."}
{"number": 3223, "change": "def train(hyp):\nif not opt.evolve:\nplot_results()  # save as results.png\nprint('%g epochs completed in %.3f hours.\\n' % (epoch - start_epoch + 1, (time.time() - t0) / 3600))\n-    dist.destroy_process_group() if torch.cuda.device_count() > 1 else None\n+    dist.destroy_process_group() if device.type != 'cpu' and torch.cuda.device_count() > 1 else None\ntorch.cuda.empty_cache()\nreturn results\n", "fix_pattern": "<condition>: The fix applies when there is a check for the device type or if the code is running on multiple CUDA devices.\n\n<pattern>: The pattern is to modify the condition for destroying the process group based on the device type.\n\n<code_one>: dist.destroy_process_group() if torch.cuda.device_count() > 1 else None\n\n<code_two>: dist.destroy_process_group() if device.type != 'cpu' and torch.cuda.device_count() > 1 else None\n\nFix_pattern: In the condition of checking the device type, if the pattern for destroying the process group is detected in the code, then change the code from <code_one> to <code_two> to fix the API misuse."}
{"number": 3238, "change": "class PinholeCamera:\n>>> _ = torch.manual_seed(0)\n>>> x = torch.rand(1, 2)\n>>> depth = torch.ones(1, 1)\n-            >>> I = torch.eye(4)[None]\n+            >>> K = torch.eye(4)[None]\n>>> E = torch.eye(4)[None]\n>>> h = torch.ones(1)\n>>> w = torch.ones(1)\n>>> pinhole = kornia.geometry.camera.PinholeCamera(K, E, h, w)\n-            >>> pinhole.unproject_points(x, depth)\n+            >>> pinhole.unproject(x, depth)\ntensor([[0.4963, 0.7682, 1.0000]])\n\"\"\"\nP = self.intrinsics @ self.extrinsics\n", "fix_pattern": "<condition>: The code is trying to unproject points using a PinholeCamera object.\n<pattern>: The code is calling the method \"unproject_points\" on the \"pinhole\" object.\n<code_one>: pinhole.unproject_points(x, depth)\n<code_two>: pinhole.unproject(x, depth)\nFix_pattern: In the condition of using the PinholeCamera object to unproject points, if the method \"unproject_points\" is detected, then change the code from pinhole.unproject_points(x, depth) to pinhole.unproject(x, depth) to fix the API misuse."}
{"number": 3239, "change": "class TorchHook:\nif type(native_func) in [types.FunctionType, types.BuiltinFunctionType]:\n# 3. Build the hooked function\nnew_func = self.get_hooked_func(native_func)\n-                # 4. Move the native function\n-                setattr(torch_module, f\"native_{func}\", native_func)\n+                # 4. Move the native function to its original module\n+                # /!\\ Can be different from the torch_module!\n+                # Ex: in torch.py `torch.argmax = torch.functional.argmax`\n+                # ... So torch.argmax.__module__ is 'torch.functional' != 'torch'\n+                setattr(eval(native_func.__module__), f\"native_{func}\", native_func)\n# 5. Put instead the hooked one\nsetattr(torch_module, func, new_func)\n", "fix_pattern": "<condition>: The condition is checking if the type of the native_func is either types.FunctionType or types.BuiltinFunctionType.\n<pattern>: The pattern is moving the native function to its original module.\n<code_one>: The code that is removed is setting the native function as an attribute to the torch_module.\n<code_two>: The code that is added is setting the native function as an attribute to the original module of the native function.\nFix_pattern: In the condition of checking the type of the native function, if the function is detected as either types.FunctionType or types.BuiltinFunctionType, then remove the code that sets the native function as an attribute to the torch_module and instead add the code that sets the native function as an attribute to its original module."}
{"number": 3242, "change": "class TowerContext(object):\nglobal _CurrentTowerContext\nassert _CurrentTowerContext is None, \"Cannot nest TowerContext!\"\n_CurrentTowerContext = self\n-        curr_vs = tf.get_variable_scope()\n-        assert curr_vs.name == '', \"Cannot nest TowerContext with an existing variable scope!\"\n+        if self.is_training:\n+            curr_vs = tf.get_variable_scope()\n+            assert curr_vs.name == '', \"In training, cannot nest TowerContext with an existing variable scope!\"\n\nself._ctxs = self._get_scopes()\nself._ctxs.append(self._collection_guard)\n", "fix_pattern": "<condition>:\nThere is a need to check if a TowerContext is being nested within another TowerContext.\n\n<pattern>:\nThe pattern detected is when there is an existing variable scope inside the nested TowerContext.\n\n<code_one>:\nThe code that needs to be removed is the assertion that checks if the current variable scope is empty.\n\n<code_two>:\nThe code that needs to be added is a new assertion that checks if the current variable scope is empty only when in training mode.\n\nFix_pattern:\nIn the condition of nesting TowerContext, if an existing variable scope is detected, then remove the assertion that checks if the current variable scope is empty and add a new assertion that checks if the current variable scope is empty only when in training mode to fix the API misuse."}
{"number": 3244, "change": "class SelfMultiheadAttn(nn.Module):\nself.register_parameter('lyr_norm_beta_weights', None)\nself.lyr_nrm_gamma_weights = None\nself.lyr_nrm_beta_weights  = None\n-                self.lyr_nrm = torch.nn.LayerNorm(embed_dim)\n+                self.lyr_nrm = FusedLayerNorm(embed_dim)\nself.reset_parameters()\n\nif self.include_norm_add:\n", "fix_pattern": "<condition>: When using the `SelfMultiheadAttn` class with the `include_norm_add` attribute set to `True`.\n<pattern>: If the initialisation of `self.lyr_nrm` is done using `torch.nn.LayerNorm()`.\n<code_one>: `self.lyr_nrm = torch.nn.LayerNorm(embed_dim)`\n<code_two>: `self.lyr_nrm = FusedLayerNorm(embed_dim)`\nFix_pattern: In the condition of `include_norm_add` being `True`, if the initialisation of `self.lyr_nrm` is done using `torch.nn.LayerNorm()`, then change the initialization to `FusedLayerNorm(embed_dim)` to fix the API misuse."}
{"number": 3250, "change": "class ScaleSpaceDetector(nn.Module):\nmax_coords_best = _scale_index_to_scale(max_coords_best, sigmas_oct)\n\n# Create local affine frames (LAFs)\n-            rotmat = angle_to_rotation_matrix(torch.zeros(B, N))\n+            rotmat = angle_to_rotation_matrix(torch.zeros(B, N).to(max_coords_best.device).to(max_coords_best.dtype))\ncurrent_lafs = torch.cat([self.mr_size * max_coords_best[:, :, 0].view(B, N, 1, 1) * rotmat,\nmax_coords_best[:, :, 1:3].view(B, N, 2, 1)], dim=3)\n# Normalize LAFs\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: The code is trying to create local affine frames (LAFs).\n<code_one>: The code is creating a rotation matrix with zero values.\n<code_two>: The code is creating a rotation matrix with zero values, but also moving it to the device and data type of \"max_coords_best\".\nFix_pattern: In the condition of creating local affine frames, if the rotation matrix is created with zero values, then move it to the device and data type of \"max_coords_best\" to fix the API misuse."}
{"number": 3271, "change": "class T5EncoderModel(T5PreTrainedModel):\nclass PreTrainedModel\n\"\"\"\nfor layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n+            self.encoder.block[layer].layer[0].SelfAttention.prune_heads(heads)\n\n@add_start_docstrings_to_model_forward(T5_ENCODER_INPUTS_DOCSTRING)\n@replace_return_docstrings(output_type=BaseModelOutput, config_class=_CONFIG_FOR_DOC)\n", "fix_pattern": "<condition>: The condition is not clear from the given information.\n\n<pattern>: The pattern is to replace the code `encoder.layer[layer].attention.prune_heads(heads)` with `encoder.block[layer].layer[0].SelfAttention.prune_heads(heads)`.\n\n<code_one>: `encoder.layer[layer].attention.prune_heads(heads)`\n\n<code_two>: `encoder.block[layer].layer[0].SelfAttention.prune_heads(heads)`\n\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then replace <code_one> with <code_two> to fix the API misuse."}
{"number": 3283, "change": "class ResNet_Cifar(ModelDesc):\nce_cost = tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits)\nce_cost = tf.reduce_mean(ce_cost, name='cross_entropy_loss')\n\n-        single_label = tf.to_int32(tf.argmax(label, axis=1))\n-        wrong = tf.to_float(tf.logical_not(tf.nn.in_top_k(logits, single_label, 1)), name='wrong_vector')\n+        single_label = tf.cast(tf.argmax(label, axis=1), tf.int32)\n+        wrong = tf.cast(tf.logical_not(tf.nn.in_top_k(logits, single_label, 1)), tf.float32, name='wrong_vector')\n# monitor training error\nadd_moving_summary(tf.reduce_mean(wrong, name='train_error'), ce_cost)\nadd_param_summary(('.*/W', ['histogram']))\n", "fix_pattern": "<condition>: When converting labels to integers using tf.to_int32() and then finding the wrong predictions.\n<pattern>: The pattern is to replace tf.to_int32() with tf.cast() and specify the data type as tf.int32.\n<code_one>: tf.to_int32(tf.argmax(label, axis=1))\n<code_two>: tf.cast(tf.argmax(label, axis=1), tf.int32)\nFix_pattern: In the condition of converting labels to integers using tf.to_int32() and then finding the wrong predictions, if tf.to_int32() is detected, then replace it with tf.cast() and specify the data type as tf.int32 to fix the API misuse."}
{"number": 3309, "change": "class PermuteTransform(Transform):\nvector of zeros works.\n\"\"\"\n\n-        return torch.zeros_like(x)\n+        return torch.zeros(x.size()[:-1])\n", "fix_pattern": "<condition>: When using the method `torch.zeros_like()`  \n<pattern>: If the output size is a vector of zeros    \n<code_one>: `return torch.zeros_like(x)`  \n<code_two>: `return torch.zeros(x.size()[:-1])`  \nFix_pattern: In the condition of using `torch.zeros_like()`, if the output size is a vector of zeros, then change `return torch.zeros_like(x)` to `return torch.zeros(x.size()[:-1])` to fix the API misuse."}
{"number": 3313, "change": "def select_device(device='', batch_size=0, newline=True):\nfor i, d in enumerate(devices):\np = torch.cuda.get_device_properties(i)\ns += f\"{'' if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / (1 << 20):.0f}MiB)\\n\"  # bytes to MB\n+    elif mps:\n+        s += 'MPS\\n'\nelse:\ns += 'CPU\\n'\n\nif not newline:\ns = s.rstrip()\nLOGGER.info(s.encode().decode('ascii', 'ignore') if platform.system() == 'Windows' else s)  # emoji-safe\n-    return torch.device('cuda:0' if cuda else 'cpu')\n+    return torch.device('cuda:0' if cuda else 'mps' if mps else 'cpu')\n\n\ndef time_sync():\n", "fix_pattern": "<condition>: The condition is when selecting a device in the `select_device` function.\n<pattern>: The pattern is the use of `torch.device('cuda:0' if cuda else 'cpu')`.\n<code_one>: The code to be removed is `return torch.device('cuda:0' if cuda else 'cpu')`.\n<code_two>: The code to be added is `elif mps: s += 'MPS\\n' return torch.device('cuda:0' if cuda else 'mps' if mps else 'cpu')`.\nFix_pattern: In the condition of selecting a device, if the pattern of using `torch.device('cuda:0' if cuda else 'cpu')` is detected, then remove the code `return torch.device('cuda:0' if cuda else 'cpu')` and add the code `elif mps: s += 'MPS\\n' return torch.device('cuda:0' if cuda else 'mps' if mps else 'cpu')` to fix the API misuse."}
{"number": 3315, "change": "def _get_cached_vs(name):\n@contextmanager\ndef _enter_vs_reuse_ns(name):\nvs = _get_cached_vs(name)\n+    # XXX Not good to enter the cached vs directly, because this will clean-up custom getter\n+    # with tf.variable_scope(name, reuse=tf.AUTO_REUSE):    # available in 1.4 only\nwith tf.variable_scope(vs):\nwith tf.name_scope(vs.original_name_scope):\nyield vs\n", "fix_pattern": "Condition:\nIn the context section, there is no clear condition identified.\n\nPattern:\nNo pattern is detected in the code removed section.\n\nCode One:\nNo code is removed.\n\nCode Two:\nIn the code added section, the following code is added:\n'''\n    # XXX Not good to enter the cached vs directly, because this will clean-up custom getter\n    # with tf.variable_scope(name, reuse=tf.AUTO_REUSE):    # available in 1.4 only\n'''\n\nFix Pattern:\nNo pre-condition is needed. In the condition of the code, if the pattern of entering the cached vs directly is detected, then replace the code with:\n'''\n    # XXX Not good to enter the cached vs directly, because this will clean-up custom getter\n    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):    # available in 1.4 only\n'''"}
{"number": 3317, "change": "def iou(\n\nExample:\n\n-        >>> target = torch.randint(0, 1, (10, 25, 25))\n+        >>> target = torch.randint(0, 2, (10, 25, 25))\n>>> pred = torch.tensor(target)\n>>> pred[2:5, 7:13, 9:15] = 1 - pred[2:5, 7:13, 9:15]\n>>> iou(pred, target)\n-        tensor(0.4914)\n+        tensor(0.9660)\n\n\"\"\"\nnum_classes = get_num_classes(pred=pred, target=target, num_classes=num_classes)\n", "fix_pattern": "<condition>: The condition is not clearly mentioned in the given context.\n\n<pattern>: The pattern identified is the change in the values used for generating the `target` tensor.\n\n<code_one>: The original code used `torch.randint(0, 1, (10, 25, 25))` to generate the `target` tensor.\n\n<code_two>: The fix code uses `torch.randint(0, 2, (10, 25, 25))` to generate the `target` tensor.\n\nFix_pattern: In the condition of unspecified condition, if the pattern of generating the `target` tensor using `torch.randint(0, 1, (10, 25, 25))` is detected, then the fix is to change it to `torch.randint(0, 2, (10, 25, 25))` to ensure correct API usage."}
{"number": 3324, "change": "class E2E(STInterface, torch.nn.Module):\nisinstance(m, MultiHeadedAttention) and m.attn is not None\n):  # skip MHA for submodules\nret[name] = m.attn.cpu().numpy()\n+        self.train()\nreturn ret\n", "fix_pattern": "<condition>: The code is checking if an object is an instance of the MultiHeadedAttention class and if its `attn` attribute is not None.\n<pattern>: There is no clear pattern identified in the code removed section.\n<code_one>: No specific code is removed in this case.\n<code_two>: In the code added section, the `self.train()` statement is added.\nFix_pattern: In the condition of `isinstance(m, MultiHeadedAttention) and m.attn is not None`, if the condition is satisfied, then add `self.train()` to fix the API misuse."}
{"number": 3357, "change": "class TestHausdorffLoss:\nassert_close(actual, expected)\n\n@pytest.mark.parametrize(\"hd,shape\", [\n-        [kornia.losses.HausdorffERLoss, (10, 10)],\n-        [kornia.losses.HausdorffERLoss3D, (10, 10, 10)],\n+        [kornia.losses.HausdorffERLoss, (5, 5)],\n+        [kornia.losses.HausdorffERLoss3D, (5, 5, 5)],\n])\n-    @pytest.mark.skip(reason='It passed, but will take too much time to run.')\ndef test_gradcheck(self, hd, shape, device):\nnum_classes = 3\nlogits = torch.rand(2, num_classes, *shape, device=device)\n", "fix_pattern": "<condition>: In the test_gradcheck method of the TestHausdorffLoss class.\n<pattern>: The code is using the HausdorffERLoss and HausdorffERLoss3D classes with (10, 10) and (10, 10, 10) as arguments respectively.\n<code_one>: [kornia.losses.HausdorffERLoss, (10, 10)] and [kornia.losses.HausdorffERLoss3D, (10, 10, 10)]\n<code_two>: [kornia.losses.HausdorffERLoss, (5, 5)] and [kornia.losses.HausdorffERLoss3D, (5, 5, 5)]\nFix_pattern: In the test_gradcheck method of the TestHausdorffLoss class, if the code is using the HausdorffERLoss and HausdorffERLoss3D classes with (10, 10) and (10, 10, 10) as arguments respectively, then change [kornia.losses.HausdorffERLoss, (10, 10)] to [kornia.losses.HausdorffERLoss, (5, 5)] and [kornia.losses.HausdorffERLoss3D, (10, 10, 10)] to [kornia.losses.HausdorffERLoss3D, (5, 5, 5)] to fix the API misuse."}
{"number": 3362, "change": "class TargetIndegree(object):\n\nif pseudo is not None and self.cat:\npseudo = pseudo.view(-1, 1) if pseudo.dim() == 1 else pseudo\n-            data.weight = torch.cat([pseudo, deg.type_as(pseudo)], dim=-1)\n+            data.edge_attr = torch.cat([pseudo, deg.type_as(pseudo)], dim=-1)\nelse:\n-            data.weight = deg\n+            data.edge_attr = deg\n\nreturn data\n", "fix_pattern": "<condition>:\nThe condition for the fix pattern is if the variable \"pseudo\" is not None and the variable \"self.cat\" is True.\n\n<pattern>:\nThe pattern that is detected is the incorrect assignment of the variable \"data.weight\" in the code removed section.\n\n<code_one>:\nThe code that is removed is:\n```\ndata.weight = torch.cat([pseudo, deg.type_as(pseudo)], dim=-1)\ndata.weight = deg\n```\n\n<code_two>:\nThe code that is added is:\n```\ndata.edge_attr = torch.cat([pseudo, deg.type_as(pseudo)], dim=-1)\ndata.edge_attr = deg\n```\n\nFix_pattern:\nIn the condition of \"pseudo is not None\" and \"self.cat is True\", if the incorrect assignment of \"data.weight\" to \"deg\" is detected, then change \"data.weight\" to \"data.edge_attr\" to fix the API misuse."}
{"number": 3372, "change": "class LukeModelIntegrationTests(unittest.TestCase):\nexpected_shape = torch.Size((1, 1, 1024))\nself.assertEqual(outputs.entity_last_hidden_state.shape, expected_shape)\n\n-        expected_slice = torch.tensor([[0.0466, -0.0106, -0.0179]])\n+        expected_slice = torch.tensor([[0.0466, -0.0106, -0.0179]]).to(torch_device)\nself.assertTrue(torch.allclose(outputs.entity_last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n", "fix_pattern": "<condition>: The test case is checking for the shape and values of a tensor.\n<pattern>: The expected_slice tensor is being added to the assert statement in the test case.\n<code_one>: expected_slice = torch.tensor([[0.0466, -0.0106, -0.0179]])\n<code_two>: expected_slice = torch.tensor([[0.0466, -0.0106, -0.0179]]).to(torch_device)\nFix_pattern: In the condition of checking the tensor shape and values, if the expected_slice tensor is not matching the actual value, then add the expected_slice tensor with the to() method to fix the API misuse."}
{"number": 3376, "change": "Returns:\n\"\"\"\n\n\n-class Rouge(nlp.Metric):\n+class Rouge(datasets.Metric):\ndef _info(self):\n-        return nlp.MetricInfo(\n+        return datasets.MetricInfo(\ndescription=_DESCRIPTION,\ncitation=_CITATION,\ninputs_description=_KWARGS_DESCRIPTION,\n-            features=nlp.Features(\n+            features=datasets.Features(\n{\n-                    \"predictions\": nlp.Value(\"string\", id=\"sequence\"),\n-                    \"references\": nlp.Value(\"string\", id=\"sequence\"),\n+                    \"predictions\": datasets.Value(\"string\", id=\"sequence\"),\n+                    \"references\": datasets.Value(\"string\", id=\"sequence\"),\n}\n),\ncodebase_urls=[\"https://github.com/google-research/google-research/tree/master/rouge\"],\n", "fix_pattern": "<condition>: API misuse in the codebase\n<pattern>: Incorrect usage of the nlp.Metric class\n<code_one>: class Rouge(nlp.Metric)\n<code_two>: class Rouge(datasets.Metric)\nFix_pattern: In the condition of API misuse, if the incorrect usage of the nlp.Metric class is detected, then change the code from \"class Rouge(nlp.Metric)\" to \"class Rouge(datasets.Metric)\" to fix the API misuse."}
{"number": 3383, "change": "class MixedInt8T5Test(unittest.TestCase):\n`flan-t5-small` uses `T5DenseGatedActDense` whereas `t5-small` uses `T5DenseReluDense`. We need to test\nboth cases.\n\"\"\"\n+        import bitsandbytes as bnb\n+\nfrom transformers import T5ForConditionalGeneration\n\n# test with `t5-small`\nmodel = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_8bit=True, device_map=\"auto\")\n+\n+        # there was a bug with decoders - this test checks that it is fixed\n+        self.assertTrue(isinstance(model.decoder.block[0].layer[0].SelfAttention.q, bnb.nn.Linear8bitLt))\n+\nencoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(0)\n_ = model.generate(**encoded_input)\n", "fix_pattern": "<condition>: The condition is not clear in the given context.\n\n<pattern>: The pattern is not clear in the given code. \n\n<code_one>: No code is removed in the given code.\n\n<code_two>: The code added is \"import bitsandbytes as bnb\" and \"self.assertTrue(isinstance(model.decoder.block[0].layer[0].SelfAttention.q, bnb.nn.Linear8bitLt))\".\n\nFix_pattern: In an unknown condition, the fix consists of importing the \"bitsandbytes\" module and adding a specific check using the \"isinstance\" function."}
{"number": 3384, "change": "sess = tf.InteractiveSession()\nnetwork.print_params(False)\n\nsaver = tf.train.Saver()\n-if not os.path.isfile(\"inception_v3.ckpt\"):\n+if not os.path.isfile(MODEL_PATH):\nraise Exception(\n\"Please download inception_v3 ckpt from https://github.com/tensorflow/models/tree/master/research/slim\"\n)\n\ntry:  # TF12+\n-    saver.restore(sess, \"./inception_v3.ckpt\")\n+    saver.restore(sess, MODEL_PATH)\nexcept Exception:  # TF11\n-    saver.restore(sess, \"inception_v3.ckpt\")\n+    saver.restore(sess, MODEL_PATH)\nprint(\"Model Restored\")\n\ny = network.outputs\n", "fix_pattern": "<condition>: Checking if a file exists\n<pattern>: Checking if a specific file does not exist\n<code_one>: \nif not os.path.isfile(\"inception_v3.ckpt\"):\n    saver.restore(sess, \"./inception_v3.ckpt\")\n    saver.restore(sess, \"inception_v3.ckpt\")\n\n<code_two>:\nif not os.path.isfile(MODEL_PATH):\n    saver.restore(sess, MODEL_PATH)\n    saver.restore(sess, MODEL_PATH)\n\nFix_pattern:\nIn the condition of checking if a file exists, if a specific file does not exist, then remove the code that restores the checkpoint from the default location and add the code to restore the checkpoint from the specified MODEL_PATH to fix the API misuse."}
{"number": 3392, "change": "class Detect(nn.Module):\ny = torch.cat((xy, wh, conf), 4)\nz.append(y.view(bs, -1, self.no))\n\n-        return x if self.training else (torch.cat(z, 1), x)\n+        return x if self.training else (torch.cat(z, 1),) if self.export else (torch.cat(z, 1), x)\n\ndef _make_grid(self, nx=20, ny=20, i=0):\nd = self.anchors[i].device\n", "fix_pattern": "<condition>: The condition is when the variable \"self.export\" is False.\n<pattern>: The pattern is the absence of the variable \"x\" in the returned value.\n<code_one>: The code that was removed is \"x\".\n<code_two>: The code that was added is \", x\" after \"(torch.cat(z, 1)\".\nFix_pattern: In the condition of \"self.export\" being False, if the absence of \"x\" in the returned value is detected, then add \"x\" after \"(torch.cat(z, 1)\" to fix the API misuse."}
{"number": 3394, "change": "def benchmark_indices_mapping():\nfunctions = (select, sort, shuffle, train_test_split, shard)\nwith tempfile.TemporaryDirectory() as tmp_dir:\nprint(\"generating dataset\")\n-        features = nlp.Features({\"text\": nlp.Value(\"string\"), \"numbers\": nlp.Value(\"float32\")})\n+        features = datasets.Features({\"text\": datasets.Value(\"string\"), \"numbers\": datasets.Value(\"float32\")})\ndataset = generate_example_dataset(\nos.path.join(tmp_dir, \"dataset.arrow\"), features, num_examples=SPEED_TEST_N_EXAMPLES\n)\n", "fix_pattern": "<condition>: The code is using the `nlp` module.\n<pattern>: The code is incorrectly using the `nlp.Features` class.\n<code_one>: `features = nlp.Features({\"text\": nlp.Value(\"string\"), \"numbers\": nlp.Value(\"float32\")})`\n<code_two>: `features = datasets.Features({\"text\": datasets.Value(\"string\"), \"numbers\": datasets.Value(\"float32\")})`\nFix_pattern: In the condition of using the `nlp` module, if the `nlp.Features` class is detected, then change the `nlp.Features` class to `datasets.Features` to fix the API misuse."}
{"number": 3395, "change": "def main():\n# train\nlogging.info('backend = ' + args.backend)\nif args.backend == \"chainer\":\n-        from espnet.lmchainer.asr_chainer import train\n+        from espnet.asr.chainer.asr_chainer import train\ntrain(args)\nelif args.backend == \"pytorch\":\n-        from espnet.lmpytorch.asr_pytorch import train\n+        from espnet.asr.pytorch.asr_pytorch import train\ntrain(args)\nelse:\nraise ValueError(\"Only chainer and pytorch are supported.\")\n", "fix_pattern": "<condition>: The condition is when the `args.backend` variable is being checked for a specific value.\n\n<pattern>: The pattern being detected is the import statements for the `train` function from the `espnet.lmchainer.asr_chainer` and `espnet.lmpytorch.asr_pytorch` modules.\n\n<code_one>: \n```\nfrom espnet.lmchainer.asr_chainer import train\nfrom espnet.lmpytorch.asr_pytorch import train\n```\n\n<code_two>:\n```\nfrom espnet.asr.chainer.asr_chainer import train\nfrom espnet.asr.pytorch.asr_pytorch import train\n```\n\nFix_pattern:\nIn the condition of checking the `args.backend` variable, if the pattern of importing `train` function from the `espnet.lmchainer.asr_chainer` and `espnet.lmpytorch.asr_pytorch` modules is detected, then change the import statement to import the `train` function from the `espnet.asr.chainer.asr_chainer` and `espnet.asr.pytorch.asr_pytorch` modules to fix the API misuse."}
{"number": 3406, "change": "def get_checkpoint_path(model_path):\nlogger.warn(\n\"Checkpoint path {} is auto-corrected to {}.\".format(model_path, new_path))\nmodel_path = new_path\n-    assert os.path.isfile(model_path) or os.path.isfile(model_path + '.index'), model_path\n+    assert tf.gfile.Exists(model_path) or tf.gfile.Exists(model_path + '.index'), model_path\nreturn model_path\n", "fix_pattern": "<condition>: The code is checking if a file exists.\n<pattern>: The code is using the os.path.isfile() method to check if a file exists.\n<code_one>: assert os.path.isfile(model_path) or os.path.isfile(model_path + '.index'), model_path\n<code_two>: assert tf.gfile.Exists(model_path) or tf.gfile.Exists(model_path + '.index'), model_path\nFix_pattern: In the condition of checking if a file exists, if the pattern of using os.path.isfile() is detected, then change the code from assert os.path.isfile(model_path) or os.path.isfile(model_path + '.index'), model_path to assert tf.gfile.Exists(model_path) or tf.gfile.Exists(model_path + '.index'), model_path to fix the API misuse."}
{"number": 3408, "change": "class TextClassifier(flair.nn.Model):\nself.document_embeddings.embed(sentences)\n\ntext_embedding_list = [sentence.get_embedding().unsqueeze(0) for sentence in sentences]\n-        text_embedding_tensor = torch.cat(text_embedding_list, 0)\n+        text_embedding_tensor = torch.cat(text_embedding_list, 0).to(flair.device)\n\nlabel_scores = self.decoder(text_embedding_tensor)\n", "fix_pattern": "<condition>: The code is using the `torch.cat()` function to concatenate a list of tensors.\n<pattern>: The code does not include the `.to(flair.device)` method after calling `torch.cat()` to move the concatenated tensor to the appropriate device.\n<code_one>: `text_embedding_tensor = torch.cat(text_embedding_list, 0)`\n<code_two>: `text_embedding_tensor = torch.cat(text_embedding_list, 0).to(flair.device)`\nFix_pattern: In the condition of the code where `torch.cat()` is used to concatenate tensors, if the `text_embedding_tensor` is not moved to the correct device, then add the `.to(flair.device)` method after calling `torch.cat()` to fix the API misuse."}
{"number": 3415, "change": "def recog_v2(args):\nfor idx, name in enumerate(js.keys(), 1):\nlogging.info('(%d/%d) decoding ' + name, idx, len(js.keys()))\nbatch = [(name, js[name])]\n-            enc = model.encode(load_inputs_and_targets(batch)[0][0])\n+            feat = load_inputs_and_targets(batch)[0][0]\n+            enc = model.encode(torch.as_tensor(feat).to(device))\nnbest_hyps = beam_search(\nx=enc,\nsos=model.sos,\n", "fix_pattern": "<condition>: In the given code, the condition is that the variable \"enc\" needs to be assigned a value using the \"model.encode\" function.\n<pattern>: The pattern is that the \"model.encode\" function is being used on the wrong argument in the original code.\n<code_one>: The original code is using the \"model.encode\" function on \"load_inputs_and_targets(batch)[0][0]\".\n<code_two>: The fix involves using \"model.encode\" function on \"torch.as_tensor(feat).to(device)\" instead.\nFix_pattern: In the condition of needing to assign a value to the \"enc\" variable using the \"model.encode\" function, if the code is using the wrong argument, then remove the code that assigns the wrong argument to \"enc\" and add the correct code that assigns \"torch.as_tensor(feat).to(device)\" to \"enc\" to fix the API misuse."}
{"number": 3419, "change": "class DecoderRNNT(torch.nn.Module):\nnormscore = recog_args.score_norm_transducer\n\nz_list, c_list = self.zero_state(h.unsqueeze(0))\n-        eys = torch.zeros((1, self.embed_dim))\n+        eys = to_device(self, torch.zeros((1, self.embed_dim)))\n\n_, (z_list, c_list) = self.rnn_forward(eys, None)\n", "fix_pattern": "<condition>: The code is inside a class method of a class named \"DecoderRNNT\" and is part of a larger codebase.\n\n<pattern>: The pattern is an instance where \"eys\" is assigned the value of \"torch.zeros((1, self.embed_dim))\" without using any additional function calls.\n\n<code_one>: \n\n```python\neys = torch.zeros((1, self.embed_dim))\n```\n\n<code_two>: \n\n```python\neys = to_device(self, torch.zeros((1, self.embed_dim)))\n```\n\nFix_pattern: \n\nIn the condition of being inside the class method of \"DecoderRNNT\", if the pattern of assigning \"eys\" the value of \"torch.zeros((1, self.embed_dim))\" is detected, then change the code from \"eys = torch.zeros((1, self.embed_dim))\" to \"eys = to_device(self, torch.zeros((1, self.embed_dim)))\" to fix the API misuse."}
{"number": 3421, "change": "def parse_npz(f):\n\nadj = sp.csr_matrix((f['adj_data'], f['adj_indices'], f['adj_indptr']),\nf['adj_shape']).tocoo()\n-    edge_index = torch.tensor([adj.row, adj.col])\n+    edge_index = torch.tensor([adj.row, adj.col], dtype=torch.long)\nedge_index, _ = remove_self_loops(edge_index)\nedge_index = to_undirected(edge_index, x.size(0))  # Internal coalesce.\n", "fix_pattern": "<condition>: The condition is when creating a tensor edge_index from adj.row and adj.col.\n<pattern>: The pattern is to change the data type of the tensor from the default data type to torch.long.\n<code_one>: The code to be removed is \"dtype=torch.long\".\n<code_two>: The code to be added is \"dtype=torch.long\".\nFix_pattern: In the condition of creating a tensor edge_index from adj.row and adj.col, if the default data type is used, then change the code from \"dtype=torch.long\" to \"dtype=torch.long\" to fix the API misuse."}
{"number": 3441, "change": "class GroupViTTextTransformer(nn.Module):\n\n# text_embeds.shape = [batch_size, sequence_length, transformer.width]\n# take features from the eot embedding (eot_token is the highest number in each sequence)\n-        pooled_output = last_hidden_state[torch.arange(last_hidden_state.shape[0]), input_ids.argmax(dim=-1)]\n+        # casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14\n+        pooled_output = last_hidden_state[\n+            torch.arange(last_hidden_state.shape[0]), input_ids.to(torch.int).argmax(dim=-1)\n+        ]\n\nif not return_dict:\nreturn (last_hidden_state, pooled_output) + encoder_outputs[1:]\n", "fix_pattern": "<condition>:\nThe condition is \"if not return_dict\".\n\n<pattern>:\nThe pattern is \"last_hidden_state[torch.arange(last_hidden_state.shape[0]), input_ids.argmax(dim=-1)]\".\n\n<code_one>:\nThe code removed is \"pooled_output = last_hidden_state[torch.arange(last_hidden_state.shape[0]), input_ids.argmax(dim=-1)]\".\n\n<code_two>:\nThe code added is \"pooled_output = last_hidden_state[torch.arange(last_hidden_state.shape[0]), input_ids.to(torch.int).argmax(dim=-1)]\".\n\nFix_pattern:\nIn the condition of \"if not return_dict\", if the pattern of \"last_hidden_state[torch.arange(last_hidden_state.shape[0]), input_ids.argmax(dim=-1)]\" is detected, then change the code from \"pooled_output = last_hidden_state[torch.arange(last_hidden_state.shape[0]), input_ids.argmax(dim=-1)]\" to \"pooled_output = last_hidden_state[torch.arange(last_hidden_state.shape[0]), input_ids.to(torch.int).argmax(dim=-1)]\" to fix the API misuse."}
{"number": 3442, "change": "def train(args, train_dataset, model, tokenizer, labels, pad_token_label_id):\nif args.fp16:\nwith amp.scale_loss(loss, optimizer) as scaled_loss:\nscaled_loss.backward()\n-                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\nelse:\nloss.backward()\n-                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\ntr_loss += loss.item()\nif (step + 1) % args.gradient_accumulation_steps == 0:\n+                if args.fp16:\n+                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n+                else:\n+                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n+\nscheduler.step()  # Update learning rate schedule\noptimizer.step()\nmodel.zero_grad()\n", "fix_pattern": "<condition>: The condition is whether the argument `args.fp16` is `True`.\n\n<pattern>: The pattern is that the code is clipping the gradient norm using the `torch.nn.utils.clip_grad_norm_` function.\n\n<code_one>: The original code was clipping the gradient norm of `amp.master_params(optimizer)` and `model.parameters()`.\n\n<code_two>: The fixed code clips the gradient norm of `amp.master_params(optimizer)` and `model.parameters()` only if `args.fp16` is `True`, otherwise it does nothing.\n\nFix_pattern: In the condition of `args.fp16` being `True`, if the pattern of clipping the gradient norm is detected, then the `code_one` clipping all parameters of `optimizer` and `model` changes to `code_two` clipping `amp.master_params(optimizer)` and `model.parameters()` respectively to fix the API misuse."}
{"number": 3442, "change": "def train(args, train_dataset, model, tokenizer, labels, pad_token_label_id):\nif args.fp16:\nwith amp.scale_loss(loss, optimizer) as scaled_loss:\nscaled_loss.backward()\n-                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\nelse:\nloss.backward()\n-                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\ntr_loss += loss.item()\nif (step + 1) % args.gradient_accumulation_steps == 0:\n+                if args.fp16:\n+                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n+                else:\n+                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n+\nscheduler.step()  # Update learning rate schedule\noptimizer.step()\nmodel.zero_grad()\n", "fix_pattern": "<condition>: The condition is whether the argument `args.fp16` is True or False.\n\n<pattern>: The pattern is to clip the gradient norm using the `torch.nn.utils.clip_grad_norm_` function.\n\n<code_one>: The code that was removed is `torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)` and `torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)`.\n\n<code_two>: The code that was added is:\n```\nif args.fp16:\n    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\nelse:\n    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n```\n\nFix_pattern: In the condition of `args.fp16`, if the pattern of clipping the gradient norm is detected, then remove the code `torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)` and `torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)` and replace it with the code:\n```\nif args.fp16:\n    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\nelse:\n    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n```"}
{"number": 3452, "change": "class PReluLayer(Layer):\n\n# with tf.name_scope(name) as scope:\nwith tf.variable_scope(name):\n-            alphas = tf.get_variable(name='alphas', shape=w_shape, initializer=a_init, dtype=D_TYPE, **a_init_args)\n+            alphas = tf.get_variable(name='alphas', shape=w_shape, initializer=a_init, dtype=LayersConfig.tf_dtype, **a_init_args)\ntry:  # TF 1.0\nself.outputs = tf.nn.relu(self.inputs) + tf.multiply(alphas, (self.inputs - tf.abs(self.inputs))) * 0.5\nexcept Exception:  # TF 0.12\n", "fix_pattern": "<condition>: In the code, there is a variable named \"alphas\" that is used.\n\n<pattern>: The pattern is that the \"alphas\" variable is declared using the \"tf.get_variable\" method.\n\n<code_one>: The original code declares the \"alphas\" variable using the \"tf.get_variable\" method.\n\n<code_two>: The fixed code declares the \"alphas\" variable using the \"tf.get_variable\" method, but with an additional argument \"dtype=LayersConfig.tf_dtype\".\n\nFix_pattern: In the condition of using the \"alphas\" variable, if it is declared using the \"tf.get_variable\" method, then change the declaration to include the additional argument \"dtype=LayersConfig.tf_dtype\" to fix the API misuse."}
{"number": 3470, "change": "class Input(Layer):\nlogging.info(\"Input  %s: %s\" % (self.name, str(shape)))\n\nshape_without_none = [_ if _ is not None else 1 for _ in shape]\n-        self.outputs = self.forward(tf.initializers.random_normal()(shape_without_none))\n+        self.outputs = self.forward(tf.compat.v1.initializers.random_normal()(shape_without_none))\n\ndef __call__(self, prev_layer):\n# FIXME: better exception raising\n", "fix_pattern": "<condition>: The condition is not stated explicitly in the provided code snippet. No pre-condition is needed.\n\n<pattern>: The pattern is a change in the API usage.\n\n<code_one>: tf.initializers.random_normal()\n\n<code_two>: tf.compat.v1.initializers.random_normal()\n\nFix_pattern: In the condition of no pre-condition, if the tf.initializers.random_normal() API is detected, then change it to tf.compat.v1.initializers.random_normal() to fix the API misuse."}
{"number": 3477, "change": "class Generator(datasets.GeneratorBasedBuilder):\nreturn datasets.DatasetInfo(features=self.config.features)\n\ndef _split_generators(self, dl_manager):\n-        return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={})]\n+        return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs=self.config.gen_kwargs)]\n\n-    def _generate_examples(self):\n-        for idx, ex in enumerate(self.config.generator(**self.config.gen_kwargs)):\n+    def _generate_examples(self, **gen_kwargs):\n+        for idx, ex in enumerate(self.config.generator(**gen_kwargs)):\nyield idx, ex\n", "fix_pattern": "<condition>:\nThe condition is that the code is using the `datasets.GeneratorBasedBuilder` class.\n\n<pattern>:\nThe pattern is that the `_split_generators` method is yielding `idx, ex`.\n\n<code_one>:\nThe code in the `_split_generators` method is removed:\n```\nreturn [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={})]\ndef _generate_examples(self):\n    for idx, ex in enumerate(self.config.generator(**self.config.gen_kwargs)):\n```\n\n<code_two>:\nThe code in the `_generate_examples` method is added:\n```\nreturn [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs=self.config.gen_kwargs)]\ndef _generate_examples(self, **gen_kwargs):\n    for idx, ex in enumerate(self.config.generator(**gen_kwargs)):\n```\n\nFix_pattern:\nIn the condition of using the `datasets.GeneratorBasedBuilder` class, if the `_split_generators` method is yielding `idx, ex`, then the code in the `_split_generators` method should be removed and the code in the `_generate_examples` method should be added to fix the API misuse."}
{"number": 3492, "change": "def run(\nseen, windows, dt = 0, [], (Profile(), Profile(), Profile())\nfor path, im, im0s, vid_cap, s in dataset:\nwith dt[0]:\n-            im = torch.from_numpy(im).to(device)\n+            im = torch.from_numpy(im).to(model.device)\nim = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\nim /= 255  # 0 - 255 to 0.0 - 1.0\nif len(im.shape) == 3:\n", "fix_pattern": "<condition>: The condition is when the length of the shape of the variable \"im\" is equal to 3.\n<pattern>: The pattern is to replace the line \"im = torch.from_numpy(im).to(device)\".\n<code_one>: The original code is \"im = torch.from_numpy(im).to(device)\".\n<code_two>: The fixed code is \"im = torch.from_numpy(im).to(model.device)\".\nFix_pattern: In the condition when the length of the shape of \"im\" is equal to 3, the line \"im = torch.from_numpy(im).to(device)\" should be replaced with \"im = torch.from_numpy(im).to(model.device)\" to fix the API misuse."}
{"number": 3513, "change": "class OPTAttention(nn.Module):\nattn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\nattn_weights = torch.max(attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min))\nattn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-            dtype_attn_weights = attn_weights.dtype\n\n# upcast to fp32 if the weights are in fp16. Please see https://github.com/huggingface/transformers/pull/17437\n-        if dtype_attn_weights == torch.float16:\n-            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(dtype_attn_weights)\n+        if attn_weights.dtype == torch.float16:\n+            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(torch.float16)\nelse:\nattn_weights = nn.functional.softmax(attn_weights, dim=-1)\n", "fix_pattern": "<condition>: The condition is that the `dtype_attn_weights` variable is of type `torch.float16`.\n\n<pattern>: The pattern is to call the `nn.functional.softmax` function with the `dtype` argument as `torch.float32` and then convert the result to `dtype_attn_weights`.\n\n<code_one>: The code that is removed is `attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(dtype_attn_weights)`.\n\n<code_two>: The code that is added is `attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(torch.float16)`.\n\nFix_pattern: In the condition of `dtype_attn_weights` being of type `torch.float16`, if the pattern of calling `nn.functional.softmax` with `dtype=torch.float32` and then converting the result to `dtype_attn_weights` is detected, then the code `attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(dtype_attn_weights)` should be changed to `attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(torch.float16)` to fix the API misuse."}
{"number": 3531, "change": "class ScoreSdeVpScheduler(SchedulerMixin, ConfigMixin):\nself.discrete_sigmas = None\nself.timesteps = None\n\n-    def set_timesteps(self, num_inference_steps):\n-        self.timesteps = torch.linspace(1, self.config.sampling_eps, num_inference_steps)\n+    def set_timesteps(self, num_inference_steps, device: Union[str, torch.device] = None):\n+        self.timesteps = torch.linspace(1, self.config.sampling_eps, num_inference_steps, device=device)\n\ndef step_pred(self, score, x, t, generator=None):\nif self.timesteps is None:\n", "fix_pattern": "<condition>: The condition is that the variable \"self.timesteps\" is None.\n\n<pattern>: The pattern is that the method \"set_timesteps\" is called with only the argument \"num_inference_steps\".\n\n<code_one>: The original code has the method \"set_timesteps\" defined as:\n```python\ndef set_timesteps(self, num_inference_steps):\n    self.timesteps = torch.linspace(1, self.config.sampling_eps, num_inference_steps)\n```\n\n<code_two>: The fixed code has the method \"set_timesteps\" redefined as:\n```python\ndef set_timesteps(self, num_inference_steps, device: Union[str, torch.device] = None):\n    self.timesteps = torch.linspace(1, self.config.sampling_eps, num_inference_steps, device=device)\n```\n\nFix_pattern: In the condition of \"self.timesteps\" being None, if the pattern of calling the method \"set_timesteps\" with only the argument \"num_inference_steps\" is detected, then the code \"self.timesteps = torch.linspace(1, self.config.sampling_eps, num_inference_steps)\" should be changed to \"self.timesteps = torch.linspace(1, self.config.sampling_eps, num_inference_steps, device=device)\" to fix the API misuse."}
{"number": 3533, "change": "class DeepSpeedEngine(Module):\nmodel_dtype = torch.bfloat16\n\nif self._config.grad_accum_dtype == None:\n-            if model_dtype == torch.bfloat16:\n+            if model_dtype == torch.bfloat16 and not self.zero_optimization():\ngrad_accum_dtype = torch.float32\nelse:\ngrad_accum_dtype = model_dtype\n", "fix_pattern": "<condition>: The condition is when the variable \"model_dtype\" is equal to torch.bfloat16.\n\n<pattern>: The pattern to be detected is if the condition mentioned above is true.\n\n<code_one>: The code that needs to be removed is the following if statement: \n   if model_dtype == torch.bfloat16:\n\n<code_two>: The code that needs to be added is the following if statement:\n   if model_dtype == torch.bfloat16 and not self.zero_optimization():\n\nFix_pattern:\nIn the condition of \"model_dtype\" being equal to torch.bfloat16, if this condition is true, then remove the if statement \"if model_dtype == torch.bfloat16:\" and add the following condition \"if model_dtype == torch.bfloat16 and not self.zero_optimization()\" to fix the API misuse."}
{"number": 3555, "change": "def _create(name, pretrained=True, channels=3, classes=80, autoshape=True, verbo\ncfg = list((Path(__file__).parent / 'models').rglob(f'{name}.yaml'))[0]  # model.yaml path\nmodel = Model(cfg, channels, classes)  # create model\nif pretrained:\n-                attempt_download(fname)  # download if not found locally\n-                ckpt = torch.load(fname, map_location=torch.device('cpu'))  # load\n+                ckpt = torch.load(attempt_download(fname), map_location=torch.device('cpu'))  # load\nmsd = model.state_dict()  # model state_dict\ncsd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32\ncsd = {k: v for k, v in csd.items() if msd[k].shape == v.shape}  # filter\n", "fix_pattern": "<condition>: When loading a checkpoint in the code\n<pattern>: The code was downloading the checkpoint file if it was not found locally before loading it\n<code_one>: attempt_download(fname)  # download if not found locally\n<code_two>: ckpt = torch.load(attempt_download(fname), map_location=torch.device('cpu'))  # load\nFix_pattern: In the condition of loading a checkpoint, if the checkpoint file is not found locally, then the code attempts to download it before loading."}
{"number": 3559, "change": "class Model(ModelDesc):\n# For visualization in tensorboard\npadded1 = tf.pad(sampled1, [[0, 0], [HALF_DIFF, HALF_DIFF], [HALF_DIFF, HALF_DIFF], [0, 0]])\npadded2 = tf.pad(sampled2, [[0, 0], [HALF_DIFF, HALF_DIFF], [HALF_DIFF, HALF_DIFF], [0, 0]])\n-        img_orig = tf.concat(1, [image[:, :, :, 0], image[:, :, :, 1]])  # b x 2h  x w\n-        transform1 = tf.concat(1, [padded1[:, :, :, 0], padded1[:, :, :, 1]])\n-        transform2 = tf.concat(1, [padded2[:, :, :, 0], padded2[:, :, :, 1]])\n-        stacked = tf.concat(2, [img_orig, transform1, transform2], 'viz')\n+        img_orig = tf.concat_v2([image[:, :, :, 0], image[:, :, :, 1]], 1)  # b x 2h  x w\n+        transform1 = tf.concat_v2([padded1[:, :, :, 0], padded1[:, :, :, 1]], 1)\n+        transform2 = tf.concat_v2([padded2[:, :, :, 0], padded2[:, :, :, 1]], 1)\n+        stacked = tf.concat_v2([img_orig, transform1, transform2], 2, 'viz')\ntf.summary.image('visualize',\ntf.expand_dims(stacked, -1), max_images=30)\n\n-        sampled = tf.concat(3, [sampled1, sampled2], 'sampled_concat')\n+        sampled = tf.concat_v2([sampled1, sampled2], 3, 'sampled_concat')\nlogits = (LinearWrap(sampled)\n.apply(symbf.batch_flatten)\n.FullyConnected('fc1', out_dim=256, nl=tf.nn.relu)\n", "fix_pattern": "<condition>: The condition is not clearly mentioned in the given code snippet.\n<pattern>: The pattern is to change the function used for concatenation from `tf.concat` to `tf.concat_v2`.\n<code_one>: The original code is using `tf.concat` function for concatenation.\n<code_two>: The updated code uses `tf.concat_v2` function for concatenation.\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then change the <code_one> to <code_two> to fix the API misuse."}
{"number": 3567, "change": "def BatchNorm(x, use_local_stat=None, decay=0.9, epsilon=1e-5):\n\nn_out = shape[-1]  # channel\nassert n_out is not None\n-    beta = tf.get_variable('beta', [n_out])\n+    beta = tf.get_variable('beta', [n_out],\n+            initializer=tf.zeros_initializer)\ngamma = tf.get_variable('gamma', [n_out],\n-        initializer=tf.ones_initializer)\n+            initializer=tf.ones_initializer)\n\nif len(shape) == 2:\nbatch_mean, batch_var = tf.nn.moments(x, [0], keep_dims=False)\n", "fix_pattern": "<condition>: The condition is when the shape of the input tensor has a length of 2.\n<pattern>: The pattern is to change the initialization of the 'beta' variable from using 'tf.ones_initializer' to using 'tf.zeros_initializer' followed by 'tf.ones_initializer'.\n<code_one>: The code removed is 'initializer=tf.ones_initializer'.\n<code_two>: The code added is 'initializer=tf.zeros_initializer) initializer=tf.ones_initializer)'.\nFix_pattern: In the condition of the input tensor having a shape length of 2, if the 'beta' variable is initialized using 'tf.ones_initializer', then change it to first be initialized using 'tf.zeros_initializer' followed by 'tf.ones_initializer' to fix the API misuse."}
{"number": 3569, "change": "def densenet_block(incoming, nb_layers, growth, bottleneck=True,\n\"\"\"\ndensenet = incoming\n\n-    for i in range(nb_layers):\n+    with tf.variable_scope(scope, default_name=name, values=[incoming],\n+                           reuse=reuse) as scope:\n\n-        with tf.variable_scope(scope, default_name=name, values=[incoming],\n-                               reuse=reuse) as scope:\n+        for i in range(nb_layers):\n\n# Identity\nconn = densenet\n", "fix_pattern": "<condition>: The condition is not specified in the given context.\n\n<pattern>: The pattern is to change the indentation of the for loop.\n\n<code_one>: The original code is indented incorrectly.\n\n<code_two>: The corrected code has the for loop properly indented.\n\nFix_pattern: In the condition of <condition>, if the incorrect indentation pattern is detected, then change the <code_one> to <code_two> to fix the API misuse."}
{"number": 3573, "change": "class BertForQuestionAnswering(nn.Module):\ndef compute_loss(logits, positions):\nmax_position = positions.max().item()\none_hot = torch.FloatTensor(batch_size, max(max_position, seq_length) +1, device=input_ids.device).zero_()\n-                one_hot = one_hot.scatter(1, positions, 1)\n+                one_hot = one_hot.scatter(1, positions.cpu(), 1) # Second argument need to be LongTensor and not cuda.LongTensor\none_hot = one_hot[:, :seq_length]\nlog_probs = nn.functional.log_softmax(logits, dim = -1).view(batch_size, seq_length)\nloss = -torch.mean(torch.sum(one_hot*log_probs), dim = -1)\n", "fix_pattern": "<condition>: \nThe condition is not clearly mentioned in the given context.\n\n<pattern>:\nThe pattern is to change the second argument of the `scatter` function from `positions` to `positions.cpu()`.\n\n<code_one>:\nThe code that needs to be changed is `one_hot.scatter(1, positions, 1)`.\n\n<code_two>:\nThe updated code is `one_hot.scatter(1, positions.cpu(), 1)`.\n\nFix_pattern:\nIn the condition of the given context, if the code `one_hot.scatter(1, positions, 1)` is detected, then change it to `one_hot.scatter(1, positions.cpu(), 1)` to fix the API misuse."}
{"number": 3585, "change": "class CrossViT(nn.Module):\n\n# NOTE: was before branch token section, move to here to assure all branch token are before layer norm\nxs = [norm(xs[i]) for i, norm in enumerate(self.norm)]\n-        return [xo[:, 0] for xo in xs]\n+        return xs\n\ndef forward(self, x):\nxs = self.forward_features(x)\n-        ce_logits = [head(xs[i]) for i, head in enumerate(self.head)]\n+        ce_logits = [head(xs[i][:, 0]) for i, head in enumerate(self.head)]\nif not isinstance(self.head[0], nn.Identity):\nce_logits = torch.mean(torch.stack(ce_logits, dim=0), dim=0)\nreturn ce_logits\n", "fix_pattern": "<condition>: The condition is not explicitly mentioned in the provided code snippet.\n\n<pattern>: The pattern is the incorrect usage of the `ce_logits` variable. It is created using `ce_logits = [head(xs[i]) for i, head in enumerate(self.head)]`, but it is not returned properly in the `forward` method.\n\n<code_one>: \n\n```\nce_logits = [head(xs[i]) for i, head in enumerate(self.head)]\n```\n\n<code_two>: \n\n```\nce_logits = [head(xs[i][:, 0]) for i, head in enumerate(self.head)]\n```\n\nFix_pattern: In the condition of forward method, if the pattern `[head(xs[i]) for i, head in enumerate(self.head)]` is detected, then change the code from `ce_logits = [head(xs[i]) for i, head in enumerate(self.head)]` to `ce_logits = [head(xs[i][:, 0]) for i, head in enumerate(self.head)]` to fix the API misuse."}
{"number": 3598, "change": "def coalesce(edge_index, edge_attr=None, num_nodes=None):\n_, perm = unique(index)\nedge_index = edge_index[:, perm]\nelse:\n-        sparse = getattr(torch.sparse, edge_attr.type().split('.')[-1])\n+        t = torch.cuda if edge_attr.is_cuda else torch\n+        sparse = getattr(t.sparse, edge_attr.type().split('.')[-1])\nn = num_nodes\nsize = torch.Size([n, n] + list(edge_attr.size())[1:])\nadj = sparse(edge_index, edge_attr, size).coalesce()\n", "fix_pattern": "<condition>: The code is checking whether `edge_attr` is of type `torch.sparse`. \n<pattern>: `edge_attr` type is determined using `edge_attr.type().split('.')[-1]`.\n<code_one>: `sparse = getattr(torch.sparse, edge_attr.type().split('.')[-1])`\n<code_two>: `t = torch.cuda if edge_attr.is_cuda else torch` and `sparse = getattr(t.sparse, edge_attr.type().split('.')[-1])`\nFix_pattern: In the condition of checking the type of `edge_attr` as `torch.sparse`, if the type is detected, then the code replaces `edge_attr.type().split('.')[-1]` with `t = torch.cuda if edge_attr.is_cuda else torch` to fix the API misuse."}
{"number": 3616, "change": "def md5sum(filename):\n\n\ndef switch_mps_device(model_name, device):\n-    if model_name not in MPS_SUPPORT_MODELS and (\n-        device == \"mps\" or device == torch.device(\"mps\")\n-    ):\n+    if model_name not in MPS_SUPPORT_MODELS and str(device) == \"mps\":\nlogger.info(f\"{model_name} not support mps, switch to cpu\")\nreturn torch.device(\"cpu\")\nreturn device\n", "fix_pattern": "<condition>: The condition is when the model name is not in the list of MPS support models and the device is \"mps\" or a torch device(\"mps\").\n<pattern>: The pattern is checking if the device is \"mps\" by comparing the string representation of the device with \"mps\".\n<code_one>: The code that is removed is the condition statement mentioned above.\n<code_two>: The code that is added is the modified condition statement that checks if the string representation of the device is \"mps\".\nFix_pattern: In the condition of the model name not being in the MPS support models and the device being \"mps\" or a torch device(\"mps\"), the code is changed from checking the device directly to checking the string representation of the device."}
{"number": 3623, "change": "class ImagePreprocessingPass(unittest.TestCase):\nx4 = mb.add(x=x1, y=x3)\nreturn mb.relu(x=x4)\n\n-        proto = converter._convert(prog, inputs=[ImageType(name=\"x\", shape=(10, 20, 30, 3), channel_first=False)], convert_from=\"mil\", convert_to=\"nn_proto\")\n-        model = models.MLModel(proto)\n-        assert model is not None\n-        assert len(model._spec.neuralNetwork.layers) == 3\n+        mlmodel = ct.convert(prog,\n+            inputs=[ImageType(name=\"x\", shape=(10, 20, 30, 3),\n+              channel_first=False)],\n+            source=\"mil\", convert_to=\"nn_proto\")\n+        assert mlmodel is not None\n+        assert len(mlmodel.get_spec().neuralNetwork.layers) == 3\n", "fix_pattern": "<condition>: There is a need to convert a model from \"mil\" format to \"nn_proto\" format.\n<pattern>: The code for converting the model is using the converter._convert() method.\n<code_one>: The code for converting the model using converter._convert() method.\n<code_two>: The code for converting the model using ct.convert() method.\nFix_pattern: In the condition of needing to convert the model from \"mil\" format to \"nn_proto\" format, if the code for converting the model using converter._convert() method is detected, then remove the code and add the code for converting the model using ct.convert() method to fix the API misuse."}
{"number": 3633, "change": "def add_dataset_args(parser, train=False, gen=False):\nreturn group\n\n\n-def add_distributed_training_args(parser):\n+def add_distributed_training_args(parser, default_world_size=None):\ngroup = parser.add_argument_group(\"Distributed training\")\n# fmt: off\n+    if default_world_size is None:\n+        default_world_size = max(1, torch.cuda.device_count())\ngroup.add_argument('--distributed-world-size', type=int, metavar='N',\n-                       default=max(1, torch.cuda.device_count()),\n+                       default=default_world_size,\nhelp='total number of GPUs across all nodes (default: all visible GPUs)')\ngroup.add_argument('--distributed-rank', default=0, type=int,\nhelp='rank of the current worker')\n", "fix_pattern": "<condition>: The condition is when adding distributed training args to a parser.\n<pattern>: The pattern is that the default value for the distributed world size is calculated as the maximum of 1 and the number of visible GPUs using the torch.cuda.device_count() function.\n<code_one>: The code that was removed is the default value calculation for the distributed world size.\n<code_two>: The code that was added is a new implementation of the add_distributed_training_args() function with an optional default_world_size parameter and a default value calculation using the max() function.\nFix_pattern: In the condition of adding distributed training args to a parser, if the default value for the distributed world size is calculated as the maximum of 1 and the number of visible GPUs, then remove the code for the default value calculation and add a new implementation of the add_distributed_training_args() function with an optional default_world_size parameter and a default value calculation using the max() function."}
{"number": 3660, "change": "class OpenSlr(datasets.GeneratorBasedBuilder):\n# set absolute path for audio file\npath = os.path.join(path_to_datas[i], f\"{filename}.wav\")\ncounter += 1\n-                        yield counter, {\"path\": path, \"sentence\": sentence}\n+                        yield counter, {\"path\": path, \"audio\": path, \"sentence\": sentence}\n", "fix_pattern": "<condition>: The code is generating a dataset using the OpenSlr class.\n<pattern>: The code is yielding a dictionary with the key \"path\".\n<code_one>: \"path\" key is being used in the yielded dictionary.\n<code_two>: The key \"path\" is being changed to \"audio\" in the yielded dictionary.\nFix_pattern: In the condition of using the OpenSlr class to generate a dataset, if the code is yielding a dictionary with the key \"path\", then change the key \"path\" to \"audio\" in the yielded dictionary to fix the API misuse."}
{"number": 3662, "change": "def find_homography_dlt(\nU, S, V = torch.svd(A)\nexcept:\nwarnings.warn('SVD did not converge', RuntimeWarning)\n-        return torch.empty((points1_norm.size(0), 3, 3), device=points1.device)\n+        return torch.empty((points1_norm.size(0), 3, 3), device=device, dtype=dtype)\n\nH = V[..., -1].view(-1, 3, 3)\nH = transform2.inverse() @ (H @ transform1)\n", "fix_pattern": "<condition>:\n\nThe condition in this context is when the Singular Value Decomposition (SVD) does not converge.\n\n<pattern>:\n\nThe pattern in this context is that the code is returning an empty tensor.\n\n<code_one>:\n\n'''\nreturn torch.empty((points1_norm.size(0), 3, 3), device=points1.device)\n'''\n\n<code_two>:\n\n'''\nreturn torch.empty((points1_norm.size(0), 3, 3), device=device, dtype=dtype)\n'''\n\nFix_pattern:\n\nIn the condition of the SVD not converging, if the code is returning an empty tensor, then change the code to return the empty tensor with the additional arguments for device and data type."}
{"number": 3686, "change": "class ESPnetASRMixModel(AbsESPnetModel):\nignore_label=self.ignore_id,\n)\n)\n-        loss_att = torch.mean(loss_att)\n+        loss_att = torch.stack(loss_att, dim=0).mean()\nacc_att = np.mean(acc_att)\n\n# Compute cer/wer using attention-decoder\n", "fix_pattern": "<condition>: This fix pattern is applicable when there is a variable named \"loss_att\" that needs to be recalculated using a different method.\n\n<pattern>: The pattern is that the variable \"loss_att\" is being recalculated by calling the \"torch.mean()\" function in the original code, but it needs to be replaced with \"torch.stack(loss_att, dim=0).mean()\" to fix the API misuse.\n\n<code_one>: The original code is using \"torch.mean(loss_att)\".\n\n<code_two>: The fixed code should use \"torch.stack(loss_att, dim=0).mean()\".\n\nFix_pattern: In the condition of having a variable named \"loss_att\", if the wrong method of \"torch.mean()\" is detected, then change the code from \"torch.mean(loss_att)\" to \"torch.stack(loss_att, dim=0).mean()\" to fix the API misuse."}
{"number": 3693, "change": "class RagConfig(PretrainedConfig):\ndecoder_config = kwargs.pop(\"generator\")\ndecoder_model_type = decoder_config.pop(\"model_type\")\n\n-        from .configuration_auto import AutoConfig\n+        from ..auto.configuration_auto import AutoConfig\n\nself.question_encoder = AutoConfig.for_model(question_encoder_model_type, **question_encoder_config)\nself.generator = AutoConfig.for_model(decoder_model_type, **decoder_config)\n", "fix_pattern": "<condition>: When the import statement is used to import a module.\n<pattern>: Import statement is changed to correct the path or module name.\n<code_one>: from .configuration_auto import AutoConfig\n<code_two>: from ..auto.configuration_auto import AutoConfig\nFix_pattern: In the condition of import statement, if the incorrect path or module name is detected, then the code is changed from <code_one> to <code_two> to fix the API misuse."}
{"number": 3697, "change": "TENSOR_CLASS_NAMES = (\n\"Tensor\",\n)\n\n-ST = TypeVar(\"ST\")\n+ST = t.TypeVar(\"ST\")\n\n\n-def _isinstance_wrapper(obj: ST, sobj: Union[str, type, Sequence]) -> bool:\n+def _isinstance_wrapper(obj: ST, sobj: t.Union[str, type, t.Sequence]) -> bool:\n\"\"\"\n`isinstance` wrapper to check tensor spec\n", "fix_pattern": "<condition>: The code is checking the type of an object using an `isinstance` wrapper.\n\n<pattern>: The TypeVar annotation for the `ST` variable is missing.\n\n<code_one>: `ST = TypeVar(\"ST\")`\n\n<code_two>: `ST = t.TypeVar(\"ST\")`\n\nFix_pattern: In the condition of checking the object's type using an `isinstance` wrapper, if the `TypeVar` annotation for the `ST` variable is missing, then change `ST = TypeVar(\"ST\")` to `ST = t.TypeVar(\"ST\")` to fix the API misuse."}
{"number": 3705, "change": "class WaveNet(object):\nThe variables are all scoped to the given name.\n'''\nwith tf.variable_scope(name):\n-            input_batch = self.encode(input_batch)\n+            input_batch = mu_law_encode(input_batch,\n+                                        self.quantization_channels)\nencoded = self._one_hot(input_batch)\nraw_output = self._create_network(encoded)\n", "fix_pattern": "<condition>: The code is within a class named \"WaveNet\" with variables scoped to a given name.\n<pattern>: The \"input_batch\" variable is being encoded before being passed to the network.\n<code_one>: input_batch = self.encode(input_batch)\n<code_two>: input_batch = mu_law_encode(input_batch, self.quantization_channels)\nFix_pattern: In the condition of being within the class \"WaveNet\", if the \"input_batch\" variable needs to be encoded, then change the code \"input_batch = self.encode(input_batch)\" to \"input_batch = mu_law_encode(input_batch, self.quantization_channels)\" to fix the API misuse."}
{"number": 3706, "change": "def tensordot(\nout: Optional[torch.Tensor] = None,\n) -> torch.Tensor:\n# find the type to promote to\n-    dtype = torch.promote_types(x1.dtype, x2.dtype)\n+    dtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))\n# type conversion to one that torch.tensordot can work with\nx1, x2 = x1.type(torch.float32), x2.type(torch.float32)\n", "fix_pattern": "<condition>: The code is performing type conversion using the \"torch.type\" method.\n<pattern>: The \"torch.promote_types\" method is being used for type promotion.\n<code_one>: `dtype = torch.promote_types(x1.dtype, x2.dtype)`\n<code_two>: `dtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))`\nFix_pattern: In the condition of type conversion using the \"torch.type\" method, if type promotion using the \"torch.promote_types\" method is detected, then change `dtype = torch.promote_types(x1.dtype, x2.dtype)` to `dtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))` to fix the API misuse."}
{"number": 3724, "change": "def test_train_step_epoch_end_scalar(tmpdir):\ntrain_step_out = out.training_step_output_for_epoch_end\nassert len(train_step_out) == 1\ntrain_step_out = train_step_out[0][0]\n-    assert isinstance(train_step_out, torch.Tensor)\n-    assert train_step_out.item() == 171\n+    assert isinstance(train_step_out['minimize'], torch.Tensor)\n+    assert train_step_out['minimize'].item() == 171\n\n# make sure the optimizer closure returns the correct things\nopt_closure_result = trainer.train_loop.training_step_and_backward(\n", "fix_pattern": "<condition>: The condition of the fix pattern is when the variable `train_step_out` is being checked for its type and value.\n\n<pattern>: The pattern is detecting whether `train_step_out` is an instance of `torch.Tensor` and if its value is equal to 171.\n\n<code_one>: The code that is being removed is the following:\n```\nassert isinstance(train_step_out, torch.Tensor)\nassert train_step_out.item() == 171\n```\n\n<code_two>: The code that is being added is the following:\n```\nassert isinstance(train_step_out['minimize'], torch.Tensor)\nassert train_step_out['minimize'].item() == 171\n```\n\nFix_pattern: In the condition of `train_step_out` being checked for its type and value, if the pattern of `train_step_out` being an instance of `torch.Tensor` and its value being equal to 171 is detected, then the code is changed from checking `train_step_out` to `train_step_out['minimize']` for its type and value, and the value to be checked remains the same as 171."}
{"number": 3780, "change": "class OneHotCategorical(Distribution):\nsample. The last dimension is used for the one-hot encoding.\n:rtype: torch.autograd.Variable.\n\"\"\"\n-        return Variable(torch.stack([t.expand_as(self.ps) for t in torch_eye(*self.event_shape())]))\n+        result = torch.stack([t.expand_as(self.ps) for t in torch_eye(*self.event_shape())])\n+        if self.ps.is_cuda:\n+            result = result.cuda(self.ps.get_device())\n+        return Variable(result)\n\ndef analytic_mean(self):\n\"\"\"\n", "fix_pattern": "<condition>: The code is using torch.stack to expand the dimensions of a tensor.\n<pattern>: The pattern is that the code is missing a check for whether the tensor is already on the GPU.\n<code_one>: The code is returning the expanded tensor without checking if it needs to be moved to the GPU.\n<code_two>: The code has been modified to check if the tensor needs to be moved to the GPU before returning it.\nFix_pattern: In the condition of using torch.stack, if the tensor needs to be moved to the GPU, then add code to move the tensor to the GPU before returning it."}
{"number": 3788, "change": "def bidirectional_rnn(incoming, rnncell_fw, rnncell_bw, return_seq=False,\ntf.add_to_collection(tf.GraphKeys.ACTIVATIONS, outputs[-1])\n\nif dynamic:\n-        outputs = tf.transpose(tf.pack(outputs), [1, 0, 2])\n-        o = advanced_indexing_op(outputs, sequence_length)\n+        if return_seq:\n+            o = outputs\n+        else:\n+            outputs = tf.transpose(tf.pack(outputs), [1, 0, 2])\n+            o = advanced_indexing_op(outputs, sequence_length)\nelse:\no = outputs if return_seq else outputs[-1]\n", "fix_pattern": "<condition>: If <pattern> is detected in the condition statement.\n<pattern>: A specific code pattern or condition that needs to be addressed.\n<code_one>: The code that needs to be removed or changed.\n<code_two>: The desired code that will fix the issue.\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then <remove/add/change> the <code_one> to <code_two> to fix the API misuse."}
{"number": 3813, "change": "\"\\n\",\n\"# For evaluation we use exactly normalized rather than\\n\",\n\"# approximately normalized.\\n\",\n-        \"sts_encode1 = tf.nn.l2_normalize(embed(sts_input1))\\n\",\n-        \"sts_encode2 = tf.nn.l2_normalize(embed(sts_input2))\\n\",\n-        \"sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\"\n+        \"sts_encode1 = tf.nn.l2_normalize(embed(sts_input1), axis=1)\\n\",\n+        \"sts_encode2 = tf.nn.l2_normalize(embed(sts_input2), axis=1)\\n\",\n+        \"sim_scores = -tf.acos(tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1))\"\n]\n},\n{\n", "fix_pattern": "<condition>: The code is utilizing a function or API incorrectly.\n<pattern>: The code is not using the `axis` parameter correctly when calling the `tf.nn.l2_normalize` function.\n<code_one>: The incorrect usage of `tf.nn.l2_normalize` with missing `axis` parameter.\n<code_two>: The corrected usage of `tf.nn.l2_normalize` with the `axis` parameter specified as `axis=1`.\nFix_pattern: In the condition of incorrect usage of `tf.nn.l2_normalize`, if missing `axis` parameter is detected, then add `axis=1` to the `tf.nn.l2_normalize` function call to fix the API misuse."}
{"number": 3831, "change": "class TFFastSpeech2(TFFastSpeech):\nduration_outputs = self.duration_predictor(\n[last_encoder_hidden_states, speaker_ids, attention_mask]\n)  # [batch_size, length]\n-        duration_outputs = tf.math.exp(duration_outputs) - 1.0\n+        duration_outputs = tf.nn.relu(tf.math.exp(duration_outputs) - 1.0)\nduration_outputs = tf.cast(\ntf.math.round(duration_outputs * speed_ratios), tf.int32\n)\n", "fix_pattern": "<condition>: No clear condition can be identified.\n<pattern>: The pattern is to add the tf.nn.relu function to the duration_outputs calculation.\n<code_one>: None.\n<code_two>: duration_outputs = tf.nn.relu(tf.math.exp(duration_outputs) - 1.0)\nFix_pattern: In the condition of no clear condition, if the pattern of calculating the duration_outputs is detected, then change the calculation by adding the tf.nn.relu function to fix the API misuse."}
{"number": 3849, "change": "floor_divide.support_native_out = True\n\n\ndef floormod(\n-    x: torch.Tensor, y: torch.Tensor, *, out: Optional[torch.Tensor] = None\n+    x: torch.Tensor, y: torch.Tensor, /, *, out: Optional[torch.Tensor] = None\n) -> torch.Tensor:\n+    x, y = ivy.promote_types_of_inputs(x, y)\nreturn x % y\n", "fix_pattern": "<condition>: The condition is that the \"floor_divide\" function was previously marked to support native out.\n\n<pattern>: The pattern is that the function \"floormod\" does not have the correct signature or parameter types.\n\n<code_one>: The code that needs to be removed is the existing signature of the \"floormod\" function, which is:\n\n    def floormod(\n    ) -> torch.Tensor:\n    \n<code_two>: The code that needs to be added is the corrected signature of the \"floormod\" function, which is:\n\n    def floormod(\n        x: torch.Tensor, y: torch.Tensor, /, *, out: Optional[torch.Tensor] = None\n    ) -> torch.Tensor:\n        x, y = ivy.promote_types_of_inputs(x, y)\n\nFix_pattern: In the condition of \"floor_divide\" supporting native out, if the function \"floormod\" is detected without the correct signature, then remove the existing signature and add the corrected signature to fix the API misuse."}
{"number": 3880, "change": "def _prepare_output_docstrings(output_type, config_class):\n\n# Add the return introduction\nfull_output_type = f\"{output_type.__module__}.{output_type.__name__}\"\n-    intro = RETURN_INTRODUCTION.format(full_output_type=full_output_type, config_class=config_class)\n+    intro = TF_RETURN_INTRODUCTION if output_type.__name__.startswith(\"TF\") else PT_RETURN_INTRODUCTION\n+    intro = intro.format(full_output_type=full_output_type, config_class=config_class)\nreturn intro + docstrings\n", "fix_pattern": "condition: None\npattern: Updating the introduction for return statement based on the output type\ncode_one: intro = RETURN_INTRODUCTION.format(full_output_type=full_output_type, config_class=config_class)\ncode_two: intro = TF_RETURN_INTRODUCTION if output_type.__name__.startswith(\"TF\") else PT_RETURN_INTRODUCTION; intro = intro.format(full_output_type=full_output_type, config_class=config_class)\nFix_pattern: In the condition of introducing the return statement, if the pattern of the output type is detected, then change the code for generating the introduction to accommodate the specific output type."}
{"number": 3915, "change": "class RolloutWorker(EvaluatorInterface, ParallelIteratorWorker):\npolicy_config = policy_config or {}\nif (tf and policy_config.get(\"eager\")\nand not policy_config.get(\"no_eager_on_workers\")):\n-            tf.enable_eager_execution()\n+            # This check is necessary for certain all-framework tests that\n+            # use tf's eager_mode() context generator.\n+            if not tf.executing_eagerly():\n+                tf.enable_eager_execution()\n\nif log_level:\nlogging.getLogger(\"ray.rllib\").setLevel(log_level)\n", "fix_pattern": "<condition>: The condition is that the policy_config dictionary has a key \"eager\" that is True, and the key \"no_eager_on_workers\" is False.\n<pattern>: The pattern is that tf.enable_eager_execution() is being called unconditionally.\n<code_one>: The code being removed is tf.enable_eager_execution().\n<code_two>: The code being added is an if statement that checks if tf is not executing eagerly, and then calls tf.enable_eager_execution().\nFix_pattern: In the condition of the policy_config dictionary having the \"eager\" key set to True and the \"no_eager_on_workers\" key set to False, if the tf.enable_eager_execution() code is detected, then it is removed and replaced with an if statement to check if tf is not executing eagerly before calling tf.enable_eager_execution()."}
{"number": 3947, "change": "class Model(object):\n\nelif action_spec['type'] == 'float':\nif 'min_value' in action_spec:\n-                    exploration = tf.clip_by_value(\n+                    exploration_value = tf.clip_by_value(\nt=exploration_value,\nclip_value_min=action_spec['min_value'],\nclip_value_max=action_spec['max_value']\n)\n\n-                action += tf.reshape(exploration, tf.shape(action))\n+                action += tf.reshape(exploration_value, tf.shape(action))\n\nreturn action\n", "fix_pattern": "<condition>: 'min_value' is in action_spec\n<pattern>: tf.clip_by_value( is used to clip the action\n<code_one>: exploration = tf.clip_by_value(\n<code_two>: exploration_value = tf.clip_by_value(\nFix_pattern: \nIn the condition of 'min_value' being in action_spec, if tf.clip_by_value( is used to clip the action, then change the code from exploration = tf.clip_by_value( to exploration_value = tf.clip_by_value( to fix the API misuse."}
{"number": 3960, "change": "class TestRandomPerspective:\nassert out_perspective[0].shape == x_data.shape\nassert out_perspective[1].shape == (1, 3, 3)\nassert_allclose(out_perspective[0], x_data)\n-        assert_allclose(out_perspective[1], torch.eye(3, device=device))\n+        assert_allclose(out_perspective[1], torch.eye(3, device=device)[None])\n\ndef test_transform_module_should_return_expected_transform(self, device):\ntorch.manual_seed(0)\n", "fix_pattern": "<condition>: The code is checking for equality between two arrays\n<pattern>: The code is asserting the equality using the assert_allclose() function\n<code_one>: assert_allclose(out_perspective[1], torch.eye(3, device=device))\n<code_two>: assert_allclose(out_perspective[1], torch.eye(3, device=device)[None])\nFix_pattern: In the condition of checking array equality, if the code is asserting the equality using the assert_allclose() function, then change the <code_one> to <code_two> to fix the API misuse."}
{"number": 3963, "change": "def average_grads(all_grads):\nfor grad_and_vars in zip(*all_grads):\n# Ngpu * 2\nv = grad_and_vars[0][1]\n-            all_grads = [g for (g, _) in grad_and_vars]\n+            grads = [g for (g, _) in grad_and_vars]\n\nwith tf.device(v.device):       # colocate summed grad with var\ngrad = tf.multiply(\n-                    tf.add_n(all_grads), 1.0 / nr_tower)\n+                    tf.add_n(grads), 1.0 / nr_tower)\nret.append((grad, v))\nreturn ret\n", "fix_pattern": "<condition>: The code is attempting to compute the average of gradients.\n<pattern>: The pattern is incorrect usage of variable names in the computation of the average.\n<code_one>: The code is using the variable name \"all_grads\" instead of \"grads\" in the computation.\n<code_two>: The correct variable name to use is \"grads\" in the computation.\nFix_pattern: In the condition of computing the average of gradients, if the incorrect variable name \"all_grads\" is detected, then change the code from using \"all_grads\" to \"grads\" to fix the API misuse."}
{"number": 3982, "change": "from ivy.container import Container\n\n\ndef variable(x):\n-    with ivy.dev(x, as_native=True):\n+    with tf.device(ivy.dev(x, as_native=True)):\nreturn tf.Variable(x, trainable=True)\n", "fix_pattern": "<condition>: The fix pattern can be applied when there is an API misuse with a specific condition in the code.\n<pattern>: The pattern to be detected that indicates the API misuse.\n<code_one>: The code that needs to be removed or changed in order to fix the API misuse.\n<code_two>: The code that needs to be added or modified to fix the API misuse.\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then <remove/add/change> <code_one> to <code_two> to fix the API misuse."}
{"number": 3992, "change": "class Deterministic(TFActionDistribution):\nreturn self.inputs\n\n@override(TFActionDistribution)\n-    def sampled_action_logp(self):\n-        return 0.0\n+    def logp(self, x):\n+        return tf.zeros_like(self.inputs)\n\n@override(TFActionDistribution)\ndef _build_sample_op(self):\n", "fix_pattern": "<condition>: The code is part of a class that overrides a method from a parent class. \n<pattern>: The method <code_one> is being removed from the class.\n<code_one>: def sampled_action_logp(self):\n<code_two>: def logp(self, x):\nFix_pattern: In the condition of class overriding, if the method \"sampled_action_logp\" is detected, then remove it and add the method \"logp\" to fix the API misuse."}
{"number": 4000, "change": "def test_cholesky_transform(batch_shape, dim, transform):\nassert_close(log_det, torch.slogdet(jacobian)[1])\n\nassert log_det.shape == batch_shape\n-    assert_close(y, x_mat.cholesky())\n+    assert_close(y, torch.linalg.cholesky(x_mat))\nassert_close(transform.inv(y), x_mat)\n", "fix_pattern": "<condition>: The condition is not explicitly mentioned in the given context.\n\n<pattern>: The pattern detected is an API misuse.\n\n<code_one>: The code that is removed is the assertion `assert_close(y, x_mat.cholesky())`.\n\n<code_two>: The code that is added is the assertion `assert_close(y, torch.linalg.cholesky(x_mat))`.\n\nFix_pattern: In the condition of no pre condition is needed, if an API misuse is detected (asserting that `y` is equal to `x_mat.cholesky()`), then remove the code `x_mat.cholesky()` and change it to `torch.linalg.cholesky(x_mat)` to fix the API misuse."}
{"number": 4025, "change": "class SimpleTaggerTest(AllenNlpTestCase):\ntraining_arrays = self.dataset.as_arrays()\n\n# TODO(Mark): clean this up once the Trainer is finalised.\n-        sequence = training_arrays[\"tokens\"][0]\n+        sequence = training_arrays[\"tokens\"][\"tokens\"]\ntags = training_arrays[\"tags\"]\n-        training_arrays = {\"tokens\": Variable(torch.from_numpy(sequence)),  # pylint: disable=no-member\n+        training_arrays = {\"tokens\": {\"tokens\": Variable(torch.from_numpy(sequence))},  # pylint: disable=no-member\n\"tags\": Variable(torch.from_numpy(tags))}  # pylint: disable=no-member\n_ = self.model.forward(**training_arrays)\n\ndef test_tag_returns_distributions_per_token(self):\n-        text = TextField([\"This\", \"is\", \"a\", \"sentence\"], token_indexers=[SingleIdTokenIndexer()])\n+        text = TextField([\"This\", \"is\", \"a\", \"sentence\"], token_indexers={\"tokens\": SingleIdTokenIndexer()})\noutput = self.model.tag(text)\npossible_tags = self.vocab.get_index_to_token_vocabulary(\"tags\").values()\nfor tag in output[\"tags\"]:\n", "fix_pattern": "<condition>: API misuse in the code\n<pattern>: The input data structure for the \"tokens\" field is incorrect.\n<code_one>: sequence = training_arrays[\"tokens\"][0]\n<code_two>: sequence = training_arrays[\"tokens\"][\"tokens\"]\nFix_pattern: In the condition of API misuse, if the incorrect data structure for the \"tokens\" field is detected, then change the code from \"training_arrays[\"tokens\"][0]\" to \"training_arrays[\"tokens\"][\"tokens\"]\" to fix the API misuse."}
{"number": 4040, "change": "def main():\nglobal_step += 1\n\n# Save a trained model\n-        logger.info(\"** ** * Saving fine - tuned model ** ** * \")\nmodel_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\noutput_model_file = os.path.join(args.output_dir, WEIGHTS_NAME)\noutput_config_file = os.path.join(args.output_dir, CONFIG_NAME)\n-        if args.do_train:\n+        if args.do_train and torch.distributed.get_rank() == 0:\n+            logger.info(\"** ** * Saving fine - tuned model ** ** * \")\ntorch.save(model_to_save.state_dict(), output_model_file)\nmodel_to_save.config.to_json_file(output_config_file)\ntokenizer.save_vocabulary(args.output_dir)\n", "fix_pattern": "<condition>: The condition is \"if args.do_train\".\n\n<pattern>: The pattern is \"logger.info(\"** ** * Saving fine-tuned model ** ** * \")\"\n\n<code_one>: The code one is \"if args.do_train:\"\n\n<code_two>: The code two is \"if args.do_train and torch.distributed.get_rank() == 0:\"\n\nFix_pattern: In the condition of \"if args.do_train\", if the pattern \"logger.info(\"** ** * Saving fine-tuned model ** ** * \")\" is detected, then remove the code \"if args.do_train:\" and add the code \"if args.do_train and torch.distributed.get_rank() == 0:\" to fix the API misuse."}
{"number": 4067, "change": "def triu_inverse(x):\nB_Dinv = B / x.bottom_diag.unsqueeze(-2)\n\nidentity = torch.eye(head_size, dtype=A.dtype, device=A.device)\n-    top_left = torch.triangular_solve(identity, A, upper=True)[0]\n+    top_left = torch.linalg.solve_triangular(A, identity, upper=True)\ntop_right = -top_left.matmul(B_Dinv)  # complexity: head_size^2 x N\ntop = torch.cat([top_left, top_right], -1)\nbottom_diag = x.bottom_diag.reciprocal()\n", "fix_pattern": "<condition>: The code is using the function \"torch.triangular_solve\" to solve a triangular system of equations.\n<pattern>: The code was using the \"upper=True\" argument in the \"torch.triangular_solve\" function to solve the upper triangular system.\n<code_one>: top_left = torch.triangular_solve(identity, A, upper=True)[0]\n<code_two>: top_left = torch.linalg.solve_triangular(A, identity, upper=True)\nFix_pattern: In the condition of using \"torch.triangular_solve\" function to solve a triangular system of equations, if the upper triangular system is being solved, then change \"torch.triangular_solve\" to \"torch.linalg.solve_triangular\" to fix the API misuse."}
{"number": 4068, "change": "def train():\n\n# Compute loss\nloss, loss_items = compute_loss(pred, targets, model)\n+            if torch.isnan(loss):\n+                print('WARNING: nan loss detected, skipping batch ', loss_items)\n+                continue\n\n# Scale loss by nominal batch_size of 64\nloss *= batch_size / 64\n", "fix_pattern": "<condition>: Loss value is calculated and scaled by batch size in the code.\n<pattern>: Detetion of NaN loss value.\n<code_one>: N/A (No code is removed)\n<code_two>: The code added is \"if torch.isnan(loss): print('WARNING: nan loss detected, skipping batch ', loss_items) continue\"\nFix_pattern: In the condition where the loss value is calculated and scaled by batch size, if NaN loss is detected, the code prints a warning message and skips the current batch."}
{"number": 4070, "change": "def concat(\nxs = list(xs)\nhighest_dtype = xs[0].dtype\nfor i in xs:\n-        highest_dtype = tf.experimental.numpy.promote_types(highest_dtype, i.dtype)\n+        highest_dtype = ivy.as_native_dtype(ivy.promote_types(highest_dtype, i.dtype))\n\nfor i in range(len(xs)):\nif is_axis_none:\n", "fix_pattern": "<condition>: The condition is not clearly specified in the given context.\n\n<pattern>: The pattern is that the code is using the TensorFlow experimental numpy module to promote types, and it needs to be replaced with the Ivy library's native dtype promotion function.\n\n<code_one>: The code is using the TensorFlow experimental numpy promote_types() function.\n\n<code_two>: The code should use Ivy's as_native_dtype() and promote_types() functions.\n\nFix_pattern: In the condition where the code requires type promotion, the TensorFlow experimental numpy promote_types() function is being replaced with Ivy's as_native_dtype() and promote_types() functions to fix the API misuse."}
{"number": 4072, "change": "else:\nversion_str += f'+{git_hash}'\n\ntorch_version = \".\".join([TORCH_MAJOR, TORCH_MINOR])\n-cuda_version = \".\".join(torch.version.cuda.split('.')[:2])\n+# Set cuda_version to 0.0 if cpu-only\n+cuda_version = \"0.0\"\n+if torch.version.cuda is not None:\n+    cuda_version = \".\".join(torch.version.cuda.split('.')[:2])\ntorch_info = {\"version\": torch_version, \"cuda_version\": cuda_version}\n\nprint(f\"version={version_str}, git_hash={git_hash}, git_branch={git_branch}\")\n", "fix_pattern": "<condition>:\nThe condition is when the torch.version.cuda is not None.\n\n<pattern>:\nThe pattern is to join the first two elements of the torch.version.cuda split by '.' to get the cuda version.\n\n<code_one>:\nThe code that is being removed is:\ncuda_version = \".\".join(torch.version.cuda.split('.')[:2])\n\n<code_two>:\nThe code that is being added is:\ncuda_version = \"0.0\"\nif torch.version.cuda is not None:\n    cuda_version = \".\".join(torch.version.cuda.split('.')[:2])\n\nFix_pattern:\nIn the condition of torch.version.cuda is not None, if the pattern of joining the first two elements of the torch.version.cuda split by '.' to get the cuda version is detected, then remove the code cuda_version = \".\".join(torch.version.cuda.split('.')[:2]) and add the code cuda_version = \"0.0\"\\nif torch.version.cuda is not None:\\n    cuda_version = \".\".join(torch.version.cuda.split('.')[:2]) to fix the API misuse."}
{"number": 4084, "change": "class TestElmoTokenRepresentation(ElmoTestCase):\nfor k in range(10):\nchar_indices = indices[\"elmo\"][(k * 50):((k + 1) * 50)]\nsentences.append(\n-                    indexer.pad_token_sequence(\n+                    indexer.as_padded_tensor(\n{'key': char_indices}, desired_num_tokens={'key': 50}, padding_lengths={}\n)['key']\n)\n-        batch = torch.from_numpy(numpy.array(sentences))\n+        batch = torch.stack(sentences)\n\nelmo_token_embedder = _ElmoCharacterEncoder(self.options_file, self.weight_file)\nelmo_token_embedder_output = elmo_token_embedder(batch)\n", "fix_pattern": "<condition>: When using the `indexer.pad_token_sequence` method.\n<pattern>: When passing a batch that is a numpy array of sentences to the `indexer.pad_token_sequence` method.\n<code_one>: `batch = torch.from_numpy(numpy.array(sentences))`\n<code_two>: `batch = torch.stack(sentences)`\nFix_pattern: In the condition of using the `indexer.pad_token_sequence` method, if a batch that is a numpy array of sentences is detected, then change `batch = torch.from_numpy(numpy.array(sentences))` to `batch = torch.stack(sentences)` to fix the API misuse."}
{"number": 4086, "change": "def multiclass_nms(multi_bboxes, multi_scores, score_thr, nms_cfg, max_num=-1):\nelse:\n_bboxes = multi_bboxes[cls_inds, i * 4:(i + 1) * 4]\n_scores = multi_scores[cls_inds, i]\n+        if score_factors is not None:\n+            _scores *= score_factors[cls_inds]\ncls_dets = torch.cat([_bboxes, _scores[:, None]], dim=1)\ncls_dets, _ = nms_op(cls_dets, **nms_cfg_)\n-        cls_labels = multi_bboxes.new_full(\n-            (cls_dets.shape[0], ), i - 1, dtype=torch.long)\n+        cls_labels = multi_bboxes.new_full((cls_dets.shape[0], ),\n+                                           i - 1,\n+                                           dtype=torch.long)\nbboxes.append(cls_dets)\nlabels.append(cls_labels)\nif bboxes:\n", "fix_pattern": "<condition>: There is a condition where the variable \"bboxes\" is not empty.\n<pattern>: The pattern is that a variable \"cls_labels\" is assigned a new tensor using \"multi_bboxes.new_full\" function.\n<code_one>: \n```python\ncls_labels = multi_bboxes.new_full((cls_dets.shape[0], ), i - 1, dtype=torch.long)\n```\n<code_two>: \n```python\nif score_factors is not None:\n    _scores *= score_factors[cls_inds]\ncls_labels = multi_bboxes.new_full((cls_dets.shape[0], ), i - 1, dtype=torch.long)\n```\nFix_pattern: \nIn the condition of \"if bboxes:\", if the pattern of assigning a new tensor using \"multi_bboxes.new_full\" to \"cls_labels\" is detected, then remove the code that assigns \"cls_labels\" to a new tensor and add the code that checks for \"score_factors\" and multiplies \"_scores\" by \"score_factors[cls_inds]\" before assigning \"cls_labels\" to the new tensor. This fix is used to correct the API misuse."}
{"number": 4087, "change": "class CommonTestCases:\nmodel = model_class(config)\nself.assertIsInstance(\nmodel.get_input_embeddings(),\n-                    torch.nn.Embedding\n+                    (torch.nn.Embedding, AdaptiveEmbedding)\n)\nmodel.set_input_embeddings(torch.nn.Embedding(10, 10))\nx = model.get_output_embeddings()\n", "fix_pattern": "<condition>: The code is using an instance of a specific class. \n<pattern>: The code is using an old class for embedding.\n<code_one>: torch.nn.Embedding\n<code_two>: (torch.nn.Embedding, AdaptiveEmbedding)\nFix_pattern: In the condition of using an instance of the class torch.nn.Embedding, if the old class for embedding is detected, then change torch.nn.Embedding to (torch.nn.Embedding, AdaptiveEmbedding) to fix the API misuse."}
{"number": 4092, "change": "\"@config_enumerate\\n\",\n\"def model(data, num_components=3):\\n\",\n\"    print('Running model with {} data points'.format(len(data)))\\n\",\n-    \"    p = pyro.sample(\\\"p\\\", dist.Dirichlet(0.5 * torch.ones(3)))\\n\",\n+    \"    p = pyro.sample(\\\"p\\\", dist.Dirichlet(0.5 * torch.ones(num_components)))\\n\",\n\"    scale = pyro.sample(\\\"scale\\\", dist.LogNormal(0, num_components))\\n\",\n\"    with pyro.plate(\\\"components\\\", num_components):\\n\",\n\"        loc = pyro.sample(\\\"loc\\\", dist.Normal(0, 10))\\n\",\n", "fix_pattern": "<condition>:\nThe condition is that the code is using a constant value (3) instead of a variable (num_components) when creating a Dirichlet distribution.\n\n<pattern>:\nThe pattern is detected when the code is using a constant value for the number of components in a Dirichlet distribution.\n\n<code_one>:\nThe code that needs to be removed is: \"p = pyro.sample(\\\"p\\\", dist.Dirichlet(0.5 * torch.ones(3)))\".\n\n<code_two>:\nThe code that needs to be added is: \"p = pyro.sample(\\\"p\\\", dist.Dirichlet(0.5 * torch.ones(num_components)))\".\n\nFix_pattern:\nIn the condition of using a constant value instead of a variable for the number of components in a Dirichlet distribution, the code needs to remove the line \"p = pyro.sample(\\\"p\\\", dist.Dirichlet(0.5 * torch.ones(3)))\" and add the line \"p = pyro.sample(\\\"p\\\", dist.Dirichlet(0.5 * torch.ones(num_components)))\" to fix the API misuse."}
{"number": 4100, "change": "class DQNTorchModel(TorchModelV2, nn.Module):\nif self.num_atoms > 1:\n# Distributional Q-learning uses a discrete support z\n# to represent the action value distribution\n-            z = torch.range(0.0, self.num_atoms - 1, dtype=torch.float32)\n+            z = torch.range(\n+                0.0, self.num_atoms - 1,\n+                dtype=torch.float32).to(action_scores.device)\nz = self.v_min + \\\nz * (self.v_max - self.v_min) / float(self.num_atoms - 1)\n", "fix_pattern": "<condition>: `self.num_atoms > 1`\n<pattern>: Assigning a range of values to variable `z`\n<code_one>: `z = torch.range(0.0, self.num_atoms - 1, dtype=torch.float32)`\n<code_two>: ```\nz = torch.range(\n    0.0, self.num_atoms - 1,\n    dtype=torch.float32).to(action_scores.device)\n```\nFix_pattern: In the condition of `self.num_atoms > 1`, if the pattern of assigning a range of values to variable `z` is detected, then change `code_one` to `code_two` to fix the API misuse."}
{"number": 4119, "change": "class LocalMetricTest(parameterized.TestCase):\n@slow\ndef test_load_real_metric(self, metric_name):\ndoctest.ELLIPSIS_MARKER = \"[...]\"\n-        metric_module = importlib.import_module(datasets.load.prepare_module(os.path.join(\"metrics\", metric_name))[0])\n+        metric_module = importlib.import_module(\n+            datasets.load.metric_module_factory(os.path.join(\"metrics\", metric_name)).module_path\n+        )\n# run doctest\nwith self.use_local_metrics():\nresults = doctest.testmod(metric_module, verbose=True, raise_on_error=True)\n", "fix_pattern": "<condition>: The code is in the context of a test case class.\n\n<pattern>: The import statement for a metric module is using a deprecated function from the `datasets.load` module.\n\n<code_one>: `importlib.import_module(datasets.load.prepare_module(os.path.join(\"metrics\", metric_name))[0])`\n\n<code_two>: `importlib.import_module(datasets.load.metric_module_factory(os.path.join(\"metrics\", metric_name)).module_path)`\n\nFix_pattern: In the condition of a test case class, if the import statement using `datasets.load.prepare_module` is detected, then change it to `datasets.load.metric_module_factory` to fix the API misuse."}
{"number": 4140, "change": "class ModelTesterMixin:\nmodel.to(torch_device)\nmodel.eval()\nwith torch.no_grad():\n-                outputs = model(**inputs_dict)\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\nattentions = outputs[-1]\nself.assertEqual(model.config.output_hidden_states, False)\nself.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n", "fix_pattern": "<condition>: The code is inside a test method or a test case.\n<pattern>: The original code is making a direct API call to the model.\n<code_one>: The direct API call to the model - `outputs = model(**inputs_dict)`\n<code_two>: Code that handles the API call differently - `outputs = model(**self._prepare_for_class(inputs_dict, model_class))`\nFix_pattern: In the condition of being inside a test method or a test case, if a direct API call to the model is detected, then change the direct API call to a modified version in order to fix the API misuse."}
{"number": 4189, "change": "class Embed(base.AbstractModule):\nregularizer=self._regularizers.get(self.EMBEDDINGS, None),\ntrainable=self._trainable)\n\n+    # On the backwards pass, we want to convert the gradient from\n+    # indexed-slices to a regular tensor before sending it back to the\n+    # parameter server. This avoids excess computation on the parameter server.\n+\n+    embeddings = util.convert_gradient_to_tensor(self._embeddings)\n+\n# Lookup embeddings\n-    return tf.nn.embedding_lookup(\n-        self._embeddings, ids, name=\"embedding_lookup\")\n+    return tf.nn.embedding_lookup(embeddings, ids, name=\"embedding_lookup\")\n\n@property\ndef vocab_size(self):\n", "fix_pattern": "<condition>: The code is inside a class method.\n\n<pattern>: The method `tf.nn.embedding_lookup` is called.\n\n<code_one>: `tf.nn.embedding_lookup(self._embeddings, ids, name=\"embedding_lookup\")`\n\n<code_two>: `embeddings = util.convert_gradient_to_tensor(self._embeddings)` followed by `tf.nn.embedding_lookup(embeddings, ids, name=\"embedding_lookup\")`\n\nFix_pattern: In the condition of a class method, if the method `tf.nn.embedding_lookup` is called, then replace the code `tf.nn.embedding_lookup(self._embeddings, ids, name=\"embedding_lookup\")` with `embeddings = util.convert_gradient_to_tensor(self._embeddings)` followed by `tf.nn.embedding_lookup(embeddings, ids, name=\"embedding_lookup\")` to fix the API misuse."}
{"number": 4192, "change": "def test_hook(history):\ntf.summary.scalar('c1', c1)\nsummary_op = tf.summary.merge_all()\n\n-        hook = wandb_tensorflow.WandbHook(summary_op)\n+        hook = wandb_tensorflow.WandbHook(summary_op, history=history)\nwith tf.train.MonitoredTrainingSession(hooks=[hook]) as sess:\nsummary, acc = sess.run([summary_op, c1])\n\nassert wandb_tensorflow.tf_summary_to_dict(summary) == {'c1': 42.0}\n+    print(history.rows)\n+    # TODO(adrian): there is still some kind of bug here where the history\n+    # is being shared with another test that manages to add rows before this one.\nassert history.rows[0]['c1'] == 42.0\n", "fix_pattern": "<condition>: \nThere is a need to fix API misuse in the code.\n\n<pattern>: \nThe pattern is the incorrect usage of the `WandbHook` class.\n\n<code_one>: \nThe code that needs to be removed is the instantiation of `WandbHook` without the `history` argument.\n\n<code_two>:\nThe code that needs to be added is the instantiation of `WandbHook` with the `history` argument.\n\nFix_pattern: \nIn the condition of API misuse, if the incorrect usage of `WandbHook` is detected, then remove the instantiation of `WandbHook` without the `history` argument and add the instantiation of `WandbHook` with the `history` argument to fix the API misuse."}
{"number": 4214, "change": "class LightweightConvolution2D(nn.Module):\n# convolution along frequency axis\nweight_f = F.softmax(self.weight_f, dim=-1)\nweight_f = F.dropout(weight_f, self.dropout_rate, training=self.training)\n-        weight_new = torch.zeros(B * T, 1, self.kernel_size, device=x.device).copy_(weight_f)\n+        weight_new = torch.zeros(B * T, 1, self.kernel_size, device=x.device, dtype=x.dtype).copy_(weight_f)\nxf = F.conv1d(x.view(1, B * T, C), weight_new, padding=self.padding_size, groups=B * T).view(B, T, C)\n\n# lightconv\n", "fix_pattern": "<condition>: The fix pattern applies when there is a need to assign a new value to a variable.\n\n<pattern>: The pattern is to modify the variable assignment by removing or adding code.\n\n<code_one>: The code that needs to be removed or changed.\n\n<code_two>: The code that needs to be added or modified.\n\nFix_pattern: In the condition of assigning a new value to a variable, if a specific pattern is detected, then remove or modify the code to fix the API misuse."}
{"number": 4221, "change": "class ViltEmbeddings(nn.Module):\nx = x.flatten(2).transpose(1, 2)\n# Set `device` here, otherwise `patch_index` will always be on `CPU` and will fail near the end for torch>=1.13\npatch_index = torch.stack(\n-            torch.meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1\n+            meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1\n).to(device=x_mask.device)\npatch_index = patch_index[None, None, :, :, :]\npatch_index = patch_index.expand(x_mask.shape[0], x_mask.shape[1], -1, -1, -1)\n", "fix_pattern": "<condition>: In the condition of the code snippet provided, there is no pre-condition needed.\n\n<pattern>: The pattern detected is the usage of the method \"torch.meshgrid\" to create a meshgrid of indices.\n\n<code_one>: The code snippet \"torch.meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1\" is removed.\n\n<code_two>: The code snippet \"meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1\" is added.\n\nFix_pattern: In the condition of no pre-condition, if the usage of \"torch.meshgrid\" is detected, then remove the code snippet \"torch.meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1\" and add the code snippet \"meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1\" to fix the API misuse."}
{"number": 4247, "change": "def test_multinomial(\n):\nprob_dtype, batch_size, population_size, num_samples, replace, probs = everything\n# tensorflow does not support multinomial without replacement\n-    if backend_fw == \"tensorflow\":\n-        assume(replace is True)\n+    if backend_fw == ivy.functional.backends.tensorflow:\n+        assume(replace)\n\ndef call():\nreturn helpers.test_function(\n", "fix_pattern": "<condition>:\nThe condition is that the backend framework is set to \"tensorflow\".\n\n<pattern>:\nThe pattern is that there is an assumption made about the value of the variable \"replace\". In this case, the assumption is that \"replace\" is True.\n\n<code_one>:\nThe code that is removed is the check for the value of the variable \"replace\" being True.\n\n<code_two>:\nThe code that is added is the check for the value of the variable \"replace\" being True, using the ivy.functional.backends.tensorflow module.\n\nFix_pattern:\nIn the condition of the backend framework being set to \"tensorflow\", if the assumption is made that \"replace\" is True, then remove the check for the value of \"replace\" being True and add a check for the value of \"replace\" being True using the ivy.functional.backends.tensorflow module to fix the API misuse."}
{"number": 4249, "change": "class MonotoneConvexTest(tf.test.TestCase):\ntest_time = tf.constant([1.1, 2.7], dtype=dtype)\ninterpolated, _ = monotone_convex.interpolate(test_time, interval_values,\ninterval_times)\n-    gradient_1y = self.evaluate(tf.gradients(interpolated[0], knot_1y)[0])\n-    gradient_zero = self.evaluate(tf.gradients(interpolated[1], knot_1y)[0])\n+    gradient_1y = self.evaluate(tf.convert_to_tensor(\n+        tf.gradients(interpolated[0], knot_1y)[0]))\n+    gradient_zero = self.evaluate(tf.convert_to_tensor(\n+        tf.gradients(interpolated[1], knot_1y)[0]))\n+\nself.assertAlmostEqual(gradient_1y[0], 0.42)\nself.assertAlmostEqual(gradient_zero[0], 0.0)\n", "fix_pattern": "<condition>: When using the `tf.gradients` function to compute gradients.\n<pattern>: The `self.evaluate` function is used to evaluate the gradients.\n<code_one>: `self.evaluate(tf.gradients(interpolated[0], knot_1y)[0])`\n<code_two>: `self.evaluate(tf.convert_to_tensor(tf.gradients(interpolated[0], knot_1y)[0]))`\nFix_pattern: In the condition of using `tf.gradients`, if the `self.evaluate` function is used to evaluate the gradients, then replace it with `tf.convert_to_tensor(tf.gradients(...))` to fix the API misuse."}
{"number": 4258, "change": "def train(args):\nrnn = RNNLM(args.n_vocab, args.layer, args.unit, args.type, args.dropout_rate)\nmodel = ClassifierWithState(rnn)\nif args.ngpu > 0:\n-        model = torch.nn.DataParallel(model).cuda()\n+        model = torch.nn.DataParallel(model, device_ids=list(range(args.ngpu))).cuda()\nsetattr(model, \"reporter\", model.module.reporter)\ngpu_id = 0\nelse:\n", "fix_pattern": "<condition>: If the number of GPUs (args.ngpu) is greater than 0.\n<pattern>: Using torch.nn.DataParallel to parallelize the model on multiple GPUs.\n<code_one>: model = torch.nn.DataParallel(model).cuda()\n<code_two>: model = torch.nn.DataParallel(model, device_ids=list(range(args.ngpu))).cuda()\nFix_pattern: In the condition of args.ngpu > 0, if torch.nn.DataParallel(model) is detected, then change model = torch.nn.DataParallel(model).cuda() to model = torch.nn.DataParallel(model, device_ids=list(range(args.ngpu))).cuda() to fix the API misuse."}
{"number": 4269, "change": "class PositionalEncoding(nn.Module):\npe[:, 0::2] = torch.sin(position * div_term)\npe[:, 1::2] = torch.cos(position * div_term)\npe = pe.unsqueeze(0).transpose(0, 1)\n-        self.register_buffer('pe', pe)\n+        self.pe = nn.Parameter(pe, requires_grad=False)\n\ndef forward(self, x):\nx = x + self.pe[:x.size(0), :]\n", "fix_pattern": "<condition>: When initializing a buffer in a class\n<pattern>: The code is using self.register_buffer('pe', pe) to initialize the buffer\n<code_one>: self.register_buffer('pe', pe)\n<code_two>: self.pe = nn.Parameter(pe, requires_grad=False)\nFix_pattern: In the condition of initializing a buffer in a class, if the code self.register_buffer('pe', pe) is detected, then remove self.register_buffer('pe', pe) and add self.pe = nn.Parameter(pe, requires_grad=False) to fix the API misuse."}
{"number": 4276, "change": "class CTC(torch.nn.Module):\n# expected shape of seqLength x batchSize x alphabet_size\ndtype = ys_hat.dtype\nys_hat = ys_hat.transpose(0, 1)\n-        if self.ctc_type == \"warpctc\":\n+        if self.ctc_type == \"warpctc\" or dtype == torch.float16:\n# warpctc only supports float32\nys_hat = ys_hat.to(dtype=torch.float32)\nelse:\n", "fix_pattern": "<condition>:\nThe condition is when the API being used is not compatible with a certain data type.\n\n<pattern>:\nThe pattern is that the code checks for a specific API condition.\n\n<code_one>:\nThe code being removed checks for a specific API condition.\n\n<code_two>:\nThe code being added includes the condition being checked and an additional condition.\n\nFix_pattern:\nIn the condition of checking the API compatibility, if a specific API condition is detected, then remove the code checking that condition and add the code that also checks for an additional condition to fix the API misuse."}
{"number": 4282, "change": "class ExpectedRiskMinimization(DecoderTrainer[Callable[[StateType], torch.Tensor\nstate.score,\nstate.action_history):\nif self._normalize_by_length:\n-                    path_length = nn_util.new_variable_with_data(model_score,\n-                                                                 torch.Tensor([len(history)]))\n+                    path_length = Variable(model_score.data.new([len(history)]))\nmodel_score = model_score / path_length\nbatch_scores[batch_index].append(model_score)\nreturn batch_scores\n", "fix_pattern": "<condition>: The condition is when the variable \"path_length\" is being defined and initialized in the code.\n<pattern>: The pattern is that the variable \"path_length\" is being defined and initialized using the \"nn_util.new_variable_with_data\" function with the input arguments \"model_score\" and \"torch.Tensor([len(history)])\".\n<code_one>: The code being removed is the line that defines and initializes \"path_length\" using \"nn_util.new_variable_with_data\".\n<code_two>: The code being added is the line that redefines and initializes \"path_length\" using \"Variable(model_score.data.new([len(history)]))\".\nFix_pattern: In the condition of defining and initializing \"path_length\", if the pattern of using \"nn_util.new_variable_with_data(model_score, torch.Tensor([len(history)]))\" is detected, then replace it with \"Variable(model_score.data.new([len(history)]))\" to fix the API misuse."}
{"number": 4290, "change": "def test_load_no_dev_data_explicit(tasks_base_path):\n\ndef test_multi_corpus(tasks_base_path):\n\n-    corpus_1 = flair.datasets.NER_GERMAN_GERMEVAL(tasks_base_path)\n+    corpus_1 = flair.datasets.ColumnCorpus(tasks_base_path  / \"germeval_14\", column_format={0: \"text\", 2: \"ner\"})\n\ncorpus_2 = flair.datasets.ColumnCorpus(tasks_base_path / \"fashion\", column_format={0: \"text\", 2: \"ner\"})\n# get two corpora as one\n", "fix_pattern": "<condition>: \nNo clear condition is needed.\n\n<pattern>: \nReplacing one corpus with another corpus.\n\n<code_one>: \ncorpus_1 = flair.datasets.NER_GERMAN_GERMEVAL(tasks_base_path)\n\n<code_two>: \ncorpus_1 = flair.datasets.ColumnCorpus(tasks_base_path / \"germeval_14\", column_format={0: \"text\", 2: \"ner\"})\n\nFix_pattern: \nIn the condition of no clear condition, if a pattern of replacing one corpus with another corpus is detected, then remove the code corpus_1 = flair.datasets.NER_GERMAN_GERMEVAL(tasks_base_path) and add the code corpus_1 = flair.datasets.ColumnCorpus(tasks_base_path / \"germeval_14\", column_format={0: \"text\", 2: \"ner\"}) to fix the API misuse."}
{"number": 4301, "change": "def test_forward(use_token_averaged_energy, reduction_factor):\nes, elens = layer(xs, torch.LongTensor([384, 128]))\nassert es.shape[1] == max(elens)\nelse:\n-        ds = torch.LongTensor([[3, 3, 1], [3, 0, 0]])\n+        ds = torch.LongTensor([[3, 3, 1], [3, 0, 0]]) // reduction_factor\ndlens = torch.LongTensor([3, 1])\nes, _ = layer(\nxs, torch.LongTensor([384, 128]), durations=ds, durations_lengths=dlens\n", "fix_pattern": "<condition>: The condition is not clearly specified in the provided code section. No pre-condition is needed.\n\n<pattern>: The <pattern> is the need to add the \"reduction_factor\" to the initialization of the \"ds\" variable.\n\n<code_one>: The \"ds\" variable is initialized as \"torch.LongTensor([[3, 3, 1], [3, 0, 0]])\".\n\n<code_two>: The \"ds\" variable is modified to \"torch.LongTensor([[3, 3, 1], [3, 0, 0]]) // reduction_factor\".\n\nFix_pattern: In the condition of this code section, if the pattern of not including the \"reduction_factor\" in the initialization of the \"ds\" variable is detected, then add the \"// reduction_factor\" to fix the API misuse."}
{"number": 4310, "change": "class Loggers():\nif self.wandb:\nself.wandb.log({\"Labels\": [wandb.Image(str(x), caption=x.name) for x in paths]})\n\n-    def on_train_batch_end(self, ni, model, imgs, targets, paths, plots):\n+    def on_train_batch_end(self, ni, model, imgs, targets, paths, plots, sync_bn):\n# Callback runs on train batch end\nif plots:\nif ni == 0:\n-                with warnings.catch_warnings():\n-                    warnings.simplefilter('ignore')  # suppress jit trace warning\n-                    self.tb.add_graph(torch.jit.trace(de_parallel(model), imgs[0:1], strict=False), [])\n+                if not sync_bn:  # tb.add_graph() --sync known issue https://github.com/ultralytics/yolov5/issues/3754\n+                    with warnings.catch_warnings():\n+                        warnings.simplefilter('ignore')  # suppress jit trace warning\n+                        self.tb.add_graph(torch.jit.trace(de_parallel(model), imgs[0:1], strict=False), [])\nif ni < 3:\nf = self.save_dir / f'train_batch{ni}.jpg'  # filename\nThread(target=plot_images, args=(imgs, targets, paths, f), daemon=True).start()\n", "fix_pattern": "<condition>: The condition is checking if `sync_bn` is `False`.\n\n<pattern>: The pattern is the removal of the line `self.tb.add_graph(torch.jit.trace(de_parallel(model), imgs[0:1], strict=False), [])`.\n\n<code_one>: The code being removed is `self.tb.add_graph(torch.jit.trace(de_parallel(model), imgs[0:1], strict=False), [])`.\n\n<code_two>: The code being added is a conditional check before adding the graph, `if not sync_bn`.\n\nFix_pattern: \n\nIn the condition of `sync_bn` being `False`, if the line `self.tb.add_graph(torch.jit.trace(de_parallel(model), imgs[0:1], strict=False), [])` is detected, then remove the line and add the conditional check `if not sync_bn` before adding the graph. This fix is applied to fix the API misuse."}
{"number": 4311, "change": "class Model:\nenc = tf.layers.max_pooling1d(enc, 2, 1, padding=\"same\")  # (N, T, K * E / 2)\n\n### Conv1D projections\n-            enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_1\")  # (N, T, E/2)\n-            enc = normalize(enc, type=hp.norm_type, is_training=self.is_training, activation_fn=tf.nn.relu)\n-            enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_2\")  # (N, T, E/2)\n+            enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_1\", activation_fn=tf.nn.relu)  # (N, T, E/2)\n+            enc = normalize(enc, type=hp.norm_type, is_training=self.is_training)\n+            enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_2\", activation_fn=None)  # (N, T, E/2)\nenc += prenet_out  # (N, T, E/2) # residual connections\n\n### Highway Nets\n", "fix_pattern": "<condition>: The condition is not clearly stated in the context.\n\n<pattern>: The pattern is that the code in the removed section is modifying the 'enc' variable by applying two convolutional layers and normalization.\n\n<code_one>: The code_one is 'enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_1\")  # (N, T, E/2) enc = normalize(enc, type=hp.norm_type, is_training=self.is_training, activation_fn=tf.nn.relu) enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_2\")  # (N, T, E/2)'.\n\n<code_two>: The code_two is 'enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_1\", activation_fn=tf.nn.relu)  # (N, T, E/2) enc = normalize(enc, type=hp.norm_type, is_training=self.is_training) enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_2\", activation_fn=None)  # (N, T, E/2)'.\n\nFix_pattern: In the condition of unidentified context, if the pattern of modifying 'enc' variable through convolutional layers and normalization is detected, then the code_one should be removed and the code_two should be added to fix the API misuse."}
{"number": 4314, "change": "def build_targets(p, targets, model):\ntcls, tbox, indices, av = [], [], [], []\nreject, use_all_anchors = True, True\ngain = torch.ones(6, device=targets.device)  # normalized to gridspace gain\n+\n+    # m = list(model.modules())[-1]\n+    # for i in range(m.nl):\n+    #     anchor_vec = m.anchor_vec[i]\nmulti_gpu = type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel)\nfor i, j in enumerate(model.yolo_layers):\n# get number of grid points and anchor vec for this yolo layer\nanchor_vec = model.module.module_list[j].anchor_vec if multi_gpu else model.module_list[j].anchor_vec\n\n# iou of targets-anchors\n-        gain[2:] = torch.tensor(p[i].shape)[[2, 3, 2, 3]]  # xyxy gain\n+        gain[2:] = torch.tensor(p[i].shape)[[3, 2, 3, 2]]  # xyxy gain\nt, a = targets * gain, []\ngwh = t[:, 4:6]\nif nt:\n", "fix_pattern": "<condition>: There is a need to update the gain tensor with dimensions [2:] in the code.\n<pattern>: The gain tensor is being updated by using the dimensions of the input tensor p[i] in the XYXY order.\n<code_one>: gain[2:] = torch.tensor(p[i].shape)[[2, 3, 2, 3]]  # xyxy gain\n<code_two>: gain[2:] = torch.tensor(p[i].shape)[[3, 2, 3, 2]]  # xyxy gain\nFix_pattern: In the condition of updating the gain tensor, if the pattern of updating the gain tensor with dimensions in the XYXY order is detected, then change the code_one to code_two to fix the API misuse."}
{"number": 4318, "change": "def linear_resample(x, num_samples, axis=-1):\nnum_x_dims = len(x_shape)\naxis = axis % num_x_dims\nnum_vals = x.shape[axis]\n-    if x.dtype not in ['float16','float32','float64']:\n-        x=tf.cast(x,tf.float32)\n+    if x.dtype not in [\"float16\", \"float32\", \"float64\"]:\n+        x = tf.cast(x, tf.float32)\nxp = tf.range(num_vals, dtype=tf.float32)\nx_coords = tf.range(num_samples, dtype=tf.float32) * (\n-                (num_vals - 1) / (num_samples - 1)\n+            (num_vals - 1) / (num_samples - 1)\n)\nelse:\nxp = tf.range(num_vals, dtype=x.dtype)\n", "fix_pattern": "<condition>: The condition is that the data type of variable 'x' should not be one of ['float16', 'float32', 'float64'].\n<pattern>: The pattern is to cast the data type of 'x' to 'tf.float32' if it does not match the specified data types.\n<code_one>: The code that needs to be removed is the following:\n    if x.dtype not in ['float16','float32','float64']:\n        x=tf.cast(x,tf.float32)\n                (num_vals - 1) / (num_samples - 1)\n<code_two>: The code that needs to be added is the following:\n    if x.dtype not in [\"float16\", \"float32\", \"float64\"]:\n        x = tf.cast(x, tf.float32)\n            (num_vals - 1) / (num_samples - 1)\nFix_pattern: In the condition of the specified data type not being ['float16', 'float32', 'float64'], the code is cast to 'tf.float32' to fix the API misuse."}
{"number": 4340, "change": "def split(data, batch):\nif data.x is not None:\nslices['x'] = node_slice\nelse:\n-        data.num_nodes = torch.bincount(batch).tolist()\n-        slices['num_nodes'] = torch.arange(len(data.num_nodes) + 1)\n+        # Imitate `collate` functionality:\n+        data._num_nodes = torch.bincount(batch).tolist()\n+        data.num_nodes = batch.numel()\nif data.edge_attr is not None:\nslices['edge_attr'] = edge_slice\nif data.y is not None:\n", "fix_pattern": "<condition>: Checking if `data.y` is not None.\n<pattern>: Changing the attribute `data.num_nodes` assignment.\n<code_one>: Assigning `data.num_nodes` using `torch.bincount(batch).tolist()` and creating slices using `torch.arange(len(data.num_nodes) + 1)`.\n<code_two>: Assigning `_num_nodes` using `torch.bincount(batch).tolist()` and assigning `data.num_nodes` using `batch.numel()`.\nFix_pattern: In the condition of checking if `data.y` is not None, if the pattern of assigning `data.num_nodes` using `torch.bincount(batch).tolist()` and creating slices using `torch.arange(len(data.num_nodes) + 1)` is detected, then change the assignment of `data.num_nodes` to `_num_nodes` using `torch.bincount(batch).tolist()` and assign `data.num_nodes` using `batch.numel()` to fix the API misuse."}
{"number": 4350, "change": "class CTRLModel(CTRLPreTrainedModel):\ninputs_embeds = self.w(input_ids)\n# inputs_embeds = embedded.unsqueeze(0) if len(input_ids.shape)<2 else embedded\nseq_len = input_ids.shape[-1]\n-        mask = torch.triu(torch.ones(seq_len, seq_len), 1).to(inputs_embeds.device)\n+        mask = torch.triu(torch.ones(seq_len + past_length, seq_len + past_length), 1).to(inputs_embeds.device)\n\ninputs_embeds *= np.sqrt(self.d_model_size)\n", "fix_pattern": "<condition>: The code is using the CTRLModel class.\n\n<pattern>: The code is creating a mask tensor.\n\n<code_one>: The code is creating the mask tensor using `torch.triu` with the size of `seq_len`.\n\n<code_two>: The code is creating the mask tensor using `torch.triu` with the size of `seq_len + past_length`.\n\nFix_pattern: In the condition of using the CTRLModel class, if creating the mask tensor with size `seq_len` is detected, then change the code to create the mask tensor with size `seq_len + past_length` to fix the API misuse."}
{"number": 4370, "change": "def test_is_small_dataset(\ndataset_size, config_max_in_memory_dataset_size, env_max_in_memory_dataset_size, monkeypatch\n):\nif config_max_in_memory_dataset_size != \"default\":\n-        monkeypatch.setattr(\n-            datasets.config, \"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", config_max_in_memory_dataset_size\n-        )\n+        monkeypatch.setattr(datasets.config, \"IN_MEMORY_MAX_SIZE\", config_max_in_memory_dataset_size)\n\n-    max_in_memory_dataset_size = datasets.config.HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\n+    max_in_memory_dataset_size = datasets.config.IN_MEMORY_MAX_SIZE\nif config_max_in_memory_dataset_size == \"default\":\nif env_max_in_memory_dataset_size:\nassert max_in_memory_dataset_size == env_max_in_memory_dataset_size\n", "fix_pattern": "<condition>: The condition is that the variable \"config_max_in_memory_dataset_size\" should not be equal to \"default\".\n\n<pattern>: The pattern is that the API misuse is fixed by changing the code that sets the value of \"max_in_memory_dataset_size\".\n\n<code_one>: The code that is removed is the following:\n```\n        monkeypatch.setattr(\n            datasets.config, \"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", config_max_in_memory_dataset_size\n        )\n    max_in_memory_dataset_size = datasets.config.HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\n```\n\n<code_two>: The code that is added is the following:\n```\n        monkeypatch.setattr(datasets.config, \"IN_MEMORY_MAX_SIZE\", config_max_in_memory_dataset_size)\n    max_in_memory_dataset_size = datasets.config.IN_MEMORY_MAX_SIZE\n```\n\nFix_pattern: In the condition of not equal to \"default\", if the API misuse of setting \"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\" is detected, then change the code that sets the value to \"IN_MEMORY_MAX_SIZE\" to fix the API misuse."}
{"number": 4378, "change": "def _compute_equalized_tiles(interp_tiles: torch.Tensor, luts: torch.Tensor) ->\ntiles_equalized: torch.Tensor = torch.zeros_like(interp_tiles, dtype=torch.long)\n\n# compute the interpolation weights (shapes are 2 x TH x TW because they must be applied to 2 interp tiles)\n-    ih = torch.arange(2 * th - 1, -1, -1, device=interp_tiles.device).div(2. * th - 1)[None].T.expand(2 * th, tw)\n+    ih = torch.arange(2 * th - 1, -1, -1, device=interp_tiles.device).div(\n+        2. * th - 1)[None].transpose(-2, -1).expand(2 * th, tw)\nih = ih.unfold(0, th, th).unfold(1, tw, tw)  # 2 x 1 x TH x TW\niw = torch.arange(2 * tw - 1, -1, -1, device=interp_tiles.device).div(2. * tw - 1).expand(th, 2 * tw)\niw = iw.unfold(0, th, th).unfold(1, tw, tw)  # 1 x 2 x TH x TW\n", "fix_pattern": "<condition>: The code is computing interpolation weights for two dimensional tiles.\n\n<pattern>: The pattern in the code is the calculation of 'ih' tensor.\n\n<code_one>: The code is using torch.arange and the expression '2 * th - 1' to calculate the 'ih' tensor.\n\n<code_two>: The code is using torch.arange, '2 * th - 1' expression, and the transpose function to calculate the 'ih' tensor.\n\nFix_pattern: in the condition of computing interpolation weights for two dimensional tiles, if the 'ih' tensor is calculated using torch.arange and the expression '2 * th - 1', then change the expression 'ih = torch.arange(2 * th - 1, -1, -1, device=interp_tiles.device).div(2. * th - 1)[None].T.expand(2 * th, tw)' to 'ih = torch.arange(2 * th - 1, -1, -1, device=interp_tiles.device).div(2. * th - 1)[None].transpose(-2, -1).expand(2 * th, tw)' to fix the API misuse."}
{"number": 4381, "change": "class DeepSpeedEngine(Module):\nif self.zero_optimization_partition_weights() and any(\n[hasattr(param,\n'ds_id') for param in self.module.parameters()]):\n-                assert all([param.dtype == torch.half for param in self.module.parameters()]), f\"Model must initialized in fp16 mode for ZeRO Stage 3.\"\n+                assert all([param.dtype == torch.half for param in self.module.parameters()]), \"fp16 is enabled but one or several model parameters have dtype that is not fp16\"\nself.module.half()\nelse:\n-            assert all([param.dtype == torch.float for param in self.module.parameters()]), f\"fp16 is not enabled but one or several model parameters have dtype of fp16\"\n+            assert all([param.dtype == torch.float for param in self.module.parameters()]), \"fp16 is not enabled but one or several model parameters have dtype of fp16\"\n\nif not self.dont_change_device:\nself.module.to(self.device)\n", "fix_pattern": "<condition>: The condition is if `self.zero_optimization_partition_weights()` is true and any parameter in `self.module.parameters()` has the attribute `ds_id`.\n<pattern>: The pattern is the misuse of the API, where the check for `param.dtype == torch.half` is incorrect.\n<code_one>: The code that is removed is the assert statement that checks if all parameters have a dtype of `torch.half`.\n<code_two>: The code that is added is the corrected assert statement that checks if all parameters have a dtype of `torch.half`.\nFix_pattern: In the condition of `self.zero_optimization_partition_weights()` and any parameter in `self.module.parameters()` that has the attribute `ds_id`, if the incorrect pattern of checking `param.dtype == torch.half` is detected, then remove the incorrect assert statement and add the corrected assert statement to fix the API misuse."}
{"number": 4397, "change": "class TrainerTrainLoopMixin(ABC):\nif self.reload_dataloaders_every_epoch:\nself.reset_train_dataloader(self.get_model())\n\n+        # track local dataloader so TPU can wrap each epoch\n+        train_dataloader = self.train_dataloader\n+\n# on TPU we have to wrap it under the ParallelLoader\nif self.use_tpu:\ndevice = xm.xla_device()\n-            self.train_dataloader = xla_pl.ParallelLoader(self.train_dataloader, [device])\n-            self.train_dataloader = self.train_dataloader.per_device_loader(device)\n+            train_dataloader = xla_pl.ParallelLoader(train_dataloader, [device])\n+            train_dataloader = train_dataloader.per_device_loader(device)\n\n# run epoch\nfor batch_idx, batch in self.profiler.profile_iterable(\n-            enumerate(self.train_dataloader), \"get_train_batch\"\n+            enumerate(train_dataloader), \"get_train_batch\"\n):\n# stop epoch if we limited the number of training batches\nif batch_idx >= self.num_training_batches:\n", "fix_pattern": "<condition>: When the variable \"self.reload_dataloaders_every_epoch\" is true.\n<pattern>: The code block that sets up the parallel loader and per device loader for the train dataloader is removed.\n<code_one>: The code block that sets up the parallel loader and per device loader for the train dataloader.\n<code_two>: The code block that sets up the parallel loader and per device loader for a local train dataloader.\nFix_pattern: In the condition of \"self.reload_dataloaders_every_epoch\" being true, if the code block that sets up the parallel loader and per device loader for the train dataloader is detected, then change it to the code block that sets up the parallel loader and per device loader for a local train dataloader to fix the API misuse."}
{"number": 4407, "change": "class LKJCorrCholesky(TorchDistribution):\nsuper().__init__(torch.Size(), torch.Size((d, d)), validate_args=validate_args)\n\ndef sample(self, sample_shape=torch.Size()):\n-        y = self._gen.sample(sample_shape=self.batch_shape + sample_shape).detach()\n+        with torch.no_grad():\n+            y = self._gen.sample(sample_shape=sample_shape + self.batch_shape)\nz = y.mul(2).add(-1.0)\nreturn _vector_to_l_cholesky(z)\n", "fix_pattern": "<condition>: The condition is not clearly specified in the given context.\n\n<pattern>: There is no specific pattern mentioned in the given context.\n\n<code_one>: The code that is removed is `y = self._gen.sample(sample_shape=self.batch_shape + sample_shape).detach()`.\n\n<code_two>: The code that is added is `with torch.no_grad(): y = self._gen.sample(sample_shape=sample_shape + self.batch_shape)`.\n\nFix_pattern: In the context, there was an API misuse where the code was using `detach()` function to detach the gradient computation before performing an operation. To fix this, the code change involved removing the `.detach()` part and adding a `with torch.no_grad()` block to disable gradient computation."}
{"number": 4429, "change": "def fit_circle_in_2d(\nn_provided = points2d.shape[0]\nif n_provided < 3:\nraise ValueError(f\"{n_provided} points are not enough to determine a circle\")\n-    solution = lstsq(design, rhs)\n-    center = solution[:2] / 2\n-    radius = torch.sqrt(solution[2] + (center ** 2).sum())\n+    solution = lstsq(design, rhs[:, None])\n+    center = solution[:2, 0] / 2\n+    radius = torch.sqrt(solution[2, 0] + (center ** 2).sum())\nif n_points > 0:\nif angles is not None:\nwarnings.warn(\"n_points ignored because angles provided\")\n", "fix_pattern": "<condition>: `n_provided < 3`\n<pattern>: `solution[:2] / 2` should be changed to `solution[:2, 0] / 2` and `solution[2]` should be changed to `solution[2, 0]`\n<code_one>: `solution[:2] / 2`\n<code_two>: `solution[:2, 0] / 2` and `solution[2]` to `solution[2, 0]`\nFix_pattern: In the condition of `n_provided < 3`, if the pattern `solution[:2] / 2` is detected, then change the `solution[:2] / 2` to `solution[:2, 0] / 2` and change `solution[2]` to `solution[2, 0]` to fix the API misuse."}
{"number": 4450, "change": "def generate_audio_mask_noise(audio_values, audio_mask=None, mask_ratio=0.75, ma\nif mask_type == \"frame-level\":\nnum_time_patches = seq_len // freq_len\nnoise = (\n-            torch.rand(batch_size, num_time_patches).unsqueeze(-1).repeat(1, 1, freq_len).view(batch_size, seq_len)\n+            torch.rand(batch_size, num_time_patches, device=audio_values.device)\n+            .unsqueeze(-1)\n+            .repeat(1, 1, freq_len)\n+            .view(batch_size, seq_len)\n)  # noise in [0, 1]\nelif mask_type == \"patch-level\":\n-        noise = torch.rand(batch_size, seq_len)  # noise in [0, 1]\n+        noise = torch.rand(batch_size, seq_len, device=audio_values.device)  # noise in [0, 1]\nlen_keep = int(seq_len * (1 - mask_ratio))\nreturn noise, len_keep\n", "fix_pattern": "<condition>: The condition is when the mask_type is \"frame-level\" or \"patch-level\".\n\n<pattern>: The pattern is that torch.rand() is used to generate noise in both cases.\n\n<code_one>: The original code uses torch.rand(batch_size, num_time_patches).unsqueeze(-1).repeat(1, 1, freq_len).view(batch_size, seq_len) to generate noise when mask_type is \"frame-level\". And it uses torch.rand(batch_size, seq_len) to generate noise when mask_type is \"patch-level\".\n\n<code_two>: The fixed code uses torch.rand(batch_size, num_time_patches, device=audio_values.device).unsqueeze(-1).repeat(1, 1, freq_len).view(batch_size, seq_len) to generate noise when mask_type is \"frame-level\". And it uses torch.rand(batch_size, seq_len, device=audio_values.device) to generate noise when mask_type is \"patch-level\".\n\nFix_pattern: In the condition of mask_type being \"frame-level\" or \"patch-level\", if torch.rand() is used to generate noise, then replace code_one with code_two to fix the API misuse."}
{"number": 4456, "change": "class TFCvtDropPath(tf.keras.layers.Layer):\nreturn x\nkeep_prob = 1 - self.drop_prob\nshape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n-        random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n+        random_tensor = keep_prob + tf.random.uniform(shape, 0, 1, dtype=self.compute_dtype)\nrandom_tensor = tf.floor(random_tensor)\nreturn (x / keep_prob) * random_tensor\n", "fix_pattern": "<condition>: The condition is that there is a need to initialize a random_tensor variable.\n\n<pattern>: The pattern is that the random_tensor variable is being initialized incorrectly.\n\n<code_one>: The original code initializes the random_tensor variable using the expression \"keep_prob + tf.random.uniform(shape, 0, 1)\" without specifying the data type.\n\n<code_two>: The fix is to add the \"dtype=self.compute_dtype\" argument to the tf.random.uniform() function call, specifying the data type.\n\nFix_pattern: In the condition of needing to initialize a random_tensor variable, if the pattern of incorrect initialization is detected, then change the code from \"keep_prob + tf.random.uniform(shape, 0, 1)\" to \"keep_prob + tf.random.uniform(shape, 0, 1, dtype=self.compute_dtype)\" to fix the API misuse."}
{"number": 4467, "change": "class CLIPSegTextTransformer(nn.Module):\n# take features from the eot embedding (eot_token is the highest number in each sequence)\n# casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14\npooled_output = last_hidden_state[\n-            torch.arange(last_hidden_state.shape[0], device=input_ids.device), input_ids.to(torch.int).argmax(dim=-1)\n+            torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),\n+            input_ids.to(dtype=torch.int, device=last_hidden_state.device).argmax(dim=-1),\n]\n\nif not return_dict:\n", "fix_pattern": "<condition>: The condition where the variable \"return_dict\" is False.\n<pattern>: The pattern is to modify the input sequence by changing the data type and applying the \"argmax\" operation.\n<code_one>: The code that is removed is \"torch.arange(last_hidden_state.shape[0], device=input_ids.device), input_ids.to(torch.int).argmax(dim=-1)\".\n<code_two>: The code that is added is \"torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device), input_ids.to(dtype=torch.int, device=last_hidden_state.device).argmax(dim=-1)\".\nFix_pattern: In the condition where \"return_dict\" is False, if the pattern of modifying the input sequence by changing the data type and applying the \"argmax\" operation is detected, then remove the code \"torch.arange(last_hidden_state.shape[0], device=input_ids.device), input_ids.to(torch.int).argmax(dim=-1)\" and add the code \"torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device), input_ids.to(dtype=torch.int, device=last_hidden_state.device).argmax(dim=-1)\" to fix the API misuse."}
{"number": 4476, "change": "def tversky_loss(input: torch.Tensor, target: torch.Tensor,\n# compute the actual dice score\ndims = (1, 2, 3)\nintersection = torch.sum(input_soft * target_one_hot, dims)\n-    fps = torch.sum(input_soft * (1. - target_one_hot), dims)\n-    fns = torch.sum((1. - input_soft) * target_one_hot, dims)\n+    fps = torch.sum(input_soft * (-target_one_hot + 1.), dims)\n+    fns = torch.sum((-input_soft + 1.) * target_one_hot, dims)\n\nnumerator = intersection\ndenominator = intersection + alpha * fps + beta * fns\ntversky_loss = numerator / (denominator + eps)\n-    return torch.mean(1. - tversky_loss)\n+    return torch.mean(-tversky_loss + 1.)\n\n\nclass TverskyLoss(nn.Module):\n", "fix_pattern": "<condition>: The condition is when calculating the Tversky loss in the given context.\n\n<pattern>: The pattern is detecting the computation of false positives (fps) and false negatives (fns).\n\n<code_one>: The code being removed is the calculation of fps and fns using multiplication and subtraction.\n\n<code_two>: The code being added is the calculation of fps and fns using multiplication and addition.\n\nFix_pattern: In the condition of calculating the Tversky loss, if the pattern of computing fps and fns using multiplication and subtraction is detected, then remove the code_one and replace it with code_two to fix the API misuse."}
{"number": 4498, "change": "class Trainer(object):\nlast_optim_state = state.get(\"last_optimizer_state\", None)\nif last_optim_state == -1:\nmaster_path = re.sub(\"shard[0-9]+\", \"shard0\", filename)\n-                    last_optim_state = torch.load(master_path, map_location='cpu')['last_optimizer_state']\n+                    local_master_path = PathManager.get_local_path(master_path)\n+                    last_optim_state = torch.load(local_master_path, map_location='cpu')['last_optimizer_state']\n\n# If doing zero_sharding, do not broadcast global optimizer\n# state. Later we will broadcast sharded states to each rank\n", "fix_pattern": "<condition>:\nThe condition is that the variable \"last_optim_state\" is equal to -1.\n\n<pattern>:\nThe pattern that is detected is an API misuse. Specifically, the code is attempting to load a file using \"torch.load()\" with the variable \"master_path\".\n\n<code_one>:\nThe code that is removed is:\n\"last_optim_state = torch.load(master_path, map_location='cpu')['last_optimizer_state']\"\n\n<code_two>:\nThe code that is added is:\n\"local_master_path = PathManager.get_local_path(master_path)\nlast_optim_state = torch.load(local_master_path, map_location='cpu')['last_optimizer_state']\"\n\nFix_pattern:\nIn the condition where \"last_optim_state\" equals -1, the API misuse of loading a file using \"torch.load(master_path)\" is fixed by first obtaining the local path of the file using \"PathManager.get_local_path(master_path)\", and then loading the file using \"torch.load(local_master_path)\"."}
{"number": 4507, "change": "def finfo(type: Union[DType, str, tf.Tensor, tf.Variable]) -> Finfo:\nif isinstance(type, tf.Tensor):\ntype = type.dtype\nif ivy.as_native_dtype(type) == tf.bfloat16:\n-        return Finfo(tf.experimental.numpy.finfo(tf.float32))\n+        return Finfo(Bfloat16Finfo())\nreturn Finfo(tf.experimental.numpy.finfo(ivy.as_native_dtype(type)))\n", "fix_pattern": "<condition>: The condition is that the input type should be a tf.Tensor.\n\n<pattern>: The pattern is that if the input type is a tf.Tensor with dtype tf.float32, it should be changed to have dtype tf.bfloat16.\n\n<code_one>: The code that is removed is `return Finfo(tf.experimental.numpy.finfo(tf.float32))`.\n\n<code_two>: The code that is added is `return Finfo(Bfloat16Finfo())`.\n\nFix_pattern: In the condition of the input type being a tf.Tensor, if the input type is tf.float32, the code should be changed to return Finfo with dtype as tf.bfloat16."}
{"number": 4508, "change": "def segment_diff(x,\n\nneeds_fix = tf.scatter_nd(\nfix_indices,\n-        tf.reshape(tf.ones_like(fix_indices, dtype=tf.bool), [-1]),\n+        # Unfortunately, scatter_nd doesn't support bool on GPUs so we need to\n+        # do ints here and then convert to bool.\n+        tf.reshape(tf.ones_like(fix_indices, dtype=tf.int32), [-1]),\nshape=tf.shape(x))\n# If exclusive is False, then needs_fix means we need to replace the values\n# in raw_diffs at those locations with the values in x.\n+    needs_fix = tf.cast(needs_fix, dtype=tf.bool)\nif not exclusive:\nreturn tf.where(needs_fix, x, raw_diffs)\n", "fix_pattern": "<condition>: The condition is if the variable \"exclusive\" is False.\n<pattern>: The pattern is checking for the existence of a variable called \"needs_fix\".\n<code_one>: The code that was removed is tf.reshape(tf.ones_like(fix_indices, dtype=tf.bool), [-1]).\n<code_two>: The code that was added is tf.reshape(tf.ones_like(fix_indices, dtype=tf.int32), [-1]), needs_fix = tf.cast(needs_fix, dtype=tf.bool).\nFix_pattern: In the condition of \"exclusive\" being False, if the variable \"needs_fix\" is detected, then remove the code tf.reshape(tf.ones_like(fix_indices, dtype=tf.bool), [-1]) and add the code tf.reshape(tf.ones_like(fix_indices, dtype=tf.int32), [-1]), needs_fix = tf.cast(needs_fix, dtype=tf.bool) to fix the API misuse."}
{"number": 4524, "change": "class HardNet(nn.Module):\n# training totally unstable.\nreturn (x - mp.detach()) / (sp.detach() + eps)\n\n-    def forward(self, input: torch.Tensor) -> torch.Tensor:   # type: ignore\n+    def forward(self, input: torch.Tensor) -> torch.Tensor:\nx_norm: torch.Tensor = self._normalize_input(input)\nx_features: torch.Tensor = self.features(x_norm)\nx_out = x_features.view(x_features.size(0), -1)\n", "fix_pattern": "<condition>: The code is missing or has incorrect implementation of a specific method or function.\n<pattern>: The forward method is missing or has incorrect definition.\n<code_one>: def forward(self, input: torch.Tensor) -> torch.Tensor:   # type: ignore\n<code_two>: def forward(self, input: torch.Tensor) -> torch.Tensor:\nFix_pattern: In the condition of missing or incorrect implementation of the forward method, remove/add/change the code defining the forward method to fix the API misuse."}
{"number": 4528, "change": "def do_test_log_likelihood(run,\nlayer_key[0])])\nelse:\nexpected_mean_logstd = fc(\n-                        fc(obs_batch,\n-                           vars[\"_hidden_layers.0._model.0.weight\"]),\n-                        vars[\"_logits._model.0.weight\"])\n+                        fc(\n+                            obs_batch,\n+                            np.transpose(\n+                                vars[\"_hidden_layers.0._model.0.weight\"])),\n+                        np.transpose(vars[\"_logits._model.0.weight\"]))\nmean, log_std = np.split(expected_mean_logstd, 2, axis=-1)\nif logp_func is None:\nexpected_logp = np.log(norm.pdf(a, mean, np.exp(log_std)))\n", "fix_pattern": "<condition>: The condition is when the variable \"logp_func\" is None.\n<pattern>: The pattern is that the code is using the \"fc\" function to pass the \"obs_batch\" and the weight variables, but the weight variables are not correctly transposed.\n<code_one>: The code being removed is:\n```\n                        fc(obs_batch,\n                           vars[\"_hidden_layers.0._model.0.weight\"]),\n                        vars[\"_logits._model.0.weight\"])\n```\n<code_two>: The code being added is:\n```\n                        fc(\n                            obs_batch,\n                            np.transpose(\n                                vars[\"_hidden_layers.0._model.0.weight\"])),\n                        np.transpose(vars[\"_logits._model.0.weight\"]))\n```\nFix_pattern: In the condition of \"logp_func is None\", if the pattern of using the \"fc\" function with weight variables is detected, then change the weight variables to their transpose using the \"np.transpose\" function to fix the API misuse."}
{"number": 4555, "change": "class roibatchLoader(data.Dataset):\n# for ratio cross 1, we make it to be 1.\ntarget_ratio = 1\n\n-        self.ratio_list_batch[left_idx:(right_idx+1)] = target_ratio\n+        self.ratio_list_batch[left_idx:(right_idx+1)] = torch.tensor(target_ratio.astype(np.float64)) # trainset ratio list ,each batch is same number\n\n\ndef __getitem__(self, index):\n", "fix_pattern": "<condition>: When assigning a value to a variable in a class method.\n<pattern>: Using a value directly without any type casting or conversion.\n<code_one>: self.ratio_list_batch[left_idx:(right_idx+1)] = target_ratio\n<code_two>: self.ratio_list_batch[left_idx:(right_idx+1)] = torch.tensor(target_ratio.astype(np.float64))\nFix_pattern: In the condition of assigning a value to a variable in a class method, if a value is directly assigned without any type casting or conversion, then change the assignment statement to include type casting or conversion to fix the API misuse."}
{"number": 4561, "change": "class CapsNet(object):\nassert self.masked_v.get_shape() == [cfg.batch_size, 1, 16, 1]\n# Method 2. masking with true label, default mode\nelse:\n-                self.masked_v = tf.matmul(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)), transpose_a=True)\n+                # self.masked_v = tf.matmul(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)), transpose_a=True)\n+                self.masked_v = tf.multply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)))\nself.v_length = tf.sqrt(tf.reduce_sum(tf.square(self.caps2), axis=2, keep_dims=True) + epsilon)\n\n# 2. Reconstructe the MNIST images with 3 FC layers\n", "fix_pattern": "<condition>: The condition is that the shape of `self.masked_v` should be equal to [cfg.batch_size, 1, 16, 1].\n<pattern>: The pattern is that `self.masked_v` is calculated using the `tf.matmul()` function.\n<code_one>: The code that is being removed is `self.masked_v = tf.matmul(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)), transpose_a=True)`.\n<code_two>: The code that is being added is `self.masked_v = tf.multply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)))`.\nFix_pattern: In the condition of `self.masked_v` having the correct shape, if the pattern of calculating `self.masked_v` using `tf.matmul()` is detected, then change `self.masked_v` to be calculated using `tf.multply()` to fix the API misuse."}
{"number": 4563, "change": "def shift_rgb(image: torch.Tensor, r_shift: torch.Tensor, g_shift: torch.Tensor,\n\nshifts = [r_shift, g_shift, b_shift]\n\n-    shifted = (image + torch.Tensor(shifts).view(1, 3, 1, 1).to(image)).clamp_(min=0, max=1)\n+    shifted = (image + torch.stack(shifts).view(-1, 3, 1, 1).to(image)).clamp_(min=0, max=1)\n\nreturn shifted\n", "fix_pattern": "<condition>: The condition is not explicitly mentioned in the given context.\n\n<pattern>: The pattern in question is changing the way the shifts are stacked and viewed.\n\n<code_one>: The original code used `torch.Tensor(shifts).view(1, 3, 1, 1)`\n\n<code_two>: The fixed code uses `torch.stack(shifts).view(-1, 3, 1, 1)`\n\nFix_pattern: In the condition of the given context, if the pattern of `torch.Tensor(shifts).view(1, 3, 1, 1)` is detected, then change it to `torch.stack(shifts).view(-1, 3, 1, 1)` to fix the API misuse."}
{"number": 4564, "change": "class TestImageFeatureEmbeddings(AllenNlpTestCase):\nsuper().__init__()\n\nself.image_embeddings = torch.nn.Linear(feature_size, embedding_size)\n-                self.image_location_embeddings = torch.nn.Linear(4, embedding_size)\n+                self.image_location_embeddings = torch.nn.Linear(4, embedding_size, bias=False)\nself.layer_norm = torch.nn.LayerNorm(embedding_size, eps=1e-12)\nself.dropout = torch.nn.Dropout(dropout)\n", "fix_pattern": "<condition>: There is a linear layer defined for image location embeddings.\n\n<pattern>: The linear layer for image location embeddings should be initialized with the parameter \"bias=False\".\n\n<code_one>: self.image_location_embeddings = torch.nn.Linear(4, embedding_size)\n\n<code_two>: self.image_location_embeddings = torch.nn.Linear(4, embedding_size, bias=False)\n\nFix_pattern: In the condition of having a linear layer for image location embeddings, if the linear layer is not initialized with the \"bias=False\" parameter, then change the initialization of the linear layer to include \"bias=False\" to fix the API misuse."}
{"number": 4585, "change": "class BooleanAccuracy(Metric):\n\n# We want to skip predictions that are completely masked;\n# so we'll keep predictions that aren't.\n-            keep = mask.view(batch_size, -1).max(dim=1)[0].float()\n+            keep = mask.view(batch_size, -1).max(dim=1)[0]\nelse:\n-            keep = torch.ones(batch_size, device=predictions.device).float()\n+            keep = torch.ones(batch_size, device=predictions.device).bool()\n\npredictions = predictions.view(batch_size, -1)\ngold_labels = gold_labels.view(batch_size, -1)\n", "fix_pattern": "<condition>: The condition is not clearly stated in the context section.\n\n<pattern>: The pattern is to change the type of the variable \"keep\" from float to bool.\n\n<code_one>: The code that is removed is \"keep = torch.ones(batch_size, device=predictions.device).float()\".\n\n<code_two>: The code that is added is \"keep = torch.ones(batch_size, device=predictions.device).bool()\".\n\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then change the type of the variable \"keep\" from float to bool by replacing <code_one> with <code_two> to fix the API misuse."}
{"number": 4646, "change": "class InvertedResidual(BaseModule):\nout = self.linear_conv(out)\n\nif self.with_res_shortcut:\n-                return x + out\n+                return x + self.drop_path(out)\nelse:\nreturn out\n", "fix_pattern": "<condition>: The API misuse occurs when there is a missing code block for the 'if' branch.\n\n<pattern>: The pattern is that the code block 'return x + out' needs to be added.\n\n<code_one>: None\n\n<code_two>: 'return x + self.drop_path(out)'\n\nFix_pattern: In the condition of missing code block for the 'if' branch, if the pattern 'return x + out' is detected, then add the code 'return x + self.drop_path(out)' to fix the API misuse."}
{"number": 4655, "change": "torch_scatter = None\ndef dev(x: torch.Tensor, as_native: bool = False) -> Union[ivy.Device, torch.device]:\ndv = x.device\nif as_native:\n-        return torch.device(dv.replace(\"gpu\", \"cuda\"))\n+        return torch.device(dv.type.replace(\"gpu\", \"cuda\"))\nreturn as_ivy_dev(dv)\n", "fix_pattern": "<condition>:\nThe condition is \"as_native\" is True.\n\n<pattern>:\nThe pattern is replacing \"gpu\" with \"cuda\" in the string representation of the device.\n\n<code_one>:\nThe code to be removed is \"return torch.device(dv.replace(\"gpu\", \"cuda\"))\".\n\n<code_two>:\nThe code to be added is \"return torch.device(dv.type.replace(\"gpu\", \"cuda\"))\".\n\nFix_pattern:\nIn the condition of \"as_native\" being True, if the pattern of replacing \"gpu\" with \"cuda\" is detected in the string representation of the device, then change the code \"return torch.device(dv.replace(\"gpu\", \"cuda\"))\" to \"return torch.device(dv.type.replace(\"gpu\", \"cuda\"))\" to fix the API misuse."}
{"number": 4708, "change": "class Bernoulli(Distribution):\nself.shape = shape\naction_size = util.prod(self.shape)\n\n-        with tf.name_scope(name=scope):\n-            self.logit = Linear(size=action_size, bias=log(probability), scope='logit')\n+        self.logit = Linear(size=action_size, bias=log(probability), scope='logit')\n\nsuper(Bernoulli, self).__init__(scope, summary_labels)\n", "fix_pattern": "<condition>: There is a need to initialize the `self.logit` variable in the `Bernoulli` class.\n<pattern>: The `self.logit` variable is not properly initialized in the code.\n<code_one>: The code that initializes the `self.logit` variable in the `Bernoulli` class.\n<code_two>: The corrected code that initializes the `self.logit` variable in the `Bernoulli` class.\nFix_pattern: In the condition of needing to initialize the `self.logit` variable in the `Bernoulli` class, if the pattern of the `self.logit` variable not being properly initialized is detected, then remove the incorrect code that initializes `self.logit` and add the corrected code to properly initialize `self.logit` to fix the API misuse."}
{"number": 4715, "change": "def adalam_core(\nfinal_matches, idxs, counts = torch.unique(final_matches, dim=0, return_inverse=True, return_counts=True)\n_, ind_sorted = torch.sort(idxs)\ncum_sum = counts.cumsum(0)\n-        cum_sum = torch.cat((torch.tensor([0]), cum_sum[:-1]))\n+        cum_sum = torch.cat((torch.tensor([0], dtype=cum_sum.dtype, device=cum_sum.device), cum_sum[:-1]))\nfirst_indicies = ind_sorted[cum_sum]\naccepted_dist = accepted_dist[first_indicies]\nif return_dist:\n", "fix_pattern": "<condition>: The condition is when the variable \"cum_sum\" is being used in the code.\n\n<pattern>: The pattern is to change the initialization of \"cum_sum\" variable.\n\n<code_one>: The code that is removed is \"cum_sum = torch.cat((torch.tensor([0]), cum_sum[:-1]))\".\n\n<code_two>: The code that is added is \"cum_sum = torch.cat((torch.tensor([0], dtype=cum_sum.dtype, device=cum_sum.device), cum_sum[:-1]))\".\n\nFix_pattern: In the condition of using the \"cum_sum\" variable, change the initialization of \"cum_sum\" by removing the line \"cum_sum = torch.cat((torch.tensor([0]), cum_sum[:-1]))\" and adding the line \"cum_sum = torch.cat((torch.tensor([0], dtype=cum_sum.dtype, device=cum_sum.device), cum_sum[:-1]))\" to fix the API misuse."}
{"number": 4743, "change": "def execute_with_gradients(\nreturn grads\n\nif isinstance(y, ivy.NativeArray):\n-        grads = grad_func(torch.clone(y))\n+        grads = _set_duplicates(\n+            grad_func(torch.clone(y)), required_duplicate_index_chains\n+        )\nelse:\n# ToDo: use functorch.jacrev if it fixes the issue with broken memory reference\narray_idxs = ivy.nested_argwhere(y, lambda x: ivy.is_native_array(x))\n", "fix_pattern": "<condition>: isinstance(y, ivy.NativeArray)\n<pattern>: isinstance(y, ivy.NativeArray) and grad_func(torch.clone(y))\n<code_one>: grads = grad_func(torch.clone(y))\n<code_two>: grads = _set_duplicates(grad_func(torch.clone(y)), required_duplicate_index_chains)\nFix_pattern: In the condition of isinstance(y, ivy.NativeArray), if grad_func(torch.clone(y)) is detected, then remove the line grads = grad_func(torch.clone(y)) and add the line grads = _set_duplicates(grad_func(torch.clone(y)), required_duplicate_index_chains) to fix the API misuse."}
{"number": 4780, "change": "def _load_weights(model: VisionTransformer, checkpoint_path: str, prefix: str =\nmodel.pos_embed.copy_(pos_embed_w)\nmodel.norm.weight.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/scale']))\nmodel.norm.bias.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/bias']))\n-    if model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:\n+    if isinstance(model.head, nn.Linear) and model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:\nmodel.head.weight.copy_(_n2p(w[f'{prefix}head/kernel']))\nmodel.head.bias.copy_(_n2p(w[f'{prefix}head/bias']))\nfor i, block in enumerate(model.blocks.children()):\n", "fix_pattern": "<condition>: There is a condition checking the shape of the `model.head.bias` array.\n<pattern>: The pattern is adding a check to ensure that `model.head` is an instance of `nn.Linear`.\n<code_one>: The code being removed is the condition check `if model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:`.\n<code_two>: The code being added is the check `if isinstance(model.head, nn.Linear) and model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:`.\nFix_pattern: In the condition of checking the shape of the `model.head.bias` array, if `model.head` is not an instance of `nn.Linear`, then add the condition check to fix the API misuse."}
{"number": 4794, "change": "class MeanSquaredLogError(Metric):\npreds: Predictions from model\ntarget: Ground truth values\n\"\"\"\n-        self._check_same_shape(preds, target)\n-        squared_log_error = torch.pow(torch.log1p(preds) - torch.log1p(target), 2)\n+        sum_squared_log_error, n_obs = _mean_squared_log_error_update(preds, target)\n\n-        self.sum_squared_log_error += torch.sum(squared_log_error)\n-        self.total += target.numel()\n+        self.sum_squared_log_error += sum_squared_log_error\n+        self.total += n_obs\n\ndef compute(self):\n\"\"\"\nCompute mean squared logarithmic error over state.\n\"\"\"\n-        return self.sum_squared_log_error / self.total\n+        return _mean_squared_log_error_compute(self.sum_squared_log_error, self.total)\n", "fix_pattern": "<condition>: The code is a part of a class that computes mean squared logarithmic error.\n<pattern>: The pattern is that the code is using multiple lines of code to compute the mean squared logarithmic error.\n<code_one>: The code that is removed is calculating the error using the formula \"torch.pow(torch.log1p(preds) - torch.log1p(target), 2)\".\n<code_two>: The code that is added is calling a function \"_mean_squared_log_error_update(preds, target)\" to compute the error.\nFix_pattern: In the condition of computing mean squared logarithmic error, if multiple lines of code are used to calculate the error, then remove the code that calculates the error using the formula and replace it with a function call to compute the error."}
{"number": 4798, "change": "class Wav2VecCtc(BaseFairseqModel):\n\nif net_output[\"padding_mask\"] is not None and net_output[\"padding_mask\"].any():\nnumber_of_classes = logits.size(-1)\n-            masking_tensor = torch.ones(number_of_classes) * float(\"-inf\")\n-            masking_tensor[0] = float(\"inf\")\n+            masking_tensor = torch.ones(\n+                number_of_classes, device=logits.device\n+            ) * float(\"-inf\")\n+            masking_tensor[0] = 0\nlogits[net_output[\"padding_mask\"].T] = masking_tensor.type_as(logits)\n\nif normalize:\n", "fix_pattern": "<condition>: The condition is that `net_output[\"padding_mask\"]` is not None and `net_output[\"padding_mask\"].any()`.\n<pattern>: The pattern is to initialize a `masking_tensor` with `torch.ones(number_of_classes) * float(\"-inf\")` and set `masking_tensor[0]` to `float(\"inf\")`.\n<code_one>: The code that is removed is:\n```\nmasking_tensor = torch.ones(number_of_classes) * float(\"-inf\")\nmasking_tensor[0] = float(\"inf\")\n```\n<code_two>: The code that is added is:\n```\nmasking_tensor = torch.ones(number_of_classes, device=logits.device) * float(\"-inf\")\nmasking_tensor[0] = 0\n```\nFix_pattern: In the condition of `net_output[\"padding_mask\"] is not None and net_output[\"padding_mask\"].any()`, if the pattern of initializing `masking_tensor` with `torch.ones(number_of_classes) * float(\"-inf\")` and setting `masking_tensor[0]` to `float(\"inf\")` is detected, then change `masking_tensor` to be initialized with `torch.ones(number_of_classes, device=logits.device) * float(\"-inf\")` and set `masking_tensor[0]` to 0 to fix the API misuse."}
{"number": 4830, "change": "class Trainer:\n\"\"\"\nfor k, v in inputs.items():\nif isinstance(v, torch.Tensor):\n-                inputs[k] = v.to(self.args.device)\n+                kwargs = dict(device=self.args.device)\n+                if self.deepspeed and inputs[k].dtype != torch.int64:\n+                    # NLP models inputs are int64 and those get adjusted to the right dtype of the\n+                    # embedding. Other models such as wav2vec2's inputs are already float and thus\n+                    # may need special handling to match the dtypes of the model\n+                    kwargs.update(dict(dtype=self.args.hf_deepspeed_config.dtype()))\n+\n+                inputs[k] = v.to(**kwargs)\n\nif self.args.past_index >= 0 and self._past is not None:\ninputs[\"mems\"] = self._past\n", "fix_pattern": "<condition>: The condition is when a variable named `v` is being iterated over in a loop.\n\n<pattern>: The pattern is detecting if the variable `v` is an instance of the `torch.Tensor` class.\n\n<code_one>: The code being removed is `inputs[k] = v.to(self.args.device)`.\n\n<code_two>: The code being added is:\n```\nkwargs = dict(device=self.args.device)\nif self.deepspeed and inputs[k].dtype != torch.int64:\n    kwargs.update(dict(dtype=self.args.hf_deepspeed_config.dtype()))\ninputs[k] = v.to(**kwargs)\n```\n\nFix_pattern: In the condition of iterating over a variable `v`, if `v` is an instance of the `torch.Tensor` class, then change the code `inputs[k] = v.to(self.args.device)` to the code:\n```\nkwargs = dict(device=self.args.device)\nif self.deepspeed and inputs[k].dtype != torch.int64:\n    kwargs.update(dict(dtype=self.args.hf_deepspeed_config.dtype()))\ninputs[k] = v.to(**kwargs)\n```"}
{"number": 4853, "change": "class QLoss:\n# priority is robust and insensitive to `prioritized_replay_alpha`\nself.td_error = tf.nn.softmax_cross_entropy_with_logits(\nlabels=m, logits=q_logits_t_selected)\n-            self.loss = tf.reduce_mean(self.td_error * importance_weights)\n+            self.loss = tf.reduce_mean(\n+                self.td_error * tf.cast(importance_weights, tf.float32))\nself.stats = {\n# TODO: better Q stats for dist dqn\n\"mean_td_error\": tf.reduce_mean(self.td_error),\n", "fix_pattern": "<condition>: In the context section, there is no clear condition stated.\n<pattern>: The pattern identified is \"multiply a tensor by another tensor\".\n<code_one>: The code that was removed is \"self.loss = tf.reduce_mean(self.td_error * importance_weights)\".\n<code_two>: The code that was added is \"self.loss = tf.reduce_mean(self.td_error * tf.cast(importance_weights, tf.float32))\".\nFix_pattern: In the condition of no specific condition, if the pattern of multiplying a tensor by another tensor is detected, then change <code_one> to <code_two> to fix the API misuse."}
{"number": 4861, "change": "try:\nif _torch_available:\nimport torch\n\n-        if torch.__version__ < version.Version(\"1.12\"):\n+        if version.Version(torch.__version__) < version.Version(\"1.12\"):\nraise ValueError(\"PyTorch should be >= 1.12\")\nlogger.debug(f\"Successfully imported xformers version {_xformers_version}\")\nexcept importlib_metadata.PackageNotFoundError:\n", "fix_pattern": "<condition>: If the condition of `_torch_available` is met.\n<pattern>: If the version of PyTorch is less than \"1.12\".\n<code_one>: `if torch.__version__ < version.Version(\"1.12\")`\n<code_two>: `if version.Version(torch.__version__) < version.Version(\"1.12\")`\nFix_pattern: In the condition of `_torch_available`, if the version of PyTorch is less than \"1.12\", then change `if torch.__version__ < version.Version(\"1.12\")` to `if version.Version(torch.__version__) < version.Version(\"1.12\")` to fix the API misuse."}
{"number": 4871, "change": "def move_data_to_device(batch: Any, device: Union[str, torch.device]) -> Any:\n\nkwargs = {}\n# Don't issue non-blocking transfers to CPU\n-        if isinstance(data, Tensor) and device not in _CPU_DEVICES:\n+        # Same with MPS due to a race condition bug: https://github.com/pytorch/pytorch/issues/83015\n+        if isinstance(data, Tensor) and isinstance(device, torch.device) and device.type not in _BLOCKING_DEVICE_TYPES:\nkwargs[\"non_blocking\"] = True\ndata_output = data.to(device, **kwargs)\nif data_output is not None:\n", "fix_pattern": "<condition>: If data is of type Tensor and device is not in _CPU_DEVICES\n<pattern>: API misuse\n<code_one>: if isinstance(data, Tensor) and device not in _CPU_DEVICES\n<code_two>: if isinstance(data, Tensor) and isinstance(device, torch.device) and device.type not in _BLOCKING_DEVICE_TYPES\nFix_pattern: In the condition of checking if data is of type Tensor and device is not in _CPU_DEVICES, remove the code block that performs the check and replace it with a new code block that checks if data is of type Tensor and device is of type torch.device, and device type is not in _BLOCKING_DEVICE_TYPES."}
{"number": 4874, "change": "def select_device(device='', batch_size=0, newline=True):\nif cpu:\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force torch.cuda.is_available() = False\nelif device:  # non-cpu device requested\n-        nd = torch.cuda.device_count()  # number of CUDA devices\n-        assert torch.cuda.is_available(), 'CUDA is not available, use `--device cpu` or do not pass a --device'\n+        nd = device_count()  # number of CUDA devices\nassert nd > int(max(device.split(','))), f'Invalid `--device {device}` request, valid devices are 0 - {nd - 1}'\n-        os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable (must be after asserts)\n+        os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable - must be before assert is_available()\n+        assert torch.cuda.is_available(), 'CUDA is not available, use `--device cpu` or do not pass a --device'\n\ncuda = not cpu and torch.cuda.is_available()\nif cuda:\n", "fix_pattern": "<condition>: Checking if CUDA is available and a non-CPU device is requested.\n<pattern>: Misuse of the CUDA API.\n<code_one>: \n```\nnd = torch.cuda.device_count()  # number of CUDA devices\nassert torch.cuda.is_available(), 'CUDA is not available, use `--device cpu` or do not pass a --device'\nos.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable (must be after asserts)\n```\n<code_two>:\n```\nnd = device_count()  # number of CUDA devices\nos.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable - must be before assert is_available()\nassert torch.cuda.is_available(), 'CUDA is not available, use `--device cpu` or do not pass a --device'\n```\nFix_pattern: In the condition of checking if CUDA is available and a non-CPU device is requested, if there is a misuse of the CUDA API, then remove the code to get the number of CUDA devices (`nd = torch.cuda.device_count()`), change the order of setting the CUDA visible devices environment variable (`os.environ['CUDA_VISIBLE_DEVICES'] = device`) to be placed before `assert torch.cuda.is_available()`, and add code to get the number of CUDA devices (`nd = device_count()`)."}
{"number": 4911, "change": "class TfKerasModelArtifact(Artifact):\ndef model_file_path(self, base_path):\nreturn os.path.join(base_path, self.name + self._model_extension)\n\n-    def pack(self, model):\n+    def pack(self, model):  # pylint:disable=arguments-differ\nself.model = model\n\ndef get(self):\nreturn self.model\n\n-    def load(self, base_path):  # pylint:disable=arguments-differ\n-        from tensorflow.keras.models import load_model\n+    def load(self, base_path):\n+        try:\n+            from tensorflow.keras.models import load_model\n+        except ImportError:\n+            raise ImportError(\"tensorflow package is required to use TfKerasModelArtifact\")\nself.model = load_model(self.model_file_path(base_path))\n\ndef save(self, base_path):\n", "fix_pattern": "<condition>:\nThe condition is when importing the `load_model` function from the `tensorflow.keras.models` module.\n\n<pattern>:\nThe pattern is that the `load` method is redefined and there is a potential API misuse in the original code.\n\n<code_one>:\nIn the original code, the `load` method is defined as:\n```python\ndef load(self, base_path):  # pylint:disable=arguments-differ\n    from tensorflow.keras.models import load_model\n```\n\n<code_two>:\nThe code is changed to add an exception handling block and raise an ImportError if the `tensorflow` package is not available:\n```python\ndef load(self, base_path):\n    try:\n        from tensorflow.keras.models import load_model\n    except ImportError:\n        raise ImportError(\"tensorflow package is required to use TfKerasModelArtifact\")\n```\n\nFix_pattern:\nIn the condition of importing the `load_model` function from `tensorflow.keras.models`, if the `load` method is redefined, then add exception handling block to check for the availability of the `tensorflow` package and raise an ImportError if it is not available to fix the API misuse."}
{"number": 4952, "change": "class TFMarianSinusoidalPositionalEmbedding(tf.keras.layers.Layer):\nposition_enc = np.array(\n[[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]\n)\n+        table = np.zeros_like(position_enc)\n# index 0 is all zero\n-        position_enc[:, 0 : dim // 2] = np.sin(position_enc[:, 0::2])\n-        position_enc[:, dim // 2 :] = np.cos(position_enc[:, 1::2])\n+        table[:, 0 : dim // 2] = np.sin(position_enc[:, 0::2])\n+        table[:, dim // 2 :] = np.cos(position_enc[:, 1::2])\n# convert to tensor\n-        table = tf.convert_to_tensor(position_enc)\n+        table = tf.convert_to_tensor(table)\ntf.stop_gradient(table)\nreturn table\n", "fix_pattern": "<condition>: The condition is not mentioned in the given context.\n\n<pattern>: The pattern is to switch the assignment of `position_enc` to `table`.\n\n<code_one>: \n```\nposition_enc[:, 0 : dim // 2] = np.sin(position_enc[:, 0::2])\nposition_enc[:, dim // 2 :] = np.cos(position_enc[:, 1::2])\ntable = tf.convert_to_tensor(position_enc)\n```\n\n<code_two>: \n```\ntable = np.zeros_like(position_enc)\ntable[:, 0 : dim // 2] = np.sin(position_enc[:, 0::2])\ntable[:, dim // 2 :] = np.cos(position_enc[:, 1::2])\ntable = tf.convert_to_tensor(table)\n```\n\nFix_pattern: In the condition where the code snippet `position_enc[:, 0 : dim // 2] = np.sin(position_enc[:, 0::2])` and `position_enc[:, dim // 2 :] = np.cos(position_enc[:, 1::2])` is present, it should be changed to `table = np.zeros_like(position_enc)`, `table[:, 0 : dim // 2] = np.sin(position_enc[:, 0::2])`, `table[:, dim // 2 :] = np.cos(position_enc[:, 1::2])`, and finally `table = tf.convert_to_tensor(table)` to fix the API misuse."}
{"number": 5002, "change": "def get_outputs_sizes_torch(\n\n\ndef create_model_inputs_torch(\n-    batch_size: int, input_infos: List[InputInfo]\n+    input_infos: List[InputInfo],\n) -> List[torch.Tensor]:\ninput_tensors = (\n-        torch.randn((batch_size, *input_info.size))\n+        torch.randn(*input_info.size)\nif input_info.dtype is DataType.FLOAT32\nelse torch.randint(\n-            size=(batch_size, *input_info.size),\n+            size=input_info.size,\nlow=input_info.min_value or 0,\nhigh=input_info.max_value or 100,\n)\n", "fix_pattern": "<condition>: The condition is that the input_info.dtype is DataType.FLOAT32.\n<pattern>: The pattern detected is that the input_info.size is being used incorrectly.\n<code_one>: The code being removed is `torch.randn((batch_size, *input_info.size))`.\n<code_two>: The code being added is `torch.randn(*input_info.size)`.\nFix_pattern: In the condition of input_info.dtype is DataType.FLOAT32, if the input_info.size is being used incorrectly, then remove `torch.randn((batch_size, *input_info.size))` and add `torch.randn(*input_info.size)` to fix the API misuse."}
{"number": 5036, "change": "def infer_inputs_from_restored_call_function(fn):\nif isinstance(x, tf.SparseTensorSpec):\nreturn tf.SparseTensorSpec(common_shape, x.dtype)\nelif isinstance(x, tf.RaggedTensorSpec):\n-      return tf.RaggedTensorSpec(common_shape, x.dtype)\n+      return tf.RaggedTensorSpec(\n+          common_shape,\n+          x.dtype,\n+          ragged_rank=x.ragged_rank,\n+          row_splits_dtype=x.row_splits_dtype,\n+          flat_values_spec=x.flat_values_spec)\nreturn tf.TensorSpec(common_shape, x.dtype, x.name)\n\nspec = fn.concrete_functions[0].structured_input_signature\n", "fix_pattern": "<condition>: The condition is \"if isinstance(x, tf.RaggedTensorSpec)\".\n\n<pattern>: The pattern is to replace the code block \"return tf.RaggedTensorSpec(common_shape, x.dtype)\" with a modified version.\n\n<code_one>: The code that is removed is \"return tf.RaggedTensorSpec(common_shape, x.dtype)\".\n\n<code_two>: The code that is added is:\n\n```\nreturn tf.RaggedTensorSpec(\n  common_shape,\n  x.dtype,\n  ragged_rank=x.ragged_rank,\n  row_splits_dtype=x.row_splits_dtype,\n  flat_values_spec=x.flat_values_spec)\n```\n\nFix_pattern:\nIn the condition of \"if isinstance(x, tf.RaggedTensorSpec)\", if the code block \"return tf.RaggedTensorSpec(common_shape, x.dtype)\" is detected, then change it to:\n\n```\nreturn tf.RaggedTensorSpec(\n  common_shape,\n  x.dtype,\n  ragged_rank=x.ragged_rank,\n  row_splits_dtype=x.row_splits_dtype,\n  flat_values_spec=x.flat_values_spec)\n```\n\nThis fix pattern fixes the API misuse."}
{"number": 5052, "change": "def _torch_solve_cast(input: torch.Tensor, A: torch.Tensor) -> Tuple[torch.Tenso\nif dtype not in (torch.float32, torch.float64):\ndtype = torch.float32\n\n-    out = solve(A.to(dtype), input.to(dtype))\n+    out = torch.linalg.solve(A.to(dtype), input.to(dtype))\n\nreturn (out.to(input.dtype), out)\n", "fix_pattern": "<condition>:\nThe condition is that the dtype variable is not equal to either torch.float32 or torch.float64.\n\n<pattern>:\nThe pattern is that the solve function is being called with the A and input tensors being casted to the dtype variable.\n\n<code_one>:\nCode that is being removed:\nout = solve(A.to(dtype), input.to(dtype))\n\n<code_two>:\nCode that is being added:\nout = torch.linalg.solve(A.to(dtype), input.to(dtype))\n\nFix_pattern:\nIn the condition of the dtype variable not being equal to torch.float32 or torch.float64, if the solve function is called with the A and input tensors being casted to the dtype variable, then the code should be modified to use the torch.linalg.solve function instead."}
{"number": 5072, "change": "def test_dynamic_quantization(train_dic, recog_dic, quantize_dic):\ntrain_args = get_default_train_args(**train_dic)\nrecog_args = get_default_recog_args(**recog_dic)\n\n+    if not is_torch_1_5_plus:\n+        q_dtype = torch.qint8\n+    else:\n+        q_dtype = quantize_dic[\"mod\"]\n+\nmodel = E2E(idim, odim, train_args)\nmodel = torch.quantization.quantize_dynamic(\n-        model, quantize_dic[\"mod\"], dtype=quantize_dic[\"dtype\"]\n+        model, q_dtype, dtype=quantize_dic[\"dtype\"]\n)\n\nbeam_search = BeamSearchTransducer(\n", "fix_pattern": "<condition>: The condition is not clear from the given context.\n\n<pattern>: The pattern detected is an incorrect argument passed to the `quantize_dynamic()` function.\n\n<code_one>: The code that needs to be removed is `model, quantize_dic[\"mod\"], dtype=quantize_dic[\"dtype\"]`.\n\n<code_two>: The code that needs to be added is `model, q_dtype, dtype=quantize_dic[\"dtype\"]`.\n\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then remove the <code_one> and add the <code_two> to fix the API misuse."}
{"number": 5080, "change": "class GNNExplainer(torch.nn.Module):\nif node_idx == -1:\nhard_edge_mask = torch.BoolTensor([True] * edge_index.size(1),\ndevice=edge_mask.device)\n-            subset = torch.arange(\n-                edge_index.max() + 1,\n-                device=edge_index.device if y is None else y.device)\n+            subset = torch.arange(edge_index.max().item() + 1,\n+                                  device=edge_index.device)\n+            y = None\n+\nelse:\n# Only operate on a k-hop subgraph around `node_idx`.\nsubset, edge_index, _, hard_edge_mask = k_hop_subgraph(\n", "fix_pattern": "<condition>: node_idx == -1\n<pattern>: subset = torch.arange(\n                edge_index.max() + 1,\n                device=edge_index.device if y is None else y.device)\n<code_one>: subset, edge_index, _, hard_edge_mask = k_hop_subgraph(\n<code_two>: subset = torch.arange(edge_index.max().item() + 1,\n                                  device=edge_index.device)\n            y = None\nFix_pattern: In the condition of node_idx == -1, if the pattern subset = torch.arange(edge_index.max() + 1, device=edge_index.device if y is None else y.device) is detected, then remove the code subset, edge_index, _, hard_edge_mask = k_hop_subgraph( and add subset = torch.arange(edge_index.max().item() + 1, device=edge_index.device) and y = None to fix the API misuse."}
{"number": 5102, "change": "class FSMTHeadTests(unittest.TestCase):\nconfig, *_ = self._get_config_and_data()\ninput_ids = _long_tensor(([4, 4, 2]))\ndecoder_input_ids = _long_tensor([[26388, 2, config.pad_token_id]])\n-        ignore = float(\"-inf\")\n+        causal_mask_dtype = torch.float32\n+        ignore = torch.finfo(causal_mask_dtype).min\ndecoder_input_ids, decoder_attn_mask, causal_mask = _prepare_fsmt_decoder_inputs(\n-            config, input_ids, decoder_input_ids\n+            config, input_ids, decoder_input_ids, causal_mask_dtype=causal_mask_dtype\n)\nexpected_causal_mask = torch.tensor(\n[[0, ignore, ignore], [0, 0, ignore], [0, 0, 0]]  # never attend to the final token, because its pad\n", "fix_pattern": "<condition>: The condition is not explicitly mentioned in the code snippet provided. Therefore, no clear condition can be identified.\n\n<pattern>: The pattern that is detected is the definition and assignment of the variable \"ignore\" with the value float(\"-inf\") in the code removed section.\n\n<code_one>: The code that is removed is the line \"ignore = float(\"-inf\")\".\n\n<code_two>: The code that is added is the line \"ignore = torch.finfo(causal_mask_dtype).min\".\n\nFix_pattern: In the condition of no specific condition, if the pattern of defining and assigning the variable \"ignore\" with the value float(\"-inf\") is detected, then change the line \"ignore = float(\"-inf\")\" to \"ignore = torch.finfo(causal_mask_dtype).min\" to fix the API misuse."}
{"number": 5106, "change": "def main():\nif args.do_train:\ntorch.save(model_to_save.state_dict(), output_model_file)\n\n-    # Load a trained model that you have fine-tuned\n-    model_state_dict = torch.load(output_model_file)\n-    model = BertForQuestionAnswering.from_pretrained(args.bert_model, state_dict=model_state_dict)\n+        # Load a trained model that you have fine-tuned\n+        model_state_dict = torch.load(output_model_file)\n+        model = BertForQuestionAnswering.from_pretrained(args.bert_model, state_dict=model_state_dict)\n+    else:\n+        model = BertForQuestionAnswering.from_pretrained(args.bert_model)\n+\nmodel.to(device)\n\nif args.do_predict and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n", "fix_pattern": "<condition>: args.do_predict and (args.local_rank == -1 or torch.distributed.get_rank() == 0)\n<pattern>: API misuse\n<code_one>: Loading a trained model using torch.load()\n<code_two>: Loading a trained model using torch.load() and adding an else condition to create a default model if the condition is not met\nFix_pattern: In the condition of args.do_predict and (args.local_rank == -1 or torch.distributed.get_rank() == 0), if API misuse is detected (loading a trained model using torch.load()), then remove the existing code for loading the model and add the corrected code for loading the model along with an else condition to create a default model if the condition is not met."}
{"number": 5124, "change": "class TFT5ModelIntegrationTests(unittest.TestCase):\nlabels = tokenizer(\"Hi I am\", return_tensors=\"tf\").input_ids\n\nloss = model(input_ids, labels=labels).loss\n-        mtf_score = -tf.math.reduce_sum(loss).numpy()\n+        mtf_score = -tf.math.reduce_mean(loss).numpy()\n\n-        EXPECTED_SCORE = -60.7397\n+        EXPECTED_SCORE = -7.594554\nself.assertTrue(abs(mtf_score - EXPECTED_SCORE) < 1e-4)\n\n@slow\n", "fix_pattern": "<condition>: `loss` is calculated using `tf.math.reduce_sum()` function.\n<pattern>: The `mtf_score` is calculated using `-tf.math.reduce_sum(loss).numpy()`.\n<code_one>: `mtf_score = -tf.math.reduce_sum(loss).numpy()`\n<code_two>: `mtf_score = -tf.math.reduce_mean(loss).numpy()`\nFix_pattern: In the condition of `loss` calculation using `tf.math.reduce_sum()`, if the `mtf_score` calculation is `-tf.math.reduce_sum(loss).numpy()`, then change the code to calculate `mtf_score` using `-tf.math.reduce_mean(loss).numpy()` to fix the API misuse."}
{"number": 5177, "change": "class CLIPModelTester:\n\ndef create_and_check_model(self, config, input_ids, attention_mask, pixel_values):\nmodel = CLIPModel(config).to(torch_device).eval()\n-        result = model(input_ids, pixel_values, attention_mask)\n+        with torch.no_grad():\n+            result = model(input_ids, pixel_values, attention_mask)\nself.parent.assertEqual(\nresult.logits_per_image.shape, (self.vision_model_tester.batch_size, self.text_model_tester.batch_size)\n)\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: None\n<code_one>: None\n<code_two>: None\nFix_pattern: No fix pattern identified"}
{"number": 5178, "change": "def inverse_pose(pose):\n\npose_inv = pose.clone()\npose_inv[..., :3, 0:3] = torch.transpose(pose[..., :3, :3], 1, 2)\n-    pose_inv[..., :3, 2:3] = torch.matmul(\n-        -1.0 * pose_inv[..., :3, :3], pose[..., :3, 2:3])\n+    pose_inv[..., :3, 3:4] = torch.matmul(\n+        -1.0 * pose_inv[..., :3, :3], pose[..., :3, 3:4])\n\nif len(pose_shape) == 2:\npose_inv = torch.squeeze(pose_inv, dim=0)\n", "fix_pattern": "<condition>:\n\nThe condition in the given context is \"len(pose_shape) == 2\". This condition checks if the length of the pose_shape variable is equal to 2.\n\n<pattern>:\n\nThe pattern is \"pose_inv[..., :3, 2:3] = torch.matmul(-1.0 * pose_inv[..., :3, :3], pose[..., :3, 2:3])\". This pattern is detected in the code removed section.\n\n<code_one>:\n\nThe code one is \"pose_inv[..., :3, 2:3]\".\n\n<code_two>:\n\nThe code two is \"pose_inv[..., :3, 3:4]\".\n\nFix_pattern:\n\nIn the condition of \"len(pose_shape) == 2\", if the pattern \"pose_inv[..., :3, 2:3] = torch.matmul(-1.0 * pose_inv[..., :3, :3], pose[..., :3, 2:3])\" is detected, then change the code \"pose_inv[..., :3, 2:3]\" to \"pose_inv[..., :3, 3:4]\" to fix the API misuse."}
{"number": 5191, "change": "class Input(Layer):\nlogging.info(\"Input  %s: %s\" % (self.name, str(shape)))\n\nshape_without_none = [_ if _ is not None else 1 for _ in shape]\n-        self.outputs = self.forward(tf.initializers.constant(value=0.0)(shape_without_none), is_train=False)\n+        self.outputs = self.forward(tf.initializers.random_normal()(shape_without_none), is_train=False)\n\ndef __call__(self, prev_layer):\n# FIXME: better exception raising\n", "fix_pattern": "<condition>: When initializing a constant value in a TensorFlow layer.\n<pattern>: Using tf.initializers.constant to initialize the value.\n<code_one>: tf.initializers.constant(value=0.0)(shape_without_none)\n<code_two>: tf.initializers.random_normal()(shape_without_none)\nFix_pattern: In the condition of initializing a constant value in a TensorFlow layer, if using tf.initializers.constant to initialize the value is detected, then change tf.initializers.constant(value=0.0)(shape_without_none) to tf.initializers.random_normal()(shape_without_none) to fix the API misuse."}
{"number": 5194, "change": "class CascadeRoIHead(BaseRoIHead, BBoxTestMixin, MaskTestMixin):\nms_scores.append(bbox_results['cls_score'])\n\nif i < self.num_stages - 1:\n-                    bbox_label = bbox_results['cls_score'].argmax(dim=1)\n+                    bbox_label = bbox_results['cls_score'][:, :-1].argmax(\n+                        dim=1)\nrois = self.bbox_head[i].regress_by_class(\nrois, bbox_label, bbox_results['bbox_pred'],\nimg_meta[0])\n", "fix_pattern": "<condition>: The condition is \"if i < self.num_stages - 1\".\n<pattern>: The pattern is \"bbox_results['cls_score'].argmax(dim=1)\".\n<code_one>: The code that needs to be removed is \"bbox_results['cls_score'].argmax(dim=1)\".\n<code_two>: The code that needs to be added is \"bbox_results['cls_score'][:, :-1].argmax(dim=1)\".\nFix_pattern: In the condition of \"if i < self.num_stages - 1\", if the pattern \"bbox_results['cls_score'].argmax(dim=1)\" is detected, then remove the code \"bbox_results['cls_score'].argmax(dim=1)\" and add the code \"bbox_results['cls_score'][:, :-1].argmax(dim=1)\" to fix the API misuse."}
{"number": 5198, "change": "def test_model_saving_loading():\n# make prediction\n# assert that both predictions are the same\nnew_pred = model_2(x)\n-    assert torch.eq(pred_before_saving, new_pred)\n+    assert torch.all(torch.eq(pred_before_saving, new_pred)).item() == 1\n\nclear_save_dir()\n", "fix_pattern": "<condition>: There is a condition/assertion in the code that checks if two variables are equal (using `torch.eq`).\n\n<pattern>: The pattern is that the assertion is changed to use `torch.all` to check if all elements of the two variables are equal.\n\n<code_one>: The code being removed is `assert torch.eq(pred_before_saving, new_pred)`.\n\n<code_two>: The code being added is `assert torch.all(torch.eq(pred_before_saving, new_pred)).item() == 1`.\n\nFix_pattern: In the condition of checking variable equality, if the assertion is using `torch.eq`, then change it to use `torch.all(torch.eq())` to fix the API misuse."}
{"number": 5219, "change": "class TorchCustomLossModel(TorchModelV2, nn.Module):\n\n# Compute the IL loss.\naction_dist = TorchCategorical(logits, self.model_config)\n-        imitation_loss = torch.mean(\n-            -action_dist.logp(torch.from_numpy(batch[\"actions\"])))\n+        imitation_loss = torch.mean(-action_dist.logp(\n+            torch.from_numpy(batch[\"actions\"]).to(policy_loss[0].device)))\nself.imitation_loss_metric = imitation_loss.item()\nself.policy_loss_metric = np.mean([l.item() for l in policy_loss])\n", "fix_pattern": "<condition>:\nThe condition is when computing the imitation loss in a TorchCustomLossModel class.\n\n<pattern>:\nThe pattern is that the action distribution log probabilities need to be computed using the device of the first policy loss element.\n\n<code_one>:\nThe code that was removed is:\n```python\nimitation_loss = torch.mean(-action_dist.logp(torch.from_numpy(batch[\"actions\"])))\n```\n\n<code_two>:\nThe code that was added is:\n```python\nimitation_loss = torch.mean(-action_dist.logp(torch.from_numpy(batch[\"actions\"]).to(policy_loss[0].device)))\n```\n\nFix_pattern:\nIn the condition of computing the imitation loss in a TorchCustomLossModel class, if the action distribution log probabilities need to be computed using the device of the first policy loss element, then replace the code `action_dist.logp(torch.from_numpy(batch[\"actions\"]))` with `action_dist.logp(torch.from_numpy(batch[\"actions\"]).to(policy_loss[0].device))` to fix the API misuse."}
{"number": 5242, "change": "class TFXLNetMainLayer(tf.keras.layers.Layer):\nassert input_mask is None or attention_mask is None, \"You can only use one of input_mask (uses 1 for padding) \" \\\n\"or attention_mask (uses 0 for padding, added for compatbility with BERT). Please choose one.\"\nif input_mask is None and attention_mask is not None:\n-            input_mask = 1.0 - attention_mask\n+            input_mask = 1.0 - tf.cast(attention_mask, dtype=dtype_float)\nif input_mask is not None and perm_mask is not None:\ndata_mask = input_mask[None] + perm_mask\nelif input_mask is not None and perm_mask is None:\n", "fix_pattern": "<condition>: input_mask is None and attention_mask is not None\n<pattern>: Assigning a value to input_mask based on attention_mask\n<code_one>: input_mask = 1.0 - attention_mask\n<code_two>: input_mask = 1.0 - tf.cast(attention_mask, dtype=dtype_float)\nFix_pattern: In the condition of input_mask is None and attention_mask is not None, if assigning a value to input_mask based on attention_mask is detected, then change the code input_mask = 1.0 - attention_mask to input_mask = 1.0 - tf.cast(attention_mask, dtype=dtype_float) to fix the API misuse."}
{"number": 5247, "change": "from tqdm import tqdm\n\n\ndef download_wmt_dataset(src_lang=\"ro\", tgt_lang=\"en\", dataset=\"wmt16\", save_dir=None) -> None:\n-    \"\"\"Download a dataset using the nlp package and save it to the format expected by finetune.py\n+    \"\"\"Download a dataset using the datasets package and save it to the format expected by finetune.py\nFormat of save_dir: train.source, train.target, val.source, val.target, test.source, test.target.\n\nArgs:\nsrc_lang: <str> source language\ntgt_lang: <str> target language\n-        dataset: <str> wmt16, wmt17, etc. wmt16 is a good start as it's small. To get the full list run `import nlp; print([d.id for d in nlp.list_datasets() if \"wmt\" in d.id])`\n+        dataset: <str> wmt16, wmt17, etc. wmt16 is a good start as it's small. To get the full list run `import datasets; print([d.id for d in datasets.list_datasets() if \"wmt\" in d.id])`\nsave_dir: <str>, where to save the datasets, defaults to f'{dataset}-{src_lang}-{tgt_lang}'\n\nUsage:\n>>> download_wmt_dataset('ro', 'en', dataset='wmt16') # saves to wmt16-ro-en\n\"\"\"\ntry:\n-        import nlp\n+        import datasets\nexcept (ModuleNotFoundError, ImportError):\n-        raise ImportError(\"run pip install nlp\")\n+        raise ImportError(\"run pip install datasets\")\npair = f\"{src_lang}-{tgt_lang}\"\nprint(f\"Converting {dataset}-{pair}\")\n-    ds = nlp.load_dataset(dataset, pair)\n+    ds = datasets.load_dataset(dataset, pair)\nif save_dir is None:\nsave_dir = f\"{dataset}-{pair}\"\nsave_dir = Path(save_dir)\n", "fix_pattern": "<condition>: No clear condition can be identified.\n\n<pattern>: Importing and using the `nlp` package to load datasets.\n\n<code_one>: \n'''\nimport nlp\nraise ImportError(\"run pip install nlp\")\nds = nlp.load_dataset(dataset, pair)\n'''\n\n<code_two>: \n'''\nimport datasets\nraise ImportError(\"run pip install datasets\")\nds = datasets.load_dataset(dataset, pair)\n'''\n\nFix_pattern: In the condition of no clear condition, if importing and using the `nlp` package to load datasets is detected, then remove the code importing `nlp` and change the dataset loading code to use the `datasets` package instead."}
{"number": 5277, "change": "class FaceAlignment:\nout += flip(self.face_alignment_net(flip(inp)).detach(), is_label=True)\nout = out.cpu().numpy()\n\n-            pts, pts_img = get_preds_fromhm(out, center, scale)\n-            pts, pts_img = pts.view(68, 2) * 4, pts_img.view(68, 2)\n+            pts, pts_img = get_preds_fromhm(out, center.numpy(), scale)\npts, pts_img = torch.from_numpy(pts), torch.from_numpy(pts_img)\n+            pts, pts_img = pts.view(68, 2) * 4, pts_img.view(68, 2)\n\nif self.landmarks_type == LandmarksType._3D:\nheatmaps = np.zeros((68, 256, 256), dtype=np.float32)\n", "fix_pattern": "<condition>:\nThe condition is when the `self.landmarks_type` variable is equal to `LandmarksType._3D`.\n\n<pattern>:\nThe pattern is to convert `pts` and `pts_img` to a 68x2 tensor by using the `view()` method and then multiplying it by 4.\n\n<code_one>:\n```\npts, pts_img = get_preds_fromhm(out, center, scale)\npts, pts_img = pts.view(68, 2) * 4, pts_img.view(68, 2)\n```\n\n<code_two>:\n```\npts, pts_img = get_preds_fromhm(out, center.numpy(), scale)\npts, pts_img = pts.view(68, 2) * 4, pts_img.view(68, 2)\n```\n\nFix_pattern:\nIn the condition of `self.landmarks_type` being equal to `LandmarksType._3D`, if the code pattern of converting `pts` and `pts_img` to a 68x2 tensor by using the `view()` method and multiplying it by 4 is detected, then change `center` to `center.numpy()` to fix the API misuse."}
{"number": 5310, "change": "class CTCPrefixScorer(PartialScorerInterface):\ndef score_partial(self, y, ids, state, x):\nprev_score, state = state\npresub_score, new_st = self.impl(y.cpu(), ids.cpu(), state)\n-        tscore = torch.as_tensor(presub_score - prev_score, device=y.device)\n+        tscore = torch.as_tensor(presub_score - prev_score, device=x.device, dtype=x.dtype)\nreturn tscore, (presub_score, new_st)\n", "fix_pattern": "<condition>: The condition is when the device and/or dtype of the input variables need to be changed.\n<pattern>: The pattern is changing the device and/or dtype of the input variables.\n<code_one>: The code to be removed is `device=y.device`.\n<code_two>: The code to be added is `device=x.device, dtype=x.dtype`.\nFix_pattern: In the condition of changing the device and/or dtype of the input variables, remove the code `device=y.device` and add the code `device=x.device, dtype=x.dtype` to fix the API misuse."}
{"number": 5314, "change": "class Model(torch.nn.Module, Registrable):\nadd_batch_dimension=True,\ncuda_device=cuda_device,\nfor_training=False)\n-        outputs = self.forward(**model_input)\n+        outputs = self.decode(self.forward(**model_input))\n\nfor name, output in list(outputs.items()):\noutput = output[0]\n", "fix_pattern": "<condition>: No pre condition is needed.\n<pattern>: None\n<code_one>: None\n<code_two>: None\nFix_pattern: No fix pattern is needed."}
{"number": 5322, "change": "def inplace_update(\nelif ivy.is_ivy_array(x):\nx.data = val_native\nelse:\n-            raise ivy.exceptions.IvyException(\n-                \"TensorFlow does not support inplace updates of the tf.Tensor\"\n-            )\n+            x = ivy.to_ivy(x_native)\nreturn x\nelse:\nreturn val\n", "fix_pattern": "<condition>: ivy.is_ivy_array(x)\n<pattern>: The code is checking if the input variable \"x\" is an Ivy array.\n<code_one>: raise ivy.exceptions.IvyException(...)\n<code_two>: x = ivy.to_ivy(x_native)\nFix_pattern: In the condition of ivy.is_ivy_array(x), if the input is not an Ivy array, raise an exception. Remove the line that raises the exception and replace it with x = ivy.to_ivy(x_native) to convert the input to an Ivy array and fix the API misuse."}
{"number": 5337, "change": "class RelationExtractor(flair.nn.DefaultClassifier):\n\nrelation_embeddings.append(embedding)\n\n-            # stack and drop out\n-            all_relations = torch.stack(relation_embeddings)\n+            # stack and drop out (squeeze and unsqueeze)\n+            all_relations = torch.stack(relation_embeddings).unsqueeze(1)\n\nall_relations = self.dropout(all_relations)\nall_relations = self.locked_dropout(all_relations)\nall_relations = self.word_dropout(all_relations)\n\n+            all_relations = all_relations.squeeze(1)\n+\n# send through decoder\nif self.non_linear_decoder:\nsentence_relation_scores = self.decoder_2(self.nonlinearity(self.decoder_1(all_relations)))\n", "fix_pattern": "<condition>: The code is trying to stack a tensor named \"relation_embeddings\".\n\n<pattern>: The code is removing the tensor stacking operation.\n\n<code_one>: \n```\nall_relations = torch.stack(relation_embeddings)\n```\n\n<code_two>: \n```\nall_relations = torch.stack(relation_embeddings).unsqueeze(1)\nall_relations = all_relations.squeeze(1)\n```\n\nFix_pattern: In the condition of attempting to stack a tensor named \"relation_embeddings\", if the tensor is detected, then add the tensor stacking operation with a call to `unsqueeze(1)` immediately after `torch.stack(relation_embeddings)` and then call `squeeze(1)`."}
{"number": 5340, "change": "class HullWhiteBermudanSwaptionTest(parameterized.TestCase, tf.test.TestCase):\nself.float_leg_end_times) - np.array(self.float_leg_start_times)\nself.fixed_leg_daycount_fractions = self.float_leg_daycount_fractions\nself.fixed_leg_coupon = 0.011 * np.ones_like(self.fixed_leg_payment_times)\n-    self.zero_rate_fn = lambda x: 0.01 * tf.ones_like(x)\n+    zero_rate_fn = lambda x: 0.01 * tf.expand_dims(tf.ones_like(x), axis=-1)\n+    self.zero_rate_fn = zero_rate_fn\n\nsuper(HullWhiteBermudanSwaptionTest, self).setUp()\n", "fix_pattern": "<condition>: No clear condition can be identified.\n<pattern>: Change the lambda function by adding tf.expand_dims and axis argument.\n<code_one>: self.zero_rate_fn = lambda x: 0.01 * tf.ones_like(x)\n<code_two>: zero_rate_fn = lambda x: 0.01 * tf.expand_dims(tf.ones_like(x), axis=-1)\nFix_pattern: In the condition of no clear condition, if the lambda function self.zero_rate_fn is defined as multiplying 0.01 with tf.ones_like(x), then change it to multiplying 0.01 with tf.expand_dims(tf.ones_like(x), axis=-1) to fix the API misuse."}
{"number": 5347, "change": "class TestSparseClipGrad(AllenNlpTestCase):\n# Now try to clip the gradients.\n_ = sparse_clip_norm([embedding.weight], 1.5)\n# Final norm should be 1.5\n-        grad = embedding.weight.grad.data.coalesce()\n-        self.assertAlmostEqual(grad._values().norm(2.0), 1.5, places=5) # pylint: disable=protected-access\n+        grad = embedding.weight.grad.coalesce()  # pylint: disable=no-member\n+        self.assertAlmostEqual(grad._values().norm(2.0).item(), 1.5, places=5) # pylint: disable=protected-access\n", "fix_pattern": "<condition>:\nThe condition is not clear from the provided context.\n\n<pattern>:\nThe pattern is to update the code that calculates the norm of the gradient.\n\n<code_one>:\ngrad._values().norm(2.0)\n\n<code_two>:\ngrad._values().norm(2.0).item()\n\nFix_pattern:\nIn the condition of (no specified condition), if the pattern of calculating the norm of the gradient using 'grad._values().norm(2.0)' is detected, then replace '<code_one>' with '<code_two>' to fix the API misuse."}
{"number": 5370, "change": "class CategoricalOneHotPolicy(StochasticPolicy):\n\ndef __init__(self, network, session, state, random, action_count=1, scope='policy'):\nwith tf.variable_scope(scope):\n-            action_layer = linear(layer_input=network.output, config={'num_outputs': action_count}, scope='outputs')\n-\n-            distribution = tf.nn.softmax(action_layer)\n-            sample = tf.map_fn(lambda t: tf.multinomial(logits=t, num_samples=1), elems=distribution, dtype=tf.int64)\n+            logits = linear(layer_input=network.output, config={'num_outputs': action_count}, scope='outputs')\n+            distribution = tf.nn.softmax(logits)\n+            sample = tf.map_fn(lambda t: tf.multinomial(logits=t, num_samples=1), elems=logits, dtype=tf.int64)\n\nsuper(CategoricalOneHotPolicy, self).__init__(network, [distribution, sample], session, state, random, action_count)\nself.dist = Categorical(random)\n", "fix_pattern": "<condition>: The condition in which the fix pattern is applied is when there is a need to change the usage of the API.\n\n<pattern>: The pattern that is detected is the usage of the deprecated API.\n\n<code_one>: The code that is removed is the old usage of the deprecated API.\n\n<code_two>: The code that is added is the new usage of the updated API.\n\nFix_pattern: In the condition of API deprecation, if the old API usage is detected, then the old code is removed and replaced with the new code to fix the API misuse."}
{"number": 5398, "change": "def main(args):\n\n# Get input and output tensors\nimages_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n+            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\nembeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\ntpr, fpr, accuracy, val, val_std, far = lfw.validate(sess, paths,\nactual_issame, args.seed, 60,\n-                images_placeholder, embeddings, nrof_folds=args.lfw_nrof_folds)\n+                images_placeholder, phase_train_placeholder, embeddings, nrof_folds=args.lfw_nrof_folds)\nprint('Accuracy: %1.3f+-%1.3f' % (np.mean(accuracy), np.std(accuracy)))\nprint('Validation rate: %2.5f+-%2.5f @ FAR=%2.5f' % (val, val_std, far))\n", "fix_pattern": "<condition>:\nThere is a missing input placeholder for the TensorFlow model.\n\n<pattern>:\nThe input placeholder for the model is added to the code.\n\n<code_one>:\nimages_placeholder, embeddings, nrof_folds=args.lfw_nrof_folds\n\n<code_two>:\nimages_placeholder, phase_train_placeholder, embeddings, nrof_folds=args.lfw_nrof_folds\n\nFix_pattern:\nIn the condition of a missing input placeholder, if the code containing the missing placeholder is detected, then the code containing the missing placeholder is added to fix the API misuse."}
{"number": 5438, "change": "def train_cifar():\noptimizer = optim.Adam(get_parameters(model), lr=3e-4)\nelse:\n#optimizer = optim.SGD(get_parameters(model), lr=0.001)\n-    optimizer = optim.SGD(get_parameters(model), lr=0.003, momentum=0.85, nesterov=True)\n+    optimizer = optim.SGD(get_parameters(model), lr=Tensor([0.003]).realize(), momentum=0.85, nesterov=True)\n\n# 97 steps in 2 seconds = 20ms / step\n# step is 1163.42 GOPS = 56 TFLOPS!!!, 41% of max 136\n", "fix_pattern": "<condition>: It is not clearly mentioned in the given context.\n<pattern>: The pattern is not explicitly mentioned in the given information.\n<code_one>: The code removed is optimizer = optim.SGD(get_parameters(model), lr=0.003, momentum=0.85, nesterov=True).\n<code_two>: The code added is optimizer = optim.SGD(get_parameters(model), lr=Tensor([0.003]).realize(), momentum=0.85, nesterov=True).\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then change the <code_one> to <code_two> to fix the API misuse."}
{"number": 5451, "change": "class NCSNpp(ModelMixin, ConfigMixin):\nfor i_level in reversed(range(self.num_resolutions)):\nfor i_block in range(num_res_blocks + 1):\nout_ch = nf * ch_mult[i_level]\n+                in_ch = in_ch + hs_c.pop()\nmodules.append(\nResnetBlock(\n-                        in_channels=in_ch + hs_c.pop(),\n+                        in_channels=in_ch,\nout_channels=out_ch,\ntemb_channels=4 * nf,\noutput_scale_factor=np.sqrt(2.0),\n", "fix_pattern": "Condition:\nThe condition in this case is not clear, as the provided code snippet does not give any context or information about the specific condition or issue being addressed.\n\nPattern:\nThe pattern that is detected is the removal of the `in_channels` parameter from the `ResnetBlock` instantiation and the addition of a modified `in_channels` parameter.\n\nCode One:\nThe code that is removed is `in_channels=in_ch + hs_c.pop()`.\n\nCode Two:\nThe code that is added is:\n```\nin_ch = in_ch + hs_c.pop()\nin_channels=in_ch,\n```\n\nFix Pattern:\nIn the condition of unknown, if the pattern of removing `in_channels=in_ch + hs_c.pop()` is detected, then the fix is to change the code to `in_ch = in_ch + hs_c.pop()` and add `in_channels=in_ch`."}
{"number": 5476, "change": "def ga_loc_target(gt_bboxes_list,\nall_ignore_map.append(ignore_map)\nfor img_id in range(img_per_gpu):\ngt_bboxes = gt_bboxes_list[img_id]\n-        scale = torch.sqrt((gt_bboxes[:, 2] - gt_bboxes[:, 0] + 1) *\n-                           (gt_bboxes[:, 3] - gt_bboxes[:, 1] + 1))\n+        scale = torch.sqrt((gt_bboxes[:, 2] - gt_bboxes[:, 0]) *\n+                           (gt_bboxes[:, 3] - gt_bboxes[:, 1]))\nmin_anchor_size = scale.new_full(\n(1, ), float(anchor_scale * anchor_strides[0]))\n# assign gt bboxes to different feature levels w.r.t. their scales\n", "fix_pattern": "<condition>:\nThe condition is not clearly stated in the code provided. No pre condition is needed.\n\n<pattern>:\nThe pattern that is detected is a calculation of the scale variable using torch.sqrt() to calculate the square root of the product of two differences: (gt_bboxes[:, 2] - gt_bboxes[:, 0] + 1) * (gt_bboxes[:, 3] - gt_bboxes[:, 1] + 1).\n\n<code_one>:\nThe original code that needs to be changed is:\nscale = torch.sqrt((gt_bboxes[:, 2] - gt_bboxes[:, 0] + 1) * (gt_bboxes[:, 3] - gt_bboxes[:, 1] + 1))\n\n<code_two>:\nThe corrected code should be:\nscale = torch.sqrt((gt_bboxes[:, 2] - gt_bboxes[:, 0]) * (gt_bboxes[:, 3] - gt_bboxes[:, 1]))\n\nFix_pattern:\nIn the condition where the calculation of the scale variable is performed, the original code incorrectly adds 1 to each difference before calculating the square root. To fix this API misuse, the code should be modified to remove the additional 1."}
{"number": 5478, "change": "def create_model(to_device=True, dim_in=None, dim_out=None):\nif 'classification' in cfg.dataset.task_type and dim_out == 2:\ndim_out = 1\n\n-    model = network_dict[cfg.model.type](dim_in=dim_in, dim_out=dim_out)\n+    model = GraphGymModule(dim_in, dim_out, cfg)\nif to_device:\nmodel.to(torch.device(cfg.device))\nreturn model\n", "fix_pattern": "<condition>: 'classification' in cfg.dataset.task_type and dim_out == 2\n<pattern>: API misuse\n<code_one>: model = network_dict[cfg.model.type](dim_in=dim_in, dim_out=dim_out)\n<code_two>: model = GraphGymModule(dim_in, dim_out, cfg)\nFix_pattern: In the condition of 'classification' in cfg.dataset.task_type and dim_out == 2, if API misuse is detected, then remove the code model = network_dict[cfg.model.type](dim_in=dim_in, dim_out=dim_out) and add the code model = GraphGymModule(dim_in, dim_out, cfg) to fix the API misuse."}
{"number": 5523, "change": "class MapGradient(GradientProcessor):\nfor grad, var in grads:\nif re.match(self.regex, var.op.name):\nmatched = True\n-                with tf.device(grad.device):\n-                    grad = self.func(grad, var)\n+                grad = self.func(grad, var)\nif grad is not None:\nret.append((grad, var))\nelse:\n", "fix_pattern": "<condition>:\nThe condition in this case is a regex match on the variable's operation name.\n\n<pattern>:\nThe pattern is that if the regex match is successful, a specific piece of code is removed.\n\n<code_one>:\nThe piece of code that is removed is:\n\n'''\nwith tf.device(grad.device):\n    grad = self.func(grad, var)\n'''\n\n<code_two>:\nThe code that is added is:\n\n'''\ngrad = self.func(grad, var)\n'''\n\nFix_pattern:\nIn the condition of the regex match on the variable's operation name, if the specific code block with the device assignment is detected, then the code block is removed and replaced with just the function call."}
{"number": 5532, "change": "def _compute_luts(tiles_x_im: torch.Tensor, num_bins: int = 256, clip: float = 4\nhistos: torch.Tensor = torch.empty((tiles.shape[0], num_bins), device=tiles.device)\nif not diff:\nfor i in range(tiles.shape[0]):\n-            histos[i] = torch.histc(tiles[i], bins=num_bins, min=0, max=1)\n+            histos[i] = _torch_histc_cast(tiles[i], bins=num_bins, min=0, max=1)\nelse:\nbins: torch.Tensor = torch.linspace(0, 1, num_bins, device=tiles.device)\nhistos = histogram(tiles, bins, torch.tensor(0.001)).squeeze()\n", "fix_pattern": "<condition>: The condition is \"if not diff\".\n\n<pattern>: The pattern is using torch.histc() function.\n\n<code_one>: The code that needs to be removed is \"histos[i] = torch.histc(tiles[i], bins=num_bins, min=0, max=1)\".\n\n<code_two>: The code that needs to be added is \"histos[i] = _torch_histc_cast(tiles[i], bins=num_bins, min=0, max=1)\".\n\nFix_pattern: In the condition of \"if not diff\", if the pattern of using torch.histc() function is detected, then remove the line \"histos[i] = torch.histc(tiles[i], bins=num_bins, min=0, max=1)\" and replace it with \"histos[i] = _torch_histc_cast(tiles[i], bins=num_bins, min=0, max=1)\" to fix the API misuse."}
{"number": 5535, "change": "class DetrEncoderLayer(nn.Module):\nhidden_states = residual + hidden_states\nhidden_states = self.final_layer_norm(hidden_states)\n\n-        if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\n-            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n-            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n+        if self.training:\n+            if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\n+                clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n+                hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n\noutputs = (hidden_states,)\n", "fix_pattern": "<condition>: There is no clear condition mentioned in the context.\n<pattern>: Checking for infinite or NaN values in the hidden_states tensor.\n<code_one>: Removing the if condition that checks for infinite or NaN values and clamping the hidden_states tensor.\n<code_two>: Adding an additional condition to check if the module is training before performing the checks and clamping.\nFix_pattern: In the condition of no clear condition, if there are infinite or NaN values detected in the hidden_states tensor, then remove the if condition that checks for these values and clamps the tensor. Instead, add an additional condition to check if the module is training before performing the checks and clamping."}
{"number": 5542, "change": "class StableDiffusionModelHijack:\nif len(emb.shape) == 1:\nemb = emb.unsqueeze(0)\n\n-            self.word_embeddings[name] = emb.detach()\n+            self.word_embeddings[name] = emb.detach().to(device)\nself.word_embeddings_checksums[name] = f'{const_hash(emb.reshape(-1)*100)&0xffff:04x}'\n\nids = tokenizer([name], add_special_tokens=False)['input_ids'][0]\n", "fix_pattern": "<condition>: The condition is that the length of the input 'emb' is 1.\n<pattern>: The pattern is that the code is missing a device specification when assigning 'emb' to 'self.word_embeddings[name]'.\n<code_one>: The code that is removed is \"self.word_embeddings[name] = emb.detach()\".\n<code_two>: The code that is added is \"self.word_embeddings[name] = emb.detach().to(device)\".\nFix_pattern: In the condition of the length of 'emb' being 1, if the code is missing a device specification when assigning 'emb' to 'self.word_embeddings[name]', then the fix is to change the code from \"self.word_embeddings[name] = emb.detach()\" to \"self.word_embeddings[name] = emb.detach().to(device)\"."}
{"number": 5579, "change": "class AdalamFilter:\n\"Please either provide orientations or set 'orientation_difference_threshold' to None to disable orientations filtering\"  # noqa: E501\n)\nk1, k2, d1, d2, o1, o2, s1, s2 = self.__to_torch(k1, k2, d1, d2, o1, o2, s1, s2)\n+        if len(d2) <= 1:\n+            return _no_match(d1)\ndistmat = dist_matrix(d1, d2, is_normalized=False)\ndd12, nn12 = torch.topk(distmat, k=2, dim=1, largest=False)  # (n1, 2)\n", "fix_pattern": "<condition>:\nlen(d2) <= 1\n\n<pattern>:\nThe pattern is checking whether the length of the variable d2 is less than or equal to 1.\n\n<code_one>:\n_no_match(d1)\n\n<code_two>:\nreturn _no_match(d1)\n\nFix_pattern:\nIn the condition of len(d2) <= 1, if the length of d2 is less than or equal to 1, then change the code _no_match(d1) to return _no_match(d1) to fix the API misuse."}
{"number": 5598, "change": "class Model:\nreturn ppgs, preds_ppg, logits_ppg, pred_spec, pred_mel\n\ndef loss_net2(self):\n-        loss_spec = tf.reduce_mean(tf.abs(self.pred_spec - self.y_spec))\n-        loss_mel = tf.reduce_mean(tf.abs(self.pred_mel - self.y_mel))\n+        loss_spec = tf.reduce_mean(tf.squared_difference(self.pred_spec, self.y_spec))\n+        loss_mel = tf.reduce_mean(tf.squared_difference(self.pred_mel, self.y_mel))\nloss = loss_spec + loss_mel\nreturn loss\n", "fix_pattern": "<condition>: The code is calculating loss using the absolute difference between predicted and target values.\n<pattern>: The pattern is to replace the absolute difference with the squared difference.\n<code_one>: `tf.reduce_mean(tf.abs(self.pred_spec - self.y_spec))` and `tf.reduce_mean(tf.abs(self.pred_mel - self.y_mel))`\n<code_two>: `tf.reduce_mean(tf.squared_difference(self.pred_spec, self.y_spec))` and `tf.reduce_mean(tf.squared_difference(self.pred_mel, self.y_mel))`\nFix_pattern: In the condition of calculating loss using the absolute difference, if the pattern of calculating loss is detected, then change the code from `<code_one>` to `<code_two>` to fix the API misuse."}
{"number": 5618, "change": "class Trainer(object):\n\ndef is_consistent(tensor):\nmax_abs_diff = torch.max(torch.abs(tensor - tensor[0]))\n-                return (max_abs_diff / (tensor[0] + 1e-6) < 1e-6).all()\n+                return (\n+                    not torch.isfinite(tensor).any()\n+                    or (max_abs_diff / (tensor[0] + 1e-6) < 1e-6).all()\n+                )\n\nif not is_consistent(self._grad_norm_buf):\npretty_detail = \"\\n\".join(\n", "fix_pattern": "<condition>: The condition is when the variable self._grad_norm_buf is not consistent.\n\n<pattern>: The pattern is to check if the maximum absolute difference (max_abs_diff) divided by (tensor[0] + 1e-6) is less than 1e-6.\n\n<code_one>: The code that needs to be removed is \"return (max_abs_diff / (tensor[0] + 1e-6) < 1e-6).all()\".\n\n<code_two>: The code that needs to be added is \"not torch.isfinite(tensor).any()\" before the return statement.\n\nFix_pattern: In the condition of self._grad_norm_buf inconsistency, if the pattern of checking max_abs_diff divided by (tensor[0] + 1e-6) being less than 1e-6 is detected, then remove the code \"return (max_abs_diff / (tensor[0] + 1e-6) < 1e-6).all()\" and add \"not torch.isfinite(tensor).any()\" before the return statement to fix the API misuse."}
{"number": 5637, "change": "def normalize_homography3d(dst_pix_trans_src_pix: torch.Tensor,\n# compute the transformation pixel/norm for src/dst\nsrc_norm_trans_src_pix: torch.Tensor = normal_transform_pixel3d(\nsrc_d, src_h, src_w).to(dst_pix_trans_src_pix)\n-    src_pix_trans_src_norm = torch.inverse(src_norm_trans_src_pix)\n+\n+    src_pix_trans_src_norm = _torch_inverse_cast(src_norm_trans_src_pix)\ndst_norm_trans_dst_pix: torch.Tensor = normal_transform_pixel3d(\ndst_d, dst_h, dst_w).to(dst_pix_trans_src_pix)\n# compute chain transformations\n", "fix_pattern": "<condition>: No pre condition is needed.\n<pattern>: A code line is removed and replaced with a modified version of the same code line.\n<code_one>: src_pix_trans_src_norm = torch.inverse(src_norm_trans_src_pix)\n<code_two>: src_pix_trans_src_norm = _torch_inverse_cast(src_norm_trans_src_pix)\nFix_pattern: In the condition where the code line `src_pix_trans_src_norm = torch.inverse(src_norm_trans_src_pix)` exists, the API `torch.inverse()` is misused and should be fixed. The fix pattern is to replace the line with `src_pix_trans_src_norm = _torch_inverse_cast(src_norm_trans_src_pix)`."}
{"number": 5655, "change": "def harmonic_mean(a, weights=None):\nreturn sum(weights) / sum(w/x for x, w in zip(a, weights))\n\n# torch utils\n-def get_optimizer(name, parameters, lr, betas=(0.9, 0.999)):\n+def get_optimizer(name, parameters, lr, betas=(0.9, 0.999), eps=1e-8):\nif name == 'sgd':\nreturn torch.optim.SGD(parameters, lr=lr)\nelif name == 'adagrad':\nreturn torch.optim.Adagrad(parameters, lr=lr)\nelif name == 'adam':\n-        return torch.optim.Adam(parameters, lr=lr, betas=betas) # use default lr\n+        return torch.optim.Adam(parameters, lr=lr, betas=betas, eps=eps)\nelif name == 'adamax':\nreturn torch.optim.Adamax(parameters) # use default lr\nelse:\n", "fix_pattern": "<condition>: The condition is when calling the `get_optimizer` function with the parameters `name`, `parameters`, `lr`, and `betas`.\n\n<pattern>: The pattern is that the argument `eps=1e-8` is added to the `torch.optim.Adam` function call.\n\n<code_one>: The code that was removed is the previous `torch.optim.Adam` function call in the `get_optimizer` function.\n\n<code_two>: The code that was added is the updated `torch.optim.Adam` function call in the `get_optimizer` function.\n\nFix_pattern: \n\nIn the condition of calling the `get_optimizer` function with the parameters `name`, `parameters`, `lr`, and `betas`, if there is a missing `eps` argument, then add the `eps=1e-8` argument to the `torch.optim.Adam` function to fix the API misuse."}
{"number": 5662, "change": "class FlatVarHelper(object):\nself.session = session\nshapes = map(get_shape, variables)\ntotal_size = sum(np.prod(shape) for shape in shapes)\n-        self.theta = theta = tf.placeholder(tf.float32, [total_size])\n+        self.theta = tf.placeholder(tf.float32, [total_size])\nstart = 0\nassigns = []\n\nfor (shape, variable) in zip(shapes, variables):\nsize = np.prod(shape)\n-            assigns.append(tf.assign(variable, tf.reshape(theta[start:start + size], shape)))\n+            assigns.append(tf.assign(variable, tf.reshape(self.theta[start:start + size], shape)))\nstart += size\n\nself.set_op = tf.group(*assigns)\n", "fix_pattern": "<condition>: The condition is that there is a need to assign a placeholder value to a variable.\n\n<pattern>: The pattern is that the code should be modified to use the correct placeholder value.\n\n<code_one>: The code that is removed is \"self.theta = theta = tf.placeholder(tf.float32, [total_size])\". \n\n<code_two>: The code that is added is \"self.theta = tf.placeholder(tf.float32, [total_size])\".\n\nFix_pattern: In the condition of needing to assign a placeholder value to a variable, if the code \"self.theta = theta = tf.placeholder(tf.float32, [total_size])\" is detected, then remove it and add the code \"self.theta = tf.placeholder(tf.float32, [total_size])\" to fix the API misuse."}
{"number": 5664, "change": "def matmul(\ndtype_from = tf.as_dtype(x1.dtype)\n\nif transpose_a:\n-        x1 = tf.transpose(x1)\n+        x1 = tf.linalg.matrix_transpose(x1)\nif transpose_b:\n-        x2 = tf.transpose(x2)\n+        x2 = tf.linalg.matrix_transpose(x2)\n\nif adjoint_a:\nx1 = tf.linalg.adjoint(x1)\n", "fix_pattern": "<condition>: The condition is when a transpose operation is performed on a tensor.\n\n<pattern>: The pattern is to replace the tf.transpose() function with tf.linalg.matrix_transpose().\n\n<code_one>: The code_one is tf.transpose(x1) \n\n<code_two>: The code_two is tf.linalg.matrix_transpose(x1)\n\nFix_pattern: In the condition of applying a transpose operation on a tensor, if tf.transpose() is detected, then replace tf.transpose() with tf.linalg.matrix_transpose() to fix the API misuse."}
{"number": 5669, "change": "class LongformerOnnxConfig(OnnxConfig):\n)\nimport torch\n\n+        # for some reason, replacing this code by inputs[\"global_attention_mask\"] = torch.randint(2, inputs[\"input_ids\"].shape, dtype=torch.int64)\n+        # makes the export fail randomly\ninputs[\"global_attention_mask\"] = torch.zeros_like(inputs[\"input_ids\"])\n# make every second token global\ninputs[\"global_attention_mask\"][:, ::2] = 1\n+\nreturn inputs\n", "fix_pattern": "<condition>: The code is trying to assign a value to a key in the inputs dictionary.\n\n<pattern>: The code is setting the value of the \"global_attention_mask\" key in the inputs dictionary.\n\n<code_one>: inputs[\"global_attention_mask\"] = torch.zeros_like(inputs[\"input_ids\"])\n\n<code_two>: inputs[\"global_attention_mask\"] = torch.randint(2, inputs[\"input_ids\"].shape, dtype=torch.int64)\n\nFix_pattern: In the condition of setting the value for the \"global_attention_mask\" key in the inputs dictionary, if the pattern of assigning zeros to the key is detected, then change the code to assign random integers between 0 and 1 to the key to fix the API misuse."}
{"number": 5680, "change": "def test_ellipsis_simplify():\ndef test_pointer_tensor_simplify():\n\"\"\"Test the simplification of PointerTensor\"\"\"\n\n-    alice = syft.VirtualWorker(id=\"alice\")\n+    alice = syft.VirtualWorker(syft.torch.hook, id=\"alice\")\ninput_tensor = PointerTensor(id=1000, location=alice, owner=alice)\n\noutput = _simplify(input_tensor)\n", "fix_pattern": "<condition>: When creating a `PointerTensor` object in the `test_ellipsis_simplify` function.\n<pattern>: The `alice` variable is assigned to `syft.VirtualWorker(id=\"alice\")`.\n<code_one>: `alice = syft.VirtualWorker(id=\"alice\")`\n<code_two>: `alice = syft.VirtualWorker(syft.torch.hook, id=\"alice\")`\nFix_pattern: In the condition of creating a `PointerTensor` object in the `test_ellipsis_simplify` function, if the `alice` variable is assigned to `syft.VirtualWorker(id=\"alice\")`, then change the assignment to `alice = syft.VirtualWorker(syft.torch.hook, id=\"alice\")` to fix the API misuse."}
{"number": 5702, "change": "class TFFlaubertMainLayer(TFXLMMainLayer):\nposition_ids = tf.expand_dims(tf.range(slen), axis=0)\nelse:\n# assert shape_list(position_ids) == [bs, slen]  # (slen, bs)\n-            tf.debugging.assert_equal(shape_list(position_ids), [bs, slen])\n+            tf.debugging.assert_equal(\n+                shape_list(position_ids), [bs, slen]\n+            ), f\"Position id shape {shape_list(position_ids)} and input shape {[bs, slen]} mismatched\"\n# position_ids = position_ids.transpose(0, 1)\n\n# langs\nif langs is not None:\n# assert shape_list(langs) == [bs, slen]  # (slen, bs)\n-            tf.debugging.assert_equal(shape_list(langs), [bs, slen])\n+            tf.debugging.assert_equal(\n+                shape_list(langs), [bs, slen]\n+            ), f\"Lang shape {shape_list(langs)} and input shape {[bs, slen]} mismatched\"\n# langs = langs.transpose(0, 1)\n\n# Prepare head mask if needed\n", "fix_pattern": "<condition>: N/A\n<pattern>: The pattern is that the code is asserting the shape of the \"position_ids\" and \"langs\" variables.\n<code_one>: \n```\ntf.debugging.assert_equal(shape_list(position_ids), [bs, slen])\ntf.debugging.assert_equal(shape_list(langs), [bs, slen])\n```\n<code_two>: \n```\ntf.debugging.assert_equal(shape_list(position_ids), [bs, slen]), f\"Position id shape {shape_list(position_ids)} and input shape {[bs, slen]} mismatched\"\ntf.debugging.assert_equal(shape_list(langs), [bs, slen]), f\"Lang shape {shape_list(langs)} and input shape {[bs, slen]} mismatched\"\n```\nFix_pattern: \nIn the condition of no pre condition is needed, if the pattern of asserting the shape of \"position_ids\" and \"langs\" is detected, then change the code from the original shape assertion to shape assertion with an error message."}
{"number": 5704, "change": "def get_detector(trained_model, device='cpu'):\nnet.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\nelse:\nnet.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n-        net = torch.nn.DataParallel(net)\n+        net = torch.nn.DataParallel(net).to(device)\ncudnn.benchmark = False\n\nnet.eval()\n", "fix_pattern": "<condition>: The condition is that the code block is executing when the device is not set to 'cpu'.\n\n<pattern>: The pattern is the removal of the line `net = torch.nn.DataParallel(net)`.\n\n<code_one>: The code that was removed is `net = torch.nn.DataParallel(net)`.\n\n<code_two>: The code that was added is `net = torch.nn.DataParallel(net).to(device)`.\n\nFix_pattern: In the condition of the device not being 'cpu', if the line `net = torch.nn.DataParallel(net)` is detected, then remove it and replace it with `net = torch.nn.DataParallel(net).to(device)` to fix the API misuse."}
{"number": 5723, "change": "class CheckpointMergerPipeline(DiffusionPipeline):\ntheta_0 = theta_0()\n\nupdate_theta_0 = getattr(module, \"load_state_dict\")\n-                    theta_1 = torch.load(checkpoint_path_1)\n+                    theta_1 = torch.load(checkpoint_path_1, map_location=\"cpu\")\n\n-                    theta_2 = torch.load(checkpoint_path_2) if checkpoint_path_2 else None\n+                    theta_2 = torch.load(checkpoint_path_2, map_location=\"cpu\") if checkpoint_path_2 else None\n\nif not theta_0.keys() == theta_1.keys():\nprint(\"SKIPPING ATTR \", attr, \" DUE TO MISMATCH\")\n", "fix_pattern": "<condition>: None\n\n<pattern>: API misuse\n\n<code_one>: torch.load(checkpoint_path_1)\n<code_two>: torch.load(checkpoint_path_1, map_location=\"cpu\")\n\nFix_pattern: In the condition of API misuse, if the pattern of torch.load with only one argument is detected, then modify the code by adding the map_location=\"cpu\" argument to fix the API misuse."}
{"number": 5744, "change": "class TowerContext(object):\nself._ctxs.append(tf.variable_scope(self._name))\nelse:\n# use existing variable scope\n+                reuse = self.index > 0 or (not self.is_training)\nself._ctxs.append(tf.variable_scope(\n-                    tf.get_variable_scope(), reuse=self.index > 0))\n+                    tf.get_variable_scope(), reuse=reuse))\nself._ctxs.append(tf.name_scope(self._name))\nself._ctxs.append(tf.device(self._device))\nfor c in self._ctxs:\n", "fix_pattern": "<condition>: Checking if a variable scope already exists.\n<pattern>: tf.get_variable_scope()\n<code_one>: reuse=self.index > 0\n<code_two>: reuse = self.index > 0 or (not self.is_training)\nFix_pattern: In the condition of checking if a variable scope already exists, if tf.get_variable_scope() is detected, then change reuse=self.index > 0 to reuse = self.index > 0 or (not self.is_training) to fix the API misuse."}
{"number": 5785, "change": "class NaturalGradient(Optimizer):\n#     tf.math.reduce_sum(input_tensor=(loss_grad * delta))\n#     for loss_grad, delta in zip(loss_gradients, estimated_deltas.values())\n# ])\n-                return estimated_deltas.fmap(function=tf_util.identity)\n+                return [tf_util.identity(input=delta) for delta in estimated_deltas.values()]\n\nif self.only_positive_updates:\n# Natural gradient step only works if constant > 0 (epsilon to avoid zero division)\n", "fix_pattern": "<condition>: self.only_positive_updates\n<pattern>: if constant > 0 (epsilon to avoid zero division)\n<code_one>: return estimated_deltas.fmap(function=tf_util.identity)\n<code_two>: return [tf_util.identity(input=delta) for delta in estimated_deltas.values()]\nFix_pattern: In the condition of `self.only_positive_updates`, if the constant is greater than 0 (with an epsilon to avoid zero division), the code `return estimated_deltas.fmap(function=tf_util.identity)` is changed to `return [tf_util.identity(input=delta) for delta in estimated_deltas.values()]` to fix the API misuse."}
{"number": 5807, "change": "class BoringModelTPU(BoringModel):\n@pl_multi_process_test\ndef test_model_tpu_one_core():\n\"\"\"Tests if device/debug flag is set correctely when training and after teardown for TPUSpawnPlugin.\"\"\"\n-    trainer = Trainer(tpu_cores=1, fast_dev_run=True, plugin=TPUSpawnPlugin(debug=True))\n+    trainer = Trainer(tpu_cores=1, fast_dev_run=True, strategy=TPUSpawnPlugin(debug=True))\n# assert training type plugin attributes for device setting\nassert isinstance(trainer.training_type_plugin, TPUSpawnPlugin)\nassert not trainer.training_type_plugin.on_gpu\nassert trainer.training_type_plugin.on_tpu\n-    assert trainer.training_type_plugin.root_device == torch.device(\"xla\")\n+    assert trainer.training_type_plugin.root_device == torch.device(\"xla\", index=1)\nmodel = BoringModelTPU()\ntrainer.fit(model)\nassert \"PT_XLA_DEBUG\" not in os.environ\n", "fix_pattern": "<condition>: The condition is not clearly specified in the given context.\n\n<pattern>: The pattern is a code change that involves removing the \"plugin=TPUSpawnPlugin(debug=True)\" parameter and its corresponding assertion in the code, and adding the \"strategy=TPUSpawnPlugin(debug=True)\" parameter and the updated assertion.\n\n<code_one>: The code to be removed is \"trainer = Trainer(tpu_cores=1, fast_dev_run=True, plugin=TPUSpawnPlugin(debug=True))\" and \"assert trainer.training_type_plugin.root_device == torch.device(\"xla\")\".\n\n<code_two>: The code to be added is \"trainer = Trainer(tpu_cores=1, fast_dev_run=True, strategy=TPUSpawnPlugin(debug=True))\" and \"assert trainer.training_type_plugin.root_device == torch.device(\"xla\", index=1)\".\n\nFix_pattern: In the condition of unspecified condition, if the pattern of \"plugin=TPUSpawnPlugin(debug=True)\" is detected, then the code \"trainer = Trainer(tpu_cores=1, fast_dev_run=True, plugin=TPUSpawnPlugin(debug=True))\" and \"assert trainer.training_type_plugin.root_device == torch.device(\"xla\")\" should be removed and replaced with \"trainer = Trainer(tpu_cores=1, fast_dev_run=True, strategy=TPUSpawnPlugin(debug=True))\" and \"assert trainer.training_type_plugin.root_device == torch.device(\"xla\", index=1)\". This fixes the API misuse."}
{"number": 5819, "change": "class TestAugmentationSequential:\ndata_keys=[\"input\"],\nrandom_apply=random_apply,\nreturn_transform=return_transform,\n+            same_on_batch=same_on_batch,\n)\nout = aug(inp)\nif aug.return_label:\nout, label = out\nif return_transform and isinstance(out, (tuple, list)):\nout = out[0]\n-        assert out.shape == inp.shape\n+        assert out.shape[-3:] == inp.shape[-3:]\nreproducibility_test(inp, aug)\n\n@pytest.mark.parametrize('random_apply', [1, (2, 2), (1, 2), (2,), 10, True, False])\n", "fix_pattern": "<condition>:\nThe condition is when the test augmentation sequential class is used and the parameter random_apply is set.\n\n<pattern>:\nThe pattern is to check if the shape of the output is equal to the shape of the input.\n\n<code_one>:\nThe code that was removed is the assertion statement `assert out.shape == inp.shape`.\n\n<code_two>:\nThe code that was added is the assertion statement `assert out.shape[-3:] == inp.shape[-3:]`.\n\nFix_pattern:\nIn the condition of using the test augmentation sequential class with the random_apply parameter, if the shape of the output is not equal to the shape of the input, then change the code `assert out.shape == inp.shape` to `assert out.shape[-3:] == inp.shape[-3:]` to fix the API misuse."}
{"number": 5824, "change": "def compute_q_noisy_max_torch(counts, noise_eps):\n\nif type(counts) != torch.tensor:\n\n-        counts = torch.tensor(counts, dtype=torch.float)\n+        counts = torch.tensor(tensors_to_literals(counts), dtype=torch.float)\n\n_, winner = counts.max(0)\ncounts_normalized = noise_eps * (\n", "fix_pattern": "<condition>: The condition is that the variable \"counts\" is not of type torch.tensor.\n<pattern>: The pattern detected is that \"counts\" needs to be converted to a torch.tensor.\n<code_one>: The code that was removed is \"counts = torch.tensor(counts, dtype=torch.float)\".\n<code_two>: The code that was added is \"counts = torch.tensor(tensors_to_literals(counts), dtype=torch.float)\".\nFix_pattern: In the condition of \"counts\" not being a torch.tensor, the fix is to convert \"counts\" to a torch.tensor using the code \"counts = torch.tensor(tensors_to_literals(counts), dtype=torch.float)\"."}
{"number": 5870, "change": "class Model(ModelDesc):\nisTrain = get_current_tower_context().is_training\nif isTrain:\n# beam search is too slow to run in training\n-            predictions = tf.to_int32(\n-                tf.nn.ctc_greedy_decoder(logits, seqlen)[0][0])\n+            predictions = tf.cast(\n+                tf.nn.ctc_greedy_decoder(logits, seqlen)[0][0], tf.int32)\nelse:\n-            predictions = tf.to_int32(\n-                tf.nn.ctc_beam_search_decoder(logits, seqlen)[0][0])\n+            predictions = tf.cast(\n+                tf.nn.ctc_beam_search_decoder(logits, seqlen)[0][0], tf.int32)\nerr = tf.edit_distance(predictions, label, normalize=True)\nerr.set_shape([None])\nerr = tf.reduce_mean(err, name='error')\n", "fix_pattern": "<condition>: `isTrain` is `True`\n<pattern>: `tf.to_int32(tf.nn.ctc_greedy_decoder(logits, seqlen)[0][0])` and `tf.to_int32(tf.nn.ctc_beam_search_decoder(logits, seqlen)[0][0])` are used to convert predictions to `tf.int32` type.\n<code_one>: `tf.to_int32(tf.nn.ctc_greedy_decoder(logits, seqlen)[0][0])` and `tf.to_int32(tf.nn.ctc_beam_search_decoder(logits, seqlen)[0][0])`\n<code_two>: `tf.cast(tf.nn.ctc_greedy_decoder(logits, seqlen)[0][0], tf.int32)` and `tf.cast(tf.nn.ctc_beam_search_decoder(logits, seqlen)[0][0], tf.int32)`\nFix_pattern: In the condition of `isTrain = True`, if `tf.to_int32(tf.nn.ctc_greedy_decoder(logits, seqlen)[0][0])` and `tf.to_int32(tf.nn.ctc_beam_search_decoder(logits, seqlen)[0][0])` are detected, then change `tf.to_int32(tf.nn.ctc_greedy_decoder(logits, seqlen)[0][0])` to `tf.cast(tf.nn.ctc_greedy_decoder(logits, seqlen)[0][0], tf.int32)` and change `tf.to_int32(tf.nn.ctc_beam_search_decoder(logits, seqlen)[0][0])` to `tf.cast(tf.nn.ctc_beam_search_decoder(logits, seqlen)[0][0], tf.int32)` to fix the API misuse."}
{"number": 5883, "change": "class PytorchGraphTest(unittest.TestCase):\nclass myLinear(torch.nn.Module):\ndef __init__(self):\nsuper(myLinear, self).__init__()\n-                self.l = torch.nn.Linear(3, 5)\n+                self.linear = torch.nn.Linear(3, 5)\n\ndef forward(self, x):\n-                return self.l(x)\n+                return self.linear(x)\n\nwith SummaryWriter(comment='LinearModel') as w:\nw.add_graph(myLinear(), dummy_input, True)\n\ndef test_wrong_input_size(self):\nprint('expect error here:')\n-        with self.assertRaises(RuntimeError) as e_info:\n+        with self.assertRaises(TypeError):\ndummy_input = torch.rand(1, 9)\nmodel = torch.nn.Linear(3, 5)\nwith SummaryWriter(comment='expect_error') as w:\n", "fix_pattern": "<condition>: The code is trying to add a graph to TensorBoard using SummaryWriter.\n<pattern>: The linear model used in the graph is incorrectly defined.\n<code_one>: self.l = torch.nn.Linear(3, 5)\n<code_two>: self.linear = torch.nn.Linear(3, 5)\nFix_pattern: In the condition of attempting to add a graph to TensorBoard using SummaryWriter, if the linear model definition is incorrect, then change the code from \"self.l = torch.nn.Linear(3, 5)\" to \"self.linear = torch.nn.Linear(3, 5)\" to fix the API misuse."}
{"number": 5886, "change": "class MaskRCNN():\n# TODO: can this be optimized to avoid duplicating the anchors?\nanchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape)\n# A hack to get around Keras's bad support for constants\n-            anchors = KL.Lambda(lambda x: tf.constant(anchors), name=\"anchors\")(input_image)\n+            anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=\"anchors\")(input_image)\nelse:\nanchors = input_anchors\n", "fix_pattern": "<condition>: The condition is when there is a need to manipulate or modify a variable named \"anchors\".\n\n<pattern>: The pattern is to change the way the \"anchors\" variable is assigned or created.\n\n<code_one>: The original code assigned the \"anchors\" variable using a Lambda function and the tf.constant method.\n\n<code_two>: The fix requires changing the assignment to use a Lambda function and the tf.Variable method.\n\nFix_pattern:\nIn the condition of needing to manipulate the \"anchors\" variable, if the pattern of assigning the variable using a Lambda function and the tf.constant method is detected, then the fix is to remove that code and add a new code that assigns the \"anchors\" variable using a Lambda function and the tf.Variable method."}
{"number": 5932, "change": "class InvConvNear(nn.Module):\nreturn z, logdet\n\ndef store_inverse(self):\n-        self.weight_inv = torch.inverse(\n+        weight_inv = torch.inverse(\nself.weight.float()).to(dtype=self.weight.dtype)\n+        self.weight_inv = nn.Parameter(weight_inv, requires_grad=False)\n\n\nclass CouplingBlock(nn.Module):\n", "fix_pattern": "<condition>: The condition of the fix pattern is when there is code that calculates the inverse of a matrix in the context.\n\n<pattern>: The pattern detected is the use of the torch.inverse() function.\n\n<code_one>: The code that is removed is self.weight_inv = torch.inverse(\n\n<code_two>: The code that is added is weight_inv = torch.inverse(\n        self.weight_inv = nn.Parameter(weight_inv, requires_grad=False)\n\nFix_pattern: In the condition of calculating the inverse of a matrix using torch.inverse(), the fix pattern is to remove the code that assigns the inverse to a variable outside the function and instead assign it directly to a class parameter, using nn.Parameter()."}
{"number": 6008, "change": "class Imagen(BaseGaussianDiffusion):\ndevice = next(self.parameters()).device\n\nlowres_sample_noise_level = default(lowres_sample_noise_level, self.lowres_sample_noise_level)\n-        lowres_noise_times = torch.full((batch_size,), lowres_sample_noise_level, device = device, dtype = torch.long)\n+        lowres_noise_times = torch.full((batch_size,), int(lowres_sample_noise_level * self.num_timesteps), device = device, dtype = torch.long)\n\nfor unet_number, unet, channel, image_size, learned_variance in tqdm(zip(range(1, len(self.unets) + 1), self.unets, self.sample_channels, self.image_sizes, self.learned_variance)):\n", "fix_pattern": "<condition>:\nThe condition is when the variable \"lowres_sample_noise_level\" is being used.\n\n<pattern>:\nThe pattern is that the value of \"lowres_sample_noise_level\" needs to be multiplied by \"self.num_timesteps\" to fix the API misuse.\n\n<code_one>:\nThe code that needs to be removed is:\n\"lowres_noise_times = torch.full((batch_size,), lowres_sample_noise_level, device = device, dtype = torch.long)\"\n\n<code_two>:\nThe code that needs to be added is:\n\"lowres_noise_times = torch.full((batch_size,), int(lowres_sample_noise_level * self.num_timesteps), device = device, dtype = torch.long)\"\n\nFix_pattern:\nIn the condition of \"lowres_sample_noise_level\" being used, if the pattern of not multiplying it by \"self.num_timesteps\" is detected, then change the code \"lowres_noise_times = torch.full((batch_size,), lowres_sample_noise_level, device = device, dtype = torch.long)\" to \"lowres_noise_times = torch.full((batch_size,), int(lowres_sample_noise_level * self.num_timesteps), device = device, dtype = torch.long)\" to fix the API misuse."}
{"number": 6026, "change": "def any(\nkeepdims: bool = False,\nout: Optional[torch.Tensor] = None,\n) -> torch.Tensor:\n-    x = ivy.asarray(x).type(torch.bool)\n+    x = torch.as_tensor(x).type(torch.bool)\nif axis is None:\nnum_dims = len(x.shape)\naxis = list(range(num_dims))\n", "fix_pattern": "<condition>: axis is None\n<pattern>: Converting a variable to a specific data type\n<code_one>: x = ivy.asarray(x).type(torch.bool)\n<code_two>: x = torch.as_tensor(x).type(torch.bool)\nFix_pattern: In the condition of `axis is None`, if converting `x` to `torch.BoolTensor` is detected, then change the line `x = ivy.asarray(x).type(torch.bool)` to `x = torch.as_tensor(x).type(torch.bool)` to fix the API misuse."}
{"number": 6071, "change": "def batched_forward(\nif st >= end:\ncontinue\nout_list.append(model_dev(data[st:end].to(device), **kwargs))\n-        out = torch.cat(out_list, dim=0)\n+        out = concatenate(out_list, 0)\nreturn out.to(data.device)\nreturn model(data, **kwargs)\n", "fix_pattern": "<condition>: The condition is when the variable \"st\" is greater than or equal to \"end\".\n<pattern>: The pattern is the misuse of the torch.cat function.\n<code_one>: The code that is removed is \"out = torch.cat(out_list, dim=0)\".\n<code_two>: The code that is added is \"out = concatenate(out_list, 0)\".\nFix_pattern: In the condition of \"if st >= end\", if the misuse of the torch.cat function is detected, then remove the code \"out = torch.cat(out_list, dim=0)\" and change it to \"out = concatenate(out_list, 0)\" to fix the API misuse."}
{"number": 6080, "change": "class EfficientFormer(nn.Module):\ndef get_classifier(self):\nreturn self.head, self.head_dist\n\n-    def reset_classifier(self, num_classes, global_pool=None, distillation=None):\n+    def reset_classifier(self, num_classes, global_pool=None):\nself.num_classes = num_classes\nif global_pool is not None:\nself.global_pool = global_pool\nself.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n-        if self.dist:\n-            self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n+        self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n@torch.jit.ignore\ndef set_distilled_training(self, enable=True):\n", "fix_pattern": "<condition>: The condition is when the `reset_classifier` method is called with the parameter `distillation` in the `EfficientFormer` class.\n\n<pattern>: The pattern that is detected is the presence of `self.dist` in the if statement within the `reset_classifier` method.\n\n<code_one>: The code that is removed is `self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()`.\n\n<code_two>: The code that is added is `self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()`.\n\nFix_pattern: In the condition of `reset_classifier` method in the `EfficientFormer` class, if the pattern `self.dist` is detected, then remove the code `self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()` and add the same code in order to fix the API misuse."}
{"number": 6084, "change": "class PretrainedTransformerEmbedder(TokenEmbedder):\ndef get_output_dim(self):\nreturn self.output_dim\n\n-    def forward(self, token_ids: torch.LongTensor) -> torch.Tensor:  # type: ignore\n+    def forward(\n+        self, token_ids: torch.LongTensor, attention_mask: torch.LongTensor\n+    ) -> torch.Tensor:  # type: ignore\n\n-        return self.transformer_model(token_ids)[0]\n+        return self.transformer_model(input_ids=token_ids, attention_mask=attention_mask)[0]\n", "fix_pattern": "<condition>: The existing forward method only takes a single argument, token_ids.\n\n<pattern>: A second argument, attention_mask, needs to be added to the forward method.\n\n<code_one>: \n```\ndef forward(self, token_ids: torch.LongTensor) -> torch.Tensor:  # type: ignore\n    return self.transformer_model(token_ids)[0]\n```\n\n<code_two>:\n```\ndef forward(\n    self, token_ids: torch.LongTensor, attention_mask: torch.LongTensor\n) -> torch.Tensor:  # type: ignore\n    return self.transformer_model(input_ids=token_ids, attention_mask=attention_mask)[0]\n```\n\nFix_pattern:\nIn the condition of having a forward method that takes only token_ids as argument, if the pattern of missing attention_mask is detected, then change the existing forward method (<code_one>) to include the attention_mask argument as well (<code_two>) to fix the API misuse."}
{"number": 6099, "change": "torch_scatter = None\ndef dev(x: torch.Tensor, as_native: bool = False) -> Union[ivy.Device, torch.device]:\ndv = x.device\nif as_native:\n-        return torch.device(dv.type.replace(\"gpu\", \"cuda\"))\n+        if isinstance(dv, torch.device):\n+            dv = dv.type\n+        return torch.device(dv.replace(\"gpu\", \"cuda\"))\nreturn as_ivy_dev(dv)\n", "fix_pattern": "<condition>: The condition of the fix pattern is the variable `as_native` being `True`.\n<pattern>: The pattern that is detected is the usage of `torch.device(dv.type.replace(\"gpu\", \"cuda\"))`.\n<code_one>: The code that is removed is `return torch.device(dv.type.replace(\"gpu\", \"cuda\"))`.\n<code_two>: The code that is added is:\n```\nif isinstance(dv, torch.device):\n    dv = dv.type\nreturn torch.device(dv.replace(\"gpu\", \"cuda\"))\n```\nFix_pattern: In the condition of `as_native` being `True`, if the pattern of using `torch.device(dv.type.replace(\"gpu\", \"cuda\"))` is detected, then change the code from `return torch.device(dv.type.replace(\"gpu\", \"cuda\"))` to the added code snippet. This fix pattern fixes the API misuse."}
{"number": 6112, "change": "def median(\ntemp = input\nif hasattr(axis, \"__iter__\"):\nfor dim in axis:\n-            temp = torch.median(\n+            temp = torch.quantile(\ntemp,\n+                0.5,\ndim=dim,\nkeepdim=keepdims,\n)[0]\n-        return input\n+        return temp\nelse:\n-        return torch.median(\n+        return torch.quantile(\ninput,\n+            0.5,\ndim=axis,\nkeepdim=keepdims,\n)[0]\n", "fix_pattern": "<condition>: The condition is checking if the \"axis\" variable has the attribute \"__iter__\", which means it is an iterable.\n\n<pattern>: The pattern is the misuse of the \"torch.median\" function, which should be replaced with the \"torch.quantile\" function.\n\n<code_one>: The code being removed is the incorrect use of the \"torch.median\" function.\n\n<code_two>: The code being added is the correct use of the \"torch.quantile\" function.\n\nFix_pattern: In the condition of checking if the \"axis\" variable is iterable, if the misuse of the \"torch.median\" function is detected, then replace the incorrect usage with the \"torch.quantile\" function to fix the API misuse."}
{"number": 6121, "change": "def test_welford_dense(n_samples, dim_size):\nsamples.append(sample)\nw.update(sample)\n\n-    sample_cov = np.cov(torch.stack(samples).data.numpy(), bias=False, rowvar=False)\n-    estimates = w.get_covariance(regularize=False).data.numpy()\n+    sample_cov = np.cov(torch.stack(samples).data.cpu().numpy(), bias=False, rowvar=False)\n+    estimates = w.get_covariance(regularize=False).data.cpu().numpy()\nassert_equal(estimates, sample_cov)\n", "fix_pattern": "<condition>: There is a need to compute the covariance matrix and estimates.\n<pattern>: The code used for computing the covariance matrix and estimates needs to be modified.\n<code_one>: np.cov(torch.stack(samples).data.numpy(), bias=False, rowvar=False)\n<code_two>: np.cov(torch.stack(samples).data.cpu().numpy(), bias=False, rowvar=False)\nFix_pattern: In the condition of needing to compute the covariance matrix and estimates, if the code <code_one> is detected, then change it to <code_two> to fix the API misuse."}
{"number": 6133, "change": "def mean_iou(\n# TODO: is it possible to vectorize this ?\n# iterate over classes\nfor class_id in range(num_classes):\n-        tp: torch.Tensor = conf_mat[..., class_id, class_id].float()\n+        tp: torch.Tensor = conf_mat[..., None, class_id, class_id]\ntotal = torch.sum(conf_mat[..., class_id, :], dim=-1, keepdim=True) + \\\ntorch.sum(conf_mat[..., :, class_id], dim=-1, keepdim=True)\niou_val: torch.Tensor = tp / (total.float() - tp + 1e-6)\n", "fix_pattern": "<condition>: The condition is not explicitly mentioned in the given code snippet. It is possible that the condition is related to the calculation of mean IoU for multiple classes.\n\n<pattern>: The pattern that is detected is the need to adjust the dimensions of `conf_mat` tensor.\n\n<code_one>: The code that is removed is `tp: torch.Tensor = conf_mat[..., class_id, class_id].float()`\n\n<code_two>: The code that is added is `tp: torch.Tensor = conf_mat[..., None, class_id, class_id]`\n\nFix_pattern: In the condition of calculating mean IoU for multiple classes, if the dimensions of `conf_mat` tensor need adjustment, then remove the line `tp: torch.Tensor = conf_mat[..., class_id, class_id].float()` and add the line `tp: torch.Tensor = conf_mat[..., None, class_id, class_id]` to fix the API misuse."}
{"number": 6219, "change": "class BeamSearch(Search):\nscores_buf = top_prediction[0]\nindices_buf = top_prediction[1]\n# Project back into relative indices and beams\n-        beams_buf = indices_buf // vocab_size\n+        beams_buf = torch.div(indices_buf, vocab_size, rounding_mode='trunc')\nindices_buf = indices_buf.fmod(vocab_size)\n\n# At this point, beams_buf and indices_buf are single-dim and contain relative indices\n", "fix_pattern": "<condition>: When performing calculations on indices that are relative to a vocabulary size in the context of BeamSearch.\n\n<pattern>: The code was using integer division (//) to calculate the beams_buf, which resulted in rounding towards negative infinity.\n\n<code_one>: beams_buf = indices_buf // vocab_size\n\n<code_two>: beams_buf = torch.div(indices_buf, vocab_size, rounding_mode='trunc')\n\nFix_pattern: In the condition of performing calculations on indices that are relative to a vocabulary size, if integer division (//) is detected, then change the code from using integer division to using torch.div with rounding_mode='trunc' to fix the API misuse."}
{"number": 6260, "change": "class NanDetector:\ndef _detect(self, tensor, name, backward):\nerr = None\nif (\n-            tensor.numel() >= 2\n-        ):  # single value tensors (like the loss) will not provide much info\n+            torch.is_floating_point(tensor)\n+            # single value tensors (like the loss) will not provide much info\n+            and tensor.numel() >= 2\n+        ):\nwith torch.no_grad():\nif torch.isnan(tensor).any():\nerr = \"NaN\"\n", "fix_pattern": "<condition>: The fix is applied in the condition of a specific if statement.\n<pattern>: The pattern detected is the use of the \"torch.isnan(tensor).any()\" function.\n<code_one>: The code being removed is \"tensor.numel() >= 2\" which checks if the tensor has more than 2 elements.\n<code_two>: The code being added is \"torch.is_floating_point(tensor)\" which checks if the tensor is a floating-point tensor.\nFix_pattern: In the condition of the if statement, if the pattern \"torch.isnan(tensor).any()\" is detected, then remove the code \"tensor.numel() >= 2\" and add the code \"torch.is_floating_point(tensor)\" to fix the API misuse."}
{"number": 6262, "change": "class MetaLayer(torch.nn.Module):\n# u: [B, F_u]\n# batch: [N] with max entry B - 1.\nrow, col = edge_index\n-                out = torch.cat([x[col], edge_attr], dim=1)\n+                out = torch.cat([x[row], edge_attr], dim=1)\nout = self.node_mlp_1(out)\n-                out = scatter_mean(out, row, dim=0, dim_size=x.size(0))\n+                out = scatter_mean(out, col, dim=0, dim_size=x.size(0))\nout = torch.cat([x, out, u[batch]], dim=1)\nreturn self.node_mlp_2(out)\n", "fix_pattern": "<condition>: The condition is not clearly mentioned in the given context.\n\n<pattern>: The pattern is to change the indices used in the `torch.cat` and `scatter_mean` functions.\n\n<code_one>: The original code uses `x[col]` and `row` as indices in the `torch.cat` function and `scatter_mean` function.\n\n<code_two>: The fixed code uses `x[row]` and `col` as indices in the `torch.cat` function and `scatter_mean` function.\n\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then change the indices used in the `torch.cat` and `scatter_mean` functions from <code_one> to <code_two> to fix the API misuse."}
{"number": 6277, "change": "def mu_law_encode(audio, quantization_channels):\nwith tf.name_scope('encode'):\nmu = quantization_channels - 1\n# Perform mu-law companding transformation (ITU-T, 1988).\n-        magnitude = tf.log(1 + mu * tf.abs(audio)) / tf.log(1. + mu)\n+        # Minimum operation is here to deal with rare large amplitudes caused by resampling.\n+        magnitude = tf.log(1 + mu * tf.minimum(tf.abs(audio), 1.0)) / tf.log(1. + mu)\nsignal = tf.sign(audio) * magnitude\n# Quantize signal to the specified number of levels.\nreturn tf.cast((signal + 1) / 2 * mu + 0.5, tf.int32)\n", "fix_pattern": "Condition: The code is performing mu-law companding transformation on audio data using TensorFlow.\nPattern: The pattern detected is that the code is calculating the magnitude of the audio signal using the formula `tf.log(1 + mu * tf.abs(audio)) / tf.log(1. + mu)`.\nCode One: `magnitude = tf.log(1 + mu * tf.abs(audio)) / tf.log(1. + mu)`\nCode Two: `# Minimum operation is here to deal with rare large amplitudes caused by resampling.\nmagnitude = tf.log(1 + mu * tf.minimum(tf.abs(audio), 1.0)) / tf.log(1. + mu)`\nFix Pattern: In the condition of performing mu-law companding transformation, if the pattern of calculating the magnitude of the audio signal is detected, then change the code from Code One to Code Two to fix the API misuse. Specifically, replace `tf.abs(audio)` with `tf.minimum(tf.abs(audio), 1.0)` in the calculation of the magnitude to handle rare large amplitudes caused by resampling."}
{"number": 6316, "change": "class GARPNHead(GuidedAnchorHead):\nif cfg.min_bbox_size > 0:\nw = proposals[:, 2] - proposals[:, 0]\nh = proposals[:, 3] - proposals[:, 1]\n-                valid_inds = torch.nonzero((w >= cfg.min_bbox_size) &\n-                                           (h >= cfg.min_bbox_size)).squeeze()\n+                valid_inds = torch.nonzero(\n+                    (w >= cfg.min_bbox_size) & (h >= cfg.min_bbox_size),\n+                    as_tuple=False).squeeze()\nproposals = proposals[valid_inds, :]\nscores = scores[valid_inds]\nproposals = torch.cat([proposals, scores.unsqueeze(-1)], dim=-1)\n", "fix_pattern": "<condition>: The minimum bounding box size (cfg.min_bbox_size) is greater than 0.\n<pattern>: Detecting valid indices where both the width (w) and height (h) of the proposals are greater than or equal to cfg.min_bbox_size.\n<code_one>: (w >= cfg.min_bbox_size) & (h >= cfg.min_bbox_size)\n<code_two>: (w >= cfg.min_bbox_size) & (h >= cfg.min_bbox_size), as_tuple=False\nFix_pattern: In the condition of cfg.min_bbox_size > 0, if (w >= cfg.min_bbox_size) & (h >= cfg.min_bbox_size) is detected, then change the code_one to code_two to fix the API misuse."}
{"number": 6376, "change": "class VideoSequential(ImageSequential):\ndata_format: str = \"BTCHW\",\nsame_on_frame: bool = True,\nrandom_apply: Union[int, bool, Tuple[int, int]] = False,\n+        random_apply_weights: Optional[List[float]] = None,\n) -> None:\n-        super().__init__(*args, same_on_batch=None, return_transform=None, keepdim=None, random_apply=random_apply)\n+        super().__init__(\n+            *args, same_on_batch=None, return_transform=None, keepdim=None, random_apply=random_apply,\n+            random_apply_weights=random_apply_weights\n+        )\nself.same_on_frame = same_on_frame\nself.data_format = data_format.upper()\nif self.data_format not in [\"BCTHW\", \"BTCHW\"]:\n", "fix_pattern": "<condition>: The condition is that the \"data_format\" variable should be either \"BCTHW\" or \"BTCHW\".\n\n<pattern>: The pattern is that the \"random_apply\" argument in the initialization of the class should be changed.\n\n<code_one>: The code that needs to be removed is \"super().__init__(*args, same_on_batch=None, return_transform=None, keepdim=None, random_apply=random_apply)\".\n\n<code_two>: The code that needs to be added is the following:\n```\n        random_apply_weights: Optional[List[float]] = None,\n        super().__init__(\n            *args, same_on_batch=None, return_transform=None, keepdim=None, random_apply=random_apply,\n            random_apply_weights=random_apply_weights\n        )\n```\n\nFix_pattern: In the condition of the \"data_format\", if the \"random_apply\" argument is detected, then change the code from <code_one> to <code_two> to fix the API misuse."}
{"number": 6382, "change": "def model(X, params, mesh, labels=None, past=None, scope='model', reuse=False, t\n# wpe has shape [ctx, embd]\n# positions_for would have shape [batch, seq]\n# h has shape [batch, seq, embd]\n-        zerodim = mtf.Dimension('singleton', 0)\n\n-        h = mtf.gather(wte, X, zerodim) + mtf.gather(wpe, positions_for(X, past_length, batch_dim), zerodim)\n+        h = mtf.gather(wte, X, vocab_dim) + mtf.gather(wpe, positions_for(X, past_length, batch_dim), vocab_dim)\n\n# Transformer\npresents = []\n", "fix_pattern": "<condition>: This fix pattern does not have a specific condition.\n\n<pattern>: The pattern detected is a code change involving two lines of code. The first line is a gather operation involving the variables 'wte' and 'X', and the second line is a gather operation involving the variables 'wpe' and 'positions_for(X, past_length, batch_dim)'.\n\n<code_one>: The code being removed is \"zerodim = mtf.Dimension('singleton', 0)\\nh = mtf.gather(wte, X, zerodim) + mtf.gather(wpe, positions_for(X, past_length, batch_dim), zerodim)\".\n\n<code_two>: The code being added is \"h = mtf.gather(wte, X, vocab_dim) + mtf.gather(wpe, positions_for(X, past_length, batch_dim), vocab_dim)\".\n\nFix_pattern: \nIn the condition of no specific condition, if the pattern of gathering 'wte' and 'X' using 'zerodim', and gathering 'wpe' and 'positions_for(X, past_length, batch_dim)' using 'zerodim' is detected, then remove the code \"zerodim = mtf.Dimension('singleton', 0)\", change the gather operations to use 'vocab_dim' instead of 'zerodim', and update the variable assignment for 'h' accordingly. This fix pattern aims to fix the API misuse by using the correct dimension for the gather operations."}
{"number": 6415, "change": "def run_api_experiment(input_features, output_features, dataset, **kwargs):\nloaded_state = loaded_model.model.state_dict()\nbcast_state = hvd.broadcast_object(loaded_state)\nfor loaded, bcast in zip(loaded_state.values(), bcast_state.values()):\n-            assert np.allclose(loaded, bcast)\n+            assert torch.allclose(loaded, bcast)\nfinally:\nif output_dir:\nshutil.rmtree(output_dir, ignore_errors=True)\n", "fix_pattern": "<condition>:\nThe condition here is when working with a distributed framework like Horovod.\n\n<pattern>:\nThe pattern detected is an assertion that checks if two arrays, 'loaded' and 'bcast', are close to each other.\n\n<code_one>:\nThe code that was removed is the line with the assert statement:\n```\nassert np.allclose(loaded, bcast)\n```\n\n<code_two>:\nThe code that was added is a modified version of the assert statement that uses the torch module:\n```\nassert torch.allclose(loaded, bcast)\n```\n\nFix_pattern:\nIn the condition of using Horovod, if an assertion is used to check the closeness of two arrays, then the code is modified to use the torch module instead of the numpy module for array comparison to fix the API misuse."}
{"number": 6447, "change": "import logging\nlogger = logging.getLogger(__name__)\n\nTorchTrainer = None\n-TorchTrainable = None\nTrainingOperator = None\n+BaseTorchTrainable = None\n\ntry:\nimport torch  # noqa: F401\n\n-    from ray.util.sgd.torch.torch_trainer import (TorchTrainer, TorchTrainable)\n+    from ray.util.sgd.torch.torch_trainer import (TorchTrainer,\n+                                                  BaseTorchTrainable)\n\nfrom ray.util.sgd.torch.training_operator import TrainingOperator\n\n-    __all__ = [\"TorchTrainer\", \"TorchTrainable\", \"TrainingOperator\"]\n+    __all__ = [\"TorchTrainer\", \"BaseTorchTrainable\", \"TrainingOperator\"]\nexcept ImportError:\nlogger.warning(\"PyTorch not found. TorchTrainer will not be available\")\n", "fix_pattern": "<condition>: The condition is that PyTorch is not found or imported.\n<pattern>: The pattern is that the TorchTrainable class is removed and replaced with BaseTorchTrainable class.\n<code_one>: The code that is removed is \"TorchTrainable = None\\n    from ray.util.sgd.torch.torch_trainer import (TorchTrainer, TorchTrainable)\\n    __all__ = [\"TorchTrainer\", \"TorchTrainable\", \"TrainingOperator\"]\"\n<code_two>: The code that is added is \"BaseTorchTrainable = None\\n    from ray.util.sgd.torch.torch_trainer import (TorchTrainer,\\n                                                  BaseTorchTrainable)\\n    __all__ = [\"TorchTrainer\", \"BaseTorchTrainable\", \"TrainingOperator\"]\"\nFix_pattern: In the condition of PyTorch not found or imported, if the TorchTrainable class is detected, then remove the code \"TorchTrainable = None\\n    from ray.util.sgd.torch.torch_trainer import (TorchTrainer, TorchTrainable)\\n    __all__ = [\"TorchTrainer\", \"TorchTrainable\", \"TrainingOperator\"]\" and replace it with \"BaseTorchTrainable = None\\n    from ray.util.sgd.torch.torch_trainer import (TorchTrainer,\\n                                                  BaseTorchTrainable)\\n    __all__ = [\"TorchTrainer\", \"BaseTorchTrainable\", \"TrainingOperator\"]\" to fix the API misuse."}
{"number": 6455, "change": "def rand_segments(\nT = segment_size\nif _x_lenghts is None:\n_x_lenghts = T\n-    len_diff = _x_lenghts - segment_size + 1\n+    len_diff = _x_lenghts - segment_size\nif let_short_samples:\n_x_lenghts[len_diff < 0] = segment_size\n-        len_diff = _x_lenghts - segment_size + 1\n+        len_diff = _x_lenghts - segment_size\nelse:\nassert all(\nlen_diff > 0\n), f\" [!] At least one sample is shorter than the segment size ({segment_size}). \\n {_x_lenghts}\"\n-    segment_indices = (torch.rand([B]).type_as(x) * len_diff).long()\n-    ret = segment(x, segment_indices, segment_size)\n+    segment_indices = (torch.rand([B]).type_as(x) * (len_diff + 1)).long()\n+    ret = segment(x, segment_indices, segment_size, pad_short=pad_short)\nreturn ret, segment_indices\n", "fix_pattern": "<condition>: If _x_lenghts is None and let_short_samples is True.\n\n<pattern>: \n\nRemove the line \"len_diff = _x_lenghts - segment_size + 1\" and add the line \"len_diff = _x_lenghts - segment_size\".\n\n<code_one>: \n\nlen_diff = _x_lenghts - segment_size + 1\n\n<code_two>: \n\nlen_diff = _x_lenghts - segment_size\n\nFix_pattern: \n\nIn the condition of if _x_lenghts is None and let_short_samples is True, if the line \"len_diff = _x_lenghts - segment_size + 1\" is detected, then remove the line \"len_diff = _x_lenghts - segment_size + 1\" and add the line \"len_diff = _x_lenghts - segment_size\" to fix the API misuse."}
{"number": 6467, "change": "def log_tensorboard_graph(tb, model, imgsz=(640, 640)):\ntry:\np = next(model.parameters())  # for device, type\nimgsz = (imgsz, imgsz) if isinstance(imgsz, int) else imgsz  # expand\n-        im = torch.empty((1, 3, *imgsz)).to(p.device).type_as(p)  # input image\n+        im = torch.zeros((1, 3, *imgsz)).to(p.device).type_as(p)  # input image (WARNING: must be zeros, not empty)\nwith warnings.catch_warnings():\nwarnings.simplefilter('ignore')  # suppress jit trace warning\ntb.add_graph(torch.jit.trace(de_parallel(model), im, strict=False), [])\n", "fix_pattern": "<condition>: The condition is when a tensor is initialized using the \"torch.empty\" method.\n<pattern>: The pattern is detecting the use of \"torch.empty\" to initialize a tensor.\n<code_one>: The code being removed is \"im = torch.empty((1, 3, *imgsz)).to(p.device).type_as(p)\".\n<code_two>: The code being added is \"im = torch.zeros((1, 3, *imgsz)).to(p.device).type_as(p)\".\nFix_pattern: In the condition of detecting the use of \"torch.empty\" to initialize a tensor, the code \"im = torch.empty((1, 3, *imgsz)).to(p.device).type_as(p)\" is changed to \"im = torch.zeros((1, 3, *imgsz)).to(p.device).type_as(p)\" to fix the API misuse."}
{"number": 6489, "change": "class LayerNorm2d(nn.LayerNorm):\nreturn F.layer_norm(\nx.permute(0, 2, 3, 1), self.normalized_shape, self.weight, self.bias, self.eps).permute(0, 3, 1, 2)\nelse:\n-            s, u = torch.var_mean(x, dim=1, keepdim=True)\n+            s, u = torch.var_mean(x, dim=1, unbiased=False, keepdim=True)\nx = (x - u) * torch.rsqrt(s + self.eps)\nx = x * self.weight[:, None, None] + self.bias[:, None, None]\nreturn x\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: The pattern is to change the argument \"unbiased\" from True to False in the function call to torch.var_mean().\n<code_one>: The code that was removed is \"s, u = torch.var_mean(x, dim=1, keepdim=True)\".\n<code_two>: The code that was added is \"s, u = torch.var_mean(x, dim=1, unbiased=False, keepdim=True)\".\nFix_pattern: In the condition of no clear condition is needed, if the pattern of changing the argument \"unbiased\" from True to False is detected, then remove the code \"s, u = torch.var_mean(x, dim=1, keepdim=True)\" and replace it with \"s, u = torch.var_mean(x, dim=1, unbiased=False, keepdim=True)\" to fix the API misuse."}
{"number": 6504, "change": "def build_lm_labels(sequence, pad_token):\ndef build_mask(sequence, pad_token):\n\"\"\" Builds the mask. The attention mechanism will only attend to positions\nwith value 1. \"\"\"\n-    mask = sequence.clone()\n-    mask[mask != pad_token] = 1\n-    mask[mask == pad_token] = 0\n+    mask = torch.ones_like(sequence)\n+    idx_pad_tokens = (sequence == pad_token)\n+    mask[idx_pad_tokens] = 0\nreturn mask\n", "fix_pattern": "<condition>: The condition is when there is a need to build a mask using a specific value.\n<pattern>: The pattern is to create a mask variable and update its values based on the condition.\n<code_one>: The code that needs to be removed is where the mask variable is created and its values are updated directly using specific conditions.\n<code_two>: The code that needs to be added is where the mask variable is created using a torch function and its values are updated based on a condition using indexing.\nFix_pattern: In the condition of needing to build a mask using a specific value, if a pattern of directly updating the mask variable is detected, then remove the code that directly updates the mask variable and instead create the mask variable using a torch function and update its values using indexing based on a condition."}
{"number": 6508, "change": "class TFTransfoXLMainLayer(tf.keras.layers.Layer):\n\n# There are `mlen + qlen` steps that can be cached into mems\nnew_mems = []\n-        end_idx = mlen + max(0, qlen)\n-        beg_idx = max(0, end_idx - self.mem_len)\n+        end_idx = mlen + tf.math.maximum(0, qlen)\n+        beg_idx = tf.math.maximum(0, end_idx - tf.convert_to_tensor(self.mem_len))\nfor i in range(len(hids)):\n\ncat = tf.concat([mems[i], hids[i]], axis=0)\n", "fix_pattern": "<condition>: The code is in the context of a class called `TFTransfoXLMainLayer` with a for loop iterating over `hids`.\n\n<pattern>: The pattern is a computation that calculates the values of `end_idx` and `beg_idx` based on `mlen`, `qlen`, and `self.mem_len`.\n\n<code_one>: The original code of `end_idx` and `beg_idx` is using the `max()` function to compare the values of `0` and `self.mem_len`.\n\n<code_two>: The fixed code of `end_idx` and `beg_idx` is using the `tf.math.maximum()` function to compare the values of `0` and `self.mem_len`.\n\nFix_pattern: In the condition of iterating over `hids`, if the computation for `end_idx` and `beg_idx` involves comparing values between `0` and `self.mem_len`, then replace the usage of `max()` with `tf.math.maximum()` to fix the API misuse."}
{"number": 6517, "change": "def from_tfds_to_path(tfds_dataset_name: str, split: str, hub_ds_path: str, batc\nreturn from_tfds(tfds_ds=tfds_ds, ds=ds)\n\n\n-def from_tfds(tfds_ds: (tensorflow.data.Dataset), ds: (Dataset)):\n+def from_tfds(tfds_ds: tensorflow.data.Dataset, ds: Dataset):\n+    \"\"\"Converts a tfds dataset to hub dataset\n+    Args:\n+        tfds_ds (tensorflow.data.Dataset): A tfds_dataset object.\n+        ds (Dataset) : A Hub dataset object where Tensor will be created.\n+    Returns:\n+        A hub dataset\n+    \"\"\"\ntfds_numpy = tfds.as_numpy(tfds_ds)  # Convert `tf.data.Dataset` to Python generator\n\nfor ex in tqdm(tfds_numpy):\n", "fix_pattern": "<condition>: When converting a tfds dataset to a hub dataset.\n<pattern>: The parameter type annotations for `tfds_ds` and `ds` were different.\n<code_one>: `def from_tfds(tfds_ds: (tensorflow.data.Dataset), ds: (Dataset)):`\n<code_two>: `def from_tfds(tfds_ds: tensorflow.data.Dataset, ds: Dataset):`\nFix_pattern: In the condition of converting a tfds dataset to a hub dataset, if the parameter type annotations for `tfds_ds` and `ds` are different, then change the type annotations of `tfds_ds` and `ds` in the function signature to fix the API misuse."}
{"number": 6546, "change": "def unpackbits_masks(masks):\nunpacked = tf.bitwise.bitwise_and(tf.expand_dims(masks, -1), bits) > 0\nunpacked = tf.reshape(\nunpacked,\n-        tf.concat([tf.shape(masks)[:-1], [-1]], axis=0))\n+        tf.concat([tf.shape(masks)[:-1], [8 * tf.shape(masks)[-1]]], axis=0))\nreturn unpacked\n", "fix_pattern": "<condition>: The code involves bitwise operations in TensorFlow.\n<pattern>: The code is using tf.concat() to reshape a tensor.\n<code_one>: tf.concat([tf.shape(masks)[:-1], [-1]], axis=0))\n<code_two>: tf.concat([tf.shape(masks)[:-1], [8 * tf.shape(masks)[-1]]], axis=0))\nFix_pattern: In the condition of bitwise operation in TensorFlow, if tf.concat() is used to reshape a tensor, then change tf.concat([tf.shape(masks)[:-1], [-1]], axis=0)) to tf.concat([tf.shape(masks)[:-1], [8 * tf.shape(masks)[-1]]], axis=0)) to fix the API misuse."}
{"number": 6551, "change": "def _take_channels(*xs, ignore_channels=None):\nreturn xs\nelse:\nchannels = [channel for channel in range(xs[0].shape[1]) if channel not in ignore_channels]\n-        xs = [torch.index_select(x, dim=1, index=torch.tensor(channels)) for x in xs]\n+        xs = [torch.index_select(x, dim=1, index=torch.tensor(channels).to(x.device)) for x in xs]\nreturn xs\n", "fix_pattern": "<condition>: The condition is when `ignore_channels` is not None.\n<pattern>: The pattern is the use of `torch.index_select` to select specific channels in `xs`.\n<code_one>: The code that was removed is `xs = [torch.index_select(x, dim=1, index=torch.tensor(channels)) for x in xs]`.\n<code_two>: The code that was added is `xs = [torch.index_select(x, dim=1, index=torch.tensor(channels).to(x.device)) for x in xs]`.\nFix_pattern: In the condition of `ignore_channels` not being None, if the pattern of using `torch.index_select` is detected, then change `code_one` to `code_two` to fix the API misuse."}
{"number": 6613, "change": "class TokenCharactersEncoder(TokenEmbedder):\nself._dropout = lambda x: x\n\ndef get_output_dim(self) -> int:\n-        return self._encoder._module.get_output_dim()  # pylint: disable=protected-access\n+        return self._encoder._module.get_output_dim()\n\n-    def forward(self, token_characters: torch.Tensor) -> torch.Tensor:  # pylint: disable=arguments-differ\n+    def forward(self, token_characters: torch.Tensor) -> torch.Tensor:\nmask = (token_characters != 0).long()\nreturn self._dropout(self._encoder(self._embedding(token_characters), mask))\n\n# The setdefault requires a custom from_params\n@classmethod\ndef from_params(cls, vocab: Vocabulary, params: Params) -> 'TokenCharactersEncoder':  # type: ignore\n-        # pylint: disable=arguments-differ\n+\nembedding_params: Params = params.pop(\"embedding\")\n# Embedding.from_params() uses \"tokens\" as the default namespace, but we need to change\n# that to be \"token_characters\" by default. If num_embeddings is present, set default namespace\n", "fix_pattern": "<condition>: The code is using the outdated method of accessing the dimensions of the encoder module.\n<pattern>: The pattern is accessing the dimensions of the encoder module using the \"_module\" attribute.\n<code_one>: return self._encoder._module.get_output_dim()  # pylint: disable=protected-access\n<code_two>: return self._encoder.get_output_dim()\nFix_pattern: In the condition of using the outdated method of accessing the dimensions of the encoder module, remove the code \"return self._encoder._module.get_output_dim()  # pylint: disable=protected-access\" and change it to \"return self._encoder.get_output_dim()\" to fix the API misuse."}
{"number": 6629, "change": "class Normalize(Preprocessor):\n\"\"\"\n\ndef __init__(self, scope='normalize', summary_labels=()):\n-        super(Normalize).__init__(scope, summary_labels)\n+        super(Normalize, self).__init__(scope=scope, summary_labels=summary_labels)\n\ndef tf_process(self, tensor):\n# Min/max across every axis except batch dimension.\n-        min = tf.reduce_min(input_tensor=tensor, axis=np.arange(1, util.rank(tensor)))\n-        max = tf.reduce_max(input_tensor=tensor, axis=np.arange(1, util.rank(tensor)))\n+        min_value = tf.reduce_min(input_tensor=tensor, axis=np.arange(1, util.rank(tensor)))\n+        max_value = tf.reduce_max(input_tensor=tensor, axis=np.arange(1, util.rank(tensor)))\n\n-        return (tensor - min) / (max - min + util.epsilon)\n+        return (tensor - min_value) / (max_value - min_value + util.epsilon)\n", "fix_pattern": "<condition>: The condition is when initializing an object of the Normalize class.\n<pattern>: The pattern is incorrect parameter usage in the super() function call.\n<code_one>: The code that was removed is \"super(Normalize).__init__(scope, summary_labels)\".\n<code_two>: The corrected code is \"super(Normalize, self).__init__(scope=scope, summary_labels=summary_labels)\".\nFix_pattern: In the condition of initializing an object of the Normalize class, if incorrect parameter usage is detected in the super() function call, then remove \"super(Normalize).__init__(scope, summary_labels)\" and add \"super(Normalize, self).__init__(scope=scope, summary_labels=summary_labels)\" to fix the API misuse."}
{"number": 6695, "change": "class Conv(Layer):\ninputs = tf.pad(inputs, self._compute_causal_padding(inputs))\n\nif self.groups > 1:\n-            outputs = self._jit_compiled_convolution_op(inputs, self.kernel)\n+            outputs = self._jit_compiled_convolution_op(\n+                inputs, tf.convert_to_tensor(self.kernel)\n+            )\nelse:\noutputs = self.convolution_op(inputs, self.kernel)\n", "fix_pattern": "<condition>:\nThe condition is when the \"groups\" attribute is greater than 1.\n\n<pattern>:\nThe pattern is the incorrect use of the \"_jit_compiled_convolution_op\" method.\n\n<code_one>:\nThe code removed is \"outputs = self._jit_compiled_convolution_op(inputs, self.kernel)\".\n\n<code_two>:\nThe code added is \"outputs = self._jit_compiled_convolution_op(inputs, tf.convert_to_tensor(self.kernel))\".\n\nFix_pattern:\nIn the condition of \"groups > 1\", if the incorrect usage of \"_jit_compiled_convolution_op\" is detected, then the code \"outputs = self._jit_compiled_convolution_op(inputs, self.kernel)\" should be changed to \"outputs = self._jit_compiled_convolution_op(inputs, tf.convert_to_tensor(self.kernel))\" to fix the API misuse."}
{"number": 6714, "change": "def main():\n'op_names': ['features.6', 'features.9', 'features.13', 'features.16', 'features.20', 'classifier.2', 'classifier.5']\n}]\n\n-    quantizer = BNNQuantizer(model, configure_list)\n+    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n+    quantizer = BNNQuantizer(model, configure_list, optimizer)\nmodel = quantizer.compress()\n\nprint('=' * 10 + 'train' + '=' * 10)\n-    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\nbest_top1 = 0\nfor epoch in range(400):\nprint('# Epoch {} #'.format(epoch))\n", "fix_pattern": "<condition>: The code requires the optimizer to be defined before the quantizer.\n<pattern>: The code initializes the optimizer before the quantizer.\n<code_one>: quantizer = BNNQuantizer(model, configure_list)\n<code_two>: quantizer = BNNQuantizer(model, configure_list, optimizer)\nFix_pattern: In the condition where the optimizer needs to be defined before the quantizer, the fix pattern is to initialize the optimizer before the quantizer by changing the order of their initialization."}
{"number": 6720, "change": "class Dataset(DatasetInfoMixin, IndexableMixin):\n- if `dataset_path` is a path of a dataset dict directory: a :class:`DatasetDict` with each split.\n\"\"\"\n# copies file from filesystem if it is remote filesystem to local filesystem and modifies dataset_path to temp directory containing local copies\n+        fs = fsspec.filesystem(\"file\") if fs is None else fs\n+        dataset_dict_json_path = Path(dataset_path, config.DATASETDICT_JSON_FILENAME).as_posix()\n+        dataset_info_path = Path(dataset_path, config.DATASET_INFO_FILENAME).as_posix()\n+        if not fs.isfile(dataset_info_path) and fs.isfile(dataset_dict_json_path):\n+            raise FileNotFoundError(\n+                f\"No such file or directory: '{dataset_info_path}'. Expected to load a Dataset object, but got a DatasetDict. Please use datasets.load_from_disk instead.\"\n+            )\n+\nif is_remote_filesystem(fs):\nsrc_dataset_path = extract_path_from_uri(dataset_path)\ntmp_dir = tempfile.TemporaryDirectory()\n", "fix_pattern": "<condition>: If `dataset_path` is a path of a dataset dict directory: a :class:`DatasetDict` with each split.\n\n<pattern>: is_remote_filesystem(fs)\n\n<code_one>: src_dataset_path = extract_path_from_uri(dataset_path)\ntmp_dir = tempfile.TemporaryDirectory()\n\n<code_two>: \n```\nfs = fsspec.filesystem(\"file\") if fs is None else fs\ndataset_dict_json_path = Path(dataset_path, config.DATASETDICT_JSON_FILENAME).as_posix()\ndataset_info_path = Path(dataset_path, config.DATASET_INFO_FILENAME).as_posix()\nif not fs.isfile(dataset_info_path) and fs.isfile(dataset_dict_json_path):\n    raise FileNotFoundError(\n        f\"No such file or directory: '{dataset_info_path}'. Expected to load a Dataset object, but got a DatasetDict. Please use datasets.load_from_disk instead.\"\n    )\n```\n\nFix_pattern: In the condition of `dataset_path` being a path of a dataset dict directory, a :class:`DatasetDict` with each split, if `is_remote_filesystem(fs)` is detected, then remove the code `if dataset_path is a path of a dataset dict directory: a :class:`DatasetDict` with each split.` and add the code block above to fix the API misuse."}
{"number": 6759, "change": "def test_graph_store():\ndef test_graph_store_conversion():\ngraph_store = MyGraphStore()\n\n-    coo = (row, col) = get_edge_index(100, 100, 300)\n+    coo = (row, col) = get_random_edge_index(100, 100, 300)\nadj = SparseTensor(row=row, col=col, sparse_sizes=(100, 100))\ncsr, csc = adj.csr()[:2], adj.csc()[:2][::-1]\n", "fix_pattern": "<condition>: API misuse in the test_graph_store_conversion() function.\n<pattern>: Incorrect function call to get_edge_index().\n<code_one>: coo = (row, col) = get_edge_index(100, 100, 300)\n<code_two>: coo = (row, col) = get_random_edge_index(100, 100, 300)\nFix_pattern: In the condition of test_graph_store_conversion(), if an incorrect function call to get_edge_index() is detected, then change the code coo = (row, col) = get_edge_index(100, 100, 300) to coo = (row, col) = get_random_edge_index(100, 100, 300) to fix the API misuse."}
{"number": 6763, "change": "class StableDiffusionInpaintPipelineLegacy(DiffusionPipeline):\ninit_latents_orig = init_latents\n\n# add noise to latents using the timesteps\n-        noise = torch.randn(init_latents.shape, generator=generator, device=self.device, dtype=dtype)\n+        noise = randn_tensor(init_latents.shape, generator=generator, device=self.device, dtype=dtype)\ninit_latents = self.scheduler.add_noise(init_latents, noise, timestep)\nlatents = init_latents\nreturn latents, init_latents_orig, noise\n", "fix_pattern": "<condition>: The code is using the torch.randn function to generate a random tensor.\n\n<pattern>: The torch.randn function is being replaced with a custom function randn_tensor.\n\n<code_one>: noise = torch.randn(init_latents.shape, generator=generator, device=self.device, dtype=dtype)\n\n<code_two>: noise = randn_tensor(init_latents.shape, generator=generator, device=self.device, dtype=dtype)\n\nFix_pattern: In the condition of using torch.randn to generate a random tensor, replace the code noise = torch.randn(init_latents.shape, generator=generator, device=self.device, dtype=dtype) with noise = randn_tensor(init_latents.shape, generator=generator, device=self.device, dtype=dtype) to fix the API misuse."}
{"number": 6803, "change": "def fully_connected_with_w(x, use_bias=True, sn=False, reuse=False, scope='linea\nif sn :\nw = tf.get_variable(\"kernel\", [channels, 1], tf.float32,\ninitializer=weight_init, regularizer=weight_regularizer)\n+            w = spectral_norm(w)\n+\nif use_bias :\nbias = tf.get_variable(\"bias\", [1],\ninitializer=tf.constant_initializer(0.0))\n\n-                x = tf.matmul(x, spectral_norm(w)) + bias\n+                x = tf.matmul(x, w) + bias\nelse :\n-                x = tf.matmul(x, spectral_norm(w))\n+                x = tf.matmul(x, w)\n+\nelse :\nx = tf.layers.dense(x, units=1, kernel_initializer=weight_init, kernel_regularizer=weight_regularizer, use_bias=use_bias)\n", "fix_pattern": "<condition>: sn is true\n<pattern>: x = tf.matmul(x, spectral_norm(w)) + bias and x = tf.matmul(x, spectral_norm(w))\n<code_one>: x = tf.matmul(x, spectral_norm(w)) + bias\n<code_two>: w = spectral_norm(w)\nFix_pattern: in the condition of sn being true, if x = tf.matmul(x, spectral_norm(w)) + bias and x = tf.matmul(x, spectral_norm(w)) are detected, then change x = tf.matmul(x, spectral_norm(w)) + bias to x = tf.matmul(x, w) + bias and change x = tf.matmul(x, spectral_norm(w)) to x = tf.matmul(x, w) to fix the API misuse."}
{"number": 6811, "change": "def audio_config():\n@pytest.mark.parametrize(\"encoder\", [\"rnn\", \"stacked_cnn\", \"parallel_cnn\", \"stacked_parallel_cnn\", \"rnn\", \"cnnrnn\"])\ndef test_audio_input_feature(audio_config: Dict, encoder: str) -> None:\naudio_config.update({\"encoder\": encoder})\n-    audio_input_feature = AudioInputFeature(audio_config)\n-    audio_tensor = torch.randn([BATCH_SIZE, SEQ_SIZE, AUDIO_W_SIZE], dtype=torch.float32)\n+    audio_input_feature = AudioInputFeature(audio_config).to(DEVICE)\n+    audio_tensor = torch.randn([BATCH_SIZE, SEQ_SIZE, AUDIO_W_SIZE], dtype=torch.float32).to(DEVICE)\nencoder_output = audio_input_feature(audio_tensor)\n-    assert encoder_output[\"encoder_output\"].shape[1:] == audio_input_feature.encoder_obj.output_shape\n+    assert encoder_output[\"encoder_output\"].shape[1:] == audio_input_feature.output_shape\n", "fix_pattern": "<condition>: No pre condition is needed.\n<pattern>: API misuse\n<code_one>: audio_input_feature = AudioInputFeature(audio_config)\n<code_two>: audio_input_feature = AudioInputFeature(audio_config).to(DEVICE)\nFix_pattern: In the condition of API misuse, if the code `audio_input_feature = AudioInputFeature(audio_config)` is detected, then change it to `audio_input_feature = AudioInputFeature(audio_config).to(DEVICE)` to fix the API misuse."}
{"number": 6839, "change": "def trunc(\nelif not (\"int\" in str(x.dtype)):\nif not ret.get_shape().ndims == 0:\nret = tf.tensor_scatter_nd_update(\n-                x, tf.where(tf.greater(x, 0)), tf.math.floor(x[x > 0])\n+                x, tf.where(tf.greater_equal(x, 0)), tf.math.floor(x[x >= 0])\n)\nret = tf.tensor_scatter_nd_update(\nret, tf.where(tf.less(x, 0)), tf.math.ceil(x[x < 0])\n)\nelse:\n-            ret = (tf.math.floor if ret > 0 else tf.math.ceil)(ret)\n+            ret = (tf.math.floor if ret >= 0 else tf.math.ceil)(ret)\nreturn ret\n", "fix_pattern": "<condition>:\nThe condition is not specified in the given context.\n\n<pattern>:\nThe pattern is to change the condition of the if statement from \"tf.greater(x, 0)\" to \"tf.greater_equal(x, 0)\".\n\n<code_one>:\nThe code that was removed is \"x, tf.where(tf.greater(x, 0)), tf.math.floor(x[x > 0])\\nret = (tf.math.floor if ret > 0 else tf.math.ceil)(ret)\".\n\n<code_two>:\nThe code that was added is \"x, tf.where(tf.greater_equal(x, 0)), tf.math.floor(x[x >= 0])\\nret = (tf.math.floor if ret >= 0 else tf.math.ceil)(ret)\".\n\nFix_pattern:\nIn the condition of <condition>, if <pattern> is detected, then change <code_one> to <code_two> to fix the API misuse."}
{"number": 6843, "change": "class FocalLoss(nn.Module):\ndevice=input.device, dtype=input.dtype)\n\n# compute the actual focal loss\n-        prob = input_soft * target_one_hot\n+        prob = input_soft * target_one_hot + self.eps\nfocal = -torch.log(prob) * self.alpha * (1. - prob) ** self.gamma\n-        loss_tmp = 1. - torch.sum(focal, dim=1)\n+        loss_tmp = torch.sum(focal, dim=1)\n\nloss = -1\nif self.reduction == 'none':\n", "fix_pattern": "<condition>: The condition is that the reduction variable is set to 'none'.\n\n<pattern>: The pattern is the calculation of the focal loss and the subsequent calculation of the temporary loss.\n\n<code_one>: The code that is removed is the calculation of the probability times the target one-hot encoding and the calculation of the temporary loss using 1 minus the sum of the focal loss.\n\n<code_two>: The code that is added is the calculation of the probability times the target one-hot encoding plus the epsilon value and the calculation of the temporary loss using the sum of the focal loss.\n\nFix_pattern: In the condition of 'none' reduction, if the pattern of calculating the focal loss and temporary loss is detected, then remove the code for calculating the probability times the target one-hot encoding and the code for calculating the temporary loss using 1 minus the sum of the focal loss, and instead add the code for calculating the probability times the target one-hot encoding plus the epsilon value and the code for calculating the temporary loss using the sum of the focal loss."}
{"number": 6895, "change": "class InMemoryDataset(Dataset):\n\nfor item, key in product(data_list, keys):\ndata[key].append(item[key])\n-            s = slices[key][-1] + item[key].size(item.cat_dim(key))\n+            s = slices[key][-1] + item[key].size(item.cat_dim(key, item))\nslices[key].append(s)\n\nfor key in keys:\n-            data[key] = torch.cat(data[key], dim=data_list[0].cat_dim(key))\n+            data[key] = torch.cat(\n+                data[key], dim=data_list[0].cat_dim(key, item))\nslices[key] = torch.LongTensor(slices[key])\n\nreturn data, slices\n", "fix_pattern": "<condition>: The code is working with a list of data and keys.\n\n<pattern>: The code is updating the size of the slices and concatenating the data.\n\n<code_one>: \ns = slices[key][-1] + item[key].size(item.cat_dim(key))\ndata[key] = torch.cat(data[key], dim=data_list[0].cat_dim(key))\n\n<code_two>: \ns = slices[key][-1] + item[key].size(item.cat_dim(key, item))\ndata[key] = torch.cat(data[key], dim=data_list[0].cat_dim(key, item))\n\nFix_pattern: In the condition of working with a list of data and keys, if the pattern of updating the size of slices and concatenating data is detected, then change the code_one to code_two to fix the API misuse."}
{"number": 6896, "change": "class CLIPModel(CLIPPreTrainedModel):\n\nself.visual_projection = nn.Linear(self.vision_embed_dim, self.projection_dim, bias=False)\nself.text_projection = nn.Linear(self.text_embed_dim, self.projection_dim, bias=False)\n-        self.logit_scale = nn.Parameter(torch.ones([]))\n+        self.logit_scale = nn.Parameter(torch.ones([]) * self.config.logit_scale_init_value)\n\nself.init_weights()\n", "fix_pattern": "<condition>: No clear condition is needed.\n<pattern>: Change in initialization of `self.logit_scale`.\n<code_one>: `self.logit_scale = nn.Parameter(torch.ones([]))`\n<code_two>: `self.logit_scale = nn.Parameter(torch.ones([]) * self.config.logit_scale_init_value)`\nFix_pattern: In the condition of no clear condition is needed, if a change in initialization of `self.logit_scale` is detected, then change the code from `self.logit_scale = nn.Parameter(torch.ones([]))` to `self.logit_scale = nn.Parameter(torch.ones([]) * self.config.logit_scale_init_value)` to fix the API misuse."}
{"number": 6898, "change": "class BBoxHead(nn.Module):\nkeep_inds = pos_is_gts_.new_ones(num_rois)\nkeep_inds[:len(pos_is_gts_)] = pos_keep\n\n-            bboxes_list.append(bboxes[keep_inds])\n+            bboxes_list.append(bboxes[keep_inds.type(torch.bool)])\n\nreturn bboxes_list\n", "fix_pattern": "<condition>: When accessing an array or tensor using boolean indexing\n<pattern>: Using an outdated way of boolean indexing\n<code_one>: bboxes[keep_inds]\n<code_two>: bboxes[keep_inds.type(torch.bool)]\nFix_pattern: In the condition of using boolean indexing, if an outdated way of boolean indexing is detected, then change the code from bboxes[keep_inds] to bboxes[keep_inds.type(torch.bool)] to fix the API misuse."}
{"number": 6930, "change": "class BloomForSequenceClassification(BloomPreTrainedModel):\nsequence_lengths = -1\nelse:\nif input_ids is not None:\n-                sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(dim=-1) - 1\n+                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)\nelse:\nsequence_lengths = -1\nlogger.warning(\n", "fix_pattern": "<condition>: There are different conditions depending on the presence or absence of certain variables or inputs.\n<pattern>: The code is used to calculate the sequence lengths based on the input_ids.\n<code_one>: sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(dim=-1) - 1\n<code_two>: sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)\nFix_pattern: In the condition where the input_ids variable is not None, the sequence_lengths calculation is fixed by adding the code <code_two> instead of <code_one> to ensure proper API usage."}
{"number": 6965, "change": "class RecurrentTFModelV2(TFModelV2):\nshape=(None, obs_space.shape[0]))\nstate_in_h = tf.keras.layers.Input(shape=(256, ))\nstate_in_c = tf.keras.layers.Input(shape=(256, ))\n-                seq_in = tf.keras.layers.Input(shape=())\n+                seq_in = tf.keras.layers.Input(shape=(), dtype=tf.int32)\n\n# Send to LSTM cell\nlstm_out, state_h, state_c = tf.keras.layers.LSTM(\n", "fix_pattern": "<condition>: When using the `tf.keras.layers.Input` function to define an input layer in a model.\n\n<pattern>: If the `shape` argument of `tf.keras.layers.Input` is set to `()`.\n\n<code_one>: `shape=()`\n\n<code_two>: `shape=(), dtype=tf.int32`\n\nFix_pattern: In the condition of defining an input layer using `tf.keras.layers.Input`, if the `shape` argument is set to `()`, then change the `shape` argument to `(), dtype=tf.int32` to fix the API misuse."}
{"number": 6975, "change": "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\ntry:\nimport tensorflow as tf\n-    assert int(tf.__version__[0]) >= 2\n+    assert hasattr(tf, '__version__') and int(tf.__version__[0]) >= 2\n_tf_available = True  # pylint: disable=invalid-name\nlogger.info(\"TensorFlow version {} available.\".format(tf.__version__))\nexcept (ImportError, AssertionError):\n", "fix_pattern": "<condition>: In the context of an API misuse related issue.\n<pattern>: If the API misuse is due to a specific condition.\n<code_one>: The code that is causing the API misuse.\n<code_two>: The code that needs to be modified/fixed.\nFix_pattern: In the condition of API misuse, if the specific pattern is detected, then change <code_one> to <code_two> to fix the API misuse."}
{"number": 6979, "change": "class ModelCheckpoint(Callback):\n\n# do not save nan, replace with +/- inf\nif isinstance(current, torch.Tensor) and torch.isnan(current):\n-            current = torch.tensor(float(\"inf\" if self.mode == \"min\" else \"-inf\"))\n+            current = torch.tensor(float(\"inf\" if self.mode == \"min\" else \"-inf\"), device=current.device)\n\nfilepath = self._get_metric_interpolated_filepath_name(monitor_candidates, trainer, del_filepath)\n", "fix_pattern": "<condition>: Used when checking if a variable is a specific type and has a certain attribute.\n<pattern>: Detected if the condition checks for the variable type and attribute, but does not include the correct attribute check.\n<code_one>: The incorrect attribute check in the condition.\n<code_two>: The correct attribute check to fix the API misuse.\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then change <code_one> to <code_two> to fix the API misuse."}
{"number": 6981, "change": "class QNAFModel(QModel):\nl_matrix = flat_stddev\nl_matrix = tf.exp(l_matrix)\nelse:\n-            l_matrix = tf.map_fn(fn=tf.diag, elems=flat_stddev)\n+            l_matrix = tf.linalg.diag(diagonal=flat_stddev)\n\nl_entries = self.l_entries[name].apply(x=embedding)\nl_entries = tf.exp(l_entries)\n", "fix_pattern": "<condition>: The condition is when the variable \"l_matrix\" is being assigned a value using \"tf.map_fn(fn=tf.diag, elems=flat_stddev)\".\n<pattern>: The pattern is to replace \"tf.map_fn(fn=tf.diag, elems=flat_stddev)\" with \"tf.linalg.diag(diagonal=flat_stddev)\".\n<code_one>: The code being removed is \"l_matrix = tf.map_fn(fn=tf.diag, elems=flat_stddev)\".\n<code_two>: The code being added is \"l_matrix = tf.linalg.diag(diagonal=flat_stddev)\".\nFix_pattern: In the condition of the variable \"l_matrix\" being assigned a value using \"tf.map_fn(fn=tf.diag, elems=flat_stddev)\", replace \"tf.map_fn(fn=tf.diag, elems=flat_stddev)\" with \"tf.linalg.diag(diagonal=flat_stddev)\" to fix the API misuse."}
{"number": 7035, "change": "def bag_config():\n@pytest.mark.parametrize(\"encoder\", [\"embed\"])\ndef test_bag_input_feature(bag_config: Dict, encoder: str) -> None:\nbag_config.update({\"encoder\": encoder})\n-    bag_input_feature = BagInputFeature(bag_config)\n-    bag_tensor = torch.randn([BATCH_SIZE, SEQ_SIZE, BAG_W_SIZE], dtype=torch.float32)\n+    bag_input_feature = BagInputFeature(bag_config).to(DEVICE)\n+    bag_tensor = torch.randn([BATCH_SIZE, SEQ_SIZE, BAG_W_SIZE], dtype=torch.float32).to(DEVICE)\nencoder_output = bag_input_feature(bag_tensor)\n-    assert encoder_output[\"encoder_output\"].shape[1:][1:] == bag_input_feature.encoder_obj.output_shape\n+    assert encoder_output[\"encoder_output\"].shape[1:][1:] == bag_input_feature.output_shape\n", "fix_pattern": "<condition>: The condition is not explicitly mentioned in the code snippet provided. \n\n<pattern>: The pattern is to modify the instantiation and usage of the `BagInputFeature` object.\n\n<code_one>: The original code instantiates `bag_input_feature` without applying any operations or methods to it.\n\n<code_two>: The fixed code instantiates `bag_input_feature` and applies the `.to(DEVICE)` method to move it to a specified device.\n\nFix_pattern: In the condition of N/A, if the pattern of instantiating `BagInputFeature` is detected, then change the code to include `.to(DEVICE)` to fix the API misuse."}
{"number": 7045, "change": "def test_auto_scale_batch_size_set_model_attribute(tmpdir, use_hparams):\n\ntrainer = Trainer(default_root_dir=tmpdir, max_epochs=1, auto_scale_batch_size=True)\ntrainer.tune(model, datamodule_fit)\n-    assert trainer.datamodule == datamodule_fit\nafter_batch_size = model.hparams.batch_size if use_hparams else model.batch_size\n+    assert trainer.datamodule == datamodule_fit\nassert before_batch_size != after_batch_size\n+    assert after_batch_size <= len(trainer.train_dataloader.dataset)\nassert datamodule_fit.batch_size == after_batch_size\n# should be left unchanged, since it was not passed to .tune()\nassert datamodule_model.batch_size == 111\n", "fix_pattern": "<condition>: The condition is not clearly stated in the given context.\n\n<pattern>: The pattern is not clearly stated in the given context.\n\n<code_one>: The code \"assert trainer.datamodule == datamodule_fit\" is removed.\n\n<code_two>: The code \"assert after_batch_size <= len(trainer.train_dataloader.dataset)\" is added.\n\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then remove the <code_one> and add <code_two> to fix the API misuse."}
{"number": 7063, "change": "class PytorchBackendCompiler(Compiler):\ninput_sample = input_data.get_list(1)[0]\nif self.device is Device.GPU:\nif quantization_type is QuantizationType.HALF:\n-                input_sample = [t.cuda().half() for t in input_sample]\n+                input_sample = [\n+                    t.cuda().half() if torch.is_floating_point(t) else t.cuda()\n+                    for t in input_sample\n+                ]\nelse:\ninput_sample = [t.cuda() for t in input_sample]\n", "fix_pattern": "<condition>: self.device is Device.GPU\n\n<pattern>: input_sample = [t.cuda().half() for t in input_sample]\n\n<code_one>: [t.cuda().half() for t in input_sample]\n\n<code_two>: [\n                t.cuda().half() if torch.is_floating_point(t) else t.cuda()\n                for t in input_sample\n            ]\n\nFix_pattern: In the condition of self.device is Device.GPU, if input_sample is assigned the pattern [t.cuda().half() for t in input_sample], then remove the code [t.cuda().half() for t in input_sample] and replace it with [\n                t.cuda().half() if torch.is_floating_point(t) else t.cuda()\n                for t in input_sample\n            ] to fix the API misuse."}
{"number": 7079, "change": "def test_api_training_determinism(csv_filename):\n\ndivergence = False\nfor weight_1, weight_2 in zip(model_weights_1, model_weights_2):\n-            if not np.allclose(weight_1, weight_2):\n+            if not torch.allclose(weight_1, weight_2):\ndivergence = True\nbreak\nassert divergence, 'model_1 and model_2 have identical weights with different seeds!'\n\nfor weight_1, weight_3 in zip(model_weights_1, model_weights_3):\n-            assert np.allclose(weight_1, weight_3)\n+            assert torch.allclose(weight_1, weight_3)\n\n\ndef run_api_commands(\n", "fix_pattern": "<condition>: The condition where model weights are being compared\n<pattern>: The pattern of using the numpy function `np.allclose` for comparing weights\n<code_one>: The code line using `np.allclose` for weight comparison\n<code_two>: The code line using `torch.allclose` for weight comparison\nFix_pattern: In the condition of model weight comparison, if `np.allclose` is detected, then replace it with `torch.allclose` to fix the API misuse."}
{"number": 7109, "change": "class TFOptimizer(Optimizer):\narguments: Dict of arguments for passing to fn_loss as **kwargs.\nfn_loss: A callable taking arguments as kwargs and returning the loss op.\n\"\"\"\n-        loss = fn_loss(**arguments)\n+        # Trivial operation to enforce control dependency\n+        previous_variables = [util.identity_operation(x=variable) for variable in variables]\n\n# Force loss value to be calculated.\n-        with tf.control_dependencies(control_inputs=(loss,)):\n-            # Trivial operation to enforce control dependency\n-            previous_variables = [util.identity_operation(x=variable) for variable in variables]\n+        with tf.control_dependencies(control_inputs=previous_variables):\n+            loss = fn_loss(**arguments)\n\n# The actual tensorflow minimize op.\n-        with tf.control_dependencies(control_inputs=previous_variables):\n+        with tf.control_dependencies(control_inputs=(loss,)):\n# colocate_gradients_with_ops=True\napplied = self.optimizer.minimize(loss=loss, var_list=variables)\n", "fix_pattern": "<condition>: The code is using TensorFlow's minimize function to optimize a loss function.\n<pattern>: The loss value is not being calculated before the minimize operation.\n<code_one>: The code is calculating the loss value using the fn_loss function and creating control dependencies for previous_variables.\n<code_two>: The code is creating control dependencies for previous_variables and then calculating the loss value using the fn_loss function.\nFix_pattern: In the condition of optimizing a loss function using TensorFlow's minimize function, if the loss value is not being calculated before the minimize operation, then calculate the loss value before creating control dependencies for previous_variables to fix the API misuse."}
{"number": 7132, "change": "class Trainer(Registrable):\nself.optimizer.zero_grad()\n\nloss = self.batch_loss(batch, for_training=True)\n+            if torch.isnan(loss):\n+                raise ValueError(\"nan loss encountered\")\n+\nloss.backward()\n\ntrain_loss += loss.item()\n", "fix_pattern": "<condition>: The condition is when there is a loss computed during training.\n<pattern>: The pattern is detecting if the loss is NaN (Not a Number).\n<code_one>: No code is removed in this case as the pattern is detecting an issue rather than removing any code.\n<code_two>: The code added is an exception being raised with the message \"nan loss encountered\".\nFix_pattern: In the condition of loss computation during training, if a NaN loss is detected, then raise an exception with the message \"nan loss encountered\" to fix the API misuse."}
{"number": 7145, "change": "def patch_tf_keras():\nfrom tensorflow.python.keras.engine import training_arrays\nfrom tensorflow.python.keras.engine import training_generator\n\n-    training_v2 = wandb.util.import_module('tensorflow.python.keras.engine.training_v2')\n+    training_v2 = wandb.util.get_module('tensorflow.python.keras.engine.training_v2')\nold_arrays = training_arrays.fit_loop\nold_generator = training_generator.fit_generator\nif training_v2:\n", "fix_pattern": "<condition>: The condition is the presence of the imported module 'tensorflow.python.keras.engine.training_v2'.\n\n<pattern>: The pattern is the incorrect usage of the old module to assign the value to the variable 'training_v2'.\n\n<code_one>: The code one is the assignment of the value of 'training_arrays.fit_loop' and 'training_generator.fit_generator' to the variables 'old_arrays' and 'old_generator', respectively.\n\n<code_two>: The code two is the assignment of the value of 'wandb.util.get_module('tensorflow.python.keras.engine.training_v2')' to the variable 'training_v2'.\n\nFix_pattern: In the condition of 'if training_v2:', if the pattern of 'training_v2 = wandb.util.import_module('tensorflow.python.keras.engine.training_v2')' is detected, then change 'training_v2 = wandb.util.import_module('tensorflow.python.keras.engine.training_v2')' to 'training_v2 = wandb.util.get_module('tensorflow.python.keras.engine.training_v2')' to fix the API misuse."}
{"number": 7154, "change": "def corr2d(X, K):  #@save\nY = tf.Variable(tf.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)))\nfor i in range(Y.shape[0]):\nfor j in range(Y.shape[1]):\n-            Y[i, j].assign(tf.reduce_sum(X[i: i + h, j: j + w] * K))\n+            Y[i, j].assign(tf.cast(tf.reduce_sum(\n+                X[i: i + h, j: j + w] * K), dtype=tf.float32))\nreturn Y\n", "fix_pattern": "<condition>: The code is using TensorFlow library for a convolution operation.\n\n<pattern>: The code is missing a cast operation to change the data type of the calculated sum before assigning it to the variable Y.\n\n<code_one>: Y[i, j].assign(tf.reduce_sum(X[i: i + h, j: j + w] * K))\n\n<code_two>: Y[i, j].assign(tf.cast(tf.reduce_sum(X[i: i + h, j: j + w] * K), dtype=tf.float32))\n\nFix_pattern: In the condition of TensorFlow convolution operation, if the code is missing a cast operation, then change the assignment statement of Y[i, j] to include the cast operation with dtype=tf.float32 to fix the API misuse."}
{"number": 7185, "change": "def get_item(\nx: torch.Tensor,\nquery: torch.Tensor,\n) -> torch.Tensor:\n-    if ivy.dtype(query, as_native=True) is torch.bool:\n-        return x.__getitem__(query)\n-    return x.__getitem__(query.to(torch.int64))\n+    if ivy.is_array(query) and ivy.dtype(query, as_native=True) is not torch.bool:\n+        return x.__getitem__(query.to(torch.int64))\n+    return x.__getitem__(query)\n\n\ndef to_numpy(x: torch.Tensor, /, *, copy: bool = True) -> np.ndarray:\n", "fix_pattern": "<condition>:\nThe condition is that the input query tensor is of type torch.Tensor.\n\n<pattern>:\nThe pattern is that the input query tensor is checked for its dtype using the ivy.dtype() function, and then the result of this check is used to determine which type of indexing to use when accessing elements of the input tensor.\n\n<code_one>:\nThe code that is removed is the following:\n    if ivy.dtype(query, as_native=True) is torch.bool:\n        return x.__getitem__(query)\n    return x.__getitem__(query.to(torch.int64))\n\n<code_two>:\nThe code that is added is the following:\n    if ivy.is_array(query) and ivy.dtype(query, as_native=True) is not torch.bool:\n        return x.__getitem__(query.to(torch.int64))\n    return x.__getitem__(query)\n\nFix_pattern:\nIn the condition of the input query tensor being of type torch.Tensor, if the dtype of the query tensor is not torch.bool, the indexing operation is performed using the query tensor casted to torch.int64. This fix pattern handles the API misuse by properly handling the indexing based on the dtype of the query tensor."}
{"number": 7191, "change": "class PulsarPointsRenderer(nn.Module):\nraster_rad = self.rasterizer.raster_settings.radius\nif kwargs.get(\"radius_world\", False):\nreturn raster_rad\n-        if isinstance(raster_rad, torch.Tensor) and raster_rad.numel() > 1:\n+        if (\n+            isinstance(raster_rad, torch.Tensor)\n+            and raster_rad.numel() > 1\n+            and raster_rad.ndim > 1\n+        ):\n# In this case it must be a batched torch tensor.\nraster_rad = raster_rad[cloud_idx]\nif orthogonal_projection:\n", "fix_pattern": "<condition>: The condition is that the variable \"raster_rad\" is an instance of the torch.Tensor class and it has more than one element.\n\n<pattern>: The pattern is that the code checks for the condition using the \"isinstance(raster_rad, torch.Tensor)\" check and then additional check using \"raster_rad.numel() > 1\".\n\n<code_one>: The code being removed is \"and raster_rad.ndim > 1\" in the if statement.\n\n<code_two>: The code being added is \"and raster_rad.ndim > 1\" in the if statement.\n\nFix_pattern: In the condition of \"raster_rad\" being an instance of the torch.Tensor class and it having more than one element, the fix is to add the check \"and raster_rad.ndim > 1\" in the if statement."}
{"number": 7215, "change": "class GPTNeoXForCausalLM(GPTNeoXPreTrainedModel):\nattention_mask = input_ids.new_ones(input_shape)\n\n# cut decoder_input_ids if past is used\n-        if past is not None:\n+        if past and past[0] is not None:\ninput_ids = input_ids[:, -1:]\n\nreturn {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past}\n", "fix_pattern": "<condition>: If the \"past\" variable is not None.\n<pattern>: Remove the check for \"past\" variable in the condition.\n<code_one>: \"if past is not None:\"\n<code_two>: \"if past and past[0] is not None:\"\nFix_pattern: In the condition of \"if past is not None\", if the check for \"past\" variable is detected, then remove the check for \"past\" variable and modify the condition to \"if past and past[0] is not None:\". This fix is done to address the API misuse."}
{"number": 7261, "change": "class PolicyGradientModel(Model):\n\nwith tf.variable_scope('distribution'):\nfor action, distribution in self.distribution.items():\n-                distribution.create_tf_operations(x=self.network.output, deterministic=self.deterministic)\n+                with tf.variable_scope(action):\n+                    distribution.create_tf_operations(x=self.network.output, deterministic=self.deterministic)\nself.action_taken[action] = distribution.sample()\n\nif self.baseline:\n", "fix_pattern": "<condition>: When using a TensorFlow distribution with a variable scope.\n\n<pattern>: The 'create_tf_operations' method of the distribution is called outside of a variable scope.\n\n<code_one>: distribution.create_tf_operations(x=self.network.output, deterministic=self.deterministic)\n\n<code_two>: with tf.variable_scope(action):\n                distribution.create_tf_operations(x=self.network.output, deterministic=self.deterministic)\n\nFix_pattern: In the condition of using a TensorFlow distribution with a variable scope, if the code 'distribution.create_tf_operations(x=self.network.output, deterministic=self.deterministic)' is detected outside of a variable scope, then it should be removed and added with the code 'with tf.variable_scope(action): distribution.create_tf_operations(x=self.network.output, deterministic=self.deterministic)' to fix the API misuse."}
{"number": 7271, "change": "def generate_examples(features: dict, num_examples=100, seq_shapes=None):\ndef generate_example_dataset(dataset_path, features, num_examples=100, seq_shapes=None):\ndummy_data = generate_examples(features, num_examples=num_examples, seq_shapes=seq_shapes)\n\n-    writer = datasets.ArrowWriter(features=features, path=dataset_path)\n-    for key, record in dummy_data:\n-        example = features.encode_example(record)\n-        writer.write(example)\n+    with datasets.ArrowWriter(features=features, path=dataset_path) as writer:\n+        for key, record in dummy_data:\n+            example = features.encode_example(record)\n+            writer.write(example)\n\n-    num_final_examples, num_bytes = writer.finalize()\n+        num_final_examples, num_bytes = writer.finalize()\n\nassert (\nnum_final_examples == num_examples\n", "fix_pattern": "<condition>: The code is using the `generate_examples` function to generate example data and write it to a dataset using `ArrowWriter`.\n\n<pattern>: The pattern is that the code is not properly using the `ArrowWriter` for writing the examples to the dataset.\n\n<code_one>: The code is using the `ArrowWriter` outside of a `with` statement.\n\n<code_two>: The code is using the `ArrowWriter` within a `with` statement.\n\nFix_pattern: In the condition of using the `generate_examples` function to generate example data and write it to a dataset using `ArrowWriter`, if the pattern of using the `ArrowWriter` outside of a `with` statement is detected, then the code should be changed to use the `ArrowWriter` within a `with` statement to fix the API misuse."}
{"number": 7293, "change": "class DistributedModel(object):\n\ngrad_var_list = list(zip(self.gradients, self.global_network.get_variables()))\n\n-            global_step_inc = self.global_step.assign_add(self.batch_size)\n+            global_step_inc = self.global_step.assign_add(tf.shape(self.state)[0])\n\nself.assign_global_to_local = tf.group(*[v1.assign(v2) for v1, v2 in\nzip(self.local_network.get_variables(),\n", "fix_pattern": "<condition>: The code is part of a class called \"DistributedModel\".\n\n<pattern>: The pattern is that a variable named \"global_step_inc\" is being assigned the result of the \"assign_add\" operation on a variable named \"global_step\" and another variable or tensor.\n\n<code_one>: \nglobal_step_inc = self.global_step.assign_add(self.batch_size)\n\n<code_two>: \nglobal_step_inc = self.global_step.assign_add(tf.shape(self.state)[0])\n\nFix_pattern:\nIn the condition of being part of the \"DistributedModel\" class, if the pattern of assigning the result of \"assign_add\" on \"global_step\" and another variable or tensor is detected, then change the <code_one> to <code_two> to fix the API misuse."}
{"number": 7298, "change": "class Model(ModelDesc):\nfor idx, b in enumerate([b1, b2, b3, b4, b5, final_map]):\noutput = tf.nn.sigmoid(b, name='output{}'.format(idx+1))\nxentropy = class_balanced_sigmoid_cross_entropy(\n-                b, edgemap,\n+                tf.squeeze(b, [3]), edgemap,\nname='xentropy{}'.format(idx+1))\ncosts.append(xentropy)\n", "fix_pattern": "<condition>: The condition is not specified in the given context.\n<pattern>: The pattern is to replace the usage of the variable \"b\" with \"tf.squeeze(b, [3])\".\n<code_one>: The code \"b\" is removed.\n<code_two>: The code \"tf.squeeze(b, [3])\" is added.\nFix_pattern: In the condition where \"b\" is used, if the pattern of using \"b\" is detected, then replace \"b\" with \"tf.squeeze(b, [3])\" to fix the API misuse."}
{"number": 7333, "change": "class TensorBoard(Callback):\nembedding_input = tf.reshape(embedding_input,\n(step, int(embedding_size)))\nshape = (self.embeddings_data[0].shape[0], int(embedding_size))\n-                    embedding = tf.Variable(tf.zeros(shape),\n-                                            name=layer.name + '_embedding')\n+                    embedding = K.variable(K.zeros(shape),\n+                                           name=layer.name + '_embedding')\nembeddings_vars[layer.name] = embedding\nbatch = tf.assign(embedding[batch_id:batch_id + step],\nembedding_input)\n", "fix_pattern": "<condition>:\nThe condition is when initializing a \"Variable\" using \"tf.zeros\" in the TensorBoard class.\n\n<pattern>:\nThe pattern is to replace \"tf.Variable\" with \"K.variable\" and replace \"tf.zeros\" with \"K.zeros\".\n\n<code_one>:\nThe code that was removed is:\n        embedding = tf.Variable(tf.zeros(shape), name=layer.name + '_embedding')\n\n<code_two>:\nThe code that was added is:\n        embedding = K.variable(K.zeros(shape), name=layer.name + '_embedding')\n\nFix_pattern:\nIn the condition of initializing a \"Variable\" using \"tf.zeros\" in the TensorBoard class, if the pattern of \"embedding = tf.Variable(tf.zeros(shape), name=layer.name + '_embedding')\" is detected, then replace it with \"embedding = K.variable(K.zeros(shape), name=layer.name + '_embedding')\" to fix the API misuse."}
{"number": 7336, "change": "def build_targets(p, targets, model):\nif use_all_anchors:\nna = anchor_vec.shape[0]  # number of anchors\na = torch.arange(na).view(-1, 1).repeat(1, nt).view(-1)\n-                t = targets.repeat(na, 1)\n+                t = t.repeat(na, 1)\nelse:  # use best anchor only\niou, a = iou.max(0)  # best iou and anchor\n", "fix_pattern": "<condition>: The condition is when the variable \"use_all_anchors\" is True.\n<pattern>: The pattern is the repetition of the variable \"targets\" with \"na\" number of times.\n<code_one>: The code that is removed is \"t = targets.repeat(na, 1)\".\n<code_two>: The code that is added is \"t = t.repeat(na, 1)\".\nFix_pattern: In the condition of \"use_all_anchors\" being True, if the pattern of \"t = targets.repeat(na, 1)\" is detected, then it should be changed to \"t = t.repeat(na, 1)\" to fix the API misuse."}
{"number": 7364, "change": "def zca_mean(inp: torch.Tensor, dim: int = 0,\nelse:\ncov = cov / float(N)\n\n-    U, S, _ = torch.svd(cov)\n+    U, S, _ = _torch_svd_cast(cov)\n\nS = S.reshape(-1, 1)\nS_inv_root: torch.Tensor = torch.rsqrt(S + eps)\n", "fix_pattern": "<condition>: The code uses the `torch.svd` function.\n<pattern>: The code is replaced with a custom `_torch_svd_cast` function.\n<code_one>: `U, S, _ = torch.svd(cov)`\n<code_two>: `U, S, _ = _torch_svd_cast(cov)`\nFix_pattern: In the condition of using the `torch.svd` function, the code is replaced with the `_torch_svd_cast` function to fix the API misuse."}
{"number": 7375, "change": "class PretrainedTransformerMismatchedEmbedder(TokenEmbedder):\nspan_embeddings_sum = span_embeddings.sum(2)\nspan_embeddings_len = span_mask.sum(2)\n# Shape: (batch_size, num_orig_tokens, embedding_size)\n-        orig_embeddings = span_embeddings_sum / span_embeddings_len\n+        orig_embeddings = span_embeddings_sum / torch.clamp_min(span_embeddings_len, 1)\n\n# All the places where the span length is zero, write in zeros.\norig_embeddings[(span_embeddings_len == 0).expand(orig_embeddings.shape)] = 0\n", "fix_pattern": "<condition>: `span_embeddings_len` is a tensor representing the length of each span.\n<pattern>: Zero division error occurs when the length of a span is zero.\n<code_one>: `orig_embeddings = span_embeddings_sum / span_embeddings_len`\n<code_two>: `orig_embeddings = span_embeddings_sum / torch.clamp_min(span_embeddings_len, 1)`\nFix_pattern: In the condition where the length of a span is zero, change the division operation from `span_embeddings_sum / span_embeddings_len` to `span_embeddings_sum / torch.clamp_min(span_embeddings_len, 1)` to fix the zero division error."}
{"number": 7386, "change": "class PipelineTesterMixin:\n\nwith tempfile.TemporaryDirectory() as tmpdir:\npipe.save_pretrained(tmpdir)\n-            pipe_loaded = self.pipeline_class.from_pretrained(tmpdir)\n+            pipe_loaded = self.pipeline_class.from_pretrained(tmpdir, torch_dtype=torch.float16)\npipe_loaded.to(torch_device)\npipe_loaded.set_progress_bar_config(disable=None)\n", "fix_pattern": "<condition>:\nThe condition is that the pipeline needs to be loaded from a pretrained model.\n\n<pattern>:\nThe pattern is that the `from_pretrained` method is called with only one argument.\n\n<code_one>:\nThe `pipe_loaded = self.pipeline_class.from_pretrained(tmpdir)` line is removed.\n\n<code_two>:\nThe `pipe_loaded = self.pipeline_class.from_pretrained(tmpdir, torch_dtype=torch.float16)` line is added.\n\nFix_pattern:\nIn the condition of loading a pipeline from a pretrained model, if the `from_pretrained` method is called with only one argument, then remove the line `pipe_loaded = self.pipeline_class.from_pretrained(tmpdir)` and add the line `pipe_loaded = self.pipeline_class.from_pretrained(tmpdir, torch_dtype=torch.float16)` to fix the API misuse."}
{"number": 7392, "change": "def main():\n# train\nlogging.info('backend = ' + args.backend)\nif args.backend == \"chainer\":\n-        from espnet.lm.chain.lm_chainer import train\n+        from espnet.lm.chain.lm import train\ntrain(args)\nelif args.backend == \"pytorch\":\n-        from espnet.lm.pytorch.lm_pytorch import train\n+        from espnet.lm.pytorch.lm import train\ntrain(args)\nelse:\nraise ValueError(\"Only chainer and pytorch are supported.\")\n", "fix_pattern": "<condition>: The condition is checking the value of the `args.backend` variable.\n\n<pattern>: The pattern is the incorrect imports for the lm_chainer and lm_pytorch modules.\n\n<code_one>: The code being removed is the incorrect imports.\n\n<code_two>: The code being added is the correct imports.\n\nFix_pattern: \n\nIn the condition of `args.backend`, if incorrect imports for modules lm_chainer and lm_pytorch (`espnet.lm.chain.lm_chainer` and `espnet.lm.pytorch.lm_pytorch`) are detected, then remove the incorrect imports and add the correct imports (`espnet.lm.chain.lm` and `espnet.lm.pytorch.lm`) to fix the API misuse."}
{"number": 7443, "change": "class SimpleSummarizationPipelineTests(unittest.TestCase):\n# Bias output towards L\nV, C = model.lm_head.weight.shape\n\n-        bias = torch.zeros(V, requires_grad=True)\n+        bias = torch.zeros(V)\nbias[76] = 10\n\nmodel.lm_head.bias = torch.nn.Parameter(bias)\n", "fix_pattern": "<condition>: The condition is when a bias variable needs to be created and assigned to the `'lm_head.bias'` attribute of the model.\n<pattern>: The pattern is that the bias variable is originally created with the `requires_grad=True` argument.\n<code_one>: `bias = torch.zeros(V, requires_grad=True)`\n<code_two>: `bias = torch.zeros(V)`\nFix_pattern: In the condition of assigning a bias variable to the `'lm_head.bias'` attribute of the model, if the bias variable is created with the `requires_grad=True` argument, then change it to `bias = torch.zeros(V)` to fix the API misuse."}
{"number": 7451, "change": "def to_hetero(module: Module, metadata: Metadata, aggr: str = \"sum\",\nimport torch\nfrom torch_geometric.nn import SAGEConv, to_hetero\n\n-        Net(torch.nn.Module):\n+        class GNN(torch.nn.Module):\ndef __init__(self):\n-                self.conv1 = SAGEConv(-1, 16)\n-                self.conv2 = SAGEConv(16, 16)\n+                self.conv1 = SAGEConv((-1, -1), 32)\n+                self.conv2 = SAGEConv((32, 32), 32)\n\ndef forward(self, x, edge_index):\nx = self.conv1(x, edge_index).relu()\nx = self.conv2(x, edge_index).relu()\nreturn x\n\n-        model = Net()\n+        model = GNN()\n\nnode_types = ['paper', 'author']\nedge_types = [\n", "fix_pattern": "<condition>: There is a need to fix API misuse in the code.\n<pattern>: The pattern detected is the incorrect initialization of SAGEConv in the \"Net\" class.\n<code_one>: The code that needs to be removed is the initialization of SAGEConv with incorrect parameters (-1, 16) and (16, 16).\n<code_two>: The code that needs to be added is the correct initialization of SAGEConv with the parameters (-1, -1) and (32, 32).\nFix_pattern: In the condition of API misuse, if incorrect initialization of SAGEConv is detected, then remove the incorrect initialization code and add the correct initialization code to fix the API misuse."}
{"number": 7456, "change": "def get_batch_statistics(outputs, targets, iou_threshold):\ncontinue\n\n# Filter target_boxes by pred_label so that we only match against boxes of our own label\n-                filtered_target_position, filtered_targets = zip(*filter(lambda x: target_labels[x] == pred_label, enumerate(target_boxes)))\n-\n+                filtered_target_position, filtered_targets = zip(*filter(lambda x: target_labels[x[0]] == pred_label, enumerate(target_boxes)))\n+\n# Find the best matching target for our predicted box\n-                iou, box_filtered_index = bbox_iou(pred_box.unsqueeze(0), filtered_targets).max(0)\n-\n+                iou, box_filtered_index = bbox_iou(pred_box.unsqueeze(0), torch.stack(filtered_targets)).max(0)\n+\n# Remap the index in the list of filtered targets for that label to the index in the list with all targets.\nbox_index = filtered_target_position[box_filtered_index]\n", "fix_pattern": "<condition>: The condition is that there is a need to filter target boxes based on a predicted label.\n\n<pattern>: The pattern is that the lambda function used to filter the target boxes is incorrect.\n\n<code_one>: The code that needs to be removed is:\n```python\nfiltered_target_position, filtered_targets = zip(*filter(lambda x: target_labels[x] == pred_label, enumerate(target_boxes)))\niou, box_filtered_index = bbox_iou(pred_box.unsqueeze(0), filtered_targets).max(0)\n```\n\n<code_two>: The code that needs to be added is:\n```python\nfiltered_target_position, filtered_targets = zip(*filter(lambda x: target_labels[x[0]] == pred_label, enumerate(target_boxes)))\niou, box_filtered_index = bbox_iou(pred_box.unsqueeze(0), torch.stack(filtered_targets)).max(0)\n```\n\nFix_pattern: In the condition of filtering target boxes by a predicted label, if the lambda function used to filter the target boxes is incorrect, then remove the first occurrence of the pattern and replace it with the second occurrence of the pattern to fix the API misuse."}
{"number": 7493, "change": "class XLNetRelativeAttention(nn.Module):\nattn_vec = torch.einsum('bnij,jbnd->ibnd', attn_prob, v_head_h)\n\nif self.output_attentions:\n-            return attn_vec, attn_prob\n+            return attn_vec, torch.einsum('bnij->ijbn', attn_prob)\n\nreturn attn_vec\n", "fix_pattern": "<condition>: The code is checking if the output_attentions flag is True.\n<pattern>: There was a return statement returning attn_vec and attn_prob.\n<code_one>: The return statement returning attn_vec and attn_prob.\n<code_two>: The return statement is changed to return attn_vec and a modified version of attn_prob using torch.einsum.\nFix_pattern: In the condition of checking if output_attentions is True, if a return statement returning attn_vec and attn_prob is detected, then change the return statement to return attn_vec and a modified version of attn_prob using torch.einsum to fix the API misuse."}
{"number": 7499, "change": "class VGG(Model):\n\ninputs = inputs * 255 - np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape([1, 1, 1, 3])\n\n-        out = self.layers(inputs)\n+        out = self.layers.forward(inputs)\nreturn out\n", "fix_pattern": "<condition>: There is an API misuse in the code.\n<pattern>: The method \"layers\" is being called on an instance of the class.\n<code_one>: \"out = self.layers(inputs)\"\n<code_two>: \"out = self.layers.forward(inputs)\"\nFix_pattern: In the condition of an API misuse, if the pattern of calling the \"layers\" method on an instance of the class is detected, then change the code from \"out = self.layers(inputs)\" to \"out = self.layers.forward(inputs)\" to fix the API misuse."}
{"number": 7519, "change": "class CapsNet(object):\nelse:\n# self.masked_v = tf.matmul(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)), transpose_a=True)\nself.masked_v = tf.multiply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)))\n-                self.v_length = tf.sqrt(tf.reduce_sum(tf.square(self.caps2), axis=2, keep_dims=True) + epsilon)\n+                self.v_length = tf.sqrt(reduce_sum(tf.square(self.caps2), axis=2, keepdims=True) + epsilon)\n\n# 2. Reconstructe the MNIST images with 3 FC layers\n# [batch_size, 1, 16, 1] => [batch_size, 16] => [batch_size, 512]\n", "fix_pattern": "<condition>: The condition is not clear in the given context.\n\n<pattern>: The pattern detected is an incorrect usage of the tf.reduce_sum function.\n\n<code_one>: The code that was removed is \"tf.reduce_sum(tf.square(self.caps2), axis=2, keep_dims=True) + epsilon\".\n\n<code_two>: The code that was added is \"reduce_sum(tf.square(self.caps2), axis=2, keepdims=True) + epsilon\".\n\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then remove \"tf.\" from \"tf.reduce_sum\" in <code_one> and change \"keep_dims=True\" to \"keepdims=True\" in <code_two> to fix the API misuse."}
{"number": 7528, "change": "class Uniform(Distribution):\nif x.size != a.size():\na = a.expand_as(x)\nb = b.expand_as(x)\n-        l = x.ge(a).type_as(a)\n-        u = x.le(b).type_as(b)\n+        lb = x.ge(a).type_as(a)\n+        ub = x.le(b).type_as(b)\nbatch_log_pdf_shape = self.batch_shape(a, b) + (1,)\n-        return torch.sum(torch.log(l.mul(u)) - torch.log(b - a), -1).contiguous().view(batch_log_pdf_shape)\n+        return torch.sum(torch.log(lb.mul(ub)) - torch.log(b - a), -1).contiguous().view(batch_log_pdf_shape)\n\ndef analytic_mean(self, a=None, b=None):\na, b = self._sanitize_input(a, b)\n", "fix_pattern": "<condition>: x.size != a.size()\n<pattern>: x.ge(a).type_as(a)\n<code_one>: l = x.ge(a).type_as(a)\n<code_two>: lb = x.ge(a).type_as(a)\nFix_pattern: \nIn the condition of x.size != a.size(), if x.ge(a).type_as(a) is detected, then remove l = x.ge(a).type_as(a) and add lb = x.ge(a).type_as(a) to fix the API misuse."}
{"number": 7559, "change": "class SSIMLoss(torch.nn.Module):\n\nif ssim_loss.item() > 1.0:\nprint(f\" > SSIM loss is out-of-range {ssim_loss.item()}, setting it 1.0\")\n-            ssim_loss = torch.tensor([1.0])\n+            ssim_loss = torch.tensor([1.0], device=ssim_loss.device)\n\nif ssim_loss.item() < 0.0:\nprint(f\" > SSIM loss is out-of-range {ssim_loss.item()}, setting it 0.0\")\n-            ssim_loss = torch.tensor([0.0])\n+            ssim_loss = torch.tensor([0.0], device=ssim_loss.device)\n\nreturn ssim_loss\n", "fix_pattern": "<condition>: ssim_loss.item() > 1.0 or ssim_loss.item() < 0.0\n<pattern>: ssim_loss = torch.tensor([1.0]) or ssim_loss = torch.tensor([0.0])\n<code_one>: ssim_loss = torch.tensor([1.0]) or ssim_loss = torch.tensor([0.0])\n<code_two>: ssim_loss = torch.tensor([1.0], device=ssim_loss.device) or ssim_loss = torch.tensor([0.0], device=ssim_loss.device)\nFix_pattern: In the condition of ssim_loss.item() > 1.0 or ssim_loss.item() < 0.0, if ssim_loss = torch.tensor([1.0]) or ssim_loss = torch.tensor([0.0]) is detected, then change ssim_loss = torch.tensor([1.0]) or ssim_loss = torch.tensor([0.0]) to ssim_loss = torch.tensor([1.0], device=ssim_loss.device) or ssim_loss = torch.tensor([0.0], device=ssim_loss.device) to fix the API misuse."}
{"number": 7597, "change": "class LstmTagger(Model):\n\ndef forward(self,\nsentence: Dict[str, torch.Tensor],\n-                labels: torch.Tensor = None) -> torch.Tensor:\n+                labels: torch.Tensor = None) -> Dict[str, torch.Tensor]:\nmask = get_text_field_mask(sentence)\nembeddings = self.word_embeddings(sentence)\nencoder_out = self.encoder(embeddings, mask)\n", "fix_pattern": "<condition>: The condition is the method signature of the forward() method in the class LstmTagger.\n\n<pattern>: The pattern to be detected is the change in the method signature of the forward() method.\n\n<code_one>: The code to be removed is the return type annotation of the forward() method, which is torch.Tensor.\n\n<code_two>: The code to be added is the return type annotation of the forward() method, which is Dict[str, torch.Tensor].\n\nFix_pattern: In the condition of the forward() method, if the pattern of the return type annotation being torch.Tensor is detected, then remove the code_one (torch.Tensor) and change it to code_two (Dict[str, torch.Tensor]) to fix the API misuse."}
{"number": 7601, "change": "class TensorFlowEstimator(BaseEstimator):\n# Set up a single operator to merge all the summaries\nsummary_op = tf.merge_all_summaries()\n# Set up summary writer to a tmp directory\n-        self._summary_writer = tf.train.SummaryWriter('/tmp/tensorflow_logs', graph_def=sess.graph_def)\n+        self._summary_writer = tf.train.SummaryWriter('/tmp/tensorflow_logs', graph_def=self._session.graph_def)\n\ndef fit(self, X, y):\n\"\"\"Builds a neural network model given provided `model_fn` and training\n", "fix_pattern": "<condition>: The condition where an API misuse is detected.\n<pattern>: The specific pattern or error that is identified.\n<code_one>: The code that needs to be removed or changed.\n<code_two>: The code that needs to be added or modified to fix the API misuse.\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then <remove/add/change> the <code_one> to <code_two> to fix the API misuse."}
{"number": 7602, "change": "def test_inputs(framework: str | None) -> list[tuple[ModuleType, FrameworkTestMo\n)\nexcept ModuleNotFoundError as e:\nlogger.warning(\n-                f\"Failed to find test module for framework {framework_name} (tests.integration.frameworks.models.{framework_name})\"\n+                f\"Failed to find test module for framework {framework_name} (tests.integration.frameworks.models.{framework_name}): {e}\"\n)\n\nreturn [\n", "fix_pattern": "<condition>: The condition is a ModuleNotFoundError exception being raised.\n\n<pattern>: The pattern is a specific error message being logged when the test module for a framework cannot be found.\n\n<code_one>: The code being removed is the specific error message being logged without the exception information.\n\n<code_two>: The code being added is the specific error message being logged along with the exception information.\n\nFix_pattern: In the condition of a ModuleNotFoundError being raised, if the specific error message pattern is detected, then change the code that logs just the error message to instead log the error message along with the exception information to provide more context and information about the error."}
{"number": 7619, "change": "class Trainer(\n)\nreturn {}\n\n-            ckpt = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n+            ckpt = pl_load(ckpt_path, map_location=lambda storage, loc: storage)\nmodel.load_state_dict(ckpt['state_dict'])\n\n# attach dataloaders\n", "fix_pattern": "<condition>:\nThere is a need to load a state dictionary from a checkpoint file.\n\n<pattern>:\nThe code used to load the state dictionary is using the `torch.load()` function.\n\n<code_one>:\n`ckpt = torch.load(ckpt_path, map_location=lambda storage, loc: storage)`\n\n<code_two>:\n`ckpt = pl_load(ckpt_path, map_location=lambda storage, loc: storage)`\n\nFix_pattern:\nIn the condition of needing to load a state dictionary from a checkpoint file, if the pattern of using `torch.load()` is detected, then the fix is to change the code from `torch.load()` to `pl_load()` to fix the API misuse."}
{"number": 7621, "change": "def from_networkx(G):\n\nG = nx.convert_node_labels_to_integers(G)\nG = G.to_directed() if not nx.is_directed(G) else G\n-    edge_index = torch.tensor(list(G.edges)).t().contiguous()\n+    edge_index = torch.LongTensor(list(G.edges)).t().contiguous()\n\ndata = {}\n", "fix_pattern": "<condition>: The code is using the networkx library and converting the graph to a PyTorch tensor.\n<pattern>: The code is using the `torch.tensor()` function to convert the edge list to a PyTorch tensor.\n<code_one>: `edge_index = torch.tensor(list(G.edges)).t().contiguous()`\n<code_two>: `edge_index = torch.LongTensor(list(G.edges)).t().contiguous()`\nFix_pattern: In the condition of using the networkx library and converting the graph to a PyTorch tensor, if the code uses `torch.tensor()` to convert the edge list, then it should be changed to `torch.LongTensor()` to fix the API misuse."}
{"number": 7643, "change": "class AttentionDecoder(RNNDecoder):\nlogits=self.vocab_size,\npredicted_ids=tf.TensorShape([]),\ncell_output=self.cell.output_size,\n-        attention_scores=tf.concat([0, self.attention_values[1:-1]], 0),\n+        attention_scores=tf.concat(\n+            [[0], tf.shape(self.attention_values)[1:-1]], 0),\nattention_context=self.attention_values.get_shape()[-1])\n\n@property\n", "fix_pattern": "<condition>: None\n\n<pattern>: The attention_scores tensor is being constructed from the attention_values tensor.\n\n<code_one>: \n```\nattention_scores=tf.concat([0, self.attention_values[1:-1]], 0)\n```\n\n<code_two>:\n```\nattention_scores=tf.concat(\n    [[0], tf.shape(self.attention_values)[1:-1]], 0)\n```\n\nFix_pattern: In the condition of no specific condition, if the attention_scores tensor is being constructed from the attention_values tensor, then change the code from `attention_scores=tf.concat([0, self.attention_values[1:-1]], 0)` to `attention_scores=tf.concat([[0], tf.shape(self.attention_values)[1:-1]], 0)` to fix the API misuse."}
{"number": 7658, "change": "def convert_points_from_homogeneous(points: torch.Tensor,\n\n# we check for points at infinity\nz_vec: torch.Tensor = points[..., -1:]\n-    scale: torch.Tensor = 1. / torch.clamp(z_vec, eps)\n+    scale: torch.Tensor = torch.tensor(1.) / torch.clamp(z_vec, eps)\n\nreturn scale * points[..., :-1]\n", "fix_pattern": "<condition>: The code is checking for points at infinity.\n<pattern>: The code is removing the division operation by calculating the reciprocal of the tensor.\n<code_one>: scale: torch.Tensor = 1. / torch.clamp(z_vec, eps)\n<code_two>: scale: torch.Tensor = torch.tensor(1.) / torch.clamp(z_vec, eps)\nFix_pattern: In the condition of checking for points at infinity, if a division by a tensor is detected, then change the code from <code_one> to <code_two> to fix the API misuse."}
{"number": 7719, "change": "class MeanAbsoluteError(Metric):\npreds: Predictions from model\ntarget: Ground truth values\n\"\"\"\n-        self._check_same_shape(preds, target)\n-        abs_error = torch.abs(preds - target)\n+        sum_abs_error, n_obs = _mean_absolute_error_update(preds, target)\n\n-        self.sum_abs_error += torch.sum(abs_error)\n-        self.total += target.numel()\n+        self.sum_abs_error += sum_abs_error\n+        self.total += n_obs\n\ndef compute(self):\n\"\"\"\nComputes mean absolute error over state.\n\"\"\"\n-        return self.sum_abs_error / self.total\n+        return _mean_absolute_error_compute(self.sum_abs_error, self.total)\n", "fix_pattern": "<condition>: The condition is not clearly identified in the given context.\n\n<pattern>: The pattern is detecting and replacing the computation of mean absolute error with a more efficient and modular implementation.\n\n<code_one>: The code being removed is the existing computation of mean absolute error, which involves checking shape, computing absolute error, summing absolute error, and calculating the mean.\n\n<code_two>: The code being added is a new function call to update the sum of absolute error and the number of observations, and another function call to compute the mean absolute error using the updated values.\n\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then remove the <code_one> and replace it with <code_two> to fix the API misuse."}
{"number": 7723, "change": "class RandomSampler(BaseSampler):\nelse:\ndevice = 'cpu'\ngallery = torch.tensor(gallery, dtype=torch.long, device=device)\n-        perm = torch.randperm(gallery.numel(), device=gallery.device)[:num]\n+        # This is a temporary fix. We can revert the following code\n+        # when PyTorch fixes the abnormal return of torch.randperm.\n+        # See: https://github.com/open-mmlab/mmdetection/pull/5014\n+        perm = torch.randperm(gallery.numel())[:num].to(device=gallery.device)\nrand_inds = gallery[perm]\nif not is_tensor:\nrand_inds = rand_inds.cpu().numpy()\n", "fix_pattern": "<condition>: The condition of the fix pattern is when there is an API misuse in the code.\n<pattern>: The pattern to be detected is the incorrect usage of the `torch.randperm` function.\n<code_one>: The code that needs to be removed is `perm = torch.randperm(gallery.numel(), device=gallery.device)[:num]`.\n<code_two>: The code that needs to be added is `perm = torch.randperm(gallery.numel())[:num].to(device=gallery.device)`.\nFix_pattern: In the condition of an API misuse, if the incorrect usage of the `torch.randperm` function is detected, then remove the code that uses the `device` parameter in `torch.randperm` and add `.to(device=gallery.device)` to fix the API misuse."}
{"number": 7739, "change": "class Stft(torch.nn.Module, InversibleInterface):\npad = self.n_fft // 2\nilens = ilens + 2 * pad\n\n-            olens = torch.div((ilens - self.n_fft), self.hop_length) + 1\n+            olens = (\n+                torch.div((ilens - self.n_fft), self.hop_length, rounding_mode=\"floor\")\n+                + 1\n+            )\noutput.masked_fill_(make_pad_mask(olens, output, 1), 0.0)\nelse:\nolens = None\n", "fix_pattern": "<condition>: The condition is not explicitly mentioned in the given context. \n<pattern>: The pattern is to update the calculation of \"olens\" variable in the else block.\n<code_one>: The original code is \"olens = torch.div((ilens - self.n_fft), self.hop_length) + 1\".\n<code_two>: The updated code is \"olens = (torch.div((ilens - self.n_fft), self.hop_length, rounding_mode=\"floor\") + 1)\".\nFix_pattern: In the else block, update the calculation of \"olens\" by changing the division to include rounding_mode=\"floor\" option. The updated code would be \"olens = (torch.div((ilens - self.n_fft), self.hop_length, rounding_mode=\"floor\") + 1)\"."}
{"number": 7781, "change": "class ImageInputFeature(ImageFeatureMixin, InputFeature):\n)\n\ndef forward(self, inputs: torch.Tensor) -> torch.Tensor:\n-        assert isinstance(inputs, torch.Tensor)\n-        assert inputs.dtype in [torch.float32]\n+        assert isinstance(inputs, torch.Tensor), f\"inputs to image feature must be a torch tensor, got {type(inputs)}\"\n+        assert inputs.dtype in [torch.float32], f\"inputs to image feature must be a float32 tensor, got {inputs.dtype}\"\n\ninputs_encoded = self.encoder_obj(inputs)\n", "fix_pattern": "<condition>: The input parameter must be a torch tensor of type float32.\n<pattern>: Checking the type and dtype of the inputs.\n<code_one>: assert isinstance(inputs, torch.Tensor) and assert inputs.dtype in [torch.float32]\n<code_two>: assert isinstance(inputs, torch.Tensor), f\"inputs to image feature must be a torch tensor, got {type(inputs)}\" and assert inputs.dtype in [torch.float32], f\"inputs to image feature must be a float32 tensor, got {inputs.dtype}\"\nFix_pattern: In the condition of the input being a torch tensor of type float32, if the type and dtype of the inputs are not correct, then remove the code `assert isinstance(inputs, torch.Tensor) and assert inputs.dtype in [torch.float32]` and add the code `assert isinstance(inputs, torch.Tensor), f\"inputs to image feature must be a torch tensor, got {type(inputs)}\"` and `assert inputs.dtype in [torch.float32], f\"inputs to image feature must be a float32 tensor, got {inputs.dtype}\"` to fix the API misuse."}
{"number": 7789, "change": "def test_hetero_to_undirected():\ndata['v', 'w'].edge_weight = edge_weight\ndata['v', 'w'].edge_attr = edge_attr\n\n+    from torch_geometric.transforms import ToUndirected\ndata = ToUndirected()(data)\n-    assert data['v', 'v'].edge_index.tolist() == [[0, 0, 1, 2, 2, 3],\n-                                                  [1, 2, 0, 0, 3, 2]]\n+    assert data['v', 'v'].edge_index.tolist() == [[0, 1, 2, 3], [1, 0, 3, 2]]\nassert data['v', 'v'].edge_weight.tolist() == edge_weight[perm].tolist()\nassert data['v', 'v'].edge_attr.tolist() == edge_attr[perm].tolist()\nassert data['v', 'w'].edge_index.tolist() == edge_index.tolist()\nassert data['v', 'w'].edge_weight.tolist() == edge_weight.tolist()\nassert data['v', 'w'].edge_attr.tolist() == edge_attr.tolist()\n-    assert data['w', 'v'].edge_index.tolist() == [[3, 1, 0], [2, 0, 2]]\n+    assert data['w', 'v'].edge_index.tolist() == [[3, 1], [2, 0]]\nassert data['w', 'v'].edge_weight.tolist() == edge_weight.tolist()\nassert data['w', 'v'].edge_attr.tolist() == edge_attr.tolist()\n", "fix_pattern": "<condition>: N/A\n<pattern>: The pattern that was detected is a specific assertion statement that checks if the edge_index has been correctly transformed.\n<code_one>: The code that was removed is the original assertion statement that checks the edge_index values.\n<code_two>: The code that was added is a modified assertion statement that checks the transformed edge_index values.\nFix_pattern: In the condition of no pre-condition needed, if the specific assertion statement that checks the edge_index values is detected, then remove the original assertion statement and add a modified assertion statement that checks the transformed edge_index values to fix the API misuse."}
{"number": 7809, "change": "if __name__ == '__main__':\n\n# Load pytorch model\ngoogle_utils.attempt_download(opt.weights)\n-    model = torch.load(opt.weights, map_location=torch.device('cpu'))['model']\n+    model = torch.load(opt.weights, map_location=torch.device('cpu'))['model'].float()\nmodel.eval()\nmodel.fuse()\n", "fix_pattern": "<condition>: The code is executed when __name__ is equal to '__main__'\n<pattern>: The code is loading a pytorch model\n<code_one>: The original code is loading the model using torch.load()\n<code_two>: The fixed code is loading the model using torch.load() and converting it to a float()\nFix_pattern: In the condition of \"__name__ == '__main__'\", if loading a pytorch model is detected, then change the code to load the model using torch.load() and convert it to a float() to fix the API misuse."}
{"number": 7821, "change": "def test_forward(use_token_averaged_energy):\n)\nxs = torch.randn(2, 256)\nif not use_token_averaged_energy:\n-        layer(xs, torch.LongTensor([256, 128]))\n+        es, elens = layer(xs, torch.LongTensor([256, 128]))\n+        assert es.shape[1] == max(elens)\nelse:\nds = torch.LongTensor([[3, 0, 2], [3, 0, 0]])\ndlens = torch.LongTensor([3, 1])\n", "fix_pattern": "<condition>: The condition is `use_token_averaged_energy` being `True`.\n<pattern>: The pattern is a code block that calculates `es` and `elens` using the `layer` function and asserts a specific shape condition.\n<code_one>: The code block `layer(xs, torch.LongTensor([256, 128]))` is removed.\n<code_two>: The code block `es, elens = layer(xs, torch.LongTensor([256, 128]))\\n        assert es.shape[1] == max(elens)` is added.\nFix_pattern: In the condition of `use_token_averaged_energy` being `True`, if the code block `layer(xs, torch.LongTensor([256, 128]))` is detected, then remove it and add the code block `es, elens = layer(xs, torch.LongTensor([256, 128]))\\n        assert es.shape[1] == max(elens)` to fix the API misuse."}
{"number": 7866, "change": "from .degree import DegreeAdj\n\nclass DegreeTest(TestCase):\ndef test_degree_adj(self):\n+        index = torch.LongTensor([[0, 0, 1, 2], [1, 2, 0, 1]])\nweight = torch.FloatTensor([2, 3, 4, 6])\n-        edge = torch.LongTensor([[0, 0, 1, 2], [1, 2, 0, 1]])\n-        adj = torch.sparse.FloatTensor(edge, weight, torch.Size([3, 3]))\n+        adj = torch.sparse.FloatTensor(index, weight, torch.Size([3, 3]))\n\ntransform = DegreeAdj()\n\n-        _, adj = transform((None, adj))\n+        _, adj, _ = transform((None, adj, None))\nadj = adj.to_dense()\n\nexpected_adj_out = [\n", "fix_pattern": "<condition>: The condition is not clearly mentioned in the given context.\n\n<pattern>: The pattern is that the variable \"adj\" is initially created as a sparse tensor using the \"torch.sparse.FloatTensor\" function with the variable \"edge\" as an argument.\n\n<code_one>: The code that is removed is:\n\n    adj = torch.sparse.FloatTensor(edge, weight, torch.Size([3, 3]))\n    _, adj = transform((None, adj))\n\n<code_two>: The code that is added is:\n\n    adj = torch.sparse.FloatTensor(index, weight, torch.Size([3, 3]))\n    _, adj, _ = transform((None, adj, None))\n\nFix_pattern: In the condition where the \"adj\" variable is created using the \"torch.sparse.FloatTensor\" function with the \"edge\" argument, it is being replaced with the same function but using the \"index\" argument instead. The additional code for returning and assigning values is also added."}
{"number": 7912, "change": "class Trainer:\nelse:\ntr_loss_step = self.training_step(model, inputs)\n\n-                if args.logging_nan_inf_filter and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step)):\n-                    # if loss is nan or inf simply add the average of previous logged losses\n-                    tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\n+                if args.logging_nan_inf_filter and not is_torch_tpu_available():\n+                    if torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step):\n+                        # if loss is nan or inf simply add the average of previous logged losses\n+                        tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\nelse:\ntr_loss += tr_loss_step\n", "fix_pattern": "<condition>: The condition is when the argument \"args.logging_nan_inf_filter\" is True and torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step) is detected.\n<pattern>: The pattern is to remove the if condition that checks for \"args.logging_nan_inf_filter\" and add an additional if condition that checks if torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step). The code inside the if condition remains the same.\n<code_one>: The code to be removed is:\n\n                if args.logging_nan_inf_filter and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step)):\n                    # if loss is nan or inf simply add the average of previous logged losses\n                    tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\n\n<code_two>: The code to be added is:\n\n                if args.logging_nan_inf_filter and not is_torch_tpu_available():\n                    if torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step):\n                        # if loss is nan or inf simply add the average of previous logged losses\n                        tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\n\nFix_pattern: In the condition of having \"args.logging_nan_inf_filter\" and detecting torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step), remove the if condition and add an additional if condition that checks the same condition, and add the code inside the if condition."}
{"number": 7948, "change": "class SGD(base.Module):\noptimizer_utils.check_same_dtype(update, parameter)\nlearning_rate = tf.cast(self.learning_rate, update.dtype.base_dtype)\nif isinstance(update, tf.IndexedSlices):\n-          parameter.scatter_nd_sub(\n-              update.indices, update.values * learning_rate)\n+          parameter.scatter_sub(\n+              tf.IndexedSlices(update.values * learning_rate, update.indices))\nelse:\nparameter.assign_sub(update * learning_rate)\n", "fix_pattern": "<condition>: The condition is checking if the variable \"update\" is an instance of tf.IndexedSlices.\n<pattern>: The pattern suggests that when the variable \"update\" is an instance of tf.IndexedSlices, the code should use the \"scatter_nd_sub\" method of the \"parameter\" object to perform the update operation.\n<code_one>: parameter.scatter_nd_sub(update.indices, update.values * learning_rate)\n<code_two>: parameter.scatter_sub(tf.IndexedSlices(update.values * learning_rate, update.indices))\nFix_pattern: In the condition of checking if \"update\" is an instance of tf.IndexedSlices, if this condition is true, then replace the line using \"parameter.scatter_nd_sub\" with \"parameter.scatter_sub\" using a tf.IndexedSlices object created with the appropriate arguments."}
{"number": 7954, "change": "for it in range(1000000):\nT_sample = T(torch.cat([X, z_sample], 1))\n\ndisc = torch.mean(-T_sample)\n-    loglike = -nn.binary_cross_entropy(X_sample, X)\n+    loglike = -nn.binary_cross_entropy(X_sample, X, size_average=False) / mb_size\n\nelbo = -(disc + loglike)\n", "fix_pattern": "<condition>: The code is attempting to calculate the negative binary cross entropy loss between two tensors, X_sample and X.\n\n<pattern>: The pattern is that the size_average argument is being added to the binary_cross_entropy function call.\n\n<code_one>: The original code does not have the size_average argument.\n\n<code_two>: The fixed code adds the size_average=False argument to the binary_cross_entropy function.\n\nFix_pattern: In the condition of calculating the negative binary cross entropy loss between two tensors, if the size_average argument is not present in the binary_cross_entropy function call, then the code should add size_average=False to the function call to fix the API misuse."}
{"number": 7961, "change": "class SamplePoints(object):\npos = pos / pos_max\n\narea = (pos[face[1]] - pos[face[0]]).cross(pos[face[2]] - pos[face[0]])\n-        area = torch.sqrt((area**2).sum(dim=-1)) / 2\n+        area = area.norm(p=2, dim=1) / 2\n\nprob = area / area.sum()\nsample = torch.multinomial(prob, self.num, replacement=True)\n", "fix_pattern": "<condition>: None, no pre condition is needed.\n<pattern>: Calculation of the area is being modified to use a different method.\n<code_one>: area = torch.sqrt((area**2).sum(dim=-1)) / 2\n<code_two>: area = area.norm(p=2, dim=1) / 2\nFix_pattern: In the condition where the calculation of the area is being performed, if the code for calculating the area is 'area = torch.sqrt((area**2).sum(dim=-1)) / 2', then replace it with 'area = area.norm(p=2, dim=1)/2'."}
{"number": 7968, "change": "class PaintByExamplePipeline(DiffusionPipeline):\nimage_embeddings = image_embeddings.view(bs_embed * num_images_per_prompt, seq_len, -1)\n\nif do_classifier_free_guidance:\n-            uncond_embeddings = uncond_embeddings.repeat(1, image_embeddings.shape[0], 1)\n-            uncond_embeddings = uncond_embeddings.view(bs_embed * num_images_per_prompt, 1, -1)\n+            negative_prompt_embeds = negative_prompt_embeds.repeat(1, image_embeddings.shape[0], 1)\n+            negative_prompt_embeds = negative_prompt_embeds.view(bs_embed * num_images_per_prompt, 1, -1)\n\n# For classifier free guidance, we need to do two forward passes.\n# Here we concatenate the unconditional and text embeddings into a single batch\n# to avoid doing two forward passes\n-            image_embeddings = torch.cat([uncond_embeddings, image_embeddings])\n+            image_embeddings = torch.cat([negative_prompt_embeds, image_embeddings])\n\nreturn image_embeddings\n", "fix_pattern": "<condition>: do_classifier_free_guidance is True.\n<pattern>: uncond_embeddings is repeated, reshaped, and concatenated with image_embeddings.\n<code_one>: uncond_embeddings = uncond_embeddings.repeat(1, image_embeddings.shape[0], 1)\n            uncond_embeddings = uncond_embeddings.view(bs_embed * num_images_per_prompt, 1, -1)\n            image_embeddings = torch.cat([uncond_embeddings, image_embeddings])\n<code_two>: negative_prompt_embeds is repeated, reshaped, and concatenated with image_embeddings.\n            negative_prompt_embeds = negative_prompt_embeds.repeat(1, image_embeddings.shape[0], 1)\n            negative_prompt_embeds = negative_prompt_embeds.view(bs_embed * num_images_per_prompt, 1, -1)\n            image_embeddings = torch.cat([negative_prompt_embeds, image_embeddings])\nFix_pattern: In the condition of do_classifier_free_guidance being True, if the pattern of concatenating uncond_embeddings with image_embeddings is detected, then remove the code_one and add the code_two to fix the API misuse."}
{"number": 7977, "change": "class SequenceGenerator(nn.Module):\ncand_size = 2 * beam_size  # 2 x beam size in case half are EOS\n\n# offset arrays for converting between different indexing schemes\n-        bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens)\n-        cand_offsets = torch.arange(0, cand_size).type_as(tokens)\n+        bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens).to(src_tokens.device)\n+        cand_offsets = torch.arange(0, cand_size).type_as(tokens).to(src_tokens.device)\n\nreorder_state: Optional[Tensor] = None\nbatch_idxs: Optional[Tensor] = None\n", "fix_pattern": "<condition>: None\n<pattern>: The code is missing a device assignment for the `to()` method.\n<code_one>: `(torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens)`\n<code_two>: `(torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens).to(src_tokens.device)`\nFix_pattern: In the condition of no pre condition is needed, if a missing device assignment is detected, then add `.to(src_tokens.device)` to <code_one> to fix the API misuse."}
{"number": 8014, "change": "class MeanSquaredError(Metric):\npreds: Predictions from model\ntarget: Ground truth values\n\"\"\"\n-        self._check_same_shape(preds, target)\n-        squared_error = torch.pow(preds - target, 2)\n+        sum_squared_error, n_obs = _mean_squared_error_update(preds, target)\n\n-        self.sum_squared_error += torch.sum(squared_error)\n-        self.total += target.numel()\n+        self.sum_squared_error += sum_squared_error\n+        self.total += n_obs\n\ndef compute(self):\n\"\"\"\nComputes mean squared error over state.\n\"\"\"\n-        return self.sum_squared_error / self.total\n+        return _mean_squared_error_compute(self.sum_squared_error, self.total)\n", "fix_pattern": "<condition>: \nNo clear condition is needed.\n\n<pattern>: \nThe pattern detected is that the computation of mean squared error is being performed incorrectly.\n\n<code_one>: \nThe code that is being removed is:\n\n        self._check_same_shape(preds, target)\n        squared_error = torch.pow(preds - target, 2)\n        self.sum_squared_error += torch.sum(squared_error)\n        self.total += target.numel()\n        return self.sum_squared_error / self.total\n\n<code_two>: \nThe code that is being added is:\n\n        sum_squared_error, n_obs = _mean_squared_error_update(preds, target)\n        self.sum_squared_error += sum_squared_error\n        self.total += n_obs\n        return _mean_squared_error_compute(self.sum_squared_error, self.total)\n\nFix_pattern:\nIn the condition of no pre condition is needed, if the pattern of incorrect computation of mean squared error is detected, then remove the code that incorrectly computes the mean squared error and replace it with the code that correctly computes the mean squared error using the functions `_mean_squared_error_update` and `_mean_squared_error_compute`."}
{"number": 8045, "change": "def test_unnormalized_normal(kernel, jit):\nposterior.append(samples)\n\nposterior = torch.stack([sample[\"z\"] for sample in posterior])\n-    assert_equal(torch.mean(posterior), true_mean, prec=0.1)\n-    assert_equal(torch.std(posterior), true_std, prec=0.1)\n+    assert_close(torch.mean(posterior), true_mean, rtol=0.05)\n+    assert_close(torch.std(posterior), true_std, rtol=0.05)\n", "fix_pattern": "<condition>: N/A\n<pattern>: The pattern is to replace the use of `assert_equal` with `assert_close`.\n<code_one>: `assert_equal(torch.mean(posterior), true_mean, prec=0.1)`\n<code_two>: `assert_close(torch.mean(posterior), true_mean, rtol=0.05)`\nFix_pattern: In the condition where `assert_equal(torch.mean(posterior), true_mean, prec=0.1)` is used, the fix is to replace it with `assert_close(torch.mean(posterior), true_mean, rtol=0.05)` to fix the API misuse."}
{"number": 8166, "change": "def bjerksund_stensland(*,\nvolatilities=volatilities,\nstrikes=strikes,\nexpiries=expiries,\n-                forwards=forwards,\n+                spots=spots,\ndiscount_rates=discount_rates,\ncost_of_carries=cost_of_carries,\nis_call_options=is_call_options),\n# For put options, adjust inputs according to call-put transformation\n# function:  P(S, X, T, r, b, sigma) = C(X, S, T, r - b, -b, sigma)\ntf.where(is_call_options,\n-                bjerksund_stensland_model(forwards, strikes, expiries, discount_rates,\n+                bjerksund_stensland_model(spots, strikes, expiries, discount_rates,\ncost_of_carries, volatilities),\n-                bjerksund_stensland_model(strikes, forwards, expiries, discount_rates -\n+                bjerksund_stensland_model(strikes, spots, expiries, discount_rates -\ncost_of_carries, -cost_of_carries, volatilities)))\n\nreturn american_prices\n", "fix_pattern": "<condition>: Checking if the option is a put option\n<pattern>: Misuse of API parameters by passing wrong inputs\n<code_one>: bjerksund_stensland_model(forwards, strikes, expiries, discount_rates,\n<code_two>: bjerksund_stensland_model(spots, strikes, expiries, discount_rates,\nFix_pattern: In the condition of checking if the option is a put option, if the misuse of API parameters by passing wrong inputs is detected, then change the bjerksund_stensland_model function call parameter from forwards to spots to fix the API misuse."}
{"number": 8205, "change": "def model(y):\n# Vector of variances for each of the d variables\ntheta = pyro.sample(\"theta\", dist.HalfCauchy(torch.ones(d, **options)))\n# Lower cholesky factor of a correlation matrix\n-    eta = torch.ones(1, **options)  # Implies a uniform distribution over correlation matrices\n-    L_omega = pyro.sample(\"L_omega\", dist.LKJCorrCholesky(d, eta))\n+    concentration = torch.ones((), **options)  # Implies a uniform distribution over correlation matrices\n+    L_omega = pyro.sample(\"L_omega\", dist.LKJCholesky(d, concentration))\n# Lower cholesky factor of the covariance matrix\nL_Omega = torch.mm(torch.diag(theta.sqrt()), L_omega)\n# For inference with SVI, one might prefer to use torch.bmm(theta.sqrt().diag_embed(), L_omega)\n", "fix_pattern": "<condition>: The code is using the dist.LKJCorrCholesky() function from Pyro library for sampling a lower Cholesky factor of a correlation matrix.\n<pattern>: The pattern is that the code is using an old version of the dist.LKJCorrCholesky() function which takes a single value \"eta\" as an argument.\n<code_one>: The code is using \"eta\" as an argument in the dist.LKJCorrCholesky() function.\n<code_two>: The code should use \"concentration\" as an argument in the updated version of the dist.LKJCholesky() function.\nFix_pattern: In the condition of using the dist.LKJCorrCholesky() function with the \"eta\" argument, replace the \"eta\" argument with the \"concentration\" argument in the updated dist.LKJCholesky() function to fix the API misuse."}
{"number": 8242, "change": "class DistilBertModelIntergrationTest(unittest.TestCase):\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ninput_ids = torch.tensor([[0, 345, 232, 328, 740, 140, 1695, 69, 6078, 1588, 2]])\nattention_mask = torch.tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n-        output = model(input_ids, attention_mask=attention_mask)[0]\n+        with torch.no_grad():\n+            output = model(input_ids, attention_mask=attention_mask)[0]\nexpected_shape = torch.Size((1, 11, 768))\nself.assertEqual(output.shape, expected_shape)\nexpected_slice = torch.tensor(\n", "fix_pattern": "<condition>: API misuse in torch model prediction.\n<pattern>: Missing torch.no_grad() context manager.\n<code_one>: output = model(input_ids, attention_mask=attention_mask)[0]\n<code_two>: with torch.no_grad(): output = model(input_ids, attention_mask=attention_mask)[0]\nFix_pattern: In the condition of API misuse in torch model prediction, if missing torch.no_grad() context manager is detected, then change the <code_one> to <code_two> to fix the API misuse."}
{"number": 8259, "change": "class Model(object):\nelse:\nassert not config.global_model and config.session is None\ntf.reset_default_graph()\n-            self.session = tf.Session()\n+            self.session = config.session = tf.Session()\n\nif config.distributed and not config.global_model:\n# Global and local model for asynchronous updates\n", "fix_pattern": "<condition>: `config.distributed and not config.global_model` is True.\n<pattern>: `self.session` is not set correctly.\n<code_one>: `self.session = tf.Session()`.\n<code_two>: `self.session = config.session = tf.Session()`.\nFix_pattern: In the condition of `config.distributed` being True and `config.global_model` being False, the `self.session` is not set correctly. To fix this API misuse, change the code from `self.session = tf.Session()` to `self.session = config.session = tf.Session()`."}
{"number": 8273, "change": "def test(loader):\ntarget = batch['user', 'item'].edge_label.long().cpu()\n\npreds.append(pred)\n-        targets.append(pred)\n+        targets.append(target)\n\npred = torch.cat(preds, dim=0).numpy()\n-    target = torch.cat(target, dim=0).numpy()\n+    target = torch.cat(targets, dim=0).numpy()\n\n+    pred = pred > 0.5\nacc = accuracy_score(target, pred)\nprec = precision_score(target, pred)\nrec = recall_score(target, pred)\n", "fix_pattern": "<condition>: The condition is not clearly specified in the given code.\n\n<pattern>: The pattern is to replace the code that appends 'pred' to the 'targets' list.\n\n<code_one>: The code that gets removed is \"targets.append(pred)\".\n\n<code_two>: The code that replaces <code_one> is \"targets.append(target)\".\n\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then remove the <code_one> and add <code_two> to fix the API misuse."}
{"number": 8283, "change": "from ray.rllib.optimizers import SampleBatch, TFMultiGPUSupport\nclass DQNEvaluator(TFMultiGPUSupport):\n\"\"\"The base DQN Evaluator that does not include the replay buffer.\"\"\"\n\n-    def __init__(self, env_creator, config, logdir):\n+    def __init__(self, registry, env_creator, config, logdir):\nenv = env_creator()\n-        env = wrap_dqn(env, config[\"model\"])\n+        env = wrap_dqn(registry, env, config[\"model\"])\nself.env = env\nself.config = config\n\ntf_config = tf.ConfigProto(**config[\"tf_session_args\"])\nself.sess = tf.Session(config=tf_config)\n-        self.dqn_graph = models.DQNGraph(env, config, logdir)\n+        self.dqn_graph = models.DQNGraph(registry, env, config, logdir)\n\n# Create the schedule for exploration starting from 1.\nself.exploration = LinearSchedule(\n", "fix_pattern": "<condition>: The code is trying to instantiate the DQNEvaluator class.\n<pattern>: The DQNEvaluator class has a constructor that takes an additional argument \"registry\".\n<code_one>: def __init__(self, env_creator, config, logdir):\n<code_two>: def __init__(self, registry, env_creator, config, logdir):\nFix_pattern: In the condition of instantiating the DQNEvaluator class, if the pattern of missing \"registry\" argument is detected, then add the \"registry\" argument to the constructor to fix the API misuse."}
{"number": 8296, "change": "def ctc_label_dense_to_sparse(labels, label_lengths):\nmax_num_labels_tns = tf.pack([label_shape[1]])\n\ndef range_less_than(previous_state, current_input):\n-        return tf.expand_dims(tf.range(label_shape[1]), 0) < current_input\n+        return tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(max_num_labels_tns, current_input)\n\n-    init = tf.cast(tf.fill(max_num_labels_tns, 0), tf.bool)\n+    init = tf.cast(tf.fill([1, label_shape[1]], 0), tf.bool)\ndense_mask = functional_ops.scan(range_less_than, label_lengths,\ninitializer=init, parallel_iterations=1)\ndense_mask = dense_mask[:, 0, :]\n", "fix_pattern": "<condition>: The condition is when we need to create a dense mask for label_lengths.\n<pattern>: The pattern is to check if the range of labels is less than the current input.\n<code_one>: The code being removed is \"tf.expand_dims(tf.range(label_shape[1]), 0) < current_input\".\n<code_two>: The code being added is \"tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(max_num_labels_tns, current_input)\".\nFix_pattern: In the condition of creating a dense mask for label_lengths, if the range of labels is less than the current input, then change the code from \"tf.expand_dims(tf.range(label_shape[1]), 0) < current_input\" to \"tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(max_num_labels_tns, current_input)\" to fix the API misuse."}
{"number": 8304, "change": "class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix\n)\n\n# Build new embeddings\n-        new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim).to(self.device)\n+        new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim).to(\n+            self.device, dtype=old_embeddings.weight.dtype\n+        )\n\n# initialize all new embeddings (in particular added tokens)\nself._init_weights(new_embeddings)\n", "fix_pattern": "<condition>: The code is initializing new embeddings.\n<pattern>: The code is not specifying the data type of the embeddings.\n<code_one>: nn.Embedding(new_num_tokens, old_embedding_dim).to(self.device)\n<code_two>: nn.Embedding(new_num_tokens, old_embedding_dim).to(self.device, dtype=old_embeddings.weight.dtype)\nFix_pattern: In the condition of initializing new embeddings, if the code does not specify the data type, then change the code_one to code_two to fix the API misuse."}
{"number": 8311, "change": "def resize_images(X, height_factor, width_factor, dim_ordering):\npositive integers.\n'''\nif dim_ordering == 'th':\n+        original_shape = int_shape(X)\nnew_shape = tf.shape(X)[2:]\nnew_shape *= tf.constant(np.array([height_factor, width_factor]).astype('int32'))\nX = permute_dimensions(X, [0, 2, 3, 1])\nX = tf.image.resize_nearest_neighbor(X, new_shape)\n-        return permute_dimensions(X, [0, 3, 1, 2])\n+        X = permute_dimensions(X, [0, 3, 1, 2])\n+        X.set_shape((None, None, original_shape[2] * height_factor, original_shape[3] * width_factor))\n+        return X\nelif dim_ordering == 'tf':\n+        original_shape = int_shape(X)\nnew_shape = tf.shape(X)[1:3]\nnew_shape *= tf.constant(np.array([height_factor, width_factor]).astype('int32'))\n-        return tf.image.resize_nearest_neighbor(X, new_shape)\n+        X = tf.image.resize_nearest_neighbor(X, new_shape)\n+        X.set_shape((None, original_shape[1] * height_factor, original_shape[2] * width_factor, None))\n+        return X\nelse:\nraise Exception('Invalid dim_ordering: ' + dim_ordering)\n", "fix_pattern": "<condition>: dim_ordering == 'th' or dim_ordering == 'tf'\n<pattern>: if 'th' then remove permute_dimensions(X, [0, 3, 1, 2]) and remove tf.image.resize_nearest_neighbor(X, new_shape), if 'tf' then remove permute_dimensions(X, [0, 2, 3, 1]) and remove tf.image.resize_nearest_neighbor(X, new_shape)\n<code_one>: permute_dimensions(X, [0, 3, 1, 2]) or permute_dimensions(X, [0, 2, 3, 1])\n<code_two>: permute_dimensions(X, [0, 3, 1, 2]) or tf.image.resize_nearest_neighbor(X, new_shape)\nFix_pattern: In the condition of dim_ordering, if 'th' is detected, then remove permute_dimensions(X, [0, 3, 1, 2]) and remove tf.image.resize_nearest_neighbor(X, new_shape). Instead, add permute_dimensions(X, [0, 3, 1, 2]). If 'tf' is detected, then remove permute_dimensions(X, [0, 2, 3, 1]) and remove tf.image.resize_nearest_neighbor(X, new_shape). Instead, add tf.image.resize_nearest_neighbor(X, new_shape)."}
{"number": 8319, "change": "class BaseModel():\nsave_filename = '%s_net_%s.pth' % (which_epoch, name)\nsave_path = os.path.join(self.save_dir, save_filename)\nnet = getattr(self, 'net' + name)\n-                torch.module.save(net.cpu().state_dict(), save_path)\n+                torch.save(net.module.cpu().state_dict(), save_path)\nif len(self.gpu_ids) and torch.cuda.is_available():\nnet.cuda(self.gpu_ids[0])\n", "fix_pattern": "<condition>: The code is saving a PyTorch model using the `torch.module.save()` function.\n \n<pattern>: The code is using the `torch.module.save()` function to save the model.\n\n<code_one>: `torch.module.save(net.cpu().state_dict(), save_path)`\n\n<code_two>: `torch.save(net.module.cpu().state_dict(), save_path)`\n\nFix_pattern: In the condition of saving a PyTorch model, if `torch.module.save()` is detected, then change `torch.module.save()` to `torch.save()` to fix the API misuse."}
{"number": 8358, "change": "def clip(\n*,\nout: Optional[Union[tf.Tensor, tf.Variable]] = None,\n) -> Union[tf.Tensor, tf.Variable]:\n+    assert tf.reduce_all(tf.less(x_min, x_max)), \"Min value must be less than max.\"\nif hasattr(x_min, \"dtype\") and hasattr(x_max, \"dtype\"):\npromoted_type = tf.experimental.numpy.promote_types(x.dtype, x_min.dtype)\npromoted_type = tf.experimental.numpy.promote_types(promoted_type, x_max.dtype)\n", "fix_pattern": "<condition>: The condition is that the variables x_min and x_max have the attribute \"dtype\".\n<pattern>: The pattern is that the minimum value must be less than the maximum value.\n<code_one>: There is no code that needs to be removed in this fix pattern.\n<code_two>: The code that needs to be added is: \"assert tf.reduce_all(tf.less(x_min, x_max)), \"Min value must be less than max.\"\nFix_pattern: In the condition of having the \"dtype\" attribute for x_min and x_max, if the pattern of the minimum value being less than the maximum value is not met, then the code should be added to raise an assertion error."}
{"number": 8385, "change": "class DiceCoeff(Function):\n\ndef forward(self, input, target):\nself.save_for_backward(input, target)\n-        self.inter = torch.dot(input.view(-1), target.view(-1)) + 0.0001\n-        self.union = torch.sum(input) + torch.sum(target) + 0.0001\n+        eps = 0.0001\n+        self.inter = torch.dot(input.view(-1), target.view(-1))\n+        self.union = torch.sum(input) + torch.sum(target) + eps\n\n-        t = 2 * self.inter.float() / self.union.float()\n+        t = (2 * self.inter.float() + eps) / self.union.float()\nreturn t\n\n# This function has only a single output, so it gets only one gradient\n", "fix_pattern": "<condition>: The forward() function in the DiceCoeff class.\n<pattern>: Detecting the occurrence of adding 0.0001 to the calculation of self.inter and self.union.\n<code_one>: Adding 0.0001 to the calculation of self.inter and self.union.\n<code_two>: Replacing the addition of 0.0001 with the variable eps.\nFix_pattern: In the condition of the forward() function, if the pattern of adding 0.0001 is detected to the calculation of self.inter and self.union, then remove the addition of 0.0001 and replace it with the variable eps to fix the API misuse."}
{"number": 8394, "change": "def cross_entropy_seq(logits, target_seqs):#, batch_size=1, num_steps=None):\nloss = sequence_loss_by_example_fn(\n[logits],\n[tf.reshape(target_seqs, [-1])],\n-        [tf.ones_like(tf.reshape(targets, [-1]), dtype=tf.float32)])\n+        [tf.ones_like(tf.reshape(target_seqs, [-1]), dtype=tf.float32)])\n# [tf.ones([batch_size * num_steps])])\n-    cost = tf.reduce_sum(loss) / batch_size\n+    cost = tf.reduce_sum(loss) #/ batch_size\nreturn cost\n", "fix_pattern": "<condition>: The condition is not clearly specified in the given context.\n\n<pattern>: The pattern is not clearly specified in the given code.\n\n<code_one>: The code that was removed is \"[tf.ones_like(tf.reshape(targets, [-1]), dtype=tf.float32)])\\ncost = tf.reduce_sum(loss) / batch_size\".\n\n<code_two>: The code that was added is \"[tf.ones_like(tf.reshape(target_seqs, [-1]), dtype=tf.float32)])\\ncost = tf.reduce_sum(loss)\".\n\nFix_pattern: In the condition, if there is a specific pattern detected, the code \"[tf.ones_like(tf.reshape(targets, [-1]), dtype=tf.float32)])\\ncost = tf.reduce_sum(loss) / batch_size\" should be changed to \"[tf.ones_like(tf.reshape(target_seqs, [-1]), dtype=tf.float32)])\\ncost = tf.reduce_sum(loss)\"."}
{"number": 8440, "change": "def stats(policy: Policy, train_batch: SampleBatch):\nstats_dict[\"var_IS\"] = is_stat_var\n\nif policy.config[\"use_kl_loss\"]:\n-        stats_dict[\"kl\"] = policy.get_tower_stats(\"mean_kl_loss\")\n+        stats_dict[\"kl\"] = torch.mean(\n+            torch.stack(policy.get_tower_stats(\"mean_kl_loss\")))\nstats_dict[\"KL_Coeff\"] = policy.kl_coeff\n\nreturn stats_dict\n", "fix_pattern": "<condition>: The condition is whether the policy.config[\"use_kl_loss\"] is True.\n<pattern>: The pattern is that the mean of the \"mean_kl_loss\" stats is calculated incorrectly.\n<code_one>: The incorrect code is `stats_dict[\"kl\"] = policy.get_tower_stats(\"mean_kl_loss\")`.\n<code_two>: The correct code is `stats_dict[\"kl\"] = torch.mean(torch.stack(policy.get_tower_stats(\"mean_kl_loss\")))`.\nFix_pattern: In the condition of `policy.config[\"use_kl_loss\"]` being True, if the pattern of incorrectly calculating the mean of \"mean_kl_loss\" stats is detected, then the incorrect code `stats_dict[\"kl\"] = policy.get_tower_stats(\"mean_kl_loss\")` should be changed to the correct code `stats_dict[\"kl\"] = torch.mean(torch.stack(policy.get_tower_stats(\"mean_kl_loss\")))`. This will fix the API misuse."}
{"number": 8475, "change": "class BertModel(BertPreTrainedModel):\nseq_ids = torch.arange(seq_length, device=device)\ncausal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\ncausal_mask = causal_mask.to(\n-                    torch.long\n-                )  # not converting to long will cause errors with pytorch version < 1.3\n+                    attention_mask.dtype\n+                )  # causal and attention masks must have same type with pytorch version < 1.3\nextended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\nelse:\nextended_attention_mask = attention_mask[:, None, None, :]\n", "fix_pattern": "<condition>: The condition is when the PyTorch version is less than 1.3.\n<pattern>: The pattern is the incorrect data type of the attention mask.\n<code_one>: The code that needs to be removed is \"torch.long\".\n<code_two>: The code that needs to be added is \"attention_mask.dtype\".\nFix_pattern: In the condition when the PyTorch version is less than 1.3, if the incorrect data type of the attention mask is detected, then remove the code \"torch.long\" and add the code \"attention_mask.dtype\" to fix the API misuse."}
{"number": 8538, "change": "class Module(object):\nuse_while_v2=False\n):\nModule.global_scope.append('while')\n-        if maximum_iterations is not None and maximum_iterations.dtype not in (tf.int32, tf.int64):\n+        if maximum_iterations is not None and maximum_iterations.dtype is not tf.int32:\nmaximum_iterations = tf.dtypes.cast(x=maximum_iterations, dtype=tf.int32)\nif use_while_v2:\nx = while_v2.while_loop(\n", "fix_pattern": "<condition>: The condition is that maximum_iterations is not None and maximum_iterations.dtype is not tf.int32.\n<pattern>: The pattern is that maximum_iterations.dtype needs to be checked and compared with tf.int32.\n<code_one>: The code removed is \"maximum_iterations.dtype not in (tf.int32, tf.int64)\".\n<code_two>: The code added is \"maximum_iterations.dtype is not tf.int32\".\nFix_pattern: In the condition of checking the dtype of maximum_iterations, if the dtype is not tf.int32, change the code from checking \"not in (tf.int32, tf.int64)\" to \"is not tf.int32\" to fix the API misuse."}
{"number": 8551, "change": "class PartialFC(Module):\nlogits.backward(grad)\nif total_features.grad is not None:\ntotal_features.grad.detach_()\n-        x_grad: torch.Tensor = torch.zeros_like(features)\n-        x_grad.mul_(self.world_size)\n-\n+        x_grad: torch.Tensor = torch.zeros_like(features, requires_grad=True)\n# feature gradient all-reduce\ndist.reduce_scatter(x_grad, list(total_features.grad.chunk(self.world_size, dim=0)))\n+        x_grad = x_grad * self.world_size\n# backward backbone\nreturn x_grad, loss_v\n", "fix_pattern": "<condition>: The condition is when the \"total_features.grad\" variable is not None.\n\n<pattern>: The pattern is to multiply \"x_grad\" by \"self.world_size\".\n\n<code_one>: The original code was \"x_grad.mul_(self.world_size)\".\n\n<code_two>: The fixed code is \"x_grad = x_grad * self.world_size\".\n\nFix_pattern: In the condition of \"total_features.grad is not None\", if the pattern of multiplying \"x_grad\" by \"self.world_size\" is detected, then change the code from \"x_grad.mul_(self.world_size)\" to \"x_grad = x_grad * self.world_size\" to fix the API misuse."}
{"number": 8686, "change": "class TransfoXLModel(TransfoXLPreTrainedModel):\nelse:\nmask_shift_len = qlen\ndec_attn_mask = (torch.triu(all_ones, 1+mlen)\n-                    + torch.tril(all_ones, -mask_shift_len)).byte()[:, :, None] # -1\n+                    + torch.tril(all_ones, -mask_shift_len)).bool()[:, :, None] # -1\nelse:\ndec_attn_mask = torch.triu(\n-                word_emb.new_ones(qlen, klen), diagonal=1+mlen).byte()[:,:,None]\n+                word_emb.new_ones(qlen, klen), diagonal=1+mlen).bool()[:,:,None]\n\nhids = []\nattentions = []\n", "fix_pattern": "<condition>: The condition is not clearly stated in the given context.\n\n<pattern>: The pattern is the usage of the method `byte()` to convert a tensor to a byte tensor.\n\n<code_one>: The code being removed is `byte()[:, :, None]` in two places.\n\n<code_two>: The code being added is `bool()[:, :, None]` in the same two places.\n\nFix_pattern: In the condition of <condition>, if <pattern> is detected, then change the <code_one> to <code_two> to fix the API misuse."}
{"number": 8693, "change": "class EarlyStopping(Callback):\nf\" {self.monitor} = {current} {self.order_dict[self.mode]} {self.divergence_threshold}.\"\n\" Signaling Trainer to stop.\"\n)\n-        elif self.monitor_op(current - self.min_delta, self.best_score.to(trainer.lightning_module.device)):\n+        elif self.monitor_op(current - self.min_delta, self.best_score.to(current.device)):\nshould_stop = False\nreason = self._improvement_message(current)\nself.best_score = current\n", "fix_pattern": "<condition>:\nThe condition is when there is an \"elif\" statement in the code.\n\n<pattern>:\nThe pattern is that the code in the \"elif\" statement is comparing the \"current - min_delta\" value with \"best_score.to(trainer.lightning_module.device)\".\n\n<code_one>:\nThe code being removed is \"self.best_score.to(trainer.lightning_module.device)\".\n\n<code_two>:\nThe code being added is \"self.best_score.to(current.device)\".\n\nFix_pattern:\nIn the condition of \"elif\", if the pattern of comparing \"current - min_delta\" with \"best_score.to(trainer.lightning_module.device)\" is detected, then the code \"self.best_score.to(trainer.lightning_module.device)\" should be changed to \"self.best_score.to(current.device)\" to fix the API misuse."}
{"number": 8698, "change": "def tensordot(\nout: Optional[Union[tf.Tensor, tf.Variable]] = None,\n) -> Union[tf.Tensor, tf.Variable]:\n# find type to promote to\n-    dtype = tf.experimental.numpy.promote_types(x1.dtype, x2.dtype)\n+    dtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))\n\n# type casting to float32 which is acceptable for tf.tensordot\nx1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)\n", "fix_pattern": "<condition>: In the given context, there is no clear condition identified.\n\n<pattern>: In the given code removal section, the pattern is to remove the line of code that involves the use of \"tf.experimental.numpy.promote_types\".\n\n<code_one>: The code that needs to be removed is \"dtype = tf.experimental.numpy.promote_types(x1.dtype, x2.dtype)\".\n\n<code_two>: The code that needs to be added is \"dtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))\".\n\nFix_pattern: In the condition of no pre-condition needed, if the pattern of using \"tf.experimental.numpy.promote_types\" is detected, then remove the line \"dtype = tf.experimental.numpy.promote_types(x1.dtype, x2.dtype)\" and add the line \"dtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))\" to fix the API misuse."}
{"number": 8699, "change": "def log_cosh(y_true, y_pred):\n>>> x = y_pred - y_true\n>>> assert np.allclose(\n...     loss.numpy(),\n-  ...     np.mean(x + np.log(np.exp(-2. * x) + 1.) - math_ops.log(2.), axis=-1),\n+  ...     np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.), axis=-1),\n...     atol=1e-5)\n\nArgs:\n", "fix_pattern": "<condition>: The condition is not specified in the given information.\n\n<pattern>: The pattern that needs to be detected is the misuse of the API `math_ops.log()`.\n\n<code_one>: The code that needs to be removed is `math_ops.log()`.\n\n<code_two>: The code that needs to be added is `tf.math.log()`.\n\nFix_pattern: In the condition of unmentioned condition, if the misuse of the API `math_ops.log()` is detected, then the `math_ops.log()` code should be removed and replaced with `tf.math.log()` to fix the API misuse."}
{"number": 8738, "change": "class EntityLinker(flair.nn.DefaultClassifier[Sentence, Span]):\n)\n\ndef emb_mean(self, span, embedding_names):\n-        return torch.mean(torch.cat([token.get_embedding(embedding_names) for token in span], 0), 0)\n+        return torch.mean(torch.stack([token.get_embedding(embedding_names) for token in span], 0), 0)\n\ndef _get_data_points_from_sentence(self, sentence: Sentence) -> List[Span]:\nreturn sentence.get_spans(self.label_type)\n", "fix_pattern": "<condition>: The code is a part of a class called \"EntityLinker\" that subclasses \"flair.nn.DefaultClassifier\" and implements a method called \"emb_mean\".\n\n<pattern>: The pattern here is the usage of the \"torch.cat\" function to concatenate embeddings.\n\n<code_one>: The code that needs to be changed is \"torch.cat([token.get_embedding(embedding_names) for token in span], 0), 0)\".\n\n<code_two>: The code that should replace the above code is \"torch.stack([token.get_embedding(embedding_names) for token in span], 0), 0)\".\n\nFix_pattern: In the condition of the \"emb_mean\" method, if the usage of \"torch.cat\" is detected, then change the code of \"torch.cat\" to \"torch.stack\" to fix the API misuse."}
{"number": 8745, "change": "def finish_episode():\nrewards = torch.Tensor(rewards)\nrewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\nfor (log_prob, value), r in zip(saved_actions, rewards):\n-        reward = r - value.data[0, 0]\n+        reward = r - value.data[0]\npolicy_losses.append(-log_prob * reward)\nvalue_losses.append(F.smooth_l1_loss(value, Variable(torch.Tensor([r]))))\noptimizer.zero_grad()\n-    loss = torch.cat(policy_losses).sum() + torch.cat(value_losses).sum()\n+    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\nloss.backward()\noptimizer.step()\ndel model.rewards[:]\n", "fix_pattern": "<condition>: The condition is when calculating the reward in the given code snippet.\n<pattern>: The pattern is to access the value element in the tensor using indexing.\n<code_one>: The code being removed is `value.data[0, 0]`.\n<code_two>: The code being added is `value.data[0]`.\nFix_pattern: In the condition of calculating the reward, if the code accessing the value element in the tensor using indexing is `value.data[0, 0]`, then change it to `value.data[0]` to fix the API misuse."}
{"number": 8748, "change": "def _get_random_features_initializer(initializer, shape):\nrandom_features_initializer = initializer\nif isinstance(initializer, str):\nif initializer.lower() == 'gaussian':\n-      random_features_initializer = tf.compat.v1.random_normal_initializer(\n-          stddev=1.0)\n+      random_features_initializer = initializers.RandomNormal(stddev=1.0)\nelif initializer.lower() == 'laplacian':\n-      random_features_initializer = tf.compat.v1.constant_initializer(\n+      random_features_initializer = initializers.Constant(\n_get_cauchy_samples(loc=0.0, scale=1.0, shape=shape))\n\nelse:\n", "fix_pattern": "<condition>: Checking if the initializer is a string\n<pattern>: If the initializer is a string and it matches a specific string value\n<code_one>: Assigning a specific initializer based on the matched string value using TensorFlow's APIs\n<code_two>: Assigning a specific initializer based on the matched string value using the Keras' initializers module\nFix_pattern: In the condition of checking if the initializer is a string, if a specific string value is detected, then change the assignment of the initializer from TensorFlow's API to Keras' initializer module to fix the API misuse."}
{"number": 8765, "change": "class SyncMultiGPUTrainer(MultiGPUTrainer,\nsuper(SyncMultiGPUTrainer, self)._setup()\ngrad_list = MultiGPUTrainer._multi_tower_grads(\nself.config.tower, lambda: self._get_cost_and_grad()[1])\n+\n+        # debug tower performance:\n+        #ops = [k[0] for k in grad_list[1]] + [k[0] for k in grad_list[0]]\n+        #self.train_op = tf.group(*ops)\n+        #return\n+\ngrads = SyncMultiGPUTrainer._average_grads(grad_list)\ngrads = apply_grad_processors(grads, self.model.get_gradient_processor())\n", "fix_pattern": "<condition>: No pre condition is needed.\n<pattern>: No pattern is detected.\n<code_one>: No code is removed.\n<code_two>: No code is added.\nFix_pattern: No fix pattern is identified."}
{"number": 8824, "change": "def _replace_global_by_local(kwargs):\nif 'collections' in kwargs:\ncollections = kwargs['collections']\nif not collections:\n-        collections = {tf.GraphKeys.GLOBAL_VARIABLES}\n+        collections = {tfv1.GraphKeys.GLOBAL_VARIABLES}\nelse:\ncollections = set(collections.copy())\n-    collections.remove(tf.GraphKeys.GLOBAL_VARIABLES)\n-    collections.add(tf.GraphKeys.LOCAL_VARIABLES)\n+    collections.remove(tfv1.GraphKeys.GLOBAL_VARIABLES)\n+    collections.add(tfv1.GraphKeys.LOCAL_VARIABLES)\nkwargs['collections'] = list(collections)\n", "fix_pattern": "<condition>: If 'collections' is in the dictionary 'kwargs'.\n<pattern>: If the set 'collections' contains the value 'tf.GraphKeys.GLOBAL_VARIABLES'.\n<code_one>: collections.remove(tf.GraphKeys.GLOBAL_VARIABLES)\n<code_two>: collections.add(tfv1.GraphKeys.LOCAL_VARIABLES)\nFix_pattern: In the condition of 'if 'collections' is in the dictionary 'kwargs', if the set 'collections' contains the value 'tf.GraphKeys.GLOBAL_VARIABLES', then remove 'tf.GraphKeys.GLOBAL_VARIABLES' from 'collections' and add 'tfv1.GraphKeys.LOCAL_VARIABLES' to 'collections' to fix the API misuse."}
{"number": 8832, "change": "class OrthogonalRegularizer(Regularizer):\nsize = inputs.shape[1]\nproduct_no_diagonal = product * (1. - tf.eye(size, dtype=inputs.dtype))\nnum_pairs = size * (size - 1.) / 2.\n-    return self.factor * 0.5 * tf.reduce_sum(product_no_diagonal) / num_pairs\n+    return self.factor * 0.5 * tf.reduce_sum(tf.abs(product_no_diagonal)) / num_pairs\n\ndef get_config(self):\nreturn {'factor': float(self.factor), 'mode': self.mode}\n", "fix_pattern": "<condition>: The condition is when calculating the regularization term in the OrthogonalRegularizer class.\n<pattern>: The pattern is that the absolute values of product_no_diagonal should be calculated instead of just the product_no_diagonal itself.\n<code_one>: The code that needed to be removed is \"tf.reduce_sum(product_no_diagonal)\".\n<code_two>: The code that needed to be added is \"tf.reduce_sum(tf.abs(product_no_diagonal))\".\nFix_pattern: In the condition of calculating the regularization term in the OrthogonalRegularizer class, if the product_no_diagonal is detected, then change the code for calculating the sum of product_no_diagonal to calculate the sum of the absolute values of product_no_diagonal in order to fix the API misuse."}
{"number": 8901, "change": "def test(data,\n\n# Load model\ngoogle_utils.attempt_download(weights)\n-        model = torch.load(weights, map_location=device)['model'].float().fuse()  # load to FP32\n+        model = torch.load(weights, map_location=device)['model'].float().fuse().to(device)  # load to FP32\nimgsz = check_img_size(imgsz, s=model.stride.max())  # check img_size\n\n# Multi-GPU disabled, incompatible with .half() https://github.com/ultralytics/yolov5/issues/99\n", "fix_pattern": "<condition>: Multi-GPU disabled\n<pattern>: Incompatibility with .half()\n<code_one>: model = torch.load(weights, map_location=device)['model'].float().fuse()  # load to FP32\n<code_two>: model = torch.load(weights, map_location=device)['model'].float().fuse().to(device)  # load to FP32\nFix_pattern: In the condition of multi-GPU disabled, if there is an incompatibility with .half(), then change the code to load the model from FP32 to FP32 and move it to the specified device to fix the API misuse."}
{"number": 8947, "change": "class TorchDiagGaussian(TorchDistributionWrapper):\n@override(ActionDistribution)\ndef __init__(self, inputs, model):\nsuper().__init__(inputs, model)\n-        mean, log_std = torch.chunk(inputs, 2, dim=1)\n+        mean, log_std = torch.chunk(self.inputs, 2, dim=1)\nself.dist = torch.distributions.normal.Normal(mean, torch.exp(log_std))\n\n@override(ActionDistribution)\n", "fix_pattern": "<condition>: When accessing attributes of an object within a class method.\n<pattern>: Assigning attribute values from the input arguments directly to variables.\n<code_one>: mean, log_std = torch.chunk(inputs, 2, dim=1)\n<code_two>: mean, log_std = torch.chunk(self.inputs, 2, dim=1)\nFix_pattern: In the condition of accessing attributes of an object within a class method, if assigning attribute values from the input arguments directly to variables is detected, then change the code from \"mean, log_std = torch.chunk(inputs, 2, dim=1)\" to \"mean, log_std = torch.chunk(self.inputs, 2, dim=1)\" to fix the API misuse."}
{"number": 8951, "change": "def clip(\n*,\nout: Optional[torch.Tensor] = None,\n) -> torch.Tensor:\n+    assert torch.all(torch.less(x_min, x_max)), \"Min value must be less than max.\"\nif hasattr(x_min, \"dtype\"):\npromoted_type = torch.promote_types(x_min.dtype, x_max.dtype)\npromoted_type = torch.promote_types(promoted_type, x.dtype)\n", "fix_pattern": "<condition>: Condition is not clearly identified in the provided code snippet.\n<pattern>: No specific pattern detected.\n<code_one>: No code removed.\n<code_two>: assert torch.all(torch.less(x_min, x_max)), \"Min value must be less than max.\"\nFix_pattern: In this case, no specific fix pattern can be identified due to the lack of clear condition and pattern. However, the added code (assert statement) is ensuring that the minimum value (x_min) is less than the maximum value (x_max) in order to address a potential API misuse."}
{"number": 8997, "change": "def train(args, logdir):\nsteps_per_epoch=hp.train1.steps_per_epoch,\n# session_config=session_conf\n)\n-    ckpt = args.ckpt if args.ckpt else tf.train.latest_checkpoint(logdir)\n+    ckpt = '{}/{}'.format(logdir, args.ckpt) if args.ckpt else tf.train.latest_checkpoint(logdir)\nif ckpt:\ntrain_conf.session_init = SaverRestore(ckpt)\n", "fix_pattern": "<condition>: If `ckpt` is not provided as an argument or is empty.\n<pattern>: Update the value of `ckpt` to include `logdir` and `args.ckpt`.\n<code_one>: `ckpt = args.ckpt if args.ckpt else tf.train.latest_checkpoint(logdir)`\n<code_two>: `ckpt = '{}/{}'.format(logdir, args.ckpt) if args.ckpt else tf.train.latest_checkpoint(logdir)`\nFix_pattern: In the condition of `ckpt` not being provided or empty, update the value of `ckpt` to include `logdir` and `args.ckpt` to fix the API misuse."}
{"number": 9026, "change": "def is_variable(x):\n\n\ndef execute_with_gradients(func, xs, retain_grads=False):\n-    with _tf.GradientTape() as tape:\n+    with _tf.GradientTape(persistent=retain_grads, watch_accessed_variables=False) as tape:\n+        tape.watch(xs)\nfunc_ret = func(xs)\nif isinstance(func_ret, tuple):\ny = func_ret[0]\n", "fix_pattern": "<condition>: When using TensorFlow's GradientTape for calculating gradients.\n<pattern>: Not specifying the \"persistent\" and \"watch_accessed_variables\" parameters in the GradientTape context manager.\n<code_one>: \"with _tf.GradientTape() as tape:\"\n<code_two>: \"with _tf.GradientTape(persistent=retain_grads, watch_accessed_variables=False) as tape:\"\nFix_pattern: In the condition of using TensorFlow's GradientTape for calculating gradients, if the pattern of not specifying the \"persistent\" and \"watch_accessed_variables\" parameters is detected, then change the code from \"with _tf.GradientTape() as tape:\" to \"with _tf.GradientTape(persistent=retain_grads, watch_accessed_variables=False) as tape:\" to fix the API misuse."}
{"number": 9048, "change": "def test_sequence_tagger_param_selector(results_base_path, tasks_base_path):\n\n@pytest.mark.integration\ndef test_text_classifier_param_selector(results_base_path, tasks_base_path):\n-    corpus = flair.datasets.ClassificationCorpus(tasks_base_path / \"imdb\")\n+    corpus = flair.datasets.ClassificationCorpus(tasks_base_path / \"imdb\", label_type=\"sentiment\")\nlabel_type = \"sentiment\"\n\nsearch_space = SearchSpace()\n\n# document embeddings parameter\n-    search_space.add(Parameter.TRANSFORMER_MODEL, hp.choice, options=[\"albert-base-v1\"])\n+    search_space.add(Parameter.TRANSFORMER_MODEL, hp.choice, options=[\"sshleifer/tiny-distilbert-base-cased\"])\nsearch_space.add(Parameter.LAYERS, hp.choice, options=[\"-1\", \"-2\"])\n\n# training parameter\n", "fix_pattern": "<condition>: The code is testing a sequence tagger or text classifier param selector.\n\n<pattern>: The code is adding a search space parameter for the transformer model.\n\n<code_one>: The original code adds the \"albert-base-v1\" option to the search space.\n\n<code_two>: The fixed code adds the \"sshleifer/tiny-distilbert-base-cased\" option to the search space.\n\nFix_pattern: In the condition of testing a sequence tagger or text classifier param selector, if the code adds the \"albert-base-v1\" option to the search space, then remove it and add the \"sshleifer/tiny-distilbert-base-cased\" option instead to fix the API misuse."}
{"number": 9098, "change": "\"\\n\",\n\"\\n\",\n\"\\\"\\\"\\\"fm part\\\"\\\"\\\"\\n\",\n-    \"fm_first_order = tf.nn.embedding_lookup(weights['feature_embeddings'],feat_index)\\n\",\n+    \"fm_first_order = tf.nn.embedding_lookup(weights['feature_bias'],feat_index)\\n\",\n\"fm_first_order = tf.reduce_sum(tf.multiply(fm_first_order,reshaped_feat_value),2)\\n\",\n\"\\n\",\n\"summed_features_emb = tf.reduce_sum(embeddings,1)\\n\",\n", "fix_pattern": "<condition>: The condition is the presence of a specific code line related to embedding lookup.\n\n<pattern>: The pattern is the replacement of an embedding lookup for feature embeddings with an embedding lookup for feature bias.\n\n<code_one>: The code being removed is \"fm_first_order = tf.nn.embedding_lookup(weights['feature_embeddings'],feat_index)\\n\".\n\n<code_two>: The code being added is \"fm_first_order = tf.nn.embedding_lookup(weights['feature_bias'],feat_index)\\n\".\n\nFix_pattern: In the condition of embedding lookup for feature embeddings, if detected, then remove the code for embedding lookup for feature embeddings and replace it with the code for embedding lookup for feature bias to fix the API misuse."}
{"number": 9197, "change": "class Seq2SeqModel(ModelBase):\nif \"embedding\" in variable.name:\ntmp = tf.clip_by_norm(\ngradient.values, self.params[\"optimizer.clip_embed_gradients\"])\n-        grad = tf.IndexedSlices(tmp, gradient.indices, gradient.dense_shape)\n+        gradient = tf.IndexedSlices(tmp, gradient.indices, gradient.dense_shape)\nclipped_gradients.append(gradient)\nvariables.append(variable)\nreturn list(zip(clipped_gradients, variables))\n", "fix_pattern": "<condition>: checking if \"embedding\" is in the variable name.\n<pattern>: replacing `grad` with `gradient` in the line of code.\n<code_one>: `grad = tf.IndexedSlices(tmp, gradient.indices, gradient.dense_shape)`\n<code_two>: `gradient = tf.IndexedSlices(tmp, gradient.indices, gradient.dense_shape)`\nFix_pattern: In the condition of checking if \"embedding\" is in the variable name, if the code line `grad = tf.IndexedSlices(tmp, gradient.indices, gradient.dense_shape)` is detected, then replace `grad` with `gradient` to fix the API misuse."}
{"number": 9236, "change": "def median(\nkeepdims: Optional[bool] = False,\nout: Optional[torch.tensor] = None,\n) -> torch.tensor:\n+    temp = input\nif hasattr(axis, \"__iter__\"):\nfor dim in axis:\n-            input = torch.median(\n-                input,\n+            temp = torch.median(\n+                temp,\ndim=dim,\nkeepdim=keepdims,\n)[0]\n", "fix_pattern": "<condition>: `axis` is an iterable.\n<pattern>: `input` is passed directly to `torch.median()`.\n<code_one>: `input = torch.median(input,`\n<code_two>: `temp = torch.median(temp,`\nFix_pattern: In the condition of `axis` being an iterable, if `input` is passed directly to `torch.median()`, then replace `input = torch.median(input,` with `temp = torch.median(temp,` to fix the API misuse."}
{"number": 9246, "change": "class Dropout(Layer):\n\nskip_dropout = tf.math.logical_not(x=Module.retrieve_tensor(name='update'))\nzero = tf.constant(value=0.0, dtype=util.tf_dtype(dtype='float'))\n-        skip_dropout = tf.math.logical_or(x=apply_dropout, y=tf.math.equal(x=rate, y=zero))\n+        skip_dropout = tf.math.logical_or(x=skip_dropout, y=tf.math.equal(x=rate, y=zero))\nreturn self.cond(pred=skip_dropout, true_fn=no_dropout, false_fn=apply_dropout)\n", "fix_pattern": "<condition>: The condition is checking if the variable \"skip_dropout\" should be True or False.\n\n<pattern>: The pattern that is detected is an incomplete condition in the code that sets the \"skip_dropout\" variable.\n\n<code_one>: The code that is removed is \"skip_dropout = tf.math.logical_or(x=apply_dropout, y=tf.math.equal(x=rate, y=zero))\".\n\n<code_two>: The code that is added is \"skip_dropout = tf.math.logical_or(x=skip_dropout, y=tf.math.equal(x=rate, y=zero))\".\n\nFix_pattern: In the condition of setting the \"skip_dropout\" variable, if an incomplete condition is detected, then the code should be changed from \"<code_one>\" to \"<code_two>\" to fix the API misuse."}
{"number": 9260, "change": "def test_cpu_amp_precision_context_manager(tmpdir):\nassert not hasattr(plugin, \"scaler\")\ncontext_manager = plugin.autocast_context_manager()\nassert isinstance(context_manager, torch.cpu.amp.autocast)\n-    assert context_manager.dtype == torch.bfloat16\n+    assert context_manager.fast_dtype == torch.bfloat16\n\n\n@pytest.mark.skipif(not _TORCH_CPU_AMP_AVAILABLE, reason=\"Torch CPU AMP is not available.\")\n", "fix_pattern": "<condition>: The code is checking if the context manager's dtype is equal to torch.bfloat16.\n<pattern>: The code is checking the value of the dtype attribute of the context manager.\n<code_one>: assert context_manager.dtype == torch.bfloat16\n<code_two>: assert context_manager.fast_dtype == torch.bfloat16\nFix_pattern: In the condition of checking the dtype attribute of the context manager, the code was changed from context_manager.dtype to context_manager.fast_dtype to fix the API misuse."}
{"number": 9270, "change": "class Model(ModelDesc):\nzc = tf.one_hot(ids, 10, name='zc_train')\nzc = tf.placeholder_with_default(zc, [None, 10], name='zc')\n\n-        z = tf.random_uniform(tf.pack([tf.shape(zc)[0], 90]), -1, 1, name='z_train')\n+        z = tf.random_uniform(tf.stack([tf.shape(zc)[0], 90]), -1, 1, name='z_train')\nz = tf.placeholder_with_default(z, [None, 90], name='z')\n-        z = tf.concat(1, [zc, z], name='fullz')\n+        z = tf.concat_v2([zc, z], 1, name='fullz')\n\nwith argscope([Conv2D, Deconv2D, FullyConnected],\nW_init=tf.truncated_normal_initializer(stddev=0.02)):\n", "fix_pattern": "<condition>: When using tf.pack() to reshape a tensor\n<pattern>: Use tf.stack() instead of tf.pack()\n<code_one>: tf.pack([tf.shape(zc)[0], 90])\n<code_two>: tf.stack([tf.shape(zc)[0], 90])\nFix_pattern: In the condition of using tf.pack() to reshape a tensor, if tf.pack() is detected, then change tf.pack() to tf.stack() to fix the API misuse."}
{"number": 9328, "change": "class SignatureDict(NestedDict):\nassert isinstance(arg, TensorDict)\nargs.append(spec.kwargs_to_args(kwargs=arg))\nelse:\n-                assert isinstance(arg, (tf.IndexedSlices, tf.Tensor, tf.Variable))\n+                assert isinstance(arg, (tf.IndexedSlices, tf.Tensor, tf.Variable)), (name, spec, arg)\nargs.append(arg)\nreturn args\n", "fix_pattern": "<condition>: This fix pattern is applicable when a condition is checked in the code.\n<pattern>: The pattern to be detected is the incorrect condition check.\n<code_one>: The incorrect condition check is `assert isinstance(arg, (tf.IndexedSlices, tf.Tensor, tf.Variable))`.\n<code_two>: The correct condition check should be `assert isinstance(arg, (tf.IndexedSlices, tf.Tensor, tf.Variable)), (name, spec, arg)`.\nFix_pattern: In the condition of checking the type of `arg`, if the incorrect condition check is detected, then the incorrect condition check should be replaced with the correct condition check to fix the API misuse."}
{"number": 9412, "change": "def collect_env():\nenv_info['GCC'] = gcc\n\nenv_info['PyTorch'] = torch.__version__\n-    env_info['PyTorch compiling details'] = torch.__config__.show()\n+    env_info['PyTorch compiling details'] = get_build_config()\n\nenv_info['TorchVision'] = torchvision.__version__\n", "fix_pattern": "<condition>: The condition is when there is a need to retrieve PyTorch compiling details in the environment information.\n\n<pattern>: The pattern is that the code for retrieving PyTorch compiling details is changed.\n\n<code_one>: The code that is being removed is `env_info['PyTorch compiling details'] = torch.__config__.show()`\n\n<code_two>: The code that is being added is `env_info['PyTorch compiling details'] = get_build_config()`\n\nFix_pattern: In the condition of needing to retrieve PyTorch compiling details in the environment information, if the code `env_info['PyTorch compiling details'] = torch.__config__.show()` is detected, then change it to `env_info['PyTorch compiling details'] = get_build_config()` to fix the API misuse."}
{"number": 9490, "change": "class LabelSmoother:\n# will ignore them in any case.\nlabels.clamp_min_(0)\nnll_loss = log_probs.gather(dim=-1, index=labels)\n-        smoothed_loss = log_probs.sum(dim=-1, keepdim=True)\n+        # works for fp16 input tensor too, by internally upcasting it to fp32\n+        smoothed_loss = log_probs.sum(dim=-1, keepdim=True, dtype=torch.float32)\n\nnll_loss.masked_fill_(padding_mask, 0.0)\nsmoothed_loss.masked_fill_(padding_mask, 0.0)\n", "fix_pattern": "<condition>: API misuse in the calculation of smoothed_loss in the LabelSmoother class.\n<pattern>: The original code was using the sum() method without specifying the dtype, which caused issues with fp16 input tensors.\n<code_one>: smoothed_loss = log_probs.sum(dim=-1, keepdim=True)\n<code_two>: smoothed_loss = log_probs.sum(dim=-1, keepdim=True, dtype=torch.float32)\nFix_pattern: In the condition of API misuse, if the code_one pattern is detected, then change the code_one to code_two to fix the API misuse."}
{"number": 9535, "change": "def prod(\ndtype = tf.uint64\nif ivy.exists(out):\nreturn ivy.inplace_update(\n-            out, tf.experimental.numpy.prod(x, axis=axis, keepdims=keepdims)\n+            out,\n+            tf.experimental.numpy.prod(x, axis=axis, dtype=dtype, keepdims=keepdims),\n)\nelse:\nreturn tf.experimental.numpy.prod(x, axis, dtype, keepdims)\n", "fix_pattern": "<condition>: The condition is that the variable \"out\" already exists.\n<pattern>: The pattern is that the function \"tf.experimental.numpy.prod\" is called with parameters \"x\", \"axis\", \"keepdims\", and \"out\" is assigned the result.\n<code_one>: The code that is removed is \"out, tf.experimental.numpy.prod(x, axis=axis, keepdims=keepdims)\".\n<code_two>: The code that is added is \"out, tf.experimental.numpy.prod(x, axis=axis, dtype=dtype, keepdims=keepdims)\".\nFix_pattern: In the condition of \"out\" already existing, if the pattern of calling \"tf.experimental.numpy.prod\" with parameters \"x\", \"axis\", and \"keepdims\" is detected, then change the code from \"out, tf.experimental.numpy.prod(x, axis=axis, keepdims=keepdims)\" to \"out, tf.experimental.numpy.prod(x, axis=axis, dtype=dtype, keepdims=keepdims)\" to fix the API misuse."}
{"number": 9575, "change": "def is_cuda_available() -> bool:\nUnlike :func:`torch.cuda.is_available`, this function will do its best not to create a CUDA context for fork\nsupport, if the platform allows it.\n\"\"\"\n-    if \"fork\" not in torch.multiprocessing.get_all_start_methods():\n+    if \"fork\" not in torch.multiprocessing.get_all_start_methods() or _is_forking_disabled():\nreturn torch.cuda.is_available()\nwith multiprocessing.get_context(\"fork\").Pool(1) as pool:\nreturn pool.apply(torch.cuda.is_available)\n", "fix_pattern": "<condition>:\nNone\n\n<pattern>:\nChecking if the string \"fork\" is not in the list of all start methods obtained from torch.multiprocessing.\n\n<code_one>:\nif \"fork\" not in torch.multiprocessing.get_all_start_methods():\n\n<code_two>:\nAdding a condition to check if forking is disabled using the _is_forking_disabled() function.\n\nFix_pattern:\nIn the condition of checking if the string \"fork\" is not in the list of all start methods obtained from torch.multiprocessing, if forking is disabled, then change the code line to include the check for forking disabled using the _is_forking_disabled() function."}
{"number": 9631, "change": "class ConditionalDetrModelIntegrationTests(unittest.TestCase):\nresults = feature_extractor.post_process_object_detection(\noutputs, threshold=0.3, target_sizes=[image.size[::-1]]\n)[0]\n-        expected_scores = torch.tensor([0.8330, 0.8313, 0.8039, 0.6829, 0.5355])\n+        expected_scores = torch.tensor([0.8330, 0.8313, 0.8039, 0.6829, 0.5355]).to(torch_device)\nexpected_labels = [75, 17, 17, 75, 63]\n-        expected_slice_boxes = torch.tensor([38.3089, 72.1022, 177.6293, 118.4512])\n+        expected_slice_boxes = torch.tensor([38.3089, 72.1022, 177.6293, 118.4512]).to(torch_device)\n\nself.assertEqual(len(results[\"scores\"]), 5)\nself.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-4))\n", "fix_pattern": "<condition>: N/A\n<pattern>: If the code includes the variable 'expected_scores' and 'expected_slice_boxes' being defined using torch.tensor()\n<code_one>: expected_scores = torch.tensor([0.8330, 0.8313, 0.8039, 0.6829, 0.5355])\n<code_two>: expected_scores = torch.tensor([0.8330, 0.8313, 0.8039, 0.6829, 0.5355]).to(torch_device)\nFix_pattern: In the condition where 'expected_scores' and 'expected_slice_boxes' are defined using torch.tensor(), the code should be modified to include '.to(torch_device)' at the end of the tensor declaration to fix the API misuse."}
{"number": 9633, "change": "class GraphVarParam(HyperParam):\nself._readable_name, self.var_name = get_op_var_name(name)\n\ndef setup_graph(self):\n-        all_vars = tf.all_variables()\n+        try:\n+            all_vars = tf.global_variables()\n+        except:\n+            # TODO\n+            all_vars = tf.all_variables()\n+\nfor v in all_vars:\nif v.name == self.var_name:\nself.var = v\n", "fix_pattern": "<condition>:\nWhen dealing with TensorFlow operations.\n\n<pattern>:\nThe pattern is that the code in the condition was using the deprecated `tf.all_variables()` function to get all variables, which has been replaced with `tf.global_variables()`.\n\n<code_one>:\n`all_vars = tf.all_variables()`\n\n<code_two>:\n`all_vars = tf.global_variables()`\n\nFix_pattern:\nIn the condition of dealing with TensorFlow operations, if the code is using `tf.all_variables()`, then replace it with `tf.global_variables()` to fix the API misuse."}
{"number": 9636, "change": "class FAUST(Dataset):\nindex = self.index[:, i]\nweight = torch.FloatTensor(index.size(1)).fill_(1)\ninput = torch.FloatTensor(position.size(0)).fill_(1)\n-        adj = torch.sparse.FloatTensor(index, weight, torch.Size([75, 75]))\n+        adj = torch.sparse.FloatTensor(index, weight, torch.Size([6890, 6890]))\ndata = (input, adj, position)\n\nif self.correspondence:\n", "fix_pattern": "<condition>:\nThe condition is that the variable \"self.correspondence\" is true.\n\n<pattern>:\nThe pattern is that the dimension of the tensor \"adj\" needs to be changed.\n\n<code_one>:\nThe code that needs to be removed is:\n```\nadj = torch.sparse.FloatTensor(index, weight, torch.Size([75, 75]))\n```\n\n<code_two>:\nThe code that needs to be added is:\n```\nadj = torch.sparse.FloatTensor(index, weight, torch.Size([6890, 6890]))\n```\n\nFix_pattern:\nIn the condition of \"self.correspondence\" being true, if the tensor \"adj\" has a dimension of [75, 75], then change it to a dimension of [6890, 6890] to fix the API misuse."}
{"number": 9698, "change": "class TestElmo(AllenNlpTestCase):\ndataset = Dataset(instances)\nvocab = Vocabulary()\ndataset.index_instances(vocab)\n-        character_ids = dataset.as_array_dict()['elmo']['character_ids']\n+        character_ids = dataset.as_tensor_dict()['elmo']['character_ids']\n\n-        output = elmo(Variable(torch.from_numpy(character_ids)))\n+        output = elmo(character_ids)\nelmo_representations = output['elmo_representations']\nmask = output['mask']\n", "fix_pattern": "<condition>: The code is using a Dataset object and Vocabulary object.\n<pattern>: The code is using a method that returns a dictionary and then using the 'elmo' key to access a nested dictionary, and the 'character_ids' key to access a specific value.\n<code_one>: `dataset.as_array_dict()['elmo']['character_ids']`\n<code_two>: `dataset.as_tensor_dict()['elmo']['character_ids']`\nFix_pattern: In the condition of using a Dataset and Vocabulary object, if the pattern of accessing 'character_ids' through `dataset.as_array_dict()['elmo']['character_ids']` is detected, then change it to `dataset.as_tensor_dict()['elmo']['character_ids']` to fix the API misuse."}
