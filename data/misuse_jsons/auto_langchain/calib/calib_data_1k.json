[
    {
        "commit_hash": "197e7ce911d91d85eb2f91858720957c2d979cd2",
        "index": "49f363ff7..b8ab2c647 100644",
        "commit_message": "Fix device issue in a `ConvBertModelTest` test (#21438)\n\nfix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ConvBertModelTest(ModelTesterMixin, unittest.TestCase):",
            "def test_model_for_input_embeds(self):",
            "batch_size = 2",
            "seq_length = 10",
            "-        inputs_embeds = torch.rand([batch_size, seq_length, 768])",
            "+        inputs_embeds = torch.rand([batch_size, seq_length, 768], device=torch_device)",
            "config = self.model_tester.get_config()",
            "model = ConvBertModel(config=config)",
            "model.to(torch_device)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1176945)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1176946)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1176947)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1176948)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'torch_device'), position=2, insert_id=1176949)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1471,
        "neg_line": [
            "-inputs_embeds = torch.rand([batch_size, seq_length, 768])"
        ],
        "pos_line": [
            "+inputs_embeds = torch.rand([batch_size, seq_length, 768], device=torch_device)"
        ],
        "core_change": "-inputs_embeds = torch.rand([batch_size, seq_length, 768]) +inputs_embeds = torch.rand([batch_size, seq_length, 768], device=torch_device)",
        "core_API": "rand"
    },
    {
        "commit_hash": "2ac731a999c3ad0a5286766785e33501d128c531",
        "index": "5269aff5..c985921b 100644",
        "commit_message": "fix pulint issues\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "class Seq2Seq(Layer):",
            "return_seq_2d=False,",
            "name='seq2seq',",
            "):",
            "+        if cell_init_args is None:",
            "+            cell_init_args = {'state_is_tuple': True}",
            "+",
            "Layer.__init__(self, name=name)",
            "if cell_fn is None:",
            "raise Exception(\"Please put in cell_fn\")"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=2455702)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2455703)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=2455704)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2455705)",
            "Insert(target_node=IN(type=if_statement), node=('ERROR', None), position=3, insert_id=2455706)",
            "Insert(target_node=IN(type=if_statement), node=('block', ''), position=4, insert_id=2455707)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'cell_init_args'), position=0, insert_id=2455708)",
            "Insert(target_node=IN(type=comparison_operator), node=('is', 'is'), position=1, insert_id=2455709)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=2, insert_id=2455710)",
            "Insert(target_node=IN(type=ERROR), node=('block', None), position=0, insert_id=2455711)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2455712)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2455713)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'cell_init_args'), position=0, insert_id=2455714)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2455715)",
            "Insert(target_node=IN(type=assignment), node=('dictionary', None), position=2, insert_id=2455716)",
            "Insert(target_node=IN(type=dictionary), node=('{', '{'), position=0, insert_id=2455717)",
            "Insert(target_node=IN(type=dictionary), node=('pair', None), position=1, insert_id=2455718)",
            "Insert(target_node=IN(type=dictionary), node=('}', '}'), position=2, insert_id=2455719)",
            "Insert(target_node=IN(type=pair), node=('string', \"'state_is_tuple'\"), position=0, insert_id=2455720)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=2455721)",
            "Insert(target_node=IN(type=pair), node=('true', 'True'), position=2, insert_id=2455722)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 21,
        "number": 1473,
        "neg_line": [],
        "pos_line": [
            "+if cell_init_args is None:",
            "+cell_init_args = {'state_is_tuple': True}",
            "+"
        ],
        "core_change": "+if cell_init_args is None: +cell_init_args = {'state_is_tuple': True} +",
        "core_API": "__init__"
    },
    {
        "commit_hash": "5ad7a33a04d8829ad3439b5f9390bd136105f986",
        "index": "adaf574d..0d6af13d 100644",
        "commit_message": "Support for bart in allennlp-models (#4169)\n\n* Support for bart in allennlp-models\n\n- Added option to PretrainedTransformerEmbedder to allow usage with\nencoder-decoder models and created unit test.\n- Added ROUGE-N metric. ROUGE-L will follow soon\n- Added indices to tokens abstract method to TokenIndexer. Implemented\nfor PretrainedTransformerIndexer. This is useful for turning decoded\nsequences in seq2seq models into text.\n- added timestep parameter to step function in beamsearch\n- other minor changes\n\n* Implemented ROUGE-L, updated ROUGE-N, new tests\n- Implemented ROUGE-L metric (F1 score)\n- Implemented ROUGE-N recall, precision and F1 as metrics that can be\naccessed separately\n- Now computing overall ROUGE-N/L as average over scores of each\nsequence pair, rather than summing counts across all pairs and then\ncomputing the metric\n- added tests for new padding behavior in get_text_field_mask\n- added test for ROUGE-N/L\n- stylistic improvements\n\n* Polynomial lr scheduling, max tokens batch sampling, other small changes\n\n- Implemented Polynomial learning rate scheduling, which is used in\nBART. The implementeation is based on Fairseqs and tensorflows\nimplementation.\n\n- Implementation an option to specify the number of maximum tokens per\nbatch, rather than specifiying a fixed batch size. This is also used for\nfine-tuning BART. Added a unit test too.\n\n- For indices_to_tokens, removed code that removes the cls/sep tokens\nintroduced by max length. Added a test to reflect this.\n\n* Small stylistic changes\n\n* Added documentation, separated max tokens sampler, fixed circular\nimportant, memory tracking per batch, polynomical lr decay bug fix\n- Added documentation for lazy_groups_of_max_size\n- Some stylistic changes\n- Made MaxTokensBatchSampler a subclass of BucketBatchSampler\n- Annotated beam search with no grad\n- fixed bug in poly decay related to lr of first batch\n- fixed circular import, finally\n- added gpu/cpu memory tracking for tensorboard for batches (previously\nthis was only possible for epochs)\n\n* Fixed linting errors, fixed rouge test\n\n- TODO: fix\n`TestPretrainedTransformerEmbedder.test_encoder_decoder_model` and\nTestPretrainedTransformerIndexer.test_indices_to_tokens. Both issues are\nrelated to the new tokenizers\n\n* Fixed issues with new tokenizers\n- fixed issue with roberta based tokenizers in\npretrained_transformer_indexer\n- temporary fix for incorrect types ids when using max length for\ntokens_to_indices in PretrainedTransformerIndexer\n- fixed indexer test to not compare idx and idx_end of Tokens\n\n* Added max tokens batch sampler to __init__.py\n\n* Fixed max tokens sampler to account for padding\n\n* Fixed large batches due to short source sequences but long target\nsequences in max tokens batch sampler\n\n* Formatting\n\n* Filled in the changelog\n\n* Tests have moved\n\n* Fix docs\n\n* Adds a test for the max tokens sampler\n\n* Adds warning when a single instance is too big\n\n* More docs changes\n\n* Formatting\n\n* Docs\n\n* Fix old models\n\n* Fixed linting and type checking errors\n\n* Fix docs build\n\n* Fix circular imports\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "transition_probabilities = torch.tensor(",
            "",
            "",
            "def take_step(",
            "-    last_predictions: torch.Tensor, state: Dict[str, torch.Tensor]",
            "+    last_predictions: torch.Tensor, state: Dict[str, torch.Tensor], step: int",
            ") -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:",
            "\"\"\"",
            "Take decoding step."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=5, insert_id=17397)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'step'), position=6, insert_id=17398)",
            "Insert(target_node=ASTNode(type=argument_list), node=('ERROR', None), position=7, insert_id=17399)",
            "Insert(target_node=IN(type=ERROR), node=(':', ':'), position=0, insert_id=17400)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'int'), position=1, insert_id=17401)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1474,
        "neg_line": [
            "-last_predictions: torch.Tensor, state: Dict[str, torch.Tensor]"
        ],
        "pos_line": [
            "+last_predictions: torch.Tensor, state: Dict[str, torch.Tensor], step: int"
        ],
        "core_change": "-last_predictions: torch.Tensor, state: Dict[str, torch.Tensor] +last_predictions: torch.Tensor, state: Dict[str, torch.Tensor], step: int",
        "core_API": "tensor"
    },
    {
        "commit_hash": "cce92bdd5a53b7bb45524c70d03d3ba13eab5412",
        "index": "1296333b..f8a7a61d 100644",
        "commit_message": "add new_arange function + FIX BUGS of returning attn values\n\nSummary:\nImplementation of Levenshtein Transformer paper.\nAdd a new helper function \"new_arange\" to create arange tensor easily.\nFix bugs of returning attn values for NAT models\nDelete files which are not necessary or experimental.\n\nReviewed By: kahne\n\nDifferential Revision: D17652009\n\nfbshipit-source-id: 436bbb5d45de2f8067003232de4f2bd51e87719c\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class InsertionTransformerModel(LevenshteinTransformerModel):",
            "cut_off = output_tokens.ne(self.pad).sum(1).max()",
            "output_tokens = output_tokens[:, :cut_off]",
            "output_scores = output_scores[:, :cut_off]",
            "-        return {\"output_tokens\": output_tokens, \"output_scores\": output_scores}",
            "+        return {\"output_tokens\": output_tokens, \"output_scores\": output_scores, \"attn\": None}",
            "",
            "",
            "class InsertionTransformerDecoder(LevenshteinTransformerDecoder):"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=4, insert_id=217908)",
            "Insert(target_node=ASTNode(type=dictionary), node=('pair', None), position=5, insert_id=217909)",
            "Insert(target_node=IN(type=pair), node=('string', '\"attn\"'), position=0, insert_id=217910)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=217911)",
            "Insert(target_node=IN(type=pair), node=('none', 'None'), position=2, insert_id=217912)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1475,
        "neg_line": [
            "-return {\"output_tokens\": output_tokens, \"output_scores\": output_scores}"
        ],
        "pos_line": [
            "+return {\"output_tokens\": output_tokens, \"output_scores\": output_scores, \"attn\": None}"
        ],
        "core_change": "-return {\"output_tokens\": output_tokens, \"output_scores\": output_scores} +return {\"output_tokens\": output_tokens, \"output_scores\": output_scores, \"attn\": None}",
        "core_API": "ne"
    },
    {
        "commit_hash": "41ec490b6d4f69bd2d23fb500c1fd502eb715a76",
        "index": "6dc0a9ab..d78b3c3d 100644",
        "commit_message": "Fixes calibration and adds example scripts (#2431)\n\n* Adds calibration to binary and category output feature schema.\n\n* Adds type annotations for create_calibration_module.\n\n* Fixes initialization of calibration module for category features.\n\n* First pass at forest cover and mushroom edibility.\n\n* Fixed brier plot.\n\n* Adds forest cover visualizations.\n\n* Reduce epochs to 1 and default transformer params.\n\n* Adds calibration as an output feature key which should not be nested inside decoder.\n\n* Moved output_features below input_features.\n\nCo-authored-by: Daniel Treiman <daniel@predibase.com>\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BinaryOutputFeature(BinaryFeatureMixin, OutputFeature):",
            "confidence_penalty=self.loss[\"confidence_penalty\"],",
            ")",
            "",
            "-    def create_calibration_module(self, feature) -> torch.nn.Module:",
            "+    def create_calibration_module(self, feature: BinaryOutputFeatureConfig) -> torch.nn.Module:",
            "\"\"\"Creates the appropriate calibration module based on the feature config.",
            "",
            "Today, only one type of calibration (\"temperature_scaling\") is available, but more options may be supported in",
            "the future.",
            "\"\"\"",
            "-        if feature.get(\"calibration\"):",
            "+        if feature.calibration:",
            "calibration_cls = calibration.get_calibration_cls(BINARY, \"temperature_scaling\")",
            "return calibration_cls(binary=True)",
            "return None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=attribute), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=get), value='calibration')",
            "Insert(target_node=ASTNode(type=parameters), node=('typed_parameter', None), position=3, insert_id=599136)",
            "Move(target_node=IN(type=typed_parameter), node=ASTNode(type=identifier, text=feature), position=0)",
            "Insert(target_node=IN(type=typed_parameter), node=(':', ':'), position=1, insert_id=599137)",
            "Insert(target_node=IN(type=typed_parameter), node=('type', None), position=2, insert_id=599138)",
            "Insert(target_node=IN(type=type), node=('identifier', 'BinaryOutputFeatureConfig'), position=0, insert_id=599139)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"calibration\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 1476,
        "neg_line": [
            "-def create_calibration_module(self, feature) -> torch.nn.Module:",
            "-if feature.get(\"calibration\"):"
        ],
        "pos_line": [
            "+def create_calibration_module(self, feature: BinaryOutputFeatureConfig) -> torch.nn.Module:",
            "+if feature.calibration:"
        ],
        "core_change": "-def create_calibration_module(self, feature) -> torch.nn.Module: +def create_calibration_module(self, feature: BinaryOutputFeatureConfig) -> torch.nn.Module: -if feature.get(\"calibration\"): +if feature.calibration:",
        "core_API": "get"
    },
    {
        "commit_hash": "f5dbcd8128f21a8fca7e2b036f8162319cd162a8",
        "index": "2a8cb25e..f302db56 100644",
        "commit_message": "Fix layer choice on IT and deprecate \"choices\" and \"length\" (#2386)\n\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RandomMutator(Mutator):",
            "result = dict()",
            "for mutable in self.mutables:",
            "if isinstance(mutable, LayerChoice):",
            "-                gen_index = torch.randint(high=mutable.length, size=(1, ))",
            "-                result[mutable.key] = F.one_hot(gen_index, num_classes=mutable.length).view(-1).bool()",
            "+                gen_index = torch.randint(high=len(mutable), size=(1, ))",
            "+                result[mutable.key] = F.one_hot(gen_index, num_classes=len(mutable)).view(-1).bool()",
            "elif isinstance(mutable, InputChoice):",
            "if mutable.n_chosen is None:",
            "result[mutable.key] = torch.randint(high=2, size=(mutable.n_candidates,)).view(-1).bool()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('call', None), position=2, insert_id=666593)",
            "Insert(target_node=IN(type=call), node=('identifier', 'len'), position=0, insert_id=666594)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=666595)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=666596)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=mutable), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=666597)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('call', None), position=2, insert_id=666598)",
            "Insert(target_node=IN(type=call), node=('identifier', 'len'), position=0, insert_id=666599)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=666600)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=666601)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=mutable), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=666602)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=length))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=length))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 1477,
        "neg_line": [
            "-gen_index = torch.randint(high=mutable.length, size=(1, ))",
            "-result[mutable.key] = F.one_hot(gen_index, num_classes=mutable.length).view(-1).bool()"
        ],
        "pos_line": [
            "+gen_index = torch.randint(high=len(mutable), size=(1, ))",
            "+result[mutable.key] = F.one_hot(gen_index, num_classes=len(mutable)).view(-1).bool()"
        ],
        "core_change": "-gen_index = torch.randint(high=mutable.length, size=(1, )) -result[mutable.key] = F.one_hot(gen_index, num_classes=mutable.length).view(-1).bool() +gen_index = torch.randint(high=len(mutable), size=(1, )) +result[mutable.key] = F.one_hot(gen_index, num_classes=len(mutable)).view(-1).bool()",
        "core_API": "randint"
    },
    {
        "commit_hash": "6fe7db7f01123a7290e1aa1fd72c797f285dfe10",
        "index": "4f1eca6..5558341 100644",
        "commit_message": "Fix visualize activations (#1211)\n\nFix visualize activations (Squashed 4 commits by @keineahnung2345)\n\n# Get activations of a few sample layers\nactivations = model.run_graph([image], [\n    (\"input_image\",        model.keras_model.get_layer(\"input_image\").output)\n])\nleads to the error:\nInvalidArgumentError: input_image:0 is both fed and fetched.\n\nRevise the code according to https://stackoverflow.com/questions/39307108/placeholder-20-is-both-fed-and-fetched\n\n",
        "file": "Mask_RCNN.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "\"source\": [",
            "\"# Get activations of a few sample layers\\n\",",
            "\"activations = model.run_graph([image], [\\n\",",
            "-    \"    (\\\"input_image\\\",        model.keras_model.get_layer(\\\"input_image\\\").output),\\n\",",
            "+    \"    (\\\"input_image\\\",        tf.identity(model.keras_model.get_layer(\\\"input_image\\\").output)),\\n\",",
            "\"    (\\\"res2c_out\\\",          model.keras_model.get_layer(\\\"res2c_out\\\").output),\\n\",",
            "\"    (\\\"res3c_out\\\",          model.keras_model.get_layer(\\\"res3c_out\\\").output),\\n\",",
            "\"    (\\\"res4w_out\\\",          model.keras_model.get_layer(\\\"res4w_out\\\").output),  # for resnet100\\n\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"    (\\\"input_image\\\",        model.keras_model.get_layer(\\\"input_image\\\").output),\\n\"), value='\"    (\\\\\"input_image\\\\\",        tf.identity(model.keras_model.get_layer(\\\\\"input_image\\\\\").output)),\\\\n\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1479,
        "neg_line": [
            "-\"    (\\\"input_image\\\",        model.keras_model.get_layer(\\\"input_image\\\").output),\\n\","
        ],
        "pos_line": [
            "+\"    (\\\"input_image\\\",        tf.identity(model.keras_model.get_layer(\\\"input_image\\\").output)),\\n\","
        ],
        "core_change": "-\"    (\\\"input_image\\\",        model.keras_model.get_layer(\\\"input_image\\\").output),\\n\", +\"    (\\\"input_image\\\",        tf.identity(model.keras_model.get_layer(\\\"input_image\\\").output)),\\n\",",
        "core_API": "run_graph"
    },
    {
        "commit_hash": "b51c8a4aa3c574e5b42880db69bf89d2327c80a3",
        "index": "03d0eb1..d291f5c 100644",
        "commit_message": "Revert \"revert padding bug fix for now\" with fix\n\nThis reverts commit ca5819dec327be9e49412ce69909feea72f5d752.\n\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def sample_autoregressive(partial_sequences,",
            "if has_partial_sequences and remove_partial_sequences:",
            "# remove partial sequences from outputs",
            "partial_length = mtf.reduce_sum(",
            "-            mtf.to_int32(mtf.not_equal(partial_sequences, 0)),",
            "+            mtf.to_int32(mtf.not_equal(partial_sequences, padding_id)),",
            "reduced_dim=length_dim)",
            "outputs = mtf.dynamic_shift(",
            "outputs, -partial_length, length_dim, wrap=False)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'padding_id'), position=3, insert_id=1938425)",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1480,
        "neg_line": [
            "-mtf.to_int32(mtf.not_equal(partial_sequences, 0)),"
        ],
        "pos_line": [
            "+mtf.to_int32(mtf.not_equal(partial_sequences, padding_id)),"
        ],
        "core_change": "-mtf.to_int32(mtf.not_equal(partial_sequences, 0)), +mtf.to_int32(mtf.not_equal(partial_sequences, padding_id)),",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "343057e1413924152c1a3716a31775660dedb229",
        "index": "8978b8b2e..baa2fff29 100644",
        "commit_message": "Fix bart conversion script (#9923)\n\n* fix conversion script\n\n* typo\n\n* import nn\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def convert_bart_checkpoint(checkpoint_path, pytorch_dump_folder_path, hf_checkp",
            "model = BartForConditionalGeneration(config).eval()  # an existing summarization ckpt",
            "model.model.load_state_dict(state_dict)",
            "if hasattr(model, \"lm_head\"):",
            "-                model.lm_head = _make_linear_from_emb(model.model.shared)",
            "+                model.lm_head = make_linear_from_emb(model.model.shared)",
            "new_model_outputs = model.model(tokens)[0]",
            "",
            "# Check results"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=_make_linear_from_emb), value='make_linear_from_emb')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1481,
        "neg_line": [
            "-model.lm_head = _make_linear_from_emb(model.model.shared)"
        ],
        "pos_line": [
            "+model.lm_head = make_linear_from_emb(model.model.shared)"
        ],
        "core_change": "-model.lm_head = _make_linear_from_emb(model.model.shared) +model.lm_head = make_linear_from_emb(model.model.shared)",
        "core_API": "load_state_dict"
    },
    {
        "commit_hash": "7425014d2cf06d2d113ef055811316129789de22",
        "index": "09f94c513..3bd71f73c 100644",
        "commit_message": "fixed shape\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Energy(AbsFeatsExtract):",
            "else x.new_tensor(0.0)",
            "for start, end in zip(d_cumsum[:-1], d_cumsum[1:])",
            "]",
            "-        return torch.stack(x_avg).unsqueeze(-1)",
            "+        return torch.stack(x_avg)",
            "",
            "@staticmethod",
            "def _adjust_num_frames(x: torch.Tensor, num_frames: torch.Tensor) -> torch.Tensor:"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=unsqueeze))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 1484,
        "neg_line": [
            "-return torch.stack(x_avg).unsqueeze(-1)"
        ],
        "pos_line": [
            "+return torch.stack(x_avg)"
        ],
        "core_change": "-return torch.stack(x_avg).unsqueeze(-1) +return torch.stack(x_avg)",
        "core_API": "new_tensor"
    },
    {
        "commit_hash": "af8425b749439eec00a886e635827a65b302a54d",
        "index": "e964fab7c..e0f35aa88 100644",
        "commit_message": "Refactoring the TF activations functions (#7150)\n\n* Refactoring the activations functions into a common file\n\n* Apply style\n\n* remove unused import\n\n* fix tests\n\n* Fix tests.\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class TFRobertaLMHead(tf.keras.layers.Layer):",
            "config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"",
            ")",
            "self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")",
            "-        self.act = tf.keras.layers.Activation(gelu)",
            "+        self.act = get_tf_activation(\"gelu\")",
            "",
            "# The output weights are the same as the input embeddings, but there is",
            "# an output-only bias for each token."
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='get_tf_activation')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=tf), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=('string', '\"gelu\"'), position=1, insert_id=2644569)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=keras))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=layers))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Activation))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=gelu))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 1485,
        "neg_line": [
            "-self.act = tf.keras.layers.Activation(gelu)"
        ],
        "pos_line": [
            "+self.act = get_tf_activation(\"gelu\")"
        ],
        "core_change": "-self.act = tf.keras.layers.Activation(gelu) +self.act = get_tf_activation(\"gelu\")",
        "core_API": "LayerNormalization"
    },
    {
        "commit_hash": "be48e1964b7d685f77baa5a45aca29d3cfcb9883",
        "index": "8d803f03e3..d0195b6f86 100644",
        "commit_message": "[rllib] Fix per-worker exploration in Ape-X; make more kwargs required for future safety (#7504)\n\n* fix sched\n\n* lintc\n\n* lint\n\n* fix\n\n* add unit test\n\n* fix\n\n* format\n\n* fix test\n\n* fix test\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class EpsilonGreedy(Exploration):",
            "",
            "chose_random = tf.random_uniform(",
            "tf.stack([batch_size]),",
            "-            minval=0, maxval=1, dtype=epsilon.dtype) \\",
            "+            minval=0, maxval=1, dtype=tf.float32) \\",
            "< epsilon",
            "",
            "action = tf.cond("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=epsilon), value='tf')",
            "Update(target_node=ASTNode(type=identifier, text=dtype), value='float32')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1487,
        "neg_line": [
            "-minval=0, maxval=1, dtype=epsilon.dtype) \\"
        ],
        "pos_line": [
            "+minval=0, maxval=1, dtype=tf.float32) \\"
        ],
        "core_change": "-minval=0, maxval=1, dtype=epsilon.dtype) \\ +minval=0, maxval=1, dtype=tf.float32) \\",
        "core_API": "random_uniform"
    },
    {
        "commit_hash": "701d4d3c4681810a48a122cff3d11f8b2dd7af78",
        "index": "31a089d7..3e3a0077 100644",
        "commit_message": "Issue #642 - Add `AtrousDeConv2dLayer` (#662)\n\n* Update visualize.py\n\n* Update README.md\n\nAdd an example for Adversarial Learning\n\n* Update more.rst\n\nUpdate the URLs\n\n* Update more.rst\n\n* Update example.rst\n\nAdd the same link of BEGAN implementation.\n\n* Update example.rst\n\n* Update example.rst\n\n* Create tutorial_tfslim.py\n\nfixes #552\n\n* Update tutorial_tfslim.py\n\n* Update utils.py\n\nFix #565\n\n* Update utils.py\n\n* Create test_utils_predict.py\n\nrelated with #288, #565, #566\n\n* Create test_utils_predict.py\n\n* Update utils.py\n\n* Update test_utils_predict.py\n\n* Update CHANGELOG.md\n\nrelated to #566\n\n* Update test_utils_predict.py\n\n* Update CHANGELOG.md\n\n* Update CHANGELOG.md\n\n* Update CHANGELOG.md\n\n* Update test_utils_predict.py\n\n* Update test_utils_predict.py\n\n* Update test_utils_predict.py\n\n* Update test_utils_predict.py\n\n* Update test_utils_predict.py\n\n* Update test_utils_predict.py (fix Bad Coding Style)\n\n* Update test_utils_predict.py\n\n* Update CHANGELOG.md\n\n* Update CHANGELOG.md\n\n* Update CHANGELOG.md\n\n* Update CHANGELOG.md\n\n* Update convolution.py (Add AtrousConv2dTransLayer)\n\n* Add AtrousConv2dTransLayer\n\n* Fix some mistakes\n\n* Follow protocols\n\n* Fix coding style (yapf)\n\n* AtrousConv2dLayer fixed\n\n* AtrousConv2dTransposeLayer refactored\n\n* Fix coding style (yapf)\n\n* Fix error\n\n* Bias Add using premade tf func\n\n* Old TF Code Removed\n\n* Renamed to AtrousDeConv2dLayer\n\n* Update CHANGELOG.md\n\n* Release 1.8.6rc2\n\n* Documentation Fix\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class Layer_Convolution_3D_Test(unittest.TestCase):",
            "",
            "cls.input_layer = tl.layers.InputLayer(x, name='input_layer')",
            "",
            "-        print(\"input:\", cls.input_layer.all_layers)",
            "-",
            "cls.n1 = tl.layers.Conv3dLayer(cls.input_layer, shape=(2, 2, 2, 3, 32), strides=(1, 2, 2, 2, 1))",
            "",
            "cls.n2 = tl.layers.DeConv3dLayer("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=print))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"input:\"))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=cls))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=input_layer))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=all_layers))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 1489,
        "neg_line": [
            "-print(\"input:\", cls.input_layer.all_layers)",
            "-"
        ],
        "pos_line": [],
        "core_change": "-print(\"input:\", cls.input_layer.all_layers) -",
        "core_API": "InputLayer"
    },
    {
        "commit_hash": "39c71ee8a98bcbfea242e6b203556150ee64205b",
        "index": "a646eaa6..4d4dbba1 100644",
        "commit_message": "wavegrad refactoring, fixing tests for glow-tts and wavegrad\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class AngleProtoLossTests(unittest.TestCase):",
            "",
            "# check speaker loss with orthogonal d-vectors",
            "dummy_input = T.empty(3, 64)",
            "-        dummy_input = T.nn.init.orthogonal(dummy_input)",
            "+        dummy_input = T.nn.init.orthogonal_(dummy_input)",
            "dummy_input = T.cat(",
            "[",
            "dummy_input[0].repeat(5, 1, 1).transpose(0, 1),"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=orthogonal), value='orthogonal_')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1490,
        "neg_line": [
            "-dummy_input = T.nn.init.orthogonal(dummy_input)"
        ],
        "pos_line": [
            "+dummy_input = T.nn.init.orthogonal_(dummy_input)"
        ],
        "core_change": "-dummy_input = T.nn.init.orthogonal(dummy_input) +dummy_input = T.nn.init.orthogonal_(dummy_input)",
        "core_API": "empty"
    },
    {
        "commit_hash": "f223bf1e246bca5c063d77194b2e55e8fb6af0ed",
        "index": "a0867e45..829d5cf3 100644",
        "commit_message": "fixed categorical bug\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Categorical(Distribution):",
            "_ps, _vs, _one_hot = self._sanitize_input(ps, vs, one_hot)",
            "_vs = self._process_v(_vs)",
            "_ps, _vs = self._process_p(_ps, _vs)",
            "-        sample = Variable(torch.multinomial(_ps.data, 1, replacement=True))",
            "+        sample = Variable(torch.multinomial(_ps.data, 1, replacement=True).type_as(_ps.data))",
            "if _vs is not None:",
            "if isinstance(_vs, np.ndarray):",
            "# always returns a 2-d (unsqueezed 1-d) list"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=768660)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=768661)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=768662)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type_as'), position=2, insert_id=768663)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=768664)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=768665)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=768666)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_ps'), position=0, insert_id=768667)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=768668)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'data'), position=2, insert_id=768669)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 1491,
        "neg_line": [
            "-sample = Variable(torch.multinomial(_ps.data, 1, replacement=True))"
        ],
        "pos_line": [
            "+sample = Variable(torch.multinomial(_ps.data, 1, replacement=True).type_as(_ps.data))"
        ],
        "core_change": "-sample = Variable(torch.multinomial(_ps.data, 1, replacement=True)) +sample = Variable(torch.multinomial(_ps.data, 1, replacement=True).type_as(_ps.data))",
        "core_API": "_sanitize_input"
    },
    {
        "commit_hash": "09fc038e202c5411c4292d065f4ad04369bd87fc",
        "index": "2fac9c42b..5b393fc62 100644",
        "commit_message": "fix linter and add docs\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Encoder(torch.nn.Module):",
            "self.norm = LayerNorm(args.adim)",
            "",
            "def forward(self, x, mask):",
            "+        \"\"\"Embed positions in tensor",
            "+",
            "+        :param torch.Tensor x: input tensor",
            "+        :param torch.Tensor mask: input mask",
            "+        :return: position embedded tensor and mask",
            "+        :rtype Tuple[torch.Tensor, torch.Tensor]:",
            "+        \"\"\"",
            "if isinstance(self.input_layer, Conv2dSubsampling):",
            "x, mask = self.input_layer(x, mask)",
            "else:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=175042)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=175043)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"Embed positions in tensor\\n\\n        :param torch.Tensor x: input tensor\\n        :param torch.Tensor mask: input mask\\n        :return: position embedded tensor and mask\\n        :rtype Tuple[torch.Tensor, torch.Tensor]:\\n        \"\"\"'), position=0, insert_id=175044)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 6,
        "minus_line": 0,
        "AST_diff_line": 4,
        "number": 1494,
        "neg_line": [],
        "pos_line": [
            "+\"\"\"Embed positions in tensor",
            "+",
            "+:param torch.Tensor x: input tensor",
            "+:param torch.Tensor mask: input mask",
            "+:return: position embedded tensor and mask",
            "+:rtype Tuple[torch.Tensor, torch.Tensor]:",
            "+\"\"\""
        ],
        "core_change": "+\"\"\"Embed positions in tensor + +:param torch.Tensor x: input tensor +:param torch.Tensor mask: input mask +:return: position embedded tensor and mask +:rtype Tuple[torch.Tensor, torch.Tensor]: +\"\"\"",
        "core_API": "input_layer"
    },
    {
        "commit_hash": "3b91f96fc94e9e201a2c637b2f654cbdc6a21ee1",
        "index": "9a1ca6c9d..c1e91651d 100755",
        "commit_message": "Fix torch meshgrid warnings (#20475)\n\n* fix torch meshgrid warnings\n\n* support lower torch versions\n\n* don't edit examples\n\n* dont edit examples\n\n* fix ci\n\n* fix style\n\n* rebase cleanup\n\n* fix ci again\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BeitRelativePositionBias(nn.Module):",
            "# get pair-wise relative position index for each token inside the window",
            "coords_h = torch.arange(window_size[0])",
            "coords_w = torch.arange(window_size[1])",
            "-        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww",
            "+        coords = torch.stack(meshgrid([coords_h, coords_w], indexing=\"ij\"))  # 2, Wh, Ww",
            "coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww",
            "relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww",
            "relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=meshgrid), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1182429)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1182430)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'indexing'), position=0, insert_id=1182431)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1182432)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"ij\"'), position=2, insert_id=1182433)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 1495,
        "neg_line": [
            "-coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww"
        ],
        "pos_line": [
            "+coords = torch.stack(meshgrid([coords_h, coords_w], indexing=\"ij\"))  # 2, Wh, Ww"
        ],
        "core_change": "-coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww +coords = torch.stack(meshgrid([coords_h, coords_w], indexing=\"ij\"))  # 2, Wh, Ww",
        "core_API": "arange"
    },
    {
        "commit_hash": "ae89808555c8c2094561058eb4cbd50da72041a7",
        "index": "127f7a200..2a39c7e0a 100644",
        "commit_message": "fix wrong comments and typos\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def target_mask(ys_in_pad, ignore_id):",
            ":param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax)",
            ":param int ignore_id: index of padding",
            ":param torch.dtype dtype: result dtype",
            "-    :rtype: torch.Tensor",
            "+    :rtype: torch.Tensor (B, Lmax, Lmax)",
            "\"\"\"",
            "ys_mask = ys_in_pad != ignore_id",
            "m = subsequent_mask(ys_mask.size(-1), device=ys_mask.device).unsqueeze(0)",
            "+",
            "+    #ys_mask.unsqueeze(-2).shape: (1, Lmax, Lmax)",
            "+    #m.shape: (B, 1, Lmax)",
            "return ys_mask.unsqueeze(-2) & m"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=type), node=('call', None), position=0, insert_id=149823)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=149824)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=149825)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'B'), position=1, insert_id=149826)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=149827)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'Lmax'), position=3, insert_id=149828)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=149829)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'Lmax'), position=5, insert_id=149830)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=6, insert_id=149831)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 1498,
        "neg_line": [
            "-:rtype: torch.Tensor"
        ],
        "pos_line": [
            "+:rtype: torch.Tensor (B, Lmax, Lmax)",
            "+",
            "+#ys_mask.unsqueeze(-2).shape: (1, Lmax, Lmax)",
            "+#m.shape: (B, 1, Lmax)"
        ],
        "core_change": "-:rtype: torch.Tensor +:rtype: torch.Tensor (B, Lmax, Lmax) + +#ys_mask.unsqueeze(-2).shape: (1, Lmax, Lmax) +#m.shape: (B, 1, Lmax)",
        "core_API": "size"
    },
    {
        "commit_hash": "c7c7b008d92a7d22cf7d994c1ca0bb39fa696826",
        "index": "1badd3a89..a82b215af 100644",
        "commit_message": "Squashed commit of the following:\n\ncommit 047d0c474c18a87c205e566948410be16787e477\nMerge: 9396ed37d bfe7bca3a\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu May 19 09:50:02 2022 -0400\n\n    Merge pull request #4378 from akreal/fix-check_short_utt\n\n    Fix minimum input length for Conv2dSubsampling2 in check_short_utt\n\ncommit bfe7bca3a98da52714e1c45906cf826704464b7c\nAuthor: Pavel Denisov <pavel.denisov@ims.uni-stuttgart.de>\nDate:   Thu May 19 13:41:59 2022 +0200\n\n    Fix minimum input length for Conv2dSubsampling2 in check_short_utt\n\ncommit 9396ed37deb8b101fd064d46c85975ad9047bf87\nMerge: c54b585c1 e047156ec\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Thu May 19 14:50:56 2022 +0900\n\n    Merge pull request #4376 from kamo-naoyuki/libsndfile\n\n    Remove the restriction for libsndfile version\n\ncommit c54b585c1ca6693ae7ba7e299a48af762eda6adf\nMerge: 9ca49caed 88465607c\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Thu May 19 12:29:02 2022 +0900\n\n    Merge pull request #4374 from YosukeHiguchi/master\n\n    Minor fixes for the intermediate loss usage and Mask-CTC decoding\n\ncommit e047156ec8df3266259aed03742ac798e365f648\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 19 10:11:08 2022 +0900\n\n    remove version restiction for libsndfile\n\ncommit 9ca49caed98410cd7d2c71e4781819a1e92b35d9\nMerge: b008ac7d5 2952c3bca\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Thu May 19 09:38:33 2022 +0900\n\n    Merge pull request #4375 from espnet/kamo-naoyuki-patch-1\n\n    Update .mergify.yml\n\ncommit 88465607cf5e899b8ce1b93c5c9fe09b69a2ab83\nAuthor: Yosuke Higuchi <wasapon.dev@gmail.com>\nDate:   Thu May 19 07:05:29 2022 +0900\n\n    fix for test\n\ncommit 2952c3bca26a70723094d5a160387b7936f71769\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Thu May 19 06:59:02 2022 +0900\n\n    Update .mergify.yml\n\ncommit b008ac7d58e9ced1a9f8c89cc85ee69d9e9461ab\nMerge: 3c96908ed 4203c9c9c\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Thu May 19 06:32:44 2022 +0900\n\n    Merge pull request #4372 from kamo-naoyuki/isort\n\n    Add isort checking to the CI tests\n\ncommit 4de7aa562f74c596e5b616fd8278a50a707d0198\nAuthor: Yosuke Higuchi <wasapon.dev@gmail.com>\nDate:   Thu May 19 06:19:20 2022 +0900\n\n    fix for test\n\ncommit 9c83ddb46404334914764a8e4356ea8a4c3c806c\nAuthor: Yosuke Higuchi <wasapon.dev@gmail.com>\nDate:   Thu May 19 05:05:01 2022 +0900\n\n    support gpu decoding for mask-ctc\n\ncommit 49100e4f1b3fc389c5672dc2ca17973525c4bf02\nAuthor: Yosuke Higuchi <wasapon.dev@gmail.com>\nDate:   Thu May 19 05:03:29 2022 +0900\n\n    fix bug for returning intermediate states\n\ncommit 4203c9c9c9d5a68cd13d464290cead3738ed003d\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Wed May 18 17:47:22 2022 +0900\n\n    apply isort\n\ncommit d0f2eac70a5521adf59618ba3ce6603e2863f0c5\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Wed May 18 17:46:47 2022 +0900\n\n    modified for isort options\n\ncommit 8f73b73d23d34bf5f3e8ed2f625dca1916ea8683\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Wed May 18 16:38:34 2022 +0900\n\n    apply black\n\ncommit 6974dd4efc11e465d4a3d1a34190c7ed782dacee\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Wed May 18 16:35:15 2022 +0900\n\n    Add descriptions for isort\n\ncommit 24c3676a8d4c2e60d2726e9bcd9bdbed740610e0\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Wed May 18 16:16:53 2022 +0900\n\n    Apply isort\n\ncommit 3c96908edc5c592c9c99bba0640428613dc7c3cb\nMerge: c173c3093 aa5d6ffff\nAuthor: Jiatong <728307998@qq.com>\nDate:   Tue May 17 18:00:40 2022 -0700\n\n    Merge pull request #4341 from chintu619/st_bugfix\n\n    bug fixes in ST recipes\n\ncommit c173c30930631731e6836c274a591ad571749741\nMerge: e0e0620ac d38188cc3\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Tue May 17 15:20:31 2022 +0900\n\n    Merge pull request #4371 from espnet/kamo-naoyuki-patch-1\n\n    Update .mergify.yml\n\ncommit d38188cc30af6cffc4ad0233e7e705e93511c11d\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Tue May 17 13:43:40 2022 +0900\n\n    Update .mergify.yml\n\ncommit e0e0620acca0df345cf317a13c839d7d4d5c773f\nMerge: df053b8c1 2cfbbd337\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 13:01:02 2022 +0900\n\n    Merge pull request #4369 from kan-bayashi/minor_fix_jets\n\ncommit 2cfbbd337d64f68e1f937e37feeb544d972c4e0b\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 11:06:00 2022 +0900\n\n    updated jets test\n\ncommit 17ab7747fe7e0d4d6885847f2c738253a859dedf\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 11:05:52 2022 +0900\n\n    updated README\n\ncommit 6ec8c27815c6fded4c13b01b8d2707016e9e8e95\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 09:25:41 2022 +0900\n\n    updated README\n\ncommit b1e6c752b0d94f3209593e0cdbd5b43d79e8076d\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 09:19:54 2022 +0900\n\n    shorten jets test\n\ncommit df053b8c13c26fe289fc882751801fd781e9d43e\nMerge: afa8f8ec5 5aa543a9f\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Tue May 17 08:13:36 2022 +0900\n\n    Merge pull request #4364 from imdanboy/master\n\n    add e2e tts model: JETS\n\ncommit 5aa543a9ff6c329f5fc601f3aa053ffd4afb19ba\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Mon May 16 21:13:30 2022 +0900\n\n    minor fix of docstrings and comments\n\ncommit a82e78d18aca9c00bcf8f378c42e78a0de24940e\nAuthor: imdanboy <imdanboy@gmail.com>\nDate:   Fri May 13 22:28:31 2022 +0900\n\n    JETS; e2e tts model\n\ncommit afa8f8ec5b8ec77deb1a3c1531915ebbee7b80e6\nMerge: fffb3444f cd77501a8\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Fri May 13 17:36:30 2022 -0400\n\n    Merge pull request #4349 from pyf98/quantization\n\n    Add quantization in ESPnet2 for asr inference\n\ncommit fffb3444fe4d8ef2630a22dd145d6f1fb0caab46\nMerge: f840b8114 5331890e6\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 20:36:39 2022 +0900\n\n    Merge pull request #4361 from espnet/kamo-naoyuki-patch-1\n\n    Update README.md\n\ncommit aa5d6ffff67079f2cbe6a7e1eba852e459f0f6a4\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 13 05:15:32 2022 -0400\n\n    fix lm tag names\n\ncommit 3cac7bb7f732a694f4b87007271d394a9ee3838e\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 13 05:07:55 2022 -0400\n\n    resolve conflicts and fix lm_train filenames\n\ncommit ea44663e8a24ebfcaa03f3bba149e561e970fdf3\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 13 04:43:18 2022 -0400\n\n    review suggested changes\n\ncommit 650c733437da32627f88fe369555ce1955536087\nMerge: 6d1bd3a8e f840b8114\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 13 03:18:08 2022 -0400\n\n    Merge branch 'espnet_master' into st_bugfix\n\ncommit 5331890e6a6a61a3006e5e2c13d47172f5587a29\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 13:15:40 2022 +0900\n\n    Update README.md\n\ncommit f840b8114452b4803b8fb25c1f22a93da146e9ba\nMerge: 1b1241040 9cfd6af64\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 13:13:34 2022 +0900\n\n    Merge pull request #4348 from kamo-naoyuki/1.11.0\n\n    Add pytorch=1.10.2 and 1.11.0 to ci configurations\n\ncommit 9cfd6af64a28237019196cd495fbd2943790ce21\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 09:58:04 2022 +0900\n\n    fix\n\ncommit 2625be71a722e7eb030dff4f71d8dc9599a33844\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 03:46:24 2022 +0900\n\n    remove warning\n\ncommit 9a2001fac56dddf5ba1c2eaec092cb420f83f7c9\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Fri May 13 03:44:11 2022 +0900\n\n    fix for pytorch1.11 (+= became inplace op)\n\ncommit 5518b6ba0af0bba9e9d59d6c47607656f49c9988\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 22:04:42 2022 +0900\n\n    fix import order\n\ncommit 98689a5f0bfd88efffdbbcdd5d924e186d563a91\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 21:17:35 2022 +0900\n\n    change to show the error logs when jobs are failed\n\ncommit bb0d0aaa9e9f9076ac88aad425ad2f2caef369a7\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 20:40:39 2022 +0900\n\n    fix code style\n\ncommit 934b161f1f714637c3d7d47c14f8c810a9df6fe2\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 20:33:58 2022 +0900\n\n    change to show the error logs when jobs are failed\n\ncommit 5c474b96c543c3d26e95b432355bcfd2bf8dc116\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 20:20:18 2022 +0900\n\n    remove verbosity options\n\ncommit 005aad11b37acf388c6b70143ab40a5231bc7a39\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 20:04:57 2022 +0900\n\n    fix\n\ncommit 5c4b966a957062e4de298bcb69fe8cf6f1365fd1\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 19:36:11 2022 +0900\n\n    remove tests for python=3.10.0 temporary\n\ncommit 809ac3741814b7d9ebdd351b9e0e9343e236977c\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 19:27:20 2022 +0900\n\n    fix\n\ncommit 86186b744fb2bfc259909c49cc906fb0856d15bf\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 19:10:18 2022 +0900\n\n    add installation for packaging\n\ncommit 8fbac77268906075043cbecfb3e1c5625b145fce\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:59:17 2022 +0900\n\n    fix\n\ncommit b0050d97da3d0545b62a5d21b029ddd016ce6ca1\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:56:52 2022 +0900\n\n    fix\n\ncommit 6e9035d42eea31cad87a7c8b87fc79635a6df7c2\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:32:33 2022 +0900\n\n    fix\n\ncommit 1c344a95ceb83b4b44675aee5326afeb9284d8e8\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:25:35 2022 +0900\n\n    change LooseVersion to parse\n\ncommit f899a05768436cc38fb432d6f002ab667983abbd\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:09:33 2022 +0900\n\n    fix\n\ncommit 7d5242212403e740c4d5b8ebd9a346a991ea50a9\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:09:15 2022 +0900\n\n    fix\n\ncommit b7cfdd9a70559271e45de103e242228f94e837ff\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 18:05:41 2022 +0900\n\n    Change LooseVersion to parse\n\ncommit d234b9ab30bbc2bb6fd42d6335421a6f8a9ed637\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Thu May 12 17:10:40 2022 +0900\n\n    fix\n\ncommit 1b1241040e1e30e575a182b6be8b8e4602badeb8\nMerge: 39bae01e4 52c238d02\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Wed May 11 13:00:13 2022 -0400\n\n    Merge pull request #4352 from espnetUser/master\n\n    Add unit test to streaming ASR inference\n\ncommit 52c238d02d50fcfb2c4e2a5058c743c7db913eec\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Wed May 11 16:10:04 2022 +0200\n\n    Applied black formating to test_asr_inference.py for PR\n\ncommit 87c7573874aeec096dd1e902478d3dd6e2c83ad2\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Wed May 11 15:43:01 2022 +0200\n\n    Update asr_inference_streaming.py\n\n    Fix CI error on mismatch in Tensor dtypes\n\ncommit 39bae01e4a132da69b9b0d025da8c579a5f38b77\nMerge: dd24d7d41 71f3c8813\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 17:53:04 2022 +0900\n\n    Merge pull request #4355 from kan-bayashi/fix_lid_in_gan_tts\n\ncommit dd24d7d41517202b308afb186f466c8006ae4c14\nMerge: 2dde7734b f7b390582\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 17:52:09 2022 +0900\n\n    Merge pull request #4206 from WeiGodHorse/master\n\ncommit 2dde7734bade874d4f8cfe7df4be069e64259fd5\nMerge: beb336027 ec7e2b07b\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 16:27:55 2022 +0900\n\n    Merge pull request #4356 from kan-bayashi/fix_mixed_precision_vits\n\n    fix loss = NaN in VITS with mixed precision\n\ncommit 7a590ccd0da4897ef283486776f134eabe865ce0\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Wed May 11 09:25:03 2022 +0200\n\n    Applied black formating to test_asr_inference.py for PR\n\ncommit ec7e2b07bfa85c8a2292de7a2edbf1c2cd956d99\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 14:48:36 2022 +0900\n\n    fixed black\n\ncommit 2be9ddc5a2c0a7c4aad2b155fa1450222ca0c7a3\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 14:28:05 2022 +0900\n\n    fixed mixed_precision NaN (#4236)\n\ncommit 71f3c88133c7a29db54baa7eaa3b4fdf329cbdf5\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Wed May 11 13:39:59 2022 +0900\n\n    fixed optional data names for TTS\n\ncommit ee57ff94dfa2c3ced30c1b103076b4ae18fa9199\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Tue May 10 22:37:18 2022 +0200\n\n    Update asr_inference_streaming.py\n\n    Fix dtype CI error\n\ncommit 272d5d015f89f1520c82c31bd309fdce89d88f50\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Tue May 10 21:52:21 2022 +0200\n\n    Update test_asr_inference.py\n\n    Remove streaming=true parameter\n\ncommit c96e0d7f79e6e94e568b22156eb61004d5d8cf8c\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Tue May 10 21:25:57 2022 +0200\n\n    Aplied black formating to test_asr_inference.py for PR\n\ncommit cd77501a8f09b5b11bf5422b0e24b8316820af77\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Tue May 10 12:02:07 2022 -0400\n\n    fix error for rnn encoders flatten_parameters\n\ncommit 3aafdb9d92c8c61d62be72f0907da957d177aa8c\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Tue May 10 17:05:48 2022 +0200\n\n    Update asr_inference_streaming.py\n\n    Bugfix in streaming inference #4216\n\ncommit 61b50138b7e8828506a18067cc2f482e745e83d7\nAuthor: espnetUser <81252087+espnetUser@users.noreply.github.com>\nDate:   Tue May 10 16:58:14 2022 +0200\n\n    Update test_asr_inference.py\n\n    Added edge test case for streaming asr unit test and increased execution time out\n\ncommit 052dd603900362048675f65058b7a6f4bd94bc7d\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Mon May 9 23:27:41 2022 -0400\n\n    fix ci\n\ncommit 06e2a7a16a06cda326035d03c84734d18c852cd3\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Mon May 9 23:10:14 2022 -0400\n\n    apply black\n\ncommit a48423fda5ab75d1205396ca5f744dc8ca98df00\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Mon May 9 22:59:57 2022 -0400\n\n    add test for espnet2 quantization\n\ncommit acb24c886f47fec7a00063cb66423e7bd52ea0bc\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Mon May 9 22:59:39 2022 -0400\n\n    add quantization to asr_inference\n\ncommit b98fc861939310b73b50f959bc45176da10ef493\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Tue May 10 11:52:27 2022 +0900\n\n    fix\n\ncommit 3428f032d58c73902b5e6fe80307eb08cfc64ff6\nMerge: 4ff2ce124 beb336027\nAuthor: Naoyuki Kamo <naoyuki.kamo829@gmail.com>\nDate:   Tue May 10 11:42:23 2022 +0900\n\n    Merge branch 'master' into 1.11.0\n\ncommit 4ff2ce1244e0af72439deaa59226eba434a70618\nAuthor: kamo-naoyuki <naoyuki.kamo829@gmail.com>\nDate:   Tue May 10 11:34:31 2022 +0900\n\n    add pytorch=1.10.1, 1.11.0 to ci configurations\n\ncommit beb3360276aa9ff65fe84f4c5e99c0c063c2a6be\nMerge: 537f9b6c1 79cda74ba\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Mon May 9 16:27:37 2022 -0400\n\n    Merge pull request #4347 from YosukeHiguchi/espnet2_maskctc2\n\n    Minor fix for Mask-CTC forward function\n\ncommit 79cda74ba20f0b795251e23a9cb9fd624e2be02d\nAuthor: Yosuke Higuchi <wasapon.dev@gmail.com>\nDate:   Mon May 9 22:43:29 2022 +0900\n\n    add kwargs in forward argument\n\ncommit 537f9b6c14ab195cdcd21c404656c8534295f15d\nMerge: 793b999a5 9e8e75315\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Sun May 8 17:34:55 2022 -0400\n\n    Merge pull request #4343 from Emrys365/complex_support\n\n    Fix a bug in stats aggregation when PITSolver is used\n\ncommit 9e8e753154f5f71c9cb26217483427adb278759c\nAuthor: Wangyou Zhang <C0me_On@163.com>\nDate:   Sat May 7 13:16:35 2022 +0800\n\n    Apply black\n\ncommit 5ea4e087a311ab7c798950e68ae92e10b1bb41d8\nAuthor: Wangyou Zhang <C0me_On@163.com>\nDate:   Sat May 7 12:05:49 2022 +0800\n\n    Fix a bug in stats aggregation when PITSolver is used\n\ncommit 6d1bd3a8ef695a75358d019cc1b33100817c0dad\nMerge: eb6dc2d55 793b999a5\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 6 10:51:14 2022 -0400\n\n    Merge branch 'espnet:master' into st_bugfix\n\ncommit eb6dc2d55faac7e62742d0b7791d8f3a991e91d1\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 6 10:08:19 2022 -0400\n\n    typo fix\n\ncommit 8c56ee817867358f2a8130372fd914c136bd7a5b\nAuthor: Chaitanya Narisetty <cnariset@andrew.cmu.edu>\nDate:   Fri May 6 08:59:26 2022 -0400\n\n    bug fixes in ST recipes\n\n    * Change sampling frequency in `fbank.conf` and `pitch.conf` in Covost2 recipe\n    * In `run.sh`, if language is low resource, then have more speed perturbations. Fix typos for test sets\n    * In `st.sh`\n      * fix directory naming issues to avoid replacement for different language pairs\n      * Replace `>>` with `>` to replace previous inference results\n      * Fix removing of empty text in stage 4\n      * When removing utterance-ID in `ref.trn.org` or `hyp.trn.org`, the current implementation removes all words in parenthesis instead of removing just the utterance-ID from the end of each line. Fixed this by changing `perl -pe 's/\\([^\\)]+\\)//g;'` to `perl -pe 's/\\([^\\)]+\\)$//g;'`\n\ncommit f7b390582d2d77b113a92a5e52f907d5832d6f04\nAuthor:  <weixianhao@bytedance.com>\nDate:   Fri May 6 20:18:05 2022 +0800\n\n    change a test file to conform new pypinyin package\n\ncommit b83128fafc913e775a49d37a5cad24a893718020\nAuthor:  <weixianhao@bytedance.com>\nDate:   Fri May 6 17:54:20 2022 +0800\n\n    Fix missing punctuation\n\ncommit 931fd226babe69b35c6e3a6a288e5e0c901736a1\nAuthor:  <weixianhao@bytedance.com>\nDate:   Fri May 6 16:54:31 2022 +0800\n\n    reformat\n\ncommit 793b999a50af484a5eaf6227ef7556b48514ef15\nMerge: 4f41a1a06 6d0672882\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu May 5 21:54:27 2022 -0400\n\n    Merge pull request #4330 from pyf98/show_translation_result\n\n    Update show_translation_result.sh to show all decoding results under the given exp directory\n\ncommit 4f41a1a06ecd96af567bc73d1d6734531dd3cb44\nMerge: a49cc60cd f0d7cc2bf\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu May 5 21:53:10 2022 -0400\n\n    Merge pull request #4329 from roshansh-cmu/wandb\n\n    Wandb Minor Fix for Model Resume\n\ncommit a49cc60cda690e448d925c3e2bfdc5a85b3f5cd3\nMerge: de624ed58 21fba33c6\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu May 5 21:51:43 2022 -0400\n\n    Merge pull request #4338 from espnet/ftshijt-patch-1\n\n    Fix typo\n\ncommit 21fba33c69d9199c6897ffc6da8433ab94b7051d\nAuthor: Jiatong <728307998@qq.com>\nDate:   Thu May 5 21:25:10 2022 -0400\n\n    Fix typo\n\ncommit de624ed58953d17907fb241c5cb6514f27510162\nMerge: b757b89d4 fe288000d\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu May 5 16:10:44 2022 -0400\n\n    Merge pull request #4332 from simpleoier/chime6\n\n    add chime6 recipe\n\ncommit c504336661fa3cefa60b2214da39fbf0118fce49\nMerge: 50269e8b4 b757b89d4\nAuthor:  <weixianhao@bytedance.com>\nDate:   Wed May 4 21:58:43 2022 +0800\n\n    Merge remote-tracking branch 'upstream/master'\n\ncommit fe288000dbde339b4c386408af488af4bac423b6\nAuthor: simpleoier <netnetchangxk@gmail.com>\nDate:   Tue May 3 17:51:36 2022 -0400\n\n    add egs2/chime6/asr1 recipe\n\ncommit 6d06728820576ed96a729b3477a29ccab12542f1\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Sat Apr 30 20:53:52 2022 -0400\n\n    fix ci\n\ncommit 72333a892d16ef913633111120f159008812795e\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Sat Apr 30 20:34:06 2022 -0400\n\n    fix ci\n\ncommit f15e6adaafaca380ea152cf2b38d604eea3603d3\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Sat Apr 30 18:54:37 2022 -0400\n\n    quote expansion\n\ncommit f6731cd97565bf4108f1064a83f1fffea4ca351b\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Sat Apr 30 18:43:49 2022 -0400\n\n    update mt.sh\n\ncommit 552060a1d5670d0fd838bd8e10fc9e47a1122346\nAuthor: Yifan Peng <pengyf21@gmail.com>\nDate:   Sat Apr 30 18:41:41 2022 -0400\n\n    update show translation result\n\ncommit f0d7cc2bfbc8f68c42820262a8ca6e4906f3818b\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Fri Apr 29 20:57:18 2022 -0400\n\n    Delete resnet.py\n\ncommit 79c071e9ecd268a1963e8ca3863a2f5eaf34a525\nAuthor: roshansh-cmu <roshansh@andrew.cmu.edu>\nDate:   Fri Apr 29 20:54:37 2022 -0400\n\n    Wandb minor fix for model resume\n\ncommit ffe7c58ac8a255769f6952b8c7225a5158a00068\nMerge: 835033c70 b757b89d4\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Fri Apr 29 20:45:47 2022 -0400\n\n    Merge branch 'espnet:master' into master\n\ncommit b757b89d45d5574cebf44e225cbe32e3e9e4f522\nMerge: 930b380de 664414c8f\nAuthor: Tomoki Hayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Fri Apr 29 16:11:56 2022 +0900\n\n    Merge pull request #4320 from cadia-lvl/add-progress-bar\n\ncommit 930b380de02b31f8d2da4144d471e60ed41d70fc\nMerge: 2a48371b8 de81cf979\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu Apr 28 16:30:34 2022 -0400\n\n    Merge pull request #4316 from simpleoier/enh_s2t\n\n    add egs2/chime4/enh_asr1 recipe and results\n\ncommit de81cf979fd61ab13e0ab0fe0432fbbaa4776be3\nAuthor: simpleoier <netnetchangxk@gmail.com>\nDate:   Thu Apr 28 11:54:10 2022 -0400\n\n    update egs2/chime4/enh_asr1/README.md and related enh1, asr1 configs.\n\ncommit 664414c8f27d5148377ffa733c7f8369eaf7ebd4\nAuthor: kan-bayashi <hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp>\nDate:   Thu Apr 28 21:31:45 2022 +0900\n\n    fixed flake8\n\ncommit 2a48371b8ceffd4899dc08f2fc5df092ed1d8a93\nMerge: 72c1d8f2b 5a9178236\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu Apr 28 07:40:31 2022 -0400\n\n    Merge pull request #4243 from D-Keqi/master\n\n    Add streaming ST/SLU\n\ncommit 72c1d8f2bde996febde895c603722dba1634cf20\nMerge: b7f0a5a6f 406656cdc\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu Apr 28 07:37:23 2022 -0400\n\n    Merge pull request #4110 from earthmanylf/dpclanddan\n\n    Merge Deep Clustering and Deep Attractor Network to enh separator\n\ncommit b7f0a5a6fc227049c1b8735d8ac4362c27333022\nMerge: 44971ff96 2d950f962\nAuthor: Shinji Watanabe <sw005320@gmail.com>\nDate:   Thu Apr 28 07:33:11 2022 -0400\n\n    Merge pull request #4328 from Emrys365/egs2_aishell4\n\n    Rename egs2/clarity21/enh_2021 to egs2/clarity21/enh1\n\ncommit 2d950f96223fd4823203b6a4e9afdc86b2357e7e\nAuthor: Wangyou Zhang <C0me_On@163.com>\nDate:   Thu Apr 28 16:58:26 2022 +0800\n\n    Rename egs2/clarity21/enh_2021/\n\ncommit 2b663318cd1773fb8685b1e03295b6bc6889c283\nAuthor: simpleoier <netnetchangxk@gmail.com>\nDate:   Thu Apr 28 00:59:22 2022 -0400\n\n    fix small bugs and add CHiME4 enh_asr1 recipe & results\n\ncommit 406656cdcb668a77910074b4382b557b6f845c54\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Thu Apr 28 11:10:11 2022 +0800\n\n    Add custom name in __init__ in tf_domain.py; Merge test_dpcl_loss.py to test_tf_domain.py\n\ncommit 5a9178236bc1a7a4a5db82ad84773d9c43199c81\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 28 10:31:29 2022 +0800\n\n    use the another st_inference\n\ncommit 9e4bb7fa88e8c63e69712e77c5b783c64181fbc2\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 28 10:13:59 2022 +0800\n\n    fix conflict\n\ncommit 21d2ac6331ec0779b8ec2d3265ccdfabfaacbd61\nMerge: b801ddc96 44971ff96\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 28 10:12:15 2022 +0800\n\n    Merge pull request #17 from espnet/master\n\n    merge the latest espnet\n\ncommit b801ddc96aedd2a9b4e63d2e3612c3cf7417799a\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 28 10:11:11 2022 +0800\n\n    Add files via upload\n\ncommit 316cf02340a627548b71317ba04afac457f68101\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 28 10:04:29 2022 +0800\n\n    fix conflict\n\ncommit 9b33b791d7c7b509f514b7540a8ec5dd7fff9d0b\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 27 23:22:22 2022 +0800\n\n    Fix format\n\ncommit 346a42467881e5bbd9414200dd3c915935eb56dd\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 27 22:37:22 2022 +0800\n\n    Fix format\n\ncommit 44971ff962aae30c962226f1ba3d87de057ac00e\nMerge: 0ae377389 c4b93e8fd\nAuthor: Jiatong <728307998@qq.com>\nDate:   Wed Apr 27 10:13:03 2022 -0400\n\n    Merge pull request #4324 from ftshijt/master\n\n    Add Test Functions for ST Train and Inference\n\ncommit 0d3be31602306650fee44c367cbc788e0b0462db\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 27 22:09:12 2022 +0800\n\n    Fix format\n\ncommit b24d108b0d7d501b2faa1971feca5a281198d351\nMerge: 4c679c061 f1312a8b2\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 27 21:29:33 2022 +0800\n\n    Fix conflict\n\ncommit 4c679c061c1a0be411f613bdbdeb7849af19edf4\nMerge: a90e2ecef 0ae377389\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 27 21:15:33 2022 +0800\n\n    Fix conflict\n\ncommit 10e6c7ea2e5783442631213dfc20dd7b9543839d\nAuthor: Gunnar Thor <ornolfsson@gmail.com>\nDate:   Wed Apr 27 09:30:47 2022 +0000\n\n    split docstring to conform with linter\n\ncommit c4b93e8fd870954ec2649abc3fc6172d78d92166\nAuthor: ftshijt <728307998@qq.com>\nDate:   Wed Apr 27 01:49:00 2022 -0400\n\n    apply black\n\ncommit 04d0cd84878701a0ff5e09933581c98ef7e0adac\nMerge: 72b6b21d5 4a12ab320\nAuthor: ftshijt <728307998@qq.com>\nDate:   Wed Apr 27 01:27:36 2022 -0400\n\n    Merge branch 'master' of https://github.com/ftshijt/espnet\n\ncommit 72b6b21d509a26d30a454525811c3530ee6b297b\nAuthor: ftshijt <728307998@qq.com>\nDate:   Wed Apr 27 01:27:09 2022 -0400\n\n    add st unit test\n\ncommit d1e8ac3d8717f8717fb645592c25ee8cafc4060c\nAuthor: ftshijt <728307998@qq.com>\nDate:   Wed Apr 27 01:15:18 2022 -0400\n\n    update test\n\ncommit 5fb7dd619293dcd1cc02c6371c4079c22a40a23b\nAuthor: ftshijt <728307998@qq.com>\nDate:   Wed Apr 27 00:53:46 2022 -0400\n\n    remove requirement for src_token_list\n\ncommit 4118b1b21f25fc7d8aa56658cd7ff691684884be\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 27 10:31:42 2022 +0800\n\n    fix conflict\n\ncommit 5436784241eaa4f60e0990627758a841e7927651\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 27 10:06:19 2022 +0800\n\n    Update test_integration_espnet2.sh\n\ncommit 469168b4451b4922306b3393598d199a514acd50\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 27 10:04:56 2022 +0800\n\n    fix issue\n\ncommit 06ddfe19a346f1ea8b620e4eb5bf61bfdcfc3309\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 27 10:01:38 2022 +0800\n\n    fix conflict\n\ncommit 5a81f91ce6734745272e6d960261797cfcb3dd41\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 27 09:57:18 2022 +0800\n\n    fix conflict\n\ncommit 91d48d920c229af3902fc05c361ba1b5f1636c67\nAuthor: Gunnar Thor <ornolfsson@gmail.com>\nDate:   Tue Apr 26 22:21:13 2022 +0000\n\n    applied black\n\ncommit ec518ccc74b85e3b50304ab70ae5a1f069df0038\nAuthor: Gunnar Thor <ornolfsson@gmail.com>\nDate:   Wed Feb 23 11:31:56 2022 +0000\n\n    Add progress bar to phonemization\n\ncommit f1312a8b2eeecf57f740b963b832dc4a806ac5f8\nAuthor: earthmanylf <43513215+earthmanylf@users.noreply.github.com>\nDate:   Mon Apr 25 10:37:19 2022 +0800\n\n    Update README.md\n\n    Co-authored-by: Wangyou Zhang <C0me_On@163.com>\n\ncommit a90e2ecef4854884dc525345a466f33fce79bd0a\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Sun Apr 24 22:55:54 2022 +0800\n\n    Fix format problems\n\ncommit be0112bf99c7caf787feba50c7dbc47a1879dbfb\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Sun Apr 24 22:06:45 2022 +0800\n\n    Fix format problems\n\ncommit 16acdadb6dba56d0f91a3132b540a01c9bd25c89\nMerge: feb28baf9 f6a2522ad\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Sun Apr 24 21:14:02 2022 +0800\n\n    Fix conflict\n\ncommit 95be28ab0e48415922677a92639833d648f3844c\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 14:47:11 2022 +0800\n\n    Fix CI\n\ncommit a0966f61701041228c96924359b8e6678960a31a\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 14:46:10 2022 +0800\n\n    Fix CI\n\ncommit 1daecd4570f477da905e4365ff30e4c0be53ca44\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 14:44:21 2022 +0800\n\n    fix CI\n\ncommit 7261735b82173ae5ac377844fad2f3b9289e08ec\nMerge: 809106e2a f6a2522ad\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 14:21:06 2022 +0800\n\n    Merge pull request #15 from espnet/master\n\n    Merging the latest ESPnet\n\ncommit 809106e2a512990b30fd1afcf2c7bf897d185d58\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 12:33:18 2022 +0800\n\n    show the log result\n\ncommit 65b53563cac0fdc09d653112f85dd735313cb650\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Apr 23 11:10:41 2022 +0800\n\n    show the error report in the log\n\ncommit 36bdfcbfd0731e543db130b6fb756e140f9f2cb2\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 21 15:21:07 2022 +0800\n\n    fix ci\n\ncommit c8e05efd90ea4c9f775b149916d05f0f74092157\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 21 11:30:54 2022 +0800\n\n    fix ci\n\ncommit 4831a6671728e52f0b2a0766a7c4cb60dd3d470f\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 20 20:34:26 2022 +0800\n\n    fix CI\n\ncommit 26fc7e1b41c57dc5c6a6882fe20a8847ee5a055c\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 20 16:37:29 2022 +0800\n\n    Add files via upload\n\ncommit b7c7bf13f9df6d9c09888c21c5c071c15f1023bc\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 20 15:19:37 2022 +0800\n\n    fix ci\n\ncommit 2b1b6bbef15553a11862a9c74352bed95412337d\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 20 11:33:40 2022 +0800\n\n    fix fbank_pitch issue\n\ncommit 0d5736fc393332465ae49a620392735a22312c97\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 20 11:33:21 2022 +0800\n\n    fix fbank_pitch issue\n\ncommit 835033c70cb2821340481b6e3f695d3afe6cbcd0\nMerge: fcf13c412 42eb3108a\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Tue Apr 19 07:36:09 2022 -0400\n\n    Merge branch 'espnet:master' into master\n\ncommit 70c1980b7c8d396bd5d05d8eba50bf90a84bff55\nAuthor: D-Keqi <462975470@qq.com>\nDate:   Tue Apr 19 19:01:41 2022 +0800\n\n    fix CI\n\ncommit fabb3a1fd17b10cbcf252240e0c40243a8c2f971\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:39:39 2022 +0800\n\n    update the test_integration_espnet2\n\ncommit c08e023e429ad90399f3722d825ccaa33c84b291\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:36:09 2022 +0800\n\n    Update and rename tmp to path.sh\n\ncommit 838d2ecfa767585a3df0161388f5dd5de426695a\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:35:08 2022 +0800\n\n    Add files via upload\n\ncommit 62162ae8938d71f0f9040ee1e27eb40c83882808\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:33:31 2022 +0800\n\n    Create tmp\n\ncommit 9a5585e282b68d44921879385f5a3796bacd1fdb\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:33:00 2022 +0800\n\n    Delete t\n\ncommit 349f4ab3498bc296d46ad4b42a77fda25d5e2286\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:31:43 2022 +0800\n\n    add conf\n\ncommit e3486d24210cb53491518d913df2268a2f03eded\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:28:12 2022 +0800\n\n    Create t\n\ncommit 652cf1774dd442d55082652713bbadbc4b6946a6\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:27:47 2022 +0800\n\n    Delete tmp\n\ncommit 48fcab7a8d8b0ad1a97798fa823d315aa7708d3d\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:27:12 2022 +0800\n\n    add st1 of mini_an4\n\ncommit 1800b0be298111842ab2a3cf5f39a9ac79c3a86f\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Tue Apr 19 16:25:21 2022 +0800\n\n    Create tmp\n\ncommit 0a1d05b61d611ca8a7b7ca1815ae089781cbdfde\nMerge: 73ca6e4e4 952a70a70\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Apr 13 10:20:46 2022 +0800\n\n    Merge pull request #14 from espnet/master\n\n    Merge the latest ESPnet\n\ncommit 73ca6e4e4baddd5f3fb6075788ed3e902021b9c8\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:59:52 2022 +0800\n\n    fix ci\n\n    fix ci\n\ncommit acd3e0acdc4d4c6eadfa531711906aa29ffb01a0\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:58:34 2022 +0800\n\n    fix CI\n\n    fix CI\n\ncommit e6da9baea12c6383282bdb716745060be5011a08\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:16:45 2022 +0800\n\n    Add files via upload\n\ncommit fc45fa368bc55b92f94e9ae6f9a6953728f3c894\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:13:53 2022 +0800\n\n    Delete README.md\n\ncommit 5b8c0b567f6b172e2112c5460c45e44b934478a6\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:13:11 2022 +0800\n\n    Delete egs2/chime4/asr1/exp/asr_train_asr_streaming_transformer_raw_en_char/decode_asr_streamindt05_real_beamformit_2micsg_lm_lm_train_lm_en_char_valid.loss.ave_asr_model_valid.acc.ave directory\n\ncommit 87ac110aaf70e2c339bac6ed7c5b60a856acc535\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:10:14 2022 +0800\n\n    streaming slu\n\ncommit 7b7fde9752cd9cd4905d642996215a158bf8d026\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:09:27 2022 +0800\n\n    streaming slu\n\ncommit fcd129620bbbc063dd918b83961d568ad694e45a\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:08:55 2022 +0800\n\n    streaming st\n\ncommit 17fe79ca89b496e4f9b6b4caaa2497816d4855b3\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:07:28 2022 +0800\n\n    streaming st\n\ncommit 812a527bb836a2fbd12ceb6d3bcabcc728d88427\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:06:31 2022 +0800\n\n    streaming st\n\ncommit e69a6d8efcd1ae57aca6315d70a20e484d360f7f\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 17:05:25 2022 +0800\n\n    streaming st\n\ncommit e488037b8d9b3e46476874f62b095ae5b7323e19\nMerge: 9fb445053 189e1593d\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Thu Apr 7 15:32:57 2022 +0800\n\n    Merge pull request #13 from espnet/master\n\n    Update lastest espnet\n\ncommit fcf13c412842d57cf48580dd89ff0d1fc5e6c3e0\nMerge: 39700a054 c4aba12f9\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Wed Apr 6 13:35:13 2022 -0400\n\n    Merge branch 'espnet:master' into master\n\ncommit feb28baf9dd6af564fe30920c1c6e70c2258e0de\nMerge: 3e6167c51 c4aba12f9\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Wed Apr 6 19:24:06 2022 +0800\n\n    Add deep clustering end-to-end training method\n\ncommit 50269e8b4dd0696d02e5da9f70c2d7952a26f392\nAuthor: WeiGodHorse <weigodhorse@gmail.com>\nDate:   Fri Mar 25 22:58:41 2022 +0800\n\n    fix a bug in Mandarin pypinyin_g2p_phone\n\ncommit 39700a054ac5ed718a1eb74cef9b64b2144b727c\nMerge: aa706c512 14c635069\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Thu Mar 24 17:42:11 2022 -0400\n\n    Merge branch 'espnet:master' into master\n\ncommit aa706c5122391feee57d4db121a403dfd8ea0ab0\nMerge: ab2fa25af 350af365f\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Wed Mar 23 23:34:17 2022 -0400\n\n    Merge branch 'espnet:master' into master\n\ncommit ab2fa25af6dffce3ecdf3e92adaa171d3d156d50\nMerge: de5e7139b cb8181a99\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Tue Mar 8 16:03:38 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit de5e7139b65549adfcac58cb0ee23c32c50634ea\nMerge: 5ef36bcae 1bac0f080\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Tue Mar 8 15:09:20 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 5ef36bcae3fac1792ccc2aae6b7dbab715f094fe\nMerge: 597cd7bd8 0c246e23c\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Tue Mar 8 13:35:27 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 597cd7bd8a0efbe82733d19774297ab90f5c659f\nMerge: 6625f9056 f16e579e2\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Mon Mar 7 21:54:06 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 6625f9056b5087aeb13a2214c770d586c067f5e3\nMerge: 5f237866b 5e070668e\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Mon Mar 7 13:35:03 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 3e6167c51df23b7629d7830e81e8cf4ea52032fc\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Mon Mar 7 20:03:31 2022 +0800\n\n    Fixed format in some files\n\ncommit 294373a121cf0766efe623dc56b12d0990a77c93\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Mon Mar 7 18:26:49 2022 +0800\n\n    Update code and add comments in separator\n\ncommit 5f86c1104cbce4275043e11050b69191834ddbc0\nMerge: 7aa90b584 6f429608b\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Mon Mar 7 18:06:10 2022 +0800\n\n    Add experiment result in egs2/wsj0_2mix/enh1/README.md; Update code in some files\n\ncommit 5f237866b360028676c7b9e903d15839cdaa0113\nMerge: 66c1a798d 6f429608b\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Sun Mar 6 19:26:35 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 66c1a798d15f531b4c4b4c1e02cfd1eda6813f92\nMerge: 5c5eb0292 a04a98c98\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Thu Mar 3 18:14:47 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 7aa90b5844ba1d0050cfd737b2a2fabe9abd5d62\nMerge: 5f7e2e714 b274c4ea6\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Thu Mar 3 16:20:25 2022 +0800\n\n    Merge branch 'master' of github.com:espnet/espnet into dpclanddan\n\ncommit 5c5eb0292e28c19345fc71d456348f6353f2e2a4\nMerge: bd8e400fa 9863980d2\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Wed Mar 2 12:13:35 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit bd8e400fa37ebc1b77f7a938ae9275bb18de6fe5\nMerge: 58aec432d 7999009d5\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Mon Feb 28 20:37:32 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 5f7e2e7140cc7204acecda90a6ff1d5379967da6\nMerge: d3acdcc3b 637d8c333\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Sun Feb 27 13:19:45 2022 +0800\n\n    Merge branch 'master' of github.com:espnet/espnet into dpclanddan\n\ncommit d3acdcc3bd537cf3f50c8d5c4642dfc488daa656\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Fri Feb 25 18:32:30 2022 +0800\n\n    fix bugs of test_dan_separator.py\n\ncommit c54d9a4087106b56ab5ce4ec9758aeb74bca0b4c\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Fri Feb 25 16:00:30 2022 +0800\n\n    add subs to the abs_separator.py\n\ncommit c1d9be5f4f9eb32bc75fb7a8b2fe406aa997946c\nAuthor: earthmanylf <411214987@qq.com>\nDate:   Fri Feb 25 15:30:46 2022 +0800\n\n    update for dpcl and dan\n\ncommit 58aec432d97300ec12494676a19900a08a950827\nMerge: 23a537e2a 9c24b3add\nAuthor: Roshan S Sharma <36464960+roshansh-cmu@users.noreply.github.com>\nDate:   Wed Feb 23 16:17:09 2022 -0500\n\n    Merge branch 'espnet:master' into master\n\ncommit 23a537e2ad1ee9af7e8016054208d5ce1cc572fd\nAuthor: roshansh-cmu <roshansh@andrew.cmu.edu>\nDate:   Tue Feb 22 06:50:03 2022 -0500\n\n    black fix\n\ncommit 8572a57af47ef72e9f010601483b31eb96baf03f\nMerge: 969b333d9 650472b45\nAuthor: roshansh-cmu <roshansh@andrew.cmu.edu>\nDate:   Mon Feb 21 22:35:49 2022 -0500\n\n    Mergefix\n\ncommit ee20e18a5f0eef55c8b0709e1e6b9bcddf10e4e6\nMerge: 63f88c02b a3e1543e9\nAuthor: earthmanylf <43513215+earthmanylf@users.noreply.github.com>\nDate:   Wed Feb 16 14:29:36 2022 +0800\n\n    Merge pull request #1 from espnet/master\n\n    Merge from upstream\n\ncommit 9fb445053f999b64350e5e7a56a1699a727ed125\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Sep 15 00:30:05 2021 +0800\n\n    Update README.md\n\ncommit 8c6d3e1614a247b78f1b17ff2c6ef3b3725b166a\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Wed Sep 15 00:29:31 2021 +0800\n\n    Update README.md\n\ncommit 2411dbb82b08aee182df0738a47d7f6f44bdcea8\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Mon Sep 13 13:08:52 2021 +0800\n\n    Update README.md\n\ncommit 3edc1a6d816428b3e4e099271dc51c117b9c8d3b\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Mon Sep 13 13:08:25 2021 +0800\n\n    Update README.md\n\ncommit d4d4b7e450992867bc0ee91ffb467ec38ad6981c\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Sep 11 23:11:39 2021 +0800\n\n    Update README.md\n\ncommit 885ab0552dc26076b0b581eb88813f426179fdcb\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Sep 4 10:48:05 2021 +0800\n\n    add results\n\ncommit dfba960da5e60cd9d78c439b7fa0e400332fbe46\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Sep 4 10:43:36 2021 +0800\n\n    create exp\n\ncommit 391d7c78f310313ca78abc1b3341183a15336579\nAuthor: D-Keqi <61508571+D-Keqi@users.noreply.github.com>\nDate:   Sat Sep 4 10:40:23 2021 +0800\n\n    steaming results\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def make_loss_args(**kwargs):",
            "",
            "",
            "@pytest.mark.skipif(",
            "-    LooseVersion(torch.__version__) < LooseVersion(\"1.4\"),",
            "+    V(torch.__version__) < V(\"1.4\"),",
            "reason=\"Pytorch >= 1.4 is required.\",",
            ")",
            "@pytest.mark.skipif("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=LooseVersion), value='V')",
            "Update(target_node=ASTNode(type=identifier, text=LooseVersion), value='V')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1499,
        "neg_line": [
            "-LooseVersion(torch.__version__) < LooseVersion(\"1.4\"),"
        ],
        "pos_line": [
            "+V(torch.__version__) < V(\"1.4\"),"
        ],
        "core_change": "-LooseVersion(torch.__version__) < LooseVersion(\"1.4\"), +V(torch.__version__) < V(\"1.4\"),",
        "core_API": "skipif"
    },
    {
        "commit_hash": "8073c4a06491ee3af34a0a0a60ba4ec8f8520d52",
        "index": "0bc01c4..890d438 100644",
        "commit_message": "bug fix (#1084)\n\n\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class Graph(kerastuner.HyperModel, serializable.Serializable):",
            "",
            "def build(self, hp):",
            "\"\"\"Build the HyperModel into a Keras Model.\"\"\"",
            "+        tf.keras.backend.clear_session()",
            "self._register_hps(hp)",
            "self.compile()",
            "real_nodes = {}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=2473735)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2473736)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2473737)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2473738)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2473739)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2473740)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'clear_session'), position=2, insert_id=2473741)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2473742)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2473743)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2473744)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2473745)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'backend'), position=2, insert_id=2473746)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2473747)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2473748)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'keras'), position=2, insert_id=2473749)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 15,
        "number": 1503,
        "neg_line": [],
        "pos_line": [
            "+tf.keras.backend.clear_session()"
        ],
        "core_change": "+tf.keras.backend.clear_session()",
        "core_API": "clear_session"
    },
    {
        "commit_hash": "47d663f0fa97919cce60a64ffa977d9aed6cff45",
        "index": "a3b49e0d..6464229b 100644",
        "commit_message": "Add docstrings of core modules and methods (#3120)\n\n* Add docstrings of core modules and methods\n\n* Update docs and fix comments\n\n* Complete docstrings\n\n* Resolve comments\n\n* reformat docstrings\n\n* resolve comments\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class HRNet(nn.Module):",
            "return y_list",
            "",
            "def train(self, mode=True):",
            "+        \"\"\"Convert the model into training mode whill keeping the normalization",
            "+        layer freezed\"\"\"",
            "super(HRNet, self).train(mode)",
            "if mode and self.norm_eval:",
            "for m in self.modules():"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=1426601)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1426602)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"Convert the model into training mode whill keeping the normalization\\n        layer freezed\"\"\"'), position=0, insert_id=1426603)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 4,
        "number": 1506,
        "neg_line": [],
        "pos_line": [
            "+\"\"\"Convert the model into training mode whill keeping the normalization",
            "+layer freezed\"\"\""
        ],
        "core_change": "+\"\"\"Convert the model into training mode whill keeping the normalization +layer freezed\"\"\"",
        "core_API": "modules"
    },
    {
        "commit_hash": "0591b470b65a5301943fa7dbfc6a2f0a09373e86",
        "index": "3518f5cb5d..85d301aba5 100644",
        "commit_message": "ivy.Shape fixes (#2006)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def indices_where(",
            "def shape(",
            "x: Union[tf.Tensor, tf.Variable],",
            "as_array: bool = False,",
            "-) -> Union[tf.Tensor, tf.Variable, TensorShape]:",
            "+) -> Union[tf.Tensor, ivy.Shape, ivy.Array]:",
            "if as_array:",
            "-        return tf.shape(x)",
            "+        return ivy.array(tf.shape(x))",
            "else:",
            "-        return tuple(x.shape)",
            "+        return ivy.Shape(x.shape)",
            "",
            "",
            "def get_num_dims(x, as_tensor=False):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=subscript), node=('attribute', None), position=6, insert_id=2005590)",
            "Insert(target_node=ASTNode(type=return_statement), node=('call', None), position=1, insert_id=2005591)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='ivy')",
            "Update(target_node=ASTNode(type=identifier, text=Variable), value='Shape')",
            "Update(target_node=ASTNode(type=identifier, text=TensorShape), value='ivy')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=TensorShape), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2005592)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Array'), position=2, insert_id=2005593)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2005594)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2005595)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ivy'), position=0, insert_id=2005596)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2005597)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'array'), position=2, insert_id=2005598)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2005599)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2005600)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2005601)",
            "Update(target_node=ASTNode(type=identifier, text=tuple), value='ivy')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tuple), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2005602)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Shape'), position=2, insert_id=2005603)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 1507,
        "neg_line": [
            "-) -> Union[tf.Tensor, tf.Variable, TensorShape]:",
            "-return tf.shape(x)",
            "-return tuple(x.shape)"
        ],
        "pos_line": [
            "+) -> Union[tf.Tensor, ivy.Shape, ivy.Array]:",
            "+return ivy.array(tf.shape(x))",
            "+return ivy.Shape(x.shape)"
        ],
        "core_change": "-) -> Union[tf.Tensor, tf.Variable, TensorShape]: +) -> Union[tf.Tensor, ivy.Shape, ivy.Array]: -return tf.shape(x) +return ivy.array(tf.shape(x)) -return tuple(x.shape) +return ivy.Shape(x.shape)",
        "core_API": "shape"
    },
    {
        "commit_hash": "2332b5fdca636b42d2a31be289cf1a448cfe5d5d",
        "index": "9d41b55ef2..aa88484d7f 100644",
        "commit_message": "fixing test_matrix_rank. rtol to keyword argument\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def matrix_rank(",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "# ToDo: add support for default rtol value here, for the case where None is provided",
            "-    return torch.linalg.matrix_rank(x, rtol, out=out)",
            "+    return torch.linalg.matrix_rank(x, rtol=rtol, out=out)",
            "",
            "",
            "def matrix_transpose(x: torch.Tensor) -> torch.Tensor:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=348983)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=rtol), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=348984)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'rtol'), position=2, insert_id=348985)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1508,
        "neg_line": [
            "-return torch.linalg.matrix_rank(x, rtol, out=out)"
        ],
        "pos_line": [
            "+return torch.linalg.matrix_rank(x, rtol=rtol, out=out)"
        ],
        "core_change": "-return torch.linalg.matrix_rank(x, rtol, out=out) +return torch.linalg.matrix_rank(x, rtol=rtol, out=out)",
        "core_API": "matrix_rank"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "bbccda8c..8c8cd4b8 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestInitializers(AllenNlpTestCase):",
            "block_orthogonal(tensor, [7, 2, 1])",
            "",
            "def test_uniform_unit_scaling_can_initialize(self):",
            "-        tensor = Variable(torch.zeros([10, 6]))",
            "+        tensor = torch.zeros([10, 6])",
            "uniform_unit_scaling(tensor, \"linear\")",
            "",
            "assert tensor.data.max() < math.sqrt(3/10)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1509,
        "neg_line": [
            "-tensor = Variable(torch.zeros([10, 6]))"
        ],
        "pos_line": [
            "+tensor = torch.zeros([10, 6])"
        ],
        "core_change": "-tensor = Variable(torch.zeros([10, 6])) +tensor = torch.zeros([10, 6])",
        "core_API": "zeros"
    },
    {
        "commit_hash": "cb1751dc6464a20c6e7311629066771c99743fda",
        "index": "434e8d2e6..9678855d0 100644",
        "commit_message": "Hard fix for parameter double send and get\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchHook:",
            "",
            "def hooked__repr__(self):",
            "if hasattr(self, \"child\"):",
            "-                return \"Parameter containing:\\n\" + self.child.__repr__()",
            "+                return \"&Parameter containing:\\n\" + self.child.__repr__()",
            "else:",
            "return self.native_param___repr__()",
            "",
            "-        torch.nn.Parameter.__repr__ = hooked__repr__",
            "+        # torch.nn.Parameter.__repr__ = hooked__repr__",
            "",
            "# Hook .data to handle chain assignment when needed"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"Parameter containing:\\n\"), value='\"&Parameter containing:\\\\n\"')",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Parameter))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=__repr__))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=hooked__repr__))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 15,
        "number": 1510,
        "neg_line": [
            "-return \"Parameter containing:\\n\" + self.child.__repr__()",
            "-torch.nn.Parameter.__repr__ = hooked__repr__"
        ],
        "pos_line": [
            "+return \"&Parameter containing:\\n\" + self.child.__repr__()",
            "+# torch.nn.Parameter.__repr__ = hooked__repr__"
        ],
        "core_change": "-return \"Parameter containing:\\n\" + self.child.__repr__() +return \"&Parameter containing:\\n\" + self.child.__repr__() -torch.nn.Parameter.__repr__ = hooked__repr__ +# torch.nn.Parameter.__repr__ = hooked__repr__",
        "core_API": "__repr__"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "c8d33f399..9c9bcf8fb 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario ako <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class MedicalDialog(datasets.GeneratorBasedBuilder):",
            "path_to_manual_file = os.path.abspath(os.path.expanduser(dl_manager.manual_dir))",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('medical_dialog', data_dir=...)`. Manual download instructions: {})\".format(",
            "-                    path_to_manual_file, self.manual_download_instructions",
            "-                )",
            "+                f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('medical_dialog', data_dir=...)`. Manual download instructions: {self.manual_download_instructions})\"",
            ")",
            "",
            "filepaths = ["
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'medical_dialog\\', data_dir=...)`. Manual download instructions: {self.manual_download_instructions})\"'), position=1, insert_id=1781613)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('medical_dialog', data_dir=...)`. Manual download instructions: {})\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=path_to_manual_file))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 1511,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('medical_dialog', data_dir=...)`. Manual download instructions: {})\".format(",
            "-path_to_manual_file, self.manual_download_instructions",
            "-)"
        ],
        "pos_line": [
            "+f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('medical_dialog', data_dir=...)`. Manual download instructions: {self.manual_download_instructions})\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('medical_dialog', data_dir=...)`. Manual download instructions: {})\".format( -path_to_manual_file, self.manual_download_instructions -) +f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('medical_dialog', data_dir=...)`. Manual download instructions: {self.manual_download_instructions})\"",
        "core_API": "abspath"
    },
    {
        "commit_hash": "2322eb8e2f9765cb73f59b324cc46a0e9cfe803f",
        "index": "ea379a039..48c26a138 100644",
        "commit_message": "Update serving signatures and make sure we actually use them (#19034)\n\n* Override save() to use the serving signature as the default\n\n* Replace int32 with int64 in all our serving signatures\n\n* Remember one very important line so as not to break every test at once\n\n* Dtype fix for TFLED\n\n* dtype fix for shift_tokens_right in general\n\n* Dtype fixes in mBART and RAG\n\n* Fix dtypes for test_unpack_inputs\n\n* More dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Yet more mBART + RAG dtype fixes\n\n* Add a check that the model actually has a serving method\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFTapasPreTrainedModel(TFPreTrainedModel):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "\"attention_mask\": tf.TensorSpec((None, None), tf.float32, name=\"attention_mask\"),",
            "-                \"token_type_ids\": tf.TensorSpec((None, None, None), tf.int32, name=\"token_type_ids\"),",
            "+                \"token_type_ids\": tf.TensorSpec((None, None, None), tf.int64, name=\"token_type_ids\"),",
            "}",
            "]",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2361627)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2361628)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2361629)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'TensorSpec'), position=2, insert_id=2361630)",
            "Update(target_node=ASTNode(type=identifier, text=int32), value='int64')",
            "Update(target_node=ASTNode(type=identifier, text=int32), value='int64')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=TensorSpec))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 1513,
        "neg_line": [
            "-\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "-\"token_type_ids\": tf.TensorSpec((None, None, None), tf.int32, name=\"token_type_ids\"),"
        ],
        "pos_line": [
            "+\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "+\"token_type_ids\": tf.TensorSpec((None, None, None), tf.int64, name=\"token_type_ids\"),"
        ],
        "core_change": "-\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"), +\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"), -\"token_type_ids\": tf.TensorSpec((None, None, None), tf.int32, name=\"token_type_ids\"), +\"token_type_ids\": tf.TensorSpec((None, None, None), tf.int64, name=\"token_type_ids\"),",
        "core_API": "function"
    },
    {
        "commit_hash": "d28030a59f965c0f433fe404599436eb9ed902d8",
        "index": "946fdb3..28f6138 100644",
        "commit_message": "fix elu activation function (#853)\n\n\n",
        "file": "tflearn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def selu(x):",
            "\"\"\"",
            "alpha = 1.6732632423543772848170429916717",
            "scale = 1.0507009873554804934193349852946",
            "-    return scale * tf.nn.elu(x, alpha)",
            "+    return scale*tf.where(x>=0.0, x, alpha*tf.nn.elu(x))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2350073)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2350074)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2350075)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2350076)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'where'), position=2, insert_id=2350077)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2350078)",
            "Insert(target_node=IN(type=argument_list), node=('comparison_operator', None), position=1, insert_id=2350079)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2350080)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'x'), position=3, insert_id=2350081)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=2350082)",
            "Insert(target_node=IN(type=argument_list), node=('binary_operator', None), position=5, insert_id=2350083)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=6, insert_id=2350084)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'x'), position=0, insert_id=2350085)",
            "Insert(target_node=IN(type=comparison_operator), node=('>=', '>='), position=1, insert_id=2350086)",
            "Insert(target_node=IN(type=comparison_operator), node=('float', '0.0'), position=2, insert_id=2350087)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'alpha'), position=0, insert_id=2350088)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=2350089)",
            "Insert(target_node=IN(type=binary_operator), node=('call', None), position=2, insert_id=2350090)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=alpha))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 1516,
        "neg_line": [
            "-return scale * tf.nn.elu(x, alpha)"
        ],
        "pos_line": [
            "+return scale*tf.where(x>=0.0, x, alpha*tf.nn.elu(x))"
        ],
        "core_change": "-return scale * tf.nn.elu(x, alpha) +return scale*tf.where(x>=0.0, x, alpha*tf.nn.elu(x))",
        "core_API": "elu"
    },
    {
        "commit_hash": "660e0b97bd652bd3a0dfd5f847e5cf62502d0469",
        "index": "918a7fc03..ac1cc13e9 100644",
        "commit_message": "Fix train_step, test_step and tests for CLIP (#18684)\n\n* Fix train_step and test_step, correctly enable CLIP fit test\n\n* Stop using get_args on older Python versions\n\n* Don't use get_origin either\n\n* UnionType is actually even newer, don't use that either\n\n* Apply the same fix to test_loss_computation\n\n* Just realized I was accidentally skipping a bunch of tests!\n\n* Fix test_loss_computation for models without separable labels\n\n* Fix scalar losses in test_step and train_step\n\n* Stop committing your breakpoints\n\n* Fix Swin loss shape\n\n* Fix Tapas loss shape\n\n* Shape fixes for TAPAS, DeIT, HuBERT and ViTMAE\n\n* Add loss computation to TFMobileBertForPreTraining\n\n* make fixup and move copied from statement\n\n* make fixup and move copied from statement\n\n* Correct copied from\n\n* Add labels and next_sentence_label inputs to TFMobileBERT\n\n* Make sure total_loss is always defined\n\n* Update tests/test_modeling_tf_common.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Fix copied from\n\n* Ensure CTC models get labels in tests\n\n* Ensure CTC models get labels in tests\n\n* Fix tests for vit_mae\n\n* Fix tests for vit_mae\n\n* Fix tests for vit_mae\n\n* Reduce batch size for wav2vec2 testing because it was causing OOM\n\n* Skip some TAPAS tests that are failing\n\n* Skip a failing HuBERT test\n\n* make style\n\n* Fix mobilebertforpretraining test\n\n* Skip Wav2Vec2 tests that use huge amounts of mem\n\n* Skip keras_fit for Wav2Vec2 as well\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFDeiTForMaskedImageModeling(TFDeiTPreTrainedModel):",
            "total_loss = tf.reduce_sum(reconstruction_loss * mask)",
            "num_masked_pixels = (tf.reduce_sum(mask) + 1e-5) * self.config.num_channels",
            "masked_im_loss = total_loss / num_masked_pixels",
            "+            masked_im_loss = tf.reshape(masked_im_loss, (1,))",
            "",
            "if not return_dict:",
            "output = (reconstructed_pixel_values,) + outputs[1:]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=2361815)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2361816)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'masked_im_loss'), position=0, insert_id=2361817)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2361818)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2361819)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2361820)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2361821)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2361822)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2361823)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reshape'), position=2, insert_id=2361824)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2361825)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'masked_im_loss'), position=1, insert_id=2361826)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2361827)",
            "Insert(target_node=IN(type=argument_list), node=('tuple', None), position=3, insert_id=2361828)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2361829)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2361830)",
            "Insert(target_node=IN(type=tuple), node=('integer', '1'), position=1, insert_id=2361831)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2361832)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=3, insert_id=2361833)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 19,
        "number": 1517,
        "neg_line": [],
        "pos_line": [
            "+masked_im_loss = tf.reshape(masked_im_loss, (1,))"
        ],
        "core_change": "+masked_im_loss = tf.reshape(masked_im_loss, (1,))",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "994c33016eff910d50b42a1625e268c11321b53d",
        "index": "ec69e34d55..9ed59a84c6 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def imag(",
            "*,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    if(input.dtype != torch.complex64):",
            "+    if input.dtype != torch.complex64:",
            "input = input.to(torch.complex64)",
            "return torch.imag(input)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=comparison_operator), position=1)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1520,
        "neg_line": [
            "-if(input.dtype != torch.complex64):"
        ],
        "pos_line": [
            "+if input.dtype != torch.complex64:"
        ],
        "core_change": "-if(input.dtype != torch.complex64): +if input.dtype != torch.complex64:",
        "core_API": "to"
    },
    {
        "commit_hash": "c74731bd4241316cf0b56b4dfd558c099ad8160c",
        "index": "26006db..1dc7ef0 100644",
        "commit_message": "0.2.17 (#231)\n\n* delete\n\n* Print Information Changes for AutoKeras v0.3 (#229) resolves #224\n\n* Update CONTRIBUTING.md\n\n* Develop (#187)\n\n* merge (#128)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* rm print\n\n* Issue#37 and Issue #79 Save keras model/autokeras model (#122)\n\n* Issue #37 Export Keras model\n\n* Issue #79 Save autokeras model\n\n* Issue #37 and Issue#79 Fixed comments\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79 Fixed pytests\n\n* Issue #37 and Issue #79\n\n* quick fix test\n\n* Progbar (#143)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* contribute guide\n\n* Add Progress Bar\n\n* Update utils.py\n\n* Update search.py\n\n* update constant (#145)\n\n* [WIP] Issue #158 Imageregressor (#159)\n\n* Develop (#146)\n\n* merge (#128)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* rm print\n\n* Issue#37 and Issue #79 Save keras model/autokeras model (#122)\n\n* Issue #37 Export Keras model\n\n* Issue #79 Save autokeras model\n\n* Issue #37 and Issue#79 Fixed comments\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79 Fixed pytests\n\n* Issue #37 and Issue #79\n\n* quick fix test\n\n* Progbar (#143)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* contribute guide\n\n* Add Progress Bar\n\n* Update utils.py\n\n* Update search.py\n\n* update constant (#145)\n\n* Update setup.py (#147)\n\n* Update setup.py\n\n* Update setup.py\n\n* Update setup.py (#155)\n\n* requirements\n\n* Issue #158 Export ImageRegressor model\n\n* Memory (#161)\n\n* aa\n\n* limit memory\n\n* refactor to_real_layer to member functions\n\n* bug fix (#166)\n\n* doc string changed for augment (#170)\n\nI added proper documentation for class ImageSupervised  arg 'augment'. It is 'None' by default. However, if it is 'None', then it uses Constant.DATA_AUGMENTATION which is 'True'. This is misleading when trying things out.\n\n* Update constant.py\n\n* bug fix (#177)\n\n* memory limit dynamically (#180)\n\n* memory limit dynamically\n\n* test\n\n* test fixed\n\n* [MRG]Dcgan (#175)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* finish workable version of gan\n\n* add unit test and small refactoring\n\n* add unsupervised super class\n\n* Fix test_dcgan ran too long issue, put default param in unsupervised::generate(input_sample=None)\n\n* remove examples/gan.py from repo\n\n* add missing import\n\n* correct model_trainer signature\n\n* fixed the bug in return value of train_model()\n\n* Update setup.py\n\n* [WIP]Update CONTRIBUTING.md (#190)\n\n* Update CONTRIBUTING.md\n\n* Update CONTRIBUTING.md\n\n* Update mkdocs.yml\n\n* code_reuse_example\n\n* Update CONTRIBUTING.md\n\n* update develop (#206) (#207)\n\n* Update CONTRIBUTING.md\n\n* Develop (#187)\n\n* merge (#128)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* rm print\n\n* Issue#37 and Issue #79 Save keras model/autokeras model (#122)\n\n* Issue #37 Export Keras model\n\n* Issue #79 Save autokeras model\n\n* Issue #37 and Issue#79 Fixed comments\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79 Fixed pytests\n\n* Issue #37 and Issue #79\n\n* quick fix test\n\n* Progbar (#143)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* contribute guide\n\n* Add Progress Bar\n\n* Update utils.py\n\n* Update search.py\n\n* update constant (#145)\n\n* [WIP] Issue #158 Imageregressor (#159)\n\n* Develop (#146)\n\n* merge (#128)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* rm print\n\n* Issue#37 and Issue #79 Save keras model/autokeras model (#122)\n\n* Issue #37 Export Keras model\n\n* Issue #79 Save autokeras model\n\n* Issue #37 and Issue#79 Fixed comments\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79 Fixed pytests\n\n* Issue #37 and Issue #79\n\n* quick fix test\n\n* Progbar (#143)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* contribute guide\n\n* Add Progress Bar\n\n* Update utils.py\n\n* Update search.py\n\n* update constant (#145)\n\n* Update setup.py (#147)\n\n* Update setup.py\n\n* Update setup.py\n\n* Update setup.py (#155)\n\n* requirements\n\n* Issue #158 Export ImageRegressor model\n\n* Memory (#161)\n\n* aa\n\n* limit memory\n\n* refactor to_real_layer to member functions\n\n* bug fix (#166)\n\n* doc string changed for augment (#170)\n\nI added proper documentation for class ImageSupervised  arg 'augment'. It is 'None' by default. However, if it is 'None', then it uses Constant.DATA_AUGMENTATION which is 'True'. This is misleading when trying things out.\n\n* Update constant.py\n\n* bug fix (#177)\n\n* memory limit dynamically (#180)\n\n* memory limit dynamically\n\n* test\n\n* test fixed\n\n* [MRG]Dcgan (#175)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* finish workable version of gan\n\n* add unit test and small refactoring\n\n* add unsupervised super class\n\n* Fix test_dcgan ran too long issue, put default param in unsupervised::generate(input_sample=None)\n\n* remove examples/gan.py from repo\n\n* add missing import\n\n* correct model_trainer signature\n\n* fixed the bug in return value of train_model()\n\n* Update setup.py\n\n* [WIP]Update CONTRIBUTING.md (#190)\n\n* Update CONTRIBUTING.md\n\n* Update CONTRIBUTING.md\n\n* Update mkdocs.yml\n\n* code_reuse_example\n\n* Update CONTRIBUTING.md\n\n* Update search.py\n\n* Develop (#215)\n\n* update develop (#206)\n\n* Update CONTRIBUTING.md\n\n* Develop (#187)\n\n* merge (#128)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* rm print\n\n* Issue#37 and Issue #79 Save keras model/autokeras model (#122)\n\n* Issue #37 Export Keras model\n\n* Issue #79 Save autokeras model\n\n* Issue #37 and Issue#79 Fixed comments\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79 Fixed pytests\n\n* Issue #37 and Issue #79\n\n* quick fix test\n\n* Progbar (#143)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* contribute guide\n\n* Add Progress Bar\n\n* Update utils.py\n\n* Update search.py\n\n* update constant (#145)\n\n* [WIP] Issue #158 Imageregressor (#159)\n\n* Develop (#146)\n\n* merge (#128)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* rm print\n\n* Issue#37 and Issue #79 Save keras model/autokeras model (#122)\n\n* Issue #37 Export Keras model\n\n* Issue #79 Save autokeras model\n\n* Issue #37 and Issue#79 Fixed comments\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79 Fixed pytests\n\n* Issue #37 and Issue #79\n\n* quick fix test\n\n* Progbar (#143)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* contribute guide\n\n* Add Progress Bar\n\n* Update utils.py\n\n* Update search.py\n\n* update constant (#145)\n\n* Update setup.py (#147)\n\n* Update setup.py\n\n* Update setup.py\n\n* Update setup.py (#155)\n\n* requirements\n\n* Issue #158 Export ImageRegressor model\n\n* Memory (#161)\n\n* aa\n\n* limit memory\n\n* refactor to_real_layer to member functions\n\n* bug fix (#166)\n\n* doc string changed for augment (#170)\n\nI added proper documentation for class ImageSupervised  arg 'augment'. It is 'None' by default. However, if it is 'None', then it uses Constant.DATA_AUGMENTATION which is 'True'. This is misleading when trying things out.\n\n* Update constant.py\n\n* bug fix (#177)\n\n* memory limit dynamically (#180)\n\n* memory limit dynamically\n\n* test\n\n* test fixed\n\n* [MRG]Dcgan (#175)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* finish workable version of gan\n\n* add unit test and small refactoring\n\n* add unsupervised super class\n\n* Fix test_dcgan ran too long issue, put default param in unsupervised::generate(input_sample=None)\n\n* remove examples/gan.py from repo\n\n* add missing import\n\n* correct model_trainer signature\n\n* fixed the bug in return value of train_model()\n\n* Update setup.py\n\n* [WIP]Update CONTRIBUTING.md (#190)\n\n* Update CONTRIBUTING.md\n\n* Update CONTRIBUTING.md\n\n* Update mkdocs.yml\n\n* code_reuse_example\n\n* Update CONTRIBUTING.md\n\n* bug_fix (#208)\n\n* bug_fix (#214) resolves #212\n\n* Update CONTRIBUTING.md\n\n* Update setup.py\n\n* delete (#217)\n\n* Print Information Changes for AutoKeras v0.3\n\n* 0.2.17\n\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class Searcher:",
            "if not re.search('out of memory', str(e)):",
            "raise e",
            "if self.verbose:",
            "-                print('out of memory')",
            "+                print('\\nCurrent model size is too big. Discontinuing training this model to search for other models.')",
            "Constant.MAX_MODEL_SIZE = graph.size() - 1",
            "return",
            "finally:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='out of memory'), value=\"'\\\\nCurrent model size is too big. Discontinuing training this model to search for other models.'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1523,
        "neg_line": [
            "-print('out of memory')"
        ],
        "pos_line": [
            "+print('\\nCurrent model size is too big. Discontinuing training this model to search for other models.')"
        ],
        "core_change": "-print('out of memory') +print('\\nCurrent model size is too big. Discontinuing training this model to search for other models.')",
        "core_API": "search"
    },
    {
        "commit_hash": "2a27c80063de01a6661d36e84edc5ad7e51db063",
        "index": "711708ac2..df2ca8a15 100644",
        "commit_message": "Fix BigBirdModelTester (#16310)\n\n* fix\n\n* update the expected value in test_fast_integration\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BigBirdModelTest(ModelTesterMixin, unittest.TestCase):",
            "self.assertTrue(",
            "torch.allclose(",
            "hidden_states[0, 0, :5],",
            "-                    torch.tensor([1.4943, 0.0928, 0.8254, -0.2816, -0.9788], device=torch_device),",
            "+                    torch.tensor([1.4825, 0.0774, 0.8226, -0.2962, -0.9593], device=torch_device),",
            "atol=1e-3,",
            ")",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=1.4943), value='1.4825')",
            "Update(target_node=ASTNode(type=float, text=0.0928), value='0.0774')",
            "Update(target_node=ASTNode(type=float, text=0.8254), value='0.8226')",
            "Update(target_node=ASTNode(type=float, text=0.2816), value='0.2962')",
            "Update(target_node=ASTNode(type=float, text=0.9788), value='0.9593')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1525,
        "neg_line": [
            "-torch.tensor([1.4943, 0.0928, 0.8254, -0.2816, -0.9788], device=torch_device),"
        ],
        "pos_line": [
            "+torch.tensor([1.4825, 0.0774, 0.8226, -0.2962, -0.9593], device=torch_device),"
        ],
        "core_change": "-torch.tensor([1.4943, 0.0928, 0.8254, -0.2816, -0.9788], device=torch_device), +torch.tensor([1.4825, 0.0774, 0.8226, -0.2962, -0.9593], device=torch_device),",
        "core_API": "assertTrue"
    },
    {
        "commit_hash": "d9a7362be22ca8c412bebc69f054ee546b6bc18e",
        "index": "4d5f69305..a0dc352ac 100644",
        "commit_message": "fix some compatibility problems with PyTorch 1.3.0 in ESPnet\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AttentionReference(torch.nn.Module):",
            "B, _, C = psd_in.size()[:3]",
            "assert psd_in.size(2) == psd_in.size(3), psd_in.size()",
            "# psd_in: (B, F, C, C)",
            "-        psd = psd_in.masked_fill(torch.eye(C, dtype=torch.uint8,",
            "+        datatype = torch.bool if is_torch_1_2_plus else torch.uint8",
            "+        psd = psd_in.masked_fill(torch.eye(C, dtype=datatype,",
            "device=psd_in.device), 0)",
            "# psd: (B, F, C, C) -> (B, C, F)",
            "psd = (psd.sum(dim=-1) / (C - 1)).transpose(-1, -2)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=164872)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=164873)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'datatype'), position=0, insert_id=164874)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=164875)",
            "Insert(target_node=IN(type=assignment), node=('conditional_expression', None), position=2, insert_id=164876)",
            "Insert(target_node=IN(type=conditional_expression), node=('attribute', None), position=0, insert_id=164877)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=164878)",
            "Insert(target_node=IN(type=conditional_expression), node=('identifier', 'is_torch_1_2_plus'), position=2, insert_id=164879)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=164880)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=attribute), position=4)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=164881)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=164882)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=164883)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'datatype'), position=2, insert_id=164884)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 1527,
        "neg_line": [
            "-psd = psd_in.masked_fill(torch.eye(C, dtype=torch.uint8,"
        ],
        "pos_line": [
            "+datatype = torch.bool if is_torch_1_2_plus else torch.uint8",
            "+psd = psd_in.masked_fill(torch.eye(C, dtype=datatype,"
        ],
        "core_change": "-psd = psd_in.masked_fill(torch.eye(C, dtype=torch.uint8, +datatype = torch.bool if is_torch_1_2_plus else torch.uint8 +psd = psd_in.masked_fill(torch.eye(C, dtype=datatype,",
        "core_API": "size"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "87252f39..0b44918b 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CategoricalAccuracyTest(AllenNlpTestCase):",
            "assert accuracy.get_metric(reset=True) == (0.25 + 1 + 0.5) / 3.0",
            "",
            "# # # Test with mask",
            "-        mask = torch.tensor([1, 0, 1], device=device)",
            "+        mask = torch.BoolTensor([True, False, True], device=device)",
            "targets = torch.tensor([2, 1, 4], device=device)",
            "accuracy(predictions, targets, mask)",
            "assert accuracy.get_metric(reset=True) == (0.25 + 0.5) / 2.0"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='BoolTensor')",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=1, insert_id=20040)",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=4, insert_id=20041)",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=6, insert_id=20042)",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 1528,
        "neg_line": [
            "-mask = torch.tensor([1, 0, 1], device=device)"
        ],
        "pos_line": [
            "+mask = torch.BoolTensor([True, False, True], device=device)"
        ],
        "core_change": "-mask = torch.tensor([1, 0, 1], device=device) +mask = torch.BoolTensor([True, False, True], device=device)",
        "core_API": "get_metric"
    },
    {
        "commit_hash": "76b266bfbf8a4a01dd42708f70df36accc66f97d",
        "index": "09f77089..8a4af825 100644",
        "commit_message": " Upgrade to modern Python syntax (#1213)\n\n* Upgrade to modern Python syntax\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Use f-strings\n\n* Placate DeepSource Python\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def load_pointcloud_ply(filename: str, header_size: int = 8) -> torch.Tensor:",
            "if not os.path.isfile(filename):",
            "raise ValueError(\"Input filename is not an existing file.\")",
            "if not (isinstance(header_size, int) and header_size > 0):",
            "-        raise TypeError(\"Input header_size must be a positive integer. Got {}.\".format(header_size))",
            "+        raise TypeError(f\"Input header_size must be a positive integer. Got {header_size}.\")",
            "# open the file and populate tensor",
            "-    with open(filename, 'r') as f:",
            "+    with open(filename) as f:",
            "points = []",
            "",
            "# skip header"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"Input header_size must be a positive integer. Got {header_size}.\"'), position=1, insert_id=419677)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"Input header_size must be a positive integer. Got {}.\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=header_size))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=string, text='r'))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 13,
        "number": 1529,
        "neg_line": [
            "-raise TypeError(\"Input header_size must be a positive integer. Got {}.\".format(header_size))",
            "-with open(filename, 'r') as f:"
        ],
        "pos_line": [
            "+raise TypeError(f\"Input header_size must be a positive integer. Got {header_size}.\")",
            "+with open(filename) as f:"
        ],
        "core_change": "-raise TypeError(\"Input header_size must be a positive integer. Got {}.\".format(header_size)) +raise TypeError(f\"Input header_size must be a positive integer. Got {header_size}.\") -with open(filename, 'r') as f: +with open(filename) as f:",
        "core_API": "isfile"
    },
    {
        "commit_hash": "98487896ad378a5c7034bc5753f7c748dfeca399",
        "index": "66ec4a6f30..6dc2b60708 100644",
        "commit_message": "small fixes for remainder in all backends, when modulus=False.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def remainder(",
            "res_floored = torch.where(res >= 0, torch.floor(res), torch.ceil(res))",
            "diff = res - res_floored",
            "diff, x2 = ivy.promote_types_of_inputs(diff, x2)",
            "-        return torch.mul(diff, x2, out=out).to(x1.dtype)",
            "+        return torch.round(torch.mul(diff, x2, out=out), out=out).to(x1.dtype)",
            "return torch.remainder(x1, x2, out=out)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=338576)",
            "Update(target_node=ASTNode(type=identifier, text=mul), value='round')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=338577)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=338578)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=338579)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=338580)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=338581)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=338582)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'out'), position=0, insert_id=338583)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=338584)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'out'), position=2, insert_id=338585)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=338586)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=338587)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'mul'), position=2, insert_id=338588)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 1532,
        "neg_line": [
            "-return torch.mul(diff, x2, out=out).to(x1.dtype)"
        ],
        "pos_line": [
            "+return torch.round(torch.mul(diff, x2, out=out), out=out).to(x1.dtype)"
        ],
        "core_change": "-return torch.mul(diff, x2, out=out).to(x1.dtype) +return torch.round(torch.mul(diff, x2, out=out), out=out).to(x1.dtype)",
        "core_API": "where"
    },
    {
        "commit_hash": "53ed4b0c7beed94f5caa176facb9a7ad151981ba",
        "index": "4939686c..c7f63b51 100644",
        "commit_message": "Add test-cuda make target, and fix cuda bugs (#338)\n\n* Add test-cuda make target; fix cuda bugs\n\n* Fix more cuda bugs; distributions now passes\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class NormalChol(Distribution):",
            "mu, L = self._sanitize_input(mu, L)",
            "ll_1 = Variable(torch.Tensor([-0.5 * mu.size(0) * np.log(2.0 * np.pi)])",
            ".type_as(mu.data))",
            "+        if L.dim() > 2:",
            "+            raise NotImplementedError(\"torch.diag() does not support tesors of dim > 2\")",
            "ll_2 = -torch.sum(torch.log(torch.diag(L)))",
            "# torch.trtrs() does not support cuda tensors.",
            "x_chols = torch.trtrs((x - mu).unsqueeze(1).data.cpu(), L.data.cpu(), False)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=761816)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=761817)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=761818)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=761819)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=761820)",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=0, insert_id=761821)",
            "Insert(target_node=IN(type=comparison_operator), node=('>', '>'), position=1, insert_id=761822)",
            "Insert(target_node=IN(type=comparison_operator), node=('integer', '2'), position=2, insert_id=761823)",
            "Insert(target_node=IN(type=block), node=('raise_statement', None), position=0, insert_id=761824)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=761825)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=761826)",
            "Insert(target_node=IN(type=raise_statement), node=('raise', 'raise'), position=0, insert_id=761827)",
            "Insert(target_node=IN(type=raise_statement), node=('call', None), position=1, insert_id=761828)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'L'), position=0, insert_id=761829)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=761830)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dim'), position=2, insert_id=761831)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=761832)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=761833)",
            "Insert(target_node=IN(type=call), node=('identifier', 'NotImplementedError'), position=0, insert_id=761834)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=761835)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=761836)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"torch.diag() does not support tesors of dim > 2\"'), position=1, insert_id=761837)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=761838)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 23,
        "number": 1533,
        "neg_line": [],
        "pos_line": [
            "+if L.dim() > 2:",
            "+raise NotImplementedError(\"torch.diag() does not support tesors of dim > 2\")"
        ],
        "core_change": "+if L.dim() > 2: +raise NotImplementedError(\"torch.diag() does not support tesors of dim > 2\")",
        "core_API": "_sanitize_input"
    },
    {
        "commit_hash": "1a344a22aee70a4066bd10c161d59bb34bbd1f3d",
        "index": "c60bf95f..0e160a5e 100644",
        "commit_message": "fixed examples that broke due to merge\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def guide(observed_data):",
            "",
            "# do variational inference using KL_QP",
            "print(\"doing inference with simulated data\")",
            "-verbose = False",
            "+verbose = True",
            "n_steps = 3001",
            "kl_optim = KL_QP(model, guide, pyro.optim(optim.Adam, {\"lr\": 0.003, \"betas\": (0.93, 0.993)}))",
            "for step in range(n_steps):",
            "loss = kl_optim.step(observed_data)",
            "if step % 100 == 0:",
            "if verbose:",
            "-            print(\"[epoch %d] mean_mu: %.3f\" % (step, pyro.param(\"mean_mu\").data[0, 0]))",
            "+            print(\"[epoch %d] mean_mu: %.3f\" % (step, pyro.param(\"mean_mu\").data[0]))",
            "print(\"[epoch %d] sigma_mu: %.3f\" % (step,",
            "-                                                 torch.exp(pyro.param(\"log_sigma_mu\")).data[0, 0]))",
            "+                                                 torch.exp(pyro.param(\"log_sigma_mu\")).data[0]))",
            "else:",
            "print(\".\", end='')",
            "sys.stdout.flush()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('true', 'True'), position=2, insert_id=770141)",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 1535,
        "neg_line": [
            "-verbose = False",
            "-print(\"[epoch %d] mean_mu: %.3f\" % (step, pyro.param(\"mean_mu\").data[0, 0]))",
            "-torch.exp(pyro.param(\"log_sigma_mu\")).data[0, 0]))"
        ],
        "pos_line": [
            "+verbose = True",
            "+print(\"[epoch %d] mean_mu: %.3f\" % (step, pyro.param(\"mean_mu\").data[0]))",
            "+torch.exp(pyro.param(\"log_sigma_mu\")).data[0]))"
        ],
        "core_change": "-verbose = False +verbose = True -print(\"[epoch %d] mean_mu: %.3f\" % (step, pyro.param(\"mean_mu\").data[0, 0])) +print(\"[epoch %d] mean_mu: %.3f\" % (step, pyro.param(\"mean_mu\").data[0])) -torch.exp(pyro.param(\"log_sigma_mu\")).data[0, 0])) +torch.exp(pyro.param(\"log_sigma_mu\")).data[0]))",
        "core_API": "optim"
    },
    {
        "commit_hash": "4e9b453854c9ff92fc4a7ad990b7995279313fda",
        "index": "375a84da2..d70779adf 100644",
        "commit_message": "[Fix] Move init dist connection into the setup function (#6506)\n\n* Move connection setup into the setup function. Call setup hook after we set up the accelerator\n\n* Added CHANGELOG.md\n\n* fix setup order in callback test\n\n* fix input arguments in test\n\n* Mock distributed function, remove protection to turn into training type hook\n\n* Remove import\n\n* Add missing mock, ensure custom plugin does not create children process\n\n* Skip test on windows\n\n* Update deepspeed to init connection in setup\n\n* Do not initialize distributed module\n\n* Move DeepSpeed tests to special tests since dist communication is being set up\n\n* Special the test to see if this fixes CI\n\n* Delete accelerator connector test to see if its causing build to fail\n\n* Delete deepspeed test\n\n* Revert \"Delete accelerator connector test to see if its causing build to fail\"\n\nThis reverts commit edde60b8\n\n* Revert \"Delete deepspeed test\"\n\nThis reverts commit 9d317429\n\n* Reverse hook\n\n* Reverse setup hooks to debug again\n\n* Add todo so i know where i left off\n\n* For single device move in pre_dispatch after setup function\n\n* Add additional model to device hook if any additional parameters have been set\n\n* See if we can enable deepspeed tests\n\n* Revert \"See if we can enable deepspeed tests\"\n\nThis reverts commit b5450def\n\n* See if this hook approach works\n\n* Introduce new granular hooks\n\n* Remove import, fix tpu spawn by moving the function to setup\n\n* Added missing special test\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class SingleDevicePlugin(TrainingTypePlugin):",
            "",
            "self._model.to(self.root_device)",
            "",
            "-    def connect(self, model: torch.nn.Module) -> torch.nn.Module:",
            "-        self._model = model",
            "+    def setup(self, model: torch.nn.Module) -> torch.nn.Module:",
            "self.model_to_device()",
            "return self.model"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=connect), value='setup')",
            "Insert(target_node=ASTNode(type=function_definition), node=('block', ''), position=6, insert_id=1413453)",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=_model))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=model))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 1536,
        "neg_line": [
            "-def connect(self, model: torch.nn.Module) -> torch.nn.Module:",
            "-self._model = model"
        ],
        "pos_line": [
            "+def setup(self, model: torch.nn.Module) -> torch.nn.Module:"
        ],
        "core_change": "-def connect(self, model: torch.nn.Module) -> torch.nn.Module: -self._model = model +def setup(self, model: torch.nn.Module) -> torch.nn.Module:",
        "core_API": "to"
    },
    {
        "commit_hash": "f69206c8624c62fb3f89f92fd1448fde05afa966",
        "index": "aeceb486..97aa1d5a 100644",
        "commit_message": "fix adaptive softmax indexing\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AdaptiveSoftmax(nn.Module):",
            "",
            "head_sz = self.cutoff[0] + len(self.tail)",
            "log_probs[:, :head_sz] = self.lsm(head_y)",
            "-        tail_priors = log_probs[:, self.cutoff[0] - 1: head_sz - 1].clone()",
            "+        tail_priors = log_probs[:, self.cutoff[0] - self.buggy_offset: head_sz - self.buggy_offset].clone()",
            "",
            "for i in range(len(self.tail)):",
            "start = self.cutoff[i]"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('attribute', None), position=2, insert_id=222960)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('attribute', None), position=2, insert_id=222961)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=222962)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=222963)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'buggy_offset'), position=2, insert_id=222964)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=222965)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=222966)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'buggy_offset'), position=2, insert_id=222967)",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 1537,
        "neg_line": [
            "-tail_priors = log_probs[:, self.cutoff[0] - 1: head_sz - 1].clone()"
        ],
        "pos_line": [
            "+tail_priors = log_probs[:, self.cutoff[0] - self.buggy_offset: head_sz - self.buggy_offset].clone()"
        ],
        "core_change": "-tail_priors = log_probs[:, self.cutoff[0] - 1: head_sz - 1].clone() +tail_priors = log_probs[:, self.cutoff[0] - self.buggy_offset: head_sz - self.buggy_offset].clone()",
        "core_API": "lsm"
    },
    {
        "commit_hash": "cf4664e8858baeec2e84eaf2b0fcdcbb252a1ca0",
        "index": "4bae7b9f..f9d3402d 100644",
        "commit_message": "fix tests\n\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class UNet2DConditionModel(ModelMixin, ConfigMixin):",
            "# TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can",
            "# This would be a good case for the `match` statement (Python 3.10+)",
            "is_mps = sample.device.type == \"mps\"",
            "-            if torch.is_floating_point(timesteps):",
            "+            if isinstance(timestep, float):",
            "dtype = torch.float32 if is_mps else torch.float64",
            "else:",
            "dtype = torch.int32 if is_mps else torch.int64"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='isinstance')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=timesteps), value='timestep')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=94806)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'float'), position=3, insert_id=94807)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=is_floating_point))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1538,
        "neg_line": [
            "-if torch.is_floating_point(timesteps):"
        ],
        "pos_line": [
            "+if isinstance(timestep, float):"
        ],
        "core_change": "-if torch.is_floating_point(timesteps): +if isinstance(timestep, float):",
        "core_API": "is_floating_point"
    },
    {
        "commit_hash": "45d096e219379bd0b3d14bbe4633b09f818f988d",
        "index": "4e8106c..47c2666 100644",
        "commit_message": "cameras_from_opencv_projection device #1021\n\nSummary: Fix https://github.com/facebookresearch/pytorch3d/issues/1021 that cameras_from_opencv_projection always creates on CPU.\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D33508211\n\nfbshipit-source-id: fadebd45cacafd633af6a58094cf6f654529992c\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestCameraConversions(TestCaseMixin, unittest.TestCase):",
            "cameras_opencv_to_pytorch3d = cameras_from_opencv_projection(",
            "R, tvec, camera_matrix, image_size",
            ")",
            "+        self.assertEqual(cameras_opencv_to_pytorch3d.device, device)",
            "",
            "# project the 3D points with converted cameras to screen space.",
            "pts_proj_pytorch3d_screen = cameras_opencv_to_pytorch3d.transform_points_screen("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=915907)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=915908)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=915909)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=915910)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=915911)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=915912)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'assertEqual'), position=2, insert_id=915913)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=915914)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=915915)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=915916)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'device'), position=3, insert_id=915917)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=915918)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cameras_opencv_to_pytorch3d'), position=0, insert_id=915919)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=915920)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=915921)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 15,
        "number": 1539,
        "neg_line": [],
        "pos_line": [
            "+self.assertEqual(cameras_opencv_to_pytorch3d.device, device)"
        ],
        "core_change": "+self.assertEqual(cameras_opencv_to_pytorch3d.device, device)",
        "core_API": "assertEqual"
    },
    {
        "commit_hash": "a343c42f17510cdb5c6b47a33a20d9ac7b095ccd",
        "index": "4cc3044..2ccf555 100644",
        "commit_message": "Fix typo\n\n",
        "file": "CapsNet-Tensorflow.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class CapsNet(object):",
            "# Method 2. masking with true label, default mode",
            "else:",
            "# self.masked_v = tf.matmul(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)), transpose_a=True)",
            "-                self.masked_v = tf.multply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)))",
            "+                self.masked_v = tf.multiply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)))",
            "self.v_length = tf.sqrt(tf.reduce_sum(tf.square(self.caps2), axis=2, keep_dims=True) + epsilon)",
            "",
            "# 2. Reconstructe the MNIST images with 3 FC layers"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=multply), value='multiply')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1540,
        "neg_line": [
            "-self.masked_v = tf.multply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)))"
        ],
        "pos_line": [
            "+self.masked_v = tf.multiply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)))"
        ],
        "core_change": "-self.masked_v = tf.multply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1))) +self.masked_v = tf.multiply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)))",
        "core_API": "matmul"
    },
    {
        "commit_hash": "58d9a2746493717a7c9252938da7efa6006f3739",
        "index": "fa33378..ca4137e 100644",
        "commit_message": "fix type of one hot indicators in vqvae.py\n\n`tf.one_hot` defaults to `tf.float32` if no dtype is provided. This prevents `VectorQuantizerEMA` from running if its dtype is specified as `tf.float64` at init time (since the EMA tries to subtract a `tf.float32` - the one hot indicator tensor - from a `tf.float64`). This is easily fixed by passing a dtype argument to `tf.one_hot`.\n",
        "file": "sonnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class VectorQuantizerEMA(base.Module):",
            "tf.reduce_sum(self.embeddings**2, 0, keepdims=True))",
            "",
            "encoding_indices = tf.argmax(-distances, 1)",
            "-    encodings = tf.one_hot(encoding_indices, self.num_embeddings)",
            "+    encodings = tf.one_hot(encoding_indices,",
            "+                           self.num_embeddings,",
            "+                           dtype=distances.dtype)",
            "",
            "# NB: if your code crashes with a reshape error on the line below about a",
            "# Tensor containing the wrong number of values, then the most likely cause"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2175063)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2175064)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2175065)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2175066)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2175067)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'distances'), position=0, insert_id=2175068)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2175069)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=2175070)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1541,
        "neg_line": [
            "-encodings = tf.one_hot(encoding_indices, self.num_embeddings)"
        ],
        "pos_line": [
            "+encodings = tf.one_hot(encoding_indices,",
            "+self.num_embeddings,",
            "+dtype=distances.dtype)"
        ],
        "core_change": "-encodings = tf.one_hot(encoding_indices, self.num_embeddings) +encodings = tf.one_hot(encoding_indices, +self.num_embeddings, +dtype=distances.dtype)",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "434f7b4b..5a3e611e 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_utilities(head_size):",
            "mask[head_size:, head_size:] = 0.",
            "mask.view(-1)[::size + 1][head_size:] = 1.",
            "arrowhead_full = mask * cov",
            "-    expected = torch.flip(torch.flip(arrowhead_full, (-2, -1)).cholesky(), (-2, -1))",
            "+    expected = torch.flip(",
            "+        torch.linalg.cholesky(torch.flip(arrowhead_full, (-2, -1))), (-2, -1)",
            "+    )",
            "# test if those flip ops give expected upper triangular values",
            "assert_close(expected.triu(), expected)",
            "assert_close(expected.matmul(expected.t()), arrowhead_full)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=677159)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=677160)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=677161)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=677162)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cholesky'), position=2, insert_id=677163)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=677164)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=677165)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=677166)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=677167)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=677168)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cholesky))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1543,
        "neg_line": [
            "-expected = torch.flip(torch.flip(arrowhead_full, (-2, -1)).cholesky(), (-2, -1))"
        ],
        "pos_line": [
            "+expected = torch.flip(",
            "+torch.linalg.cholesky(torch.flip(arrowhead_full, (-2, -1))), (-2, -1)",
            "+)"
        ],
        "core_change": "-expected = torch.flip(torch.flip(arrowhead_full, (-2, -1)).cholesky(), (-2, -1)) +expected = torch.flip( +torch.linalg.cholesky(torch.flip(arrowhead_full, (-2, -1))), (-2, -1) +)",
        "core_API": "view"
    },
    {
        "commit_hash": "b49d9ab7fa9b6ba82d4556fe8f9804b385ec74f1",
        "index": "a245d0f689..cc5da2bd8c 100644",
        "commit_message": "fix `set` positional and keyword arguments (#2891)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "unique_inverse.unsupported_dtypes = (\"float16\",)",
            "",
            "",
            "def unique_values(",
            "-    x: torch.Tensor, *, out: Optional[torch.Tensor] = None",
            "+    x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None",
            ") -> torch.Tensor:",
            "ret = torch.unique(x)",
            "return ret"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('positional_separator', None), position=3, insert_id=343131)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=4, insert_id=343132)",
            "Insert(target_node=IN(type=positional_separator), node=('/', '/'), position=0, insert_id=343133)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 1544,
        "neg_line": [
            "-x: torch.Tensor, *, out: Optional[torch.Tensor] = None"
        ],
        "pos_line": [
            "+x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None"
        ],
        "core_change": "-x: torch.Tensor, *, out: Optional[torch.Tensor] = None +x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None",
        "core_API": "unique"
    },
    {
        "commit_hash": "a43b1e94eeb552ad1d0353591695ee08448bff71",
        "index": "91774ab..3a33d20 100644",
        "commit_message": "Stability improvements (#99)\n\n* bugfix & torch fx implementation\n\n* update torch tensorrt installation\n\n* fix sparseml colab\n\n* fix sparseml training on gpu\n\n* restored lost fixes\n\n* fixed torch_tensorrt install\n\n* fix quantization\n\n* improved tests & bugfix\n\n* prevent deepsparse installation for arm cpu\n\n* bugfix tvm & improved tests\n\n* added tests for tensorflow and onnx\n\n* fix test torchscript\n\n* fix tensorrt static quant\n\n* update notebooks\n\n* fix deepsparse bugs and implemented tests\n\n* add test for sparseml compressor\n\n* bug fixes on tensorflow backend and added tests\n\n* add version limit to tensorflow due to protobuffers 2.x not being supported from tf2onnx\n\n* update onnx version\n\n* remove numpy update\n\n* restored cpu tests\n\n* add python 3.10 in cpu tests\n\n* limit tensorflow gpu usage\n\n* fix python 3.10\n\n* improved tests\n\n* add warmup in model latency computation & add original model latency\n\n* fix pytorch tensorrt for transformers models\n\n* fixed bugs on onnx model handling\n\n* added onnx simplifier to fix tensorrt in onnx pipeline\n\n* fix deepsparse support to NO_COMPILER_INSTALLATION flag\n\n* fix model to onnx conversion problem and tensorrt issue with static quantization\n\n* add valerio citation code\n\n* added readme to notebooks folder\n\n* added tensorflow and onnx notebooks\n\n* style fix\n\n* fix tensor RT bug with static quantization when using new version of polygraphy and update pytorch resnet50 notebook\n\n* fix huggingface bug when passing tokenizer to optimize_model\n\n* updated notebooks readme and bugfix\n\n* minor fixes & added pruning with intel neural compressor\n\n* fixes and added test for intel pruning\n\n* fixes & added neural compressor quantization\n\n* fix test intel pruning compressor\n\n* added tests for neural compressor optimizer & bug fixes\n\n* removed transformers from requirements\n\n* changed openvino dynamic shape\n\n* computing latency using different data\n\n* bugfix openvino\n\n* make onnxsim optional\n\n* install onnx_sim only on intel machines\n\n* added bf16 and dynamic_quantization to neural_compressor\n\n* check output of compiled models\n\n* internal fixes\n\n* fix when no optimized model is found\n\nCo-authored-by: Valerio Sofi <v.sofi@nebuly.ai>\n",
        "file": "nebullvm.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def run_torch_model(",
            "input_tensors: List[torch.Tensor],",
            "dtype: torch.dtype = torch.float32,",
            ") -> List[torch.Tensor]:",
            "+    torch_model.eval()",
            "if torch.cuda.is_available():",
            "torch_model.cuda()",
            "if dtype != torch.half:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=653719)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=653720)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=653721)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=653722)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=653723)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch_model'), position=0, insert_id=653724)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=653725)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'eval'), position=2, insert_id=653726)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=653727)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=653728)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 11,
        "number": 1545,
        "neg_line": [],
        "pos_line": [
            "+torch_model.eval()"
        ],
        "core_change": "+torch_model.eval()",
        "core_API": "eval"
    },
    {
        "commit_hash": "c4e66556a96bd2e737ed90b1c42202a1e16da949",
        "index": "5ee9a5c2b..df610af7f 100644",
        "commit_message": "minor bug fix\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ESPnetDiarizationModel(AbsESPnetModel):",
            "num_frames = np.sum(length)",
            "return (correct, num_frames, speech_scored, speech_miss, speech_falarm,",
            "speaker_scored, speaker_miss, speaker_falarm,",
            "-                speaker_error)",
            "\\ No newline at end of file",
            "+                speaker_error)"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Move(target_node=ASTNode(type=return_statement), node=ASTNode(type=tuple), position=1)",
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=tuple), position=15)",
            "Insert(target_node=ASTNode(type=tuple), node=(',', ','), position=2, insert_id=141869)",
            "Insert(target_node=ASTNode(type=tuple), node=('ERROR', None), position=4, insert_id=141870)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=ERROR, text= ), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=No), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=newline), position=2)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=at), position=3)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=end), position=4)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=of), position=5)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=file), position=6)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 1546,
        "neg_line": [
            "-speaker_error)"
        ],
        "pos_line": [
            "+speaker_error)"
        ],
        "core_change": "-speaker_error) +speaker_error)",
        "core_API": "sum"
    },
    {
        "commit_hash": "d215aa258e77f43b6c36950bc9a484d5d849eb4f",
        "index": "96860f88..470710e5 100644",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FreeAnchorRetinaHead(RetinaHead):",
            "box_cls_prob = torch.sparse.sum(",
            "object_cls_box_prob, dim=0).to_dense()",
            "",
            "-                indices = torch.nonzero(box_cls_prob).t_()",
            "+                indices = torch.nonzero(box_cls_prob, as_tuple=False).t_()",
            "if indices.numel() == 0:",
            "image_box_prob = torch.zeros(",
            "anchors_.size(0),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=638769)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=638770)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'as_tuple'), position=0, insert_id=638771)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=638772)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=638773)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1547,
        "neg_line": [
            "-indices = torch.nonzero(box_cls_prob).t_()"
        ],
        "pos_line": [
            "+indices = torch.nonzero(box_cls_prob, as_tuple=False).t_()"
        ],
        "core_change": "-indices = torch.nonzero(box_cls_prob).t_() +indices = torch.nonzero(box_cls_prob, as_tuple=False).t_()",
        "core_API": "sum"
    },
    {
        "commit_hash": "f7c93b3ceec341c3c794e9fc18939bc5d50b0fc2",
        "index": "0277d0034..fc97e63d4 100644",
        "commit_message": "Possible fix to make AMP work with DDP in the trainer (#4728)\n\n* manually set device in trainer args\n\n* check if current device is cuda before set_device\n\n* Explicitly set GPU ID when using single GPU\n\nThis addresses https://github.com/huggingface/transformers/issues/4657#issuecomment-642228099\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class TrainingArguments:",
            "torch.distributed.init_process_group(backend=\"nccl\")",
            "device = torch.device(\"cuda\", self.local_rank)",
            "n_gpu = 1",
            "+",
            "+        if device.type == \"cuda\":",
            "+            torch.cuda.set_device(device)",
            "+",
            "return device, n_gpu",
            "",
            "@property"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=1878364)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1878365)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1878366)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1878367)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1878368)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=1878369)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=1878370)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', '\"cuda\"'), position=2, insert_id=1878371)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1878372)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=0, insert_id=1878373)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1878374)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type'), position=2, insert_id=1878375)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1878376)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1878377)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1878378)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1878379)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1878380)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'set_device'), position=2, insert_id=1878381)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1878382)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'device'), position=1, insert_id=1878383)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1878384)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1878385)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1878386)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=1878387)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 24,
        "number": 1548,
        "neg_line": [],
        "pos_line": [
            "+",
            "+if device.type == \"cuda\":",
            "+torch.cuda.set_device(device)",
            "+"
        ],
        "core_change": "+ +if device.type == \"cuda\": +torch.cuda.set_device(device) +",
        "core_API": "init_process_group"
    },
    {
        "commit_hash": "b82fe7d258a621b953dcb3b78bd03da4fef70a44",
        "index": "3d59dc6fa..690757146 100644",
        "commit_message": "Replace strided slice with tf.expand_dims (#10078)\n\n* Replace tf.newaxis -> tf.expand_dims\n\n* Fix tests\n\n* Fix tests\n\n* Use reshape when a tensors needs a double expand\n\n* Fix GPT2\n\n* Fix GPT2\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFEmbeddings(tf.keras.layers.Layer):",
            "input_shape = shape_list(inputs_embeds)[:-1]",
            "",
            "if position_ids is None:",
            "-            position_ids = tf.range(start=0, limit=input_shape[-1])[tf.newaxis, :]",
            "+            position_ids = tf.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)",
            "",
            "position_embeds = tf.gather(params=self.position_embeddings, indices=position_ids)",
            "position_embeds = tf.tile(input=position_embeds, multiples=(input_shape[0], 1, 1))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=2371841)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2371842)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2371843)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2371844)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2371845)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'expand_dims'), position=2, insert_id=2371846)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2371847)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2371848)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2371849)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2371850)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='axis')",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=tf), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2371851)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '0'), position=2, insert_id=2371852)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=newaxis))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=slice))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 1549,
        "neg_line": [
            "-position_ids = tf.range(start=0, limit=input_shape[-1])[tf.newaxis, :]"
        ],
        "pos_line": [
            "+position_ids = tf.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)"
        ],
        "core_change": "-position_ids = tf.range(start=0, limit=input_shape[-1])[tf.newaxis, :] +position_ids = tf.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)",
        "core_API": "range"
    },
    {
        "commit_hash": "98d40fed3a4515077163adab9dfd8fb2fccf1267",
        "index": "403c67a73..b8c95934f 100644",
        "commit_message": "Cleanup the usage of `layer_norm_eps` in some models (#21336)\n\n* fix\n\n* fix\n\n* make style\n\n* For CLIP\n\n* For OwlViT\n\n* For XCLIP\n\n* For CLIPSeg\n\n* For GroupViT\n\n* fix docstrings\n\n* fix docstrings\n\n* For AltCLIP\n\n* For ChineseCLIP\n\n* For Blip\n\n* For GiT\n\n* make style\n\n* update\n\n* update\n\n* update\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class GitProjection(nn.Module):",
            "super().__init__()",
            "self.config = config",
            "self.visual_projection = nn.Sequential(",
            "-            nn.Linear(config.vision_config.hidden_size, config.hidden_size), nn.LayerNorm(config.hidden_size)",
            "+            nn.Linear(config.vision_config.hidden_size, config.hidden_size),",
            "+            nn.LayerNorm(config.hidden_size, eps=config.vision_config.layer_norm_eps),",
            ")",
            "",
            "def forward(self, embeddings: torch.Tensor) -> torch.Tensor:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1532801)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1532802)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1532803)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'eps'), position=0, insert_id=1532804)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1532805)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1532806)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1532807)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1532808)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'layer_norm_eps'), position=2, insert_id=1532809)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=0, insert_id=1532810)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1532811)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'vision_config'), position=2, insert_id=1532812)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 1552,
        "neg_line": [
            "-nn.Linear(config.vision_config.hidden_size, config.hidden_size), nn.LayerNorm(config.hidden_size)"
        ],
        "pos_line": [
            "+nn.Linear(config.vision_config.hidden_size, config.hidden_size),",
            "+nn.LayerNorm(config.hidden_size, eps=config.vision_config.layer_norm_eps),"
        ],
        "core_change": "-nn.Linear(config.vision_config.hidden_size, config.hidden_size), nn.LayerNorm(config.hidden_size) +nn.Linear(config.vision_config.hidden_size, config.hidden_size), +nn.LayerNorm(config.hidden_size, eps=config.vision_config.layer_norm_eps),",
        "core_API": "Sequential"
    },
    {
        "commit_hash": "7682e97702e4317231b3afe92359de384dba1e20",
        "index": "574d6daa4..5490b5d61 100755",
        "commit_message": "[models] respect dtype of the model when instantiating it (#12316)\n\n* [models] respect dtype of the model when instantiating it\n\n* cleanup\n\n* cleanup\n\n* rework to handle non-float dtype\n\n* fix\n\n* switch to fp32 tiny model\n\n* improve\n\n* use dtype.is_floating_point\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* fix the doc\n\n* recode to use explicit torch_dtype_auto_detect, torch_dtype args\n\n* docs and tweaks\n\n* docs and tweaks\n\n* docs and tweaks\n\n* merge 2 args, add docs\n\n* fix\n\n* fix\n\n* better doc\n\n* better doc\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PretrainedConfig(PushToHubMixin):",
            "self.output_hidden_states = kwargs.pop(\"output_hidden_states\", False)",
            "self.output_attentions = kwargs.pop(\"output_attentions\", False)",
            "self.torchscript = kwargs.pop(\"torchscript\", False)  # Only used by PyTorch models",
            "+        self.torch_dtype = kwargs.pop(\"torch_dtype\", None)  # Only used by PyTorch models",
            "self.use_bfloat16 = kwargs.pop(\"use_bfloat16\", False)",
            "self.pruned_heads = kwargs.pop(\"pruned_heads\", {})",
            "self.tie_word_embeddings = kwargs.pop("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1214187)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1214188)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=1214189)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1214190)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1214191)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1214192)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1214193)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch_dtype'), position=2, insert_id=1214194)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1214195)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1214196)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'kwargs'), position=0, insert_id=1214197)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1214198)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'pop'), position=2, insert_id=1214199)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1214200)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"torch_dtype\"'), position=1, insert_id=1214201)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1214202)",
            "Insert(target_node=IN(type=argument_list), node=('none', 'None'), position=3, insert_id=1214203)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1214204)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 1554,
        "neg_line": [],
        "pos_line": [
            "+self.torch_dtype = kwargs.pop(\"torch_dtype\", None)  # Only used by PyTorch models"
        ],
        "core_change": "+self.torch_dtype = kwargs.pop(\"torch_dtype\", None)  # Only used by PyTorch models",
        "core_API": "pop"
    },
    {
        "commit_hash": "9b0b579e876cd7d3ee115a8da94b5ffb182fcf3f",
        "index": "c4986c27..b223ae5a 100644",
        "commit_message": "Adds model configs to ludwig.datasets (#2540)\n\n* Adds README and stub for reading dataset configs.\n\n* Adds __init__.py for configs, moves circular import into function scope in ludwig/datasets/__init__.py\n\n* Print config files in datasets folder.\n\n* First pass at automatic archive extraction.\n\n* Implemented downloading and extract.\n\n* Refactor DatasetConfig into its own file.\n\n* Fixed bugs downloading kaggle dataset.\n\n* Makes registry store dataset instances, not classes. Also comments out import_submodules for testing.\n\n* Typo fix.\n\n* Only pass data files on to load_unprocessed_dataframe, symlink directories.\n\n* Downloading dataset files into existing directory if exists.\n\n* Refactor: make datasets fully config-first, lazy load dataset loaders.\n\n* Implemented agnews custom loader.\n\n* Implements train/validation/test split by files, and globbing support\n\n* Adds _glob_multiple\n\n* Adds adult_census_income, agnews, allstate_claims_severity.\n\n* Implements sha256 verification, adds more datasets up to creditcard_fraud.\n\n* Adds checksums, dbpedia, electricity\n\n* Fixes gzip file name returned as string not list, adds up to forest_cover dataset.\n\n* Adds datasets up to reuters_r8\n\n* Adds all datasets which don't require a custom class.\n\n* Restore dataset import behavior by implementing module __getattr__\n\n* Adds KDD datasets.\n\n* Adds ieee_fraud.\n\n* Adds imbalanced_insurance, insurance_lite.\n\n* Adds mnist.\n\n* Completes implementation of all of the built-in datasets.\n\n* Made cache_dir optional, read from environment variable if set.\n\n* Upgrades datasets tests.\n\n* Adds test for new dataset config API.  Also adds scripts for dataset link checking.\n\n* Fixes loading allstate claims severity dataset.\n\n* Use @lru_cache(1), @cache not supported in python < 3.9\n\n* Deletes dataset registry, updates automl test utils\n\n* Fix imports of datasets API.\n\n* Adds more detail to sha256: docstring and basic README\n\n* Copy-paste link oops.\n\n* Fixes handling of nested archive types like .tar.bz  Also adds a LUDWIG_CACHE and export to the README\n\n* Adds link for twitter bots.\n\n* Fix order of splits in README.md\n\n* typo\n\n* Adds verify as a phase in doc string.\n\n* Support .pqt, .pq extensions for parquet.\n\n* Handle nested archives with longer file extensions like .csv.zip\n\n* Handle nested .gz types properly too.  Check all extensions with .endswith\n\n* Handle all archive types with .endswith\n\n* Update ludwig/datasets/loaders/split_loaders.py\n\nCo-authored-by: Joppe Geluykens <joppe@rvrie.com>\n\n* Adds explanation for export, fixes preserve_paths (should be relative to processed_dataset_dir)\n\n* Resolve preserved paths relative to raw dataset dir before move.\n\n* Catch runtime exception from extracting sub-archives.\n\n* Started adding info to README about dataset model configs.\n\n* Adds method to get model configs for datasets.\n\n* Adds mnist, titanic examples as default configs.\n\n* Export dataset before training.\n\n* Adds multiprocessing version of train_all_model_configs, and adds a few configs mosttly from automl experiments.\n\n* Adds a few more configs, removes AG news, training is too slow.\n\n* Default to only 4 processes due to memory constraints.\n\n* Visualize learning curves.\n\n* Started documenting API functions in readme.\n\n* Adds test for model configs API, updates tests to mock protected _load_dataset_config\n\n* Clear dataset cache after testing with mock datasets.\n\n* Adds best configs, improved README\n\n* higgs_best consistent formatting.\n\n* Update ludwig/datasets/README.md\n\nCo-authored-by: abidwael <103003638+abidwael@users.noreply.github.com>\n\n* Adds model commit hash to results.\n\n* Adds MSE, MAE to metric list.\n\n* Don't printout ludwig commit.\n\n* Increase display width to show more columns in print output.\n\n* Fix error in get_commit_hash\n\nCo-authored-by: Daniel Treiman <daniel@predibase.com>\nCo-authored-by: Joppe Geluykens <joppe@rvrie.com>\nCo-authored-by: abidwael <103003638+abidwael@users.noreply.github.com>\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def test_multifile_join_dataset(tmpdir, f_type):",
            "assert output_df.shape[0] == train_df.shape[0] + test_df.shape[0] + val_df.shape[0]",
            "",
            "assert dataset.state == DatasetState.TRANSFORMED",
            "+    ludwig.datasets._get_dataset_configs.cache_clear()"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1796037)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1796038)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1796039)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1796040)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1796041)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1796042)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cache_clear'), position=2, insert_id=1796043)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1796044)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1796045)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1796046)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1796047)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_get_dataset_configs'), position=2, insert_id=1796048)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ludwig'), position=0, insert_id=1796049)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1796050)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'datasets'), position=2, insert_id=1796051)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 15,
        "number": 1555,
        "neg_line": [],
        "pos_line": [
            "+ludwig.datasets._get_dataset_configs.cache_clear()"
        ],
        "core_change": "+ludwig.datasets._get_dataset_configs.cache_clear()",
        "core_API": "cache_clear"
    },
    {
        "commit_hash": "de4159a318f9a4f2f474733306c00ac326e643a7",
        "index": "07ad3593a..93c4be762 100644",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFDistilBertForMultipleChoice(TFDistilBertPreTrainedModel, TFMultipleChoic",
            "Returns:",
            "tf.Tensor with dummy inputs",
            "\"\"\"",
            "-        return {\"input_ids\": tf.constant(MULTIPLE_CHOICE_DUMMY_INPUTS)}",
            "+        return {\"input_ids\": tf.constant(MULTIPLE_CHOICE_DUMMY_INPUTS, dtype=tf.int32)}",
            "",
            "@unpack_inputs",
            "@add_start_docstrings_to_model_forward("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2357920)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2357921)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2357922)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2357923)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2357924)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2357925)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2357926)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int32'), position=2, insert_id=2357927)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1556,
        "neg_line": [
            "-return {\"input_ids\": tf.constant(MULTIPLE_CHOICE_DUMMY_INPUTS)}"
        ],
        "pos_line": [
            "+return {\"input_ids\": tf.constant(MULTIPLE_CHOICE_DUMMY_INPUTS, dtype=tf.int32)}"
        ],
        "core_change": "-return {\"input_ids\": tf.constant(MULTIPLE_CHOICE_DUMMY_INPUTS)} +return {\"input_ids\": tf.constant(MULTIPLE_CHOICE_DUMMY_INPUTS, dtype=tf.int32)}",
        "core_API": "constant"
    },
    {
        "commit_hash": "c5b6ceea4a0a151ab78b9c065584bc416d1ee275",
        "index": "538fb349..d99370ce 100644",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class Layer_Flow_Control_Test(unittest.TestCase):",
            "network = tl.layers.ReshapeLayer(net_mux, shape=(-1, 800), name='reshape')",
            "network = tl.layers.DropoutLayer(network, keep=0.5, name='drop3')",
            "# output layer",
            "-        network = tl.layers.DenseLayer(network, n_units=10, act=tf.identity, name='output')",
            "+        network = tl.layers.DenseLayer(network, n_units=10, name='output')",
            "",
            "network.print_layers()",
            "network.print_params(False)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=act))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=identity))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1557,
        "neg_line": [
            "-network = tl.layers.DenseLayer(network, n_units=10, act=tf.identity, name='output')"
        ],
        "pos_line": [
            "+network = tl.layers.DenseLayer(network, n_units=10, name='output')"
        ],
        "core_change": "-network = tl.layers.DenseLayer(network, n_units=10, act=tf.identity, name='output') +network = tl.layers.DenseLayer(network, n_units=10, name='output')",
        "core_API": "ReshapeLayer"
    },
    {
        "commit_hash": "2e750071c25809f86ee68464ec4c3bfa9adceeb9",
        "index": "7e2059bb..472ec73d 100644",
        "commit_message": "Fix bug of nas fcos (#3205)\n\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class NASFCOSHead(FCOSHead):",
            "\"\"\"Initialize weights of the head.\"\"\"",
            "# retinanet_bias_init",
            "bias_cls = bias_init_with_prob(0.01)",
            "-        normal_init(self.fcos_reg, std=0.01)",
            "-        normal_init(self.fcos_centerness, std=0.01)",
            "-        normal_init(self.fcos_cls, std=0.01, bias=bias_cls)",
            "+        normal_init(self.conv_reg, std=0.01)",
            "+        normal_init(self.conv_centerness, std=0.01)",
            "+        normal_init(self.conv_cls, std=0.01, bias=bias_cls)",
            "",
            "for branch in [self.cls_convs, self.reg_convs]:",
            "for module in branch.modules():"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1426569)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=keyword_argument), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=fcos_reg), value='conv_reg')",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'std'), position=0, insert_id=1426570)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1426571)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '0.01'), position=2, insert_id=1426572)",
            "Update(target_node=ASTNode(type=identifier, text=fcos_centerness), value='conv_centerness')",
            "Update(target_node=ASTNode(type=identifier, text=fcos_cls), value='conv_cls')",
            "Delete(target_node=ASTNode(type=identifier, text=std))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=float, text=0.01))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 12,
        "number": 1558,
        "neg_line": [
            "-normal_init(self.fcos_reg, std=0.01)",
            "-normal_init(self.fcos_centerness, std=0.01)",
            "-normal_init(self.fcos_cls, std=0.01, bias=bias_cls)"
        ],
        "pos_line": [
            "+normal_init(self.conv_reg, std=0.01)",
            "+normal_init(self.conv_centerness, std=0.01)",
            "+normal_init(self.conv_cls, std=0.01, bias=bias_cls)"
        ],
        "core_change": "-normal_init(self.fcos_reg, std=0.01) -normal_init(self.fcos_centerness, std=0.01) -normal_init(self.fcos_cls, std=0.01, bias=bias_cls) +normal_init(self.conv_reg, std=0.01) +normal_init(self.conv_centerness, std=0.01) +normal_init(self.conv_cls, std=0.01, bias=bias_cls)",
        "core_API": "modules"
    },
    {
        "commit_hash": "d10cf19171595c1847bd7d3a8c975ab3c843160b",
        "index": "6eb2b001..0a4f041d 100644",
        "commit_message": "Fix hidden layer in GCN\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class GCNWithJK(torch.nn.Module):",
            "self.convs = torch.nn.ModuleList()",
            "for i in range(num_layers - 1):",
            "self.convs.append(GCNConv(hidden, hidden))",
            "-        self.lin1 = torch.nn.Linear(num_layers * hidden, hidden)",
            "+        self.lin1 = Linear(num_layers * hidden, hidden)",
            "self.lin2 = Linear(hidden, dataset.num_classes)",
            "self.mode = mode",
            "self.kwargs = kwargs"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=Linear), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 1559,
        "neg_line": [
            "-self.lin1 = torch.nn.Linear(num_layers * hidden, hidden)"
        ],
        "pos_line": [
            "+self.lin1 = Linear(num_layers * hidden, hidden)"
        ],
        "core_change": "-self.lin1 = torch.nn.Linear(num_layers * hidden, hidden) +self.lin1 = Linear(num_layers * hidden, hidden)",
        "core_API": "ModuleList"
    },
    {
        "commit_hash": "e1f5eacab98670bc1de72c88657404a15aadd527",
        "index": "af8cfa775..1d5398778 100644",
        "commit_message": "fix dp reduction test (#6404)\n\n* fix\n\n* update\n\n* fix\n\n* move the class outside\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DataParallelPlugin(ParallelPlugin):",
            "",
            "else:",
            "",
            "-            def _reduce(tensor: torch.Tensor):",
            "-                dtype_tensor = tensor.dtype",
            "-                return tensor.float().mean().type(dtype_tensor)",
            "+            def _reduce(t: torch.Tensor):",
            "+                dtype_tensor = t.dtype",
            "+                return t.float().mean().type(dtype_tensor)",
            "",
            "tensor = apply_to_collection(tensor, torch.Tensor, _reduce)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='t')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='t')",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=543858)",
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='t')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=543859)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=543860)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 10,
        "number": 1560,
        "neg_line": [
            "-def _reduce(tensor: torch.Tensor):",
            "-dtype_tensor = tensor.dtype",
            "-return tensor.float().mean().type(dtype_tensor)"
        ],
        "pos_line": [
            "+def _reduce(t: torch.Tensor):",
            "+dtype_tensor = t.dtype",
            "+return t.float().mean().type(dtype_tensor)"
        ],
        "core_change": "-def _reduce(tensor: torch.Tensor): -dtype_tensor = tensor.dtype -return tensor.float().mean().type(dtype_tensor) +def _reduce(t: torch.Tensor): +dtype_tensor = t.dtype +return t.float().mean().type(dtype_tensor)",
        "core_API": "float"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "7e6528db76..c377608d37 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class FastModel(TFModelV2):",
            "",
            "if not self._registered:",
            "self.register_variables(",
            "-                tf.get_collection(",
            "-                    tf.GraphKeys.TRAINABLE_VARIABLES, scope=\".+/model/.+\"))",
            "+                tf1.get_collection(",
            "+                    tf1.GraphKeys.TRAINABLE_VARIABLES, scope=\".+/model/.+\"))",
            "self._registered = True",
            "",
            "return output, []"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 1562,
        "neg_line": [
            "-tf.get_collection(",
            "-tf.GraphKeys.TRAINABLE_VARIABLES, scope=\".+/model/.+\"))"
        ],
        "pos_line": [
            "+tf1.get_collection(",
            "+tf1.GraphKeys.TRAINABLE_VARIABLES, scope=\".+/model/.+\"))"
        ],
        "core_change": "-tf.get_collection( -tf.GraphKeys.TRAINABLE_VARIABLES, scope=\".+/model/.+\")) +tf1.get_collection( +tf1.GraphKeys.TRAINABLE_VARIABLES, scope=\".+/model/.+\"))",
        "core_API": "register_variables"
    },
    {
        "commit_hash": "a539ddf05d4ba0815dcb28b2fff3ec96f578e3d4",
        "index": "df7a34dd..993cad3f 100644",
        "commit_message": "GH-482: fix CUDA error with CharacterEmbeddings\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CharacterEmbeddings(TokenEmbeddings):",
            "longest_token_in_sentence = max(chars2_length)",
            "tokens_mask = torch.zeros((len(tokens_sorted_by_length), longest_token_in_sentence),",
            "dtype=torch.long, device=flair.device)",
            "+",
            "for i, c in enumerate(tokens_sorted_by_length):",
            "-                tokens_mask[i, :chars2_length[i]] = c",
            "+                tokens_mask[i, :chars2_length[i]] = torch.tensor(c, dtype=torch.long, device=flair.device)",
            "",
            "# chars for rnn processing",
            "chars = tokens_mask"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=240602)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=240603)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=240604)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=240605)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=240606)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=240607)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=240608)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=c), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=240609)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=240610)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=240611)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=5, insert_id=240612)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=6, insert_id=240613)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=240614)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=240615)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=240616)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=240617)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=240618)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=240619)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=240620)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=240621)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'long'), position=2, insert_id=240622)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'flair'), position=0, insert_id=240623)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=240624)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=240625)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 1565,
        "neg_line": [
            "-tokens_mask[i, :chars2_length[i]] = c"
        ],
        "pos_line": [
            "+",
            "+tokens_mask[i, :chars2_length[i]] = torch.tensor(c, dtype=torch.long, device=flair.device)"
        ],
        "core_change": "+ -tokens_mask[i, :chars2_length[i]] = c +tokens_mask[i, :chars2_length[i]] = torch.tensor(c, dtype=torch.long, device=flair.device)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "d057d50f5a0d140f3a5d0e044ad167545b670ec2",
        "index": "40121c64..b1ecd03c 100644",
        "commit_message": "Sorting state fixes (#576)\n\n* add failing tests\n\n* add Variable to zeros, fix sorting for non-stateful encoders\n\n* clarify tests\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ElmoLstm(_EncoderBase):",
            "batch_size,",
            "sequence_length_difference,",
            "stacked_sequence_output[0].size(-1)).fill_(0)",
            "-            zeros = torch.autograd.Variable(zeros)",
            "+            zeros = Variable(zeros)",
            "stacked_sequence_output = torch.cat([stacked_sequence_output, zeros], 2)",
            "",
            "self._update_states(final_states, restoration_indices)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=Variable), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=autograd))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 1567,
        "neg_line": [
            "-zeros = torch.autograd.Variable(zeros)"
        ],
        "pos_line": [
            "+zeros = Variable(zeros)"
        ],
        "core_change": "-zeros = torch.autograd.Variable(zeros) +zeros = Variable(zeros)",
        "core_API": "Variable"
    },
    {
        "commit_hash": "b41cffaa93e8205bd8bd309f82c33c07c420eefd",
        "index": "7a87eb3..c51e61e 100644",
        "commit_message": "Fix a few issues loading pretrained vit/bit npz weights w/ num_classes=0 __init__ arg. Missed a few other small classifier handling detail on Mlp, GhostNet, Levit. Should fix #713\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class MlpMixer(nn.Module):",
            "act_layer=act_layer, drop=drop_rate, drop_path=drop_path_rate)",
            "for _ in range(num_blocks)])",
            "self.norm = norm_layer(embed_dim)",
            "-        self.head = nn.Linear(embed_dim, self.num_classes)  # zero init",
            "+        self.head = nn.Linear(embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()",
            "",
            "self.init_weights(nlhb=nlhb)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('conditional_expression', None), position=2, insert_id=1477800)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=1477801)",
            "Insert(target_node=IN(type=conditional_expression), node=('comparison_operator', None), position=2, insert_id=1477802)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=1477803)",
            "Insert(target_node=IN(type=conditional_expression), node=('call', None), position=4, insert_id=1477804)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'num_classes'), position=0, insert_id=1477805)",
            "Insert(target_node=IN(type=comparison_operator), node=('>', '>'), position=1, insert_id=1477806)",
            "Insert(target_node=IN(type=comparison_operator), node=('integer', '0'), position=2, insert_id=1477807)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1477808)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1477809)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1477810)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1477811)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Identity'), position=2, insert_id=1477812)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1477813)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1477814)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 1568,
        "neg_line": [
            "-self.head = nn.Linear(embed_dim, self.num_classes)  # zero init"
        ],
        "pos_line": [
            "+self.head = nn.Linear(embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()"
        ],
        "core_change": "-self.head = nn.Linear(embed_dim, self.num_classes)  # zero init +self.head = nn.Linear(embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()",
        "core_API": "Linear"
    },
    {
        "commit_hash": "20c23204341672414216225c84199f41cead5bcf",
        "index": "35d17d08..8afb09be 100644",
        "commit_message": "Fix for torch device (#3161)\n\n\n",
        "file": "haystack.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def initialize_device_settings(",
            "devices_to_use = [torch.device(device) for device in range(torch.cuda.device_count())]",
            "n_gpu = torch.cuda.device_count()",
            "else:",
            "-                devices_to_use = [torch.device(\"cuda\")]",
            "+                devices_to_use = [torch.device(\"cuda:0\")]",
            "n_gpu = 1",
            "else:",
            "devices_to_use = [torch.device(\"cpu\")]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"cuda\"), value='\"cuda:0\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1569,
        "neg_line": [
            "-devices_to_use = [torch.device(\"cuda\")]"
        ],
        "pos_line": [
            "+devices_to_use = [torch.device(\"cuda:0\")]"
        ],
        "core_change": "-devices_to_use = [torch.device(\"cuda\")] +devices_to_use = [torch.device(\"cuda:0\")]",
        "core_API": "device"
    },
    {
        "commit_hash": "b0aa1d45b6f103d26529629195929fd9591b3b31",
        "index": "cf42f6c0..4812a74f 100644",
        "commit_message": "Generalize T5 modules (#5166)\n\n* initial commit\n\n* general self attn\n\n* fixing bugs, adding tests, adding docs\n\n* updating other modules\n\n* refactor\n\n* bug fix\n\n* update changelog\n\n* fix shape\n\n* fix format\n\n* address feedback\n\n* small doc fix\n\n* Update allennlp/modules/transformer/transformer_stack.py\n\nCo-authored-by: Pete <petew@allenai.org>\n\n* remove old file\n\nCo-authored-by: epwalsh <epwalsh10@gmail.com>\nCo-authored-by: Pete <petew@allenai.org>\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_loading_from_pretrained(pretrained_model_name):",
            "torch.manual_seed(SEED)",
            "hf_output = pretrained_module(hidden_states, attention_mask=attention_mask_hf)",
            "",
            "-    assert torch.allclose(output[0], hf_output[0])",
            "+    assert torch.allclose(output.final_hidden_states, hf_output[0])",
            "",
            "",
            "def test_loading_partial_pretrained_weights():"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=2024)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=output), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2025)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'final_hidden_states'), position=2, insert_id=2026)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1570,
        "neg_line": [
            "-assert torch.allclose(output[0], hf_output[0])"
        ],
        "pos_line": [
            "+assert torch.allclose(output.final_hidden_states, hf_output[0])"
        ],
        "core_change": "-assert torch.allclose(output[0], hf_output[0]) +assert torch.allclose(output.final_hidden_states, hf_output[0])",
        "core_API": "manual_seed"
    },
    {
        "commit_hash": "a2860eeaf013eff83cc0dcd86691c64add2d5df0",
        "index": "c63a02eb96..dcc1832faa 100644",
        "commit_message": "fix format failures\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "multiprocessing = (",
            "indices_where = tf.where",
            "",
            "",
            "-def shape(",
            "-    x: tf.Tensor, as_tensor: bool = False",
            "-) -> Union[tf.Tensor, List[int]]:",
            "+def shape(x: tf.Tensor, as_tensor: bool = False) -> Union[tf.Tensor, List[int]]:",
            "if as_tensor:",
            "return tf.shape(x)",
            "else:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1571,
        "neg_line": [
            "-def shape(",
            "-x: tf.Tensor, as_tensor: bool = False",
            "-) -> Union[tf.Tensor, List[int]]:"
        ],
        "pos_line": [
            "+def shape(x: tf.Tensor, as_tensor: bool = False) -> Union[tf.Tensor, List[int]]:"
        ],
        "core_change": "-def shape( -x: tf.Tensor, as_tensor: bool = False -) -> Union[tf.Tensor, List[int]]: +def shape(x: tf.Tensor, as_tensor: bool = False) -> Union[tf.Tensor, List[int]]:",
        "core_API": "shape"
    },
    {
        "commit_hash": "1ea3f44f06694aad184644f9b73135abc3eb5e29",
        "index": "2ab59420..638c6d11 100644",
        "commit_message": "Fix dtype consistency issue in random_binomial\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def random_binomial(shape, p=0.0, dtype=_FLOATX, seed=None):",
            "if seed is None:",
            "seed = np.random.randint(10e6)",
            "return tf.select(tf.random_uniform(shape, dtype=dtype, seed=seed) <= p,",
            "-                     tf.ones(shape), tf.zeros(shape))",
            "+                     tf.ones(shape, dtype=dtype),",
            "+                     tf.zeros(shape, dtype=dtype))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2117631)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2117632)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2117633)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2117634)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2117635)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2117636)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=2, insert_id=2117637)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2117638)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2117639)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=2, insert_id=2117640)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 1572,
        "neg_line": [
            "-tf.ones(shape), tf.zeros(shape))"
        ],
        "pos_line": [
            "+tf.ones(shape, dtype=dtype),",
            "+tf.zeros(shape, dtype=dtype))"
        ],
        "core_change": "-tf.ones(shape), tf.zeros(shape)) +tf.ones(shape, dtype=dtype), +tf.zeros(shape, dtype=dtype))",
        "core_API": "randint"
    },
    {
        "commit_hash": "5e1a9553fbed73995c9b81e63ba41cc70fdf89de",
        "index": "8633511..af95b3d 100644",
        "commit_message": "AutoBatch `cudnn.benchmark=True` fix (#9448)\n\n* AutoBatch `cudnn.benchmark=True` fix\n\nMay resolve https://github.com/ultralytics/yolov5/issues/9287\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update autobatch.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update autobatch.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update general.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def init_seeds(seed=0, deterministic=False):",
            "torch.manual_seed(seed)",
            "torch.cuda.manual_seed(seed)",
            "torch.cuda.manual_seed_all(seed)  # for Multi-GPU, exception safe",
            "-    torch.backends.cudnn.benchmark = True  # for faster training",
            "+    # torch.backends.cudnn.benchmark = True  # AutoBatch problem https://github.com/ultralytics/yolov5/issues/9287",
            "if deterministic and check_version(torch.__version__, '1.12.0'):  # https://github.com/ultralytics/yolov5/pull/8213",
            "torch.use_deterministic_algorithms(True)",
            "torch.backends.cudnn.deterministic = True"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=backends))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cudnn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=benchmark))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 1576,
        "neg_line": [
            "-torch.backends.cudnn.benchmark = True  # for faster training"
        ],
        "pos_line": [
            "+# torch.backends.cudnn.benchmark = True  # AutoBatch problem https://github.com/ultralytics/yolov5/issues/9287"
        ],
        "core_change": "-torch.backends.cudnn.benchmark = True  # for faster training +# torch.backends.cudnn.benchmark = True  # AutoBatch problem https://github.com/ultralytics/yolov5/issues/9287",
        "core_API": "manual_seed"
    },
    {
        "commit_hash": "975162471b4f1d2965f00def191704dfb2521582",
        "index": "83cea45..ee551a2 100644",
        "commit_message": "fix @tf.function for fastspeech\n\n",
        "file": "TensorFlowTTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFFastSpeechLengthRegulator(tf.keras.layers.Layer):",
            "[i, batch_size, outputs, encoder_masks, encoder_hidden_states, durations_gt, max_durations],",
            "shape_invariants=[i.get_shape(),",
            "batch_size.get_shape(),",
            "-                              tf.TensorShape([None, None, None]),",
            "+                              tf.TensorShape([None, None, self.config.hidden_size]),",
            "tf.TensorShape([None, None]),",
            "encoder_hidden_states.get_shape(),",
            "durations_gt.get_shape(),"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('attribute', None), position=5, insert_id=2217095)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2217096)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2217097)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'hidden_size'), position=2, insert_id=2217098)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2217099)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2217100)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=2, insert_id=2217101)",
            "Delete(target_node=ASTNode(type=none, text=None))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1578,
        "neg_line": [
            "-tf.TensorShape([None, None, None]),"
        ],
        "pos_line": [
            "+tf.TensorShape([None, None, self.config.hidden_size]),"
        ],
        "core_change": "-tf.TensorShape([None, None, None]), +tf.TensorShape([None, None, self.config.hidden_size]),",
        "core_API": "get_shape"
    },
    {
        "commit_hash": "1c4451469486a247e3f3fb50e1da45d1fb2c9edd",
        "index": "a7bb6ec17a..e2314cfe9f 100644",
        "commit_message": "Fixed failing test for linalg inv and updated array_values helper with casting inside _zeroing_and_casting (#4383)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def inv(",
            "*,",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    if tf.math.reduce_any(tf.linalg.det(x) == 0):",
            "+    if tf.math.reduce_any(tf.linalg.det(tf.cast(x, dtype=\"float64\")) == 0):",
            "ret = x",
            "else:",
            "ret = tf.linalg.inv(x)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=1993865)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=1993866)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1993867)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1993868)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1993869)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1993870)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=2, insert_id=1993871)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1993872)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=x), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1993873)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1993874)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1993875)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1993876)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"float64\"'), position=2, insert_id=1993877)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 1579,
        "neg_line": [
            "-if tf.math.reduce_any(tf.linalg.det(x) == 0):"
        ],
        "pos_line": [
            "+if tf.math.reduce_any(tf.linalg.det(tf.cast(x, dtype=\"float64\")) == 0):"
        ],
        "core_change": "-if tf.math.reduce_any(tf.linalg.det(x) == 0): +if tf.math.reduce_any(tf.linalg.det(tf.cast(x, dtype=\"float64\")) == 0):",
        "core_API": "reduce_any"
    },
    {
        "commit_hash": "015f7780f4466d1528c94bf0905ba4b6eb0e9917",
        "index": "976577fd..50942a7f 100644",
        "commit_message": "Decoder shape comments for Tacotron2, decoupled grad clip for stopnet and the rest of the network. Some variable renaming and bug fix for alignment score logging\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main(args):  # pylint: disable=redefined-outer-name",
            "criterion = nn.L1Loss() if c.model in [\"Tacotron\", \"TacotronGST\"",
            "] else nn.MSELoss()",
            "criterion_st = nn.BCEWithLogitsLoss(",
            "-        pos_weight=torch.tensor(20.0)) if c.stopnet else None",
            "+        pos_weight=torch.tensor(10)) if c.stopnet else None",
            "",
            "if args.restore_path:",
            "checkpoint = torch.load(args.restore_path)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '10'), position=1, insert_id=1271377)",
            "Delete(target_node=ASTNode(type=float, text=20.0))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1580,
        "neg_line": [
            "-pos_weight=torch.tensor(20.0)) if c.stopnet else None"
        ],
        "pos_line": [
            "+pos_weight=torch.tensor(10)) if c.stopnet else None"
        ],
        "core_change": "-pos_weight=torch.tensor(20.0)) if c.stopnet else None +pos_weight=torch.tensor(10)) if c.stopnet else None",
        "core_API": "L1Loss"
    },
    {
        "commit_hash": "7613ea12f267143ceb70a9aeb45eb20aca086e3e",
        "index": "d438175c..88abc093 100644",
        "commit_message": "Fixed img2imgalt after inpainting update\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Script(scripts.Script):",
            "",
            "p.seed = p.seed + 1",
            "",
            "-            return sampler.sample_img2img(p, p.init_latent, noise_dt, conditioning, unconditional_conditioning)",
            "+            return sampler.sample_img2img(p, p.init_latent, noise_dt, conditioning, unconditional_conditioning, image_conditioning=p.image_conditioning)",
            "",
            "p.sample = sample_extra"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=10, insert_id=1138954)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=11, insert_id=1138955)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'image_conditioning'), position=0, insert_id=1138956)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1138957)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1138958)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'p'), position=0, insert_id=1138959)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1138960)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'image_conditioning'), position=2, insert_id=1138961)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1581,
        "neg_line": [
            "-return sampler.sample_img2img(p, p.init_latent, noise_dt, conditioning, unconditional_conditioning)"
        ],
        "pos_line": [
            "+return sampler.sample_img2img(p, p.init_latent, noise_dt, conditioning, unconditional_conditioning, image_conditioning=p.image_conditioning)"
        ],
        "core_change": "-return sampler.sample_img2img(p, p.init_latent, noise_dt, conditioning, unconditional_conditioning) +return sampler.sample_img2img(p, p.init_latent, noise_dt, conditioning, unconditional_conditioning, image_conditioning=p.image_conditioning)",
        "core_API": "sample_img2img"
    },
    {
        "commit_hash": "894f21daaab90a083d6f193426e551333d20a6bf",
        "index": "9ff5a723..e831911e 100755",
        "commit_message": "Use odd shape tensor to represent parameter data in partitioned state (#981)\n\n* use wierd shaped tensor to avoid silent failures when not registering externel params\n\n* fix typo\n\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Init(InsertPostInitMethodToModuleSubClasses):",
            "",
            "see_memory_usage(f'Before partitioning param {param.ds_id} {param.shape}',",
            "force=False)",
            "-            param.data = torch.ones(1).half().to(param.device)",
            "+            param.data = torch.ones(partitioned_param_data_shape).half().to(param.device)",
            "see_memory_usage(f'After partitioning param {param.ds_id} {param.shape}',",
            "force=False)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'partitioned_param_data_shape'), position=1, insert_id=83841)",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1582,
        "neg_line": [
            "-param.data = torch.ones(1).half().to(param.device)"
        ],
        "pos_line": [
            "+param.data = torch.ones(partitioned_param_data_shape).half().to(param.device)"
        ],
        "core_change": "-param.data = torch.ones(1).half().to(param.device) +param.data = torch.ones(partitioned_param_data_shape).half().to(param.device)",
        "core_API": "ones"
    },
    {
        "commit_hash": "8dd06739238514d29b78cd88ffe91ace36e29989",
        "index": "437ec89..cbcc3d6 100644",
        "commit_message": "normalization: fix variable_scope usage\n\n",
        "file": "tflearn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def batch_normalization(incoming, beta=0.0, gamma=1.0, epsilon=1e-5,",
            "",
            "# Variable Scope fix for older TF",
            "try:",
            "-        vscope = tf.variable_scope(scope, name=name, values=[incoming],",
            "+        vscope = tf.variable_scope(scope, default_name=name, values=[incoming],",
            "reuse=reuse)",
            "except Exception:",
            "vscope = tf.variable_op_scope([incoming], scope, name, reuse=reuse)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=name), value='default_name')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1583,
        "neg_line": [
            "-vscope = tf.variable_scope(scope, name=name, values=[incoming],"
        ],
        "pos_line": [
            "+vscope = tf.variable_scope(scope, default_name=name, values=[incoming],"
        ],
        "core_change": "-vscope = tf.variable_scope(scope, name=name, values=[incoming], +vscope = tf.variable_scope(scope, default_name=name, values=[incoming],",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "6ed9882ddb2b6249463c855dcca6860161d91f3e",
        "index": "281bc96df..386988c06 100644",
        "commit_message": "use functional interface for softmax in attention (#14198)\n\n* use functional interface instead of instantiating module and immediately calling it\n\n* fix torch.nn.functional to nn.functional. Thank you Stas!\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class IntSoftmax(nn.Module):",
            "",
            "def forward(self, x, scaling_factor):",
            "if not self.quant_mode:",
            "-            return nn.Softmax(dim=-1)(x), None",
            "+            return nn.functional.softmax(x, dim=-1), None",
            "",
            "x_int = x / scaling_factor"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1536540)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1536541)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'softmax'), position=2, insert_id=1536542)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'x'), position=1, insert_id=1536543)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1536544)",
            "Update(target_node=ASTNode(type=identifier, text=Softmax), value='functional')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 1584,
        "neg_line": [
            "-return nn.Softmax(dim=-1)(x), None"
        ],
        "pos_line": [
            "+return nn.functional.softmax(x, dim=-1), None"
        ],
        "core_change": "-return nn.Softmax(dim=-1)(x), None +return nn.functional.softmax(x, dim=-1), None",
        "core_API": "Softmax"
    },
    {
        "commit_hash": "42416945c1e36a5f1d4350ee9c1ae8b134cbe841",
        "index": "e6245a55..9de81468 100644",
        "commit_message": "Don't add keras phase callback if learning phase is fixed (#1205)\n\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def setup_keras_trainer(",
            "input,",
            "get_cost,",
            "lambda: optimizer)",
            "-    if len(keras.backend.learning_phase().consumers()) > 0:",
            "+    if isinstance(keras.backend.learning_phase(), tf.Tensor) and len(keras.backend.learning_phase().consumers()) > 0:",
            "# check if learning_phase is used in this model",
            "trainer.register_callback(KerasPhaseCallback(True))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=2275243)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=2275244)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=2275245)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=2)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=2275246)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2275247)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2275248)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=2275249)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2275250)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=2275251)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2275252)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2275253)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2275254)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2275255)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2275256)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=2275257)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2275258)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2275259)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'learning_phase'), position=2, insert_id=2275260)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2275261)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2275262)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'keras'), position=0, insert_id=2275263)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2275264)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'backend'), position=2, insert_id=2275265)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 1585,
        "neg_line": [
            "-if len(keras.backend.learning_phase().consumers()) > 0:"
        ],
        "pos_line": [
            "+if isinstance(keras.backend.learning_phase(), tf.Tensor) and len(keras.backend.learning_phase().consumers()) > 0:"
        ],
        "core_change": "-if len(keras.backend.learning_phase().consumers()) > 0: +if isinstance(keras.backend.learning_phase(), tf.Tensor) and len(keras.backend.learning_phase().consumers()) > 0:",
        "core_API": "learning_phase"
    },
    {
        "commit_hash": "fda742be45270e73664c2c4fec062b4f95e00b89",
        "index": "98d81ad72..42ac4dae3 100644",
        "commit_message": "Finished automatic Torch hooking prototype notebook (#149)\n\n* specify and connect to known_workers on client instantiation\n\n* extending torch hooking prototype to all tensor types\n\n* clean up notebook, add links for navigation\n\n* upstream pull\n\n* fixed some errors, torch.* local execution works again\n\n* finished experimental hooking notebooks\n\n* fixed float tensor init hook in master and retested\n\n* rearranging new notebook\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchService(BaseService):",
            "",
            "# FLOAT TENSOR FUNCTIONS",
            "def hook_float_tensor___init__(service_self):",
            "-        def new___init__(self, tensor, owner=service_self, *args, **kwargs):",
            "-            super(torch.FloatTensor, self).__init__(*args, **kwargs)",
            "-            self = owner.register_object(self, False)",
            "+        def new___init__(self, *args):",
            "+            super(torch.FloatTensor, self).__init__()",
            "+            self = service_self.register_object(self, False)",
            "",
            "torch.FloatTensor.__init__ = new___init__"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=owner), value='service_self')",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=owner))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=service_self))",
            "Delete(target_node=ASTNode(type=default_parameter))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=**, text=**))",
            "Delete(target_node=ASTNode(type=identifier, text=kwargs))",
            "Delete(target_node=ASTNode(type=dictionary_splat_pattern))",
            "Delete(target_node=ASTNode(type=*, text=*))",
            "Delete(target_node=ASTNode(type=identifier, text=args))",
            "Delete(target_node=ASTNode(type=list_splat))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=**, text=**))",
            "Delete(target_node=ASTNode(type=identifier, text=kwargs))",
            "Delete(target_node=ASTNode(type=dictionary_splat))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 19,
        "number": 1587,
        "neg_line": [
            "-def new___init__(self, tensor, owner=service_self, *args, **kwargs):",
            "-super(torch.FloatTensor, self).__init__(*args, **kwargs)",
            "-self = owner.register_object(self, False)"
        ],
        "pos_line": [
            "+def new___init__(self, *args):",
            "+super(torch.FloatTensor, self).__init__()",
            "+self = service_self.register_object(self, False)"
        ],
        "core_change": "-def new___init__(self, tensor, owner=service_self, *args, **kwargs): -super(torch.FloatTensor, self).__init__(*args, **kwargs) -self = owner.register_object(self, False) +def new___init__(self, *args): +super(torch.FloatTensor, self).__init__() +self = service_self.register_object(self, False)",
        "core_API": "register_object"
    },
    {
        "commit_hash": "2058c42df121ccb65e0c7a58e3776229ac85084c",
        "index": "912ead700..4a9adc432 100644",
        "commit_message": "fixed init\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Transformer(TTSInterface, torch.nn.Module):",
            "self._reset_parameters(args)",
            "",
            "def _reset_parameters(self, args):",
            "-        # alpha in scaled positional encoding init",
            "-        self.encoder.embed[-1].alpha.data = torch.tensor(args.initial_encoder_alpha)",
            "-        self.decoder.embed[-1].alpha.data = torch.tensor(args.initial_decoder_alpha)",
            "+        if self.use_scaled_pos_enc:",
            "+            # alpha in scaled positional encoding init",
            "+            self.encoder.embed[-1].alpha.data = torch.tensor(args.initial_encoder_alpha)",
            "+            self.decoder.embed[-1].alpha.data = torch.tensor(args.initial_decoder_alpha)",
            "",
            "if args.transformer_init == \"pytorch\":",
            "return"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('if_statement', None), position=0, insert_id=173664)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=173665)",
            "Insert(target_node=IN(type=if_statement), node=('attribute', None), position=1, insert_id=173666)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=173667)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=block), position=3)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=173668)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=173669)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'use_scaled_pos_enc'), position=2, insert_id=173670)"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 8,
        "number": 1590,
        "neg_line": [
            "-# alpha in scaled positional encoding init",
            "-self.encoder.embed[-1].alpha.data = torch.tensor(args.initial_encoder_alpha)",
            "-self.decoder.embed[-1].alpha.data = torch.tensor(args.initial_decoder_alpha)"
        ],
        "pos_line": [
            "+if self.use_scaled_pos_enc:",
            "+# alpha in scaled positional encoding init",
            "+self.encoder.embed[-1].alpha.data = torch.tensor(args.initial_encoder_alpha)",
            "+self.decoder.embed[-1].alpha.data = torch.tensor(args.initial_decoder_alpha)"
        ],
        "core_change": "-# alpha in scaled positional encoding init -self.encoder.embed[-1].alpha.data = torch.tensor(args.initial_encoder_alpha) -self.decoder.embed[-1].alpha.data = torch.tensor(args.initial_decoder_alpha) +if self.use_scaled_pos_enc: +# alpha in scaled positional encoding init +self.encoder.embed[-1].alpha.data = torch.tensor(args.initial_encoder_alpha) +self.decoder.embed[-1].alpha.data = torch.tensor(args.initial_decoder_alpha)",
        "core_API": "_reset_parameters"
    },
    {
        "commit_hash": "8c4e6dc61b1885ece3293e9634677ca8098efdd1",
        "index": "32aef4c12..470811d15 100644",
        "commit_message": "[Tune] [PBT] [Doc] Fix and clean up PBT examples (#29060)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def demo_gan(checkpoint_paths):",
            "img_list = []",
            "fixed_noise = torch.randn(64, nz, 1, 1)",
            "for path in checkpoint_paths:",
            "-        netG_path = os.path.join(path, \"checkpoint.pt\")",
            "+        checkpoint_dict = Checkpoint.from_directory(path).to_dict()",
            "loadedG = Generator()",
            "-        loadedG.load_state_dict(torch.load(netG_path)[\"netGmodel\"])",
            "+        loadedG.load_state_dict(checkpoint_dict[\"netGmodel\"])",
            "with torch.no_grad():",
            "fake = loadedG(fixed_noise).detach().cpu()",
            "img_list.append(vutils.make_grid(fake, padding=2, normalize=True))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=netG_path), value='checkpoint_dict')",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='checkpoint_dict')",
            "Move(target_node=ASTNode(type=subscript), node=ASTNode(type=identifier, text=torch), position=0)",
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=1106825)",
            "Update(target_node=ASTNode(type=identifier, text=join), value='to_dict')",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1106826)",
            "Update(target_node=ASTNode(type=identifier, text=os), value='Checkpoint')",
            "Update(target_node=ASTNode(type=identifier, text=path), value='from_directory')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1106827)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'path'), position=1, insert_id=1106828)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1106829)",
            "Delete(target_node=ASTNode(type=identifier, text=path))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=string, text=\"checkpoint.pt\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=load))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=netG_path))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 23,
        "number": 1591,
        "neg_line": [
            "-netG_path = os.path.join(path, \"checkpoint.pt\")",
            "-loadedG.load_state_dict(torch.load(netG_path)[\"netGmodel\"])"
        ],
        "pos_line": [
            "+checkpoint_dict = Checkpoint.from_directory(path).to_dict()",
            "+loadedG.load_state_dict(checkpoint_dict[\"netGmodel\"])"
        ],
        "core_change": "-netG_path = os.path.join(path, \"checkpoint.pt\") +checkpoint_dict = Checkpoint.from_directory(path).to_dict() -loadedG.load_state_dict(torch.load(netG_path)[\"netGmodel\"]) +loadedG.load_state_dict(checkpoint_dict[\"netGmodel\"])",
        "core_API": "randn"
    },
    {
        "commit_hash": "ac315b7a4a20f72bceedf9a43e9b673c83dd6bc1",
        "index": "f4454a20..b2b47762 100644",
        "commit_message": "fixed scatter_sum call\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PNAConv(MessagePassing):",
            "return y",
            "",
            "def aggregate(self, inputs, index, dim_size=None):",
            "-        D = get_degree(inputs, index, self.node_dim, dim_size)",
            "+        D = get_degree(inputs, index, 0, dim_size)",
            "",
            "# aggregators",
            "-        inputs = torch.cat([aggregator(inputs, index, dim=self.node_dim, dim_size=dim_size)",
            "+        inputs = torch.cat([aggregator(inputs, index, dim=0, dim_size=dim_size)",
            "for aggregator in self.aggregators], dim=-1)",
            "# scalers",
            "return torch.cat([scaler(inputs, D, self.avg_d) for scaler in self.scalers], dim=-1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '0'), position=5, insert_id=1017537)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('integer', '0'), position=2, insert_id=1017538)",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=node_dim))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=node_dim))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 1592,
        "neg_line": [
            "-D = get_degree(inputs, index, self.node_dim, dim_size)",
            "-inputs = torch.cat([aggregator(inputs, index, dim=self.node_dim, dim_size=dim_size)"
        ],
        "pos_line": [
            "+D = get_degree(inputs, index, 0, dim_size)",
            "+inputs = torch.cat([aggregator(inputs, index, dim=0, dim_size=dim_size)"
        ],
        "core_change": "-D = get_degree(inputs, index, self.node_dim, dim_size) +D = get_degree(inputs, index, 0, dim_size) -inputs = torch.cat([aggregator(inputs, index, dim=self.node_dim, dim_size=dim_size) +inputs = torch.cat([aggregator(inputs, index, dim=0, dim_size=dim_size)",
        "core_API": "cat"
    },
    {
        "commit_hash": "48a4b5113d1ecdae0319a297250f2273ea73de2d",
        "index": "7cfc3cd..19d6e53 100644",
        "commit_message": "Fix CUDA mode in VAE example\n\n",
        "file": "examples.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "def test(epoch):",
            "model.eval()",
            "test_loss = 0",
            "for data, _ in test_loader:",
            "+        if args.cuda:",
            "+            data = data.cuda()",
            "data = Variable(data, volatile=True)",
            "recon_batch, mu, logvar = model(data)",
            "test_loss += loss_function(recon_batch, data, mu, logvar).data[0]"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=1828657)",
            "Insert(target_node=IN(type=block), node=('if_statement', None), position=0, insert_id=1828658)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1828659)",
            "Insert(target_node=IN(type=if_statement), node=('attribute', None), position=1, insert_id=1828660)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1828661)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1828662)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'args'), position=0, insert_id=1828663)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1828664)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=1828665)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1828666)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1828667)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'data'), position=0, insert_id=1828668)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1828669)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1828670)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1828671)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1828672)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'data'), position=0, insert_id=1828673)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1828674)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=1828675)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1828676)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1828677)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 22,
        "number": 1593,
        "neg_line": [],
        "pos_line": [
            "+if args.cuda:",
            "+data = data.cuda()"
        ],
        "core_change": "+if args.cuda: +data = data.cuda()",
        "core_API": "eval"
    },
    {
        "commit_hash": "7ea781c9a030fb2d49574d8d324e562e58dd49fb",
        "index": "2eae7a1a0..d6ecf7947 100644",
        "commit_message": "CI fix3\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def to_float(c):",
            "def complex_norm(c: Union[torch.Tensor, ComplexTensor]) -> torch.Tensor:",
            "if not is_complex(c):",
            "raise TypeError(\"Input is not a complex tensor.\")",
            "-    return torch.sqrt((c.real**2 + c.imag**2).sum(dim=-1, keepdim=True) + EPS)",
            "+    return torch.sqrt((c.real ** 2 + c.imag ** 2).sum(dim=-1, keepdim=True) + EPS)",
            "",
            "",
            "def einsum(equation, *operands):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1595,
        "neg_line": [
            "-return torch.sqrt((c.real**2 + c.imag**2).sum(dim=-1, keepdim=True) + EPS)"
        ],
        "pos_line": [
            "+return torch.sqrt((c.real ** 2 + c.imag ** 2).sum(dim=-1, keepdim=True) + EPS)"
        ],
        "core_change": "-return torch.sqrt((c.real**2 + c.imag**2).sum(dim=-1, keepdim=True) + EPS) +return torch.sqrt((c.real ** 2 + c.imag ** 2).sum(dim=-1, keepdim=True) + EPS)",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "98cc35b6a8e53e15829ce64fd8da835db0c61da9",
        "index": "ea3afa77..4f4b5cfa 100755",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class FP16_Optimizer(DeepSpeedOptimizer):",
            "will call ``model.load_state_dict()`` before",
            "``fp16_optimizer_instance.load_state_dict()`` is called.",
            "Example::",
            "-            model = torch.nn.Linear(D_in, D_out).cuda().half()",
            "+            model = torch.nn.Linear(D_in, D_out).to(get_accelerator().device_name()).half()",
            "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)",
            "optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)",
            "..."
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1816336)",
            "Update(target_node=ASTNode(type=identifier, text=cuda), value='to')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1816337)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=1816338)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1816339)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1816340)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1816341)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=1816342)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1816343)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device_name'), position=2, insert_id=1816344)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1816345)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1816346)",
            "Insert(target_node=IN(type=call), node=('identifier', 'get_accelerator'), position=0, insert_id=1816347)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 1596,
        "neg_line": [
            "-model = torch.nn.Linear(D_in, D_out).cuda().half()"
        ],
        "pos_line": [
            "+model = torch.nn.Linear(D_in, D_out).to(get_accelerator().device_name()).half()"
        ],
        "core_change": "-model = torch.nn.Linear(D_in, D_out).cuda().half() +model = torch.nn.Linear(D_in, D_out).to(get_accelerator().device_name()).half()",
        "core_API": "load_state_dict"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "539e84e9c5..4171494027 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class VisionNetwork(Model):",
            "activation=activation,",
            "padding=\"valid\",",
            "name=\"fc1\")",
            "-            fc2 = tf.layers.conv2d(",
            "+            fc2 = tf1.layers.conv2d(",
            "fc1,",
            "num_outputs, [1, 1],",
            "activation=None,"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1597,
        "neg_line": [
            "-fc2 = tf.layers.conv2d("
        ],
        "pos_line": [
            "+fc2 = tf1.layers.conv2d("
        ],
        "core_change": "-fc2 = tf.layers.conv2d( +fc2 = tf1.layers.conv2d(",
        "core_API": "conv2d"
    },
    {
        "commit_hash": "2766dd1d6e84ec1fa45f5ca799e3b47676bd34b6",
        "index": "72e7d8d5..f465c638 100644",
        "commit_message": "Fix #813 - GlowTTS training  (#814)\n\n* Fix #813\n\n* Update glow_tts recipe\n\n* Fix glow-tts test\n\n* Linter fix\n\n* Run data dep init only in training\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GlowTTSLoss(torch.nn.Module):",
            "return_dict = {}",
            "# flow loss - neg log likelihood",
            "pz = torch.sum(scales) + 0.5 * torch.sum(torch.exp(-2 * scales) * (z - means) ** 2)",
            "-        log_mle = self.constant_factor + (pz - torch.sum(log_det)) / (torch.sum(y_lengths) * z.shape[1])",
            "+        log_mle = self.constant_factor + (pz - torch.sum(log_det)) / (torch.sum(y_lengths) * z.shape[2])",
            "# duration loss - MSE",
            "-        # loss_dur = torch.sum((o_dur_log - o_attn_dur)**2) / torch.sum(x_lengths)",
            "+        loss_dur = torch.sum((o_dur_log - o_attn_dur) ** 2) / torch.sum(x_lengths)",
            "# duration loss - huber loss",
            "-        loss_dur = torch.nn.functional.smooth_l1_loss(o_dur_log, o_attn_dur, reduction=\"sum\") / torch.sum(x_lengths)",
            "+        # loss_dur = torch.nn.functional.smooth_l1_loss(o_dur_log, o_attn_dur, reduction=\"sum\") / torch.sum(x_lengths)",
            "return_dict[\"loss\"] = log_mle + loss_dur",
            "return_dict[\"log_mle\"] = log_mle",
            "return_dict[\"loss_dur\"] = loss_dur"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=nn), value='sum')",
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=1255785)",
            "Insert(target_node=IN(type=binary_operator), node=('parenthesized_expression', None), position=0, insert_id=1255786)",
            "Insert(target_node=IN(type=binary_operator), node=('**', '**'), position=1, insert_id=1255787)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '2'), position=2, insert_id=1255788)",
            "Update(target_node=ASTNode(type=integer, text=1), value='2')",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1255789)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('binary_operator', None), position=1, insert_id=1255790)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1255791)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=o_dur_log), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=1255792)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=o_attn_dur), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=functional))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=smooth_l1_loss))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=reduction))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"sum\"))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 25,
        "number": 1599,
        "neg_line": [
            "-log_mle = self.constant_factor + (pz - torch.sum(log_det)) / (torch.sum(y_lengths) * z.shape[1])",
            "-# loss_dur = torch.sum((o_dur_log - o_attn_dur)**2) / torch.sum(x_lengths)",
            "-loss_dur = torch.nn.functional.smooth_l1_loss(o_dur_log, o_attn_dur, reduction=\"sum\") / torch.sum(x_lengths)"
        ],
        "pos_line": [
            "+log_mle = self.constant_factor + (pz - torch.sum(log_det)) / (torch.sum(y_lengths) * z.shape[2])",
            "+loss_dur = torch.sum((o_dur_log - o_attn_dur) ** 2) / torch.sum(x_lengths)",
            "+# loss_dur = torch.nn.functional.smooth_l1_loss(o_dur_log, o_attn_dur, reduction=\"sum\") / torch.sum(x_lengths)"
        ],
        "core_change": "-log_mle = self.constant_factor + (pz - torch.sum(log_det)) / (torch.sum(y_lengths) * z.shape[1]) +log_mle = self.constant_factor + (pz - torch.sum(log_det)) / (torch.sum(y_lengths) * z.shape[2]) -# loss_dur = torch.sum((o_dur_log - o_attn_dur)**2) / torch.sum(x_lengths) +loss_dur = torch.sum((o_dur_log - o_attn_dur) ** 2) / torch.sum(x_lengths) -loss_dur = torch.nn.functional.smooth_l1_loss(o_dur_log, o_attn_dur, reduction=\"sum\") / torch.sum(x_lengths) +# loss_dur = torch.nn.functional.smooth_l1_loss(o_dur_log, o_attn_dur, reduction=\"sum\") / torch.sum(x_lengths)",
        "core_API": "sum"
    },
    {
        "commit_hash": "3648cbc1a8caa528224750aa2120890fde626fdd",
        "index": "19d6e53..370ad57 100644",
        "commit_message": "vae: Fix `UserWarning` (#220)\n\n\n",
        "file": "examples.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "reconstruction_function.size_average = False",
            "",
            "",
            "def loss_function(recon_x, x, mu, logvar):",
            "-    BCE = reconstruction_function(recon_x, x)",
            "+    BCE = reconstruction_function(recon_x, x.view(-1, 784))",
            "",
            "# see Appendix B from VAE paper:",
            "# Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=1828581)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=1828582)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1828583)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1828584)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=x), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1828585)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'view'), position=2, insert_id=1828586)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1828587)",
            "Insert(target_node=IN(type=argument_list), node=('unary_operator', '-1'), position=1, insert_id=1828588)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1828589)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '784'), position=3, insert_id=1828590)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=4)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 1600,
        "neg_line": [
            "-BCE = reconstruction_function(recon_x, x)"
        ],
        "pos_line": [
            "+BCE = reconstruction_function(recon_x, x.view(-1, 784))"
        ],
        "core_change": "-BCE = reconstruction_function(recon_x, x) +BCE = reconstruction_function(recon_x, x.view(-1, 784))",
        "core_API": "view"
    },
    {
        "commit_hash": "ca0109bd6899dd222b4c690fbc99e895d96e31f6",
        "index": "8457e6f14..0e75eca95 100644",
        "commit_message": "`disable_ngram_loss` fix for prophetnet (#8554)\n\n* `disable_ngram_loss` fix for prophetnet\n\n* add changes documentation\n\n* fix _compute_loss to use mean reduction and -100 to masked tokens & remove unnecessary arguments\n\n* mean label smoothing loss\n\n* small refactor\n\n* fix test\n\nCo-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ProphetNetModelTester:",
            "decoder_attention_mask=decoder_attention_mask,",
            "labels=lm_labels,",
            ")",
            "-        self.parent.assertTrue(torch.allclose(result.loss, torch.tensor(128.2925, device=torch_device), atol=1e-3))",
            "+        self.parent.assertTrue(torch.allclose(result.loss, torch.tensor(4.5819, device=torch_device), atol=1e-3))",
            "",
            "expected_logit_slice = torch.tensor(",
            "[-0.1565, 0.0418, 0.1207, 0.0030, 0.0665, 0.0467, 0.0412], device=torch_device"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=128.2925), value='4.5819')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1602,
        "neg_line": [
            "-self.parent.assertTrue(torch.allclose(result.loss, torch.tensor(128.2925, device=torch_device), atol=1e-3))"
        ],
        "pos_line": [
            "+self.parent.assertTrue(torch.allclose(result.loss, torch.tensor(4.5819, device=torch_device), atol=1e-3))"
        ],
        "core_change": "-self.parent.assertTrue(torch.allclose(result.loss, torch.tensor(128.2925, device=torch_device), atol=1e-3)) +self.parent.assertTrue(torch.allclose(result.loss, torch.tensor(4.5819, device=torch_device), atol=1e-3))",
        "core_API": "assertTrue"
    },
    {
        "commit_hash": "c725511bfc14eb86daf6edefa0d257084aa24c85",
        "index": "4de2520..8810890 100644",
        "commit_message": "Refactor for simplification (#9054)\n\n* Refactor for simplification\n\n* cleanup\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def select_device(device='', batch_size=0, newline=True):",
            "assert torch.cuda.is_available() and torch.cuda.device_count() >= len(device.replace(',', '')), \\",
            "f\"Invalid CUDA '--device {device}' requested, use '--device cpu' or pass valid CUDA device(s)\"",
            "",
            "-    if not (cpu or mps) and torch.cuda.is_available():  # prefer GPU if available",
            "+    if not cpu and not mps and torch.cuda.is_available():  # prefer GPU if available",
            "devices = device.split(',') if device else '0'  # range(torch.cuda.device_count())  # i.e. 0,1,6,7",
            "n = len(devices)  # device count",
            "if n > 1 and batch_size > 0:  # check batch_size is divisible by device_count"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=boolean_operator), node=('boolean_operator', None), position=0, insert_id=1292529)",
            "Insert(target_node=IN(type=boolean_operator), node=('not_operator', None), position=0, insert_id=1292530)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1292531)",
            "Insert(target_node=IN(type=boolean_operator), node=('not_operator', None), position=2, insert_id=1292532)",
            "Move(target_node=IN(type=not_operator), node=ASTNode(type=not, text=not), position=0)",
            "Move(target_node=IN(type=not_operator), node=ASTNode(type=identifier, text=cpu), position=1)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=1292533)",
            "Move(target_node=IN(type=not_operator), node=ASTNode(type=identifier, text=mps), position=1)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=or, text=or))",
            "Delete(target_node=ASTNode(type=boolean_operator))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))",
            "Delete(target_node=ASTNode(type=not_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 1603,
        "neg_line": [
            "-if not (cpu or mps) and torch.cuda.is_available():  # prefer GPU if available"
        ],
        "pos_line": [
            "+if not cpu and not mps and torch.cuda.is_available():  # prefer GPU if available"
        ],
        "core_change": "-if not (cpu or mps) and torch.cuda.is_available():  # prefer GPU if available +if not cpu and not mps and torch.cuda.is_available():  # prefer GPU if available",
        "core_API": "is_available"
    },
    {
        "commit_hash": "642142556d8ecdea9beb86d7618b628b1803ab98",
        "index": "ee918f24..76a89e88 100644",
        "commit_message": "use commandline-supplied cuda device name instead of cuda:0 for safetensors PR that doesn't fix anything\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def read_state_dict(checkpoint_file, print_global_state=False, map_location=None",
            "if extension.lower() == \".safetensors\":",
            "device = map_location or shared.weight_load_location",
            "if device is None:",
            "-            device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"",
            "+            device = devices.get_cuda_device_string() if torch.cuda.is_available() else \"cpu\"",
            "pl_sd = safetensors.torch.load_file(checkpoint_file, device=device)",
            "else:",
            "pl_sd = torch.load(checkpoint_file, map_location=map_location or shared.weight_load_location)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=conditional_expression), node=('call', None), position=0, insert_id=1133709)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1133710)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1133711)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'devices'), position=0, insert_id=1133712)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1133713)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get_cuda_device_string'), position=2, insert_id=1133714)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1133715)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1133716)",
            "Delete(target_node=ASTNode(type=string, text=\"cuda:0\"))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 1604,
        "neg_line": [
            "-device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
        ],
        "pos_line": [
            "+device = devices.get_cuda_device_string() if torch.cuda.is_available() else \"cpu\""
        ],
        "core_change": "-device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" +device = devices.get_cuda_device_string() if torch.cuda.is_available() else \"cpu\"",
        "core_API": "lower"
    },
    {
        "commit_hash": "14f2beffc8ebbcd95df92119194b6fdb43f61498",
        "index": "a22307046..e33d7bb35 100644",
        "commit_message": "fix: allow excluding load_path argument for bert models (#1215)\n\n* fix: allow excluding load_path argument for bert_classifier\n\nfixes #1160\n\n* fix: allow excluding load_path argument for bert ranker, sequence tagger and squad\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class BertClassifierModel(LRScheduledTFModel):",
            "pretrained_bert = str(expand_path(pretrained_bert))",
            "",
            "if tf.train.checkpoint_exists(pretrained_bert) \\",
            "-                    and not tf.train.checkpoint_exists(str(self.load_path.resolve())):",
            "+                    and not (self.load_path and tf.train.checkpoint_exists(str(self.load_path.resolve()))):",
            "logger.info('[initializing model with Bert from {}]'.format(pretrained_bert))",
            "# Exclude optimizer and classification variables from saved variables",
            "var_list = self._get_saveable_variables("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=not_operator), node=('parenthesized_expression', None), position=1, insert_id=1922795)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1922796)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('boolean_operator', None), position=1, insert_id=1922797)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1922798)",
            "Insert(target_node=IN(type=boolean_operator), node=('attribute', None), position=0, insert_id=1922799)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1922800)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1922801)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1922802)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'load_path'), position=2, insert_id=1922803)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 1608,
        "neg_line": [
            "-and not tf.train.checkpoint_exists(str(self.load_path.resolve())):"
        ],
        "pos_line": [
            "+and not (self.load_path and tf.train.checkpoint_exists(str(self.load_path.resolve()))):"
        ],
        "core_change": "-and not tf.train.checkpoint_exists(str(self.load_path.resolve())): +and not (self.load_path and tf.train.checkpoint_exists(str(self.load_path.resolve()))):",
        "core_API": "checkpoint_exists"
    },
    {
        "commit_hash": "0c166dcfb8b9e9d79d013b7967a59dfb1ac4b804",
        "index": "e91f1647..408b733f 100644",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def extract_tensor_patches(",
            "See :class:`~kornia.contrib.ExtractTensorPatches` for details.",
            "\"\"\"",
            "if not torch.is_tensor(input):",
            "-        raise TypeError(\"Input input type is not a torch.Tensor. Got {}\"",
            "-                        .format(type(input)))",
            "+        raise TypeError(\"Input input type is not a torch.Tensor. Got {}\".format(type(input)))",
            "if not len(input.shape) == 4:",
            "-        raise ValueError(\"Invalid input shape, we expect BxCxHxW. Got: {}\"",
            "-                         .format(input.shape))",
            "+        raise ValueError(\"Invalid input shape, we expect BxCxHxW. Got: {}\".format(input.shape))",
            "",
            "if padding:",
            "pad_vert, pad_horz = _pair(padding)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 1609,
        "neg_line": [
            "-raise TypeError(\"Input input type is not a torch.Tensor. Got {}\"",
            "-.format(type(input)))",
            "-raise ValueError(\"Invalid input shape, we expect BxCxHxW. Got: {}\"",
            "-.format(input.shape))"
        ],
        "pos_line": [
            "+raise TypeError(\"Input input type is not a torch.Tensor. Got {}\".format(type(input)))",
            "+raise ValueError(\"Invalid input shape, we expect BxCxHxW. Got: {}\".format(input.shape))"
        ],
        "core_change": "-raise TypeError(\"Input input type is not a torch.Tensor. Got {}\" -.format(type(input))) +raise TypeError(\"Input input type is not a torch.Tensor. Got {}\".format(type(input))) -raise ValueError(\"Invalid input shape, we expect BxCxHxW. Got: {}\" -.format(input.shape)) +raise ValueError(\"Invalid input shape, we expect BxCxHxW. Got: {}\".format(input.shape))",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "543c4be18a5f2e32d7023e9d5e20c7dfe8cf9899",
        "index": "6a6b94b..1b49c57 100644",
        "commit_message": "Fix model loading: use custom scope (#2308)\n\n\n",
        "file": "horovod.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "def RemoteTrainer(estimator, metadata, keras_utils, run_id, dataset_idx):",
            "if LooseVersion(tf.__version__) < LooseVersion(\"2.0.0\"):",
            "model.load_weights(ckpt_file)",
            "else:",
            "-                        model = k.models.load_model(ckpt_file)",
            "+                        # needs to be deserialized in the with scope",
            "+                        with k.utils.custom_object_scope(custom_objects):",
            "+                            model = k.models.load_model(ckpt_file)",
            "serialized_model = keras_utils.serialize_model(model)",
            "else:",
            "with open(ckpt_file, 'rb') as f:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=4, insert_id=2655430)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=serialized_model), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Insert(target_node=ASTNode(type=assignment), node=('ERROR', None), position=2, insert_id=2655431)",
            "Insert(target_node=IN(type=ERROR), node=('call', None), position=0, insert_id=2655432)",
            "Insert(target_node=IN(type=ERROR), node=(':', ':'), position=1, insert_id=2655433)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2655434)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2655435)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2655436)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2655437)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'custom_object_scope'), position=2, insert_id=2655438)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2655439)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'custom_objects'), position=1, insert_id=2655440)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2655441)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'with'), position=0, insert_id=2655442)",
            "Insert(target_node=IN(type=attribute), node=('ERROR', None), position=1, insert_id=2655443)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=2, insert_id=2655444)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'utils'), position=3, insert_id=2655445)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'k'), position=0, insert_id=2655446)",
            "Delete(target_node=ASTNode(type=assignment))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 1611,
        "neg_line": [
            "-model = k.models.load_model(ckpt_file)"
        ],
        "pos_line": [
            "+# needs to be deserialized in the with scope",
            "+with k.utils.custom_object_scope(custom_objects):",
            "+model = k.models.load_model(ckpt_file)"
        ],
        "core_change": "-model = k.models.load_model(ckpt_file) +# needs to be deserialized in the with scope +with k.utils.custom_object_scope(custom_objects): +model = k.models.load_model(ckpt_file)",
        "core_API": "load_weights"
    },
    {
        "commit_hash": "e0bc5d62a15990bad1f31ea97994f84d2d3029c9",
        "index": "9f6b7f1..5fe2538 100644",
        "commit_message": "Merging in fused adam optimizer, additional DDP features tested in 18.10 (#60)\n\n* test passes\n\n* notes\n\n* Using C++-side flatten and unflatten functions\n\n* Adding csrc\n\n* Persistent synchronization event so it doesn't need to be created and destroyed each time\n\n* Interop with parameter flattening in SSD\n\n* Added deterministic option to imagenet main.py\n\n* Adding options to split gradient averaging and allreduce in pure fp32\n\n* Fixing allreduce_maybe_retain call\n\n* Fixing allreduce_fallback\n\n* Also sync active_i_buckets from rank 0\n\n* Making retain_allreduce_buffers compatible with/orthogonal to delay_allreduce=True|False\n\n* Correcting syntax error, now all seems to work with SSD\n\n* Optional cpp extension build\n\n* Add mixed precision adam optimizer (#59)\n\n* Add FusedAdam Optimizer to Apex that places all the math into a cuda kernel.\n\n* Added fixes to fused_adam to get it to work with network.\n\n* wip work on python interface for adam with options\n\n* fix dispatch for halfs, add python options to handle optional half gradients and params\n\n* cleanup, get rid of grid-stride loop\n\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "for i in range(10):",
            "if not info(\"model.a\", model.module.a, 2.):  passed = False",
            "if not info(\"model.b\", model.module.b, 1.):  passed = False",
            "# torch.cuda.nvtx.range_pop()",
            "+torch.cuda.cudart().cudaProfilerStop()",
            "",
            "print(\"passed = \", passed)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1813716)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1813717)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1813718)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1813719)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=1813720)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1813721)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cudaProfilerStop'), position=2, insert_id=1813722)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1813723)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1813724)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1813725)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1813726)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1813727)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1813728)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cudart'), position=2, insert_id=1813729)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1813730)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1813731)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1813732)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1813733)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=1813734)"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 19,
        "number": 1612,
        "neg_line": [],
        "pos_line": [
            "+torch.cuda.cudart().cudaProfilerStop()"
        ],
        "core_change": "+torch.cuda.cudart().cudaProfilerStop()",
        "core_API": "range_pop"
    },
    {
        "commit_hash": "ab59f308b18622421edc67048d3b9fbfde96a9f4",
        "index": "b75f0e695..4049e09af 100644",
        "commit_message": "Future 4/n: test & legacy in test/ folder (#13295)\n\n* move: legacy >> test/\n\n* move: tests >> test/\n\n* rename unittests\n\n* update CI\n\n* tests4pl\n\n* tests_pytorch\n\n* proxi\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* ci\n\n* link\n\n* cli\n\n* standalone\n\n* fixing\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* .\n\n* Apply suggestions from code review\n\nCo-authored-by: Akihiro Nitta <nitta@akihironitta.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* alone\n\n* test -> tests\n\n* Standalone fixes\n\n* ci\n\n* Update\n\n* More fixes\n\n* Fix coverage\n\n* Fix mypy\n\n* mypy\n\n* Empty-Commit\n\n* Fix\n\n* mypy just for pl\n\n* Fix standalone\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Akihiro Nitta <nitta@akihironitta.com>\nCo-authored-by: Carlos Mochol <carlossmocholi@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_lightning_cli_link_arguments(tmpdir):",
            "parser.link_arguments(\"data.batch_size\", \"model.init_args.batch_size\")",
            "parser.link_arguments(\"data.num_classes\", \"model.init_args.num_classes\", apply_on=\"instantiate\")",
            "",
            "-    cli_args[-1] = \"--model=tests.utilities.test_cli.BoringModelRequiredClasses\"",
            "+    cli_args[-1] = \"--model=tests_pytorch.utilities.test_cli.BoringModelRequiredClasses\"",
            "",
            "with mock.patch(\"sys.argv\", [\"any.py\"] + cli_args):",
            "cli = MyLightningCLI("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"--model=tests.utilities.test_cli.BoringModelRequiredClasses\"), value='\"--model=tests_pytorch.utilities.test_cli.BoringModelRequiredClasses\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1613,
        "neg_line": [
            "-cli_args[-1] = \"--model=tests.utilities.test_cli.BoringModelRequiredClasses\""
        ],
        "pos_line": [
            "+cli_args[-1] = \"--model=tests_pytorch.utilities.test_cli.BoringModelRequiredClasses\""
        ],
        "core_change": "-cli_args[-1] = \"--model=tests.utilities.test_cli.BoringModelRequiredClasses\" +cli_args[-1] = \"--model=tests_pytorch.utilities.test_cli.BoringModelRequiredClasses\"",
        "core_API": "link_arguments"
    },
    {
        "commit_hash": "ccfb72cc502cc3fe30e168fcd220791ee8e449a2",
        "index": "22e317a..4401c3e 100644",
        "commit_message": "small fix for iou3d\n\nSummary:\nA small numerical fix for IoU for 3D boxes, fixes GH #992\n\n* Adds a check for boxes with zero side areas (invalid boxes)\n* Fixes numerical issue when two boxes have coplanar sides\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D33195691\n\nfbshipit-source-id: 8a34b4d1f1e5ec2edb6d54143930da44bdde0906\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def box3d_overlap(",
            "",
            "_check_coplanar(boxes1, eps)",
            "_check_coplanar(boxes2, eps)",
            "+    _check_nonzero(boxes1, eps)",
            "+    _check_nonzero(boxes2, eps)",
            "",
            "# pyre-fixme[16]: `_box3d_overlap` has no attribute `apply`.",
            "vol, iou = _box3d_overlap.apply(boxes1, boxes2)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=tuple_pattern), node=ASTNode(type=ERROR), position=2)",
            "Insert(target_node=ASTNode(type=ERROR), node=('tuple_pattern', None), position=1, insert_id=916330)",
            "Insert(target_node=ASTNode(type=ERROR), node=('tuple_pattern', None), position=4, insert_id=916331)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', '_check_nonzero'), position=5, insert_id=916332)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', '_check_nonzero'), position=3, insert_id=916333)",
            "Insert(target_node=IN(type=tuple_pattern), node=('(', '('), position=0, insert_id=916334)",
            "Insert(target_node=IN(type=tuple_pattern), node=('identifier', 'boxes1'), position=1, insert_id=916335)",
            "Insert(target_node=IN(type=tuple_pattern), node=(',', ','), position=2, insert_id=916336)",
            "Insert(target_node=IN(type=tuple_pattern), node=('identifier', 'eps'), position=3, insert_id=916337)",
            "Insert(target_node=IN(type=tuple_pattern), node=(')', ')'), position=4, insert_id=916338)",
            "Insert(target_node=IN(type=tuple_pattern), node=('(', '('), position=0, insert_id=916339)",
            "Insert(target_node=IN(type=tuple_pattern), node=('identifier', 'boxes2'), position=1, insert_id=916340)",
            "Insert(target_node=IN(type=tuple_pattern), node=(',', ','), position=2, insert_id=916341)",
            "Insert(target_node=IN(type=tuple_pattern), node=('identifier', 'eps'), position=3, insert_id=916342)",
            "Insert(target_node=IN(type=tuple_pattern), node=(')', ')'), position=4, insert_id=916343)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 15,
        "number": 1615,
        "neg_line": [],
        "pos_line": [
            "+_check_nonzero(boxes1, eps)",
            "+_check_nonzero(boxes2, eps)"
        ],
        "core_change": "+_check_nonzero(boxes1, eps) +_check_nonzero(boxes2, eps)",
        "core_API": "apply"
    },
    {
        "commit_hash": "ef43735898b6f242f96d2fa534af9b48d9bec804",
        "index": "a72c6f2..9ce6302 100644",
        "commit_message": "Bug fix\n\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DistributedFusedLAMB(torch.optim.Optimizer):",
            "",
            "def _do_overlapped_reduction(self, param_i, param_grads_size, param_offset, param):",
            "# handle overlapped reductions",
            "-        if param.dtype = torch.float16:",
            "+        if param.dtype == torch.float16:",
            "self._grads_fp16.append( (param.grad, self._individual_flat_grads[param_i]) )",
            "else:",
            "self._grads_fp32.append( (param.grad, self._individual_flat_grads[param_i]) )"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('comparison_operator', None), position=1, insert_id=52187)",
            "Move(target_node=IN(type=comparison_operator), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=52188)",
            "Move(target_node=IN(type=comparison_operator), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 1617,
        "neg_line": [
            "-if param.dtype = torch.float16:"
        ],
        "pos_line": [
            "+if param.dtype == torch.float16:"
        ],
        "core_change": "-if param.dtype = torch.float16: +if param.dtype == torch.float16:",
        "core_API": "append"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "3ac4c1c2..5d6b0af5 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MultiHeadSelfAttentionTest(AllenNlpTestCase):",
            "num_heads=3, input_dim=5, attention_dim=6, values_dim=9, attention_dropout_prob=0.0",
            ")",
            "tensor = torch.randn(2, 12, 5)",
            "-        mask = torch.ones([2, 12])",
            "-        mask[0, 6:] = 0",
            "+        mask = torch.ones([2, 12]).bool()",
            "+        mask[0, 6:] = False",
            "result = attention(tensor, mask)",
            "# Compute the same function without a mask, but with",
            "# only the unmasked elements - should be the same."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=19855)",
            "Insert(target_node=ASTNode(type=assignment), node=('false', 'False'), position=2, insert_id=19856)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=19857)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=19858)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=19859)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=19860)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=19861)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=19862)",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 1620,
        "neg_line": [
            "-mask = torch.ones([2, 12])",
            "-mask[0, 6:] = 0"
        ],
        "pos_line": [
            "+mask = torch.ones([2, 12]).bool()",
            "+mask[0, 6:] = False"
        ],
        "core_change": "-mask = torch.ones([2, 12]) -mask[0, 6:] = 0 +mask = torch.ones([2, 12]).bool() +mask[0, 6:] = False",
        "core_API": "randn"
    },
    {
        "commit_hash": "c89bdfbe720bc8f41c7dc6db5473a2cb0955f224",
        "index": "ec8fe1b50..b2b4f14db 100644",
        "commit_message": "Reorganize repo (#8580)\n\n* Put models in subfolders\n\n* Styling\n\n* Fix imports in tests\n\n* More fixes in test imports\n\n* Sneaky hidden imports\n\n* Fix imports in doc files\n\n* More sneaky imports\n\n* Finish fixing tests\n\n* Fix examples\n\n* Fix path for copies\n\n* More fixes for examples\n\n* Fix dummy files\n\n* More fixes for example\n\n* More model import fixes\n\n* Is this why you're unhappy GitHub?\n\n* Fix imports in conver command\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class RagTestMixin:",
            ")",
            "dataset.add_faiss_index(\"embeddings\", string_factory=\"Flat\", metric_type=faiss.METRIC_INNER_PRODUCT)",
            "tokenizer = self.bart_tokenizer if config.generator.model_type == \"bart\" else self.t5_tokenizer",
            "-        with patch(\"transformers.retrieval_rag.load_dataset\") as mock_load_dataset:",
            "+        with patch(\"transformers.models.rag.retrieval_rag.load_dataset\") as mock_load_dataset:",
            "mock_load_dataset.return_value = dataset",
            "retriever = RagRetriever(",
            "config,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"transformers.retrieval_rag.load_dataset\"), value='\"transformers.models.rag.retrieval_rag.load_dataset\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1621,
        "neg_line": [
            "-with patch(\"transformers.retrieval_rag.load_dataset\") as mock_load_dataset:"
        ],
        "pos_line": [
            "+with patch(\"transformers.models.rag.retrieval_rag.load_dataset\") as mock_load_dataset:"
        ],
        "core_change": "-with patch(\"transformers.retrieval_rag.load_dataset\") as mock_load_dataset: +with patch(\"transformers.models.rag.retrieval_rag.load_dataset\") as mock_load_dataset:",
        "core_API": "add_faiss_index"
    },
    {
        "commit_hash": "54e0bb4dd9c025a551b2071151a49f88946624d4",
        "index": "c7af5481..140c49d5 100644",
        "commit_message": "Fixed casting to FloatTensor in simple_seq2seq.py (#585)\n\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SimpleSeq2Seq(Model):",
            "# encoder_outputs : (batch_size, input_sequence_length, encoder_output_dim)",
            "# Ensuring mask is also a FloatTensor. Or else the multiplication within attention will",
            "# complain.",
            "-            encoder_outputs_mask = encoder_outputs_mask.type(torch.FloatTensor)",
            "+            encoder_outputs_mask = encoder_outputs_mask.float()",
            "# (batch_size, input_sequence_length)",
            "input_weights = self._decoder_attention(decoder_hidden_state, encoder_outputs, encoder_outputs_mask)",
            "# (batch_size, encoder_output_dim)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=type), value='float')",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=FloatTensor))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1623,
        "neg_line": [
            "-encoder_outputs_mask = encoder_outputs_mask.type(torch.FloatTensor)"
        ],
        "pos_line": [
            "+encoder_outputs_mask = encoder_outputs_mask.float()"
        ],
        "core_change": "-encoder_outputs_mask = encoder_outputs_mask.type(torch.FloatTensor) +encoder_outputs_mask = encoder_outputs_mask.float()",
        "core_API": "type"
    },
    {
        "commit_hash": "6422ee88a9a3bdfd93ec98d2def1e9f2b02eacab",
        "index": "fdb248b6..dfc20db2 100644",
        "commit_message": "Replace Distribution+TorchDistribution with a thin Distribution mixin (#769)\n\n* Sketch Distribution class as mixin\n\n* Remove TorchDistribution class\n\n* Simplify TransformedDistribution\n\n* Update torch wrappers for most distributions\n\n* Fix docs\n\n* Use dist.Reshape() to set extra_event_dims\n\n* Fix bugs in Reshape distribution\n\n* Fix rejector tests\n\n* Update _Subsample distribution\n\n* Use .reshape() method for extra_event_dims and sample_shape\n\n* Refactor Distribution -> TorchDistribution class hierarchy\n\n* Update docs\n\n* Fix json error in air.ipynb\n\n* Fix bugs in air.ipynb and abstract_infer.py\n\n* Fix distributions docs\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Delta(Distribution):",
            "\"\"\"",
            "return Variable(self.v.data.unsqueeze(0))",
            "",
            "-    def analytic_mean(self):",
            "+    @property",
            "+    def mean(self):",
            "return self.v",
            "",
            "-    def analytic_var(self):",
            "+    @property",
            "+    def variance(self):",
            "return torch.zeros_like(self.v)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('decorated_definition', None), position=3, insert_id=754535)",
            "Insert(target_node=ASTNode(type=module), node=('decorated_definition', None), position=6, insert_id=754536)",
            "Insert(target_node=IN(type=decorated_definition), node=('decorator', None), position=0, insert_id=754537)",
            "Move(target_node=IN(type=decorated_definition), node=ASTNode(type=function_definition), position=1)",
            "Insert(target_node=IN(type=decorated_definition), node=('decorator', None), position=0, insert_id=754538)",
            "Move(target_node=IN(type=decorated_definition), node=ASTNode(type=function_definition), position=1)",
            "Insert(target_node=IN(type=decorator), node=('@', '@'), position=0, insert_id=754539)",
            "Insert(target_node=IN(type=decorator), node=('identifier', 'property'), position=1, insert_id=754540)",
            "Update(target_node=ASTNode(type=identifier, text=analytic_mean), value='mean')",
            "Insert(target_node=ASTNode(type=function_definition), node=('parameters', None), position=2, insert_id=754541)",
            "Insert(target_node=IN(type=decorator), node=('@', '@'), position=0, insert_id=754542)",
            "Insert(target_node=IN(type=decorator), node=('identifier', 'property'), position=1, insert_id=754543)",
            "Update(target_node=ASTNode(type=identifier, text=analytic_var), value='variance')",
            "Move(target_node=ASTNode(type=function_definition), node=ASTNode(type=parameters), position=2)",
            "Insert(target_node=IN(type=parameters), node=('(', '('), position=0, insert_id=754544)",
            "Insert(target_node=IN(type=parameters), node=('identifier', 'self'), position=1, insert_id=754545)",
            "Insert(target_node=IN(type=parameters), node=(')', ')'), position=2, insert_id=754546)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parameters))"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 1624,
        "neg_line": [
            "-def analytic_mean(self):",
            "-def analytic_var(self):"
        ],
        "pos_line": [
            "+@property",
            "+def mean(self):",
            "+@property",
            "+def variance(self):"
        ],
        "core_change": "-def analytic_mean(self): +@property +def mean(self): -def analytic_var(self): +@property +def variance(self):",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "56cd54ad242b27cb43b4abcfb007d3be42b39877",
        "index": "6f044f5..577d5ea 100644",
        "commit_message": "fix count bug\n\n",
        "file": "deep-learning-for-image-processing.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DiceCoefficient(object):",
            "return",
            "torch.distributed.barrier()",
            "torch.distributed.all_reduce(self.cumulative_dice)",
            "+        torch.distributed.all_reduce(self.count)",
            "",
            "",
            "class MetricLogger(object):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=69082)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=69083)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=69084)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=69085)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=69086)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=69087)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'all_reduce'), position=2, insert_id=69088)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=69089)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=69090)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=69091)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=69092)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=69093)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'distributed'), position=2, insert_id=69094)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=69095)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=69096)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'count'), position=2, insert_id=69097)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 16,
        "number": 1625,
        "neg_line": [],
        "pos_line": [
            "+torch.distributed.all_reduce(self.count)"
        ],
        "core_change": "+torch.distributed.all_reduce(self.count)",
        "core_API": "barrier"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "715db994..960fc811 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_sample(n_cutpoints, pred_shape):",
            "def test_constraints():",
            "predictor = torch.randn(5)",
            "for cp in (",
            "-        tt([1, 2, 3, 4, 0]),",
            "-        tt([1, 2, 4, 3, 5]),",
            "-        tt([1, 2, 3, 4, 4]),",
            "+        torch.tensor([1, 2, 3, 4, 0]),",
            "+        torch.tensor([1, 2, 4, 3, 5]),",
            "+        torch.tensor([1, 2, 3, 4, 4]),",
            "):",
            "with pytest.raises(ValueError):",
            "OrderedLogistic(predictor, cp)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=677119)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=677120)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=677121)",
            "Update(target_node=ASTNode(type=identifier, text=tt), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tt), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=677122)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=677123)",
            "Update(target_node=ASTNode(type=identifier, text=tt), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tt), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=677124)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=677125)",
            "Update(target_node=ASTNode(type=identifier, text=tt), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tt), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=677126)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=677127)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 15,
        "number": 1626,
        "neg_line": [
            "-tt([1, 2, 3, 4, 0]),",
            "-tt([1, 2, 4, 3, 5]),",
            "-tt([1, 2, 3, 4, 4]),"
        ],
        "pos_line": [
            "+torch.tensor([1, 2, 3, 4, 0]),",
            "+torch.tensor([1, 2, 4, 3, 5]),",
            "+torch.tensor([1, 2, 3, 4, 4]),"
        ],
        "core_change": "-tt([1, 2, 3, 4, 0]), -tt([1, 2, 4, 3, 5]), -tt([1, 2, 3, 4, 4]), +torch.tensor([1, 2, 3, 4, 0]), +torch.tensor([1, 2, 4, 3, 5]), +torch.tensor([1, 2, 3, 4, 4]),",
        "core_API": "randn"
    },
    {
        "commit_hash": "dacb0b12aab6851b20af4812b7ea9391873a8f35",
        "index": "7393077da..2df0fd237 100644",
        "commit_message": "Refactor Plan internals using PlaceHolder (#2910)\n\n* Add experiment about Plans with placeholders\n\n* Add first implementation of plan using placeholders\n\n* Update notebook of test\n\n* Improve local execution of plans\n\n* Support remote execution of plans\n\n* Rm unused function\n\n* Use Operation object in plan\n\n* Remove fixed precision and sharing from Plan's multiple fetch test\n\nThese seem extraneous to the point of the test anyway, so may as well\ngo ahead and remove them rather than disabling this test.\n\n* Disable encrypted and fixed precision Plans\n\n* Remove unused `plan` attribute on `State`\n\n* Make `test_fetch_plan_multiple_times` pass\n\n* Fix typo in test name\n\n* Update serde_full unit tests (WIP)\n\n* Add example of Plan problem\n\n* Update serde unit tests\n\n* Fix output handling in plans\n\n* Fix nested Plans handling\n\n* Apply formatting to serde helpers\n\n* Adjust serde tests to match the new plan structure\n\n* Update requirements.txt to avoid a merge conflict\n\n* Undo some changes to the Torch hook\n\n* Fix Long vs Float issue in `PointerPlan` test\n\n* Remove current PromiseTensor\n\n* Remove plan.procedure\n\n* Add Trace object to clean code\n\n* Rm promise notebook\n\n* Add support for fix_precision & MPC\n\n* Clean plan code and add a tracer decorator\n\n* Clean state and rm useless functions\n\n* Clean the tracer decorator and comment code\n\n* Add comments to Plan & State new methods\n\n* Fix ordering of arguments in Plan\n\n* Fix dummy error and fix wraps decorators\n\n* Fix\n\n* Add a few comments\n\n* Merge input/output placeholders and do light cleaning\n\n* Rm useless experimental notebook\n\n* Fix serde\n\n* Increase delay in a time test\n\nCo-authored-by: Karl Higley <kmhigley@gmail.com>\nCo-authored-by: Vova Manannikov <12518480+vvmnnnkv@users.noreply.github.com>\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "TORCH_DTYPE_STR = {",
            "TORCH_STR_DTYPE = {name: cls for cls, name in TORCH_DTYPE_STR.items()}",
            "",
            "",
            "-TORCH_MFORMAT_ID = {",
            "-    torch.channels_last: 1,",
            "-    torch.contiguous_format: 2,",
            "-    torch.preserve_format: 3,",
            "-}",
            "+TORCH_MFORMAT_ID = {torch.channels_last: 1, torch.contiguous_format: 2, torch.preserve_format: 3}",
            "",
            "TORCH_ID_MFORMAT = {i: cls for cls, i in TORCH_MFORMAT_ID.items()}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 0,
        "minus_line": 3,
        "AST_diff_line": 1,
        "number": 1628,
        "neg_line": [
            "-TORCH_MFORMAT_ID = {",
            "-torch.channels_last: 1,",
            "-torch.contiguous_format: 2,",
            "-torch.preserve_format: 3,",
            "-}"
        ],
        "pos_line": [
            "+TORCH_MFORMAT_ID = {torch.channels_last: 1, torch.contiguous_format: 2, torch.preserve_format: 3}"
        ],
        "core_change": "-TORCH_MFORMAT_ID = { -torch.channels_last: 1, -torch.contiguous_format: 2, -torch.preserve_format: 3, -} +TORCH_MFORMAT_ID = {torch.channels_last: 1, torch.contiguous_format: 2, torch.preserve_format: 3}",
        "core_API": "items"
    },
    {
        "commit_hash": "2e7e4280aa6f380a4e3afad6524295a17901c56c",
        "index": "5ef86541f..a93c08345 100755",
        "commit_message": "Traced models serialization and torchscripting fix (#17206)\n\n* Fix torch.jit.script and pickling issues\n\n* Fix get_attr issues\n\n* Fix import in function\n\n* Fix GPT-J and T5 tracing for torch=1.11\n\n* Gate graph surgery on torch version\n\n* Modeling minor changes to enable TorchScripting\n\n* Model serialization / deserialization test\n\n* Remove _assert_is_none users\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MultiHeadSelfAttention(nn.Module):",
            "q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)",
            "scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)",
            "mask = (mask == 0).view(mask_reshp).expand_as(scores)  # (bs, n_heads, q_length, k_length)",
            "-        scores = scores.masked_fill(mask, -float(\"inf\"))  # (bs, n_heads, q_length, k_length)",
            "+        scores = scores.masked_fill(mask, torch.tensor(-float(\"inf\")))  # (bs, n_heads, q_length, k_length)",
            "",
            "weights = nn.functional.softmax(scores, dim=-1)  # (bs, n_heads, q_length, k_length)",
            "weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=1199121)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=1199122)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1199123)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1199124)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1199125)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1199126)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=1199127)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1199128)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=unary_operator), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 1629,
        "neg_line": [
            "-scores = scores.masked_fill(mask, -float(\"inf\"))  # (bs, n_heads, q_length, k_length)"
        ],
        "pos_line": [
            "+scores = scores.masked_fill(mask, torch.tensor(-float(\"inf\")))  # (bs, n_heads, q_length, k_length)"
        ],
        "core_change": "-scores = scores.masked_fill(mask, -float(\"inf\"))  # (bs, n_heads, q_length, k_length) +scores = scores.masked_fill(mask, torch.tensor(-float(\"inf\")))  # (bs, n_heads, q_length, k_length)",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "d4c0895cb98f5aa6f112a6f28533a49ad9d6312a",
        "index": "9eed90c5..c86fb68b 100644",
        "commit_message": "Bump torch, torchvision, python versions (#2663)\n\n* Bump torch, torchvision, python versions\n\n* Fix .nonzero() usage\n\n* Add missing licenses\n\n* Fix more .nonzero() errors\n\n* Fix errors\n\n* Fix pyplot.hist invocation\n\n* Work around torch 1.6 incompatibility with pyplot.hist\n\n* Fix more pyplot.hist() errors\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _sample_tree_approx(edge_logits):",
            "mask = (c1 != c2)",
            "valid_logits = edge_logits[mask]",
            "probs = (valid_logits - valid_logits.max()).exp()",
            "-        k = mask.nonzero()[torch.multinomial(probs, 1)[0]]",
            "+        k = mask.nonzero(as_tuple=False)[torch.multinomial(probs, 1)[0]]",
            "components[grid[:, k]] = 1",
            "edge_ids[e] = k"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=690918)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'as_tuple'), position=0, insert_id=690919)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=690920)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=690921)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1633,
        "neg_line": [
            "-k = mask.nonzero()[torch.multinomial(probs, 1)[0]]"
        ],
        "pos_line": [
            "+k = mask.nonzero(as_tuple=False)[torch.multinomial(probs, 1)[0]]"
        ],
        "core_change": "-k = mask.nonzero()[torch.multinomial(probs, 1)[0]] +k = mask.nonzero(as_tuple=False)[torch.multinomial(probs, 1)[0]]",
        "core_API": "max"
    },
    {
        "commit_hash": "4185a222dc5c4166ce84a9403337b326eeae4b91",
        "index": "1e26abc4..bbbc7326 100644",
        "commit_message": "fix some errors\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def Conv2D(x, out_channel, kernel_shape,",
            "if b_init is None:",
            "b_init = tf.constant_initializer()",
            "",
            "-    W = tf.get_variable('W', filter_shape, initializer=W_init) # TODO collections",
            "+    W = tf.get_variable('W', filter_shape, initializer=W_init)",
            "b = tf.get_variable('b', [out_channel], initializer=b_init)",
            "",
            "if split == 1:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1635,
        "neg_line": [
            "-W = tf.get_variable('W', filter_shape, initializer=W_init) # TODO collections"
        ],
        "pos_line": [
            "+W = tf.get_variable('W', filter_shape, initializer=W_init)"
        ],
        "core_change": "-W = tf.get_variable('W', filter_shape, initializer=W_init) # TODO collections +W = tf.get_variable('W', filter_shape, initializer=W_init)",
        "core_API": "constant_initializer"
    },
    {
        "commit_hash": "57c82c6900fd3025ddfea82f59b8a36b2392191b",
        "index": "cb4ee8a..ad59bd5 100644",
        "commit_message": "fix variable_scope for older TF\n\n",
        "file": "tflearn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def highway(incoming, n_units, activation='linear', transform_dropout=None,",
            "n_inputs = int(np.prod(input_shape[1:]))",
            "",
            "# Build variables and inference.",
            "-    with tf.variable_scope(scope, name, [incoming], reuse=reuse) as scope:",
            "+    with tf.variable_scope(scope, name, values=[incoming], reuse=reuse) as scope:",
            "name = scope.name",
            "",
            "W_init = weights_init"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2352254)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'values'), position=0, insert_id=2352255)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2352256)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=list), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1640,
        "neg_line": [
            "-with tf.variable_scope(scope, name, [incoming], reuse=reuse) as scope:"
        ],
        "pos_line": [
            "+with tf.variable_scope(scope, name, values=[incoming], reuse=reuse) as scope:"
        ],
        "core_change": "-with tf.variable_scope(scope, name, [incoming], reuse=reuse) as scope: +with tf.variable_scope(scope, name, values=[incoming], reuse=reuse) as scope:",
        "core_API": "prod"
    },
    {
        "commit_hash": "574f95e7ceccea4aeb4b9c0159a296f35b162250",
        "index": "40d8a441..07a2ce0a 100644",
        "commit_message": "Use nearest neighbour interpolation for masks (#1630)\n\n* If mask, use always nearest neighbour interpolation\n\n* added parameter override\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fixed pickle\n\n* fixed errors\n\n* Update colorjiggle\n\n* fixed lint\n\n* bug fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* try again\n\n* Fixed augmentation container\n\n* Bug fixes\n\n* bug fixes\n\n* Fixed lint\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Make it work\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fixed mypy\n\n* Fixed mypy\n\nCo-authored-by: shijianjian <sj8716643@126.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Jian Shi <shij0c@kaust.edu.sa>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestAugmentationBase2D:",
            "output = utils.tensor_to_gradcheck_var(output)  # to var",
            "other_transform = utils.tensor_to_gradcheck_var(other_transform)  # to var",
            "",
            "-        input_param = {'batch_prob': torch.tensor([True]), 'params': {'x': input_transform}, 'flags': {}}",
            "+        input_param = {'batch_prob': torch.tensor([True]), 'x': input_transform, 'y': {}}",
            "",
            "augmentation = AugmentationBase2D(p=1.0)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=dictionary), node=ASTNode(type=pair), position=3)",
            "Update(target_node=ASTNode(type=string, text='flags'), value=\"'y'\")",
            "Delete(target_node=ASTNode(type=string, text='params'))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type={, text={))",
            "Delete(target_node=ASTNode(type=}, text=}))",
            "Delete(target_node=ASTNode(type=dictionary))",
            "Delete(target_node=ASTNode(type=pair))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1641,
        "neg_line": [
            "-input_param = {'batch_prob': torch.tensor([True]), 'params': {'x': input_transform}, 'flags': {}}"
        ],
        "pos_line": [
            "+input_param = {'batch_prob': torch.tensor([True]), 'x': input_transform, 'y': {}}"
        ],
        "core_change": "-input_param = {'batch_prob': torch.tensor([True]), 'params': {'x': input_transform}, 'flags': {}} +input_param = {'batch_prob': torch.tensor([True]), 'x': input_transform, 'y': {}}",
        "core_API": "tensor_to_gradcheck_var"
    },
    {
        "commit_hash": "9e510e5a1ab35bf9aadf5e3e2198f643f89ce510",
        "index": "a52511e0..04107947 100755",
        "commit_message": "bug fixes for VPG/TRPO/PPO\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class OptimizedStep(MetaOptimizer):",
            "loss_before = fn_compare(reference=reference)",
            "",
            "with tf.control_dependencies(control_inputs=(loss_before,)):",
            "-            applied, diffs = self.optimizer.step(time=time, variables=variables, fn_loss=fn_loss, **kwargs)",
            "+            diffs = self.optimizer.step(time=time, variables=variables, fn_loss=fn_loss, **kwargs)",
            "",
            "-        with tf.control_dependencies(control_inputs=(applied,)):",
            "+        with tf.control_dependencies(control_inputs=diffs):",
            "if fn_reference is None:",
            "loss_step = fn_loss()",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=diffs), position=0)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'diffs'), position=2, insert_id=2242715)",
            "Delete(target_node=ASTNode(type=identifier, text=applied))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=pattern_list))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=applied))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 1643,
        "neg_line": [
            "-applied, diffs = self.optimizer.step(time=time, variables=variables, fn_loss=fn_loss, **kwargs)",
            "-with tf.control_dependencies(control_inputs=(applied,)):"
        ],
        "pos_line": [
            "+diffs = self.optimizer.step(time=time, variables=variables, fn_loss=fn_loss, **kwargs)",
            "+with tf.control_dependencies(control_inputs=diffs):"
        ],
        "core_change": "-applied, diffs = self.optimizer.step(time=time, variables=variables, fn_loss=fn_loss, **kwargs) +diffs = self.optimizer.step(time=time, variables=variables, fn_loss=fn_loss, **kwargs) -with tf.control_dependencies(control_inputs=(applied,)): +with tf.control_dependencies(control_inputs=diffs):",
        "core_API": "control_dependencies"
    },
    {
        "commit_hash": "2cdfbd18a750cabaff1499fad7473dd91f3c7fa7",
        "index": "d2e5258e..dcf0f6e3 100644",
        "commit_message": "Previously, many unit test files started with `enable_v2_behavior`, which would have caused them to run in V2 mode when executing with a V1 test flag. The correct behavior would in fact be to skip such tests when executing with a V1 test flag.\n\nThis fix significantly reduces the total V1 + V2 test load by eliminating redundancy.\n\nPiperOrigin-RevId: 424734850\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DatasetCreatorModelFitTest(test_base.DatasetCreatorModelFitTestBase):",
            "",
            "def testModelTrainTFFunction(self, strategy):",
            "model = self._model_fit(strategy)",
            "-    self.assertIsInstance(model.train_tf_function, tf.__internal__.function.Function)",
            "+    self.assertIsInstance(model.train_tf_function,",
            "+                          tf.__internal__.function.Function)",
            "",
            "",
            "if __name__ == \"__main__\":",
            "-  tf.compat.v1.enable_v2_behavior()",
            "tf.__internal__.distribute.multi_process_runner.test_main()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', ''), position=0, insert_id=2068891)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=compat))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=v1))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=enable_v2_behavior))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 1644,
        "neg_line": [
            "-self.assertIsInstance(model.train_tf_function, tf.__internal__.function.Function)",
            "-tf.compat.v1.enable_v2_behavior()"
        ],
        "pos_line": [
            "+self.assertIsInstance(model.train_tf_function,",
            "+tf.__internal__.function.Function)"
        ],
        "core_change": "-self.assertIsInstance(model.train_tf_function, tf.__internal__.function.Function) +self.assertIsInstance(model.train_tf_function, +tf.__internal__.function.Function) -tf.compat.v1.enable_v2_behavior()",
        "core_API": "_model_fit"
    },
    {
        "commit_hash": "88d239631b9eb49527c21053d79d55e012f11a3c",
        "index": "9a33aa69..1f376fd3 100644",
        "commit_message": "fixed unused code, error prone and code style  (#352)\n\n* raise not implemented error instead of pass\n\n* fix unused code\n\n* fix pylint issues.\n\n* fixed some assert error\n\n* fixed unused code\n\n* fixed all error prone\n\n* fixed most of the code style\n\n* yapf\n\n* yapf tests\n\n* fixed suggestion\n\n* yapf\n\n* remove unused code\n\n* remove unused code\n\n* fix\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def conv_layers(net_in):",
            "",
            "",
            "def conv_layers_simple_api(net_in):",
            "-    with tf.name_scope('preprocess') as scope:",
            "+    with tf.name_scope('preprocess'):",
            "\"\"\"",
            "Notice that we include a preprocessing layer that takes the RGB image",
            "with pixels values in the range of 0-255 and subtracts the mean image"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=with_item), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=as, text=as))",
            "Delete(target_node=ASTNode(type=identifier, text=scope))",
            "Delete(target_node=ASTNode(type=as_pattern_target))",
            "Delete(target_node=ASTNode(type=as_pattern))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1646,
        "neg_line": [
            "-with tf.name_scope('preprocess') as scope:"
        ],
        "pos_line": [
            "+with tf.name_scope('preprocess'):"
        ],
        "core_change": "-with tf.name_scope('preprocess') as scope: +with tf.name_scope('preprocess'):",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "3dda5ac9fcf390d8b83eb855249360711707c11c",
        "index": "24992da1..eb90fe21 100644",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MaskedLayerNorm(torch.nn.Module):",
            "num_elements = broadcast_mask.sum() * self.size",
            "mean = (tensor * broadcast_mask).sum() / num_elements",
            "masked_centered = (tensor - mean) * broadcast_mask",
            "-        std = torch.sqrt(",
            "-                (masked_centered * masked_centered).sum() / num_elements + self.eps",
            "-        )",
            "+        std = torch.sqrt((masked_centered * masked_centered).sum() / num_elements + self.eps)",
            "return self.gamma * (tensor - mean) / (std + self.eps) + self.beta"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 1648,
        "neg_line": [
            "-std = torch.sqrt(",
            "-(masked_centered * masked_centered).sum() / num_elements + self.eps",
            "-)"
        ],
        "pos_line": [
            "+std = torch.sqrt((masked_centered * masked_centered).sum() / num_elements + self.eps)"
        ],
        "core_change": "-std = torch.sqrt( -(masked_centered * masked_centered).sum() / num_elements + self.eps -) +std = torch.sqrt((masked_centered * masked_centered).sum() / num_elements + self.eps)",
        "core_API": "sum"
    },
    {
        "commit_hash": "90c226cf10e098263d1df28bda054a5f22513b4f",
        "index": "bfb6845..837c2cd 100644",
        "commit_message": "add the option to use a `FORCE_CUDA` to force cuda installation on docker (#612)\n\n* add a FORCE_CUDA flag\n\nFollowing discussion [here](https://github.com/facebookresearch/maskrcnn-benchmark/issues/167), this seemed the best solution\n\n* Update Dockerfile\n\n* Update setup.py\n\n* add FORCE_CUDA as an ARG\n\n* \tmodified:   docker/Dockerfile\n\tmodified:   setup.py\n\n* small fix to readme of demo\n\n* remove test print\n\n* keep ARG_CUDA\n\n* remove env value and use the one from ARG\n\n* keep same formatting as source\n\n* change proposed by @miguelvr\n\n* Update INSTALL.md\n\n",
        "file": "maskrcnn-benchmark.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "def get_extensions():",
            "extra_compile_args = {\"cxx\": []}",
            "define_macros = []",
            "",
            "-    if torch.cuda.is_available() and CUDA_HOME is not None:",
            "+    if (torch.cuda.is_available() and CUDA_HOME is not None) or os.getenv(\"FORCE_CUDA\", \"0\") == \"1\":",
            "extension = CUDAExtension",
            "sources += source_cuda",
            "define_macros += [(\"WITH_CUDA\", None)]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=boolean_operator), node=('parenthesized_expression', None), position=0, insert_id=1853590)",
            "Insert(target_node=ASTNode(type=boolean_operator), node=('or', 'or'), position=1, insert_id=1853591)",
            "Insert(target_node=ASTNode(type=boolean_operator), node=('comparison_operator', None), position=2, insert_id=1853592)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1853593)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=boolean_operator), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1853594)",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=0, insert_id=1853595)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=1853596)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', '\"1\"'), position=2, insert_id=1853597)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1853598)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1853599)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'os'), position=0, insert_id=1853600)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1853601)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'getenv'), position=2, insert_id=1853602)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1853603)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"FORCE_CUDA\"'), position=1, insert_id=1853604)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1853605)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"0\"'), position=3, insert_id=1853606)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1853607)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 1649,
        "neg_line": [
            "-if torch.cuda.is_available() and CUDA_HOME is not None:"
        ],
        "pos_line": [
            "+if (torch.cuda.is_available() and CUDA_HOME is not None) or os.getenv(\"FORCE_CUDA\", \"0\") == \"1\":"
        ],
        "core_change": "-if torch.cuda.is_available() and CUDA_HOME is not None: +if (torch.cuda.is_available() and CUDA_HOME is not None) or os.getenv(\"FORCE_CUDA\", \"0\") == \"1\":",
        "core_API": "is_available"
    },
    {
        "commit_hash": "ebd0bc17929b36b851fc825bbfd5fbd41f6d8111",
        "index": "c87d52f5..d997f806 100644",
        "commit_message": "Update to PyTorch 1.11.0 (#3045)\n\n* Run black\n\n* Fix ProvenanceTensor\n\n* Replace torch.triangular_solve -> torch.linalg.solve_triangular\n\n* More fixes to torch.linalg.solve_triangular\n\n* Bump torch version\n\n* Bump Python version 3.6 -> 3.7\n\n* Fix some tests\n\n* Decrease tolerance on stable tests\n\n* Remove obsolete xfail\n\n* Resolve #3032\n\n* Fix solve_triangular in ops.gaussian, revert test weakening\n\n* Fix catching of singular matrices in hmc\n\n* xfail funsor tests\n\n* Work around pandas 1.3 bug\n\n* Allow mypy to install missing stubs\n\n* Clarify triangular_solve test\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class _OMTMVNSample(Function):",
            "loc_grad = sum_leftmost(grad_output, -1)",
            "",
            "identity = eye_like(g, dim)",
            "-        R_inv = torch.triangular_solve(identity, L.t(), transpose=False, upper=True)[0]",
            "+        R_inv = torch.linalg.solve_triangular(L.t(), identity, upper=True)",
            "",
            "z_ja = z.unsqueeze(-1)",
            "g_R_inv = torch.matmul(g, R_inv).unsqueeze(-2)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=672024)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=672025)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'solve_triangular'), position=2, insert_id=672026)",
            "Update(target_node=ASTNode(type=identifier, text=transpose), value='identity')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=transpose), position=4)",
            "Update(target_node=ASTNode(type=identifier, text=triangular_solve), value='linalg')",
            "Delete(target_node=ASTNode(type=identifier, text=identity))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1651,
        "neg_line": [
            "-R_inv = torch.triangular_solve(identity, L.t(), transpose=False, upper=True)[0]"
        ],
        "pos_line": [
            "+R_inv = torch.linalg.solve_triangular(L.t(), identity, upper=True)"
        ],
        "core_change": "-R_inv = torch.triangular_solve(identity, L.t(), transpose=False, upper=True)[0] +R_inv = torch.linalg.solve_triangular(L.t(), identity, upper=True)",
        "core_API": "triangular_solve"
    },
    {
        "commit_hash": "a0c603355bb7daab9c9738753dfeeb322c4d3ae4",
        "index": "ef4e9fe3..95649044 100755",
        "commit_message": "fixed continuous action explorations\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(object):",
            "",
            "elif action_spec['type'] == 'float':",
            "for _ in range(util.rank(action) - 1):",
            "-                exploration_value = tf.expand_dims(input=exploration_value, axis=1)",
            "+                exploration_value = tf.expand_dims(input=exploration_value, axis=-1)",
            "action += exploration_value",
            "if 'min_value' in action_spec:",
            "action = tf.clip_by_value("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('unary_operator', '-1'), position=2, insert_id=2235710)",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1652,
        "neg_line": [
            "-exploration_value = tf.expand_dims(input=exploration_value, axis=1)"
        ],
        "pos_line": [
            "+exploration_value = tf.expand_dims(input=exploration_value, axis=-1)"
        ],
        "core_change": "-exploration_value = tf.expand_dims(input=exploration_value, axis=1) +exploration_value = tf.expand_dims(input=exploration_value, axis=-1)",
        "core_API": "rank"
    },
    {
        "commit_hash": "1b3e4f9fb9a7a6e9fb516eee162b68763cb48ac3",
        "index": "eb0f26cec..7a193662b 100644",
        "commit_message": "Fix sync_dist for tpus (#6950)\n\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Result(Dict):",
            "",
            "# sync across workers when using distributed training",
            "sync_fn = sync_fn or sync_ddp_if_available",
            "+",
            "if sync_dist and isinstance(value, (torch.Tensor, numbers.Number)):",
            "is_dist_initialized = torch.distributed.is_available() and torch.distributed.is_initialized()",
            "# TODO: Find a way to make the reduction only once, so we don't need to clone.",
            "-            if is_dist_initialized and isinstance(value, torch.Tensor):",
            "+            if (is_dist_initialized or tpu_distributed) and isinstance(value, torch.Tensor):",
            "value = value.clone()",
            "else:",
            "value = torch.tensor(value, device=device, dtype=torch.float)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=boolean_operator), node=('parenthesized_expression', None), position=0, insert_id=538799)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=538800)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('boolean_operator', None), position=1, insert_id=538801)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=538802)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=identifier, text=is_dist_initialized), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('or', 'or'), position=1, insert_id=538803)",
            "Insert(target_node=IN(type=boolean_operator), node=('identifier', 'tpu_distributed'), position=2, insert_id=538804)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 1653,
        "neg_line": [
            "-if is_dist_initialized and isinstance(value, torch.Tensor):"
        ],
        "pos_line": [
            "+",
            "+if (is_dist_initialized or tpu_distributed) and isinstance(value, torch.Tensor):"
        ],
        "core_change": "+ -if is_dist_initialized and isinstance(value, torch.Tensor): +if (is_dist_initialized or tpu_distributed) and isinstance(value, torch.Tensor):",
        "core_API": "is_available"
    },
    {
        "commit_hash": "dcd32c30c1295ea2f24adf46f0f67a2701f1c69a",
        "index": "51e9410d..ada607f2 100755",
        "commit_message": "Fixed problems with exploration. (#289)\n\n* Cleaned up Agent child and child-classes:\n- Made sure no variables defined  in child classes are required by parent class.\n- Moved some variables shared amongst all types of Agent classes into base Agent class.\n- TODO: Need to a) Move more variables into base Agent class (now defined by each child class, e.g. discount) or b) create intermediary classes e.g. ModelBasedAgent. Needs to be discussed amongst devs.\n- Removed redundant docstring content, e.g. if variable is already defined by a parent class' c'tor and has the exact same functionality there.\n\n* Cleaned up Agent child and child-classes:\n- Made sure no variables defined  in child classes are required by parent class.\n- Moved some variables shared amongst all types of Agent classes into base Agent class.\n- TODO: Need to a) Move more variables into base Agent class (now defined by each child class, e.g. discount) or b) create intermediary classes e.g. ModelBasedAgent. Needs to be discussed amongst devs.\n- Removed redundant docstring content, e.g. if variable is already defined by a parent class' c'tor and has the exact same functionality there.\n\n* Moved `discount` into base Agent class.\n\n* Added LearningAgent class to hold all variables necessary for learning with an optimizable model.\nAll Agent child classes inherit from LearningAgent, except the non-learning ones (RandomAgent and ConstantAgent).\n\nIntroducing the new class (LearningAgent) removed lots of redundant c'tor and initialization code.\n\nObsoleted DDQNAgent (same as DQN with double_q_model==True). Gives an informative warning now (still backwards compatible).\n\n* Cleaned up Agent child and child-classes:\n- Made sure no variables defined  in child classes are required by parent class.\n- Moved some variables shared amongst all types of Agent classes into base Agent class.\n- TODO: Need to a) Move more variables into base Agent class (now defined by each child class, e.g. discount) or b) create intermediary classes e.g. ModelBasedAgent. Needs to be discussed amongst devs.\n- Removed redundant docstring content, e.g. if variable is already defined by a parent class' c'tor and has the exact same functionality there.\n\n* Moved `discount` into base Agent class.\n\n* Added LearningAgent class to hold all variables necessary for learning with an optimizable model.\nAll Agent child classes inherit from LearningAgent, except the non-learning ones (RandomAgent and ConstantAgent).\n\nIntroducing the new class (LearningAgent) removed lots of redundant c'tor and initialization code.\n\nObsoleted DDQNAgent (same as DQN with double_q_model==True). Gives an informative warning now (still backwards compatible).\n\n* Fixed travis import errors LearningAgent.\n\n* - Added LearningAgent properly to agents/__init__.py\n\n* Fixed various pytest failures.\nMoved 'scope' back into base Agent (needed by Random and ConstantAgents).\nRemoved  unnecessary parameters from Random and Constant (non-learning) Agents.\n\n* Fixed action_exploration in `Model` and `Exploration` classes.\n\n* Fixed action_exploration in `Model` and `Exploration` classes.\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class LinearDecay(Exploration):",
            "Linear decay based on episode number.",
            "\"\"\"",
            "",
            "-    def tf_explore(self, episode, timestep, num_actions):",
            "-        return tf.random_uniform(shape=num_actions) / (tf.cast(x=episode, dtype=util.tf_dtype('float') + 1.0))",
            "+    def tf_explore(self, episode, timestep, action_shape):",
            "+        return tf.random_uniform(shape=action_shape) / (tf.cast(x=episode, dtype=util.tf_dtype('float') + 1.0))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=num_actions), value='action_shape')",
            "Update(target_node=ASTNode(type=identifier, text=num_actions), value='action_shape')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 1655,
        "neg_line": [
            "-def tf_explore(self, episode, timestep, num_actions):",
            "-return tf.random_uniform(shape=num_actions) / (tf.cast(x=episode, dtype=util.tf_dtype('float') + 1.0))"
        ],
        "pos_line": [
            "+def tf_explore(self, episode, timestep, action_shape):",
            "+return tf.random_uniform(shape=action_shape) / (tf.cast(x=episode, dtype=util.tf_dtype('float') + 1.0))"
        ],
        "core_change": "-def tf_explore(self, episode, timestep, num_actions): -return tf.random_uniform(shape=num_actions) / (tf.cast(x=episode, dtype=util.tf_dtype('float') + 1.0)) +def tf_explore(self, episode, timestep, action_shape): +return tf.random_uniform(shape=action_shape) / (tf.cast(x=episode, dtype=util.tf_dtype('float') + 1.0))",
        "core_API": "random_uniform"
    },
    {
        "commit_hash": "8b240a06617455eae59e1116af6a1a016664e963",
        "index": "920086d6d..8357d09b5 100644",
        "commit_message": "Add TFEncoderDecoderModel + Add cross-attention to some TF models (#13222)\n\n* Add cross attentions to TFGPT2Model\n\n* Add TFEncoderDecoderModel\n\n* Add TFBaseModelOutputWithPoolingAndCrossAttentions\n\n* Add cross attentions to TFBertModel\n\n* Fix past or past_key_values argument issue\n\n* Fix generation\n\n* Fix save and load\n\n* Add some checks and comments\n\n* Clean the code that deals with past keys/values\n\n* Add kwargs to processing_inputs\n\n* Add serving_output to TFEncoderDecoderModel\n\n* Some cleaning + fix use_cache value issue\n\n* Fix tests + add bert2bert/bert2gpt2 tests\n\n* Fix more tests\n\n* Ignore crossattention.bias when loading GPT2 weights into TFGPT2\n\n* Fix return_dict_in_generate in tf generation\n\n* Fix is_token_logit_eos_token bug in tf generation\n\n* Finalize the tests after fixing some bugs\n\n* Fix another is_token_logit_eos_token bug in tf generation\n\n* Add/Update docs\n\n* Add TFBertEncoderDecoderModelTest\n\n* Clean test script\n\n* Add TFEncoderDecoderModel to the library\n\n* Add cross attentions to TFRobertaModel\n\n* Add TFRobertaEncoderDecoderModelTest\n\n* make style\n\n* Change the way of position_ids computation\n\n* bug fix\n\n* Fix copies in tf_albert\n\n* Remove some copied from and apply some fix-copies\n\n* Remove some copied\n\n* Add cross attentions to some other TF models\n\n* Remove encoder_hidden_states from TFLayoutLMModel.call for now\n\n* Make style\n\n* Fix TFRemBertForCausalLM\n\n* Revert the change to longformer + Remove copies\n\n* Revert the change to albert and convbert + Remove copies\n\n* make quality\n\n* make style\n\n* Add TFRembertEncoderDecoderModelTest\n\n* make quality and fix-copies\n\n* test TFRobertaForCausalLM\n\n* Fixes for failed tests\n\n* Fixes for failed tests\n\n* fix more tests\n\n* Fixes for failed tests\n\n* Fix Auto mapping order\n\n* Fix TFRemBertEncoder return value\n\n* fix tf_rembert\n\n* Check copies are OK\n\n* Fix missing TFBaseModelOutputWithPastAndCrossAttentions is not defined\n\n* Add TFEncoderDecoderModelSaveLoadTests\n\n* fix tf weight loading\n\n* check the change of use_cache\n\n* Revert the change\n\n* Add missing test_for_causal_lm for TFRobertaModelTest\n\n* Try cleaning past\n\n* fix _reorder_cache\n\n* Revert some files to original versions\n\n* Keep as many copies as possible\n\n* Apply suggested changes - Use raise ValueError instead of assert\n\n* Move import to top\n\n* Fix wrong require_torch\n\n* Replace more assert by raise ValueError\n\n* Add test_pt_tf_model_equivalence (the test won't pass for now)\n\n* add test for loading/saving\n\n* finish\n\n* finish\n\n* Remove test_pt_tf_model_equivalence\n\n* Update tf modeling template\n\n* Remove pooling, added in the prev. commit, from MainLayer\n\n* Update tf modeling test template\n\n* Move inputs[\"use_cache\"] = False to modeling_tf_utils.py\n\n* Fix torch.Tensor in the comment\n\n* fix use_cache\n\n* Fix missing use_cache in ElectraConfig\n\n* Add a note to from_pretrained\n\n* Fix style\n\n* Change test_encoder_decoder_save_load_from_encoder_decoder_from_pt\n\n* Fix TFMLP (in TFGPT2) activation issue\n\n* Fix None past_key_values value in serving_output\n\n* Don't call get_encoderdecoder_model in TFEncoderDecoderModelTest.test_configuration_tie until we have a TF checkpoint on Hub\n\n* Apply review suggestions - style for cross_attns in serving_output\n\n* Apply review suggestions - change assert + docstrings\n\n* break the error message to respect the char limit\n\n* deprecate the argument past\n\n* fix docstring style\n\n* Update the encoder-decoder rst file\n\n* fix Unknown interpreted text role \"method\"\n\n* fix typo\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class TFMobileBertModel(TFMobileBertPreTrainedModel):",
            "",
            "return outputs",
            "",
            "-    # Copied from transformers.models.bert.modeling_tf_bert.TFBertModel.serving_output",
            "def serving_output(self, output: TFBaseModelOutputWithPooling) -> TFBaseModelOutputWithPooling:",
            "hs = tf.convert_to_tensor(output.hidden_states) if self.config.output_hidden_states else None",
            "attns = tf.convert_to_tensor(output.attentions) if self.config.output_attentions else None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1656,
        "neg_line": [
            "-# Copied from transformers.models.bert.modeling_tf_bert.TFBertModel.serving_output"
        ],
        "pos_line": [],
        "core_change": "-# Copied from transformers.models.bert.modeling_tf_bert.TFBertModel.serving_output",
        "core_API": "convert_to_tensor"
    },
    {
        "commit_hash": "3f94170a1048bbcff77b222a708470e482fdaff8",
        "index": "097c38754..f6841cb84 100644",
        "commit_message": "[WIP] Test TF Flaubert + Add {XLM, Flaubert}{TokenClassification, MultipleC (#5614)\n\n* Test TF Flaubert + Add {XLM, Flaubert}{TokenClassification, MultipleChoice} models and tests\n\n* AutoModels\n\n\nTiny tweaks\n\n* Style\n\n* Final changes before merge\n\n* Re-order for simpler review\n\n* Final fixes\n\n* Addressing @sgugger's comments\n\n* Test MultipleChoice\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ModelTesterMixin:",
            "if model_class in MODEL_FOR_MULTIPLE_CHOICE_MAPPING.values():",
            "return {",
            "k: v.unsqueeze(1).expand(-1, self.model_tester.num_choices, -1).contiguous()",
            "-                if isinstance(v, torch.Tensor) and v.ndim != 0",
            "+                if isinstance(v, torch.Tensor) and v.ndim > 1",
            "else v",
            "for k, v in inputs_dict.items()",
            "}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('>', '>'), position=1, insert_id=1235430)",
            "Update(target_node=ASTNode(type=integer, text=0), value='1')",
            "Delete(target_node=ASTNode(type=!=, text=!=))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 1657,
        "neg_line": [
            "-if isinstance(v, torch.Tensor) and v.ndim != 0"
        ],
        "pos_line": [
            "+if isinstance(v, torch.Tensor) and v.ndim > 1"
        ],
        "core_change": "-if isinstance(v, torch.Tensor) and v.ndim != 0 +if isinstance(v, torch.Tensor) and v.ndim > 1",
        "core_API": "values"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "58797b3c..f7534c8c 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestGatedCnnEncoder(AllenNlpTestCase):",
            ")",
            "",
            "token_embeddings = torch.rand(5, 10, 32)",
            "-        mask = torch.ones(5, 10)",
            "-        mask[0, 7:] = 0",
            "-        mask[1, 5:] = 0",
            "+        mask = torch.ones(5, 10).bool()",
            "+        mask[0, 7:] = False",
            "+        mask[1, 5:] = False",
            "",
            "output = cnn_encoder(token_embeddings, mask)",
            "assert len(output) == 3"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=19822)",
            "Insert(target_node=ASTNode(type=assignment), node=('false', 'False'), position=2, insert_id=19823)",
            "Insert(target_node=ASTNode(type=assignment), node=('false', 'False'), position=2, insert_id=19824)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=19825)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=19826)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=19827)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=19828)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=19829)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=19830)",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 12,
        "number": 1658,
        "neg_line": [
            "-mask = torch.ones(5, 10)",
            "-mask[0, 7:] = 0",
            "-mask[1, 5:] = 0"
        ],
        "pos_line": [
            "+mask = torch.ones(5, 10).bool()",
            "+mask[0, 7:] = False",
            "+mask[1, 5:] = False"
        ],
        "core_change": "-mask = torch.ones(5, 10) -mask[0, 7:] = 0 -mask[1, 5:] = 0 +mask = torch.ones(5, 10).bool() +mask[0, 7:] = False +mask[1, 5:] = False",
        "core_API": "rand"
    },
    {
        "commit_hash": "db07956740e6543ab7270cb096377e83474ef61a",
        "index": "9c7ccccc8..f49870710 100755",
        "commit_message": "Fix missing eps arg for LayerNorm in ElectraGeneratorPredictions (#15332)\n\n* fix missing eps\n\n* Same fix for ConvBertGeneratorPredictions\n\n* Same fix for AlbertMLMHead\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class AlbertMLMHead(nn.Module):",
            "def __init__(self, config):",
            "super().__init__()",
            "",
            "-        self.LayerNorm = nn.LayerNorm(config.embedding_size)",
            "+        self.LayerNorm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)",
            "self.bias = nn.Parameter(torch.zeros(config.vocab_size))",
            "self.dense = nn.Linear(config.hidden_size, config.embedding_size)",
            "self.decoder = nn.Linear(config.embedding_size, config.vocab_size)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1536227)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1536228)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'eps'), position=0, insert_id=1536229)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1536230)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1536231)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=0, insert_id=1536232)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1536233)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'layer_norm_eps'), position=2, insert_id=1536234)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1659,
        "neg_line": [
            "-self.LayerNorm = nn.LayerNorm(config.embedding_size)"
        ],
        "pos_line": [
            "+self.LayerNorm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)"
        ],
        "core_change": "-self.LayerNorm = nn.LayerNorm(config.embedding_size) +self.LayerNorm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)",
        "core_API": "LayerNorm"
    },
    {
        "commit_hash": "9f6b31f2cb6feca411db6fcb870fd4a8cb0aa0a6",
        "index": "beb5edb..6a485c1 100644",
        "commit_message": "fix big bug with exclamation mark being used as padding for gpt2 (#50)\n\n\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def test_pred_input(params, enc = None):",
            "bos = tf.constant(1, shape=[1, 1], dtype=tf.int64)",
            "src_seq = tf.random.uniform(shape=[1, length], minval=4, maxval=(params['n_vocab'] - 1), dtype=tf.int64)",
            "seq = tf.concat([bos, src_seq], axis=1)",
            "-    seq = tf.pad(seq, [[0, 0], [0, remaining]])",
            "+    seq = tf.pad(seq, [[0, 0], [0, remaining]], constant_values=params['padding_id'])",
            "dataset = tf.data.Dataset.from_tensors(seq)",
            "",
            "dataset = dataset.map(_dummy_labels)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1938513)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1938514)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'constant_values'), position=0, insert_id=1938515)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1938516)",
            "Insert(target_node=IN(type=keyword_argument), node=('subscript', None), position=2, insert_id=1938517)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'params'), position=0, insert_id=1938518)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1938519)",
            "Insert(target_node=IN(type=subscript), node=('string', \"'padding_id'\"), position=2, insert_id=1938520)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1938521)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 1660,
        "neg_line": [
            "-seq = tf.pad(seq, [[0, 0], [0, remaining]])"
        ],
        "pos_line": [
            "+seq = tf.pad(seq, [[0, 0], [0, remaining]], constant_values=params['padding_id'])"
        ],
        "core_change": "-seq = tf.pad(seq, [[0, 0], [0, remaining]]) +seq = tf.pad(seq, [[0, 0], [0, remaining]], constant_values=params['padding_id'])",
        "core_API": "constant"
    },
    {
        "commit_hash": "da8b822366c9aa05082c400d135219e25405541a",
        "index": "aa32e026..5e3a6cb0 100644",
        "commit_message": "fix reverse return gradietns in sobel\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpatialGradient(nn.Module):",
            "kernel: torch.Tensor = tmp_kernel.repeat(c, 1, 1, 1, 1)",
            "",
            "# convolve input tensor with sobel kernel",
            "-        return F.conv3d(input[:, :, None], kernel, padding=1, groups=c)",
            "+        kernel_flip: torch.Tensor = kernel.flip(-3)",
            "+        return F.conv3d(input[:, :, None], kernel_flip, padding=1, groups=c)",
            "",
            "",
            "class Sobel(nn.Module):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=467260)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=467261)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'kernel_flip'), position=0, insert_id=467262)",
            "Insert(target_node=IN(type=assignment), node=(':', ':'), position=1, insert_id=467263)",
            "Insert(target_node=IN(type=assignment), node=('type', None), position=2, insert_id=467264)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=3, insert_id=467265)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=4, insert_id=467266)",
            "Insert(target_node=IN(type=type), node=('attribute', None), position=0, insert_id=467267)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=467268)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=467269)",
            "Update(target_node=ASTNode(type=identifier, text=kernel), value='kernel_flip')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=467270)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=467271)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=467272)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'kernel'), position=0, insert_id=467273)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=467274)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'flip'), position=2, insert_id=467275)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=467276)",
            "Insert(target_node=IN(type=argument_list), node=('unary_operator', '-3'), position=1, insert_id=467277)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=467278)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 1663,
        "neg_line": [
            "-return F.conv3d(input[:, :, None], kernel, padding=1, groups=c)"
        ],
        "pos_line": [
            "+kernel_flip: torch.Tensor = kernel.flip(-3)",
            "+return F.conv3d(input[:, :, None], kernel_flip, padding=1, groups=c)"
        ],
        "core_change": "-return F.conv3d(input[:, :, None], kernel, padding=1, groups=c) +kernel_flip: torch.Tensor = kernel.flip(-3) +return F.conv3d(input[:, :, None], kernel_flip, padding=1, groups=c)",
        "core_API": "repeat"
    },
    {
        "commit_hash": "bc21a4593d649d4ea0f73551197507b9bc4cdb43",
        "index": "de107d4b53..3ab3be23be 100644",
        "commit_message": "[RLlib] Fix crash when kl_coeff is set to 0 (#23063)\n\nCo-authored-by: Jeroen Bdorf <jeroen@minds.ai>\nCo-authored-by: Ishant Mrinal Haloi <mrinal.haloi11@gmail.com>\nCo-authored-by: Ishant Mrinal <33053278+n30111@users.noreply.github.com>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def ppo_surrogate_loss(",
            "action_kl = prev_action_dist.kl(curr_action_dist)",
            "mean_kl_loss = reduce_mean_valid(action_kl)",
            "else:",
            "-        mean_kl_loss = 0.0",
            "+        mean_kl_loss = tf.constant(0.0)",
            "",
            "curr_entropy = curr_action_dist.entropy()",
            "mean_entropy = reduce_mean_valid(curr_entropy)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=2138162)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2138163)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2138164)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2138165)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2138166)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'constant'), position=2, insert_id=2138167)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2138168)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=float, text=0.0), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2138169)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 1664,
        "neg_line": [
            "-mean_kl_loss = 0.0"
        ],
        "pos_line": [
            "+mean_kl_loss = tf.constant(0.0)"
        ],
        "core_change": "-mean_kl_loss = 0.0 +mean_kl_loss = tf.constant(0.0)",
        "core_API": "kl"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "595c2fdb..1c812252 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TestMaxout(AllenNlpTestCase):",
            "})",
            "maxout = Maxout.from_params(params)",
            "",
            "-        constant_init = lambda tensor: torch.nn.init.constant(tensor, 1.)",
            "+        constant_init = lambda tensor: torch.nn.init.constant_(tensor, 1.)",
            "initializer = InitializerApplicator([(\".*\", constant_init)])",
            "initializer(maxout)",
            "",
            "-        input_tensor = Variable(torch.FloatTensor([[-3, 1]]))",
            "+        input_tensor = torch.FloatTensor([[-3, 1]])",
            "output = maxout(input_tensor).data.numpy()",
            "assert output.shape == (1, 3)",
            "# This output was checked by hand"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=constant), value='constant_')",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 1665,
        "neg_line": [
            "-constant_init = lambda tensor: torch.nn.init.constant(tensor, 1.)",
            "-input_tensor = Variable(torch.FloatTensor([[-3, 1]]))"
        ],
        "pos_line": [
            "+constant_init = lambda tensor: torch.nn.init.constant_(tensor, 1.)",
            "+input_tensor = torch.FloatTensor([[-3, 1]])"
        ],
        "core_change": "-constant_init = lambda tensor: torch.nn.init.constant(tensor, 1.) +constant_init = lambda tensor: torch.nn.init.constant_(tensor, 1.) -input_tensor = Variable(torch.FloatTensor([[-3, 1]])) +input_tensor = torch.FloatTensor([[-3, 1]])",
        "core_API": "from_params"
    },
    {
        "commit_hash": "b6b4fca559b25374a60fb54e20cf407e7957581d",
        "index": "15a3bad50..3298d188e 100644",
        "commit_message": "Added instructions for Azure Deployment\n\n- Fixed issue with git repo clone and docker service in ansible playbook\n- Added cron job for auto updating git repo\n- Added mitogen_linear strategy to speed up ansible\n- Linting\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class DomainClient(Client):",
            "",
            "binary_dataset = serialize(assets, to_bytes=True)",
            "",
            "-        self.datasets.create_syft(dataset=binary_dataset, metadata=metadata, platform=\"syft\")",
            "+        self.datasets.create_syft(",
            "+            dataset=binary_dataset, metadata=metadata, platform=\"syft\"",
            "+        )"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1667,
        "neg_line": [
            "-self.datasets.create_syft(dataset=binary_dataset, metadata=metadata, platform=\"syft\")"
        ],
        "pos_line": [
            "+self.datasets.create_syft(",
            "+dataset=binary_dataset, metadata=metadata, platform=\"syft\"",
            "+)"
        ],
        "core_change": "-self.datasets.create_syft(dataset=binary_dataset, metadata=metadata, platform=\"syft\") +self.datasets.create_syft( +dataset=binary_dataset, metadata=metadata, platform=\"syft\" +)",
        "core_API": "create_syft"
    },
    {
        "commit_hash": "e6f54c5b32340278474e922d456fa3eb7f74599d",
        "index": "b430c06..b33b5bc 100644",
        "commit_message": "Fix numpy to torch cls streaming bug (#9112)\n\n* Fix numpy to torch cls streaming bug\n\nResolves https://github.com/ultralytics/yolov5/issues/9111\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def run(",
            "seen, windows, dt = 0, [], (Profile(), Profile(), Profile())",
            "for path, im, im0s, vid_cap, s in dataset:",
            "with dt[0]:",
            "-            im = im.to(device)",
            "+            im = torch.Tensor(im).to(device)",
            "im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32",
            "if len(im.shape) == 3:",
            "im = im[None]  # expand for batch dim"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=1292484)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1292485)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1292486)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1292487)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1292488)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=1292489)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1292490)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=im), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1292491)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 1668,
        "neg_line": [
            "-im = im.to(device)"
        ],
        "pos_line": [
            "+im = torch.Tensor(im).to(device)"
        ],
        "core_change": "-im = im.to(device) +im = torch.Tensor(im).to(device)",
        "core_API": "to"
    },
    {
        "commit_hash": "5b05852cd0319dcc296f506e9ba26987ad9dc7e3",
        "index": "48f699ca..d6887cdb 100644",
        "commit_message": "fix example\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Net(torch.nn.Module):",
            "self.gnn3_embed = GNN(3 * 64, 64, 64, lin=False)",
            "",
            "self.lin1 = torch.nn.Linear(3 * 64, 64)",
            "-        self.lin2 = torch.nn.Linear(64, 6)",
            "+        self.lin2 = torch.nn.Linear(64, dataset.num_classes)",
            "",
            "def forward(self, x, adj, mask=None):",
            "s = self.gnn1_pool(x, adj, mask)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=1492536)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dataset'), position=0, insert_id=1492537)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1492538)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'num_classes'), position=2, insert_id=1492539)",
            "Delete(target_node=ASTNode(type=integer, text=6))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1669,
        "neg_line": [
            "-self.lin2 = torch.nn.Linear(64, 6)"
        ],
        "pos_line": [
            "+self.lin2 = torch.nn.Linear(64, dataset.num_classes)"
        ],
        "core_change": "-self.lin2 = torch.nn.Linear(64, 6) +self.lin2 = torch.nn.Linear(64, dataset.num_classes)",
        "core_API": "Linear"
    },
    {
        "commit_hash": "a9e21cf7df52071c450a2dd2c5b53920dc0910ac",
        "index": "6578e8cf..e1889f01 100644",
        "commit_message": "Support models without FPN (#133)\n\n* add two stage w/o neck and w/ upperneck\n\n* add rpn r50 c4\n\n* update c4 configs\n\n* fix\n\n* config update\n\n* update config\n\n* minor update\n\n* mask rcnn support c4 train and test\n\n* lr fix\n\n* cascade support upper_neck\n\n* add cascade c4 config\n\n* update config\n\n* update\n\n* update res_layer to new interface\n\n* refactoring\n\n* c4 configs update\n\n* refactoring\n\n* update rpn_c4 config\n\n* rename upper_neck as shared_head\n\n* update\n\n* update configs\n\n* update\n\n* update c4 configs\n\n* update according to commits\n\n* update\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class FCNMaskHead(nn.Module):",
            "scale_factor=self.upsample_ratio, mode=self.upsample_method)",
            "",
            "out_channels = 1 if self.class_agnostic else self.num_classes",
            "-        self.conv_logits = nn.Conv2d(self.conv_out_channels, out_channels, 1)",
            "+        logits_in_channel = (self.conv_out_channels",
            "+                             if self.upsample_method == 'deconv' else",
            "+                             upsample_in_channels)",
            "+        self.conv_logits = nn.Conv2d(logits_in_channel, out_channels, 1)",
            "self.relu = nn.ReLU(inplace=True)",
            "self.debug_imgs = None"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=1427385)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1427386)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'logits_in_channel'), position=0, insert_id=1427387)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1427388)",
            "Insert(target_node=IN(type=assignment), node=('parenthesized_expression', None), position=2, insert_id=1427389)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1427390)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('conditional_expression', None), position=1, insert_id=1427391)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1427392)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=1427393)",
            "Insert(target_node=IN(type=conditional_expression), node=('comparison_operator', None), position=2, insert_id=1427394)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=1427395)",
            "Insert(target_node=IN(type=conditional_expression), node=('identifier', 'upsample_in_channels'), position=4, insert_id=1427396)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'logits_in_channel'), position=1, insert_id=1427397)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=1427398)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=1427399)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', \"'deconv'\"), position=2, insert_id=1427400)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1427401)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1427402)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'upsample_method'), position=2, insert_id=1427403)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 1670,
        "neg_line": [
            "-self.conv_logits = nn.Conv2d(self.conv_out_channels, out_channels, 1)"
        ],
        "pos_line": [
            "+logits_in_channel = (self.conv_out_channels",
            "+if self.upsample_method == 'deconv' else",
            "+upsample_in_channels)",
            "+self.conv_logits = nn.Conv2d(logits_in_channel, out_channels, 1)"
        ],
        "core_change": "-self.conv_logits = nn.Conv2d(self.conv_out_channels, out_channels, 1) +logits_in_channel = (self.conv_out_channels +if self.upsample_method == 'deconv' else +upsample_in_channels) +self.conv_logits = nn.Conv2d(logits_in_channel, out_channels, 1)",
        "core_API": "Conv2d"
    },
    {
        "commit_hash": "94d20f71a3b9be4ce6764240fa34767114ad616b",
        "index": "a9f3edc9..dc54f552 100644",
        "commit_message": "Remove tl.layers.initialize_global_variables(sess) (#931)\n\n* update sampling layers\n\n* upadte zoom\n\n* fix bug zoom\n\n* typo\n\n* fix bug affine_transform_cv2 x and y\n\n* fix bug crop when crop size equal to image size\n\n* fix file docs typo\n\n* fix bug instance norm\n\n* fix docs\n\n* update examples , init variables\n\n* changelog\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "if __name__ == \"__main__\":",
            "# ============================= EVALUATION =============================",
            "# env = gym.make(GAME)",
            "# GLOBAL_AC = ACNet(GLOBAL_NET_SCOPE)",
            "-    # tl.layers.initialize_global_variables(sess)",
            "+    # sess.run(tf.global_variables_initializer())",
            "# GLOBAL_AC.load_ckpt()",
            "# while True:",
            "#     s = env.reset()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1671,
        "neg_line": [
            "-# tl.layers.initialize_global_variables(sess)"
        ],
        "pos_line": [
            "+# sess.run(tf.global_variables_initializer())"
        ],
        "core_change": "-# tl.layers.initialize_global_variables(sess) +# sess.run(tf.global_variables_initializer())",
        "core_API": "make"
    },
    {
        "commit_hash": "5fa0b17c3d10cdb6411a173a7dce42b0de56a8f2",
        "index": "423980774..a05d729a1 100755",
        "commit_message": "[Past CI]  Leave Past CI failures in the past   (#20861)\n\n* torch.jit._state\n\n* Fix past CI\n\n* Fix for perceiver\n\n* Fix REALM\n\n* Fix for Bloom\n\n* Fix for SwinMode\n\n* Fix for TrajectoryTransformerModel\n\n* Fix for test_wav2vec2_with_lm\n\n* make style\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ModelTesterMixin:",
            "",
            "torch._C._jit_clear_class_registry()",
            "torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()",
            "-        torch.jit._state._clear_class_state()",
            "+        # torch 1.8 has no `_clear_class_state` in `torch.jit._state`",
            "+        if hasattr(torch.jit._state, \"_clear_class_state\"):",
            "+            torch.jit._state._clear_class_state()",
            "",
            "def _create_and_check_torchscript(self, config, inputs_dict):",
            "if not self.test_torchscript:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=1180775)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1180776)",
            "Insert(target_node=IN(type=if_statement), node=('call', None), position=1, insert_id=1180777)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1180778)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1180779)",
            "Insert(target_node=IN(type=call), node=('identifier', 'hasattr'), position=0, insert_id=1180780)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1180781)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1180782)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1180783)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1180784)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"_clear_class_state\"'), position=3, insert_id=1180785)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1180786)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1180787)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1180788)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_state'), position=2, insert_id=1180789)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1180790)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1180791)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'jit'), position=2, insert_id=1180792)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 1672,
        "neg_line": [
            "-torch.jit._state._clear_class_state()"
        ],
        "pos_line": [
            "+# torch 1.8 has no `_clear_class_state` in `torch.jit._state`",
            "+if hasattr(torch.jit._state, \"_clear_class_state\"):",
            "+torch.jit._state._clear_class_state()"
        ],
        "core_change": "-torch.jit._state._clear_class_state() +# torch 1.8 has no `_clear_class_state` in `torch.jit._state` +if hasattr(torch.jit._state, \"_clear_class_state\"): +torch.jit._state._clear_class_state()",
        "core_API": "_jit_clear_class_registry"
    },
    {
        "commit_hash": "093e901c979c0f8b0af92f79010b53db1ddcc972",
        "index": "7807bbc..85066a0 100644",
        "commit_message": "fix deprecation warnings; remove pydoop dependency; update README\n\n",
        "file": "TensorFlowOnSpark.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def map_fun(args, ctx):",
            "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")",
            "",
            "saver = tf.train.Saver()",
            "-      summary_op = tf.merge_all_summaries()",
            "-      init_op = tf.initialize_all_variables()",
            "+      summary_op = tf.summary.merge_all()",
            "+      init_op = tf.global_variables_initializer()",
            "",
            "# Create a \"supervisor\", which oversees the training process and stores model state into HDFS",
            "-    logdir = args.model if hdfs.path.isabs(args.model) else \"hdfs://default/user/{0}/{1}\".format(getpass.getuser(), args.model)",
            "+    logdir = args.model if args.model.startswith(\"hdfs://\") else \"hdfs://default/user/{0}/{1}\".format(getpass.getuser(), args.model)",
            "print(\"tensorflow model path: {0}\".format(logdir))",
            "-    summary_writer = tf.train.SummaryWriter(\"tensorboard_%d\" %(worker_num), graph=tf.get_default_graph())",
            "+    summary_writer = tf.summary.FileWriter(\"tensorboard_%d\" %(worker_num), graph=tf.get_default_graph())",
            "",
            "if args.mode == \"train\":",
            "sv = tf.train.Supervisor(is_chief=(task_index == 0),"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2213664)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2213665)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2213666)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'merge_all'), position=2, insert_id=2213667)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2213668)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2213669)",
            "Update(target_node=ASTNode(type=identifier, text=initialize_all_variables), value='global_variables_initializer')",
            "Update(target_node=ASTNode(type=identifier, text=SummaryWriter), value='FileWriter')",
            "Update(target_node=ASTNode(type=identifier, text=merge_all_summaries), value='summary')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=isabs), value='startswith')",
            "Insert(target_node=ASTNode(type=argument_list), node=('string', '\"hdfs://\"'), position=1, insert_id=2213670)",
            "Update(target_node=ASTNode(type=identifier, text=train), value='summary')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=identifier, text=hdfs))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=path))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 22,
        "number": 1674,
        "neg_line": [
            "-summary_op = tf.merge_all_summaries()",
            "-init_op = tf.initialize_all_variables()",
            "-logdir = args.model if hdfs.path.isabs(args.model) else \"hdfs://default/user/{0}/{1}\".format(getpass.getuser(), args.model)",
            "-summary_writer = tf.train.SummaryWriter(\"tensorboard_%d\" %(worker_num), graph=tf.get_default_graph())"
        ],
        "pos_line": [
            "+summary_op = tf.summary.merge_all()",
            "+init_op = tf.global_variables_initializer()",
            "+logdir = args.model if args.model.startswith(\"hdfs://\") else \"hdfs://default/user/{0}/{1}\".format(getpass.getuser(), args.model)",
            "+summary_writer = tf.summary.FileWriter(\"tensorboard_%d\" %(worker_num), graph=tf.get_default_graph())"
        ],
        "core_change": "-summary_op = tf.merge_all_summaries() -init_op = tf.initialize_all_variables() +summary_op = tf.summary.merge_all() +init_op = tf.global_variables_initializer() -logdir = args.model if hdfs.path.isabs(args.model) else \"hdfs://default/user/{0}/{1}\".format(getpass.getuser(), args.model) +logdir = args.model if args.model.startswith(\"hdfs://\") else \"hdfs://default/user/{0}/{1}\".format(getpass.getuser(), args.model) -summary_writer = tf.train.SummaryWriter(\"tensorboard_%d\" %(worker_num), graph=tf.get_default_graph()) +summary_writer = tf.summary.FileWriter(\"tensorboard_%d\" %(worker_num), graph=tf.get_default_graph())",
        "core_API": "reduce_mean"
    },
    {
        "commit_hash": "ced4cfdbbf4496ec0d95483e470933b4fed4f95a",
        "index": "1cbc5516..debe5933 100755",
        "commit_message": "Allow saving / loading checkpoints from cloud paths (#683)\n\n* Allow saving / loading checkpoints from cloud paths\n\nAllows saving and loading checkpoints directly from cloud paths like\nAmazon S3 (s3://) and Google Cloud Storage (gs://) by using fsspec.\n\nNote: The user will have to install the relevant dependency for each\nprotocol. Otherwise fsspec will fail and specify which dependency is\nmissing.\n\n* Append suffix _fsspec to save/load function names\n\n* Add a lower bound to the fsspec dependency\n\nSkips the 0 major version.\n\n* Add missing changes from refactor\n\n* Use fsspec for remaining artifacts\n\n* Add test case with path requiring fsspec\n\n* Avoid writing logs to file unless output_path is local\n\n* Document the possibility of using paths supported by fsspec\n\n* Fix style and lint\n\n* Add missing lint fixes\n\n* Add type annotations to new functions\n\n* Use Coqpit method for converting config to dict\n\n* Fix type annotation in semi-new function\n\n* Add return type for load_fsspec\n\n* Fix bug where fs not always created\n\n* Restore the experiment removal functionality\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main(args):  # pylint: disable=redefined-outer-name",
            "model = setup_model(c)",
            "",
            "# restore model",
            "-    checkpoint = torch.load(args.checkpoint_path, map_location=\"cpu\")",
            "+    checkpoint = load_fsspec(args.checkpoint_path, map_location=\"cpu\")",
            "model.load_state_dict(checkpoint[\"model\"])",
            "",
            "if use_cuda:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='load_fsspec')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=load))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1676,
        "neg_line": [
            "-checkpoint = torch.load(args.checkpoint_path, map_location=\"cpu\")"
        ],
        "pos_line": [
            "+checkpoint = load_fsspec(args.checkpoint_path, map_location=\"cpu\")"
        ],
        "core_change": "-checkpoint = torch.load(args.checkpoint_path, map_location=\"cpu\") +checkpoint = load_fsspec(args.checkpoint_path, map_location=\"cpu\")",
        "core_API": "load"
    },
    {
        "commit_hash": "3a064bd4dd6d53ee1bd743c574918b443b7af554",
        "index": "7099ca0c5..1d8c01e24 100644",
        "commit_message": "fix `bias` keyword argument in TFDebertaEmbeddings (#17940)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class TFDebertaEmbeddings(tf.keras.layers.Layer):",
            "self.position_biased_input = getattr(config, \"position_biased_input\", True)",
            "self.initializer_range = config.initializer_range",
            "if self.embedding_size != config.hidden_size:",
            "-            self.embed_proj = tf.keras.layers.Dense(config.hidden_size, bias=False)",
            "+            self.embed_proj = tf.keras.layers.Dense(config.hidden_size, use_bias=False)",
            "self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")",
            "self.dropout = TFDebertaStableDropout(config.hidden_dropout_prob, name=\"dropout\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=bias), value='use_bias')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1678,
        "neg_line": [
            "-self.embed_proj = tf.keras.layers.Dense(config.hidden_size, bias=False)"
        ],
        "pos_line": [
            "+self.embed_proj = tf.keras.layers.Dense(config.hidden_size, use_bias=False)"
        ],
        "core_change": "-self.embed_proj = tf.keras.layers.Dense(config.hidden_size, bias=False) +self.embed_proj = tf.keras.layers.Dense(config.hidden_size, use_bias=False)",
        "core_API": "Dense"
    },
    {
        "commit_hash": "ec3aace0ae6c89e9d7d6ed33425531db3b683485",
        "index": "bd0965d2d..d44966b56 100644",
        "commit_message": "Add type annotations for Rembert/Splinter and copies (#16338)\n\n* undo black autoformat\n\n* minor fix to rembert forward with default\n\n* make fix-copies, make quality\n\n* Adding types to template model\n\n* Removing List from the template types\n\n* Remove `Optional` from a couple of types that don't accept `None`\n\nCo-authored-by: matt <rocketknight1@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MPNetPooler(nn.Module):",
            "self.dense = nn.Linear(config.hidden_size, config.hidden_size)",
            "self.activation = nn.Tanh()",
            "",
            "-    def forward(self, hidden_states):",
            "+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:",
            "# We \"pool\" the model by simply taking the hidden state corresponding",
            "# to the first token.",
            "first_token_tensor = hidden_states[:, 0]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('->', '->'), position=3, insert_id=1202762)",
            "Insert(target_node=ASTNode(type=function_definition), node=('type', None), position=4, insert_id=1202763)",
            "Insert(target_node=ASTNode(type=function_definition), node=(':', ':'), position=5, insert_id=1202764)",
            "Insert(target_node=ASTNode(type=parameters), node=('typed_parameter', None), position=3, insert_id=1202765)",
            "Insert(target_node=IN(type=type), node=('attribute', None), position=0, insert_id=1202766)",
            "Move(target_node=IN(type=typed_parameter), node=ASTNode(type=identifier, text=hidden_states), position=0)",
            "Move(target_node=IN(type=typed_parameter), node=ASTNode(type=:, text=:), position=1)",
            "Insert(target_node=IN(type=typed_parameter), node=('type', None), position=2, insert_id=1202767)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1202768)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1202769)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=1202770)",
            "Insert(target_node=IN(type=type), node=('attribute', None), position=0, insert_id=1202771)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1202772)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1202773)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=1202774)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 1679,
        "neg_line": [
            "-def forward(self, hidden_states):"
        ],
        "pos_line": [
            "+def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:"
        ],
        "core_change": "-def forward(self, hidden_states): +def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:",
        "core_API": "Linear"
    },
    {
        "commit_hash": "428516056abe41f135133e732a8d44af6ce9a234",
        "index": "5416f9a51..15f237013 100644",
        "commit_message": "[RLlib] SAC Torch (incl. Atari learning) (#7984)\n\n* Policy-classes cleanup and torch/tf unification.\n- Make Policy abstract.\n- Add `action_dist` to call to `extra_action_out_fn` (necessary for PPO torch).\n- Move some methods and vars to base Policy\n  (from TFPolicy): num_state_tensors, ACTION_PROB, ACTION_LOGP and some more.\n\n* Fix `clip_action` import from Policy (should probably be moved into utils altogether).\n\n* - Move `is_recurrent()` and `num_state_tensors()` into TFPolicy (from DynamicTFPolicy).\n- Add config to all Policy c'tor calls (as 3rd arg after obs and action spaces).\n\n* Add `config` to c'tor call to TFPolicy.\n\n* Add missing `config` to c'tor call to TFPolicy in marvil_policy.py.\n\n* Fix test_rollout_worker.py::MockPolicy and BadPolicy classes (Policy base class is now abstract).\n\n* Fix LINT errors in Policy classes.\n\n* Implement StatefulPolicy abstract methods in test cases: test_multi_agent_env.py.\n\n* policy.py LINT errors.\n\n* Create a simple TestPolicy to sub-class from when testing Policies (reduces code in some test cases).\n\n* policy.py\n- Remove abstractmethod from `apply_gradients` and `compute_gradients` (these are not required iff `learn_on_batch` implemented).\n- Fix docstring of `num_state_tensors`.\n\n* Make QMIX torch Policy a child of TorchPolicy (instead of Policy).\n\n* QMixPolicy add empty implementations of abstract Policy methods.\n\n* Store Policy's config in self.config in base Policy c'tor.\n\n* - Make only compute_actions in base Policy's an abstractmethod and provide pass\nimplementation to all other methods if not defined.\n- Fix state_batches=None (most Policies don't have internal states).\n\n* Cartpole tf learning.\n\n* Cartpole tf AND torch learning (in ~ same ts).\n\n* Cartpole tf AND torch learning (in ~ same ts). 2\n\n* Cartpole tf (torch syntax-broken) learning (in ~ same ts). 3\n\n* Cartpole tf AND torch learning (in ~ same ts). 4\n\n* Cartpole tf AND torch learning (in ~ same ts). 5\n\n* Cartpole tf AND torch learning (in ~ same ts). 6\n\n* Cartpole tf AND torch learning (in ~ same ts). Pendulum tf learning.\n\n* WIP.\n\n* WIP.\n\n* SAC torch learning Pendulum.\n\n* WIP.\n\n* SAC torch and tf learning Pendulum and Cartpole after cleanup.\n\n* WIP.\n\n* LINT.\n\n* LINT.\n\n* SAC: Move policy.target_model to policy.device as well.\n\n* Fixes and cleanup.\n\n* Fix data-format of tf keras Conv2d layers (broken for some tf-versions which have data_format=\"channels_first\" as default).\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fix and LINT.\n\n* WIP.\n\n* Test fixes and LINT.\n\n* Fixes and LINT.\n\nCo-authored-by: Sven Mika <sven@Svens-MacBook-Pro.local>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ParameterNoise(Exploration):",
            "else:",
            "for i in range(len(self.noise)):",
            "self.noise[i] = torch.normal(",
            "-                    0.0, self.stddev, size=self.noise[i].size())",
            "+                    mean=torch.zeros(self.noise[i].size()), std=self.stddev)",
            "",
            "def _tf_sample_new_noise_op(self):",
            "added_noises = []"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1124859)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=4, insert_id=1124860)",
            "Update(target_node=ASTNode(type=identifier, text=size), value='mean')",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('call', None), position=2, insert_id=1124861)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'std'), position=0, insert_id=1124862)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1124863)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=attribute), position=2)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1124864)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1124865)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1124866)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1124867)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'zeros'), position=2, insert_id=1124868)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1124869)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1124870)",
            "Delete(target_node=ASTNode(type=float, text=0.0))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 1680,
        "neg_line": [
            "-0.0, self.stddev, size=self.noise[i].size())"
        ],
        "pos_line": [
            "+mean=torch.zeros(self.noise[i].size()), std=self.stddev)"
        ],
        "core_change": "-0.0, self.stddev, size=self.noise[i].size()) +mean=torch.zeros(self.noise[i].size()), std=self.stddev)",
        "core_API": "normal"
    },
    {
        "commit_hash": "14f4df750c8dff68588038af2c917274007e967e",
        "index": "ab3e072df..ca988a7bd 100644",
        "commit_message": "Fix CI test failures related to torch_complex 0.4.0\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_solve(real_vec):",
            "if isinstance(vec2, ComplexTensor):",
            "ret2 = FC.solve(vec2, mat, return_LU=False)",
            "else:",
            "-            ret2 = torch.solve(vec2, mat)[0]",
            "+            return torch.linalg.solve(mat, vec2)",
            "assert complex_module.allclose(ret, ret2)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('type', None), position=2, insert_id=128089)",
            "Move(target_node=IN(type=type), node=ASTNode(type=call), position=0)",
            "Move(target_node=ASTNode(type=identifier, text=vec2), node=ASTNode(type=argument_list), position=3)",
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=argument_list), position=4)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=128090)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=128091)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'return'), position=0, insert_id=128092)",
            "Insert(target_node=IN(type=attribute), node=('ERROR', None), position=1, insert_id=128093)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=3, insert_id=128094)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=ret2))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 1681,
        "neg_line": [
            "-ret2 = torch.solve(vec2, mat)[0]"
        ],
        "pos_line": [
            "+return torch.linalg.solve(mat, vec2)"
        ],
        "core_change": "-ret2 = torch.solve(vec2, mat)[0] +return torch.linalg.solve(mat, vec2)",
        "core_API": "solve"
    },
    {
        "commit_hash": "4fd8977eafd658056c92ffa6294027e77cdc46c0",
        "index": "06d5f6848a..947142f1db 100644",
        "commit_message": "[RLlib] Minor cleanup in preparation to tf2.x support. (#9130)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* Fixes.\n\n* Fixes and LINT.\n\n* WIP.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ReweightedImitationLoss:",
            "# update averaged advantage norm",
            "update_adv_norm = tf.assign_add(",
            "ref=policy._ma_adv_norm,",
            "-            value=1e-6 *",
            "-            (tf.reduce_mean(tf.square(adv)) - policy._ma_adv_norm))",
            "+            value=1e-6 * (",
            "+                    tf.reduce_mean(tf.math.square(adv)) - policy._ma_adv_norm))",
            "",
            "# exponentially weighted advantages",
            "with tf.control_dependencies([update_adv_norm]):",
            "-            exp_advs = tf.exp(",
            "-                beta * tf.divide(adv, 1e-8 + tf.sqrt(policy._ma_adv_norm)))",
            "+            exp_advs = tf.math.exp(beta * tf.math.divide(",
            "+                adv, 1e-8 + tf.math.sqrt(policy._ma_adv_norm)))",
            "",
            "# log\\pi_\\theta(a|s)",
            "logprobs = action_dist.logp(actions)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2145890)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2145891)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2145892)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2145893)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2145894)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2145895)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2145896)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2145897)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2145898)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2145899)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2145900)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2145901)"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 20,
        "number": 1682,
        "neg_line": [
            "-value=1e-6 *",
            "-(tf.reduce_mean(tf.square(adv)) - policy._ma_adv_norm))",
            "-exp_advs = tf.exp(",
            "-beta * tf.divide(adv, 1e-8 + tf.sqrt(policy._ma_adv_norm)))"
        ],
        "pos_line": [
            "+value=1e-6 * (",
            "+tf.reduce_mean(tf.math.square(adv)) - policy._ma_adv_norm))",
            "+exp_advs = tf.math.exp(beta * tf.math.divide(",
            "+adv, 1e-8 + tf.math.sqrt(policy._ma_adv_norm)))"
        ],
        "core_change": "-value=1e-6 * -(tf.reduce_mean(tf.square(adv)) - policy._ma_adv_norm)) +value=1e-6 * ( +tf.reduce_mean(tf.math.square(adv)) - policy._ma_adv_norm)) -exp_advs = tf.exp( -beta * tf.divide(adv, 1e-8 + tf.sqrt(policy._ma_adv_norm))) +exp_advs = tf.math.exp(beta * tf.math.divide( +adv, 1e-8 + tf.math.sqrt(policy._ma_adv_norm)))",
        "core_API": "assign_add"
    },
    {
        "commit_hash": "afe5d42d8d1d80af911ed980c2936bfe887078f6",
        "index": "60b9dca78..b957acb6d 100644",
        "commit_message": "Black preview (#17217)\n\n* Black preview\n\n* Fixup too!\n\n* Fix check copies\n\n* Use the same version as the CI\n\n* Bump black\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "class AdamW(Optimizer):",
            "):",
            "if not no_deprecation_warning:",
            "warnings.warn(",
            "-                \"This implementation of AdamW is deprecated and will be removed in a future version. Use the\"",
            "-                \" PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\",",
            "+                \"This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch\"",
            "+                \" implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this\"",
            "+                \" warning\",",
            "FutureWarning,",
            ")",
            "require_version(\"torch>=1.5.0\")  # add_ with alpha"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('concatenated_string', None), position=0, insert_id=1621113)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', '\"This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch\"'), position=0, insert_id=1621114)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', '\" implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this\"'), position=1, insert_id=1621115)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', '\" warning\"'), position=2, insert_id=1621116)",
            "Delete(target_node=ASTNode(type=string, text=\"This implementation of AdamW is deprecated and will be removed in a future version. Use the\"))",
            "Delete(target_node=ASTNode(type=string, text=\" PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\"))",
            "Delete(target_node=ASTNode(type=concatenated_string))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 1683,
        "neg_line": [
            "-\"This implementation of AdamW is deprecated and will be removed in a future version. Use the\"",
            "-\" PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\","
        ],
        "pos_line": [
            "+\"This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch\"",
            "+\" implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this\"",
            "+\" warning\","
        ],
        "core_change": "-\"This implementation of AdamW is deprecated and will be removed in a future version. Use the\" -\" PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\", +\"This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch\" +\" implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this\" +\" warning\",",
        "core_API": "warn"
    },
    {
        "commit_hash": "fa338dd6df34e2adc02e6b3f8ba6794f2753c0e4",
        "index": "b30736ee..fec29c04 100644",
        "commit_message": "amalgam cuda hotfix\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def draw_first_k_couples(k: int, rdims: int, dv: torch.device):",
            "repeats = torch.cat(",
            "[",
            "torch.arange(max_exhaustive_search, dtype=torch.long, device=dv) + 1,",
            "-            torch.tensor([residual_search], dtype=torch.long),",
            "+            torch.tensor([residual_search], dtype=torch.long, device=dv),",
            "]",
            ")",
            "idx_sequence = torch.stack([repeats.repeat_interleave(repeats), arange_sequence(repeats)], dim=-1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=401336)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=401337)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=401338)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=401339)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dv'), position=2, insert_id=401340)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1684,
        "neg_line": [
            "-torch.tensor([residual_search], dtype=torch.long),"
        ],
        "pos_line": [
            "+torch.tensor([residual_search], dtype=torch.long, device=dv),"
        ],
        "core_change": "-torch.tensor([residual_search], dtype=torch.long), +torch.tensor([residual_search], dtype=torch.long, device=dv),",
        "core_API": "cat"
    },
    {
        "commit_hash": "a7846135a854fc322824a90dc0fd3612fe7db81b",
        "index": "fd6b988c..3aa0485f 100644",
        "commit_message": "Enable `fixed_arch` on Retiarii (#3972)\n\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DartsLayerChoice(nn.Module):",
            "yield name, p",
            "",
            "def export(self):",
            "-        return torch.argmax(self.alpha).item()",
            "+        return list(self.op_choices.keys())[torch.argmax(self.alpha).item()]",
            "",
            "",
            "class DartsInputChoice(nn.Module):",
            "def __init__(self, input_choice):",
            "super(DartsInputChoice, self).__init__()",
            "-        self.name = input_choice.key",
            "+        self.name = input_choice.label",
            "self.alpha = nn.Parameter(torch.randn(input_choice.n_candidates) * 1e-3)",
            "self.n_chosen = input_choice.n_chosen or 1"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('subscript', None), position=1, insert_id=661775)",
            "Update(target_node=ASTNode(type=identifier, text=key), value='label')",
            "Insert(target_node=IN(type=subscript), node=('call', None), position=0, insert_id=661776)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=661777)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=661778)",
            "Insert(target_node=IN(type=call), node=('identifier', 'list'), position=0, insert_id=661779)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=661780)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=661781)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=661782)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=661783)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=661784)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=661785)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=661786)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=661787)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'keys'), position=2, insert_id=661788)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=661789)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=661790)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=661791)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=661792)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'op_choices'), position=2, insert_id=661793)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 1685,
        "neg_line": [
            "-return torch.argmax(self.alpha).item()",
            "-self.name = input_choice.key"
        ],
        "pos_line": [
            "+return list(self.op_choices.keys())[torch.argmax(self.alpha).item()]",
            "+self.name = input_choice.label"
        ],
        "core_change": "-return torch.argmax(self.alpha).item() +return list(self.op_choices.keys())[torch.argmax(self.alpha).item()] -self.name = input_choice.key +self.name = input_choice.label",
        "core_API": "argmax"
    },
    {
        "commit_hash": "8df50a269122e85dca2d0b72e98de2763b60e87c",
        "index": "eb8eb99a..5235f21b 100644",
        "commit_message": "fix typo (#2718)\n\nCo-authored-by: Jeff Rasley <jerasley@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class DeepSpeedDiffusersTransformerBlock(nn.Module):",
            "self.attn_1.do_out_bias = False",
            "self.attn_1_bias = self.attn_1.attn_ob",
            "else:",
            "-            self.attn_1_bias = nn.Paramaeter(torch.zeros_like(self.norm2_g),",
            "-                                             requires_grad=False)",
            "+            self.attn_1_bias = nn.Parameter(torch.zeros_like(self.norm2_g),",
            "+                                            requires_grad=False)",
            "",
            "# Pull the bias in if we can",
            "if isinstance(self.attn_2, DeepSpeedDiffusersAttention):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Paramaeter), value='Parameter')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 1688,
        "neg_line": [
            "-self.attn_1_bias = nn.Paramaeter(torch.zeros_like(self.norm2_g),",
            "-requires_grad=False)"
        ],
        "pos_line": [
            "+self.attn_1_bias = nn.Parameter(torch.zeros_like(self.norm2_g),",
            "+requires_grad=False)"
        ],
        "core_change": "-self.attn_1_bias = nn.Paramaeter(torch.zeros_like(self.norm2_g), -requires_grad=False) +self.attn_1_bias = nn.Parameter(torch.zeros_like(self.norm2_g), +requires_grad=False)",
        "core_API": "Paramaeter"
    },
    {
        "commit_hash": "5866646cc8347e45c17ca1204f3e770712365f99",
        "index": "8b70a6f..4c43719 100644",
        "commit_message": "Fix float zeros format (#5491)\n\n* Fix float zeros format\n\n* 255 to integer\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Detections:",
            "def __init__(self, imgs, pred, files, times=None, names=None, shape=None):",
            "super().__init__()",
            "d = pred[0].device  # device",
            "-        gn = [torch.tensor([*(im.shape[i] for i in [1, 0, 1, 0]), 1., 1.], device=d) for im in imgs]  # normalizations",
            "+        gn = [torch.tensor([*(im.shape[i] for i in [1, 0, 1, 0]), 1, 1], device=d) for im in imgs]  # normalizations",
            "self.imgs = imgs  # list of images as numpy arrays",
            "self.pred = pred  # list of tensors pred[0] = (xyxy, conf, cls)",
            "self.names = names  # class names"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('integer', '1'), position=3, insert_id=1296310)",
            "Insert(target_node=ASTNode(type=list), node=('integer', '1'), position=6, insert_id=1296311)",
            "Delete(target_node=ASTNode(type=float, text=1.))",
            "Delete(target_node=ASTNode(type=float, text=1.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1689,
        "neg_line": [
            "-gn = [torch.tensor([*(im.shape[i] for i in [1, 0, 1, 0]), 1., 1.], device=d) for im in imgs]  # normalizations"
        ],
        "pos_line": [
            "+gn = [torch.tensor([*(im.shape[i] for i in [1, 0, 1, 0]), 1, 1], device=d) for im in imgs]  # normalizations"
        ],
        "core_change": "-gn = [torch.tensor([*(im.shape[i] for i in [1, 0, 1, 0]), 1., 1.], device=d) for im in imgs]  # normalizations +gn = [torch.tensor([*(im.shape[i] for i in [1, 0, 1, 0]), 1, 1], device=d) for im in imgs]  # normalizations",
        "core_API": "tensor"
    },
    {
        "commit_hash": "894f21daaab90a083d6f193426e551333d20a6bf",
        "index": "9c45b58a..5ccccb5c 100644",
        "commit_message": "Use odd shape tensor to represent parameter data in partitioned state (#981)\n\n* use wierd shaped tensor to avoid silent failures when not registering externel params\n\n* fix typo\n\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_scatter_gather():",
            "with deepspeed.zero.Init():",
            "l = torch.nn.Linear(6, 3)",
            "assert l.weight.ds_status == ZeroParamStatus.NOT_AVAILABLE",
            "-    assert l.weight.numel() == 1",
            "+    assert l.weight.shape == torch.Size(partitioned_param_data_shape)",
            "",
            "# Ensure there is no impact outside the context",
            "l2 = torch.nn.Linear(6, 3)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=comparison_operator), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('==', '=='), position=1, insert_id=83856)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('call', None), position=2, insert_id=83857)",
            "Update(target_node=ASTNode(type=identifier, text=numel), value='shape')",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=83858)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=83859)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=83860)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Size'), position=2, insert_id=83861)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'partitioned_param_data_shape'), position=1, insert_id=83862)",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type===, text===))",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 1691,
        "neg_line": [
            "-assert l.weight.numel() == 1"
        ],
        "pos_line": [
            "+assert l.weight.shape == torch.Size(partitioned_param_data_shape)"
        ],
        "core_change": "-assert l.weight.numel() == 1 +assert l.weight.shape == torch.Size(partitioned_param_data_shape)",
        "core_API": "Init"
    },
    {
        "commit_hash": "4b40c34c532f3d3f8b2222a19ba5038eb61a1ee4",
        "index": "7567dcfa..6641062a 100644",
        "commit_message": "Fix test_call.py docstring\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class TestCall(unittest.TestCase):",
            "x = np.random.randn(nb_samples, input_dim).astype(floatX)",
            "y1 = F(x)",
            "y2 = model.predict(x)",
            "+        # results of __call__ should match model.predict",
            "assert_allclose(y1, y2)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 1694,
        "neg_line": [],
        "pos_line": [
            "+# results of __call__ should match model.predict"
        ],
        "core_change": "+# results of __call__ should match model.predict",
        "core_API": "randn"
    },
    {
        "commit_hash": "673cb60808a23f1f925bc445047720df3f0ff689",
        "index": "80b1ee34..daaf0d1d 100755",
        "commit_message": "Improve z3 trace management (#1916)\n\n* Fix OOM and type mismatch\n\n* Toggle prefetching\n\n* Disable z3 prefetching for inference (temp workaround)\n\n* Fix zero3 tracing issues\n\n* Remove debug prints\n\n* Enable prefetch for inference\n\n* Code clarity\n\n* Invalidate trace cache\n\n* Trace cache invalidation when needed\nSeparate nvme prefetch from all-gather prefetch\n\n* Track last used step id\n\n* Use debug name in error message\n\n* Construct param trace from module trace\n\nCo-authored-by: Jeff Rasley <jerasley@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Init(InsertPostInitMethodToModuleSubClasses):",
            "param.all_gather()",
            "return param._orig_item()",
            "",
            "-        def ds_summary(slf: torch.Tensor) -> dict:",
            "+        def ds_summary(slf: torch.Tensor, use_debug_name: bool = False) -> dict:",
            "return {",
            "-                \"id\": slf.ds_id,",
            "+                \"id\": debug_param2name_id(slf) if use_debug_name else slf.ds_id,",
            "\"status\": slf.ds_status.name,",
            "\"numel\": slf.numel(),",
            "\"ds_numel\": slf.ds_numel,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=2, insert_id=78619)",
            "Insert(target_node=ASTNode(type=parameters), node=('typed_default_parameter', None), position=3, insert_id=78620)",
            "Insert(target_node=ASTNode(type=pair), node=('conditional_expression', None), position=2, insert_id=78621)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('identifier', 'use_debug_name'), position=0, insert_id=78622)",
            "Insert(target_node=IN(type=typed_default_parameter), node=(':', ':'), position=1, insert_id=78623)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('type', None), position=2, insert_id=78624)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('=', '='), position=3, insert_id=78625)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('false', 'False'), position=4, insert_id=78626)",
            "Insert(target_node=IN(type=conditional_expression), node=('call', None), position=0, insert_id=78627)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=78628)",
            "Insert(target_node=IN(type=conditional_expression), node=('identifier', 'use_debug_name'), position=2, insert_id=78629)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=78630)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=attribute), position=4)",
            "Insert(target_node=IN(type=type), node=('identifier', 'bool'), position=0, insert_id=78631)",
            "Insert(target_node=IN(type=call), node=('identifier', 'debug_param2name_id'), position=0, insert_id=78632)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=78633)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=78634)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'slf'), position=1, insert_id=78635)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=78636)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 1697,
        "neg_line": [
            "-def ds_summary(slf: torch.Tensor) -> dict:",
            "-\"id\": slf.ds_id,"
        ],
        "pos_line": [
            "+def ds_summary(slf: torch.Tensor, use_debug_name: bool = False) -> dict:",
            "+\"id\": debug_param2name_id(slf) if use_debug_name else slf.ds_id,"
        ],
        "core_change": "-def ds_summary(slf: torch.Tensor) -> dict: +def ds_summary(slf: torch.Tensor, use_debug_name: bool = False) -> dict: -\"id\": slf.ds_id, +\"id\": debug_param2name_id(slf) if use_debug_name else slf.ds_id,",
        "core_API": "all_gather"
    },
    {
        "commit_hash": "cb339804f275c164a5f3a3eff9a5cac19b7031c3",
        "index": "33e6d1b8..d7b3432d 100644",
        "commit_message": "Fix codacy issues (continue) (#349)\n\n* fixed rein.py | try Exception\n\n* fixed deformable conv2d bug (#346)\n\n* improve docs\n\n* fixed bug of deformable conv2d\n\n* 1) try exception with Exception / 2) use enumerate instead of range(l (#347)\n\n* 1) try exception with Exception / 2) use enumerate instead of range(len())\n\n* remove unused import in tfrecord3.py example\n\n* try Exception in tfrecord3.py\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class DeformableConv2d(Layer):",
            "offset_params = [osparam for osparam in offset_layer.all_params if osparam not in layer.all_params]",
            "offset_layers = [oslayer for oslayer in offset_layer.all_layers if oslayer not in layer.all_layers]",
            "",
            "-        self.all_params.extend(offset_params)",
            "-        self.all_layers.extend(offset_layers)",
            "-        self.all_drop.update(offset_layer.all_drop)",
            "+        self.all_params.extend(list(offset_params))",
            "+        self.all_layers.extend(list(offset_layers))",
            "+        self.all_drop.update(dict(offset_layer.all_drop))",
            "",
            "# this layer",
            "self.all_layers.extend([self.outputs])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2635711)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2635712)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2635713)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2635714)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=2635715)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2635716)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2635717)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=2635718)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2635719)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2635720)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=2635721)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2635722)",
            "Insert(target_node=IN(type=call), node=('identifier', 'list'), position=0, insert_id=2635723)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=call), node=('identifier', 'list'), position=0, insert_id=2635724)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=call), node=('identifier', 'dict'), position=0, insert_id=2635725)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 1703,
        "neg_line": [
            "-self.all_params.extend(offset_params)",
            "-self.all_layers.extend(offset_layers)",
            "-self.all_drop.update(offset_layer.all_drop)"
        ],
        "pos_line": [
            "+self.all_params.extend(list(offset_params))",
            "+self.all_layers.extend(list(offset_layers))",
            "+self.all_drop.update(dict(offset_layer.all_drop))"
        ],
        "core_change": "-self.all_params.extend(offset_params) -self.all_layers.extend(offset_layers) -self.all_drop.update(offset_layer.all_drop) +self.all_params.extend(list(offset_params)) +self.all_layers.extend(list(offset_layers)) +self.all_drop.update(dict(offset_layer.all_drop))",
        "core_API": "extend"
    },
    {
        "commit_hash": "77b75d2c78757ab5d9b198b574a7c1ec05dd3219",
        "index": "86882bb72..2028e9c7d 100644",
        "commit_message": "Fix for #3873 to change type of exponent parameter for torch.pow() call from int to float (#3924)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def gelu_new(x):",
            "\"\"\" Implementation of the gelu activation function currently in Google Bert repo (identical to OpenAI GPT).",
            "Also see https://arxiv.org/abs/1606.08415",
            "\"\"\"",
            "-    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))",
            "+    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))",
            "",
            "",
            "if torch.__version__ < \"1.4.0\":"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('float', '3.0'), position=3, insert_id=1240037)",
            "Delete(target_node=ASTNode(type=integer, text=3))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1704,
        "neg_line": [
            "-return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
        ],
        "pos_line": [
            "+return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
        ],
        "core_change": "-return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) +return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))",
        "core_API": "tanh"
    },
    {
        "commit_hash": "9b6385488617ee99188b60c0525cbe4ec7e8f2a0",
        "index": "3d9e18ca..7a190370 100644",
        "commit_message": "Improve reproduceability 2/3 (#1906)\n\n* [Repro] Correct reproducability\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* need better image\n\n* allow conversion from no state dict checkpoints\n\n* up\n\n* up\n\n* up\n\n* up\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* next try\n\n* up\n\n* up\n\n* better name\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\n* correct more\n\n* up\n\n* replace all torch randn\n\n* fix\n\n* correct\n\n* correct\n\n* finish\n\n* fix more\n\n* up\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ScoreSdeVeScheduler(SchedulerMixin, ConfigMixin):",
            "",
            "# For small batch sizes, the paper \"suggest replacing norm(z) with sqrt(d), where d is the dim. of z\"",
            "# sample noise for correction",
            "-        noise = torch.randn(sample.shape, layout=sample.layout, generator=generator).to(sample.device)",
            "+        noise = randn_tensor(sample.shape, layout=sample.layout, generator=generator).to(sample.device)",
            "",
            "# compute step size from the model_output, the noise, and the snr",
            "grad_norm = torch.norm(model_output.reshape(model_output.shape[0], -1), dim=-1).mean()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='randn_tensor')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=randn))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1707,
        "neg_line": [
            "-noise = torch.randn(sample.shape, layout=sample.layout, generator=generator).to(sample.device)"
        ],
        "pos_line": [
            "+noise = randn_tensor(sample.shape, layout=sample.layout, generator=generator).to(sample.device)"
        ],
        "core_change": "-noise = torch.randn(sample.shape, layout=sample.layout, generator=generator).to(sample.device) +noise = randn_tensor(sample.shape, layout=sample.layout, generator=generator).to(sample.device)",
        "core_API": "randn"
    },
    {
        "commit_hash": "525f4f86a92a8037aec9af34460ffa2b83f1d404",
        "index": "acf8362..3415088 100644",
        "commit_message": "Add --optimize argument (#3093)\n\nFix for c++ runtime errors in https://github.com/ultralytics/yolov5/issues/2973\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "if __name__ == '__main__':",
            "import coremltools as ct",
            "",
            "print(f'{prefix} starting export with coremltools {ct.__version__}...')",
            "-        # convert model from torchscript and apply pixel scaling as per detect.py",
            "model = ct.convert(ts, inputs=[ct.ImageType(name='image', shape=img.shape, scale=1 / 255.0, bias=[0, 0, 0])])",
            "f = opt.weights.replace('.pt', '.mlmodel')  # filename",
            "model.save(f)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1709,
        "neg_line": [
            "-# convert model from torchscript and apply pixel scaling as per detect.py"
        ],
        "pos_line": [],
        "core_change": "-# convert model from torchscript and apply pixel scaling as per detect.py",
        "core_API": "convert"
    },
    {
        "commit_hash": "1a81670718652541f5040db73b1dbbf7565f1129",
        "index": "ca3eba7..faa7bc1 100644",
        "commit_message": "fix quadratic_beta_schedule (#141)\n\n\n",
        "file": "DALLE2-pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def quadratic_beta_schedule(timesteps):",
            "scale = 1000 / timesteps",
            "beta_start = scale * 0.0001",
            "beta_end = scale * 0.02",
            "-    return torch.linspace(beta_start**2, beta_end**2, timesteps, dtype = torch.float64) ** 2",
            "+    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps, dtype = torch.float64) ** 2",
            "",
            "",
            "def sigmoid_beta_schedule(timesteps):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('float', '0.5'), position=2, insert_id=63917)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('float', '0.5'), position=2, insert_id=63918)",
            "Delete(target_node=ASTNode(type=integer, text=2))",
            "Delete(target_node=ASTNode(type=integer, text=2))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1710,
        "neg_line": [
            "-return torch.linspace(beta_start**2, beta_end**2, timesteps, dtype = torch.float64) ** 2"
        ],
        "pos_line": [
            "+return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps, dtype = torch.float64) ** 2"
        ],
        "core_change": "-return torch.linspace(beta_start**2, beta_end**2, timesteps, dtype = torch.float64) ** 2 +return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps, dtype = torch.float64) ** 2",
        "core_API": "linspace"
    },
    {
        "commit_hash": "ca257a06cc42e3345a1500391e1b1d7742e18ae0",
        "index": "1aa01eb56..87ebaa22e 100644",
        "commit_message": "Fix torchscript tests (#13701)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DistilBertModelTest(ModelTesterMixin, unittest.TestCase):",
            "",
            "with tempfile.TemporaryDirectory() as tmp:",
            "torch.jit.save(traced_model, os.path.join(tmp, \"traced_model.pt\"))",
            "-                loaded = torch.jit.load(os.path.join(tmp, \"bert.pt\"), map_location=torch_device)",
            "+                loaded = torch.jit.load(os.path.join(tmp, \"traced_model.pt\"), map_location=torch_device)",
            "loaded(inputs_dict[\"input_ids\"].to(torch_device), inputs_dict[\"attention_mask\"].to(torch_device))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"bert.pt\"), value='\"traced_model.pt\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1711,
        "neg_line": [
            "-loaded = torch.jit.load(os.path.join(tmp, \"bert.pt\"), map_location=torch_device)"
        ],
        "pos_line": [
            "+loaded = torch.jit.load(os.path.join(tmp, \"traced_model.pt\"), map_location=torch_device)"
        ],
        "core_change": "-loaded = torch.jit.load(os.path.join(tmp, \"bert.pt\"), map_location=torch_device) +loaded = torch.jit.load(os.path.join(tmp, \"traced_model.pt\"), map_location=torch_device)",
        "core_API": "TemporaryDirectory"
    },
    {
        "commit_hash": "f38aaa5be05104ddfc3452249eeb521f139ded99",
        "index": "9cf80686..dba496f9 100755",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Sequence(Preprocessor):",
            "def later_run():",
            "return tf.assign(ref=states_buffer[index], value=tensor[0])",
            "",
            "-        assignment = tf.cond(pred=(index >= 0), true_fn=later_run, false_fn=first_run)",
            "+        assignment = self.cond(pred=(index >= 0), true_fn=later_run, false_fn=first_run)",
            "",
            "with tf.control_dependencies(control_inputs=(assignment,)):",
            "previous_states = [states_buffer[(index - n - 1) % self.length] for n in range(self.length)]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='self')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1712,
        "neg_line": [
            "-assignment = tf.cond(pred=(index >= 0), true_fn=later_run, false_fn=first_run)"
        ],
        "pos_line": [
            "+assignment = self.cond(pred=(index >= 0), true_fn=later_run, false_fn=first_run)"
        ],
        "core_change": "-assignment = tf.cond(pred=(index >= 0), true_fn=later_run, false_fn=first_run) +assignment = self.cond(pred=(index >= 0), true_fn=later_run, false_fn=first_run)",
        "core_API": "assign"
    },
    {
        "commit_hash": "dab39f477f6927d5dd80c9b1008a24e16d5673fc",
        "index": "a7468cf9..364993a7 100644",
        "commit_message": "Motion blur (#2075)\n\n* fixed motion blur\n\n* silence typing test\n\n* fixed test_motionblur\n\n* Update kornia/augmentation/_2d/intensity/motion_blur.py\n\nCo-authored-by: Joo Gustavo A. Amorim <joaogustavoamorim@gmail.com>\n\nCo-authored-by: Nitai Fingerhut <nitai@Nitais-MacBook-Pro-3.local>\nCo-authored-by: Joo Gustavo A. Amorim <joaogustavoamorim@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestRandomMotionBlur3D:",
            "'angle_factor': torch.tensor([[30.0, 30.0, 30.0]]),",
            "'direction_factor': torch.tensor([-0.5]),",
            "'border_type': torch.tensor([0]),",
            "+            'idx': torch.tensor([0]),",
            "}",
            "assert gradcheck(",
            "RandomMotionBlur3D(kernel_size=3, angle=(10, 30), direction=(-0.5, 0.5), p=1.0),"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=394645)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=4, insert_id=394646)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=394647)",
            "Insert(target_node=IN(type=expression_statement), node=(',', ','), position=1, insert_id=394648)",
            "Insert(target_node=IN(type=ERROR), node=('string', \"'idx'\"), position=0, insert_id=394649)",
            "Insert(target_node=IN(type=ERROR), node=(':', ':'), position=1, insert_id=394650)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=394651)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=394652)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=string, text='border_type'), position=0)",
            "Insert(target_node=IN(type=attribute), node=('ERROR', None), position=1, insert_id=394653)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=2, insert_id=394654)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=3, insert_id=394655)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=394656)",
            "Insert(target_node=IN(type=argument_list), node=('list', None), position=1, insert_id=394657)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=394658)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=:, text=:), position=0)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'torch'), position=1, insert_id=394659)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=394660)",
            "Insert(target_node=IN(type=list), node=('integer', '0'), position=1, insert_id=394661)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=2, insert_id=394662)",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 21,
        "number": 1718,
        "neg_line": [],
        "pos_line": [
            "+'idx': torch.tensor([0]),"
        ],
        "core_change": "+'idx': torch.tensor([0]),",
        "core_API": "tensor"
    },
    {
        "commit_hash": "05255f96410e5b1eaf3bf59b767d5b4b7e2c3a35",
        "index": "fa0d4596..3f96ffc4 100644",
        "commit_message": "update audio_utils and fix mTEDx example\n\nSummary:\nupdate audio_utils and fix mTEDx example\n- Updated `audio_utils`\n  - Added support for OGG Vorbis (the only supported lossy compressed format)\n  - Added a separate `convert_to_mono()` helper function\n  - Updated `get_waveform()`\n    - added new arguments `frames` and `start` for reading part of audios\n    - added new argument `mono` for auto conversion to mono-channel audio\n    - unified returned waveform shape to channels x length (same as torchaudio default)\n- Updated mTEDx and MUST-C data prep scripts\n  - Replaced `torchaudio.info()` with `soundfile.info()` (the latter is faster and the former has incompatible interface between <0.8 and the latest 0.8)\n  - Replaced `torchaudio.load()` with `get_waveform` for auto conversion to mono channel\n\nReviewed By: jmp84\n\nDifferential Revision: D26901114\n\nfbshipit-source-id: fa9560c9714d51a91157d5141564574d4eee454d\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def extract_fbank_features(",
            "if output_path is not None and output_path.is_file() and not overwrite:",
            "return",
            "",
            "-    _waveform = waveform * (2 ** 15)  # Kaldi compliance: 16-bit signed integers",
            "-    _waveform = _waveform.squeeze().numpy()",
            "+    _waveform = _convert_to_mono(waveform, sample_rate)",
            "+    _waveform = _waveform * (2 ** 15)  # Kaldi compliance: 16-bit signed integers",
            "+    _waveform = _waveform.numpy()",
            "",
            "features = _get_kaldi_fbank(_waveform, sample_rate, n_mel_bins)",
            "if features is None:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=208106)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=208107)",
            "Insert(target_node=IN(type=assignment), node=('identifier', '_waveform'), position=0, insert_id=208108)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=208109)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=208110)",
            "Insert(target_node=IN(type=call), node=('identifier', '_convert_to_mono'), position=0, insert_id=208111)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=208112)",
            "Update(target_node=ASTNode(type=identifier, text=waveform), value='_waveform')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=208113)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'waveform'), position=1, insert_id=208114)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=208115)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'sample_rate'), position=3, insert_id=208116)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=208117)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=_waveform), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=squeeze))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 1719,
        "neg_line": [
            "-_waveform = waveform * (2 ** 15)  # Kaldi compliance: 16-bit signed integers",
            "-_waveform = _waveform.squeeze().numpy()"
        ],
        "pos_line": [
            "+_waveform = _convert_to_mono(waveform, sample_rate)",
            "+_waveform = _waveform * (2 ** 15)  # Kaldi compliance: 16-bit signed integers",
            "+_waveform = _waveform.numpy()"
        ],
        "core_change": "-_waveform = waveform * (2 ** 15)  # Kaldi compliance: 16-bit signed integers -_waveform = _waveform.squeeze().numpy() +_waveform = _convert_to_mono(waveform, sample_rate) +_waveform = _waveform * (2 ** 15)  # Kaldi compliance: 16-bit signed integers +_waveform = _waveform.numpy()",
        "core_API": "is_file"
    },
    {
        "commit_hash": "04e997fe0da22b8190da38e0d3224d9ccdf57218",
        "index": "bae7c1d7d9..d0acdaa7c7 100644",
        "commit_message": "Fix TF2 / rllib test (#5846)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "class MADDPGTFPolicy(MADDPGPostprocessing, TFPolicy):",
            "out = tf.layers.dense(out, units=hidden, activation=activation)",
            "feature = tf.layers.dense(",
            "out, units=act_space.shape[0], activation=None)",
            "-            sampler = RelaxedOneHotCategorical(",
            "+            sampler = tfp.distributions.RelaxedOneHotCategorical(",
            "temperature=1.0, logits=feature).sample()",
            "",
            "return sampler, feature, model, tf.global_variables(scope.name)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2454127)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2454128)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2454129)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=RelaxedOneHotCategorical), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tfp'), position=0, insert_id=2454130)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2454131)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'distributions'), position=2, insert_id=2454132)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 1721,
        "neg_line": [
            "-sampler = RelaxedOneHotCategorical("
        ],
        "pos_line": [
            "+sampler = tfp.distributions.RelaxedOneHotCategorical("
        ],
        "core_change": "-sampler = RelaxedOneHotCategorical( +sampler = tfp.distributions.RelaxedOneHotCategorical(",
        "core_API": "dense"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "8b02de40..98d713f4 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Entropy(Metric):",
            "mask: ``torch.Tensor``, optional (default = None).",
            "A masking tensor of shape (batch_size, ...).",
            "\"\"\"",
            "-        # Get the data from the Variables.",
            "logits, mask = self.unwrap_to_tensors(logits, mask)",
            "",
            "if mask is None:",
            "mask = torch.ones(logits.size()[:-1])",
            "",
            "-        log_probs = torch.nn.functional.log_softmax(Variable(logits), dim=-1).data",
            "+        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)",
            "probabilities = torch.exp(log_probs) * mask.unsqueeze(-1)",
            "weighted_negative_likelihood = - log_probs * probabilities",
            "entropy = weighted_negative_likelihood.sum(-1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=logits), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=data))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 1724,
        "neg_line": [
            "-# Get the data from the Variables.",
            "-log_probs = torch.nn.functional.log_softmax(Variable(logits), dim=-1).data"
        ],
        "pos_line": [
            "+log_probs = torch.nn.functional.log_softmax(logits, dim=-1)"
        ],
        "core_change": "-# Get the data from the Variables. -log_probs = torch.nn.functional.log_softmax(Variable(logits), dim=-1).data +log_probs = torch.nn.functional.log_softmax(logits, dim=-1)",
        "core_API": "unwrap_to_tensors"
    },
    {
        "commit_hash": "388cbf171d79933522560296368ff1b64cf60537",
        "index": "bd473e85..b88d75ed 100644",
        "commit_message": "Fix Theano tests.\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "def update_confusion_matrix_variables(variables_to_update,",
            "# Tile the thresholds for every prediction.",
            "thresh_tiled = K.tile(",
            "K.expand_dims(K.constant(thresholds), 1),",
            "-        K.stack([1, num_predictions]))",
            "+        K.cast(",
            "+            K.stack([1, num_predictions]),",
            "+            dtype='int32',",
            "+        )",
            "+    )",
            "",
            "# Tile the predictions for every threshold.",
            "preds_tiled = K.tile(predictions_2d, [num_thresholds, 1])"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2451082)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2451083)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'K'), position=0, insert_id=2451084)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2451085)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=2, insert_id=2451086)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2451087)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2451088)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2451089)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=2451090)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=5, insert_id=2451091)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2451092)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2451093)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'int32'\"), position=2, insert_id=2451094)"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 1726,
        "neg_line": [
            "-K.stack([1, num_predictions]))"
        ],
        "pos_line": [
            "+K.cast(",
            "+K.stack([1, num_predictions]),",
            "+dtype='int32',",
            "+)",
            "+)"
        ],
        "core_change": "-K.stack([1, num_predictions])) +K.cast( +K.stack([1, num_predictions]), +dtype='int32', +) +)",
        "core_API": "tile"
    },
    {
        "commit_hash": "8e76796fd0535d8fbc9934e7f6604a1abcfb119e",
        "index": "9fa570db57..6e6a32c682 100644",
        "commit_message": "ci: Redo `format.sh --all` script & backfill lint fixes (#9956)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class NoisyLayer(tf.keras.layers.Layer if tf else object):",
            "trainable=True,",
            "tf_name=self.prefix + \"_sigma_w\",",
            "shape=[in_size, self.out_size],",
            "-            dtype=tf.float32",
            "-        )",
            "+            dtype=tf.float32)",
            "",
            "self.sigma_b = get_variable(",
            "value=tf.keras.initializers.Constant("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=2144219)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2144220)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=dtype), position=0)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type==, text==), position=1)",
            "Insert(target_node=ASTNode(type=ERROR), node=('ERROR', None), position=2, insert_id=2144221)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=identifier, text=shape), position=0)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type==, text==), position=1)",
            "Insert(target_node=IN(type=assignment), node=('expression_list', None), position=2, insert_id=2144222)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=), text=)), position=1)",
            "Insert(target_node=IN(type=expression_list), node=('list', None), position=0, insert_id=2144223)",
            "Move(target_node=IN(type=expression_list), node=ASTNode(type=,, text=,), position=1)",
            "Move(target_node=IN(type=list), node=ASTNode(type=[, text=[), position=0)",
            "Move(target_node=IN(type=list), node=ASTNode(type=identifier, text=in_size), position=1)",
            "Move(target_node=IN(type=list), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=list), node=ASTNode(type=attribute), position=3)",
            "Move(target_node=IN(type=list), node=ASTNode(type=], text=]), position=4)",
            "Delete(target_node=ASTNode(type=list_pattern))",
            "Delete(target_node=ASTNode(type=pattern_list))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 1730,
        "neg_line": [
            "-dtype=tf.float32",
            "-)"
        ],
        "pos_line": [
            "+dtype=tf.float32)"
        ],
        "core_change": "-dtype=tf.float32 -) +dtype=tf.float32)",
        "core_API": "Constant"
    },
    {
        "commit_hash": "a00cd8b9c1e9866a58d135f3b64cc7e0f29c6d47",
        "index": "a7060f58..4018edcc 100644",
        "commit_message": "attempt to fix memory monitor with multiple CUDA devices\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class MemUsageMonitor(threading.Thread):",
            "",
            "def read(self):",
            "if not self.disabled:",
            "-            free, total = torch.cuda.mem_get_info()",
            "+            free, total = self.cuda_mem_get_info()",
            "self.data[\"free\"] = free",
            "self.data[\"total\"] = total"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='self')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=cuda), value='cuda_mem_get_info')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=cuda), position=2)",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=mem_get_info))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1731,
        "neg_line": [
            "-free, total = torch.cuda.mem_get_info()"
        ],
        "pos_line": [
            "+free, total = self.cuda_mem_get_info()"
        ],
        "core_change": "-free, total = torch.cuda.mem_get_info() +free, total = self.cuda_mem_get_info()",
        "core_API": "mem_get_info"
    },
    {
        "commit_hash": "103d65f2e96371b401c8b78ef0ee1795abdf466a",
        "index": "d2a5e5a..3ef637c 100644",
        "commit_message": "fix something nonsensical when sampling random times, thanks to @jtawade\n\n",
        "file": "imagen-pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GaussianDiffusionContinuousTimes(nn.Module):",
            "def get_times(self, batch_size, noise_level, *, device):",
            "return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)",
            "",
            "-    def sample_random_times(self, batch_size, max_thres = 0.999, *, device):",
            "-        return torch.zeros((batch_size,), device = device).float().uniform_(0, max_thres)",
            "+    def sample_random_times(self, batch_size, *, device):",
            "+        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)",
            "",
            "def get_condition(self, times):",
            "return maybe(self.log_snr)(times)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '1'), position=3, insert_id=252520)",
            "Delete(target_node=ASTNode(type=identifier, text=max_thres))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=float, text=0.999))",
            "Delete(target_node=ASTNode(type=default_parameter))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=max_thres))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 1733,
        "neg_line": [
            "-def sample_random_times(self, batch_size, max_thres = 0.999, *, device):",
            "-return torch.zeros((batch_size,), device = device).float().uniform_(0, max_thres)"
        ],
        "pos_line": [
            "+def sample_random_times(self, batch_size, *, device):",
            "+return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)"
        ],
        "core_change": "-def sample_random_times(self, batch_size, max_thres = 0.999, *, device): -return torch.zeros((batch_size,), device = device).float().uniform_(0, max_thres) +def sample_random_times(self, batch_size, *, device): +return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)",
        "core_API": "full"
    },
    {
        "commit_hash": "31563e056da7a8813071022d465384748d395d30",
        "index": "478400ff8..94d4e814f 100644",
        "commit_message": "Restore TF embeddings and attention layers to their previous version (#9890)\n\n* Refacto BERT\n\n* Restore all the concerned models\n\n* Remove print\n\n* Update template\n\n* Apply Sylvain's and Morgan's comments\n\n* Fix cast\n\n* Put the cast inside call\n\n* Remove cond in ebds\n\n* Fix funnel\n\n* Restore previous dot product (attention_scores) computation\n\n* Add ConvBERT and BART\n\n* Make all the S2S models ONNX compliant\n\n* Fix test\n\n* Fix check copies\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _expand_mask(mask: tf.Tensor, tgt_len: Optional[int] = None, past_key_values",
            "\"\"\"",
            "Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.",
            "\"\"\"",
            "-    bsz, src_len = shape_list(mask)",
            "+    src_len = shape_list(mask)[1]",
            "tgt_len = tgt_len if tgt_len is not None else src_len",
            "-",
            "-    expanded_mask = tf.cast(tf.broadcast_to(mask[:, None, None, :], (bsz, 1, tgt_len, src_len)), tf.float32)",
            "+    expanded_mask = tf.cast(tf.tile(mask[:, None, None, :], (1, 1, tgt_len, 1)), tf.float32)",
            "",
            "return (1.0 - expanded_mask) * LARGE_NEGATIVE"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=src_len), position=0)",
            "Insert(target_node=ASTNode(type=assignment), node=('subscript', None), position=3, insert_id=2372451)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2372452)",
            "Insert(target_node=IN(type=subscript), node=('integer', '1'), position=2, insert_id=2372453)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2372454)",
            "Update(target_node=ASTNode(type=identifier, text=broadcast_to), value='tile')",
            "Insert(target_node=ASTNode(type=tuple), node=('integer', '1'), position=1, insert_id=2372455)",
            "Insert(target_node=ASTNode(type=tuple), node=('integer', '1'), position=8, insert_id=2372456)",
            "Delete(target_node=ASTNode(type=identifier, text=bsz))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=pattern_list))",
            "Delete(target_node=ASTNode(type=identifier, text=bsz))",
            "Delete(target_node=ASTNode(type=identifier, text=src_len))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 1734,
        "neg_line": [
            "-bsz, src_len = shape_list(mask)",
            "-",
            "-expanded_mask = tf.cast(tf.broadcast_to(mask[:, None, None, :], (bsz, 1, tgt_len, src_len)), tf.float32)"
        ],
        "pos_line": [
            "+src_len = shape_list(mask)[1]",
            "+expanded_mask = tf.cast(tf.tile(mask[:, None, None, :], (1, 1, tgt_len, 1)), tf.float32)"
        ],
        "core_change": "-bsz, src_len = shape_list(mask) +src_len = shape_list(mask)[1] - -expanded_mask = tf.cast(tf.broadcast_to(mask[:, None, None, :], (bsz, 1, tgt_len, src_len)), tf.float32) +expanded_mask = tf.cast(tf.tile(mask[:, None, None, :], (1, 1, tgt_len, 1)), tf.float32)",
        "core_API": "cast"
    },
    {
        "commit_hash": "9f3ccaa60c6eb1ed7877b07f3863d418efaf875f",
        "index": "b19fc5c3..a6dd6711 100644",
        "commit_message": "Fix weight norm dimension in decoder (fixes #73)\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def LinearizedConv1d(in_channels, out_channels, kernel_size, dropout=0, **kwargs",
            "std = math.sqrt((4 * (1.0 - dropout)) / (m.kernel_size[0] * in_channels))",
            "m.weight.data.normal_(mean=0, std=std)",
            "m.bias.data.zero_()",
            "-    return nn.utils.weight_norm(m)",
            "+    return nn.utils.weight_norm(m, dim=2)",
            "",
            "",
            "def ConvTBC(in_channels, out_channels, kernel_size, dropout=0, **kwargs):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=225947)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=225948)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dim'), position=0, insert_id=225949)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=225950)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '2'), position=2, insert_id=225951)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1735,
        "neg_line": [
            "-return nn.utils.weight_norm(m)"
        ],
        "pos_line": [
            "+return nn.utils.weight_norm(m, dim=2)"
        ],
        "core_change": "-return nn.utils.weight_norm(m) +return nn.utils.weight_norm(m, dim=2)",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "01d11e7df5bcc494fb337967cb082d09536922a3",
        "index": "f7af3522..294b4cb0 100644",
        "commit_message": "Remove unused `type: ignore` (#1998)\n\n* Remove unused `type: ignore`\n\n- Solve some previous type annotations errors\n- Add `warn_unused_ignores = True` to mypy config\n- Fix all unused `type: ignore` pointed out by mypy\n\n* fix typing for old torch\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def load_image_to_tensor(path_file: str, device: str) -> Tensor:",
            "# for convenience use the torch dlpack parser to get a zero copy torch.Tensor",
            "# TODO: evaluate other potential API so that we can return in numpy, jax, mxnet since",
            "# the kornia_rs cv::Tensor has this ability.",
            "-    th_tensor = torch.utils.dlpack.from_dlpack(cv_tensor)  # type: ignore # HxWx3",
            "+    th_tensor = dlpack.from_dlpack(cv_tensor)  # HxWx3",
            "# move the tensor to the desired device, move the data layout to CHW and clone",
            "# to return an owned data tensor.",
            "return th_tensor.to(torch.device(device)).permute(2, 0, 1).clone()  # CxHxW"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=dlpack), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=identifier, text=utils))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1736,
        "neg_line": [
            "-th_tensor = torch.utils.dlpack.from_dlpack(cv_tensor)  # type: ignore # HxWx3"
        ],
        "pos_line": [
            "+th_tensor = dlpack.from_dlpack(cv_tensor)  # HxWx3"
        ],
        "core_change": "-th_tensor = torch.utils.dlpack.from_dlpack(cv_tensor)  # type: ignore # HxWx3 +th_tensor = dlpack.from_dlpack(cv_tensor)  # HxWx3",
        "core_API": "from_dlpack"
    },
    {
        "commit_hash": "f7cf6efc4203a34099b4cd09e7f726c0a5783228",
        "index": "f06a7a0c..5823de29 100644",
        "commit_message": "Fix gradient clipping typo (#3039)\n\n\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer(BaseTrainer):",
            "if self.gradient_clipping_config.clipglobalnorm:",
            "torch.nn.utils.clip_grad_norm_(variables, self.gradient_clipping_config.clipglobalnorm)",
            "if self.gradient_clipping_config.clipnorm:",
            "-            torch.nn.utils.clip_grad_norm_(variables, self.gradient_clipping_config.clipglobalnorm)",
            "+            torch.nn.utils.clip_grad_norm_(variables, self.gradient_clipping_config.clipnorm)",
            "if self.gradient_clipping_config.clipvalue:",
            "torch.nn.utils.clip_grad_value_(variables, self.gradient_clipping_config.clipvalue)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=clipglobalnorm), value='clipnorm')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1737,
        "neg_line": [
            "-torch.nn.utils.clip_grad_norm_(variables, self.gradient_clipping_config.clipglobalnorm)"
        ],
        "pos_line": [
            "+torch.nn.utils.clip_grad_norm_(variables, self.gradient_clipping_config.clipnorm)"
        ],
        "core_change": "-torch.nn.utils.clip_grad_norm_(variables, self.gradient_clipping_config.clipglobalnorm) +torch.nn.utils.clip_grad_norm_(variables, self.gradient_clipping_config.clipnorm)",
        "core_API": "clip_grad_norm_"
    },
    {
        "commit_hash": "515ae13504444fcd7cd2e537430e0aa1fde36684",
        "index": "2b05948..2e4636f 100644",
        "commit_message": "Use snt.distribute.Replicator for the checkpoint tests instead of tf.distribute.MirroredStrategy\n\nAlso requires adding a temporary fix for read_value on the variables\n\nPiperOrigin-RevId: 251429682\nChange-Id: If78191b838927ada7fb38291d7d93b461b88a004\n\n",
        "file": "sonnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DistributionStrategyCheckpointTest(test_utils.TestCase,",
            "",
            "def assertRestoreOnCreateInReplicaContext(self, golden, strategy,",
            "use_function):",
            "+    if self.primary_device == \"GPU\":",
            "+      self.skipTest(\"Currently not working as expected on multiple devices\")",
            "+      # TODO(b/134376796) renable this once bug is fixed",
            "with strategy.scope():",
            "module = golden.create_module()"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=2186288)",
            "Insert(target_node=IN(type=block), node=('if_statement', None), position=0, insert_id=2186289)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2186290)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=2186291)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2186292)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=2186293)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=2186294)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=2186295)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', '\"GPU\"'), position=2, insert_id=2186296)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2186297)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2186298)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2186299)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'primary_device'), position=2, insert_id=2186300)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2186301)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2186302)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2186303)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2186304)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2186305)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'skipTest'), position=2, insert_id=2186306)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2186307)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"Currently not working as expected on multiple devices\"'), position=1, insert_id=2186308)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2186309)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 3,
        "minus_line": 0,
        "AST_diff_line": 23,
        "number": 1739,
        "neg_line": [],
        "pos_line": [
            "+if self.primary_device == \"GPU\":",
            "+self.skipTest(\"Currently not working as expected on multiple devices\")",
            "+# TODO(b/134376796) renable this once bug is fixed"
        ],
        "core_change": "+if self.primary_device == \"GPU\": +self.skipTest(\"Currently not working as expected on multiple devices\") +# TODO(b/134376796) renable this once bug is fixed",
        "core_API": "skipTest"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "bb332b74..d75d6522 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BooleanAccuracyTest(AllenNlpTestCase):",
            "accuracy = BooleanAccuracy()",
            "predictions = torch.rand([5, 7], device=device)",
            "labels = torch.rand([5, 7], device=device)",
            "-        incorrect_shape_mask = torch.randint(0, 2, [5, 8], device=device)",
            "+        incorrect_shape_mask = torch.randint(0, 2, [5, 8], device=device).bool()",
            "with pytest.raises(ValueError):",
            "accuracy(predictions, labels, incorrect_shape_mask)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=20024)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=20025)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=20026)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=20027)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=20028)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=20029)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 1740,
        "neg_line": [
            "-incorrect_shape_mask = torch.randint(0, 2, [5, 8], device=device)"
        ],
        "pos_line": [
            "+incorrect_shape_mask = torch.randint(0, 2, [5, 8], device=device).bool()"
        ],
        "core_change": "-incorrect_shape_mask = torch.randint(0, 2, [5, 8], device=device) +incorrect_shape_mask = torch.randint(0, 2, [5, 8], device=device).bool()",
        "core_API": "rand"
    },
    {
        "commit_hash": "7c8daa2dcc613520636831132ebf243cea24be3f",
        "index": "6b1ebcc23..e51e4bee9 100644",
        "commit_message": "Refractor to fix no sequence error\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def perform_analysis_torch(preds, indices, noise_eps=0.1, delta=1e-5, moments=8,",
            "data_ind_eps_list = (data_ind_log_mgf - math.log(delta)) / l_list",
            "",
            "return min(eps_list_nm), min(data_ind_eps_list)",
            "+",
            "+",
            "+",
            "\\ No newline at end of file"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 1741,
        "neg_line": [],
        "pos_line": [
            "+",
            "+",
            "+"
        ],
        "core_change": "+ + +",
        "core_API": "log"
    },
    {
        "commit_hash": "e20b7e30fe17744acb74ad33c87c0963525ea921",
        "index": "c908b43e..03f6085e 100644",
        "commit_message": "fix for add difference model merging\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def run_modelmerger(primary_model_name, secondary_model_name, teritary_model_nam",
            "if theta_func1:",
            "for key in tqdm.tqdm(theta_1.keys()):",
            "if 'model' in key:",
            "-                t2 = theta_2.get(key, torch.zeros_like(theta_1[key]))",
            "-                theta_1[key] = theta_func1(theta_1[key], t2)",
            "+                if key in theta_2:",
            "+                    t2 = theta_2.get(key, torch.zeros_like(theta_1[key]))",
            "+                    theta_1[key] = theta_func1(theta_1[key], t2)",
            "+                else:",
            "+                    theta_1[key] = 0",
            "del theta_2, teritary_model",
            "",
            "for key in tqdm.tqdm(theta_0.keys()):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('if_statement', None), position=0, insert_id=1139555)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1139556)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1139557)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1139558)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=block), position=3)",
            "Insert(target_node=IN(type=if_statement), node=('else_clause', None), position=4, insert_id=1139559)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'key'), position=0, insert_id=1139560)",
            "Insert(target_node=IN(type=comparison_operator), node=('in', 'in'), position=1, insert_id=1139561)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'theta_2'), position=2, insert_id=1139562)",
            "Insert(target_node=IN(type=else_clause), node=('else', 'else'), position=0, insert_id=1139563)",
            "Insert(target_node=IN(type=else_clause), node=(':', ':'), position=1, insert_id=1139564)",
            "Insert(target_node=IN(type=else_clause), node=('block', None), position=2, insert_id=1139565)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1139566)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1139567)",
            "Insert(target_node=IN(type=assignment), node=('subscript', None), position=0, insert_id=1139568)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1139569)",
            "Insert(target_node=IN(type=assignment), node=('integer', '0'), position=2, insert_id=1139570)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'theta_1'), position=0, insert_id=1139571)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1139572)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'key'), position=2, insert_id=1139573)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1139574)"
        ],
        "plus_line": 5,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 1742,
        "neg_line": [
            "-t2 = theta_2.get(key, torch.zeros_like(theta_1[key]))",
            "-theta_1[key] = theta_func1(theta_1[key], t2)"
        ],
        "pos_line": [
            "+if key in theta_2:",
            "+t2 = theta_2.get(key, torch.zeros_like(theta_1[key]))",
            "+theta_1[key] = theta_func1(theta_1[key], t2)",
            "+else:",
            "+theta_1[key] = 0"
        ],
        "core_change": "-t2 = theta_2.get(key, torch.zeros_like(theta_1[key])) -theta_1[key] = theta_func1(theta_1[key], t2) +if key in theta_2: +t2 = theta_2.get(key, torch.zeros_like(theta_1[key])) +theta_1[key] = theta_func1(theta_1[key], t2) +else: +theta_1[key] = 0",
        "core_API": "tqdm"
    },
    {
        "commit_hash": "98cc35b6a8e53e15829ce64fd8da835db0c61da9",
        "index": "665802ce..c9a8807b 100755",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_lst_from_rank0(lst: List[int]) -> None:",
            "lst_tensor = torch.tensor(",
            "lst if dist.get_rank() == 0 else [-1] * len(lst),",
            "dtype=int,",
            "-        # device=torch.cuda.current_device(),",
            "-        device=torch.device('cuda:{}'.format(os.environ[\"LOCAL_RANK\"])),",
            "+        # device=get_accelerator().current_device_name(),",
            "+        device=torch.device(get_accelerator().device_name(os.environ[\"LOCAL_RANK\"])),",
            "requires_grad=False,",
            ")",
            "dist.broadcast(lst_tensor, src=0, async_op=False)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=75279)",
            "Update(target_node=ASTNode(type=identifier, text=format), value='device_name')",
            "Insert(target_node=IN(type=call), node=('identifier', 'get_accelerator'), position=0, insert_id=75280)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=75281)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=75282)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=75283)",
            "Delete(target_node=ASTNode(type=string, text='cuda:{}'))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 1744,
        "neg_line": [
            "-# device=torch.cuda.current_device(),",
            "-device=torch.device('cuda:{}'.format(os.environ[\"LOCAL_RANK\"])),"
        ],
        "pos_line": [
            "+# device=get_accelerator().current_device_name(),",
            "+device=torch.device(get_accelerator().device_name(os.environ[\"LOCAL_RANK\"])),"
        ],
        "core_change": "-# device=torch.cuda.current_device(), -device=torch.device('cuda:{}'.format(os.environ[\"LOCAL_RANK\"])), +# device=get_accelerator().current_device_name(), +device=torch.device(get_accelerator().device_name(os.environ[\"LOCAL_RANK\"])),",
        "core_API": "tensor"
    },
    {
        "commit_hash": "72686f53ad603a7300ab69e72a5252f09ad10d06",
        "index": "a88cace7..94d27933 100644",
        "commit_message": "bugfix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SignedGCN(torch.nn.Module):",
            "with torch.no_grad():",
            "pos_p = self.discriminate(z, pos_edge_index)[:, :2].max(dim=1)[1]",
            "neg_p = self.discriminate(z, neg_edge_index)[:, :2].max(dim=1)[1]",
            "-        pred = 1 - torch.cat([pos_p, neg_p]).cpu()",
            "+        pred = (1 - torch.cat([pos_p, neg_p])).cpu()",
            "y = torch.cat(",
            "[pred.new_ones((pos_p.size(0))),",
            "pred.new_zeros(neg_p.size(0))])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=attribute), node=('parenthesized_expression', None), position=0, insert_id=1057977)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1057978)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=binary_operator), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1057979)",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=call), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 1745,
        "neg_line": [
            "-pred = 1 - torch.cat([pos_p, neg_p]).cpu()"
        ],
        "pos_line": [
            "+pred = (1 - torch.cat([pos_p, neg_p])).cpu()"
        ],
        "core_change": "-pred = 1 - torch.cat([pos_p, neg_p]).cpu() +pred = (1 - torch.cat([pos_p, neg_p])).cpu()",
        "core_API": "no_grad"
    },
    {
        "commit_hash": "a911f2ecf4eeff06ad4cfbf999f0ddf3e31f131e",
        "index": "bfdb1d29..1ffc8273 100644",
        "commit_message": "refactor(internal): move model store to common store implementation (#2029)\n\n* delete models/store.py\n\n* feat(internal): add Model class\n\n* feat(internal): expose new model APIs\n\n* fix(internal): allow stores to be initialized with an FS\n\n* update internal modules to use new models\n\n* refactor(internal): refactor frameworks to use new modelstore\n\n* test: fix integration tests\n\n* fix(internal): fix SysPath lint\n\n* fix(internal): remove use of cached property\n\n* feat(internal): add bentoml.models.create\n\n* fix(internal): use shutil.move over os.rename\n\n* chore: patch\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nci: don't use docker (#2045)\n\n* ci: don't use docker for checks\n* ci: remove docker from Makefile\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nchore: drop python 3.6 support (#2046)\n\nchore: enable pyright for testing (#2047)\n\nfix: revert&fix typing of di containers (#2044)\n\n* fix: revert&fix typing of di containers\n\n* addpend\n\n* fix: dependencies issues\n\nfix: type\n\nfix: tests\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: CI for onnxmlir installation\n\nfix: onnxmlir tests and move to attrs\n\nfix(onnxmlir): activate env\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: uses attr.define instead\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: remove poetry\n\nfix: simple_di bug doesn't seem to work with attr.define\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: flake8\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: None check behaviour\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* revert\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nfix: schema typing (#2048)\n\nfix(internal): BentoInfo: don't try to parse creation time again (#2049)\n\n* fix: MLflow models\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: onnxmlir\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix(wip): paddlepaddle\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: paddle\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: spacy\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: tensorflow\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: transformers\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* delete models/store.py\n\n* feat(internal): add Model class\n\n* feat(internal): expose new model APIs\n\n* fix(internal): allow stores to be initialized with an FS\n\n* update internal modules to use new models\n\n* refactor(internal): refactor frameworks to use new modelstore\n\n* test: fix integration tests\n\n* fix(internal): fix SysPath lint\n\n* fix(internal): remove use of cached property\n\n* feat(internal): add bentoml.models.create\n\n* fix(internal): use shutil.move over os.rename\n\n* chore: patch\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nci: don't use docker (#2045)\n\n* ci: don't use docker for checks\n* ci: remove docker from Makefile\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nchore: drop python 3.6 support (#2046)\n\nchore: enable pyright for testing (#2047)\n\nfix: revert&fix typing of di containers (#2044)\n\n* fix: revert&fix typing of di containers\n\n* addpend\n\n* fix: dependencies issues\n\nfix: type\n\nfix: tests\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: CI for onnxmlir installation\n\nfix: onnxmlir tests and move to attrs\n\nfix(onnxmlir): activate env\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: uses attr.define instead\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: remove poetry\n\nfix: simple_di bug doesn't seem to work with attr.define\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: flake8\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: None check behaviour\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* revert\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nfix: schema typing (#2048)\n\nfix(internal): BentoInfo: don't try to parse creation time again (#2049)\n\n* fix: MLflow models\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: onnxmlir\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix(wip): paddlepaddle\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: paddle\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: spacy\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: tensorflow\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: transformers\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* chore: fix lint errors\n\n* Update test_onnx_impl.py\n\n* chore: ignore typing\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\n* fix: CI\n\nSigned-off-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n\nCo-authored-by: Aaron Pham <29749331+aarnphm@users.noreply.github.com>\n",
        "file": "BentoML.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "def wrong_module(modelstore, sklearn_onnx_model):",
            ")",
            "def test_onnx_save_load(metadata, save_proc, modelstore, sklearn_onnx_model):",
            "model, data = sklearn_onnx_model",
            "-    info = save_proc(metadata)",
            "-    assert info.metadata is not None",
            "-    assert_have_file_extension(info.path, \".onnx\")",
            "+    model = save_proc(metadata)",
            "+    assert model.info.metadata is not None",
            "+    assert_have_file_extension(model.path, \".onnx\")",
            "",
            "opts = ort.SessionOptions()",
            "opts.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL",
            "opts.log_verbosity_level = 1",
            "-    loaded = bentoml.onnx.load(info.tag, model_store=modelstore, session_options=opts)",
            "+    loaded = bentoml.onnx.load(model.tag, model_store=modelstore, session_options=opts)",
            "assert predict_arr(loaded, data)[0] == 0"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=info), value='model')",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2650925)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'model'), position=0, insert_id=2650926)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2650927)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=info), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=info), value='model')",
            "Update(target_node=ASTNode(type=identifier, text=info), value='model')"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 7,
        "number": 1747,
        "neg_line": [
            "-info = save_proc(metadata)",
            "-assert info.metadata is not None",
            "-assert_have_file_extension(info.path, \".onnx\")",
            "-loaded = bentoml.onnx.load(info.tag, model_store=modelstore, session_options=opts)"
        ],
        "pos_line": [
            "+model = save_proc(metadata)",
            "+assert model.info.metadata is not None",
            "+assert_have_file_extension(model.path, \".onnx\")",
            "+loaded = bentoml.onnx.load(model.tag, model_store=modelstore, session_options=opts)"
        ],
        "core_change": "-info = save_proc(metadata) -assert info.metadata is not None -assert_have_file_extension(info.path, \".onnx\") +model = save_proc(metadata) +assert model.info.metadata is not None +assert_have_file_extension(model.path, \".onnx\") -loaded = bentoml.onnx.load(info.tag, model_store=modelstore, session_options=opts) +loaded = bentoml.onnx.load(model.tag, model_store=modelstore, session_options=opts)",
        "core_API": "SessionOptions"
    },
    {
        "commit_hash": "7c5144ae5d3a93825eb1b7e5d12a6178f973c0ab",
        "index": "9fdb2605..6ae6bdca 100644",
        "commit_message": "Fix documentation type hints in `nn.conv` (#6746)\n\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SuperGATConv(MessagePassing):",
            "r\"\"\"Runs the forward pass of the module.",
            "",
            "Args:",
            "-            neg_edge_index (Tensor, optional): The negative edges to train",
            "-                against. If not given, uses negative sampling to calculate",
            "-                negative edges. (default: :obj:`None`)",
            "+            neg_edge_index (torch.Tensor, optional): The negative edges to",
            "+                train against. If not given, uses negative sampling to",
            "+                calculate negative edges. (default: :obj:`None`)",
            "\"\"\"",
            "N, H, C = x.size(0), self.heads, self.out_channels"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=r\"\"\"Runs the forward pass of the module.\n\nArgs:\n            neg_edge_index (Tensor, optional): The negative edges to train\n                against. If not given, uses negative sampling to calculate\n                negative edges. (default: :obj:`None`)\n\"\"\"), value='r\"\"\"Runs the forward pass of the module.\\n\\nArgs:\\n            neg_edge_index (torch.Tensor, optional): The negative edges to\\n                train against. If not given, uses negative sampling to\\n                calculate negative edges. (default: :obj:`None`)\\n\"\"\"')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 1,
        "number": 1748,
        "neg_line": [
            "-neg_edge_index (Tensor, optional): The negative edges to train",
            "-against. If not given, uses negative sampling to calculate",
            "-negative edges. (default: :obj:`None`)"
        ],
        "pos_line": [
            "+neg_edge_index (torch.Tensor, optional): The negative edges to",
            "+train against. If not given, uses negative sampling to",
            "+calculate negative edges. (default: :obj:`None`)"
        ],
        "core_change": "-neg_edge_index (Tensor, optional): The negative edges to train -against. If not given, uses negative sampling to calculate -negative edges. (default: :obj:`None`) +neg_edge_index (torch.Tensor, optional): The negative edges to +train against. If not given, uses negative sampling to +calculate negative edges. (default: :obj:`None`)",
        "core_API": "size"
    },
    {
        "commit_hash": "c664423caee20d946a3d9adc0fc55121dbd5fcc7",
        "index": "8e417dd81..3d253cb97 100644",
        "commit_message": "add redis test marker to conftest.py\nadd redis database tests to tox and .github flows\nuse print instead of std.write\nfix datasets tests to work with the new skip_checks flag\nadd skip_checks flag on dataset purge\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def pytest_configure(config: _pytest.config.Config) -> None:",
            "config.addinivalue_line(\"markers\", \"e2e: end-to-end integration tests\")",
            "config.addinivalue_line(\"markers\", \"security: security integration tests\")",
            "config.addinivalue_line(\"markers\", \"tff: PySyTFF integration tests\")",
            "+    config.addinivalue_line(\"markers\", \"redis: Dataset tests\")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1797512)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1797513)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1797514)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1797515)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=0, insert_id=1797516)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1797517)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'addinivalue_line'), position=2, insert_id=1797518)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1797519)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"markers\"'), position=1, insert_id=1797520)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1797521)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"redis: Dataset tests\"'), position=3, insert_id=1797522)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1797523)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 12,
        "number": 1749,
        "neg_line": [],
        "pos_line": [
            "+config.addinivalue_line(\"markers\", \"redis: Dataset tests\")"
        ],
        "core_change": "+config.addinivalue_line(\"markers\", \"redis: Dataset tests\")",
        "core_API": "addinivalue_line"
    },
    {
        "commit_hash": "e9d4e8eca95829e5607236fa30a0556b40813f62",
        "index": "ab43af0..7f98ca7 100644",
        "commit_message": "Fixed keepdims\n",
        "file": "facenet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Network(object):",
            "\"\"\"",
            "@layer",
            "def softmax(self, target, axis, name=None):",
            "-        max_axis = tf.reduce_max(target, axis, keep_dims=True)",
            "+        max_axis = tf.reduce_max(target, axis, keepdims=True)",
            "target_exp = tf.exp(target-max_axis)",
            "-        normalize = tf.reduce_sum(target_exp, axis, keep_dims=True)",
            "+        normalize = tf.reduce_sum(target_exp, axis, keepdims=True)",
            "softmax = tf.div(target_exp, normalize, name)",
            "return softmax"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=keep_dims), value='keepdims')",
            "Update(target_node=ASTNode(type=identifier, text=keep_dims), value='keepdims')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 1751,
        "neg_line": [
            "-max_axis = tf.reduce_max(target, axis, keep_dims=True)",
            "-normalize = tf.reduce_sum(target_exp, axis, keep_dims=True)"
        ],
        "pos_line": [
            "+max_axis = tf.reduce_max(target, axis, keepdims=True)",
            "+normalize = tf.reduce_sum(target_exp, axis, keepdims=True)"
        ],
        "core_change": "-max_axis = tf.reduce_max(target, axis, keep_dims=True) +max_axis = tf.reduce_max(target, axis, keepdims=True) -normalize = tf.reduce_sum(target_exp, axis, keep_dims=True) +normalize = tf.reduce_sum(target_exp, axis, keepdims=True)",
        "core_API": "reduce_max"
    },
    {
        "commit_hash": "db9cc6b898638ea3de5b18cb582d4052a601039f",
        "index": "98076283b..84e43f6c5 100644",
        "commit_message": "Fix travis error: black syft\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchHook:",
            "def module_move_(self, dest):",
            "return self.send(dest).end_get()",
            "",
            "-        torch.nn.Module.move =  module_move_",
            "+        torch.nn.Module.move = module_move_",
            "",
            "def module_get_(self):",
            "\"\"\"Get model parameters\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1752,
        "neg_line": [
            "-torch.nn.Module.move =  module_move_"
        ],
        "pos_line": [
            "+torch.nn.Module.move = module_move_"
        ],
        "core_change": "-torch.nn.Module.move =  module_move_ +torch.nn.Module.move = module_move_",
        "core_API": "send"
    },
    {
        "commit_hash": "339c6a00c810b0809da4780078d60469a0e9b4ea",
        "index": "c7fddd26..93294d9c 100644",
        "commit_message": "fix issue where flair was crashing for cpu only version of pytorch\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DocumentRNNEmbeddings(DocumentEmbeddings):",
            "",
            "def _apply(self, fn):",
            "major, minor, build, *_ = (int(info)",
            "-                                for info in torch.__version__.split('.'))",
            "+                                for info in torch.__version__.replace(\"+cpu\",\"\").split('.'))",
            "",
            "# fixed RNN change format for torch 1.4.0",
            "if major >= 1 and minor >= 4:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=237739)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=237740)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=237741)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=237742)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'replace'), position=2, insert_id=237743)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=237744)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"+cpu\"'), position=1, insert_id=237745)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=237746)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"\"'), position=3, insert_id=237747)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=237748)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 1753,
        "neg_line": [
            "-for info in torch.__version__.split('.'))"
        ],
        "pos_line": [
            "+for info in torch.__version__.replace(\"+cpu\",\"\").split('.'))"
        ],
        "core_change": "-for info in torch.__version__.split('.')) +for info in torch.__version__.replace(\"+cpu\",\"\").split('.'))",
        "core_API": "split"
    },
    {
        "commit_hash": "ebd0bc17929b36b851fc825bbfd5fbd41f6d8111",
        "index": "8bdc05dc..a6e7bb7b 100644",
        "commit_message": "Update to PyTorch 1.11.0 (#3045)\n\n* Run black\n\n* Fix ProvenanceTensor\n\n* Replace torch.triangular_solve -> torch.linalg.solve_triangular\n\n* More fixes to torch.linalg.solve_triangular\n\n* Bump torch version\n\n* Bump Python version 3.6 -> 3.7\n\n* Fix some tests\n\n* Decrease tolerance on stable tests\n\n* Remove obsolete xfail\n\n* Resolve #3032\n\n* Fix solve_triangular in ops.gaussian, revert test weakening\n\n* Fix catching of singular matrices in hmc\n\n* xfail funsor tests\n\n* Work around pandas 1.3 bug\n\n* Allow mypy to install missing stubs\n\n* Clarify triangular_solve test\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _precision_to_scale_tril(P):",
            "# Ref: https://nbviewer.jupyter.org/gist/fehiepsi/5ef8e09e61604f10607380467eb82006#Precision-to-scale_tril",
            "Lf = torch.linalg.cholesky(torch.flip(P, (-2, -1)))",
            "L_inv = torch.transpose(torch.flip(Lf, (-2, -1)), -2, -1)",
            "-    L = torch.triangular_solve(",
            "-        torch.eye(P.shape[-1], dtype=P.dtype, device=P.device), L_inv, upper=False",
            "-    )[0]",
            "+    L = torch.linalg.solve_triangular(",
            "+        L_inv, torch.eye(P.shape[-1], dtype=P.dtype, device=P.device), upper=False",
            "+    )",
            "return L",
            "",
            "",
            "+@ignore_torch_deprecation_warnings()",
            "def _try_possibly_intractable(fn, *args, **kwargs):",
            "# Convert ValueError into NotImplementedError.",
            "try:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('decorated_definition', None), position=5, insert_id=672061)",
            "Insert(target_node=IN(type=decorated_definition), node=('decorator', None), position=0, insert_id=672062)",
            "Move(target_node=IN(type=decorated_definition), node=ASTNode(type=function_definition), position=1)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=decorator), node=('@', '@'), position=0, insert_id=672063)",
            "Insert(target_node=IN(type=decorator), node=('call', None), position=1, insert_id=672064)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=672065)",
            "Insert(target_node=IN(type=call), node=('identifier', 'ignore_torch_deprecation_warnings'), position=0, insert_id=672066)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=672067)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=672068)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'solve_triangular'), position=2, insert_id=672069)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'L_inv'), position=1, insert_id=672070)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=672071)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=672072)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=672073)",
            "Update(target_node=ASTNode(type=identifier, text=triangular_solve), value='linalg')",
            "Delete(target_node=ASTNode(type=identifier, text=L_inv))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 23,
        "number": 1754,
        "neg_line": [
            "-L = torch.triangular_solve(",
            "-torch.eye(P.shape[-1], dtype=P.dtype, device=P.device), L_inv, upper=False",
            "-)[0]"
        ],
        "pos_line": [
            "+L = torch.linalg.solve_triangular(",
            "+L_inv, torch.eye(P.shape[-1], dtype=P.dtype, device=P.device), upper=False",
            "+)",
            "+@ignore_torch_deprecation_warnings()"
        ],
        "core_change": "-L = torch.triangular_solve( -torch.eye(P.shape[-1], dtype=P.dtype, device=P.device), L_inv, upper=False -)[0] +L = torch.linalg.solve_triangular( +L_inv, torch.eye(P.shape[-1], dtype=P.dtype, device=P.device), upper=False +) +@ignore_torch_deprecation_warnings()",
        "core_API": "cholesky"
    },
    {
        "commit_hash": "01b293b84fd2b1eeb80a01afe44d341d9c9c6486",
        "index": "7df26974..ec24265b 100644",
        "commit_message": "0.4.0 fixes\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from torch_geometric.transforms import TargetIndegree",
            "from torch_geometric.data import Data",
            "",
            "",
            "-def test_cartesian():",
            "+def test_target_indegree():",
            "assert TargetIndegree().__repr__() == 'TargetIndegree(cat=True)'",
            "",
            "-    edge_index = torch.LongTensor([[0, 1, 1, 2], [1, 0, 2, 1]])",
            "+    edge_index = torch.tensor([[0, 1, 1, 2], [1, 0, 2, 1]])",
            "data = Data(edge_index=edge_index)",
            "expected_output = [1, 0.5, 0.5, 1]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=test_cartesian), value='test_target_indegree')",
            "Update(target_node=ASTNode(type=identifier, text=LongTensor), value='tensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1755,
        "neg_line": [
            "-def test_cartesian():",
            "-edge_index = torch.LongTensor([[0, 1, 1, 2], [1, 0, 2, 1]])"
        ],
        "pos_line": [
            "+def test_target_indegree():",
            "+edge_index = torch.tensor([[0, 1, 1, 2], [1, 0, 2, 1]])"
        ],
        "core_change": "-def test_cartesian(): +def test_target_indegree(): -edge_index = torch.LongTensor([[0, 1, 1, 2], [1, 0, 2, 1]]) +edge_index = torch.tensor([[0, 1, 1, 2], [1, 0, 2, 1]])",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "74507dc8d07c2c779181f4fdd093f08b764d8eb5",
        "index": "04c9b8f5d9..05c43d2405 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def diagflat(",
            ")",
            "",
            "temp = x - torch.full(x.shape, padding_value).type(x.dtype)",
            "-    diagonal_to_add = torch.diag(temp, diagonal=offset).type(x.dtype) # diag does not support float16",
            "+    diagonal_to_add = torch.diag(temp, diagonal=offset).type(",
            "+        x.dtype",
            "+    )  # diag does not support float16",
            "",
            "diagonal_to_add = diagonal_to_add[tuple(slice(0, n) for n in output_array.shape)]",
            "diagonal_to_add = diagonal_to_add.to(x.dtype)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1758,
        "neg_line": [
            "-diagonal_to_add = torch.diag(temp, diagonal=offset).type(x.dtype) # diag does not support float16"
        ],
        "pos_line": [
            "+diagonal_to_add = torch.diag(temp, diagonal=offset).type(",
            "+x.dtype",
            "+)  # diag does not support float16"
        ],
        "core_change": "-diagonal_to_add = torch.diag(temp, diagonal=offset).type(x.dtype) # diag does not support float16 +diagonal_to_add = torch.diag(temp, diagonal=offset).type( +x.dtype +)  # diag does not support float16",
        "core_API": "full"
    },
    {
        "commit_hash": "3ab8cef99e295880e42393195734530023a741e7",
        "index": "ae1edebb..53f7ca7c 100644",
        "commit_message": "Fix VITS model SPD\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class StochasticDurationPredictor(nn.Module):",
            "h = self.post_pre(dr)",
            "h = self.post_convs(h, x_mask)",
            "h = self.post_proj(h) * x_mask",
            "-            noise = torch.rand(dr.size(0), 2, dr.size(2)).to(device=x.device, dtype=x.dtype) * x_mask",
            "+            noise = torch.randn(dr.size(0), 2, dr.size(2)).to(device=x.device, dtype=x.dtype) * x_mask",
            "z_q = noise",
            "",
            "# posterior encoder"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=rand), value='randn')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1760,
        "neg_line": [
            "-noise = torch.rand(dr.size(0), 2, dr.size(2)).to(device=x.device, dtype=x.dtype) * x_mask"
        ],
        "pos_line": [
            "+noise = torch.randn(dr.size(0), 2, dr.size(2)).to(device=x.device, dtype=x.dtype) * x_mask"
        ],
        "core_change": "-noise = torch.rand(dr.size(0), 2, dr.size(2)).to(device=x.device, dtype=x.dtype) * x_mask +noise = torch.randn(dr.size(0), 2, dr.size(2)).to(device=x.device, dtype=x.dtype) * x_mask",
        "core_API": "post_pre"
    },
    {
        "commit_hash": "351753aae55894591dafa81814eaa82a59687f09",
        "index": "432a3317c7..53eaf5d02c 100644",
        "commit_message": "[rllib] Remove dependency on TensorFlow (#4764)\n\n* remove hard tf dep\n\n* add test\n\n* comment fix\n\n* fix test\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "class VisionNetwork(Model):",
            "",
            "@override(Model)",
            "def _build_layers_v2(self, input_dict, num_outputs, options):",
            "+        import tensorflow.contrib.slim as slim",
            "+",
            "inputs = input_dict[\"obs\"]",
            "filters = options.get(\"conv_filters\")",
            "if not filters:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=2454506)",
            "Insert(target_node=IN(type=block), node=('import_statement', None), position=0, insert_id=2454507)",
            "Insert(target_node=IN(type=import_statement), node=('import', 'import'), position=0, insert_id=2454508)",
            "Insert(target_node=IN(type=import_statement), node=('aliased_import', None), position=1, insert_id=2454509)",
            "Insert(target_node=IN(type=aliased_import), node=('dotted_name', None), position=0, insert_id=2454510)",
            "Insert(target_node=IN(type=aliased_import), node=('as', 'as'), position=1, insert_id=2454511)",
            "Insert(target_node=IN(type=aliased_import), node=('identifier', 'slim'), position=2, insert_id=2454512)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'tensorflow'), position=0, insert_id=2454513)",
            "Insert(target_node=IN(type=dotted_name), node=('.', '.'), position=1, insert_id=2454514)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'contrib'), position=2, insert_id=2454515)",
            "Insert(target_node=IN(type=dotted_name), node=('.', '.'), position=3, insert_id=2454516)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'slim'), position=4, insert_id=2454517)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 1761,
        "neg_line": [],
        "pos_line": [
            "+import tensorflow.contrib.slim as slim",
            "+"
        ],
        "core_change": "+import tensorflow.contrib.slim as slim +",
        "core_API": "get"
    },
    {
        "commit_hash": "995c204337d16a6146a433cee360e5a5bfbc9a6f",
        "index": "1e58ddb1..0bc85562 100644",
        "commit_message": "Data2vec prelim (#2929)\n\nSummary:\nPreliminaries for data2vec release, include some minor improvements and bug fixes\n\nMost important change is that we now default to raising an exception when fields in config do not have a corresponding field in the model dataclass\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2929\n\nReviewed By: wnhsu\n\nDifferential Revision: D33649708\n\nPulled By: alexeib\n\nfbshipit-source-id: 629bdb4c361550740b451c570c2005bb956c6fcb\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestCheckpointUtils(unittest.TestCase):",
            "def test_load_ema_from_checkpoint(self):",
            "dummy_state = {\"a\": torch.tensor([1]), \"b\": torch.tensor([0.1])}",
            "with patch(f\"{checkpoint_utils.__name__}.PathManager.open\") as mock_open, patch(",
            "-                f\"{checkpoint_utils.__name__}.torch.load\") as mock_load:",
            "+            f\"{checkpoint_utils.__name__}.torch.load\"",
            "+        ) as mock_load:",
            "",
            "-            mock_load.return_value = {",
            "-                \"extra_state\": {",
            "-                    \"ema\": dummy_state",
            "-                }",
            "-            }",
            "+            mock_load.return_value = {\"extra_state\": {\"ema\": dummy_state}}",
            "filename = \"ema_checkpoint.pt\"",
            "state = checkpoint_utils.load_ema_from_checkpoint(filename)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 6,
        "AST_diff_line": 17,
        "number": 1762,
        "neg_line": [
            "-f\"{checkpoint_utils.__name__}.torch.load\") as mock_load:",
            "-mock_load.return_value = {",
            "-\"extra_state\": {",
            "-\"ema\": dummy_state",
            "-}",
            "-}"
        ],
        "pos_line": [
            "+f\"{checkpoint_utils.__name__}.torch.load\"",
            "+) as mock_load:",
            "+mock_load.return_value = {\"extra_state\": {\"ema\": dummy_state}}"
        ],
        "core_change": "-f\"{checkpoint_utils.__name__}.torch.load\") as mock_load: +f\"{checkpoint_utils.__name__}.torch.load\" +) as mock_load: -mock_load.return_value = { -\"extra_state\": { -\"ema\": dummy_state -} -} +mock_load.return_value = {\"extra_state\": {\"ema\": dummy_state}}",
        "core_API": "tensor"
    },
    {
        "commit_hash": "a267c65bd0f79f9c4d021345c9c8ed6ad8b7a471",
        "index": "ca0ea403..91e46a57 100644",
        "commit_message": "[feat] Added more augmentations containers (#1014)\n\n* Added a basic container\n\n* Enabled mixed sequential input for videos\n\n* fixed docs\n\n* Added augmentation sequential\n\n* Updated augmentation sequential\n\n* Updated docs and tests\n\n* Added test for inverse\n\n* bug fix for inverse\n\n* Fix tests & remove asserts\n\n* Refined enum logic\n\n* Fixed tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Removed extra import\n\n* Fixed typing and renamed InputType to DataCategory\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Updated code\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* simplify code\n\n* Fixed #1055\n\n* Added tests for sequential.\n\n* Fixed mypy\n\n* Renamed some functions and arguments.\n\n* Rename DataCategory to DataKey and added more tests.\n\n* Renamed Sequential to ImageSequential\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Apply suggestions from code review\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def transform_boxes(trans_mat: torch.Tensor, boxes: torch.Tensor, mode: str = \"x",
            "",
            "\"\"\"",
            "",
            "-    if not torch.is_tensor(boxes):",
            "-        raise TypeError(f\"Boxes type is not a torch.Tensor. Got {type(boxes)}\")",
            "-",
            "-    if not torch.is_tensor(trans_mat):",
            "-        raise TypeError(f\"Tranformation matrix type is not a torch.Tensor. Got {type(trans_mat)}\")",
            "-",
            "if not isinstance(mode, str):",
            "raise TypeError(f\"Mode must be a string. Got {type(mode)}\")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n\n    if not torch.is_tensor(boxes):\n        raise TypeError(f\"Boxes type is not a torch.Tensor. Got {type(boxes)}\")\n\n    if not torch.is_tensor(trans_mat):\n        raise TypeError(f\"Tranformation matrix type is not a torch.Tensor. Got {type(trans_mat)}\")\n\nif not isinstance(mode, str):\nraise TypeError(f\"Mode must be a string. Got {type(mode)}\"), value='\"\"\"\\n\\nif not isinstance(mode, str):\\nraise TypeError(f\"Mode must be a string. Got {type(mode)}\"')"
        ],
        "plus_line": 0,
        "minus_line": 4,
        "AST_diff_line": 1,
        "number": 1763,
        "neg_line": [
            "-if not torch.is_tensor(boxes):",
            "-raise TypeError(f\"Boxes type is not a torch.Tensor. Got {type(boxes)}\")",
            "-",
            "-if not torch.is_tensor(trans_mat):",
            "-raise TypeError(f\"Tranformation matrix type is not a torch.Tensor. Got {type(trans_mat)}\")",
            "-"
        ],
        "pos_line": [],
        "core_change": "-if not torch.is_tensor(boxes): -raise TypeError(f\"Boxes type is not a torch.Tensor. Got {type(boxes)}\") - -if not torch.is_tensor(trans_mat): -raise TypeError(f\"Tranformation matrix type is not a torch.Tensor. Got {type(trans_mat)}\") -",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "44d2847610944f56a06b7cfa54faadb66e130a83",
        "index": "945cea2c..103f1125 100644",
        "commit_message": "Metrics in distributed setting (#4525)\n\n* initial commit to ensure that metrics work correctly in distributed setting\n\n* updating global_distributed_metric to take metric object\n\n* adding distributed f1 score\n\n* adding distributed attachment scores\n\n* bug fix\n\n* adding distributed boolean accuracy\n\n* adding distributed entropy\n\n* adding distributed evalb\n\n* adding distributed mean_absolute_error\n\n* adding distributed sequence accuracy\n\n* adding distributed unigram recall\n\n* making models compatible with distributed metrics\n\n* adding distributed auc\n\n* adding distributed bleu\n\n* adding missing argument\n\n* initial commit to ensure that metrics work correctly in distributed setting\n\n* updating global_distributed_metric to take metric object\n\n* adding distributed f1 score\n\n* adding distributed attachment scores\n\n* bug fix\n\n* adding distributed boolean accuracy\n\n* adding distributed entropy\n\n* adding distributed evalb\n\n* adding distributed mean_absolute_error\n\n* adding distributed sequence accuracy\n\n* adding distributed unigram recall\n\n* making models compatible with distributed metrics\n\n* adding distributed auc\n\n* adding distributed bleu\n\n* adding missing argument\n\n* changing start method\n\n* removing unnecessary argument\n\n* adding remaining metrics, removing extra argument\n\n* allowing float values\n\n* bug fix\n\n* more bug fixes\n\n* changing average to return float\n\n* adding timeout for distributed test\n\n* testing unequal batches\n\n* adding distributed auc\n\n* adding distributed spearman correlation\n\n* adding distributed covariance and pearson correlation\n\n* changing distributed test to function, misc changes\n\n* checking batch lengths explicitly to raise errors\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MeanAbsoluteError(Metric):",
            "mean_absolute_error = self._absolute_error / self._total_count",
            "if reset:",
            "self.reset()",
            "-        return mean_absolute_error",
            "+        return {\"mae\": mean_absolute_error}",
            "",
            "@overrides",
            "def reset(self):"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('dictionary', None), position=1, insert_id=12068)",
            "Insert(target_node=IN(type=dictionary), node=('{', '{'), position=0, insert_id=12069)",
            "Insert(target_node=IN(type=dictionary), node=('pair', None), position=1, insert_id=12070)",
            "Insert(target_node=IN(type=dictionary), node=('}', '}'), position=2, insert_id=12071)",
            "Insert(target_node=IN(type=pair), node=('string', '\"mae\"'), position=0, insert_id=12072)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=12073)",
            "Move(target_node=IN(type=pair), node=ASTNode(type=identifier, text=mean_absolute_error), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 1764,
        "neg_line": [
            "-return mean_absolute_error"
        ],
        "pos_line": [
            "+return {\"mae\": mean_absolute_error}"
        ],
        "core_change": "-return mean_absolute_error +return {\"mae\": mean_absolute_error}",
        "core_API": "reset"
    },
    {
        "commit_hash": "aba604ed1d3d2047a68a74872403db43e9d19037",
        "index": "91530e4ef..f08d9fe3d 100644",
        "commit_message": "Add HF_ prefix to env var MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES (#2409)\n\n* add HF_ prefix to env var MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\n\n* update tests\n\n* style\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def test_is_small_dataset(",
            "dataset_size, config_max_in_memory_dataset_size, env_max_in_memory_dataset_size, monkeypatch",
            "):",
            "if config_max_in_memory_dataset_size != \"default\":",
            "-        monkeypatch.setattr(datasets.config, \"MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", config_max_in_memory_dataset_size)",
            "+        monkeypatch.setattr(",
            "+            datasets.config, \"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", config_max_in_memory_dataset_size",
            "+        )",
            "",
            "-    max_in_memory_dataset_size = datasets.config.MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES",
            "+    max_in_memory_dataset_size = datasets.config.HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES",
            "if config_max_in_memory_dataset_size == \"default\":",
            "if env_max_in_memory_dataset_size:",
            "assert max_in_memory_dataset_size == env_max_in_memory_dataset_size"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES), value='HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES')",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1785138)",
            "Update(target_node=ASTNode(type=string, text=\"MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\"), value='\"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\"')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'datasets'), position=0, insert_id=1785139)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1785140)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=2, insert_id=1785141)",
            "Delete(target_node=ASTNode(type=identifier, text=datasets))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=config))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 1765,
        "neg_line": [
            "-monkeypatch.setattr(datasets.config, \"MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", config_max_in_memory_dataset_size)",
            "-max_in_memory_dataset_size = datasets.config.MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES"
        ],
        "pos_line": [
            "+monkeypatch.setattr(",
            "+datasets.config, \"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", config_max_in_memory_dataset_size",
            "+)",
            "+max_in_memory_dataset_size = datasets.config.HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES"
        ],
        "core_change": "-monkeypatch.setattr(datasets.config, \"MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", config_max_in_memory_dataset_size) +monkeypatch.setattr( +datasets.config, \"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", config_max_in_memory_dataset_size +) -max_in_memory_dataset_size = datasets.config.MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES +max_in_memory_dataset_size = datasets.config.HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES",
        "core_API": "setattr"
    },
    {
        "commit_hash": "bda4f25c05dcc666607c202b4b1134e249a9da12",
        "index": "597f3194..7bb82df9 100644",
        "commit_message": "replacing .repeat(...) with .expand(...) (#2059)\n\n* expand vs repeat\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: Nitai Fingerhut <nitai@Nitais-MacBook-Pro-3.local>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RandomThinPlateSpline(GeometricAugmentationBase2D):",
            "",
            "def generate_parameters(self, shape: torch.Size) -> Dict[str, Tensor]:",
            "B, _, _, _ = shape",
            "-        src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).repeat(B, 1, 1)  # Bx5x2",
            "+        src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2",
            "dst = src + self.dist.rsample(src.shape)",
            "return dict(src=src, dst=dst)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=repeat), value='expand')",
            "Update(target_node=ASTNode(type=integer, text=1), value='5')",
            "Update(target_node=ASTNode(type=integer, text=1), value='2')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 1766,
        "neg_line": [
            "-src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).repeat(B, 1, 1)  # Bx5x2"
        ],
        "pos_line": [
            "+src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2"
        ],
        "core_change": "-src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).repeat(B, 1, 1)  # Bx5x2 +src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2",
        "core_API": "tensor"
    },
    {
        "commit_hash": "76b266bfbf8a4a01dd42708f70df36accc66f97d",
        "index": "7c5fa988..0c7ee016 100644",
        "commit_message": " Upgrade to modern Python syntax (#1213)\n\n* Upgrade to modern Python syntax\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Use f-strings\n\n* Placate DeepSource Python\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def laf_from_center_scale_ori(xy: torch.Tensor, scale: torch.Tensor, ori: torch.",
            "names = ['xy', 'scale', 'ori']",
            "for var_name, var, req_shape in zip(names, [xy, scale, ori], [(\"B\", \"N\", 2), (\"B\", \"N\", 1, 1), (\"B\", \"N\", 1)]):",
            "if not isinstance(var, torch.Tensor):",
            "-            raise TypeError(\"{} type is not a torch.Tensor. Got {}\".format(var_name, type(var)))",
            "+            raise TypeError(f\"{var_name} type is not a torch.Tensor. Got {type(var)}\")",
            "if len(var.shape) != len(req_shape):  # type: ignore  # because it does not like len(tensor.shape)",
            "raise TypeError(\"{} shape should be must be [{}]. \" \"Got {}\".format(var_name, str(req_shape), var.size()))",
            "for i, dim in enumerate(req_shape):  # type: ignore # because it wants typing for dim"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{var_name} type is not a torch.Tensor. Got {type(var)}\"'), position=1, insert_id=419637)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} type is not a torch.Tensor. Got {}\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=var_name))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=type))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=var))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 1767,
        "neg_line": [
            "-raise TypeError(\"{} type is not a torch.Tensor. Got {}\".format(var_name, type(var)))"
        ],
        "pos_line": [
            "+raise TypeError(f\"{var_name} type is not a torch.Tensor. Got {type(var)}\")"
        ],
        "core_change": "-raise TypeError(\"{} type is not a torch.Tensor. Got {}\".format(var_name, type(var))) +raise TypeError(f\"{var_name} type is not a torch.Tensor. Got {type(var)}\")",
        "core_API": "size"
    },
    {
        "commit_hash": "c5b6ceea4a0a151ab78b9c065584bc416d1ee275",
        "index": "6f2e8bd6..db338c5e 100644",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "with tf.device(device_fn):",
            "# the softmax is implemented internally in tl.cost.cross_entropy(y, y_) to",
            "# speed up computation, so we use identity here.",
            "# see tf.nn.sparse_softmax_cross_entropy_with_logits()",
            "-    network = tl.layers.DenseLayer(network, n_units=10, act=tf.identity, name='output')",
            "+    network = tl.layers.DenseLayer(network, n_units=10, act=None, name='output')",
            "",
            "# define cost function and metric.",
            "y = network.outputs"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('none', 'None'), position=2, insert_id=2634193)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=identity))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1768,
        "neg_line": [
            "-network = tl.layers.DenseLayer(network, n_units=10, act=tf.identity, name='output')"
        ],
        "pos_line": [
            "+network = tl.layers.DenseLayer(network, n_units=10, act=None, name='output')"
        ],
        "core_change": "-network = tl.layers.DenseLayer(network, n_units=10, act=tf.identity, name='output') +network = tl.layers.DenseLayer(network, n_units=10, act=None, name='output')",
        "core_API": "device"
    },
    {
        "commit_hash": "191035825a0b453ca11058f43c0beb6024241f5f",
        "index": "1fa03c5f..207ecff9 100644",
        "commit_message": "DQFD fixed, various related problems fixed, import_experience added, preprocessing shape issue fixed\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ImageResize(Preprocessor):",
            "self.size = (width, height)",
            "super(ImageResize, self).__init__(scope=scope, summary_labels=summary_labels)",
            "",
            "-    def tf_process(self, tensor):",
            "-        return tf.image.resize_images(images=tensor, size=self.size)",
            "-",
            "def processed_shape(self, shape):",
            "return self.size + (shape[-1],)",
            "+",
            "+    def tf_process(self, tensor):",
            "+        return tf.image.resize_images(images=tensor, size=self.size)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=function_definition), node=ASTNode(type=module), position=6)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 1769,
        "neg_line": [
            "-def tf_process(self, tensor):",
            "-return tf.image.resize_images(images=tensor, size=self.size)",
            "-"
        ],
        "pos_line": [
            "+",
            "+def tf_process(self, tensor):",
            "+return tf.image.resize_images(images=tensor, size=self.size)"
        ],
        "core_change": "-def tf_process(self, tensor): -return tf.image.resize_images(images=tensor, size=self.size) - + +def tf_process(self, tensor): +return tf.image.resize_images(images=tensor, size=self.size)",
        "core_API": "resize_images"
    },
    {
        "commit_hash": "11edecd75379657c1929615fa729c8f3c07dcafc",
        "index": "1680b869e..a946f7807 100755",
        "commit_message": "Fix uninitialized variables when `config.mask_feature_prob > 0` (#12705)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class HubertModel(HubertPreTrainedModel):",
            "if not getattr(self.config, \"apply_spec_augment\", True):",
            "return hidden_states",
            "",
            "+        # generate indices & apply SpecAugment along time axis",
            "+        batch_size, sequence_length, hidden_size = hidden_states.size()",
            "+",
            "if mask_time_indices is not None:",
            "# apply SpecAugment along time axis with given mask_time_indices",
            "hidden_states[mask_time_indices] = self.masked_spec_embed.to(hidden_states.dtype)",
            "elif self.config.mask_time_prob > 0 and self.training:",
            "-            # generate indices & apply SpecAugment along time axis",
            "-            batch_size, sequence_length, hidden_size = hidden_states.size()",
            "-",
            "mask_time_indices = _compute_mask_indices(",
            "(batch_size, sequence_length),",
            "mask_prob=self.config.mask_time_prob,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=2686348)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2686349)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=elif), position=0)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=boolean_operator), position=1)",
            "Insert(target_node=IN(type=assignment), node=('pattern_list', None), position=0, insert_id=2686350)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2686351)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=batch_size), value='mask_time_indices')",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', 'batch_size'), position=0, insert_id=2686352)",
            "Insert(target_node=IN(type=pattern_list), node=(',', ','), position=1, insert_id=2686353)",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', 'sequence_length'), position=2, insert_id=2686354)",
            "Insert(target_node=IN(type=pattern_list), node=(',', ','), position=3, insert_id=2686355)",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', 'hidden_size'), position=4, insert_id=2686356)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=sequence_length))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=hidden_size))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=identifier, text=mask_time_indices))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 1770,
        "neg_line": [
            "-# generate indices & apply SpecAugment along time axis",
            "-batch_size, sequence_length, hidden_size = hidden_states.size()",
            "-"
        ],
        "pos_line": [
            "+# generate indices & apply SpecAugment along time axis",
            "+batch_size, sequence_length, hidden_size = hidden_states.size()",
            "+"
        ],
        "core_change": "+# generate indices & apply SpecAugment along time axis +batch_size, sequence_length, hidden_size = hidden_states.size() + -# generate indices & apply SpecAugment along time axis -batch_size, sequence_length, hidden_size = hidden_states.size() -",
        "core_API": "size"
    },
    {
        "commit_hash": "442a07e7687560276b7490ffdffa92804f676a05",
        "index": "45a5732..9c60c7e 100644",
        "commit_message": "fix the mask problem for multi-head attenion\n\n",
        "file": "transformer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Graph():",
            "zero_pad=False,",
            "scale=False,",
            "scope=\"dec_pe\")",
            "+                self.dec *= key_masks",
            "",
            "## Dropout",
            "self.dec = tf.layers.dropout(self.dec,"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=2356722)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', ''), position=4, insert_id=2356723)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2356724)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=identifier, text=scope), position=0)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=ERROR), position=2)",
            "Insert(target_node=IN(type=assignment), node=('augmented_assignment', None), position=3, insert_id=2356725)",
            "Insert(target_node=IN(type=augmented_assignment), node=('attribute', None), position=0, insert_id=2356726)",
            "Insert(target_node=IN(type=augmented_assignment), node=('*=', '*='), position=1, insert_id=2356727)",
            "Insert(target_node=IN(type=augmented_assignment), node=('identifier', 'key_masks'), position=2, insert_id=2356728)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2356729)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2356730)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dec'), position=2, insert_id=2356731)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 1772,
        "neg_line": [],
        "pos_line": [
            "+self.dec *= key_masks"
        ],
        "core_change": "+self.dec *= key_masks",
        "core_API": "dropout"
    },
    {
        "commit_hash": "7d52a69e1e0061ff730ea4ab5e345846b1ef0e9d",
        "index": "92a44d7f..9981a765 100755",
        "commit_message": "Fixed distributed/execution bugs wrt global_model.\nAdded more documentation to model methods that did not have docstring yet.\nRenamed `ctx` into `context`.\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class MemoryModel(Model):",
            "tensors=batch",
            ")",
            "",
            "-            optimization = tf.cond(",
            "+            return tf.cond(",
            "pred=optimize,",
            "true_fn=(lambda: self.fn_optimization(**batch)),",
            "false_fn=tf.no_op",
            ")",
            "",
            "-        return optimization",
            "-",
            "def tf_import_experience(self, states, internals, actions, terminal, reward):",
            "\"\"\"",
            "Imports experiences into the TensorFlow memory structure. Can be used to import"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=3, insert_id=2235432)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=2235433)",
            "Move(target_node=IN(type=return_statement), node=ASTNode(type=call), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=optimization))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=optimization))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 1774,
        "neg_line": [
            "-optimization = tf.cond(",
            "-return optimization",
            "-"
        ],
        "pos_line": [
            "+return tf.cond("
        ],
        "core_change": "-optimization = tf.cond( +return tf.cond( -return optimization -",
        "core_API": "cond"
    },
    {
        "commit_hash": "48f4abd2e61e545104f72eb50c9ab9b100726948",
        "index": "df63d1bc..e9a093a2 100644",
        "commit_message": "fix dims typo in unipc\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class UniPC:",
            "x_t = x_t_ - expand_dims(alpha_t * B_h, dims) * (corr_res + rhos_c[-1] * D1_t)",
            "else:",
            "x_t_ = (",
            "-                expand_dims(torch.exp(log_alpha_t - log_alpha_prev_0), dimss) * x",
            "+                expand_dims(torch.exp(log_alpha_t - log_alpha_prev_0), dims) * x",
            "- expand_dims(sigma_t * h_phi_1, dims) * model_prev_0",
            ")",
            "if x_t is None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=identifier, text=x), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=dimss), value='dims')",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=expand_dims))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=sigma_t))",
            "Delete(target_node=ASTNode(type=*, text=*))",
            "Delete(target_node=ASTNode(type=identifier, text=h_phi_1))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=dims))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=*, text=*))",
            "Delete(target_node=ASTNode(type=identifier, text=model_prev_0))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 1777,
        "neg_line": [
            "-expand_dims(torch.exp(log_alpha_t - log_alpha_prev_0), dimss) * x",
            "-expand_dims(sigma_t * h_phi_1, dims) * model_prev_0"
        ],
        "pos_line": [
            "+expand_dims(torch.exp(log_alpha_t - log_alpha_prev_0), dims) * x"
        ],
        "core_change": "-expand_dims(torch.exp(log_alpha_t - log_alpha_prev_0), dimss) * x +expand_dims(torch.exp(log_alpha_t - log_alpha_prev_0), dims) * x -expand_dims(sigma_t * h_phi_1, dims) * model_prev_0",
        "core_API": "exp"
    },
    {
        "commit_hash": "46d12d3bbd69bd452703db26d1b75399eea31f86",
        "index": "5d38407f..458a9743 100644",
        "commit_message": "fix DepthWarper to accept batches\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DepthWarper(nn.Module):",
            "factor_y = (self.height - 1) / 2",
            "factor_y = factor_y.to(device)",
            "",
            "-        z = 1. / flow[:, 2]  # Nx(H*W)",
            "+        z = 1. / (flow[:, 2] + self.eps)  # Nx(H*W)",
            "x = (flow[:, 0] * z - factor_x) / factor_x",
            "y = (flow[:, 1] * z - factor_y) / factor_y"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('parenthesized_expression', None), position=2, insert_id=474941)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=474942)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('binary_operator', None), position=1, insert_id=474943)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=474944)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=subscript), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=474945)",
            "Insert(target_node=IN(type=binary_operator), node=('attribute', None), position=2, insert_id=474946)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=474947)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=474948)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'eps'), position=2, insert_id=474949)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 1779,
        "neg_line": [
            "-z = 1. / flow[:, 2]  # Nx(H*W)"
        ],
        "pos_line": [
            "+z = 1. / (flow[:, 2] + self.eps)  # Nx(H*W)"
        ],
        "core_change": "-z = 1. / flow[:, 2]  # Nx(H*W) +z = 1. / (flow[:, 2] + self.eps)  # Nx(H*W)",
        "core_API": "to"
    },
    {
        "commit_hash": "5bc61d28a0574236f91d7982ac91891275fb8825",
        "index": "d905ee92..8664657c 100644",
        "commit_message": "Fix aggregation of metrics returning dictionaries on distributed setup\n\nPiperOrigin-RevId: 509444953\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def result_wrapper(result_fn):",
            "# Wrapping result in identity so that control dependency between",
            "# update_op from `update_state` and result works in case result",
            "# returns a tensor.",
            "-                return tf.identity(result)",
            "+                return tf.nest.map_structure(tf.identity, result)",
            "",
            "# Wrapping result in merge_call. merge_call is used when we want to",
            "# leave replica mode and compute a value in cross replica mode."
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2042189)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2042190)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2042191)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'map_structure'), position=2, insert_id=2042192)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2042193)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2042194)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2042195)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nest'), position=2, insert_id=2042196)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 1781,
        "neg_line": [
            "-return tf.identity(result)"
        ],
        "pos_line": [
            "+return tf.nest.map_structure(tf.identity, result)"
        ],
        "core_change": "-return tf.identity(result) +return tf.nest.map_structure(tf.identity, result)",
        "core_API": "identity"
    },
    {
        "commit_hash": "6b423ba8bde19c831b7e8c7eb1d6d67f886d71ad",
        "index": "c0345628..a99f6030 100644",
        "commit_message": "Heterogeneous Graph Support + GraphGym (#3068)\n\n* added HGT DBLP example\n\n* typo\n\n* Merge PyG master (#52)\n\n* Adding the Facebok Page-Page dataset\n\n* type hints\n\n* documentation CI\n\n* py 3.8\n\n* fix links\n\n* fix links\n\n* fail on warning\n\n* fail on warning\n\n* fix doc\n\nCo-authored-by: benedekrozemberczki <benedek.rozemberczki@gmail.com>\n\n* revert\n\n* Fix Documentation Rendering (#51)\n\n* fix doc rendering\n\n* fix linting\n\n* retrigger checks\n\n* remove pytorch 1.7.0 legacy code (#50)\n\n* Fix `copy.deepcopy` within lazy `nn.dense.Linear` (#44)\n\n* fix deepcopy within lazy Linear\n\n* fix merge\n\n* assert exception\n\n* example to doc\n\n* resolve conflict\n\n* resolve conflict\n\n* Add Figure and Equation to `to_hetero` docstring (#60)\n\n* add tex\n\n* add svg + docstring\n\n* typo\n\n* added equation\n\n* Message Passing Hooks (#53)\n\n* add hooks\n\n* docstring\n\n* add docstring\n\n* allow modification of inputs/output\n\n* add test for modifying output\n\n* add additional asserts for modifying output test\n\n* Rename `HeteroData.get_edges` and `HeteroData.get_nodes` (#58)\n\n* rename to_edges and to_nodes\n\n* typo\n\n* `HeteroConv` (#64)\n\n* clean heteroconv\n\n* init\n\n* init\n\n* clean up\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix documentation\n\n* bipartite function\n\n* fix test CI\n\n* remove pillow version\n\n* clean up for merge\n\n* Merge PyG master (#69)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\n\n* re-merge\n\n* add lazy column to GNN cheatsheet (#70)\n\n* `to_hetero_with_bases(model)` (#63)\n\n* update\n\n* fix linting\n\n* basisconv\n\n* add ValueError\n\n* to_hetero_with_bases impl done\n\n* add test\n\n* add comments\n\n* add comments\n\n* docstring\n\n* typo\n\n* update figure\n\n* svg\n\n* typo\n\n* add test\n\n* update\n\n* add rgcn equality test\n\n* typos\n\n* update\n\n* typos\n\n* update figures\n\n* generate new svgs\n\n* fix assignment\n\n* rename\n\n* delete sorted edge types\n\n* rename\n\n* add legend\n\n* fix typo\n\n* Test: Check equal outputs of `to_hetero` and `RGCNConv` (#59)\n\n* check equal output\n\n* add sparsetensor test\n\n* check equal output\n\n* add sparsetensor test\n\n* rename\n\n* linting\n\n* add missing import\n\n* `HeteroData` support for `T.NormalizeFeatures` (#56)\n\n* normalize features\n\n* allow normalization of any feature\n\n* in-place div\n\n* normalize features\n\n* allow normalization of any feature\n\n* in-place div\n\n* fix test\n\n* no need to re-assign\n\n* `HeteroData` support for `T.AddSelfLoops` (#54)\n\n* hetero support for AddSelfLoops\n\n* check for edge_index attribute\n\n* f-string\n\n* retrigger checks\n\n* revert bipartite changes\n\n* hetero support for AddSelfLoops\n\n* check for edge_index attribute\n\n* f-string\n\n* retrigger checks\n\n* revert bipartite changes\n\n* merge master\n\n* merge master\n\n* `HeteroData` support for `T.ToSparseTensor` (#55)\n\n* hetero support for ToSparseTensor\n\n* add test\n\n* customize the attribute of SparseTensor.value\n\n* rework sort_edge_index\n\n* hetero support for ToSparseTensor\n\n* add test\n\n* customize the attribute of SparseTensor.value\n\n* rework sort_edge_index\n\n* linting\n\n* `HeteroData` support for `T.ToUndirected` (#57)\n\n* to_undirected\n\n* revert bipartite changes\n\n* coalesce + undirected enhancement\n\n* merge master\n\n* revert bipartite changes\n\n* coalesce + undirected enhancement\n\n* merge master\n\n* clean up\n\n* new default relation type\n\n* fix tests\n\n* resolve merge conflicts\n\n* resolve merge conflicts 2\n\n* resolve merge conflicts 3\n\n* Merge PyG master (#74)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\n* clean up doc for to_dense_batch\n\n* clean up\n\n* add legend to cheatsheet\n\n* Improve terminology (#2837)\n\nI think the previous version of the document uses the term 'symmetric' incorrectly. A symmetric matrix is a square matrix that is is equal to its transpose (https://en.wikipedia.org/wiki/Symmetric_matrix). However, the text is only talking about the shape of the matrix, not its content. Hence, 'square (matrix)' would be the correct term to use.\n\n* Add batch_size input to to_dense_batch (#2838)\n\n* Add batch_size input to to_dense_batch\n\n* to_dense_batch fix typo in batch_size param use\n\n* add typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Added return_attention_weights to TransformerConv. (#2807)\n\n* added return_weights functionality to tranformer\n\n* added return attn weights tests\n\n* flake8\n\n* added typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* MD17 (#2843)\n\n* Added MD17 dataset\n\n* Updated Documentation\n\n* Added link to sGDML website in doc\n\n* fixed typos in doc and made train variable description clearer\n\n* clean up\n\n* fix linting\n\n* fix doc warning\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* update doc\n\n* remove forward doc\n\n* add static graph support info to cheatsheet\n\n* fix num_nodes in case edge_index is empty\n\n* fix math formula\n\n* faster GDC import\n\n* lazy import\n\n* lazy import for datasets\n\n* lazy import for nn\n\n* Sequential jittable + traceable\n\n* typo\n\n* typo\n\n* update doc\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Schfer <moritz.schaefer@protonmail.com>\n\n* re-add\n\n* GraphGym cleaned version (#82)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Update .gitignore\n\n* Integrate GraphGym into PyG (#85)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Integrate graphgym into pyg, keep user API in project root\n\n* fix merge conflict\n\n* fix lint errors\n\n* Make optional dependencies\n\n* merge LICENSE from GraphGym\n\n* add import\n\n* clean up LICENSE\n\n* fix import\n\n* resolve merge conflicts\n\n* resolve merge conflicts 2\n\n* Merge PyG master (#87)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\n* clean up doc for to_dense_batch\n\n* clean up\n\n* add legend to cheatsheet\n\n* Improve terminology (#2837)\n\nI think the previous version of the document uses the term 'symmetric' incorrectly. A symmetric matrix is a square matrix that is is equal to its transpose (https://en.wikipedia.org/wiki/Symmetric_matrix). However, the text is only talking about the shape of the matrix, not its content. Hence, 'square (matrix)' would be the correct term to use.\n\n* Add batch_size input to to_dense_batch (#2838)\n\n* Add batch_size input to to_dense_batch\n\n* to_dense_batch fix typo in batch_size param use\n\n* add typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Added return_attention_weights to TransformerConv. (#2807)\n\n* added return_weights functionality to tranformer\n\n* added return attn weights tests\n\n* flake8\n\n* added typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* MD17 (#2843)\n\n* Added MD17 dataset\n\n* Updated Documentation\n\n* Added link to sGDML website in doc\n\n* fixed typos in doc and made train variable description clearer\n\n* clean up\n\n* fix linting\n\n* fix doc warning\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* update doc\n\n* remove forward doc\n\n* add static graph support info to cheatsheet\n\n* fix num_nodes in case edge_index is empty\n\n* fix math formula\n\n* faster GDC import\n\n* lazy import\n\n* lazy import for datasets\n\n* lazy import for nn\n\n* Sequential jittable + traceable\n\n* typo\n\n* typo\n\n* update doc\n\n* Simple models (#2869)\n\n* Inclusion of new backbone models\n\n* Eliminating head from asap.py\n\n* small correction\n\n* Create test_gcn.py\n\n* Update __init__.py\n\n* Update test_gcn.py\n\n* Left only the convolutional simple models\n\n* Tests included\n\n* update\n\n* clean up\n\n* clean up v2\n\n* fix activation\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* Example for MemPooling. (#2729)\n\n* example for mem pooling\n\n* backprop on kl loss is done at the end of an epoch. Keys in memory layers are trained only on kl loss.\n\n* added learning rate decay. Using PROTIENS_full\n\n* flake8\n\n* reduced lr. increased weight decay\n\n* changed download location\n\n* added comments\n\n* clean up\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typos\n\n* fix removeisolatednodes transform in case 'data.num_nodes' is present\n\n* fix XConv with dilation > 1\n\n* fix XConv with dilation > 1\n\n* rgcn link prediction  (#2734)\n\n* implemented LinkPrediction dataset for loading FB15k237\n\n* implemented evaluation for relational link prediction\n\n* implemented R-GCNConf link prediction example\n\n* fixed bug: wrong initial objects in negative_sampling\n\n* changed file downloader urllib.request.urlretrieve  to pytorch.data.download_url; renamed LinkPrediction class to RelationalLinkPredictionDataset\n\n* update dataset\n\n* update example script\n\n* rename\n\nCo-authored-by: Moritz <moritzblum>\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix gnnexplainer draw kwargs\n\n* remove python-louvain dependency\n\n* allow customization of output in MP jit mode\n\n* fix test for py3.6\n\n* changed normalisation to same norm from instance norm to be robust to small var (#2917)\n\n* add CITATION.cff\n\n* format\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* add basetransform ABC (#2924)\n\n* clean up BaseTransform\n\n* clean up GATConv and add comments\n\n* add max_num_neighbors as an additional argument\n\n* fix jit GATConv on PyTorch 1.8.0\n\n* fix doc\n\n* fix gnn explainer with existing self-loops\n\n* Rgcn link pred fix (#2946)\n\n* added regularization, removed typo in test\n\n* clean up\n\nCo-authored-by: Moritz <moritzblum>\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Correct gini coefficient mathcal formula (#2932)\n\n* typo\n\n* typo\n\n* Update from_networkx (#2923)\n\n* Update from_networkx\n\n* Update test\n\n* Update convert.py\n\n* Minor corrections\n\n* Update test_convert.py\n\n* Corrections\n\n* Update test_convert.py\n\n* Case where there are no edges\n\n* Correcting how edge_attr are concatenated\n\n* clean up + new test\n\n* remove unused code\n\n* add union type\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix deterministic ordering in from_networkx\n\n* recursive-include *.jinja files\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Schfer <moritz.schaefer@protonmail.com>\nCo-authored-by: PabloAMC <pmorenocf@alumnos.unex.es>\nCo-authored-by: Moritz Blum <31183934+moritzblum@users.noreply.github.com>\nCo-authored-by: fbragman <fbragman@users.noreply.github.com>\nCo-authored-by: Christopher Lee <2824685+CCInc@users.noreply.github.com>\nCo-authored-by: Tim Daubenschtz <tim@daubenschuetz.de>\n\n* resolve merge conflicts 3\n\n* resolve merge conflicts 4\n\n* Implementation of the `HGTLoader` + `ogbn-mag` example (#73)\n\n* first try\n\n* update\n\n* HGT Loader\n\n* typo\n\n* first try\n\n* update\n\n* HGT Loader\n\n* typo\n\n* bugfixes\n\n* lazy GATConv\n\n* bugfix\n\n* bugfix\n\n* full working pipeline\n\n* update\n\n* rename\n\n* docstring\n\n* typos\n\n* update\n\n* typo\n\n* typo\n\n* typo\n\n* added comments\n\n* add test\n\n* add tests\n\n* fix example\n\n* rename\n\n* linting\n\n* Random split functionalities (#72)\n\n* link split\n\n* create split\n\n* example tests\n\n* link split tests\n\n* fix linting\n\n* update docstring\n\n* undirected option, refactor and docs\n\n* add num nodes as argument to neg sampling\n\n* clean up + remove single object\n\n* update example\n\n* typo\n\n* fix compose\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* add basetransform\n\n* typo\n\n* typo\n\n* fix test\n\n* Improve `torch_geometric.data` Documentation (#98)\n\n* update data doc\n\n* typo\n\n* typo\n\n* note\n\n* typo\n\n* add docstring\n\n* only show inherited members for data and hetero_data\n\n* documentation update for batch and dataset\n\n* update doc\n\n* update\n\n* fix\n\n* record_stream\n\n* update\n\n* typo\n\n* add/fix data functionality\n\n* linting\n\n* typo\n\n* `_parent` memory leak fix (#103)\n\n* memory leak fix\n\n* Clean up\n\n* clean up\n\n* bugfix tests\n\n* typos\n\n* fix test\n\n* fix test\n\n* rename reverse\n\n* (Heterogeneous) `NeighborLoader` (#92)\n\n* initial commit\n\n* typo\n\n* neighbor loader functionality + tests\n\n* docstring\n\n* fix docstring\n\n* skip tests\n\n* fix share_memory_\n\n* typo\n\n* typo\n\n* update example\n\n* typo\n\n* share_strategy\n\n* fix cuda calls\n\n* better print\n\n* fix size\n\n* fix print\n\n* final commit\n\n* fix\n\n* some todos\n\n* preprocessed features\n\n* fix to_undirected\n\n* more documentation\n\n* update doc\n\n* fix doc\n\n* fix doc\n\n* Add benchmark code and the example with existing graph classification examples (#93)\n\n* add benchmarking utilities\n\n* update graph classification benchmark\n\n* improve code style\n\n* add pytorch-memlab for benchmark code\n\n* skip some tests when cuda is not available\n\n* add type hint when appropriate\n\n* add seed_everything to improve code\n\n* code refactoring\n\n* code refactoring\n\n* code refactoring\n\n* code improvement\n\n* remove unnecessary dataloader import\n\n* change benchmark interface with decorator\n\n* documentation improvement\n\n* linting\n\n* linting part 2\n\n* linting part 3\n\n* seed_everything\n\n* create utils file\n\n* update\n\n* use utils functions\n\n* fix test\n\n* update the profiler to the latest torch (1.8.1+)\n\n* refactor profiler and add more documentation\n\n* refactor profiler and add more documentation\n\n* resolve lint errors\n\n* resolve lint errors\n\n* update\n\n* clean up test and profile\n\n* fix linting\n\n* add to doc\n\n* fix doc\n\n* typo\n\n* update benchmark\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* Move `HGTLoader` to `torch_geometric.loader` + clean up (#104)\n\n* move files\n\n* use utils functions\n\n* fix example\n\n* update\n\n* fix tests\n\n* fix seed\n\n* fix linear test\n\n* rename\n\n* Support GraphGym custom modules outside PyG package (#102)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Integrate graphgym into pyg, keep user API in project root\n\n* fix merge conflict\n\n* fix lint errors\n\n* Make optional dependencies\n\n* merge LICENSE from GraphGym\n\n* Enable adding GraphGym customized modules outside PyG package\n\n* lint\n\n* Rename `AddTrainValTestMask` to `RandomNodeSplit` (#108)\n\n* initial commit\n\n* rename example\n\n* remove AddTrainValTestMask\n\n* fix linting\n\n* create optimizer config and scheduler config separately (#113)\n\n* create optimizer config and scheduler config separately\n\n* fix format\n\n* import explicitly\n\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\n\n* Heterogeneous Graph Tutorial (#83)\n\n* add HG tutorial roadmap\n\n* started working on hg tutorial\n\n* hg_tutorial, some text and .tex figure\n\n* added svg\n\n* hg tutorial content\n\n* fix CI\n\n* text and structure\n\n* finished first draft\n\n* fixed one code example\n\n* fixing conventions\n\n* fixing links\n\n* update svg\n\n* some smaller improvements of tutorial\n\n* improvements on tutorial\n\n* hg-tutorial: fixed compiling issue, added detailed content\n\n* added absolute links\n\n* fixed warnings\n\n* streamlined dataset section\n\n* update svg\n\n* update tutorial\n\n* update 2\n\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\n\n* typo\n\n* Move data loaders to `torch_geometric.loader` (#110)\n\n* move graphsaint\n\n* deprecations\n\n* move clusterloader\n\n* deprecations\n\n* type hints\n\n* move shadow\n\n* typo\n\n* typo\n\n* move datalistloader\n\n* dense data loader\n\n* random node sampler\n\n* fix doc\n\n* Lazy GNN operators (#89)\n\n* lazy cheb conv\n\n* lazy GraphConv\n\n* lazy GATv2Conv\n\n* lazy TAGConv\n\n* lazy FAConv\n\n* lazy FeaStConv\n\n* lazy NNConv\n\n* typo\n\n* fix tests\n\n* lazy SuperGATConv\n\n* lazy SuperGATConv fix\n\n* lazy SplineConv\n\n* fix lazy check\n\n* lazy GravNetConv\n\n* arma conv lazy\n\n* dense linear in gmmconv\n\n* typo\n\n* add test\n\n* lazy GMMConv\n\n* doc\n\n* rename (#116)\n\n* Revisit `MetaPath2Vec` (#114)\n\n* revisit metapath2vec\n\n* update\n\n* typo\n\n* update\n\n* fix doc\n\n* update\n\n* check for attributes rather than key\n\n* Clean up `torch_geometric.profile` further (#111)\n\n* remove print_layer_stats\n\n* typos\n\n* update\n\n* readme highlights and quick tour (#99)\n\n* readme highlights and quick tour\n\n* arch\n\n* arch image\n\n* arch overview\n\n* list categories\n\n* categorization\n\n* category description\n\n* Update README.md\n\nfrom Matthias\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* improved highlights\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* minor\n\n* update readme\n\n* update\n\n* update\n\n* update\n\n* update\n\n* fix url\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* move ops\n\n* toc\n\n* typo\n\n* typo\n\n* add svgs\n\n* update figure\n\n* fix links\n\n* fix size\n\n* fix size\n\n* typo\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* fix broken links\n\n* fix links\n\n* Heterogeneous Graph Sampler Tutorial (#117)\n\n* initial commit\n\n* address comments\n\n* remove todo\n\n* typo\n\n* Conversion between heterogenous and homogeneous graph objects (#115)\n\n* temp checkpoint (wip, will remove)\n\n* (wip) typed graph conversion\n\n* (wip) typed graph conversion\n\n* (wip) typed graph conversion\n\n* update\n\n* typo\n\n* delete examples\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix test\n\n* update doc\n\n* deprecate NeighborSampler (#119)\n\n* Move `torch_geometric.data.DataLoader` to `torch_geometric.loader.DataLoader` (#120)\n\n* move dataloader\n\n* rename\n\n* typos\n\n* typos\n\n* fix __cat_dim__\n\n* updategp\n\n* Deprecate `train_test_split_edges` + Modifications to `RandomLinkSplit` (#121)\n\n* deprecate train_test_split_edges\n\n* to device transform\n\n* fix example\n\n* add split_labels argument\n\n* fix autoencoder example\n\n* typos\n\n* add docstring\n\n* ARGVA\n\n* seal\n\n* adress comments\n\n* Create example to load `*.csv` and transfer to `HeteroData` (#76)\n\n* create example to load csv file and transfer to heter-data\n\n* add ipython notebook version load csv with documentation\n\n* address comment\n\n* first version of csv loading doc\n\n* first version of csv loading doc\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestions csv tutorial\n\n* example script load csv + extract fix\n\n* fixed edge index stacking dimension in example and jupyter nb\n\n* linting\n\n* linting2\n\n* rename\n\n* update\n\n* update\n\n* update\n\n* typo\n\n* typo\n\n* update\n\n* rename\n\n* update tutorial\n\n* typo\n\n* address comments\n\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* fix\n\n* typo\n\n* update\n\n* fix\n\n* fix\n\nCo-authored-by: benedekrozemberczki <benedek.rozemberczki@gmail.com>\nCo-authored-by: Rex Ying <rexying@stanford.edu>\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Schfer <moritz.schaefer@protonmail.com>\nCo-authored-by: Jiaxuan <youjiaxuan@gmail.com>\nCo-authored-by: PabloAMC <pmorenocf@alumnos.unex.es>\nCo-authored-by: Moritz Blum <31183934+moritzblum@users.noreply.github.com>\nCo-authored-by: fbragman <fbragman@users.noreply.github.com>\nCo-authored-by: Christopher Lee <2824685+CCInc@users.noreply.github.com>\nCo-authored-by: Tim Daubenschtz <tim@daubenschuetz.de>\nCo-authored-by: Yue Zhao <yzhao062@gmail.com>\nCo-authored-by: Dong Wang <dongw89@gmail.com>\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DenseGCNConv(torch.nn.Module):",
            "idx = torch.arange(N, dtype=torch.long, device=adj.device)",
            "adj[:, idx, idx] = 1 if not self.improved else 2",
            "",
            "-        out = torch.matmul(x, self.weight)",
            "+        out = self.lin(x)",
            "deg_inv_sqrt = adj.sum(dim=-1).clamp(min=1).pow(-0.5)",
            "",
            "adj = deg_inv_sqrt.unsqueeze(-1) * adj * deg_inv_sqrt.unsqueeze(-2)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=self), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=matmul), value='lin')",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=weight))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 1783,
        "neg_line": [
            "-out = torch.matmul(x, self.weight)"
        ],
        "pos_line": [
            "+out = self.lin(x)"
        ],
        "core_change": "-out = torch.matmul(x, self.weight) +out = self.lin(x)",
        "core_API": "arange"
    },
    {
        "commit_hash": "6e1d9cbae8e0d530c532f48dc3bb48bc665f96da",
        "index": "6fd61831..9e30a84d 100644",
        "commit_message": "Adalam fix2 (#1888)\n\n* fix adalam output when No seed point survived.\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AdalamFilter:",
            ")",
            "k1, k2, d1, d2, o1, o2, s1, s2 = self.__to_torch(k1, k2, d1, d2, o1, o2, s1, s2)",
            "if len(d2) <= 1:",
            "-            return _no_match(d1)",
            "+            idxs, dists = _no_match(d1)",
            "+            if return_dist:",
            "+                return idxs, dists",
            "+            return idxs",
            "distmat = dist_matrix(d1, d2, is_normalized=False)",
            "dd12, nn12 = torch.topk(distmat, k=2, dim=1, largest=False)  # (n1, 2)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=400754)",
            "Insert(target_node=ASTNode(type=block), node=('if_statement', None), position=1, insert_id=400755)",
            "Insert(target_node=ASTNode(type=block), node=('return_statement', None), position=2, insert_id=400756)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=400757)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=400758)",
            "Insert(target_node=IN(type=if_statement), node=('identifier', 'return_dist'), position=1, insert_id=400759)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=400760)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=400761)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=400762)",
            "Insert(target_node=IN(type=return_statement), node=('identifier', 'idxs'), position=1, insert_id=400763)",
            "Insert(target_node=IN(type=assignment), node=('pattern_list', None), position=0, insert_id=400764)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=400765)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=400766)",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', 'idxs'), position=0, insert_id=400767)",
            "Insert(target_node=IN(type=pattern_list), node=(',', ','), position=1, insert_id=400768)",
            "Insert(target_node=IN(type=pattern_list), node=('identifier', 'dists'), position=2, insert_id=400769)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=400770)",
            "Insert(target_node=IN(type=return_statement), node=('expression_list', None), position=1, insert_id=400771)",
            "Insert(target_node=IN(type=expression_list), node=('identifier', 'idxs'), position=0, insert_id=400772)",
            "Insert(target_node=IN(type=expression_list), node=(',', ','), position=1, insert_id=400773)",
            "Insert(target_node=IN(type=expression_list), node=('identifier', 'dists'), position=2, insert_id=400774)",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 1787,
        "neg_line": [
            "-return _no_match(d1)"
        ],
        "pos_line": [
            "+idxs, dists = _no_match(d1)",
            "+if return_dist:",
            "+return idxs, dists",
            "+return idxs"
        ],
        "core_change": "-return _no_match(d1) +idxs, dists = _no_match(d1) +if return_dist: +return idxs, dists +return idxs",
        "core_API": "__to_torch"
    },
    {
        "commit_hash": "5db8df70e816575ab89932e4676c37d4e89013b1",
        "index": "63b75aa7..bf4add17 100644",
        "commit_message": "fix\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SequenceTagger(flair.nn.Model):",
            "",
            "tags = []",
            "all_tags = []",
            "-        feature_cpu = feature.detach().to(\"cpu\")",
            "-        transitions_cpu = self.transitions.detach().to(\"cpu\")",
            "+        feature_cpu = feature.detach().cpu()",
            "+        if self.use_crf:",
            "+            transitions_cpu = self.transitions.detach().cpu()",
            "for feats, length in zip(feature_cpu, lengths):",
            "if self.use_crf:",
            "confidences, tag_seq, scores = self._viterbi_decode("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=238655)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=238656)",
            "Insert(target_node=IN(type=if_statement), node=('attribute', None), position=1, insert_id=238657)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=238658)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=238659)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=238660)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=238661)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'use_crf'), position=2, insert_id=238662)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=to), value='cpu')",
            "Update(target_node=ASTNode(type=identifier, text=to), value='cpu')",
            "Delete(target_node=ASTNode(type=string, text=\"cpu\"))",
            "Delete(target_node=ASTNode(type=string, text=\"cpu\"))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 13,
        "number": 1792,
        "neg_line": [
            "-feature_cpu = feature.detach().to(\"cpu\")",
            "-transitions_cpu = self.transitions.detach().to(\"cpu\")"
        ],
        "pos_line": [
            "+feature_cpu = feature.detach().cpu()",
            "+if self.use_crf:",
            "+transitions_cpu = self.transitions.detach().cpu()"
        ],
        "core_change": "-feature_cpu = feature.detach().to(\"cpu\") -transitions_cpu = self.transitions.detach().to(\"cpu\") +feature_cpu = feature.detach().cpu() +if self.use_crf: +transitions_cpu = self.transitions.detach().cpu()",
        "core_API": "detach"
    },
    {
        "commit_hash": "a2900a39bd7dd5682268582c9cba39e4f2a43fbc",
        "index": "dec244bd..21bc7d94 100644",
        "commit_message": "Fix dropout by temporarily replacing with nn.Dropout\n\n",
        "file": "stanza.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class LSTMwRecDropout(nn.Module):",
            "self.hidden_size = hidden_size",
            "",
            "self.dropout = dropout",
            "-        self.drop = Dropout(dropout)",
            "+        self.drop = nn.Dropout(dropout)",
            "self.rec_drop = nn.Dropout(rec_dropout)",
            "",
            "self.num_directions = 2 if bidirectional else 1"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1517126)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1517127)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1517128)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=Dropout), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1794,
        "neg_line": [
            "-self.drop = Dropout(dropout)"
        ],
        "pos_line": [
            "+self.drop = nn.Dropout(dropout)"
        ],
        "core_change": "-self.drop = Dropout(dropout) +self.drop = nn.Dropout(dropout)",
        "core_API": "Dropout"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "47b908c9..abda5ff5 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def assert_cov_validity(cov, eigenvalue_lbnd=0., condition_number_ubnd=1e6):",
            "# Symmetry",
            "assert (cov.t() == cov).all(), 'Covariance must be symmetric!'",
            "# Precompute eigenvalues for subsequent tests.",
            "-    ws, _ = torch.symeig(cov)  # The eigenvalues of cov",
            "+    ws = torch.linalg.eigvalsh(cov)  # The eigenvalues of cov",
            "w_min = torch.min(ws)",
            "w_max = torch.max(ws)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=ws), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=677072)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=677073)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'eigvalsh'), position=2, insert_id=677074)",
            "Update(target_node=ASTNode(type=identifier, text=symeig), value='linalg')",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=_))",
            "Delete(target_node=ASTNode(type=pattern_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 1795,
        "neg_line": [
            "-ws, _ = torch.symeig(cov)  # The eigenvalues of cov"
        ],
        "pos_line": [
            "+ws = torch.linalg.eigvalsh(cov)  # The eigenvalues of cov"
        ],
        "core_change": "-ws, _ = torch.symeig(cov)  # The eigenvalues of cov +ws = torch.linalg.eigvalsh(cov)  # The eigenvalues of cov",
        "core_API": "t"
    },
    {
        "commit_hash": "69ecde95342cb0e1c38e2bacba8efa6194d055ca",
        "index": "ce2ebaa..b4e2de0 100644",
        "commit_message": "fix regression layer to_one_hot (#240)\n\n",
        "file": "tflearn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def regression(incoming, placeholder=None, optimizer='adam',",
            "if placeholder is None:",
            "pscope = \"TargetsData\" if not name else name",
            "with tf.name_scope(pscope):",
            "-            placeholder = tf.placeholder(shape=input_shape, dtype=dtype, name=\"Y\")",
            "+            p_shape = [None] if to_one_hot else input_shape",
            "+            placeholder = tf.placeholder(shape=p_shape, dtype=dtype, name=\"Y\")",
            "",
            "tf.add_to_collection(tf.GraphKeys.TARGETS, placeholder)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=2353076)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2353077)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'p_shape'), position=0, insert_id=2353078)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2353079)",
            "Insert(target_node=IN(type=assignment), node=('conditional_expression', None), position=2, insert_id=2353080)",
            "Insert(target_node=IN(type=conditional_expression), node=('list', None), position=0, insert_id=2353081)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=2353082)",
            "Insert(target_node=IN(type=conditional_expression), node=('identifier', 'to_one_hot'), position=2, insert_id=2353083)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=2353084)",
            "Insert(target_node=IN(type=conditional_expression), node=('identifier', 'input_shape'), position=4, insert_id=2353085)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=2353086)",
            "Insert(target_node=IN(type=list), node=('none', 'None'), position=1, insert_id=2353087)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=2, insert_id=2353088)",
            "Update(target_node=ASTNode(type=identifier, text=input_shape), value='p_shape')"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 1796,
        "neg_line": [
            "-placeholder = tf.placeholder(shape=input_shape, dtype=dtype, name=\"Y\")"
        ],
        "pos_line": [
            "+p_shape = [None] if to_one_hot else input_shape",
            "+placeholder = tf.placeholder(shape=p_shape, dtype=dtype, name=\"Y\")"
        ],
        "core_change": "-placeholder = tf.placeholder(shape=input_shape, dtype=dtype, name=\"Y\") +p_shape = [None] if to_one_hot else input_shape +placeholder = tf.placeholder(shape=p_shape, dtype=dtype, name=\"Y\")",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "b8ff56896ccbd27a54035a90a3bc278a44541a74",
        "index": "9fcc5f2cb..7c40342f1 100644",
        "commit_message": "Fix bug of multi-gpu training in lm finetuning\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "global_step += 1",
            "",
            "# Save a trained model",
            "-    if  n_gpu > 1 and torch.distributed.get_rank() == 0  or n_gpu <=1 :",
            "+    if args.local_rank == -1 or torch.distributed.get_rank() == 0:",
            "logging.info(\"** ** * Saving fine-tuned model ** ** * \")",
            "model.save_pretrained(args.output_dir)",
            "tokenizer.save_pretrained(args.output_dir)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=boolean_operator), position=0)",
            "Insert(target_node=ASTNode(type=boolean_operator), node=('or', 'or'), position=1, insert_id=1247858)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('attribute', None), position=0, insert_id=1247859)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('==', '=='), position=1, insert_id=1247860)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('unary_operator', '-1'), position=2, insert_id=1247861)",
            "Update(target_node=ASTNode(type=identifier, text=n_gpu), value='args')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=n_gpu), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1247862)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'local_rank'), position=2, insert_id=1247863)",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=and, text=and))",
            "Delete(target_node=ASTNode(type=or, text=or))",
            "Delete(target_node=ASTNode(type=identifier, text=n_gpu))",
            "Delete(target_node=ASTNode(type=<=, text=<=))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=comparison_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1802,
        "neg_line": [
            "-if  n_gpu > 1 and torch.distributed.get_rank() == 0  or n_gpu <=1 :"
        ],
        "pos_line": [
            "+if args.local_rank == -1 or torch.distributed.get_rank() == 0:"
        ],
        "core_change": "-if  n_gpu > 1 and torch.distributed.get_rank() == 0  or n_gpu <=1 : +if args.local_rank == -1 or torch.distributed.get_rank() == 0:",
        "core_API": "get_rank"
    },
    {
        "commit_hash": "351753aae55894591dafa81814eaa82a59687f09",
        "index": "aad399c3b2..73ee1d87c6 100644",
        "commit_message": "[rllib] Remove dependency on TensorFlow (#4764)\n\n* remove hard tf dep\n\n* add test\n\n* comment fix\n\n* fix test\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def conv2d(x,",
            "filter_size=(3, 3),",
            "stride=(1, 1),",
            "pad=\"SAME\",",
            "-           dtype=tf.float32,",
            "+           dtype=None,",
            "collections=None):",
            "+    if dtype is None:",
            "+        dtype = tf.float32",
            "+",
            "with tf.variable_scope(name):",
            "stride_shape = [1, stride[0], stride[1], 1]",
            "filter_shape = ["
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('block', None), position=4, insert_id=2150006)",
            "Insert(target_node=IN(type=block), node=('if_statement', None), position=0, insert_id=2150007)",
            "Insert(target_node=ASTNode(type=default_parameter), node=('none', 'None'), position=2, insert_id=2150008)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2150009)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=2150010)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2150011)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=2150012)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'dtype'), position=0, insert_id=2150013)",
            "Insert(target_node=IN(type=comparison_operator), node=('is', 'is'), position=1, insert_id=2150014)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=2, insert_id=2150015)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2150016)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2150017)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'dtype'), position=0, insert_id=2150018)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2150019)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 1803,
        "neg_line": [
            "-dtype=tf.float32,"
        ],
        "pos_line": [
            "+dtype=None,",
            "+if dtype is None:",
            "+dtype = tf.float32",
            "+"
        ],
        "core_change": "-dtype=tf.float32, +dtype=None, +if dtype is None: +dtype = tf.float32 +",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "f27475aaeba87052e277df83beaadf002f4fd9da",
        "index": "054617a4..1f7654b2 100644",
        "commit_message": "Enable multi-process training on CPU (#4272)\n\n* Use torch.device everywhere\n\n* Update changelog\n\n* Run distributed tests even on CPU\n\n* Fix bug when running distributed tests on CPU\n\n* Remove unused imports\n\n* Update CHANGELOG.md\n\nCo-authored-by: Evan Pete Walsh <epwalsh10@gmail.com>\n\nCo-authored-by: Evan Pete Walsh <epwalsh10@gmail.com>\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestNnUtil(AllenNlpTestCase):",
            "\"b\": FakeTensor(),",
            "\"c\": (1, FakeTensor()),",
            "}",
            "-        new_device = 4",
            "+        new_device = torch.device(4)",
            "moved_obj = util.move_to_device(structured_obj, new_device)",
            "assert moved_obj[\"a\"][0].a == 1",
            "assert moved_obj[\"a\"][0].b._device == new_device"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=17979)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=17980)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=17981)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=17982)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=17983)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=17984)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=17985)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=integer, text=4), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=17986)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 1804,
        "neg_line": [
            "-new_device = 4"
        ],
        "pos_line": [
            "+new_device = torch.device(4)"
        ],
        "core_change": "-new_device = 4 +new_device = torch.device(4)",
        "core_API": "device"
    },
    {
        "commit_hash": "0396c7befa632b699417a7f0758006a4016f1e6c",
        "index": "a397cc02..edbb466c 100644",
        "commit_message": "fix collect env formatting\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "import flair",
            "",
            "def main():",
            "print(\"#### Versions:\")",
            "-    print(f\"#### Flair\\n{flair.__version__}\")",
            "-    print(f\"#### Pytorch\\n{torch.__version__}\")",
            "-    print(f\"#### Transformers\\n{transformers.__version__}\")",
            "+    print(f\"##### Flair\\n{flair.__version__}\")",
            "+    print(f\"##### Pytorch\\n{torch.__version__}\")",
            "+    print(f\"##### Transformers\\n{transformers.__version__}\")",
            "print(f\"#### GPU\\n{torch.cuda.is_available()}\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=f\"#### Flair\\n{flair.__version__}\"), value='f\"##### Flair\\\\n{flair.__version__}\"')",
            "Update(target_node=ASTNode(type=string, text=f\"#### Pytorch\\n{torch.__version__}\"), value='f\"##### Pytorch\\\\n{torch.__version__}\"')",
            "Update(target_node=ASTNode(type=string, text=f\"#### Transformers\\n{transformers.__version__}\"), value='f\"##### Transformers\\\\n{transformers.__version__}\"')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 3,
        "number": 1808,
        "neg_line": [
            "-print(f\"#### Flair\\n{flair.__version__}\")",
            "-print(f\"#### Pytorch\\n{torch.__version__}\")",
            "-print(f\"#### Transformers\\n{transformers.__version__}\")"
        ],
        "pos_line": [
            "+print(f\"##### Flair\\n{flair.__version__}\")",
            "+print(f\"##### Pytorch\\n{torch.__version__}\")",
            "+print(f\"##### Transformers\\n{transformers.__version__}\")"
        ],
        "core_change": "-print(f\"#### Flair\\n{flair.__version__}\") -print(f\"#### Pytorch\\n{torch.__version__}\") -print(f\"#### Transformers\\n{transformers.__version__}\") +print(f\"##### Flair\\n{flair.__version__}\") +print(f\"##### Pytorch\\n{torch.__version__}\") +print(f\"##### Transformers\\n{transformers.__version__}\")",
        "core_API": "is_available"
    },
    {
        "commit_hash": "ace394259607ded0ccafaee7961d0ed1ce6e344f",
        "index": "c30b6f67..cad630f4 100644",
        "commit_message": "fix #721\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def Conv2DTranspose(",
            "bias_regularizer=bias_regularizer,",
            "activity_regularizer=activity_regularizer)",
            "ret = layer.apply(inputs, scope=tf.get_variable_scope())",
            "+        ret = tf.identity(ret, name='output')",
            "",
            "ret.variables = VariableHolder(W=layer.kernel)",
            "if use_bias:",
            "ret.variables.b = layer.bias",
            "-    return tf.identity(ret, name='output')",
            "+    return ret",
            "",
            "",
            "Deconv2D = Conv2DTranspose"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=parameters), node=ASTNode(type=ERROR), position=10)",
            "Insert(target_node=ASTNode(type=ERROR), node=('=', '='), position=9, insert_id=2287630)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'tf'), position=10, insert_id=2287631)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'identity'), position=12, insert_id=2287632)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'ret'), position=32, insert_id=2287633)",
            "Insert(target_node=ASTNode(type=ERROR), node=('.', '.'), position=32, insert_id=2287634)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'ret'), position=31, insert_id=2287635)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=identity))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 1810,
        "neg_line": [
            "-return tf.identity(ret, name='output')"
        ],
        "pos_line": [
            "+ret = tf.identity(ret, name='output')",
            "+return ret"
        ],
        "core_change": "+ret = tf.identity(ret, name='output') -return tf.identity(ret, name='output') +return ret",
        "core_API": "apply"
    },
    {
        "commit_hash": "f6c76230938f8c5cd09ef85913a414c37862b206",
        "index": "f70d0dab..cf244710 100644",
        "commit_message": "Add Pre-commit bot fixes (#3722)\n\n* flake8\n\n* flake8\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from torch_geometric.nn.functional import gini",
            "",
            "",
            "def test_gini():",
            "-    w = torch.tensor(",
            "-        [",
            "-            [0., 0., 0., 0.],",
            "-            [0., 0., 0., 1000.0]",
            "-        ]",
            "-    )",
            "+    w = torch.tensor([[0., 0., 0., 0.], [0., 0., 0., 1000.0]])",
            "assert torch.isclose(gini(w), torch.tensor(0.5))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 6,
        "AST_diff_line": 17,
        "number": 1813,
        "neg_line": [
            "-w = torch.tensor(",
            "-[",
            "-[0., 0., 0., 0.],",
            "-[0., 0., 0., 1000.0]",
            "-]",
            "-)"
        ],
        "pos_line": [
            "+w = torch.tensor([[0., 0., 0., 0.], [0., 0., 0., 1000.0]])"
        ],
        "core_change": "-w = torch.tensor( -[ -[0., 0., 0., 0.], -[0., 0., 0., 1000.0] -] -) +w = torch.tensor([[0., 0., 0., 0.], [0., 0., 0., 1000.0]])",
        "core_API": "tensor"
    },
    {
        "commit_hash": "76bb45964df1e62d1411b0a9e9fc673e9a791c9a",
        "index": "28fba25a3..01b2663e4 100644",
        "commit_message": "Add Image feature (#3163)\n\n* Initial commit\n\n* Add basic decoding\n\n* Replace features.Audio with Audio\n\n* Add Image to package reference\n\n* Use np.array\n\n* Update error msg\n\n* Add mode and channel decoding\n\n* Fix return value\n\n* Finish decoding\n\n* Make CI happy\n\n* Some more fixes\n\n* Minor doc fix\n\n* Remove animated option\n\n* Pin version\n\n* Remove unused imports in setup.py\n\n* Add vision requirements to setup.py\n\n* Add initial tests\n\n* Delete other formats\n\n* Make Image feature hashable\n\n* Add more tests\n\n* Support numpy array in alter data check in TypedSequence\n\n* Fix TypedSequence converion\n\n* Finish tests\n\n* Update Image - add ImageExtensionType and supporting functions\n\n* Update encoding functions\n\n* Add support in TypedSequence for ImageExtensionType\n\n* Add tests\n\n* Remove unused import\n\n* Fix doc and style\n\n* Fix doc indentation\n\n* Improve comment\n\n* Return single image instead of dict\n\n* Return PIL Image and not dict\n\n* Encode dict\n\n* Update tests\n\n* Style\n\n* np.ndarray encoding/decoding\n\n* Minor improvements\n\n* PIL Image support in cast_to_python_objects\n\n* Test cast\n\n* Doc fix\n\n* Extension type fixes\n\n* Style\n\n* Use types_mapper in Dataset.to_pandas\n\n* Add pandas extension array for image type\n\n* Update tests\n\n* image type inference\n\n* Remvoe cast_to_python test after Quentin's change\n\n* Improve tests\n\n* Add storage type\n\n* Improve tests\n\n* Test map that returns np.ndarray\n\n* Rename functions\n\n* Add streaming test\n\n* Use image struct in all situations\n\n* Update src/datasets/features/image.py - encode_example type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Update src/datasets/features/image.py -list_image_compression_formats type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Support str in encode_objects_to_image_dicts\n\n* Update src/datasets/features/image.py - objects_to_list_of_image_dicts type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Style\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class ArabicSpeechCorpus(datasets.GeneratorBasedBuilder):",
            "{",
            "\"file\": datasets.Value(\"string\"),",
            "\"text\": datasets.Value(\"string\"),",
            "-                    \"audio\": datasets.features.Audio(sampling_rate=48_000),",
            "+                    \"audio\": datasets.Audio(sampling_rate=48_000),",
            "\"phonetic\": datasets.Value(\"string\"),",
            "\"orthographic\": datasets.Value(\"string\"),",
            "}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=features))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1816,
        "neg_line": [
            "-\"audio\": datasets.features.Audio(sampling_rate=48_000),"
        ],
        "pos_line": [
            "+\"audio\": datasets.Audio(sampling_rate=48_000),"
        ],
        "core_change": "-\"audio\": datasets.features.Audio(sampling_rate=48_000), +\"audio\": datasets.Audio(sampling_rate=48_000),",
        "core_API": "Value"
    },
    {
        "commit_hash": "04d6bf0af90d8f12b006873535c44ad6d937957e",
        "index": "6a7722cc..1e1fcf70 100644",
        "commit_message": "Changelog Fix + Travis Optimization (#795)\n\n* Add license scan report and status (#789)\n\n* Add license scan report and status\n\nSigned-off-by: fossabot <badges@fossa.io>\n\n* Update .travis.yml\n\n* Update .travis.yml\n\n* Update CHANGELOG.md\n\n* Update CHANGELOG.md\n\n* Update .travis.yml\n\n* Update .travis.yml\n\n* Update .travis.yml\n\n* Update .travis.yml\n\n* Update .travis.yml\n\n* Update .travis.yml\n\n* Update CHANGELOG.md\n\n* Update CHANGELOG.md\n\n* Update CHANGELOG.md\n\n* Update README.md\n\n* Update CHANGELOG.md\n\n* YAPF Fix\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def cosine_similarity(v1, v2):",
            "- `<https://en.wikipedia.org/wiki/Cosine_similarity>`__.",
            "",
            "\"\"\"",
            "-    return tf.reduce_sum(tf.multiply(v1, v2), 1) / (",
            "-        tf.sqrt(tf.reduce_sum(tf.multiply(v1, v1), 1)) * tf.sqrt(tf.reduce_sum(tf.multiply(v2, v2), 1))",
            "-    )",
            "+    return tf.reduce_sum(",
            "+        tf.multiply(v1, v2), 1",
            "+    ) / (tf.sqrt(tf.reduce_sum(tf.multiply(v1, v1), 1)) * tf.sqrt(tf.reduce_sum(tf.multiply(v2, v2), 1)))",
            "",
            "",
            "# Regularization Functions"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=string, text=`<https://en.wikipedia.org/wiki/Cosine_similarity>`))",
            "Delete(target_node=ASTNode(type=identifier, text=__))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 3,
        "minus_line": 4,
        "AST_diff_line": 5,
        "number": 1818,
        "neg_line": [
            "-`<https://en.wikipedia.org/wiki/Cosine_similarity>`__.",
            "-return tf.reduce_sum(tf.multiply(v1, v2), 1) / (",
            "-tf.sqrt(tf.reduce_sum(tf.multiply(v1, v1), 1)) * tf.sqrt(tf.reduce_sum(tf.multiply(v2, v2), 1))",
            "-)"
        ],
        "pos_line": [
            "+return tf.reduce_sum(",
            "+tf.multiply(v1, v2), 1",
            "+) / (tf.sqrt(tf.reduce_sum(tf.multiply(v1, v1), 1)) * tf.sqrt(tf.reduce_sum(tf.multiply(v2, v2), 1)))"
        ],
        "core_change": "-`<https://en.wikipedia.org/wiki/Cosine_similarity>`__. -return tf.reduce_sum(tf.multiply(v1, v2), 1) / ( -tf.sqrt(tf.reduce_sum(tf.multiply(v1, v1), 1)) * tf.sqrt(tf.reduce_sum(tf.multiply(v2, v2), 1)) -) +return tf.reduce_sum( +tf.multiply(v1, v2), 1 +) / (tf.sqrt(tf.reduce_sum(tf.multiply(v1, v1), 1)) * tf.sqrt(tf.reduce_sum(tf.multiply(v2, v2), 1)))",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "c1e8ff8338f7a7826930808a070a55f4909062a1",
        "index": "e60401898..86a035549 100644",
        "commit_message": "Fix tests accordingly\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_zero_refresh(workers):",
            "t = torch.tensor([2.2, -1.0])",
            "x = t.fix_prec().share(bob, alice, crypto_provider=james)",
            "",
            "-    x_sh = x.child.child.child",
            "+    x_sh = x.child.child",
            "assert (x_sh.zero().get() == torch.zeros(*t.shape).long()).all()",
            "",
            "x = t.fix_prec().share(bob, alice, crypto_provider=james)"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=attribute), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=child))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 1821,
        "neg_line": [
            "-x_sh = x.child.child.child"
        ],
        "pos_line": [
            "+x_sh = x.child.child"
        ],
        "core_change": "-x_sh = x.child.child.child +x_sh = x.child.child",
        "core_API": "tensor"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "caa46610..5224ba3a 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_gaussian_tensordot(dot_dims,",
            "nb = dot_dims",
            "nc = y_dim - dot_dims",
            "try:",
            "-        torch.cholesky(x.precision[..., na:, na:] + y.precision[..., :nb, :nb])",
            "+        torch.linalg.cholesky(x.precision[..., na:, na:] + y.precision[..., :nb, :nb])",
            "except RuntimeError:",
            "pytest.skip(\"Cannot marginalize the common variables of two Gaussians.\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=677176)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=677177)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=677178)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1825,
        "neg_line": [
            "-torch.cholesky(x.precision[..., na:, na:] + y.precision[..., :nb, :nb])"
        ],
        "pos_line": [
            "+torch.linalg.cholesky(x.precision[..., na:, na:] + y.precision[..., :nb, :nb])"
        ],
        "core_change": "-torch.cholesky(x.precision[..., na:, na:] + y.precision[..., :nb, :nb]) +torch.linalg.cholesky(x.precision[..., na:, na:] + y.precision[..., :nb, :nb])",
        "core_API": "cholesky"
    },
    {
        "commit_hash": "7f06bcd0e9176f3262ada4aad3adef2232fe42ef",
        "index": "c59953f1..d6eb1945 100644",
        "commit_message": "Add docformatter in pre-commit (#1242)\n\n* Add docformatter in pre-commit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Apply suggestions from code review\n\n* Apply suggestions from code review\n\nCo-authored-by: Christian Clauss <cclauss@me.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Christian Clauss <cclauss@me.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def load_image(file_name):",
            "",
            "",
            "def clip_and_convert_tensor(tensor):",
            "-    \"\"\"convert the input torch.Tensor to OpenCV image,clip it to be between",
            "+    \"\"\"Convert the input torch.Tensor to OpenCV image,clip it to be between.",
            "+",
            "[0, 255] and convert it to unit",
            "\"\"\"",
            "img = tgm.utils.tensor_to_image(255.0 * tensor)  # convert tensor to numpy"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"convert the input torch.Tensor to OpenCV image,clip it to be between\n[0, 255] and convert it to unit\n\"\"\"), value='\"\"\"Convert the input torch.Tensor to OpenCV image,clip it to be between.\\n\\n[0, 255] and convert it to unit\\n\"\"\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1827,
        "neg_line": [
            "-\"\"\"convert the input torch.Tensor to OpenCV image,clip it to be between"
        ],
        "pos_line": [
            "+\"\"\"Convert the input torch.Tensor to OpenCV image,clip it to be between.",
            "+"
        ],
        "core_change": "-\"\"\"convert the input torch.Tensor to OpenCV image,clip it to be between +\"\"\"Convert the input torch.Tensor to OpenCV image,clip it to be between. +",
        "core_API": "tensor_to_image"
    },
    {
        "commit_hash": "23a537e2ad1ee9af7e8016054208d5ce1cc572fd",
        "index": "23f525167..0ad369d3b 100644",
        "commit_message": "black fix\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SISNRLoss(TimeDomainLoss):",
            "# s_target = <s', s>s / ||s||^2",
            "pair_wise_dot = torch.sum(s_estimate * s_target, dim=1, keepdim=True)  # [B, 1]",
            "s_target_energy = (",
            "-            torch.sum(s_target ** 2, dim=1, keepdim=True) + self.eps",
            "+            torch.sum(s_target**2, dim=1, keepdim=True) + self.eps",
            ")  # [B, 1]",
            "pair_wise_proj = pair_wise_dot * s_target / s_target_energy  # [B, T]",
            "# e_noise = s' - s_target",
            "e_noise = s_estimate - pair_wise_proj  # [B, T]",
            "",
            "# SI-SNR = 10 * log_10(||s_target||^2 / ||e_noise||^2)",
            "-        pair_wise_si_snr = torch.sum(pair_wise_proj ** 2, dim=1) / (",
            "-            torch.sum(e_noise ** 2, dim=1) + self.eps",
            "+        pair_wise_si_snr = torch.sum(pair_wise_proj**2, dim=1) / (",
            "+            torch.sum(e_noise**2, dim=1) + self.eps",
            ")",
            "pair_wise_si_snr = 10 * torch.log10(pair_wise_si_snr + self.eps)  # [B]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 1829,
        "neg_line": [
            "-torch.sum(s_target ** 2, dim=1, keepdim=True) + self.eps",
            "-pair_wise_si_snr = torch.sum(pair_wise_proj ** 2, dim=1) / (",
            "-torch.sum(e_noise ** 2, dim=1) + self.eps"
        ],
        "pos_line": [
            "+torch.sum(s_target**2, dim=1, keepdim=True) + self.eps",
            "+pair_wise_si_snr = torch.sum(pair_wise_proj**2, dim=1) / (",
            "+torch.sum(e_noise**2, dim=1) + self.eps"
        ],
        "core_change": "-torch.sum(s_target ** 2, dim=1, keepdim=True) + self.eps +torch.sum(s_target**2, dim=1, keepdim=True) + self.eps -pair_wise_si_snr = torch.sum(pair_wise_proj ** 2, dim=1) / ( -torch.sum(e_noise ** 2, dim=1) + self.eps +pair_wise_si_snr = torch.sum(pair_wise_proj**2, dim=1) / ( +torch.sum(e_noise**2, dim=1) + self.eps",
        "core_API": "sum"
    },
    {
        "commit_hash": "64e6cd0c958a35bcf9a0af7cf9fed448498e2ea4",
        "index": "8710a9d8..b9ab085b 100644",
        "commit_message": "Fix typos in torchscript tests.\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_arma_conv():",
            "",
            "t = '(Tensor, SparseTensor, OptTensor) -> Tensor'",
            "jit = torch.jit.script(conv.jittable(t))",
            "-    assert torch.allclose(conv(x, adj.t()), out, atol=1e-6)",
            "+    assert torch.allclose(jit(x, adj.t()), out, atol=1e-6)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=conv), value='jit')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1830,
        "neg_line": [
            "-assert torch.allclose(conv(x, adj.t()), out, atol=1e-6)"
        ],
        "pos_line": [
            "+assert torch.allclose(jit(x, adj.t()), out, atol=1e-6)"
        ],
        "core_change": "-assert torch.allclose(conv(x, adj.t()), out, atol=1e-6) +assert torch.allclose(jit(x, adj.t()), out, atol=1e-6)",
        "core_API": "script"
    },
    {
        "commit_hash": "dd183a03a19970ef10b55e749f3d365558ad1076",
        "index": "e7f8e9ab3e..d6d48f5163 100644",
        "commit_message": "`sorting` fix up (#1884)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def sort(",
            "*,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    sorted_tensor, _ = torch.sort(x, dim=axis, descending=descending, out=out)",
            "+    sorted_tensor, _ = torch.sort(",
            "+        x, dim=axis, descending=descending, stable=stable, out=out",
            "+    )",
            "return sorted_tensor"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=349289)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=349290)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'stable'), position=0, insert_id=349291)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=349292)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'stable'), position=2, insert_id=349293)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1831,
        "neg_line": [
            "-sorted_tensor, _ = torch.sort(x, dim=axis, descending=descending, out=out)"
        ],
        "pos_line": [
            "+sorted_tensor, _ = torch.sort(",
            "+x, dim=axis, descending=descending, stable=stable, out=out",
            "+)"
        ],
        "core_change": "-sorted_tensor, _ = torch.sort(x, dim=axis, descending=descending, out=out) +sorted_tensor, _ = torch.sort( +x, dim=axis, descending=descending, stable=stable, out=out +)",
        "core_API": "sort"
    },
    {
        "commit_hash": "39d7a93619083cb8e37f5ef7708cf50b34e20ee1",
        "index": "f4f4bab..77f6bbf 100644",
        "commit_message": "Fix AP calculation bug #8464 (#8484)\n\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def run(",
            "",
            "if npr == 0:",
            "if nl:",
            "-                    stats.append((correct, *torch.zeros((3, 0), device=device)))",
            "+                    stats.append((correct, *torch.zeros((2, 0), device=device), labels[:, 0]))",
            "continue",
            "",
            "# Predictions"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=tuple), node=(',', ','), position=4, insert_id=1293778)",
            "Insert(target_node=ASTNode(type=tuple), node=('subscript', None), position=5, insert_id=1293779)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'labels'), position=0, insert_id=1293780)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1293781)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=2, insert_id=1293782)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=1293783)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=4, insert_id=1293784)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=5, insert_id=1293785)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=1293786)",
            "Update(target_node=ASTNode(type=integer, text=3), value='2')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 1832,
        "neg_line": [
            "-stats.append((correct, *torch.zeros((3, 0), device=device)))"
        ],
        "pos_line": [
            "+stats.append((correct, *torch.zeros((2, 0), device=device), labels[:, 0]))"
        ],
        "core_change": "-stats.append((correct, *torch.zeros((3, 0), device=device))) +stats.append((correct, *torch.zeros((2, 0), device=device), labels[:, 0]))",
        "core_API": "append"
    },
    {
        "commit_hash": "1af0d8cd2f7e89d291d99f477af16fc4239e9102",
        "index": "fcf4dbc9..26424591 100644",
        "commit_message": "[PyTorch][MXNet] Fix assign_anchor_to_bbox function (#1915)\n\n\n",
        "file": "d2l-en.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):",
            "device=device)",
            "# Assign ground-truth bounding boxes according to the threshold",
            "max_ious, indices = torch.max(jaccard, dim=1)",
            "-    anc_i = torch.nonzero(max_ious >= 0.5).reshape(-1)",
            "-    box_j = indices[max_ious >= 0.5]",
            "+    anc_i = torch.nonzero(max_ious >= iou_threshold).reshape(-1)",
            "+    box_j = indices[max_ious >= iou_threshold]",
            "anchors_bbox_map[anc_i] = box_j",
            "col_discard = torch.full((num_anchors,), -1)",
            "row_discard = torch.full((num_gt_boxes,), -1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('identifier', 'iou_threshold'), position=2, insert_id=60293)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('identifier', 'iou_threshold'), position=2, insert_id=60294)",
            "Delete(target_node=ASTNode(type=float, text=0.5))",
            "Delete(target_node=ASTNode(type=float, text=0.5))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 1834,
        "neg_line": [
            "-anc_i = torch.nonzero(max_ious >= 0.5).reshape(-1)",
            "-box_j = indices[max_ious >= 0.5]"
        ],
        "pos_line": [
            "+anc_i = torch.nonzero(max_ious >= iou_threshold).reshape(-1)",
            "+box_j = indices[max_ious >= iou_threshold]"
        ],
        "core_change": "-anc_i = torch.nonzero(max_ious >= 0.5).reshape(-1) -box_j = indices[max_ious >= 0.5] +anc_i = torch.nonzero(max_ious >= iou_threshold).reshape(-1) +box_j = indices[max_ious >= iou_threshold]",
        "core_API": "max"
    },
    {
        "commit_hash": "9b6385488617ee99188b60c0525cbe4ec7e8f2a0",
        "index": "a53d0840..741c2947 100644",
        "commit_message": "Improve reproduceability 2/3 (#1906)\n\n* [Repro] Correct reproducability\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* need better image\n\n* allow conversion from no state dict checkpoints\n\n* up\n\n* up\n\n* up\n\n* up\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* next try\n\n* up\n\n* up\n\n* better name\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\n* correct more\n\n* up\n\n* replace all torch randn\n\n* fix\n\n* correct\n\n* correct\n\n* finish\n\n* fix more\n\n* up\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ScoreSdeVePipeline(DiffusionPipeline):",
            "",
            "model = self.unet",
            "",
            "-        sample = torch.randn(*shape, generator=generator) * self.scheduler.init_noise_sigma",
            "+        sample = randn_tensor(shape, generator=generator) * self.scheduler.init_noise_sigma",
            "sample = sample.to(self.device)",
            "",
            "self.scheduler.set_timesteps(num_inference_steps)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='randn_tensor')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=shape), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=randn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=*, text=*))",
            "Delete(target_node=ASTNode(type=list_splat))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1835,
        "neg_line": [
            "-sample = torch.randn(*shape, generator=generator) * self.scheduler.init_noise_sigma"
        ],
        "pos_line": [
            "+sample = randn_tensor(shape, generator=generator) * self.scheduler.init_noise_sigma"
        ],
        "core_change": "-sample = torch.randn(*shape, generator=generator) * self.scheduler.init_noise_sigma +sample = randn_tensor(shape, generator=generator) * self.scheduler.init_noise_sigma",
        "core_API": "randn"
    },
    {
        "commit_hash": "32c3dbbb28835a58f68f2dd179d41078ba854063",
        "index": "1e52b643..f2030ec0 100644",
        "commit_message": "Fix multilabel (#4206)\n\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def compute_loss(pred, true):",
            "",
            "if cfg.model.loss_fun == 'cross_entropy':",
            "# multiclass",
            "-        if pred.ndim > 1:",
            "+        if pred.ndim > 1 and true.ndim == 1:",
            "pred = F.log_softmax(pred, dim=-1)",
            "return F.nll_loss(pred, true), pred",
            "-        # binary",
            "+        # binary or multilabel",
            "else:",
            "true = true.float()",
            "return bce_loss(pred, true), torch.sigmoid(pred)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=986882)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=986883)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=2, insert_id=986884)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=986885)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=986886)",
            "Insert(target_node=IN(type=comparison_operator), node=('integer', '1'), position=2, insert_id=986887)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'true'), position=0, insert_id=986888)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=986889)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ndim'), position=2, insert_id=986890)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 1836,
        "neg_line": [
            "-if pred.ndim > 1:",
            "-# binary"
        ],
        "pos_line": [
            "+if pred.ndim > 1 and true.ndim == 1:",
            "+# binary or multilabel"
        ],
        "core_change": "-if pred.ndim > 1: +if pred.ndim > 1 and true.ndim == 1: -# binary +# binary or multilabel",
        "core_API": "log_softmax"
    },
    {
        "commit_hash": "dac779a5473a40a8231b9dcf95587bcb15400c1a",
        "index": "8321a92..cf542bc 100644",
        "commit_message": "Fix pylint errors\n\n",
        "file": "seq2seq.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFSEquenceExampleDecoder(data_decoder.DataDecoder):",
            "",
            "# Reshape non-sparse elements just once:",
            "for k, value in all_features.items():",
            "-      if isinstance(value, parsing_ops.FixedLenFeature):",
            "-        example[k] = array_ops.reshape(example[k], value.shape)",
            "+      if isinstance(value, tf.FixedLenFeature):",
            "+        example[k] = tf.reshape(example[k], value.shape)",
            "",
            "if not items:",
            "items = self._items_to_handlers.keys()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=parsing_ops), value='tf')",
            "Update(target_node=ASTNode(type=identifier, text=array_ops), value='tf')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 1837,
        "neg_line": [
            "-if isinstance(value, parsing_ops.FixedLenFeature):",
            "-example[k] = array_ops.reshape(example[k], value.shape)"
        ],
        "pos_line": [
            "+if isinstance(value, tf.FixedLenFeature):",
            "+example[k] = tf.reshape(example[k], value.shape)"
        ],
        "core_change": "-if isinstance(value, parsing_ops.FixedLenFeature): -example[k] = array_ops.reshape(example[k], value.shape) +if isinstance(value, tf.FixedLenFeature): +example[k] = tf.reshape(example[k], value.shape)",
        "core_API": "items"
    },
    {
        "commit_hash": "9a2dabae7002258e41419491c73dd43ad61b5de7",
        "index": "9cc49f147..04009ecd9 100644",
        "commit_message": "Fix dtype issue in TF BART (#15178)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFBartForConditionalGeneration(TFBartPretrainedModel, TFCausalLanguageMode",
            "if inputs[\"labels\"] is not None:",
            "inputs[\"labels\"] = tf.where(",
            "inputs[\"labels\"] == self.config.pad_token_id,",
            "-                tf.fill(shape_list(inputs[\"labels\"]), -100),",
            "+                tf.cast(tf.fill(shape_list(inputs[\"labels\"]), -100), inputs[\"labels\"].dtype),",
            "inputs[\"labels\"],",
            ")",
            "inputs[\"use_cache\"] = False"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2367780)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2367781)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2367782)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2367783)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=2, insert_id=2367784)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2367785)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2367786)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=2367787)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2367788)",
            "Insert(target_node=IN(type=attribute), node=('subscript', None), position=0, insert_id=2367789)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2367790)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=2367791)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'inputs'), position=0, insert_id=2367792)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2367793)",
            "Insert(target_node=IN(type=subscript), node=('string', '\"labels\"'), position=2, insert_id=2367794)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2367795)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1838,
        "neg_line": [
            "-tf.fill(shape_list(inputs[\"labels\"]), -100),"
        ],
        "pos_line": [
            "+tf.cast(tf.fill(shape_list(inputs[\"labels\"]), -100), inputs[\"labels\"].dtype),"
        ],
        "core_change": "-tf.fill(shape_list(inputs[\"labels\"]), -100), +tf.cast(tf.fill(shape_list(inputs[\"labels\"]), -100), inputs[\"labels\"].dtype),",
        "core_API": "where"
    },
    {
        "commit_hash": "220dafaedcd3fb842059e9550db0e4be1c9413a9",
        "index": "c3e27e3..6ca0d2e 100644",
        "commit_message": "Fixed a problem with dropout\n",
        "file": "facenet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def inference_nn4_max_pool_96(images, phase_train=True):",
            "affn1 = _affine(resh1, 896, 128)",
            "if FLAGS.keep_probability<1.0:",
            "affn1 = control_flow_ops.cond(phase_train,",
            "-                                  lambda: tf.nn.dropout(affn1, FLAGS.keep_probability), affn1)",
            "+                                  lambda: tf.nn.dropout(affn1, FLAGS.keep_probability), lambda: affn1)",
            "norm = tf.nn.l2_normalize(affn1, 1, 1e-10)",
            "",
            "return norm"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('lambda', None), position=5, insert_id=1930914)",
            "Insert(target_node=IN(type=lambda), node=('lambda', 'lambda'), position=0, insert_id=1930915)",
            "Insert(target_node=IN(type=lambda), node=(':', ':'), position=1, insert_id=1930916)",
            "Move(target_node=IN(type=lambda), node=ASTNode(type=identifier, text=affn1), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1840,
        "neg_line": [
            "-lambda: tf.nn.dropout(affn1, FLAGS.keep_probability), affn1)"
        ],
        "pos_line": [
            "+lambda: tf.nn.dropout(affn1, FLAGS.keep_probability), lambda: affn1)"
        ],
        "core_change": "-lambda: tf.nn.dropout(affn1, FLAGS.keep_probability), affn1) +lambda: tf.nn.dropout(affn1, FLAGS.keep_probability), lambda: affn1)",
        "core_API": "cond"
    },
    {
        "commit_hash": "3004f13d36b3ea0bbcdc7f7f4fbf3e83ccb86bd6",
        "index": "5cd33f7c1..917cc48e2 100644",
        "commit_message": "Lite: Fix DataLoader shuffling when using DistributedSampler (#15931)\n\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "dataloader",
        "change": [
            "class LightningLite:",
            "",
            "@staticmethod",
            "def _get_distributed_sampler(dataloader: DataLoader, **kwargs: Any) -> DistributedSampler:",
            "+        kwargs.setdefault(\"shuffle\", isinstance(dataloader.sampler, RandomSampler))",
            "kwargs.setdefault(\"seed\", int(os.getenv(\"PL_GLOBAL_SEED\", 0)))",
            "return DistributedSamplerWrapper(dataloader.sampler, **kwargs)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=1763420)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1763421)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1763422)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1763423)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1763424)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'kwargs'), position=0, insert_id=1763425)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1763426)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'setdefault'), position=2, insert_id=1763427)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1763428)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"shuffle\"'), position=1, insert_id=1763429)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1763430)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=3, insert_id=1763431)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1763432)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=1763433)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1763434)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1763435)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1763436)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1763437)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'RandomSampler'), position=3, insert_id=1763438)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1763439)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dataloader'), position=0, insert_id=1763440)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1763441)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sampler'), position=2, insert_id=1763442)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 24,
        "number": 1842,
        "neg_line": [],
        "pos_line": [
            "+kwargs.setdefault(\"shuffle\", isinstance(dataloader.sampler, RandomSampler))"
        ],
        "core_change": "+kwargs.setdefault(\"shuffle\", isinstance(dataloader.sampler, RandomSampler))",
        "core_API": "setdefault"
    },
    {
        "commit_hash": "188df74efe0213fcddccc49cfed8904d94d776eb",
        "index": "dbe3b619..711d39a1 100644",
        "commit_message": "Add Homography Tracker API (#1389)\n\n* rebase commit\n\n* init\n\n* added test data for loftr and image registrator\n\n* real ransac tests\n\n* save\n\n* works\n\n* lint\n\n* pdated example\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* docs\n\n* docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* stupid CI OOM\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* lint\n\n* fix dtype in tests\n\n* fix test dtype\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* no-grad for homotest\n\n* fix\n\n* fix\n\n* fix random seed for ransac test\n\n* some formatting\n\n* codespell\n\n* fix signature defaults\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add fixture with data\n\n* remove files\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix deepsource\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* deepsource\n\n* fix super\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* identation fix\n\n* fix positional\n\n* fix mypy\n\n* refactor some tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove doctest\n\n* fix ransac tests\n\n* fix similarity tests\n\n* fix sha path\n\n* fix loftr precision\n\n* xfail some ransac tests\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FineMatching(nn.Module):",
            "",
            "# compute coordinates from heatmap",
            "coords_normalized = dsnt.spatial_expectation2d(heatmap[None], True)[0]  # [M, 2]",
            "-        grid_normalized = create_meshgrid(W, W, True, heatmap.device).reshape(1, -1, 2)  # [1, WW, 2]",
            "+        grid_normalized = create_meshgrid(",
            "+            W, W, normalized_coordinates=True, device=heatmap.device, dtype=heatmap.dtype",
            "+        ).reshape(1, -1, 2)  # [1, WW, 2]",
            "",
            "# compute std over <x, y>",
            "var = torch.sum(grid_normalized**2 * heatmap.view(-1, WW, 1), dim=1) - coords_normalized**2  # [M, 2]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=413478)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=8, insert_id=413479)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=9, insert_id=413480)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=10, insert_id=413481)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'normalized_coordinates'), position=0, insert_id=413482)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=413483)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=true, text=True), position=2)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=413484)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=413485)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=attribute), position=2)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=413486)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=413487)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=413488)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'heatmap'), position=0, insert_id=413489)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=413490)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=413491)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 1843,
        "neg_line": [
            "-grid_normalized = create_meshgrid(W, W, True, heatmap.device).reshape(1, -1, 2)  # [1, WW, 2]"
        ],
        "pos_line": [
            "+grid_normalized = create_meshgrid(",
            "+W, W, normalized_coordinates=True, device=heatmap.device, dtype=heatmap.dtype",
            "+).reshape(1, -1, 2)  # [1, WW, 2]"
        ],
        "core_change": "-grid_normalized = create_meshgrid(W, W, True, heatmap.device).reshape(1, -1, 2)  # [1, WW, 2] +grid_normalized = create_meshgrid( +W, W, normalized_coordinates=True, device=heatmap.device, dtype=heatmap.dtype +).reshape(1, -1, 2)  # [1, WW, 2]",
        "core_API": "spatial_expectation2d"
    },
    {
        "commit_hash": "9c7052dcc1188d950d24b7f4d2cc31d3c563ab92",
        "index": "cd9d888d..86b2e6d2 100644",
        "commit_message": "Small fixes for compatibility with pytorch master (#442)\n\n* Small fixes for campatibility with pytorch master\n\n* Support older pytorch 0.2 torch.nn.functional.softmax\n\n* Work around bugs in torch.cat()\n\n* Replace torch_cat(-) with torch.stack(-).squeeze()\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "autograd",
        "change": [
            "class Histogram(pyro.distributions.Distribution):",
            "vs.append(v)",
            "log_weights.append(log_weight)",
            "",
            "-        log_weights = torch.cat(log_weights)",
            "-        if not isinstance(log_weights, torch.autograd.Variable):",
            "+        log_weights = torch.stack(log_weights).squeeze()  # Work around bug in torch.cat().",
            "+        if not isinstance(log_weights, Variable):",
            "log_weights = Variable(log_weights)",
            "log_z = pyro.util.log_sum_exp(log_weights)",
            "ps = torch.exp(log_weights - log_z.expand_as(log_weights))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=1578795)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1578796)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1578797)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1578798)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'squeeze'), position=2, insert_id=1578799)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1578800)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1578801)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=Variable), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=cat), value='stack')",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=autograd))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 1844,
        "neg_line": [
            "-log_weights = torch.cat(log_weights)",
            "-if not isinstance(log_weights, torch.autograd.Variable):"
        ],
        "pos_line": [
            "+log_weights = torch.stack(log_weights).squeeze()  # Work around bug in torch.cat().",
            "+if not isinstance(log_weights, Variable):"
        ],
        "core_change": "-log_weights = torch.cat(log_weights) -if not isinstance(log_weights, torch.autograd.Variable): +log_weights = torch.stack(log_weights).squeeze()  # Work around bug in torch.cat(). +if not isinstance(log_weights, Variable):",
        "core_API": "append"
    },
    {
        "commit_hash": "625f512d5e285ddf1288653457e99368575c0de4",
        "index": "372bbcb08..a7d82f2b3 100644",
        "commit_message": "[TFWav2Vec2] Fix docs (#12283)\n\n* fix error\n\n* make style check happy\n\nCo-authored-by: chenhaitao <chenhaitao@qiyi.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFWav2Vec2ForCTC(TFWav2Vec2PreTrainedModel):",
            "",
            ">>> # wrap processor as target processor to encode labels",
            ">>> with processor.as_target_processor():",
            "-            >>>     labels = processor(transcription, return_tensors=\"tf\").input_values",
            "+            >>>     labels = processor(transcription, return_tensors=\"tf\").input_ids",
            "",
            ">>> loss = model(input_values, labels=labels).loss",
            "\"\"\""
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=input_values), value='input_ids')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1846,
        "neg_line": [
            "->>>     labels = processor(transcription, return_tensors=\"tf\").input_values"
        ],
        "pos_line": [
            "+>>>     labels = processor(transcription, return_tensors=\"tf\").input_ids"
        ],
        "core_change": "->>>     labels = processor(transcription, return_tensors=\"tf\").input_values +>>>     labels = processor(transcription, return_tensors=\"tf\").input_ids",
        "core_API": "as_target_processor"
    },
    {
        "commit_hash": "c6519f29f0512e209906f8265e0d049085670304",
        "index": "e901f9d..40497c9 100644",
        "commit_message": "chamfer for empty pointclouds #1174\n\nSummary: Fix divide by zero for empty pointcloud in chamfer. Also for empty batches. In process, needed to regularize num_points_per_cloud for empty batches.\n\nReviewed By: kjchalup\n\nDifferential Revision: D36311330\n\nfbshipit-source-id: 3378ab738bee77ecc286f2110a5c8dc445960340\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def point_mesh_face_distance(",
            "# weight each example by the inverse of number of points in the example",
            "point_to_cloud_idx = pcls.packed_to_cloud_idx()  # (sum(P_i),)",
            "num_points_per_cloud = pcls.num_points_per_cloud()  # (N,)",
            "+    # pyre-ignore[16]: `torch.Tensor` has no attribute `gather`",
            "weights_p = num_points_per_cloud.gather(0, point_to_cloud_idx)",
            "weights_p = 1.0 / weights_p.float()",
            "point_to_face = point_to_face * weights_p"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 1848,
        "neg_line": [],
        "pos_line": [
            "+# pyre-ignore[16]: `torch.Tensor` has no attribute `gather`"
        ],
        "core_change": "+# pyre-ignore[16]: `torch.Tensor` has no attribute `gather`",
        "core_API": "packed_to_cloud_idx"
    },
    {
        "commit_hash": "5d7cb6939e344a92b5c0eefe17b85035e2f66b0a",
        "index": "12e34306..fe2aa99e 100644",
        "commit_message": "fix bug in action range\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PolicyNetwork(Model):",
            "mean + std * z",
            ")  # TanhNormal distribution as actions; reparameterization trick",
            "",
            "-        action = self.action_range * mean if deterministic else action",
            "+        action = self.action_range * tf.math.tanh(mean) if deterministic else action",
            "return action.numpy()[0]",
            "",
            "def sample_action(self, ):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=2249732)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2249733)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2249734)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2249735)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2249736)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tanh'), position=2, insert_id=2249737)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2249738)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=mean), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2249739)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2249740)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2249741)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2249742)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 1851,
        "neg_line": [
            "-action = self.action_range * mean if deterministic else action"
        ],
        "pos_line": [
            "+action = self.action_range * tf.math.tanh(mean) if deterministic else action"
        ],
        "core_change": "-action = self.action_range * mean if deterministic else action +action = self.action_range * tf.math.tanh(mean) if deterministic else action",
        "core_API": "tanh"
    },
    {
        "commit_hash": "a542c55575980aa19076bc53595c86df2b4103e3",
        "index": "c1b8bc8..c7311c9 100644",
        "commit_message": "Fix lovasz loss (#461)\n\n\n",
        "file": "segmentation_models.pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _flatten_probas(probas, labels, ignore=None):",
            "probas = probas.view(B, 1, H, W)",
            "",
            "C = probas.size(1)",
            "-    probas = torch.movedim(probas, 0, -1)  # [B, C, Di, Dj, Dk...] -> [B, C, Di...Dk, C]",
            "+    probas = torch.movedim(probas, 1, -1)  # [B, C, Di, Dj, ...] -> [B, Di, Dj, ..., C]",
            "probas = probas.contiguous().view(-1, C)  # [P, C]",
            "",
            "labels = labels.view(-1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=0), value='1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1852,
        "neg_line": [
            "-probas = torch.movedim(probas, 0, -1)  # [B, C, Di, Dj, Dk...] -> [B, C, Di...Dk, C]"
        ],
        "pos_line": [
            "+probas = torch.movedim(probas, 1, -1)  # [B, C, Di, Dj, ...] -> [B, Di, Dj, ..., C]"
        ],
        "core_change": "-probas = torch.movedim(probas, 0, -1)  # [B, C, Di, Dj, Dk...] -> [B, C, Di...Dk, C] +probas = torch.movedim(probas, 1, -1)  # [B, C, Di, Dj, ...] -> [B, Di, Dj, ..., C]",
        "core_API": "view"
    },
    {
        "commit_hash": "0db2046b0a35d5ce7a411489a9d7d14ee254d15d",
        "index": "50aa882464..fe18fad422 100644",
        "commit_message": "[RLlib] Policy.compute_log_likelihoods() and SAC refactor. (issue #7107) (#7124)\n\n* Exploration API (+EpsilonGreedy sub-class).\n\n* Exploration API (+EpsilonGreedy sub-class).\n\n* Cleanup/LINT.\n\n* Add `deterministic` to generic Trainer config (NOTE: this is still ignored by most Agents).\n\n* Add `error` option to deprecation_warning().\n\n* WIP.\n\n* Bug fix: Get exploration-info for tf framework.\nBug fix: Properly deprecate some DQN config keys.\n\n* WIP.\n\n* LINT.\n\n* WIP.\n\n* Split PerWorkerEpsilonGreedy out of EpsilonGreedy.\nDocstrings.\n\n* Fix bug in sampler.py in case Policy has self.exploration = None\n\n* Update rllib/agents/dqn/dqn.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* Update rllib/agents/trainer.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* Change requests.\n\n* LINT\n\n* In tune/utils/util.py::deep_update() Only keep deep_updat'ing if both original and value are dicts. If value is not a dict, set\n\n* Completely obsolete syn_replay_optimizer.py's parameters schedule_max_timesteps AND beta_annealing_fraction (replaced with prioritized_replay_beta_annealing_timesteps).\n\n* Update rllib/evaluation/worker_set.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Review fixes.\n\n* Fix default value for DQN's exploration spec.\n\n* LINT\n\n* Fix recursion bug (wrong parent c'tor).\n\n* Do not pass timestep to get_exploration_info.\n\n* Update tf_policy.py\n\n* Fix some remaining issues with test cases and remove more deprecated DQN/APEX exploration configs.\n\n* Bug fix tf-action-dist\n\n* DDPG incompatibility bug fix with new DQN exploration handling (which is imported by DDPG).\n\n* Switch off exploration when getting action probs from off-policy-estimator's policy.\n\n* LINT\n\n* Fix test_checkpoint_restore.py.\n\n* Deprecate all SAC exploration (unused) configs.\n\n* Properly use `model.last_output()` everywhere. Instead of `model._last_output`.\n\n* WIP.\n\n* Take out set_epsilon from multi-agent-env test (not needed, decays anyway).\n\n* WIP.\n\n* Trigger re-test (flaky checkpoint-restore test).\n\n* WIP.\n\n* WIP.\n\n* Add test case for deterministic action sampling in PPO.\n\n* bug fix.\n\n* Added deterministic test cases for different Agents.\n\n* Fix problem with TupleActions in dynamic-tf-policy.\n\n* Separate supported_spaces tests so they can be run separately for easier debugging.\n\n* LINT.\n\n* Fix autoregressive_action_dist.py test case.\n\n* Re-test.\n\n* Fix.\n\n* Remove duplicate py_test rule from bazel.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* SAC fix.\n\n* SAC fix.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* FIX 2 examples tests.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* Renamed test file.\n\n* WIP.\n\n* Add unittest.main.\n\n* Make action_dist_class mandatory.\n\n* fix\n\n* FIX.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix explorations test case (contextlib cannot find its own nullcontext??).\n\n* Force torch to be installed for QMIX.\n\n* LINT.\n\n* Fix determine_tests_to_run.py.\n\n* Fix determine_tests_to_run.py.\n\n* WIP\n\n* Add Random exploration component to tests (fixed issue with \"static-graph randomness\" via py_function).\n\n* Add Random exploration component to tests (fixed issue with \"static-graph randomness\" via py_function).\n\n* Rename some stuff.\n\n* Rename some stuff.\n\n* WIP.\n\n* WIP.\n\n* Fix SAC.\n\n* Fix SAC.\n\n* Fix strange tf-error in ray core tests.\n\n* Fix strange ray-core tf-error in test_memory_scheduling test case.\n\n* Fix test_io.py.\n\n* LINT.\n\n* Update SAC yaml files' config.\n\nCo-authored-by: Eric Liang <ekhliang@gmail.com>\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class MADDPGTFPolicy(MADDPGPostprocessing, TFPolicy):",
            "out = obs",
            "",
            "for hidden in hiddens:",
            "-                out = tf.layers.dense(",
            "-                    out, units=hidden, activation=activation",
            "-                )",
            "+                out = tf.layers.dense(out, units=hidden, activation=activation)",
            "feature = tf.layers.dense(",
            "out, units=act_space.shape[0], activation=None)",
            "sampler = tfp.distributions.RelaxedOneHotCategorical("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 1854,
        "neg_line": [
            "-out = tf.layers.dense(",
            "-out, units=hidden, activation=activation",
            "-)"
        ],
        "pos_line": [
            "+out = tf.layers.dense(out, units=hidden, activation=activation)"
        ],
        "core_change": "-out = tf.layers.dense( -out, units=hidden, activation=activation -) +out = tf.layers.dense(out, units=hidden, activation=activation)",
        "core_API": "dense"
    },
    {
        "commit_hash": "b903d3d3c1a786ea5af6087241419dacea8aa425",
        "index": "00dfeb18..465c168c 100644",
        "commit_message": "fix einsum\n\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CrossAttention(nn.Module):",
            "# attention, what we cannot get enough of",
            "attn = sim.softmax(dim=-1)",
            "",
            "-        out = einsum('b i j, b j d -> b i d', attn, v)",
            "+        out = torch.einsum('b i j, b j d -> b i d', attn, v)",
            "out = rearrange(out, '(b h) n d -> b n (h d)', h=h)",
            "return self.to_out(out)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=115100)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=115101)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=115102)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=einsum), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1855,
        "neg_line": [
            "-out = einsum('b i j, b j d -> b i d', attn, v)"
        ],
        "pos_line": [
            "+out = torch.einsum('b i j, b j d -> b i d', attn, v)"
        ],
        "core_change": "-out = einsum('b i j, b j d -> b i d', attn, v) +out = torch.einsum('b i j, b j d -> b i d', attn, v)",
        "core_API": "softmax"
    },
    {
        "commit_hash": "b43abdb296d0bc937d02cf37060089d0a1f94c06",
        "index": "96b06fc..7205ace 100644",
        "commit_message": "Fixed GPU check to run lazily to prevent recursive subprocess creation (#1570)\n\nSigned-off-by: Travis Addair <taddair@uber.com>\n\n",
        "file": "horovod.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "handle_average_backwards_compatibility = get_average_backwards_compatibility_fun",
            "",
            "check_num_rank_power_of_2 = num_rank_is_power_2",
            "",
            "+",
            "# This function will create a default device map which includes all visible devices.",
            "# Please run this function in a subprocess",
            "def _check_has_gpu():",
            "-  import tensorflow as tf",
            "-  return tf.test.is_gpu_available()",
            "+    import tensorflow as tf",
            "+    return tf.test.is_gpu_available()",
            "+",
            "",
            "def _normalize_name(name):",
            "\"\"\"Normalizes operation name to TensorFlow rules.\"\"\""
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 1857,
        "neg_line": [
            "-import tensorflow as tf",
            "-return tf.test.is_gpu_available()"
        ],
        "pos_line": [
            "+",
            "+import tensorflow as tf",
            "+return tf.test.is_gpu_available()",
            "+"
        ],
        "core_change": "+ -import tensorflow as tf -return tf.test.is_gpu_available() +import tensorflow as tf +return tf.test.is_gpu_available() +",
        "core_API": "is_gpu_available"
    },
    {
        "commit_hash": "b6cc9d39651273e8ec2a7e334908ffa9de5c2026",
        "index": "5fcbadb1..8c9eea3d 100644",
        "commit_message": "Optimizing the memory usage in `multi_head_self_attention` and `masked_softmax` (#2405)\n\n* Optimizing the memory usage in `multi_head_self_attention` and `masked_softmax`.\n\nFixes #2185\n\n* Fixes pylint error\n\n* Fixes typo and change the attention dropout back to its original version.\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MultiHeadSelfAttention(Seq2SeqEncoder):",
            "keys_per_head = keys_per_head.view(batch_size * num_heads, timesteps, int(self._attention_dim/num_heads))",
            "",
            "# shape (num_heads * batch_size, timesteps, timesteps)",
            "-        scaled_similarities = torch.bmm(queries_per_head, keys_per_head.transpose(1, 2)) / self._scale",
            "+        scaled_similarities = torch.bmm(queries_per_head / self._scale, keys_per_head.transpose(1, 2))",
            "",
            "# shape (num_heads * batch_size, timesteps, timesteps)",
            "# Normalise the distributions, using the same mask for all heads.",
            "-        attention = masked_softmax(scaled_similarities, mask.repeat(1, num_heads).view(batch_size * num_heads, timesteps))",
            "+        attention = masked_softmax(scaled_similarities,",
            "+                                   mask.repeat(1, num_heads).view(batch_size * num_heads, timesteps),",
            "+                                   memory_efficient=True)",
            "attention = self._attention_dropout(attention)",
            "",
            "# Take a weighted sum of the values with respect to the attention"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=30503)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=30504)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=30505)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=queries_per_head), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('/', '/'), position=1, insert_id=30506)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=attribute), position=2)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'memory_efficient'), position=0, insert_id=30507)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=30508)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=30509)",
            "Delete(target_node=ASTNode(type=/, text=/))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 1859,
        "neg_line": [
            "-scaled_similarities = torch.bmm(queries_per_head, keys_per_head.transpose(1, 2)) / self._scale",
            "-attention = masked_softmax(scaled_similarities, mask.repeat(1, num_heads).view(batch_size * num_heads, timesteps))"
        ],
        "pos_line": [
            "+scaled_similarities = torch.bmm(queries_per_head / self._scale, keys_per_head.transpose(1, 2))",
            "+attention = masked_softmax(scaled_similarities,",
            "+mask.repeat(1, num_heads).view(batch_size * num_heads, timesteps),",
            "+memory_efficient=True)"
        ],
        "core_change": "-scaled_similarities = torch.bmm(queries_per_head, keys_per_head.transpose(1, 2)) / self._scale +scaled_similarities = torch.bmm(queries_per_head / self._scale, keys_per_head.transpose(1, 2)) -attention = masked_softmax(scaled_similarities, mask.repeat(1, num_heads).view(batch_size * num_heads, timesteps)) +attention = masked_softmax(scaled_similarities, +mask.repeat(1, num_heads).view(batch_size * num_heads, timesteps), +memory_efficient=True)",
        "core_API": "view"
    },
    {
        "commit_hash": "430397930ff251081065ffe194a4f59278a7e012",
        "index": "f8c68526..eaecf468 100644",
        "commit_message": "Remove the use of global variables. (#388)\n\n* fixed bug of time distributed layer and release 2 tests\n\n* speed up test mnist\n\n* fixed test file name\n\n* remove unused code\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "y_ = tf.placeholder(tf.int64, shape=[None], name='y_')",
            "# define the network",
            "def mlp(x, is_train=True, reuse=False):",
            "with tf.variable_scope(\"MLP\", reuse=reuse):",
            "-        tl.layers.set_name_reuse(reuse)",
            "network = tl.layers.InputLayer(x, name='input')",
            "network = tl.layers.DropoutLayer(network, keep=0.8, is_fix=True, is_train=is_train, name='drop1')",
            "network = tl.layers.DenseLayer(network, n_units=800, act=tf.nn.relu, name='relu1')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', ''), position=0, insert_id=2635457)",
            "Delete(target_node=ASTNode(type=identifier, text=tl))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=layers))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=set_name_reuse))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=reuse))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 1860,
        "neg_line": [
            "-tl.layers.set_name_reuse(reuse)"
        ],
        "pos_line": [],
        "core_change": "-tl.layers.set_name_reuse(reuse)",
        "core_API": "placeholder"
    },
    {
        "commit_hash": "953958372a6a960aedfc70027c451f8f6ef1b6c2",
        "index": "dbdd5d70b..d3c9fd94c 100755",
        "commit_message": "XLNet Bug when training with apex 16-bit precision (#6567)\n\n* xlnet fp16 bug fix\n\n* comment cast added\n\n* Update modeling_xlnet.py\n\nCo-authored-by: Kevin Canwen Xu <canwenxu@126.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class XLNetRelativeAttention(nn.Module):",
            "v_head_h = torch.einsum(\"ibh,hnd->ibnd\", cat, self.v)",
            "",
            "# positional heads",
            "-            k_head_r = torch.einsum(\"ibh,hnd->ibnd\", r, self.r)",
            "+            # type casting for fp16 support",
            "+            k_head_r = torch.einsum(\"ibh,hnd->ibnd\", r.type(self.r.dtype), self.r)",
            "",
            "# core attention ops",
            "attn_vec = self.rel_attn_core("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=1234674)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1234675)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1234676)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=r), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1234677)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type'), position=2, insert_id=1234678)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1234679)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1234680)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1234681)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1234682)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1234683)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1234684)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1234685)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1234686)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'r'), position=2, insert_id=1234687)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 1861,
        "neg_line": [
            "-k_head_r = torch.einsum(\"ibh,hnd->ibnd\", r, self.r)"
        ],
        "pos_line": [
            "+# type casting for fp16 support",
            "+k_head_r = torch.einsum(\"ibh,hnd->ibnd\", r.type(self.r.dtype), self.r)"
        ],
        "core_change": "-k_head_r = torch.einsum(\"ibh,hnd->ibnd\", r, self.r) +# type casting for fp16 support +k_head_r = torch.einsum(\"ibh,hnd->ibnd\", r.type(self.r.dtype), self.r)",
        "core_API": "einsum"
    },
    {
        "commit_hash": "472f39478861f1498fdbd26ed895df590e7b3c82",
        "index": "9ff5c3db8..2025bd511 100644",
        "commit_message": "Resolve some codefactor issues (#756)\n\n* remove unnecessary pass statements\n\n* use isinstance for type checks\n\n* remove unnecessary else/elif after return\n\n* remove unnecessary return statements\n\n* move doc string to top\n\n* merge isinstance calls\n\n* remove unnecessary else/elif after raise\n\n* use list comprehension\n\n* do not use len without comparison\n\n* add missing shebang\n\n* revert isinstance check back to type\n\nbroke tests, because bool is actually subclass of int\n\n* add missing period to doc string\n\n* remove unnecessary pass statements\n\n* use isinstance for type checks\n\n* remove unnecessary else/elif after return\n\n* remove unnecessary return statements\n\n* move doc string to top\n\n* merge isinstance calls\n\n* remove unnecessary else/elif after raise\n\n* use list comprehension\n\n* do not use len without comparison\n\n* add missing shebang\n\n* revert isinstance check back to type\n\nbroke tests, because bool is actually subclass of int\n\n* add missing period to doc string\n\n* Fix default ckpt path when logger exists (#771)\n\n* rename logging -> loggers (#767)\n\n* move logging >> loggers\n\n* add warning\n\n* fix tests\n\n* logging alias\n\n* formatting\n\n* formatting\n\n* use isinstance for type checks\n\n* revert isinstance check back to type\n\nbroke tests, because bool is actually subclass of int\n\n* add more detail to tbptt example (#755)\n\n* add more detail to tbptt example\n\n* warn user about new arg in training_step\n\nCo-authored-by: Vadim Bereznyuk <kuynzereb@gmail.com>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Jeremy Jordan <13970565+jeremyjordan@users.noreply.github.com>\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TrainerLoggingMixin(ABC):",
            "",
            "# when using DP, we get one output per gpu",
            "# average outputs and return",
            "-        if type(output) is torch.Tensor:",
            "+        if isinstance(output, torch.Tensor):",
            "return output.mean()",
            "",
            "for k, v in output.items():"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('call', None), position=1, insert_id=587376)",
            "Update(target_node=ASTNode(type=identifier, text=type), value='isinstance')",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=type), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=587377)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=output), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=587378)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=587379)",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=is, text=is))",
            "Delete(target_node=ASTNode(type=comparison_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 1862,
        "neg_line": [
            "-if type(output) is torch.Tensor:"
        ],
        "pos_line": [
            "+if isinstance(output, torch.Tensor):"
        ],
        "core_change": "-if type(output) is torch.Tensor: +if isinstance(output, torch.Tensor):",
        "core_API": "mean"
    },
    {
        "commit_hash": "1c825a2a9c8ade452820b27c526feaa186ff022d",
        "index": "c35fc64e2..5f3fc5b8d 100644",
        "commit_message": "Add the `on_before_backward` hook (#7865)\n\n* Add callback to hook tests and add predict test\n\n* Fix lambda callback test\n\n* Simplify lambda call test\n\n* Use LambdaCallback\n\n* Dynamically append to called for the model\n\n* Remove print\n\n* Consistency\n\n* Consistency\n\n* Prepare args/kwargs testing\n\n* yapf doesn't like dict literals\n\n* Add arguments for fit no val test\n\n* Add arguments for fit no val test\n\n* add before_backward_hook\n\n* add test\n\n* resolve flake8\n\n* resolve tests\n\n* update changelog\n\n* add on_before_backward to LightningModule\n\n* update on comments\n\n* Test arguments\n\n* Datamodule refactor\n\n* Fix eval test\n\n* remove extra file\n\n* resolve bug\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* move to hooks\n\n* update\n\n* resolve flake8\n\n* update on comments\n\n* Update full fit + val test\n\n* Update test\n\n* Remove FIXME\n\n* Remove FIXME\n\n* Undo change\n\n* Fix\n\n* Parametrize fit hook test\n\n* Comment\n\n* Parametrize fit hook test with different precision plugins\n\n* Fix tests\n\n* Parametrize fit hook test with manual optimization\n\n* Unnecessary parenthesis\n\n* WIP\n\n* Comments\n\n* Fix message\n\n* Test CI error\n\n* Revert \"Test CI error\"\n\nThis reverts commit 39c4a85a83cf32081b721f939ff83500b93f2dd3.\n\n* Add ddp training type teardown\n\n* Update CHANGELOG\n\n* Adrian's fix\n\n* Use destructor\n\n* Update CHANGELOG.md\n\n* RPC destructor\n\n* Update pytorch_lightning/plugins/training_type/ddp.py\n\n* Why do you not work :(\n\n* Missing condition\n\n* Fix deepspeed test\n\n* GC collect in conftest\n\n* Do not show warnings for special tests\n\n* Needs to run on 1.8\n\nTo avoid: \"RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:32, unhandled cuda error, NCCL version 2.4.8\"\n\n* Run torch 1.8\n\n* Skip test due to 'Python bus error'\n\n* Debug NCCL\n\n* shm size\n\n* Disable warnings for special tests\n\n* Remove NCCL_DEBUG statement\n\n* Try smaller shm size\n\n* Revert \"Skip test due to 'Python bus error'\"\n\nThis reverts commit e0a3e8785d2fecd63667da433a648f958d60ef89.\n\n* README and adjust versions\n\n* Avoid self.on_gpu call\n\n* empty cache cleanup\n\n* More garbage collection\n\n* Unroll parametrizations\n\n* Do not reuse mock\n\n* Undo changes\n\n* Undo notebooks modification\n\n* resolve test\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* delete file\n\n* Undo\n\n* Fix test\n\n* Revert \"WIP\"\n\nThis reverts commit f5828a8c426ff44275f560aec8d898f56da2cbfe.\n\n* Rename\n\n* Remove optimizers\n\n* Fix bug with LightningOptimizer\n\n* Add optimizers\n\n* update\n\n* update\n\n* Update CHANGELOG\n\n* On after backward refactor\n\n* Do not call super\n\n* Fixes\n\n* Remove should_accumulate\n\n* pre/post backward refactor\n\n* Call the LM backward hook\n\n* Update tests\n\n* Remove dev debug patch\n\n* Fix test\n\n* Remove optimizer arguments and typing\n\n* Docs fixes\n\n* Fix comment\n\n* Undo changes\n\n* Split manual and auto\n\n* Undo change\n\n* Deepsource\n\n* Remove optimizers\n\n* Undo changes\n\n* Call the hook\n\n* Docs\n\n* Docs\n\nCo-authored-by: Carlos Mocholi <carlossmocholi@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Callback(abc.ABC):",
            "\"\"\"",
            "pass",
            "",
            "+    def on_before_backward(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule', loss: torch.Tensor) -> None:",
            "+        \"\"\"Called before ``loss.backward()``.\"\"\"",
            "+        pass",
            "+",
            "def on_after_backward(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None:",
            "\"\"\"Called after ``loss.backward()`` and before optimizers do anything.\"\"\"",
            "pass"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=2, insert_id=530218)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=530219)",
            "Insert(target_node=IN(type=ERROR), node=('string', '``'), position=0, insert_id=530220)",
            "Insert(target_node=IN(type=ERROR), node=('.', '.'), position=1, insert_id=530221)",
            "Insert(target_node=IN(type=ERROR), node=('string', '\"\"\"\\n        pass\\n\\ndef on_after_backward(self, trainer: \\'pl.Trainer\\', pl_module: \\'pl.LightningModule\\') -> None:\\n\"\"\"'), position=2, insert_id=530222)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'Called'), position=3, insert_id=530223)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'after'), position=4, insert_id=530224)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=530225)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=530226)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=530227)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=ASTNode(type=attribute), node=('ERROR', None), position=1, insert_id=530228)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=530229)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=530230)",
            "Insert(target_node=IN(type=attribute), node=('string', '``'), position=0, insert_id=530231)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=ERROR), position=1)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=2, insert_id=530232)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'backward'), position=3, insert_id=530233)",
            "Update(target_node=ASTNode(type=string, text=\"\"\"\npass\n\ndef on_after_backward(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule') -> None:\n\"\"\"), value='\"\"\"\\npass\\n\\n    def on_before_backward(self, trainer: \\'pl.Trainer\\', pl_module: \\'pl.LightningModule\\', loss: torch.Tensor) -> None:\\n        \"\"\"')",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'loss'), position=0, insert_id=530234)",
            "Update(target_node=ASTNode(type=identifier, text=after), value='before')"
        ],
        "plus_line": 3,
        "minus_line": 0,
        "AST_diff_line": 21,
        "number": 1865,
        "neg_line": [],
        "pos_line": [
            "+def on_before_backward(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule', loss: torch.Tensor) -> None:",
            "+\"\"\"Called before ``loss.backward()``.\"\"\"",
            "+pass",
            "+"
        ],
        "core_change": "+def on_before_backward(self, trainer: 'pl.Trainer', pl_module: 'pl.LightningModule', loss: torch.Tensor) -> None: +\"\"\"Called before ``loss.backward()``.\"\"\" +pass +",
        "core_API": "backward"
    },
    {
        "commit_hash": "679d9a79c072c04b5ac15bc9b64285f6e881fccc",
        "index": "e6ef8459..fb9b19e3 100644",
        "commit_message": "unittest bug fix\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class Layer_Embed_Test(CustomTestCase):",
            "embed_tensor, embed_nce_loss = emb_net([inputs, labels])",
            "self.assertEqual(embed_tensor.get_shape().as_list(), [batch_size, embedding_size])",
            "",
            "-        outputs = tl.layers.Dense(n_units=10, name=\"dense\")(embed_tensor)",
            "+        outputs = tl.layers.Dense(n_units=10)(embed_tensor)",
            "model = tl.models.Model(inputs=[inputs, labels], outputs=[outputs, embed_nce_loss], name=\"word2vec_model\")",
            "out, nce = model(",
            "[np.random.randint(0, 1, size=[batch_size]), np.random.randint(0, 1, size=[batch_size, 1])],"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=name))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"dense\"))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1866,
        "neg_line": [
            "-outputs = tl.layers.Dense(n_units=10, name=\"dense\")(embed_tensor)"
        ],
        "pos_line": [
            "+outputs = tl.layers.Dense(n_units=10)(embed_tensor)"
        ],
        "core_change": "-outputs = tl.layers.Dense(n_units=10, name=\"dense\")(embed_tensor) +outputs = tl.layers.Dense(n_units=10)(embed_tensor)",
        "core_API": "assertEqual"
    },
    {
        "commit_hash": "df31dde174ce7c64421896f26116eb3a4e24d51f",
        "index": "6ca4e52..aeacccf 100644",
        "commit_message": "hasattr and DeviceBuffer type fixups\n\n",
        "file": "tinygrad.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_lazyop_shape(ast:LazyOp): return GenericShape.exec_ast(ast, GenericShape",
            "",
            "# assumes you are using ShapeTracker",
            "# used in GPUBuffer, OpenCLBuffer, and LLVMBuffer",
            "-# type: ignore",
            "-class ExplicitExecAST:",
            "+class ExplicitExecAST(DeviceBuffer):",
            "def __init__(self, shape:Union[ShapeTracker, Tuple[int, ...]], hostbuf=None):",
            "self.st = shape if isinstance(shape, ShapeTracker) else ShapeTracker(tuple(shape))",
            "self.shape = self.st.shape"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('call', None), position=0, insert_id=1163349)",
            "Insert(target_node=ASTNode(type=call), node=('ERROR', None), position=1, insert_id=1163350)",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=GenericShape), position=0)",
            "Insert(target_node=IN(type=call), node=('ERROR', None), position=1, insert_id=1163351)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=2, insert_id=1163352)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=:, text=:), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=def, text=def), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=__init__), position=2)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=class), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=ExplicitExecAST), position=1)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1163353)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'DeviceBuffer'), position=1, insert_id=1163354)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1163355)",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 14,
        "number": 1867,
        "neg_line": [
            "-# type: ignore",
            "-class ExplicitExecAST:"
        ],
        "pos_line": [
            "+class ExplicitExecAST(DeviceBuffer):"
        ],
        "core_change": "-# type: ignore -class ExplicitExecAST: +class ExplicitExecAST(DeviceBuffer):",
        "core_API": "exec_ast"
    },
    {
        "commit_hash": "8a8a19ba1eb74feb5da22399a44e81b734c2b2d8",
        "index": "07d03691..936d6bc7 100644",
        "commit_message": "docstring fixes (#10603)\n\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class Network(Layer):",
            "layer.name + '.\\n'",
            "'Note that input tensors are '",
            "'instantiated via '",
            "-                              '`tensor = tf.layers.Input(shape)`.\\n'",
            "+                              '`tensor = keras.layers.Input(shape)`.\\n'",
            "'The tensor that caused the issue was: ' +",
            "str(x.name))",
            "for x in self.outputs:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='`tensor = tf.layers.Input(shape)`.\\n'), value=\"'`tensor = keras.layers.Input(shape)`.\\\\n'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1868,
        "neg_line": [
            "-'`tensor = tf.layers.Input(shape)`.\\n'"
        ],
        "pos_line": [
            "+'`tensor = keras.layers.Input(shape)`.\\n'"
        ],
        "core_change": "-'`tensor = tf.layers.Input(shape)`.\\n' +'`tensor = keras.layers.Input(shape)`.\\n'",
        "core_API": "Input"
    },
    {
        "commit_hash": "fd1fe0dd0e24b2a24c5c852385e8277efb96166c",
        "index": "b360d571..b7694784 100644",
        "commit_message": "fix linting\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def gini(w):",
            "for row in w:",
            "t = row.repeat(row.shape[0], 1)",
            "u = (t - t.T).abs().sum() / (",
            "-                2 * (",
            "-                    row.shape[-1] ** 2 - row.shape[-1]",
            "-                ) * row.abs().mean() + torch.finfo().eps",
            "+            2 * (",
            "+                row.shape[-1] ** 2 - row.shape[-1]",
            "+            ) * row.abs().mean() + torch.finfo().eps",
            ")",
            "s += u",
            "s /= w.shape[0]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 1870,
        "neg_line": [
            "-2 * (",
            "-row.shape[-1] ** 2 - row.shape[-1]",
            "-) * row.abs().mean() + torch.finfo().eps"
        ],
        "pos_line": [
            "+2 * (",
            "+row.shape[-1] ** 2 - row.shape[-1]",
            "+) * row.abs().mean() + torch.finfo().eps"
        ],
        "core_change": "-2 * ( -row.shape[-1] ** 2 - row.shape[-1] -) * row.abs().mean() + torch.finfo().eps +2 * ( +row.shape[-1] ** 2 - row.shape[-1] +) * row.abs().mean() + torch.finfo().eps",
        "core_API": "repeat"
    },
    {
        "commit_hash": "593dd8cf58fcd2fc0ba48aa81724cd82b904f7cf",
        "index": "c3331d18..bb83580e 100644",
        "commit_message": "Fix special values overflow/underflow for amp (#3901)\n\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestScalarMix(AllenNlpTestCase):",
            "for k in range(3):",
            "mean = numpy.mean(tensors[k].data.numpy()[numpy_mask == 1])",
            "std = numpy.std(tensors[k].data.numpy()[numpy_mask == 1])",
            "-            normed_tensor = (tensors[k].data.numpy() - mean) / (std + 1e-12)",
            "+            normed_tensor = (tensors[k].data.numpy() - mean) / (",
            "+                std + util.tiny_value_of_dtype(torch.float)",
            "+            )",
            "expected_result += normed_tensor * normed_weights[k]",
            "expected_result *= 0.5"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=18901)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=18902)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=18903)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'util'), position=0, insert_id=18904)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=18905)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tiny_value_of_dtype'), position=2, insert_id=18906)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=18907)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=18908)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=18909)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=18910)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=18911)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float'), position=2, insert_id=18912)",
            "Delete(target_node=ASTNode(type=float, text=1e-12))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 1873,
        "neg_line": [
            "-normed_tensor = (tensors[k].data.numpy() - mean) / (std + 1e-12)"
        ],
        "pos_line": [
            "+normed_tensor = (tensors[k].data.numpy() - mean) / (",
            "+std + util.tiny_value_of_dtype(torch.float)",
            "+)"
        ],
        "core_change": "-normed_tensor = (tensors[k].data.numpy() - mean) / (std + 1e-12) +normed_tensor = (tensors[k].data.numpy() - mean) / ( +std + util.tiny_value_of_dtype(torch.float) +)",
        "core_API": "mean"
    },
    {
        "commit_hash": "e2e393c6f25205739b5dc9fddd460d7bfab85150",
        "index": "592f33cf2..f329f32d4 100644",
        "commit_message": "[`t5`] Fix T5 inference in `float16` + `bnb` error (#21281)\n\n* attempts to fix:\n\n- upcast input for `T5DenseActDense`\n- add the condition `self.wo.weight.dtype != torch.int8`\n- added tests on `test/mixed_int8`\n- `make fixup`\n\n* fix ci test\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class T5DenseGatedActDense(nn.Module):",
            "",
            "# To make 8bit quantization work for google/flan-t5-xxl, self.wo is kept in float32.",
            "# See https://github.com/huggingface/transformers/issues/20287",
            "-        if hidden_states.dtype != self.wo.weight.dtype:",
            "+        # we also make sure the weights are not in `int8` in case users will force `_keep_in_fp32_modules` to be `None``",
            "+        if hidden_states.dtype != self.wo.weight.dtype and self.wo.weight.dtype != torch.int8:",
            "hidden_states = hidden_states.to(self.wo.weight.dtype)",
            "",
            "hidden_states = self.wo(hidden_states)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=1177743)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1177744)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=2, insert_id=1177745)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=1177746)",
            "Insert(target_node=IN(type=comparison_operator), node=('!=', '!='), position=1, insert_id=1177747)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=2, insert_id=1177748)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1177749)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1177750)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1177751)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1177752)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1177753)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int8'), position=2, insert_id=1177754)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1177755)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1177756)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'weight'), position=2, insert_id=1177757)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1177758)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1177759)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'wo'), position=2, insert_id=1177760)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 1875,
        "neg_line": [
            "-if hidden_states.dtype != self.wo.weight.dtype:"
        ],
        "pos_line": [
            "+# we also make sure the weights are not in `int8` in case users will force `_keep_in_fp32_modules` to be `None``",
            "+if hidden_states.dtype != self.wo.weight.dtype and self.wo.weight.dtype != torch.int8:"
        ],
        "core_change": "-if hidden_states.dtype != self.wo.weight.dtype: +# we also make sure the weights are not in `int8` in case users will force `_keep_in_fp32_modules` to be `None`` +if hidden_states.dtype != self.wo.weight.dtype and self.wo.weight.dtype != torch.int8:",
        "core_API": "to"
    },
    {
        "commit_hash": "66d4b129903058365f6cf1d4fb4ac73814ca3bfc",
        "index": "12ee3b9b..187b27e0 100644",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def mnist_test_tsne_ssvae(name=None, ssvae=None, test_loader=None):",
            "\"\"\"",
            "if name is None:",
            "name = 'SS-VAE'",
            "-    data = Variable(test_loader.dataset.test_data.float())",
            "-    mnist_labels = Variable(test_loader.dataset.test_labels)",
            "+    data = test_loader.dataset.test_data.float()",
            "+    mnist_labels = test_loader.dataset.test_labels",
            "z_mu, z_sigma = ssvae.encoder_z([data, mnist_labels])",
            "plot_tsne(z_mu, mnist_labels, name)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 1877,
        "neg_line": [
            "-data = Variable(test_loader.dataset.test_data.float())",
            "-mnist_labels = Variable(test_loader.dataset.test_labels)"
        ],
        "pos_line": [
            "+data = test_loader.dataset.test_data.float()",
            "+mnist_labels = test_loader.dataset.test_labels"
        ],
        "core_change": "-data = Variable(test_loader.dataset.test_data.float()) -mnist_labels = Variable(test_loader.dataset.test_labels) +data = test_loader.dataset.test_data.float() +mnist_labels = test_loader.dataset.test_labels",
        "core_API": "float"
    },
    {
        "commit_hash": "c75bca4589c1b07de38d36ce15d64df12cb9b31b",
        "index": "659ded8..013570e 100644",
        "commit_message": "Fix comment.\n\nPiperOrigin-RevId: 185501790\n\n",
        "file": "sonnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class _ConvND(base.AbstractModule):",
            "while self._mask.shape.ndims < w.shape.ndims:",
            "self._mask = tf.expand_dims(self._mask, -1)",
            "",
            "-    # ResourceVariables currently don't support *=.",
            "+    # tf.Variable & tf.ResourceVariable don't support *=.",
            "w = w * self._mask  # pylint: disable=g-no-augmented-assignment",
            "",
            "return w"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1878,
        "neg_line": [
            "-# ResourceVariables currently don't support *=."
        ],
        "pos_line": [
            "+# tf.Variable & tf.ResourceVariable don't support *=."
        ],
        "core_change": "-# ResourceVariables currently don't support *=. +# tf.Variable & tf.ResourceVariable don't support *=.",
        "core_API": "expand_dims"
    },
    {
        "commit_hash": "c4d691a970b18eec3192247eb0ef104a0e16cc9b",
        "index": "09a8ed0e2b..f256fbe06a 100644",
        "commit_message": "added method closest_valid_dtype, fixed small bugs in iinfo and finfo methods, and other small bug fixes.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Finfo:",
            "return float(self._tf_finfo.tiny)",
            "",
            "",
            "-def finfo(datatype_in):",
            "-    return Finfo(tf.experimental.numpy.finfo(datatype_in))",
            "+def finfo(type):",
            "+    return Finfo(tf.experimental.numpy.finfo(dtype_from_str(type)))",
            "",
            "",
            "backend = 'tensorflow'"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=datatype_in), value='type')",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2034130)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2034131)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=2034132)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2034133)",
            "Insert(target_node=IN(type=call), node=('identifier', 'dtype_from_str'), position=0, insert_id=2034134)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=datatype_in), value='type')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1880,
        "neg_line": [
            "-def finfo(datatype_in):",
            "-return Finfo(tf.experimental.numpy.finfo(datatype_in))"
        ],
        "pos_line": [
            "+def finfo(type):",
            "+return Finfo(tf.experimental.numpy.finfo(dtype_from_str(type)))"
        ],
        "core_change": "-def finfo(datatype_in): -return Finfo(tf.experimental.numpy.finfo(datatype_in)) +def finfo(type): +return Finfo(tf.experimental.numpy.finfo(dtype_from_str(type)))",
        "core_API": "finfo"
    },
    {
        "commit_hash": "dc68ce0d77f6094183ca45c50510ba75447d0bac",
        "index": "75005c7d..85c9f479 100644",
        "commit_message": "Fix chief_only for input callbacks\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ImageNetModel(ModelDesc):",
            "image_dtype = tf.uint8",
            "",
            "def __init__(self, data_format='NCHW'):",
            "-        if data_format == 'NCHW':",
            "-            assert tf.test.is_gpu_available()",
            "self.data_format = data_format",
            "",
            "def _get_inputs(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', ''), position=0, insert_id=2291571)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=data_format))",
            "Delete(target_node=ASTNode(type===, text===))",
            "Delete(target_node=ASTNode(type=string, text='NCHW'))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=assert, text=assert))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=test))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=is_gpu_available))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assert_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 23,
        "number": 1881,
        "neg_line": [
            "-if data_format == 'NCHW':",
            "-assert tf.test.is_gpu_available()"
        ],
        "pos_line": [],
        "core_change": "-if data_format == 'NCHW': -assert tf.test.is_gpu_available()",
        "core_API": "is_gpu_available"
    },
    {
        "commit_hash": "946a0c0fb9f9b8543077dff38946e36ed867365a",
        "index": "83e5d78b..13434979 100644",
        "commit_message": "bug fixes for single speaker glow-tts, enable torch based amp. Make amp optional for wavegrad. Bug fixes for synthesis setup for glow-tts\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "def train(model, criterion, optimizer,",
            "",
            "# check nan loss",
            "if torch.isnan(loss).any():",
            "-          raise RuntimeError(f'Detected NaN loss at step {self.step}.')",
            "+          raise RuntimeError(f'Detected NaN loss at step {global_step}.')",
            "",
            "optimizer.zero_grad()"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=f'Detected NaN loss at step {self.step}.'), value=\"f'Detected NaN loss at step {global_step}.'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1882,
        "neg_line": [
            "-raise RuntimeError(f'Detected NaN loss at step {self.step}.')"
        ],
        "pos_line": [
            "+raise RuntimeError(f'Detected NaN loss at step {global_step}.')"
        ],
        "core_change": "-raise RuntimeError(f'Detected NaN loss at step {self.step}.') +raise RuntimeError(f'Detected NaN loss at step {global_step}.')",
        "core_API": "isnan"
    },
    {
        "commit_hash": "a579326643554b438da0d3a106205cdfa0c7e3f0",
        "index": "bf39681a..a7e9d27f 100644",
        "commit_message": "fixed tf 12 compatible tf.image.per_image_whitening\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "with tf.device('/cpu:0'):",
            "with tf.variable_scope(\"model\", reuse=reuse):",
            "tl.layers.set_name_reuse(reuse)",
            "network = tl.layers.InputLayer(x_crop, name='input_layer')",
            "-",
            "+",
            "network = tl.layers.Conv2dLayer(network, act=tf.identity,",
            "shape=[5, 5, 3, 64], strides=[1, 1, 1, 1], padding='SAME', # 64 features for each 5x5x3 patch",
            "W_init=W_init, b_init=None, name='cnn_layer1')                            # output: (batch_size, 24, 24, 64)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 1885,
        "neg_line": [
            "-"
        ],
        "pos_line": [
            "+"
        ],
        "core_change": "- +",
        "core_API": "device"
    },
    {
        "commit_hash": "f18314d628f692e7dce3413fa0569c178e140df2",
        "index": "290e1c1c..832c5dcd 100644",
        "commit_message": "fix validation bug again\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ValidationCallback(PeriodicCallback):",
            "batch_size = dp[0].shape[0]   # assume batched input",
            "",
            "cnt += batch_size",
            "-                outputs = self.sess.run(output_vars, feed_dict=feed)",
            "+                outputs = sess.run(output_vars, feed_dict=feed)",
            "cost = outputs[-1]",
            "# each batch might not have the same size in validation",
            "cost_sum += cost * batch_size"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=., text=.), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 1887,
        "neg_line": [
            "-outputs = self.sess.run(output_vars, feed_dict=feed)"
        ],
        "pos_line": [
            "+outputs = sess.run(output_vars, feed_dict=feed)"
        ],
        "core_change": "-outputs = self.sess.run(output_vars, feed_dict=feed) +outputs = sess.run(output_vars, feed_dict=feed)",
        "core_API": "run"
    },
    {
        "commit_hash": "786be2a3f8b8160640614638f2fddc11572e2bc8",
        "index": "dbfac4c3e..201b2ebf4 100755",
        "commit_message": "Should be fixed finally\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "# extract",
            "logging.info('backend = ' + args.backend)",
            "if args.backend == \"pytorch\":",
            "-        from espnet.lmpytorch.tts_pytorch import decode",
            "+        from espnet.tts.pytorch.tts_pytorch import decode",
            "decode(args)",
            "else:",
            "raise NotImplementedError(\"Only pytorch is supported.\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=lmpytorch), value='tts')",
            "Insert(target_node=ASTNode(type=dotted_name), node=('.', '.'), position=3, insert_id=178810)",
            "Insert(target_node=ASTNode(type=dotted_name), node=('identifier', 'pytorch'), position=4, insert_id=178811)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 1889,
        "neg_line": [
            "-from espnet.lmpytorch.tts_pytorch import decode"
        ],
        "pos_line": [
            "+from espnet.tts.pytorch.tts_pytorch import decode"
        ],
        "core_change": "-from espnet.lmpytorch.tts_pytorch import decode +from espnet.tts.pytorch.tts_pytorch import decode",
        "core_API": "info"
    },
    {
        "commit_hash": "5c18c0103ed979c3d02c55b4dacfcad02daf811a",
        "index": "c264a25b..87f33923 100644",
        "commit_message": "fix failing tests related to `solve_cast` on torch 1.9 (#2066)\n\n* update torch seed\n\n* rerun CI\n\n* update to use rescale\n\n* rerun ci\n\n* update tests cases to have hardcoded input\n\ndeleted:    test/utilities.py - not used anywhere\n\n* remove fail fast at ci\n\n* manual seed 0 TestImageStitcher::test_smoke\n\n* manual seed 0 TestHomographyTracker::test_real\n\n* manual seed 6 TestHomographyTracker::test_real\n\n* manual seed 245 TestImageStitcher::test_smoke and hardcoded case\n\n* manual seed 1 TestImageStitcher::test_smoke\n\n* manual seed 8 TestHomographyTracker::test_real\n\n- Comment the second frame test\n\n* rerun CI\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestLocalFeatureMatcher:",
            "pts_src = data_dev['pts0']",
            "pts_dst = data_dev['pts1']",
            "with torch.no_grad():",
            "+            torch.manual_seed(0)",
            "out = matcher(data_dev)",
            "homography, inliers = ransac(out['keypoints0'], out['keypoints1'])",
            "assert inliers.sum().item() > 50  # we have enough inliers"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=394392)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=394393)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=394394)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=394395)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=394396)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=394397)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=394398)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'manual_seed'), position=2, insert_id=394399)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=394400)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '0'), position=1, insert_id=394401)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=394402)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 12,
        "number": 1892,
        "neg_line": [],
        "pos_line": [
            "+torch.manual_seed(0)"
        ],
        "core_change": "+torch.manual_seed(0)",
        "core_API": "no_grad"
    },
    {
        "commit_hash": "7d0101dfb0894dee2e76d042afac4887e787b1a0",
        "index": "bfaf6ba4..f094ea72 100755",
        "commit_message": "Quick fix\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def SubpixelConv2d(net, scale=2, n_out_channel=None, act=tf.identity, name='subp",
            "bsize = tf.shape(X)[0] # Handling Dimension(None) type for undefined batch dim",
            "Xs=tf.split(X,r,3) #b*h*w*r*r",
            "Xr=tf.concat(Xs,2) #b*h*(r*w)*r",
            "-            X=tf.reshape(Xr,(bsize,r*a,r*b,c)) # b*(r*h)*(r*w)*c",
            "+            X=tf.reshape(Xr,(bsize,r*a,r*b,n_out_channel)) # b*(r*h)*(r*w)*c",
            "else:",
            "print(_err_log)",
            "return X"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=c), value='n_out_channel')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1893,
        "neg_line": [
            "-X=tf.reshape(Xr,(bsize,r*a,r*b,c)) # b*(r*h)*(r*w)*c"
        ],
        "pos_line": [
            "+X=tf.reshape(Xr,(bsize,r*a,r*b,n_out_channel)) # b*(r*h)*(r*w)*c"
        ],
        "core_change": "-X=tf.reshape(Xr,(bsize,r*a,r*b,c)) # b*(r*h)*(r*w)*c +X=tf.reshape(Xr,(bsize,r*a,r*b,n_out_channel)) # b*(r*h)*(r*w)*c",
        "core_API": "shape"
    },
    {
        "commit_hash": "184109f15474bae2d34c3ba7592010b3ecd30f63",
        "index": "4aeedccf..5ad5fb27 100755",
        "commit_message": "Fix various prioritized replay errors, starts but crashed occasionally.\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Memory(object):",
            "Args:",
            "loss_per_instance: Loss per instance tensor.",
            "\"\"\"",
            "-        pass",
            "+        return tf.no_op()",
            "",
            "def get_variables(self):",
            "\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n        pass\n\ndef get_variables(self):\n\"\"\"), value='\"\"\"\\n        return tf.no_op()\\n\\ndef get_variables(self):\\n\"\"\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1894,
        "neg_line": [
            "-pass"
        ],
        "pos_line": [
            "+return tf.no_op()"
        ],
        "core_change": "-pass +return tf.no_op()",
        "core_API": "no_op"
    },
    {
        "commit_hash": "21f29ce35b23de87d9522fbc4ed8f728e70eb0de",
        "index": "b9d1e4489b..f53ef9623b 100644",
        "commit_message": "added method closest_valid_dtype, fixed small bugs in iinfo and finfo methods, and other small bug fixes.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Finfo:",
            "return self._torch_finfo.tiny",
            "",
            "",
            "-def finfo(datatype_in):",
            "-    return Finfo(_torch.finfo(datatype_in))",
            "+def finfo(type):",
            "+    return Finfo(_torch.finfo(dtype_from_str(type)))",
            "",
            "",
            "backend = 'torch'"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=datatype_in), value='type')",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=379596)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=379597)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=379598)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=379599)",
            "Insert(target_node=IN(type=call), node=('identifier', 'dtype_from_str'), position=0, insert_id=379600)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=datatype_in), value='type')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1895,
        "neg_line": [
            "-def finfo(datatype_in):",
            "-return Finfo(_torch.finfo(datatype_in))"
        ],
        "pos_line": [
            "+def finfo(type):",
            "+return Finfo(_torch.finfo(dtype_from_str(type)))"
        ],
        "core_change": "-def finfo(datatype_in): -return Finfo(_torch.finfo(datatype_in)) +def finfo(type): +return Finfo(_torch.finfo(dtype_from_str(type)))",
        "core_API": "finfo"
    },
    {
        "commit_hash": "3b91f96fc94e9e201a2c637b2f654cbdc6a21ee1",
        "index": "08a09a289..f89a6c224 100644",
        "commit_message": "Fix torch meshgrid warnings (#20475)\n\n* fix torch meshgrid warnings\n\n* support lower torch versions\n\n* don't edit examples\n\n* dont edit examples\n\n* fix ci\n\n* fix style\n\n* rebase cleanup\n\n* fix ci again\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MaskFormerSwinSelfAttention(nn.Module):",
            "# get pair-wise relative position index for each token inside the window",
            "coords_h = torch.arange(self.window_size[0])",
            "coords_w = torch.arange(self.window_size[1])",
            "-        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))",
            "+        coords = torch.stack(meshgrid([coords_h, coords_w], indexing=\"ij\"))",
            "coords_flatten = torch.flatten(coords, 1)",
            "relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]",
            "relative_coords = relative_coords.permute(1, 2, 0).contiguous()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=meshgrid), position=0)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1182471)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1182472)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'indexing'), position=0, insert_id=1182473)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1182474)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"ij\"'), position=2, insert_id=1182475)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 1896,
        "neg_line": [
            "-coords = torch.stack(torch.meshgrid([coords_h, coords_w]))"
        ],
        "pos_line": [
            "+coords = torch.stack(meshgrid([coords_h, coords_w], indexing=\"ij\"))"
        ],
        "core_change": "-coords = torch.stack(torch.meshgrid([coords_h, coords_w])) +coords = torch.stack(meshgrid([coords_h, coords_w], indexing=\"ij\"))",
        "core_API": "arange"
    },
    {
        "commit_hash": "689393ea00d50b81ed9f5f3af56942d17a6384ae",
        "index": "e0e6c6fb8..f3bce827a 100644",
        "commit_message": "fix with text attributes\n\n",
        "file": "mindsdb.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FullyConnectedNet(BaseModel):",
            "nn.Linear(input_size, int(math.ceil(input_size/2))),",
            "torch.nn.LeakyReLU(),",
            "nn.Dropout(0.2),",
            "-            nn.Linear(int(math.ceil(input_size/2)), output_size)",
            "+            nn.Linear(int(math.ceil(input_size/2)), output_size),",
            "+            torch.nn.LeakyReLU()",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=5, insert_id=612411)",
            "Insert(target_node=ASTNode(type=expression_statement), node=(',', ','), position=1, insert_id=612412)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=612413)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=612414)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=612415)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=612416)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=612417)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'LeakyReLU'), position=2, insert_id=612418)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=612419)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=612420)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=612421)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=612422)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=2, insert_id=612423)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 1899,
        "neg_line": [
            "-nn.Linear(int(math.ceil(input_size/2)), output_size)"
        ],
        "pos_line": [
            "+nn.Linear(int(math.ceil(input_size/2)), output_size),",
            "+torch.nn.LeakyReLU()"
        ],
        "core_change": "-nn.Linear(int(math.ceil(input_size/2)), output_size) +nn.Linear(int(math.ceil(input_size/2)), output_size), +torch.nn.LeakyReLU()",
        "core_API": "Linear"
    },
    {
        "commit_hash": "709cc2e206f384bfacc6f2732203c3d37f09b228",
        "index": "e619f97b..39af9da5 100644",
        "commit_message": "[Fix] Missing device initializations (#672)\n\n* Fix device initialization in several variables\n\n* Fix device initialization in several variables\n\n* Skip CUDA tests for #666\n\n* Lint\n\n* Fix device initialization in several variables\n\n* Skip CUDA tests for #666\n\n* Lint\n\n* Remove half-precision tests (#649) and add a jit test\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _compute_translation_matrix(translation: torch.Tensor) -> torch.Tensor:",
            "def _compute_scaling_matrix(scale: torch.Tensor,",
            "center: torch.Tensor) -> torch.Tensor:",
            "\"\"\"Computes affine matrix for scaling.\"\"\"",
            "-    angle: torch.Tensor = torch.zeros(scale.shape[:1])",
            "+    angle: torch.Tensor = torch.zeros(scale.shape[:1], device=scale.device, dtype=scale.dtype)",
            "matrix: torch.Tensor = get_rotation_matrix2d(center, angle, scale)",
            "return matrix"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=436837)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=436838)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=436839)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=436840)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=436841)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=436842)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=436843)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=436844)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=436845)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=436846)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'scale'), position=0, insert_id=436847)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=436848)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=436849)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'scale'), position=0, insert_id=436850)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=436851)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=436852)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 1904,
        "neg_line": [
            "-angle: torch.Tensor = torch.zeros(scale.shape[:1])"
        ],
        "pos_line": [
            "+angle: torch.Tensor = torch.zeros(scale.shape[:1], device=scale.device, dtype=scale.dtype)"
        ],
        "core_change": "-angle: torch.Tensor = torch.zeros(scale.shape[:1]) +angle: torch.Tensor = torch.zeros(scale.shape[:1], device=scale.device, dtype=scale.dtype)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "bafbc65ee352cd87c3f5f191995190a609a2a690",
        "index": "7069181..dd12201 100755",
        "commit_message": "AutoAnchor bug fix\n\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def kmean_anchors(path='./data/coco128.yaml', n=9, img_size=640, thr=4.0, gen=10",
            "# Evolve",
            "npr = np.random",
            "f, sh, mp, s = fitness(k), k.shape, 0.9, 0.1  # fitness, generations, mutation prob, sigma",
            "-    for _ in tqdm(range(gen), desc='Evolving anchors with Genetic Algorithm:'):",
            "+    for _ in tqdm(range(gen), desc='Evolving anchors with Genetic Algorithm'):",
            "v = np.ones(sh)",
            "while (v == 1).all():  # mutate until a change occurs (prevent duplicates)",
            "v = ((npr.random(sh) < mp) * npr.random() * npr.randn(*sh) * s + 1).clip(0.3, 3.0)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='Evolving anchors with Genetic Algorithm:'), value=\"'Evolving anchors with Genetic Algorithm'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1905,
        "neg_line": [
            "-for _ in tqdm(range(gen), desc='Evolving anchors with Genetic Algorithm:'):"
        ],
        "pos_line": [
            "+for _ in tqdm(range(gen), desc='Evolving anchors with Genetic Algorithm'):"
        ],
        "core_change": "-for _ in tqdm(range(gen), desc='Evolving anchors with Genetic Algorithm:'): +for _ in tqdm(range(gen), desc='Evolving anchors with Genetic Algorithm'):",
        "core_API": "ones"
    },
    {
        "commit_hash": "47d663f0fa97919cce60a64ffa977d9aed6cff45",
        "index": "2ffeefb9..4f5121c7 100644",
        "commit_message": "Add docstrings of core modules and methods (#3120)\n\n* Add docstrings of core modules and methods\n\n* Update docs and fix comments\n\n* Complete docstrings\n\n* Resolve comments\n\n* reformat docstrings\n\n* resolve comments\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SamplingResult(util_mixins.NiceRepr):",
            "",
            "@property",
            "def bboxes(self):",
            "+        \"\"\"torch.Tensor: concatenated positive and negative boxes\"\"\"",
            "return torch.cat([self.pos_bboxes, self.neg_bboxes])",
            "",
            "def to(self, device):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=636687)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=636688)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"torch.Tensor: concatenated positive and negative boxes\"\"\"'), position=0, insert_id=636689)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 4,
        "number": 1906,
        "neg_line": [],
        "pos_line": [
            "+\"\"\"torch.Tensor: concatenated positive and negative boxes\"\"\""
        ],
        "core_change": "+\"\"\"torch.Tensor: concatenated positive and negative boxes\"\"\"",
        "core_API": "cat"
    },
    {
        "commit_hash": "199c9c787427ece5723d5309e1c7c524a99bc59d",
        "index": "6c3eea6..7846969 100644",
        "commit_message": "Detection() device bug fix (#1455)\n\n\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Detections:",
            "self.names = names  # class names",
            "self.xyxy = pred  # xyxy pixels",
            "self.xywh = [xyxy2xywh(x) for x in pred]  # xywh pixels",
            "-        gn = [torch.Tensor([*[im.shape[i] for i in [1, 0, 1, 0]], 1., 1.]) for im in imgs]  # normalization gains",
            "+        d = pred[0].device  # device",
            "+        gn = [torch.tensor([*[im.shape[i] for i in [1, 0, 1, 0]], 1., 1.], device=d) for im in imgs]  # normalizations",
            "self.xyxyn = [x / g for x, g in zip(self.xyxy, gn)]  # xyxy normalized",
            "self.xywhn = [x / g for x, g in zip(self.xywh, gn)]  # xywh normalized",
            "self.n = len(self.pred)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1301155)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1301156)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'd'), position=0, insert_id=1301157)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1301158)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=2, insert_id=1301159)",
            "Insert(target_node=IN(type=attribute), node=('subscript', None), position=0, insert_id=1301160)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1301161)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1301162)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'pred'), position=0, insert_id=1301163)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1301164)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=1301165)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1301166)",
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='tensor')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1301167)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1301168)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1301169)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1301170)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'd'), position=2, insert_id=1301171)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 1911,
        "neg_line": [
            "-gn = [torch.Tensor([*[im.shape[i] for i in [1, 0, 1, 0]], 1., 1.]) for im in imgs]  # normalization gains"
        ],
        "pos_line": [
            "+d = pred[0].device  # device",
            "+gn = [torch.tensor([*[im.shape[i] for i in [1, 0, 1, 0]], 1., 1.], device=d) for im in imgs]  # normalizations"
        ],
        "core_change": "-gn = [torch.Tensor([*[im.shape[i] for i in [1, 0, 1, 0]], 1., 1.]) for im in imgs]  # normalization gains +d = pred[0].device  # device +gn = [torch.tensor([*[im.shape[i] for i in [1, 0, 1, 0]], 1., 1.], device=d) for im in imgs]  # normalizations",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "1142d3942999104399c839ea28e473ab587bbe7d",
        "index": "31972483..3422acee 100644",
        "commit_message": "add centripetal head (#3390)\n\n* add centripetal head\n\n* update var names in CentripetalHead\n\n* pre-commit\n\n* modify init_weight to align performance\n\n* add config and README\n\n* fix lint\n\n* update docstring\n\n* update README.md and model_zoo.md\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class HourglassNet(nn.Module):",
            "Detector's __init__() will call backbone's init_weights() with",
            "pretrained as input, so we keep this function.",
            "\"\"\"",
            "-        pass",
            "+        # Training Centripetal Model needs to reset parameters for Conv2d",
            "+        for m in self.modules():",
            "+            if isinstance(m, nn.Conv2d):",
            "+                m.reset_parameters()",
            "",
            "def forward(self, x):",
            "\"\"\"Forward function.\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n        pass\n\ndef forward(self, x):\n\"\"\"), value='\"\"\"\\n        # Training Centripetal Model needs to reset parameters for Conv2d\\n        for m in self.modules():\\n            if isinstance(m, nn.Conv2d):\\n                m.reset_parameters()\\n\\ndef forward(self, x):\\n\"\"\"')"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1912,
        "neg_line": [
            "-pass"
        ],
        "pos_line": [
            "+# Training Centripetal Model needs to reset parameters for Conv2d",
            "+for m in self.modules():",
            "+if isinstance(m, nn.Conv2d):",
            "+m.reset_parameters()"
        ],
        "core_change": "-pass +# Training Centripetal Model needs to reset parameters for Conv2d +for m in self.modules(): +if isinstance(m, nn.Conv2d): +m.reset_parameters()",
        "core_API": "modules"
    },
    {
        "commit_hash": "98cc35b6a8e53e15829ce64fd8da835db0c61da9",
        "index": "75cde439..b9cd88d9 100644",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class PipelineModule(nn.Module):",
            "self.tied_weight_attrs = {}",
            "",
            "# Offset the random seed by the stage ID.",
            "-        #newseed = torch.cuda.initial_seed() + self._grid.get_stage_id()",
            "+        #newseed = get_accelerator().initial_seed() + self._grid.get_stage_id()",
            "#ds_utils.set_random_seed(newseed)",
            "",
            "-        #with torch.random.fork_rng(devices=[torch.cuda.current_device()]):",
            "+        #with torch.random.fork_rng(devices=[get_accelerator().current_device_name()]):",
            "self._build()",
            "-        self.to(f'cuda:{self.local_rank}')",
            "+        self.to(get_accelerator().device_name(self.local_rank))",
            "",
            "self.tied_comms = self._index_tied_modules()",
            "self._synchronize_tied_weights()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=1816457)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=1816458)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1816459)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1816460)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=1816461)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1816462)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device_name'), position=2, insert_id=1816463)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1816464)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1816465)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1816466)",
            "Insert(target_node=IN(type=call), node=('identifier', 'get_accelerator'), position=0, insert_id=1816467)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1816468)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1816469)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1816470)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'local_rank'), position=2, insert_id=1816471)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1816472)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=1)",
            "Delete(target_node=ASTNode(type=string, text=f'cuda:{self.local_rank}'))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 1915,
        "neg_line": [
            "-#newseed = torch.cuda.initial_seed() + self._grid.get_stage_id()",
            "-#with torch.random.fork_rng(devices=[torch.cuda.current_device()]):",
            "-self.to(f'cuda:{self.local_rank}')"
        ],
        "pos_line": [
            "+#newseed = get_accelerator().initial_seed() + self._grid.get_stage_id()",
            "+#with torch.random.fork_rng(devices=[get_accelerator().current_device_name()]):",
            "+self.to(get_accelerator().device_name(self.local_rank))"
        ],
        "core_change": "-#newseed = torch.cuda.initial_seed() + self._grid.get_stage_id() +#newseed = get_accelerator().initial_seed() + self._grid.get_stage_id() -#with torch.random.fork_rng(devices=[torch.cuda.current_device()]): +#with torch.random.fork_rng(devices=[get_accelerator().current_device_name()]): -self.to(f'cuda:{self.local_rank}') +self.to(get_accelerator().device_name(self.local_rank))",
        "core_API": "initial_seed"
    },
    {
        "commit_hash": "0234315776751221ef9da5d2073e32f9f54c01bb",
        "index": "a5ec0b84..d9d4bb7d 100644",
        "commit_message": "[Feat] refactor tests for kornia.color (#759)\n\n* refactor test_gray test; add jit, grad and nn pytest mark\n\n* refactor RGB conversions code and test\n\n* refactor rgb/hsv tests\n\n* refactor rgb/hls tests\n\n* refactor code and tests for rgb to luv\n\n* refactor tests for rgb to xyz\n\n* refactor test code for rgb to ycbcr\n\n* refact test code for rgb to yuv, small docs improvements\n\n* fix random initialisation in smoke tests\n\n* support doctest for color module\n\n* few ci fixes\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "texinfo_documents = [",
            "# Example configuration for intersphinx: refer to the Python standard library.",
            "intersphinx_mapping = {",
            "'python': ('https://docs.python.org/3/', None),",
            "-    'numpy': ('http://docs.scipy.org/doc/numpy/', None),",
            "-    'torch': ('http://pytorch.org/docs/master/', None),",
            "+    'numpy': ('http://numpy.org/doc/stable/', None),",
            "+    'torch': ('http://pytorch.org/docs/stable/', None),",
            "}",
            "",
            "examples_dir = os.path.join(current_path, \"tutorials\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='http://docs.scipy.org/doc/numpy/'), value=\"'http://numpy.org/doc/stable/'\")",
            "Update(target_node=ASTNode(type=string, text='http://pytorch.org/docs/master/'), value=\"'http://pytorch.org/docs/stable/'\")"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 1916,
        "neg_line": [
            "-'numpy': ('http://docs.scipy.org/doc/numpy/', None),",
            "-'torch': ('http://pytorch.org/docs/master/', None),"
        ],
        "pos_line": [
            "+'numpy': ('http://numpy.org/doc/stable/', None),",
            "+'torch': ('http://pytorch.org/docs/stable/', None),"
        ],
        "core_change": "-'numpy': ('http://docs.scipy.org/doc/numpy/', None), -'torch': ('http://pytorch.org/docs/master/', None), +'numpy': ('http://numpy.org/doc/stable/', None), +'torch': ('http://pytorch.org/docs/stable/', None),",
        "core_API": "join"
    },
    {
        "commit_hash": "955b353cd0140eb276b07f70ad2a13235286775b",
        "index": "9b037f88..07bfab93 100644",
        "commit_message": "num features to dataset, num_classes bugfix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "for epoch in range(1, 301):",
            "class Classifier(nn.Module):",
            "def __init__(self, hidden_dim):",
            "super(Classifier, self).__init__()",
            "-        self.lin = nn.Linear(hidden_dim, data.num_classes)",
            "+        self.lin = nn.Linear(hidden_dim, dataset.num_classes)",
            "",
            "def reset_parameters(self):",
            "self.lin.reset_parameters()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=data), value='dataset')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1917,
        "neg_line": [
            "-self.lin = nn.Linear(hidden_dim, data.num_classes)"
        ],
        "pos_line": [
            "+self.lin = nn.Linear(hidden_dim, dataset.num_classes)"
        ],
        "core_change": "-self.lin = nn.Linear(hidden_dim, data.num_classes) +self.lin = nn.Linear(hidden_dim, dataset.num_classes)",
        "core_API": "Linear"
    },
    {
        "commit_hash": "809bcffd5071b5865201a57b5d882f718a2e51be",
        "index": "60671ecb97..b35f8680ef 100644",
        "commit_message": "Tensorflow frontend array decorators (#5824)\n\n* inital changes for tensorflow decorators frontend\n\n* lint\n\n* changes\n\n* lint\n\n* fixed issue with .data in testing\n\n* changes\n\n* lint\n\n* changes\n\n* changes\n\n* changes\n\n* changes\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "class Tensor:",
            "",
            "def __repr__(self):",
            "return (",
            "-            \"ivy.functional.frontends.torch.Tensor(\"",
            "-            + str(ivy.to_list(self.data))",
            "-            + \")\"",
            "+            \"ivy.functional.frontends.torch.Tensor(\" + str(ivy.to_list(self.data)) + \")\"",
            ")",
            "",
            "# Instance Methoods #"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 1921,
        "neg_line": [
            "-\"ivy.functional.frontends.torch.Tensor(\"",
            "-+ str(ivy.to_list(self.data))",
            "-+ \")\""
        ],
        "pos_line": [
            "+\"ivy.functional.frontends.torch.Tensor(\" + str(ivy.to_list(self.data)) + \")\""
        ],
        "core_change": "-\"ivy.functional.frontends.torch.Tensor(\" -+ str(ivy.to_list(self.data)) -+ \")\" +\"ivy.functional.frontends.torch.Tensor(\" + str(ivy.to_list(self.data)) + \")\"",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "b2394d3dcc38d777f21de8b2f02a645839dbaa05",
        "index": "4022288..4fb2bcd 100644",
        "commit_message": "fix flake8\n\n",
        "file": "TensorFlowTTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class TFMelGANDiscriminator(tf.keras.layers.Layer):",
            "return outs",
            "",
            "def _apply_weightnorm(self, list_layers):",
            "-        \"\"\"Try apply weightnorm for all layer in list_layers\"\"\"",
            "+        \"\"\"Try apply weightnorm for all layer in list_layers.\"\"\"",
            "for i in range(len(list_layers)):",
            "try:",
            "layer_name = list_layers[i].name.lower()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Try apply weightnorm for all layer in list_layers\"\"\"), value='\"\"\"Try apply weightnorm for all layer in list_layers.\"\"\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1923,
        "neg_line": [
            "-\"\"\"Try apply weightnorm for all layer in list_layers\"\"\""
        ],
        "pos_line": [
            "+\"\"\"Try apply weightnorm for all layer in list_layers.\"\"\""
        ],
        "core_change": "-\"\"\"Try apply weightnorm for all layer in list_layers\"\"\" +\"\"\"Try apply weightnorm for all layer in list_layers.\"\"\"",
        "core_API": "lower"
    },
    {
        "commit_hash": "29f45a45765e78d68e15113a4203de0548040314",
        "index": "d3df3dc63..1a56930dc 100644",
        "commit_message": "pip install espnet[train] (#3755)\n\n* Add pytorch=1.10.0 to CI configuration\n\n* fix:   test/espnet2/bin/test_k2_asr_inference.py\n\n* fix:   espnet2/main_funcs/pack_funcs.py\n\n* pip install espnet[train]\n\n* modified:   ci/install.sh\n\n* fix\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train(args):",
            "set_early_stop(trainer, args)",
            "",
            "if args.tensorboard_dir is not None and args.tensorboard_dir != \"\":",
            "+        from torch.utils.tensorboard import SummaryWriter",
            "+",
            "trainer.extend(",
            "TensorboardLogger(",
            "SummaryWriter(args.tensorboard_dir),"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=131820)",
            "Insert(target_node=IN(type=block), node=('import_from_statement', None), position=0, insert_id=131821)",
            "Insert(target_node=IN(type=import_from_statement), node=('from', 'from'), position=0, insert_id=131822)",
            "Insert(target_node=IN(type=import_from_statement), node=('dotted_name', None), position=1, insert_id=131823)",
            "Insert(target_node=IN(type=import_from_statement), node=('import', 'import'), position=2, insert_id=131824)",
            "Insert(target_node=IN(type=import_from_statement), node=('dotted_name', None), position=3, insert_id=131825)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'torch'), position=0, insert_id=131826)",
            "Insert(target_node=IN(type=dotted_name), node=('.', '.'), position=1, insert_id=131827)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'utils'), position=2, insert_id=131828)",
            "Insert(target_node=IN(type=dotted_name), node=('.', '.'), position=3, insert_id=131829)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'tensorboard'), position=4, insert_id=131830)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'SummaryWriter'), position=0, insert_id=131831)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 1925,
        "neg_line": [],
        "pos_line": [
            "+from torch.utils.tensorboard import SummaryWriter",
            "+"
        ],
        "core_change": "+from torch.utils.tensorboard import SummaryWriter +",
        "core_API": "extend"
    },
    {
        "commit_hash": "5c18c0103ed979c3d02c55b4dacfcad02daf811a",
        "index": "5c509f4c..00000000",
        "commit_message": "fix failing tests related to `solve_cast` on torch 1.9 (#2066)\n\n* update torch seed\n\n* rerun CI\n\n* update to use rescale\n\n* rerun ci\n\n* update tests cases to have hardcoded input\n\ndeleted:    test/utilities.py - not used anywhere\n\n* remove fail fast at ci\n\n* manual seed 0 TestImageStitcher::test_smoke\n\n* manual seed 0 TestHomographyTracker::test_real\n\n* manual seed 6 TestHomographyTracker::test_real\n\n* manual seed 245 TestImageStitcher::test_smoke and hardcoded case\n\n* manual seed 1 TestImageStitcher::test_smoke\n\n* manual seed 8 TestHomographyTracker::test_real\n\n- Comment the second frame test\n\n* rerun CI\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "-import pytest",
            "-import torch",
            "-",
            "-",
            "-@pytest.fixture",
            "-def data_loftr():",
            "-    url = 'https://github.com/kornia/data_test/blob/main/loftr_outdoor_and_homography_data.pt?raw=true'",
            "-    return torch.hub.load_state_dict_from_url(url)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 1926,
        "neg_line": [
            "-import pytest",
            "-import torch",
            "-",
            "-",
            "-@pytest.fixture",
            "-def data_loftr():",
            "-url = 'https://github.com/kornia/data_test/blob/main/loftr_outdoor_and_homography_data.pt?raw=true'",
            "-return torch.hub.load_state_dict_from_url(url)"
        ],
        "pos_line": [],
        "core_change": "-import pytest -import torch - - -@pytest.fixture -def data_loftr(): -url = 'https://github.com/kornia/data_test/blob/main/loftr_outdoor_and_homography_data.pt?raw=true' -return torch.hub.load_state_dict_from_url(url)",
        "core_API": "load_state_dict_from_url"
    },
    {
        "commit_hash": "92aa1d19c2412caf25dd84056e5387fad43b700d",
        "index": "3613311..a30c66d 100644",
        "commit_message": "change testing framework from nosetests to pytest.\n\nfix #210\n\n",
        "file": "einops.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_torch_layer():",
            "",
            "# tracing (freezing)",
            "model3 = torch.jit.trace(model2, example_inputs=input)",
            "-        torch.testing.assert_allclose(model1(input), model3(input), atol=1e-3, rtol=1e-3)",
            "-        torch.testing.assert_allclose(model1(input + 1), model3(input + 1), atol=1e-3, rtol=1e-3)",
            "+        torch.testing.assert_close(model1(input), model3(input), atol=1e-3, rtol=1e-3)",
            "+        torch.testing.assert_close(model1(input + 1), model3(input + 1), atol=1e-3, rtol=1e-3)",
            "",
            "model4 = torch.jit.trace(model2, example_inputs=input)",
            "-        torch.testing.assert_allclose(model1(input), model4(input), atol=1e-3, rtol=1e-3)",
            "-        torch.testing.assert_allclose(model1(input + 1), model4(input + 1), atol=1e-3, rtol=1e-3)",
            "+        torch.testing.assert_close(model1(input), model4(input), atol=1e-3, rtol=1e-3)",
            "+        torch.testing.assert_close(model1(input + 1), model4(input + 1), atol=1e-3, rtol=1e-3)",
            "",
            "",
            "def test_torch_layers_scripting():"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=116892)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=116893)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=116894)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'assert_close'), position=2, insert_id=116895)",
            "Update(target_node=ASTNode(type=identifier, text=assert_allclose), value='assert_close')",
            "Update(target_node=ASTNode(type=identifier, text=assert_allclose), value='assert_close')",
            "Update(target_node=ASTNode(type=identifier, text=assert_allclose), value='assert_close')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=116896)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=116897)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'testing'), position=2, insert_id=116898)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=testing))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=assert_allclose))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 18,
        "number": 1927,
        "neg_line": [
            "-torch.testing.assert_allclose(model1(input), model3(input), atol=1e-3, rtol=1e-3)",
            "-torch.testing.assert_allclose(model1(input + 1), model3(input + 1), atol=1e-3, rtol=1e-3)",
            "-torch.testing.assert_allclose(model1(input), model4(input), atol=1e-3, rtol=1e-3)",
            "-torch.testing.assert_allclose(model1(input + 1), model4(input + 1), atol=1e-3, rtol=1e-3)"
        ],
        "pos_line": [
            "+torch.testing.assert_close(model1(input), model3(input), atol=1e-3, rtol=1e-3)",
            "+torch.testing.assert_close(model1(input + 1), model3(input + 1), atol=1e-3, rtol=1e-3)",
            "+torch.testing.assert_close(model1(input), model4(input), atol=1e-3, rtol=1e-3)",
            "+torch.testing.assert_close(model1(input + 1), model4(input + 1), atol=1e-3, rtol=1e-3)"
        ],
        "core_change": "-torch.testing.assert_allclose(model1(input), model3(input), atol=1e-3, rtol=1e-3) -torch.testing.assert_allclose(model1(input + 1), model3(input + 1), atol=1e-3, rtol=1e-3) +torch.testing.assert_close(model1(input), model3(input), atol=1e-3, rtol=1e-3) +torch.testing.assert_close(model1(input + 1), model3(input + 1), atol=1e-3, rtol=1e-3) -torch.testing.assert_allclose(model1(input), model4(input), atol=1e-3, rtol=1e-3) -torch.testing.assert_allclose(model1(input + 1), model4(input + 1), atol=1e-3, rtol=1e-3) +torch.testing.assert_close(model1(input), model4(input), atol=1e-3, rtol=1e-3) +torch.testing.assert_close(model1(input + 1), model4(input + 1), atol=1e-3, rtol=1e-3)",
        "core_API": "trace"
    },
    {
        "commit_hash": "ddebbdc53544c64a1957cf0ec04a34578e9b57ba",
        "index": "f36874fc..5e3777fe 100644",
        "commit_message": "Make most metrics work on GPU (#3851)\n\n* Make most metrics work on GPU\n\n* Make metric tests work on both GPU and CPU\n\n* Add a test for the test utility\n\n* mypy\n\n* Update allennlp/common/testing/test_case.py\n\nCo-Authored-By: Mark Neumann <markn@allenai.org>\n\n* Fix a PR comment\n\n* flake8\n\nCo-authored-by: Mark Neumann <markn@allenai.org>\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BooleanAccuracy(Metric):",
            "# so we'll keep predictions that aren't.",
            "keep = mask.view(batch_size, -1).max(dim=1)[0].float()",
            "else:",
            "-            keep = torch.ones(batch_size).float()",
            "+            keep = torch.ones(batch_size, device=predictions.device).float()",
            "",
            "predictions = predictions.view(batch_size, -1)",
            "gold_labels = gold_labels.view(batch_size, -1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=20613)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=20614)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=20615)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=20616)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=20617)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'predictions'), position=0, insert_id=20618)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=20619)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=20620)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1928,
        "neg_line": [
            "-keep = torch.ones(batch_size).float()"
        ],
        "pos_line": [
            "+keep = torch.ones(batch_size, device=predictions.device).float()"
        ],
        "core_change": "-keep = torch.ones(batch_size).float() +keep = torch.ones(batch_size, device=predictions.device).float()",
        "core_API": "view"
    },
    {
        "commit_hash": "b43abdb296d0bc937d02cf37060089d0a1f94c06",
        "index": "76f2ec9..190cf43 100644",
        "commit_message": "Fixed GPU check to run lazily to prevent recursive subprocess creation (#1570)\n\nSigned-off-by: Travis Addair <taddair@uber.com>\n\n",
        "file": "horovod.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class MPITests(tf.test.TestCase):",
            "\"\"\"Test on GPU using NCCL that the Adasum correctly computes 2D tensors.\"\"\"",
            "hvd.init()",
            "# TODO support non-MPI Adasum operation",
            "-        if not hvd.mpi_enabled() or not tf.test.is_gpu_available() or not hvd.nccl_built():",
            "+        if not hvd.mpi_enabled() or not hvd.gpu_available('tensorflow') or not hvd.nccl_built():",
            "return",
            "rank = hvd.rank()",
            "rank_tensors = []"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='hvd')",
            "Update(target_node=ASTNode(type=identifier, text=test), value='gpu_available')",
            "Insert(target_node=ASTNode(type=argument_list), node=('string', \"'tensorflow'\"), position=1, insert_id=1946231)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=is_gpu_available))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 1930,
        "neg_line": [
            "-if not hvd.mpi_enabled() or not tf.test.is_gpu_available() or not hvd.nccl_built():"
        ],
        "pos_line": [
            "+if not hvd.mpi_enabled() or not hvd.gpu_available('tensorflow') or not hvd.nccl_built():"
        ],
        "core_change": "-if not hvd.mpi_enabled() or not tf.test.is_gpu_available() or not hvd.nccl_built(): +if not hvd.mpi_enabled() or not hvd.gpu_available('tensorflow') or not hvd.nccl_built():",
        "core_API": "init"
    },
    {
        "commit_hash": "9eb5a232d0f90f1fdb69f68e7d58fa2cfae314cc",
        "index": "cbc0d5d423..3947c0142c 100644",
        "commit_message": "made indexing method more explicit in torch shuffle method as a temporary fix to issues with torch.Tensor.__getitem__ being incorrectly reset after wrapping the torch module for compiling. This issue needs further attention.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def seed(seed_value: int = 0) -> None:",
            "",
            "def shuffle(x):",
            "batch_size = x.shape[0]",
            "-    return x[torch.randperm(batch_size)]",
            "+    return torch.index_select(x, 0, torch.randperm(batch_size))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('call', None), position=1, insert_id=381083)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=381084)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=381085)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=381086)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=381087)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'index_select'), position=2, insert_id=381088)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=381089)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=x), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=381090)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '0'), position=3, insert_id=381091)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=381092)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=5)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=6, insert_id=381093)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 1931,
        "neg_line": [
            "-return x[torch.randperm(batch_size)]"
        ],
        "pos_line": [
            "+return torch.index_select(x, 0, torch.randperm(batch_size))"
        ],
        "core_change": "-return x[torch.randperm(batch_size)] +return torch.index_select(x, 0, torch.randperm(batch_size))",
        "core_API": "randperm"
    },
    {
        "commit_hash": "94de0229fb0d835ca9f3019ff5195edb3c354910",
        "index": "cdf4c38c..f56e76c7 100755",
        "commit_message": "Fix inference api & add more description on inference engine tutorial (#1711)\n\n\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class HFGPTJLayerPolicy(DSPolicy):",
            "kw = self.client_module.attn.k_proj.weight",
            "vw = self.client_module.attn.v_proj.weight",
            "",
            "-        qkvw = torch.cat((qw, kw, vw), dim=0)",
            "+        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0))",
            "",
            "return self.linear_layer, \\",
            "qkvw, \\"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('identifier', 'Parameter'), position=0, insert_id=80246)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=80247)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=80248)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=80249)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1932,
        "neg_line": [
            "-qkvw = torch.cat((qw, kw, vw), dim=0)"
        ],
        "pos_line": [
            "+qkvw = Parameter(torch.cat((qw, kw, vw), dim=0))"
        ],
        "core_change": "-qkvw = torch.cat((qw, kw, vw), dim=0) +qkvw = Parameter(torch.cat((qw, kw, vw), dim=0))",
        "core_API": "cat"
    },
    {
        "commit_hash": "d67e72139e45bccbc5d356a456f41cee8f72eb1c",
        "index": "e69c1e9e..83ae83a6 100644",
        "commit_message": "Fix heuristic in util.get_token_ids_from_text_field_tensors (#4184)\n\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ElmoTokenEmbedder(TokenEmbedder):",
            "The ELMo representations for the input sequence, shape",
            "`(batch_size, timesteps, embedding_dim)`",
            "\"\"\"",
            "-        elmo_output = self._elmo(tokens, word_inputs)",
            "+        elmo_output = self._elmo(elmo_tokens, word_inputs)",
            "elmo_representations = elmo_output[\"elmo_representations\"][0]",
            "if self._projection:",
            "projection = self._projection"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1934,
        "neg_line": [
            "-elmo_output = self._elmo(tokens, word_inputs)"
        ],
        "pos_line": [
            "+elmo_output = self._elmo(elmo_tokens, word_inputs)"
        ],
        "core_change": "-elmo_output = self._elmo(tokens, word_inputs) +elmo_output = self._elmo(elmo_tokens, word_inputs)",
        "core_API": "_elmo"
    },
    {
        "commit_hash": "ee36a6f3e310ff905f6d74b8d9eac4c3a220b188",
        "index": "cb2df23e..663c90f8 100644",
        "commit_message": "Fixed Weight Decay Regularization in Adam\n\nSee https://arxiv.org/abs/1711.05101\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MultiprocessingTrainer(MultiprocessingEventLoop):",
            "'betas': eval(self.args.adam_betas),",
            "'weight_decay': self.args.weight_decay,",
            "}",
            "-            return torch.optim.Adam(self.model.parameters(), **self._override_optim_state)",
            "+            return Adam(self.model.parameters(), **self._override_optim_state)",
            "elif self.args.optimizer == 'nag':",
            "self._override_optim_state = {",
            "'lr': self.args.lr[0],"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=Adam), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=optim))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 1936,
        "neg_line": [
            "-return torch.optim.Adam(self.model.parameters(), **self._override_optim_state)"
        ],
        "pos_line": [
            "+return Adam(self.model.parameters(), **self._override_optim_state)"
        ],
        "core_change": "-return torch.optim.Adam(self.model.parameters(), **self._override_optim_state) +return Adam(self.model.parameters(), **self._override_optim_state)",
        "core_API": "Adam"
    },
    {
        "commit_hash": "ebd0bc17929b36b851fc825bbfd5fbd41f6d8111",
        "index": "b1cd36b3..589117f3 100644",
        "commit_message": "Update to PyTorch 1.11.0 (#3045)\n\n* Run black\n\n* Fix ProvenanceTensor\n\n* Replace torch.triangular_solve -> torch.linalg.solve_triangular\n\n* More fixes to torch.linalg.solve_triangular\n\n* Bump torch version\n\n* Bump Python version 3.6 -> 3.7\n\n* Fix some tests\n\n* Decrease tolerance on stable tests\n\n* Remove obsolete xfail\n\n* Resolve #3032\n\n* Fix solve_triangular in ops.gaussian, revert test weakening\n\n* Fix catching of singular matrices in hmc\n\n* xfail funsor tests\n\n* Work around pandas 1.3 bug\n\n* Allow mypy to install missing stubs\n\n* Clarify triangular_solve test\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "\"source\": [",
            "\"xs = torch.linspace(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5, steps=100)\\n\",",
            "\"ys = torch.linspace(X[:, 1].min() - 0.5, X[:, 1].max() + 0.5, steps=100)\\n\",",
            "-    \"try:\\n\",",
            "-    \"    # torch 1.10 or greater defaults to using indexing\\n\",",
            "-    \"    xx, yy = torch.meshgrid(xs, ys, indexing=\\\"xy\\\")\\n\",",
            "-    \"except:\\n\",",
            "-    \"    xx, yy = torch.meshgrid(xs, ys)\\n\",",
            "-    \"    xx = xx.t()\\n\",",
            "-    \"    yy = yy.t()\\n\",",
            "-    \"\\n\",",
            "+    \"xx, yy = torch.meshgrid(xs, ys, indexing=\\\"xy\\\")\\n\",",
            "\"\\n\",",
            "\"with torch.no_grad():\\n\",",
            "\"    mean, var = model(torch.vstack((xx.ravel(), yy.ravel())).t())\\n\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"try:\\n\"), value='\"xx, yy = torch.meshgrid(xs, ys, indexing=\\\\\"xy\\\\\")\\\\n\"')",
            "Delete(target_node=ASTNode(type=string, text=\"    # torch 1.10 or greater defaults to using indexing\\n\"))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=string, text=\"    xx, yy = torch.meshgrid(xs, ys, indexing=\\\"xy\\\")\\n\"))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=string, text=\"except:\\n\"))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=string, text=\"    xx, yy = torch.meshgrid(xs, ys)\\n\"))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=string, text=\"    xx = xx.t()\\n\"))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=string, text=\"    yy = yy.t()\\n\"))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=string, text=\"\\n\"))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 8,
        "AST_diff_line": 15,
        "number": 1938,
        "neg_line": [
            "-\"try:\\n\",",
            "-\"    # torch 1.10 or greater defaults to using indexing\\n\",",
            "-\"    xx, yy = torch.meshgrid(xs, ys, indexing=\\\"xy\\\")\\n\",",
            "-\"except:\\n\",",
            "-\"    xx, yy = torch.meshgrid(xs, ys)\\n\",",
            "-\"    xx = xx.t()\\n\",",
            "-\"    yy = yy.t()\\n\",",
            "-\"\\n\","
        ],
        "pos_line": [
            "+\"xx, yy = torch.meshgrid(xs, ys, indexing=\\\"xy\\\")\\n\","
        ],
        "core_change": "-\"try:\\n\", -\"    # torch 1.10 or greater defaults to using indexing\\n\", -\"    xx, yy = torch.meshgrid(xs, ys, indexing=\\\"xy\\\")\\n\", -\"except:\\n\", -\"    xx, yy = torch.meshgrid(xs, ys)\\n\", -\"    xx = xx.t()\\n\", -\"    yy = yy.t()\\n\", -\"\\n\", +\"xx, yy = torch.meshgrid(xs, ys, indexing=\\\"xy\\\")\\n\",",
        "core_API": "linspace"
    },
    {
        "commit_hash": "109d9f4eb387c8e490a0ada82ec16b9bc1e9bf98",
        "index": "b08c67fc..cc003019 100644",
        "commit_message": "Minor fix of indentation in TensorFlow backend. (#5967)\n\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def rnn(step_function, inputs, initial_states,",
            "states = return_states",
            "successive_outputs.append(output)",
            "successive_states.append(states)",
            "-                last_output = successive_outputs[-1]",
            "-                new_states = successive_states[-1]",
            "-                outputs = tf.stack(successive_outputs)",
            "+            last_output = successive_outputs[-1]",
            "+            new_states = successive_states[-1]",
            "+            outputs = tf.stack(successive_outputs)",
            "else:",
            "for inp in input_list:",
            "output, states = step_function(inp, states + constants)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 1941,
        "neg_line": [
            "-last_output = successive_outputs[-1]",
            "-new_states = successive_states[-1]",
            "-outputs = tf.stack(successive_outputs)"
        ],
        "pos_line": [
            "+last_output = successive_outputs[-1]",
            "+new_states = successive_states[-1]",
            "+outputs = tf.stack(successive_outputs)"
        ],
        "core_change": "-last_output = successive_outputs[-1] -new_states = successive_states[-1] -outputs = tf.stack(successive_outputs) +last_output = successive_outputs[-1] +new_states = successive_states[-1] +outputs = tf.stack(successive_outputs)",
        "core_API": "append"
    },
    {
        "commit_hash": "7db18ce0c28c0b7c93e9a61af55433f2d88d4213",
        "index": "73a2b569..0f124fe2 100644",
        "commit_message": "fix mypy errors\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ExtractTensorPatches(nn.Module):",
            "kernel[i, i] += 1.0",
            "return kernel.view(window_range, 1, window_size[0], window_size[1])",
            "",
            "-    def forward(self, input: torch.Tensor) -> torch.Tensor:",
            "+    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore",
            "if not torch.is_tensor(input):",
            "raise TypeError(\"Input input type is not a torch.Tensor. Got {}\"",
            ".format(type(input)))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1943,
        "neg_line": [
            "-def forward(self, input: torch.Tensor) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore"
        ],
        "core_change": "-def forward(self, input: torch.Tensor) -> torch.Tensor: +def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore",
        "core_API": "view"
    },
    {
        "commit_hash": "e0288b141173da88ee87e85060750af241bdb170",
        "index": "30513e7d1..66915e7ff 100644",
        "commit_message": "fix length bonus in cuda\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LengthBonus(ScorerInterface):",
            "torch.float32 scores for y (B)",
            "and next state for ys",
            "\"\"\"",
            "-        return torch.tensor([1.0]).expand(self.n), None",
            "+        return torch.tensor([1.0], device=y.device).expand(self.n), None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=168825)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=168826)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=168827)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=168828)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=168829)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'y'), position=0, insert_id=168830)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=168831)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=168832)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1947,
        "neg_line": [
            "-return torch.tensor([1.0]).expand(self.n), None"
        ],
        "pos_line": [
            "+return torch.tensor([1.0], device=y.device).expand(self.n), None"
        ],
        "core_change": "-return torch.tensor([1.0]).expand(self.n), None +return torch.tensor([1.0], device=y.device).expand(self.n), None",
        "core_API": "tensor"
    },
    {
        "commit_hash": "b5e751b61f656671e0b8c3b37e9d4d495d250679",
        "index": "3e02c640..b47cf465 100644",
        "commit_message": "fix upsample\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def fpn_model(features):",
            "if idx == 0:",
            "lat_sum_5432.append(lat)",
            "else:",
            "-                lat = lat + upsample2x('upsample_c{}'.format(5 - idx), lat_sum_5432[-1])",
            "+                lat = lat + upsample2x('upsample_lat{}'.format(6 - idx), lat_sum_5432[-1])",
            "lat_sum_5432.append(lat)",
            "p2345 = [Conv2D('posthoc_3x3_p{}'.format(i + 2), c, num_channel, 3)",
            "for i, c in enumerate(lat_sum_5432[::-1])]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='upsample_c{}'), value=\"'upsample_lat{}'\")",
            "Update(target_node=ASTNode(type=integer, text=5), value='6')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 1949,
        "neg_line": [
            "-lat = lat + upsample2x('upsample_c{}'.format(5 - idx), lat_sum_5432[-1])"
        ],
        "pos_line": [
            "+lat = lat + upsample2x('upsample_lat{}'.format(6 - idx), lat_sum_5432[-1])"
        ],
        "core_change": "-lat = lat + upsample2x('upsample_c{}'.format(5 - idx), lat_sum_5432[-1]) +lat = lat + upsample2x('upsample_lat{}'.format(6 - idx), lat_sum_5432[-1])",
        "core_API": "append"
    },
    {
        "commit_hash": "6d4f83cae02129b7f49acf022561711cd937e8b8",
        "index": "b43e9e47..0b851483 100644",
        "commit_message": "Drop JIT support for `core.check`, `Boxes`, and others (#2219)\n\n* Drop JIT support for `core.check` API\n\n- Consequently for this, we drop support of JIT on the following items: (in of dynamo)\n  - enhance\n    - AdjustSigmoid\n    - AdjustLog\n    - AddWeighted\n  - geometry\n    - UndistortPoints\n    - bbox and Boxes - follow up on #2218\n    - EuclideanDistance\n    - TransformPoints\n    - HomographyWarper\n    - WarpPerspective\n    - UpscaleDouble\n  - losses\n\n* Update typing with pyupgrade\n* drop all jit related from bbox and boxes\n\nfrom #2218\n* fix/skip failing dynamo tests\n* fix loss hd\n* fix typing\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MS_SSIMLoss(nn.Module):",
            "return g.reshape(-1)",
            "",
            "def _fspecial_gauss_2d(",
            "-        self, size: int, sigma: float, device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None",
            "+        self, size: int, sigma: float, device: torch.device | None = None, dtype: torch.dtype | None = None",
            ") -> torch.Tensor:",
            "\"\"\"Create 2-D gauss kernel."
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=type), node=('binary_operator', None), position=0, insert_id=388480)",
            "Insert(target_node=ASTNode(type=type), node=('binary_operator', None), position=0, insert_id=388481)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('|', '|'), position=1, insert_id=388482)",
            "Insert(target_node=IN(type=binary_operator), node=('none', 'None'), position=2, insert_id=388483)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('|', '|'), position=1, insert_id=388484)",
            "Insert(target_node=IN(type=binary_operator), node=('none', 'None'), position=2, insert_id=388485)",
            "Delete(target_node=ASTNode(type=identifier, text=Optional))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=identifier, text=Optional))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 1951,
        "neg_line": [
            "-self, size: int, sigma: float, device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None"
        ],
        "pos_line": [
            "+self, size: int, sigma: float, device: torch.device | None = None, dtype: torch.dtype | None = None"
        ],
        "core_change": "-self, size: int, sigma: float, device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None +self, size: int, sigma: float, device: torch.device | None = None, dtype: torch.dtype | None = None",
        "core_API": "reshape"
    },
    {
        "commit_hash": "d14c819a206e895d049e067f6da320270f4215d7",
        "index": "7cb7ac3d..95d1b336 100644",
        "commit_message": "GraphGym: Update to latest PyTorch Lightning version (#5135)\n\n* refactor accelerator\n\n* update\n\n* Update torch_geometric/graphgym/train.py\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def create_model(to_device=True, dim_in=None, dim_out=None) -> GraphGymModule:",
            "",
            "model = GraphGymModule(dim_in, dim_out, cfg)",
            "if to_device:",
            "-        model.to(torch.device(cfg.device))",
            "+        model.to(torch.device(cfg.accelerator))",
            "return model"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=device), value='accelerator')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1956,
        "neg_line": [
            "-model.to(torch.device(cfg.device))"
        ],
        "pos_line": [
            "+model.to(torch.device(cfg.accelerator))"
        ],
        "core_change": "-model.to(torch.device(cfg.device)) +model.to(torch.device(cfg.accelerator))",
        "core_API": "to"
    },
    {
        "commit_hash": "ee57ff94dfa2c3ced30c1b103076b4ae18fa9199",
        "index": "ffd6e03c5..b2910a018 100755",
        "commit_message": "Update asr_inference_streaming.py\n\nFix dtype CI error\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Speech2TextStreaming:",
            "has_enough_samples = False if speech.size(0) <= self.win_length else True",
            "if not has_enough_samples:",
            "if is_final:",
            "-                pad = torch.zeros(self.win_length - speech.size(0))",
            "+                pad = torch.zeros(self.win_length - speech.size(0), dtype=getattr(torch, self.dtype))",
            "speech = torch.cat([speech, pad], dim=0)",
            "else:",
            "feats = None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=123450)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=123451)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=123452)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=123453)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=123454)",
            "Insert(target_node=IN(type=keyword_argument), node=('call', None), position=2, insert_id=123455)",
            "Insert(target_node=IN(type=call), node=('identifier', 'getattr'), position=0, insert_id=123456)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=123457)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=123458)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'torch'), position=1, insert_id=123459)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=123460)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=123461)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=123462)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=123463)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=123464)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 1958,
        "neg_line": [
            "-pad = torch.zeros(self.win_length - speech.size(0))"
        ],
        "pos_line": [
            "+pad = torch.zeros(self.win_length - speech.size(0), dtype=getattr(torch, self.dtype))"
        ],
        "core_change": "-pad = torch.zeros(self.win_length - speech.size(0)) +pad = torch.zeros(self.win_length - speech.size(0), dtype=getattr(torch, self.dtype))",
        "core_API": "size"
    },
    {
        "commit_hash": "25136c630d9022e428e7cfde796ee78cab2e216a",
        "index": "890bac02ae..b7aaef7591 100644",
        "commit_message": "small fix to `cholesky`\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def cholesky(",
            "else:",
            "ret = torch.transpose(",
            "torch.linalg.cholesky(",
            "-                torch.transpose(x, dim0=len(x.shape) - 1, dim1=len(x.shape) - 2),",
            "-                out=out,",
            "+                torch.transpose(x, dim0=len(x.shape) - 1, dim1=len(x.shape) - 2)",
            "),",
            "dim0=len(x.shape) - 1,",
            "dim1=len(x.shape) - 2,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 1960,
        "neg_line": [
            "-torch.transpose(x, dim0=len(x.shape) - 1, dim1=len(x.shape) - 2),",
            "-out=out,"
        ],
        "pos_line": [
            "+torch.transpose(x, dim0=len(x.shape) - 1, dim1=len(x.shape) - 2)"
        ],
        "core_change": "-torch.transpose(x, dim0=len(x.shape) - 1, dim1=len(x.shape) - 2), -out=out, +torch.transpose(x, dim0=len(x.shape) - 1, dim1=len(x.shape) - 2)",
        "core_API": "transpose"
    },
    {
        "commit_hash": "80c799a91c3b2c31847bf38c1f010987d0cacc05",
        "index": "2fea86e..7f3ddba 100644",
        "commit_message": "fix sampling once and for all\n\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def sample_autoregressive(partial_sequences,",
            "",
            "ids_this_step = mtf.sample_with_temperature(",
            "logits, other_features[\"vocab_dim\"], temperature)",
            "+        ids_this_step = mtf.shift(ids_this_step, offset=1, dim=length_dim, wrap=False)",
            "one_new_id = ids_this_step * mtf.one_hot(position, length_dim, dtype=tf.int32)",
            "-        one_new_id = mtf.shift(one_new_id, offset=1, dim=length_dim, wrap=False)",
            "new_ids = ids + one_new_id",
            "new_position = position + 1",
            "return [new_position, new_ids]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=0, insert_id=1938790)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('*', '*'), position=3, insert_id=1938791)",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=call), position=4)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1938792)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('ERROR', None), position=1, insert_id=1938793)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=2, insert_id=1938794)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shift'), position=3, insert_id=1938795)",
            "Update(target_node=ASTNode(type=identifier, text=one_new_id), value='ids_this_step')",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'ids_this_step'), position=0, insert_id=1938796)",
            "Insert(target_node=IN(type=ERROR), node=('=', '='), position=1, insert_id=1938797)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'mtf'), position=2, insert_id=1938798)",
            "Delete(target_node=ASTNode(type=*, text=*))",
            "Delete(target_node=ASTNode(type=identifier, text=one_new_id))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=mtf))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=shift))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 1961,
        "neg_line": [
            "-one_new_id = mtf.shift(one_new_id, offset=1, dim=length_dim, wrap=False)"
        ],
        "pos_line": [
            "+ids_this_step = mtf.shift(ids_this_step, offset=1, dim=length_dim, wrap=False)"
        ],
        "core_change": "+ids_this_step = mtf.shift(ids_this_step, offset=1, dim=length_dim, wrap=False) -one_new_id = mtf.shift(one_new_id, offset=1, dim=length_dim, wrap=False)",
        "core_API": "sample_with_temperature"
    },
    {
        "commit_hash": "8fbf6b73e9c42511aad03aa8211c3337a4a8740a",
        "index": "1490f5e..196a13e 100644",
        "commit_message": "Sharded data sources (#456)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/456\n\nMoves sharding from Data to DataSouce, so that we can have DataSources that can shard in different ways.\n\n- Move shard() utility to data/utils\n- Introduce ShardedDataSource (base class), RowShardedDataSource, and BlockShardedTSVDataSource\n\nDefault behavior is to shard the configured DataSource in a RowShardedDataSource, which results in previous behavior.  RowShardedDataSource is a light wrapper which returns shard(data_source.train)\n\nIf the configured DataSource is an instance of ShardedDataSource, then it has sharding capability and we don't wrap it.\n\nBlockShardedTSVDataSource is a TSV data source, which divides the file into blocks and returns csv readers over each block for each shard. This is useful for when row order is important in the file, e.g. when rows are sentences of a long document.  Used for MaskedLM training. Blocks are the same byte size, but not guaranteed to have the same number of rows.  Therefore the Data object needs to set epoch_size for distributed training to work correctly.\n\nIntroduce epoch_size for base Data object as an option.  If set, epochs are set to be a fixed number of batches.  Underlying dataset is cycled over infinitely.  If not set, behavior is as before.\n\nRemove epoch size from batch samplers, since it's no longer needed.\n\nConsolidate DisjointMultitaskData and CrossLingualLMData to share more code.\n\nReviewed By: kartikayk\n\nDifferential Revision: D14840136\n\nfbshipit-source-id: 1eaba18c730a39a069ee701827195954e77b22fc\n\n",
        "file": "pytext.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def create_module(",
            "name = type(module).__name__",
            "if getattr(module_config, \"load_path\", None):",
            "print(f\"Loading state of module {name} from {module_config.load_path} ...\")",
            "-        module.load_state_dict(torch.load(module_config.load_path))",
            "+        module.load_state_dict(torch.load(module_config.load_path, map_location=\"cpu\"))",
            "if getattr(module_config, \"freeze\", False):",
            "print(f\"Freezing the parameters of module {name} ...\")",
            "module.freeze()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=885823)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=885824)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'map_location'), position=0, insert_id=885825)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=885826)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"cpu\"'), position=2, insert_id=885827)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1965,
        "neg_line": [
            "-module.load_state_dict(torch.load(module_config.load_path))"
        ],
        "pos_line": [
            "+module.load_state_dict(torch.load(module_config.load_path, map_location=\"cpu\"))"
        ],
        "core_change": "-module.load_state_dict(torch.load(module_config.load_path)) +module.load_state_dict(torch.load(module_config.load_path, map_location=\"cpu\"))",
        "core_API": "load_state_dict"
    },
    {
        "commit_hash": "5545206401e9bcd94652e4ef5c801be07bea34bc",
        "index": "8c76a6a..132a783 100644",
        "commit_message": "Fixed issues raised in the codacy report (#502)\n\n* Fixed issues raised in the codacy report\n\n* Update graph.py\n\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class Graph:",
            "if pre_node[temp_id] == temp_id:",
            "break",
            "temp_id = pre_node[temp_id]",
            "-        assert temp_id == pre_node[temp_id]",
            "+        if temp_id != pre_node[temp_id]:",
            "+            raise AssertionError(\"Error: main chain end condition not met.\")",
            "ret.reverse()",
            "return ret"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=2561750)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2561751)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=comparison_operator), position=1)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2561752)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=2561753)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('!=', '!='), position=1, insert_id=2561754)",
            "Insert(target_node=IN(type=block), node=('raise_statement', None), position=0, insert_id=2561755)",
            "Insert(target_node=IN(type=raise_statement), node=('raise', 'raise'), position=0, insert_id=2561756)",
            "Insert(target_node=IN(type=raise_statement), node=('call', None), position=1, insert_id=2561757)",
            "Insert(target_node=IN(type=call), node=('identifier', 'AssertionError'), position=0, insert_id=2561758)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2561759)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2561760)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"Error: main chain end condition not met.\"'), position=1, insert_id=2561761)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2561762)",
            "Delete(target_node=ASTNode(type=assert, text=assert))",
            "Delete(target_node=ASTNode(type===, text===))",
            "Delete(target_node=ASTNode(type=assert_statement))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1967,
        "neg_line": [
            "-assert temp_id == pre_node[temp_id]"
        ],
        "pos_line": [
            "+if temp_id != pre_node[temp_id]:",
            "+raise AssertionError(\"Error: main chain end condition not met.\")"
        ],
        "core_change": "-assert temp_id == pre_node[temp_id] +if temp_id != pre_node[temp_id]: +raise AssertionError(\"Error: main chain end condition not met.\")",
        "core_API": "reverse"
    },
    {
        "commit_hash": "6ba2231d7227825dfb76e9df161824d04234e69f",
        "index": "75481ecb..c0181ef0 100644",
        "commit_message": "Reproducibility 3/3 (#1924)\n\n* make tests deterministic\n\n* run slow tests\n\n* prepare for testing\n\n* finish\n\n* refactor\n\n* add print statements\n\n* finish more\n\n* correct some test failures\n\n* more fixes\n\n* set up to correct tests\n\n* more corrections\n\n* up\n\n* fix more\n\n* more prints\n\n* add\n\n* up\n\n* up\n\n* up\n\n* uP\n\n* uP\n\n* more fixes\n\n* uP\n\n* up\n\n* up\n\n* up\n\n* up\n\n* fix more\n\n* up\n\n* up\n\n* clean tests\n\n* up\n\n* up\n\n* up\n\n* more fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* make\n\n* correct\n\n* finish\n\n* finish\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AutoencoderKLIntegrationTests(unittest.TestCase):",
            "",
            "def get_generator(self, seed=0):",
            "if torch_device == \"mps\":",
            "-            return torch.Generator().manual_seed(seed)",
            "+            return torch.manual_seed(seed)",
            "return torch.Generator(device=torch_device).manual_seed(seed)",
            "",
            "@parameterized.expand("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=attribute), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=Generator))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 1969,
        "neg_line": [
            "-return torch.Generator().manual_seed(seed)"
        ],
        "pos_line": [
            "+return torch.manual_seed(seed)"
        ],
        "core_change": "-return torch.Generator().manual_seed(seed) +return torch.manual_seed(seed)",
        "core_API": "Generator"
    },
    {
        "commit_hash": "6f83515e9ef227c5dae371a3907be283fe8e0d08",
        "index": "f557a19..c5a6230 100644",
        "commit_message": "text classifier bug fix (#991)\n\n* update\n\n* bug fix\n\n* bug fix\n\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class AutoTuner(kerastuner.engine.multi_execution_tuner.MultiExecutionTuner):",
            "self.oracle.update_trial(",
            "trial.trial_id, metrics=averaged_metrics, step=self._reported_step)",
            "",
            "+        tf.keras.backend.clear_session()",
            "+",
            "def search(self,",
            "callbacks=None,",
            "fit_on_val_data=False,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=1900544)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1900545)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1900546)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1900547)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1900548)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1900549)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'clear_session'), position=2, insert_id=1900550)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1900551)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1900552)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1900553)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1900554)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'backend'), position=2, insert_id=1900555)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1900556)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1900557)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'keras'), position=2, insert_id=1900558)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 15,
        "number": 1970,
        "neg_line": [],
        "pos_line": [
            "+tf.keras.backend.clear_session()",
            "+"
        ],
        "core_change": "+tf.keras.backend.clear_session() +",
        "core_API": "update_trial"
    },
    {
        "commit_hash": "d984b10335bd590c75dc188f6be15d1c6062b5dc",
        "index": "db8bd8c6d..777247161 100644",
        "commit_message": "Add 'with torch.no_grad()' to BEiT integration test forward passes (#14961)\n\n* Add 'with torch.no_grad()' to BEiT integration test forward pass\n\n* Fix inconsistent use of tabs and spaces in indentation\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BeitModelIntegrationTest(unittest.TestCase):",
            "inputs = feature_extractor(images=image, return_tensors=\"pt\").to(torch_device)",
            "",
            "# forward pass",
            "-        outputs = model(**inputs)",
            "+        with torch.no_grad():",
            "+            outputs = model(**inputs)",
            "logits = outputs.logits",
            "",
            "# verify the logits"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('with_statement', None), position=2, insert_id=1207425)",
            "Insert(target_node=IN(type=with_statement), node=('with', 'with'), position=0, insert_id=1207426)",
            "Insert(target_node=IN(type=with_statement), node=('with_clause', None), position=1, insert_id=1207427)",
            "Insert(target_node=IN(type=with_statement), node=(':', ':'), position=2, insert_id=1207428)",
            "Insert(target_node=IN(type=with_statement), node=('block', None), position=3, insert_id=1207429)",
            "Insert(target_node=IN(type=with_clause), node=('with_item', None), position=0, insert_id=1207430)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=with_item), node=('call', None), position=0, insert_id=1207431)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1207432)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1207433)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1207434)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1207435)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'no_grad'), position=2, insert_id=1207436)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1207437)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1207438)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 1972,
        "neg_line": [
            "-outputs = model(**inputs)"
        ],
        "pos_line": [
            "+with torch.no_grad():",
            "+outputs = model(**inputs)"
        ],
        "core_change": "-outputs = model(**inputs) +with torch.no_grad(): +outputs = model(**inputs)",
        "core_API": "no_grad"
    },
    {
        "commit_hash": "8d8677e03b5ab7cd65d965d187438a79cdfb350b",
        "index": "fcb2694..ed9132d 100644",
        "commit_message": "Fix #139. Broken SKResNets after BlurPool addition, as a plus, SKResNets support AA now too.\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class SelectiveKernelConv(nn.Module):",
            "groups = min(out_channels, groups)",
            "",
            "conv_kwargs = dict(",
            "-            stride=stride, groups=groups, drop_block=drop_block, act_layer=act_layer, norm_layer=norm_layer)",
            "+            stride=stride, groups=groups, drop_block=drop_block, act_layer=act_layer, norm_layer=norm_layer,",
            "+            aa_layer=aa_layer)",
            "self.paths = nn.ModuleList([",
            "ConvBnAct(in_channels, out_channels, kernel_size=k, dilation=d, **conv_kwargs)",
            "for k, d in zip(kernel_size, dilation)])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=10, insert_id=1481825)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=11, insert_id=1481826)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'aa_layer'), position=0, insert_id=1481827)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1481828)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'aa_layer'), position=2, insert_id=1481829)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1974,
        "neg_line": [
            "-stride=stride, groups=groups, drop_block=drop_block, act_layer=act_layer, norm_layer=norm_layer)"
        ],
        "pos_line": [
            "+stride=stride, groups=groups, drop_block=drop_block, act_layer=act_layer, norm_layer=norm_layer,",
            "+aa_layer=aa_layer)"
        ],
        "core_change": "-stride=stride, groups=groups, drop_block=drop_block, act_layer=act_layer, norm_layer=norm_layer) +stride=stride, groups=groups, drop_block=drop_block, act_layer=act_layer, norm_layer=norm_layer, +aa_layer=aa_layer)",
        "core_API": "ModuleList"
    },
    {
        "commit_hash": "a308dbc7f74dbda127e6c341afba8710fde873e5",
        "index": "6818f19d..e55caf62 100644",
        "commit_message": "[PyTorch] Add Multiple GPUs Scratch (#1596)\n\n* [PyTorch] Add Multiple GPUs Scratch\n\n* fix discuss tab link\n",
        "file": "d2l-en.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "def train_ch11(trainer_fn, states, hyperparams, data_iter,",
            "def train_concise_ch11(trainer_fn, hyperparams, data_iter, num_epochs=2):",
            "# Initialization",
            "net = tf.keras.Sequential()",
            "-    net.add(tf.keras.layers.Dense(1,",
            "+    net.add(tf.keras.layers.Dense(1,",
            "kernel_initializer=tf.random_normal_initializer(stddev=0.01)))",
            "optimizer = trainer_fn(**hyperparams)",
            "loss = tf.keras.losses.MeanSquaredError()",
            "-    # Note: L2 Loss = 1/2 * MSE Loss. TensorFlow has MSE Loss which is",
            "+    # Note: L2 Loss = 1/2 * MSE Loss. TensorFlow has MSE Loss which is",
            "# slightly different from MXNet's L2Loss by a factor of 2. Hence we halve",
            "# the loss value to get L2Loss in TensorFlow",
            "animator = d2l.Animator(xlabel='epoch', ylabel='loss',"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 1975,
        "neg_line": [
            "-net.add(tf.keras.layers.Dense(1,",
            "-# Note: L2 Loss = 1/2 * MSE Loss. TensorFlow has MSE Loss which is"
        ],
        "pos_line": [
            "+net.add(tf.keras.layers.Dense(1,",
            "+# Note: L2 Loss = 1/2 * MSE Loss. TensorFlow has MSE Loss which is"
        ],
        "core_change": "-net.add(tf.keras.layers.Dense(1, +net.add(tf.keras.layers.Dense(1, -# Note: L2 Loss = 1/2 * MSE Loss. TensorFlow has MSE Loss which is +# Note: L2 Loss = 1/2 * MSE Loss. TensorFlow has MSE Loss which is",
        "core_API": "Sequential"
    },
    {
        "commit_hash": "c402309470740e17973eeee22a7b90123f85b310",
        "index": "6f4daad4e..cf80914ae 100644",
        "commit_message": "fix CI error\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ESPnetEnhancementModel(AbsESPnetModel):",
            "losses = torch.stack([pair_loss(p) for p in all_permutations], dim=1)",
            "loss, perm = torch.min(losses, dim=1)",
            "perm = torch.index_select(",
            "-                torch.tensor(all_permutations, device=device, dtype=torch.long),",
            "-                0,",
            "-                perm,",
            "+                torch.tensor(all_permutations, device=device, dtype=torch.long), 0, perm",
            ")",
            "else:",
            "loss = torch.tensor("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 1,
        "number": 1977,
        "neg_line": [
            "-torch.tensor(all_permutations, device=device, dtype=torch.long),",
            "-0,",
            "-perm,"
        ],
        "pos_line": [
            "+torch.tensor(all_permutations, device=device, dtype=torch.long), 0, perm"
        ],
        "core_change": "-torch.tensor(all_permutations, device=device, dtype=torch.long), -0, -perm, +torch.tensor(all_permutations, device=device, dtype=torch.long), 0, perm",
        "core_API": "stack"
    },
    {
        "commit_hash": "365c56d22465a72478cc4f4aa07981fd71b7d281",
        "index": "e257745b..3e4aad8b 100644",
        "commit_message": "fix ns in replicated mode\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def regularize_cost_from_collection(name='regularize_cost'):",
            "losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)",
            "if len(losses) > 0:",
            "logger.info(\"Add REGULARIZATION_LOSSES of {} tensors on the total cost.\".format(len(losses)))",
            "-        reg_loss = tf.add_n(losses)",
            "+        reg_loss = tf.add_n(losses, name=name)",
            "return reg_loss",
            "else:",
            "return None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2290900)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2290901)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'name'), position=0, insert_id=2290902)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2290903)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'name'), position=2, insert_id=2290904)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1978,
        "neg_line": [
            "-reg_loss = tf.add_n(losses)"
        ],
        "pos_line": [
            "+reg_loss = tf.add_n(losses, name=name)"
        ],
        "core_change": "-reg_loss = tf.add_n(losses) +reg_loss = tf.add_n(losses, name=name)",
        "core_API": "get_collection"
    },
    {
        "commit_hash": "b7175a27018425d46a01b9e5a0595e6e9b1ab6a1",
        "index": "9dfd42c36..af67a8442 100644",
        "commit_message": "fixed imports in tests and gpt2 config test\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class TFSequenceSummary(tf.keras.layers.Layer):",
            "if training and self.first_dropout is not None:",
            "output = self.first_dropout(output)",
            "",
            "-        output = self.summary(output)",
            "+        if self.summary is not None:",
            "+            output = self.summary(output)",
            "",
            "if self.activation is not None:",
            "output = self.activation(output)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=2644904)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2644905)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=2644906)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2644907)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=2644908)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=2644909)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=2644910)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=2644911)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=2644912)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2644913)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2644914)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'summary'), position=2, insert_id=2644915)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 1979,
        "neg_line": [
            "-output = self.summary(output)"
        ],
        "pos_line": [
            "+if self.summary is not None:",
            "+output = self.summary(output)"
        ],
        "core_change": "-output = self.summary(output) +if self.summary is not None: +output = self.summary(output)",
        "core_API": "first_dropout"
    },
    {
        "commit_hash": "4de68a42a0df89df01a999ee48f361f24c8c19d4",
        "index": "b3a7df6d..8a353231 100644",
        "commit_message": "Improves API docs and docstring consistency (#4244)\n\n* refactor py2md\n\n* improve py2md, warn if backticks missing\n\n* ensure backticks consistent\n\n* remove docstring help test\n\n* fixes and handle more edge cases\n\n* add failing test for pydoc-markdown bug\n\n* update pydoc-markdown\n\n* fix some links\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class GatedSum(torch.nn.Module):",
            "",
            "input_dim : `int`, required",
            "The dimensionality of the input. We assume the input have shape `(..., input_dim)`.",
            "-    activation : `Activation`, optional (default = torch.nn.Sigmoid())",
            "+    activation : `Activation`, optional (default = `torch.nn.Sigmoid()`)",
            "The activation function to use.",
            "\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('string', '`torch.nn.Sigmoid()`'), position=2, insert_id=1309152)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Sigmoid))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 1983,
        "neg_line": [
            "-activation : `Activation`, optional (default = torch.nn.Sigmoid())"
        ],
        "pos_line": [
            "+activation : `Activation`, optional (default = `torch.nn.Sigmoid()`)"
        ],
        "core_change": "-activation : `Activation`, optional (default = torch.nn.Sigmoid()) +activation : `Activation`, optional (default = `torch.nn.Sigmoid()`)",
        "core_API": "Sigmoid"
    },
    {
        "commit_hash": "34307bb358b568b271362f40cb82ad4ca1ef0e8f",
        "index": "f0a890314..c38f956cd 100644",
        "commit_message": "Fix tests (#14289)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BeitModelTest(ModelTesterMixin, unittest.TestCase):",
            "# this can then be incorporated into _prepare_for_class in test_modeling_common.py",
            "elif model_class.__name__ == \"BeitForSemanticSegmentation\":",
            "batch_size, num_channels, height, width = inputs_dict[\"pixel_values\"].shape",
            "-                inputs_dict[\"labels\"] = torch.zeros([self.model_tester.batch_size, height, width]).long()",
            "+                inputs_dict[\"labels\"] = torch.zeros(",
            "+                    [self.model_tester.batch_size, height, width], device=torch_device",
            "+                ).long()",
            "model = model_class(config)",
            "model.to(torch_device)",
            "model.train()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1209791)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1209792)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1209793)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1209794)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'torch_device'), position=2, insert_id=1209795)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 1986,
        "neg_line": [
            "-inputs_dict[\"labels\"] = torch.zeros([self.model_tester.batch_size, height, width]).long()"
        ],
        "pos_line": [
            "+inputs_dict[\"labels\"] = torch.zeros(",
            "+[self.model_tester.batch_size, height, width], device=torch_device",
            "+).long()"
        ],
        "core_change": "-inputs_dict[\"labels\"] = torch.zeros([self.model_tester.batch_size, height, width]).long() +inputs_dict[\"labels\"] = torch.zeros( +[self.model_tester.batch_size, height, width], device=torch_device +).long()",
        "core_API": "zeros"
    },
    {
        "commit_hash": "8b605fc4751858dceda77131462a7a8f36d2f8b1",
        "index": "141e6098a..34e23b808 100644",
        "commit_message": "Fix TensorFlow flags for TensorFlow 1.4 (FLAGS object was being conflated with the flags module), fix writing empty configs to disk (was causing a parse error).\n\n",
        "file": "wandb.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "class Config(object):",
            "return defaults",
            "",
            "def __str__(self):",
            "-        s = \"wandb_version: 1\\n\\n\"",
            "-        s += yaml.dump(self.as_dict(), default_flow_style=False)",
            "+        s = \"wandb_version: 1\"",
            "+        as_dict = self.as_dict()",
            "+        if as_dict:  # adding an empty dictionary here causes a parse error",
            "+            s += '\\n\\n' + yaml.dump(as_dict, default_flow_style=False)",
            "return s"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=1, insert_id=2463316)",
            "Insert(target_node=ASTNode(type=block), node=('if_statement', None), position=2, insert_id=2463317)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2463318)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2463319)",
            "Insert(target_node=IN(type=if_statement), node=('identifier', 'as_dict'), position=1, insert_id=2463320)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2463321)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=2463322)",
            "Update(target_node=ASTNode(type=string, text=\"wandb_version: 1\\n\\n\"), value='\"wandb_version: 1\"')",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'as_dict'), position=0, insert_id=2463323)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2463324)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=ASTNode(type=augmented_assignment), node=('binary_operator', None), position=2, insert_id=2463325)",
            "Insert(target_node=IN(type=binary_operator), node=('string', \"'\\\\n\\\\n'\"), position=0, insert_id=2463326)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=2463327)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'as_dict'), position=1, insert_id=2463328)"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 1987,
        "neg_line": [
            "-s = \"wandb_version: 1\\n\\n\"",
            "-s += yaml.dump(self.as_dict(), default_flow_style=False)"
        ],
        "pos_line": [
            "+s = \"wandb_version: 1\"",
            "+as_dict = self.as_dict()",
            "+if as_dict:  # adding an empty dictionary here causes a parse error",
            "+s += '\\n\\n' + yaml.dump(as_dict, default_flow_style=False)"
        ],
        "core_change": "-s = \"wandb_version: 1\\n\\n\" -s += yaml.dump(self.as_dict(), default_flow_style=False) +s = \"wandb_version: 1\" +as_dict = self.as_dict() +if as_dict:  # adding an empty dictionary here causes a parse error +s += '\\n\\n' + yaml.dump(as_dict, default_flow_style=False)",
        "core_API": "dump"
    },
    {
        "commit_hash": "98d638c70cdbe751153c10fc571c34beac228347",
        "index": "de25de65..347b8118 100644",
        "commit_message": "Mma refactor (#2087)\n\nSummary:\nFixing issues ([3546](https://github.com/pytorch/fairseq/issues/3546)) with latency augmented training for mma due to the change of fairseq APIs\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2087\n\nReviewed By: hygong-fb\n\nDifferential Revision: D29851286\n\nPulled By: xutaima\n\nfbshipit-source-id: 6c3077db06b89c23b312b28527d7395a725f3b3a\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TransformerEncoderLayerBase(nn.Module):",
            "# the attention weight (before softmax) for some padded element in query",
            "# will become -inf, which results in NaN in model parameters",
            "if attn_mask is not None:",
            "-            attn_mask = attn_mask.masked_fill(attn_mask.to(torch.bool), -1e8)",
            "+            attn_mask = attn_mask.masked_fill(",
            "+                attn_mask.to(torch.bool),",
            "+                -1e8 if x.dtype == torch.float32 else -1e4",
            "+            )",
            "",
            "residual = x",
            "if self.normalize_before:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('conditional_expression', None), position=3, insert_id=205567)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=unary_operator), position=0)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=205568)",
            "Insert(target_node=IN(type=conditional_expression), node=('comparison_operator', None), position=2, insert_id=205569)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=205570)",
            "Insert(target_node=IN(type=conditional_expression), node=('unary_operator', None), position=4, insert_id=205571)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=205572)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=205573)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=2, insert_id=205574)",
            "Insert(target_node=IN(type=unary_operator), node=('-', '-'), position=0, insert_id=205575)",
            "Insert(target_node=IN(type=unary_operator), node=('float', '1e4'), position=1, insert_id=205576)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x'), position=0, insert_id=205577)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=205578)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=205579)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=205580)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=205581)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=205582)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 1988,
        "neg_line": [
            "-attn_mask = attn_mask.masked_fill(attn_mask.to(torch.bool), -1e8)"
        ],
        "pos_line": [
            "+attn_mask = attn_mask.masked_fill(",
            "+attn_mask.to(torch.bool),",
            "+-1e8 if x.dtype == torch.float32 else -1e4",
            "+)"
        ],
        "core_change": "-attn_mask = attn_mask.masked_fill(attn_mask.to(torch.bool), -1e8) +attn_mask = attn_mask.masked_fill( +attn_mask.to(torch.bool), +-1e8 if x.dtype == torch.float32 else -1e4 +)",
        "core_API": "masked_fill"
    },
    {
        "commit_hash": "0ab3171e1348173fed8a889131ceb8a6e241a843",
        "index": "9e2f3359..8a0804d9 100755",
        "commit_message": "Fixing Huber Loss calculation for affine portion.\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class QModel(Model):",
            "",
            "# If loss clipping is used, calculate the huber loss",
            "if config.clip_loss > 0.0:",
            "-                huber_loss = tf.where(condition=(tf.abs(delta) < config.clip_loss), x=(0.5 * self.loss_per_instance), y=(tf.abs(delta) - 0.5))",
            "+                huber_loss = tf.where(condition=(tf.abs(delta) < config.clip_loss), x=(0.5 * self.loss_per_instance),",
            "+                                      y=config.clip_loss * tf.abs(delta) - 0.5 * config.clip_loss ** 2)",
            "self.q_loss = tf.reduce_mean(input_tensor=huber_loss, axis=0)",
            "else:",
            "self.q_loss = tf.reduce_mean(input_tensor=self.loss_per_instance, axis=0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('binary_operator', None), position=2, insert_id=2242736)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=binary_operator), position=0)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=-, text=-), position=1)",
            "Insert(target_node=IN(type=binary_operator), node=('binary_operator', None), position=2, insert_id=2242737)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('attribute', None), position=0, insert_id=2242738)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('*', '*'), position=1, insert_id=2242739)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=float, text=0.5), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=2242740)",
            "Insert(target_node=IN(type=binary_operator), node=('binary_operator', None), position=2, insert_id=2242741)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=0, insert_id=2242742)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2242743)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'clip_loss'), position=2, insert_id=2242744)",
            "Insert(target_node=IN(type=binary_operator), node=('attribute', None), position=0, insert_id=2242745)",
            "Insert(target_node=IN(type=binary_operator), node=('**', '**'), position=1, insert_id=2242746)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '2'), position=2, insert_id=2242747)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=0, insert_id=2242748)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2242749)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'clip_loss'), position=2, insert_id=2242750)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 1989,
        "neg_line": [
            "-huber_loss = tf.where(condition=(tf.abs(delta) < config.clip_loss), x=(0.5 * self.loss_per_instance), y=(tf.abs(delta) - 0.5))"
        ],
        "pos_line": [
            "+huber_loss = tf.where(condition=(tf.abs(delta) < config.clip_loss), x=(0.5 * self.loss_per_instance),",
            "+y=config.clip_loss * tf.abs(delta) - 0.5 * config.clip_loss ** 2)"
        ],
        "core_change": "-huber_loss = tf.where(condition=(tf.abs(delta) < config.clip_loss), x=(0.5 * self.loss_per_instance), y=(tf.abs(delta) - 0.5)) +huber_loss = tf.where(condition=(tf.abs(delta) < config.clip_loss), x=(0.5 * self.loss_per_instance), +y=config.clip_loss * tf.abs(delta) - 0.5 * config.clip_loss ** 2)",
        "core_API": "where"
    },
    {
        "commit_hash": "b221776a1429d39178d3dfc0a8fea08fc846e079",
        "index": "edd691630..3720d4f66 100644",
        "commit_message": "fixes issue #1398\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TestTorchVariable(TestCase):",
            "",
            "datasets = [(data_bob, target_bob), (data_alice, target_alice)]",
            "",
            "-        for iter in range(6):",
            "+        for iter in range(2):",
            "",
            "for data, target in datasets:",
            "model.send(data.owners[0])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=6), value='2')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 1991,
        "neg_line": [
            "-for iter in range(6):"
        ],
        "pos_line": [
            "+for iter in range(2):"
        ],
        "core_change": "-for iter in range(6): +for iter in range(2):",
        "core_API": "send"
    },
    {
        "commit_hash": "e72501dc8c9e371f4aebf5c6595350fb303dbf34",
        "index": "28a5d8c608..9dc0de62be 100644",
        "commit_message": "statistical tests fixup\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def prod(",
            "if dtype is None:",
            "dtype = _infer_dtype(x.dtype)",
            "axis = tuple(axis) if isinstance(axis, list) else axis",
            "-    return tf.experimental.numpy.prod(x, axis, dtype, keepdims)",
            "+    return tf.experimental.numpy.prod(x, axis=axis, dtype=dtype, keepdims=keepdims)",
            "",
            "",
            "def std("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1993384)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=6, insert_id=1993385)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=7, insert_id=1993386)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=8, insert_id=1993387)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=axis), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1993388)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'axis'), position=2, insert_id=1993389)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=dtype), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1993390)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=2, insert_id=1993391)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=keepdims), position=0)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1993392)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'keepdims'), position=2, insert_id=1993393)",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 1992,
        "neg_line": [
            "-return tf.experimental.numpy.prod(x, axis, dtype, keepdims)"
        ],
        "pos_line": [
            "+return tf.experimental.numpy.prod(x, axis=axis, dtype=dtype, keepdims=keepdims)"
        ],
        "core_change": "-return tf.experimental.numpy.prod(x, axis, dtype, keepdims) +return tf.experimental.numpy.prod(x, axis=axis, dtype=dtype, keepdims=keepdims)",
        "core_API": "prod"
    },
    {
        "commit_hash": "ab91b045153a2d23f98f0a93ccb3854ef163e28b",
        "index": "bcecc81..60be521 100644",
        "commit_message": "Fix GPU issue on the blazeface detector.\n\n",
        "file": "face-alignment.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BlazeFace(nn.Module):",
            "for i in range(raw_box_tensor.shape[0]):",
            "boxes = detection_boxes[i, mask[i]]",
            "scores = detection_scores[i, mask[i]].unsqueeze(dim=-1)",
            "-            output_detections.append(torch.cat((boxes, scores), dim=-1))",
            "+            output_detections.append(torch.cat((boxes, scores), dim=-1).to('cpu'))",
            "",
            "return output_detections"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=200209)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=200210)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=200211)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=200212)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=200213)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'cpu'\"), position=1, insert_id=200214)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=200215)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 1997,
        "neg_line": [
            "-output_detections.append(torch.cat((boxes, scores), dim=-1))"
        ],
        "pos_line": [
            "+output_detections.append(torch.cat((boxes, scores), dim=-1).to('cpu'))"
        ],
        "core_change": "-output_detections.append(torch.cat((boxes, scores), dim=-1)) +output_detections.append(torch.cat((boxes, scores), dim=-1).to('cpu'))",
        "core_API": "append"
    },
    {
        "commit_hash": "d700ddbf246b6d91dcd59c074584f9a30fde4ee0",
        "index": "218381a0..8cb2e9ca 100644",
        "commit_message": "Integration of `nn.aggr` within `MessagPassing` (#4779)\n\n* Add sum alias class\n\n* Add message passing integration with nn.aggr\n\n* Cleanup MessagePassing\n\n* Raise errors in resolver\n\n* changelog\n\n* Add arg_kwargs support for\n\n* update\n\n* error\n\n* update\n\n* update\n\n* fix test\n\n* update\n\n* update\n\n* typo\n\n* typo\n\n* typo\n\n* reset\n\n* update\n\n* update\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def test_activation_resolver():",
            "@pytest.mark.parametrize('aggr_tuple', [",
            "(torch_geometric.nn.aggr.MeanAggregation, 'mean'),",
            "(torch_geometric.nn.aggr.SumAggregation, 'sum'),",
            "+    (torch_geometric.nn.aggr.SumAggregation, 'add'),",
            "(torch_geometric.nn.aggr.MaxAggregation, 'max'),",
            "(torch_geometric.nn.aggr.MinAggregation, 'min'),",
            "(torch_geometric.nn.aggr.MulAggregation, 'mul'),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('tuple', None), position=10, insert_id=1489350)",
            "Insert(target_node=ASTNode(type=ERROR), node=(',', ','), position=11, insert_id=1489351)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=1489352)",
            "Insert(target_node=IN(type=tuple), node=('attribute', None), position=1, insert_id=1489353)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=1489354)",
            "Insert(target_node=IN(type=tuple), node=('string', \"'add'\"), position=3, insert_id=1489355)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=4, insert_id=1489356)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1489357)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1489358)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'SumAggregation'), position=2, insert_id=1489359)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1489360)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1489361)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'aggr'), position=2, insert_id=1489362)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch_geometric'), position=0, insert_id=1489363)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1489364)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=2, insert_id=1489365)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 16,
        "number": 1998,
        "neg_line": [],
        "pos_line": [
            "+(torch_geometric.nn.aggr.SumAggregation, 'add'),"
        ],
        "core_change": "+(torch_geometric.nn.aggr.SumAggregation, 'add'),",
        "core_API": "parametrize"
    },
    {
        "commit_hash": "44d2847610944f56a06b7cfa54faadb66e130a83",
        "index": "a0fa4499..4c5126d2 100644",
        "commit_message": "Metrics in distributed setting (#4525)\n\n* initial commit to ensure that metrics work correctly in distributed setting\n\n* updating global_distributed_metric to take metric object\n\n* adding distributed f1 score\n\n* adding distributed attachment scores\n\n* bug fix\n\n* adding distributed boolean accuracy\n\n* adding distributed entropy\n\n* adding distributed evalb\n\n* adding distributed mean_absolute_error\n\n* adding distributed sequence accuracy\n\n* adding distributed unigram recall\n\n* making models compatible with distributed metrics\n\n* adding distributed auc\n\n* adding distributed bleu\n\n* adding missing argument\n\n* initial commit to ensure that metrics work correctly in distributed setting\n\n* updating global_distributed_metric to take metric object\n\n* adding distributed f1 score\n\n* adding distributed attachment scores\n\n* bug fix\n\n* adding distributed boolean accuracy\n\n* adding distributed entropy\n\n* adding distributed evalb\n\n* adding distributed mean_absolute_error\n\n* adding distributed sequence accuracy\n\n* adding distributed unigram recall\n\n* making models compatible with distributed metrics\n\n* adding distributed auc\n\n* adding distributed bleu\n\n* adding missing argument\n\n* changing start method\n\n* removing unnecessary argument\n\n* adding remaining metrics, removing extra argument\n\n* allowing float values\n\n* bug fix\n\n* more bug fixes\n\n* changing average to return float\n\n* adding timeout for distributed test\n\n* testing unequal batches\n\n* adding distributed auc\n\n* adding distributed spearman correlation\n\n* adding distributed covariance and pearson correlation\n\n* changing distributed test to function, misc changes\n\n* checking batch lengths explicitly to raise errors\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Perplexity(Average):",
            "\"\"\"",
            "average_loss = super().get_metric(reset)",
            "if average_loss == 0:",
            "-            return 0.0",
            "+            perplexity = 0.0",
            "",
            "# Exponentiate the loss to compute perplexity",
            "-        return float(torch.exp(average_loss))",
            "+        perplexity = float(torch.exp(average_loss))",
            "+",
            "+        return perplexity"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=12113)",
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=5, insert_id=12114)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=12115)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=12116)",
            "Insert(target_node=IN(type=return_statement), node=('identifier', 'perplexity'), position=1, insert_id=12117)",
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=12118)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'perplexity'), position=0, insert_id=12119)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=12120)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=12121)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'perplexity'), position=0, insert_id=12122)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=12123)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=float, text=0.0), position=2)",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=return_statement))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 1999,
        "neg_line": [
            "-return 0.0",
            "-return float(torch.exp(average_loss))"
        ],
        "pos_line": [
            "+perplexity = 0.0",
            "+perplexity = float(torch.exp(average_loss))",
            "+",
            "+return perplexity"
        ],
        "core_change": "-return 0.0 +perplexity = 0.0 -return float(torch.exp(average_loss)) +perplexity = float(torch.exp(average_loss)) + +return perplexity",
        "core_API": "exp"
    },
    {
        "commit_hash": "c560114324409864273621b113cca726a2e9216c",
        "index": "3c0784f7..4ca95fba 100644",
        "commit_message": "Fix #750\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class Trainer:",
            "\" > Model restored from step %d\" % checkpoint[\"step\"],",
            ")",
            "restore_step = checkpoint[\"step\"]",
            "+        torch.cuda.empty_cache()",
            "return model, optimizer, scaler, restore_step",
            "",
            "def _get_loader("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1879360)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1879361)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1879362)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1879363)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1879364)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1879365)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'empty_cache'), position=2, insert_id=1879366)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1879367)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1879368)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1879369)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1879370)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=1879371)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 12,
        "number": 2000,
        "neg_line": [],
        "pos_line": [
            "+torch.cuda.empty_cache()"
        ],
        "core_change": "+torch.cuda.empty_cache()",
        "core_API": "empty_cache"
    },
    {
        "commit_hash": "1924c7cc3da2c2309f57571a45ff0cfdff021490",
        "index": "= unet_number - 1",
        "commit_message": "fix issue with mixed precision and gradient clipping\n\n",
        "file": "DALLE2-pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class DecoderTrainer(nn.Module):",
            "index = unet_number - 1",
            "unet = self.decoder.unets[index]",
            "",
            "-        if exists(self.max_grad_norm):",
            "-            nn.utils.clip_grad_norm_(unet.parameters(), self.max_grad_norm)",
            "-",
            "optimizer = getattr(self, f'optim{index}')",
            "scaler = getattr(self, f'scaler{index}')",
            "",
            "+        if exists(self.max_grad_norm):",
            "+            scaler.unscale_(optimizer)",
            "+            nn.utils.clip_grad_norm_(unet.parameters(), self.max_grad_norm)",
            "+",
            "scaler.step(optimizer)",
            "scaler.update()",
            "optimizer.zero_grad()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=if_statement), node=ASTNode(type=module), position=6)",
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=1323048)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1323049)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1323050)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1323051)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'scaler'), position=0, insert_id=1323052)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1323053)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'unscale_'), position=2, insert_id=1323054)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1323055)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'optimizer'), position=1, insert_id=1323056)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1323057)"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 2003,
        "neg_line": [
            "-if exists(self.max_grad_norm):",
            "-nn.utils.clip_grad_norm_(unet.parameters(), self.max_grad_norm)",
            "-"
        ],
        "pos_line": [
            "+if exists(self.max_grad_norm):",
            "+scaler.unscale_(optimizer)",
            "+nn.utils.clip_grad_norm_(unet.parameters(), self.max_grad_norm)",
            "+"
        ],
        "core_change": "-if exists(self.max_grad_norm): -nn.utils.clip_grad_norm_(unet.parameters(), self.max_grad_norm) - +if exists(self.max_grad_norm): +scaler.unscale_(optimizer) +nn.utils.clip_grad_norm_(unet.parameters(), self.max_grad_norm) +",
        "core_API": "clip_grad_norm_"
    },
    {
        "commit_hash": "a59eb349c5616c1b48ae9225028fb41ec1feb6aa",
        "index": "fe0dbd281..818191b72 100644",
        "commit_message": "fix missing \"models\" in pipeline test module (#17090)\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "def get_tiny_config_from_class(configuration_class):",
            "",
            "try:",
            "model_slug = model_type.replace(\"-\", \"_\")",
            "-        module = importlib.import_module(f\".test_modeling_{model_slug}\", package=f\"tests.{model_slug}\")",
            "+        module = importlib.import_module(f\".test_modeling_{model_slug}\", package=f\"tests.models.{model_slug}\")",
            "model_tester_class = getattr(module, f\"{camel_case_model_name}ModelTester\", None)",
            "except (ImportError, AttributeError):",
            "logger.error(f\"No model tester class for {configuration_class.__name__}\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=f\"tests.{model_slug}\"), value='f\"tests.models.{model_slug}\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2005,
        "neg_line": [
            "-module = importlib.import_module(f\".test_modeling_{model_slug}\", package=f\"tests.{model_slug}\")"
        ],
        "pos_line": [
            "+module = importlib.import_module(f\".test_modeling_{model_slug}\", package=f\"tests.models.{model_slug}\")"
        ],
        "core_change": "-module = importlib.import_module(f\".test_modeling_{model_slug}\", package=f\"tests.{model_slug}\") +module = importlib.import_module(f\".test_modeling_{model_slug}\", package=f\"tests.models.{model_slug}\")",
        "core_API": "replace"
    },
    {
        "commit_hash": "6ed9882ddb2b6249463c855dcca6860161d91f3e",
        "index": "a79b45239..e9b57adc8 100755",
        "commit_message": "use functional interface for softmax in attention (#14198)\n\n* use functional interface instead of instantiating module and immediately calling it\n\n* fix torch.nn.functional to nn.functional. Thank you Stas!\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "class MultiHeadSelfAttention(nn.Module):",
            "mask = (mask == 0).view(mask_reshp).expand_as(scores)  # (bs, n_heads, q_length, k_length)",
            "scores = scores.masked_fill(mask, -float(\"inf\"))  # (bs, n_heads, q_length, k_length)",
            "",
            "-        weights = nn.Softmax(dim=-1)(scores)  # (bs, n_heads, q_length, k_length)",
            "+        weights = nn.functional.softmax(scores, dim=-1)  # (bs, n_heads, q_length, k_length)",
            "weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)",
            "",
            "# Mask heads if we want to"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1755105)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1755106)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'softmax'), position=2, insert_id=1755107)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'scores'), position=1, insert_id=1755108)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1755109)",
            "Update(target_node=ASTNode(type=identifier, text=Softmax), value='functional')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=scores))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 2007,
        "neg_line": [
            "-weights = nn.Softmax(dim=-1)(scores)  # (bs, n_heads, q_length, k_length)"
        ],
        "pos_line": [
            "+weights = nn.functional.softmax(scores, dim=-1)  # (bs, n_heads, q_length, k_length)"
        ],
        "core_change": "-weights = nn.Softmax(dim=-1)(scores)  # (bs, n_heads, q_length, k_length) +weights = nn.functional.softmax(scores, dim=-1)  # (bs, n_heads, q_length, k_length)",
        "core_API": "masked_fill"
    },
    {
        "commit_hash": "354b9811f57cbf918a904c6edf174273e228a073",
        "index": "9566a5f5a..a3c03995b 100644",
        "commit_message": "Fix black formatting\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Reporter:",
            "if LooseVersion(torch.__version__) >= LooseVersion(\"1.4.0\"):",
            "if torch.cuda.is_initialized():",
            "stats[\"gpu_max_cached_mem_GB\"] = (",
            "-                    torch.cuda.max_memory_reserved() / 2 ** 30",
            "+                    torch.cuda.max_memory_reserved() / 2**30",
            ")",
            "else:",
            "if torch.cuda.is_available() and torch.cuda.max_memory_cached() > 0:",
            "-                stats[\"gpu_cached_mem_GB\"] = torch.cuda.max_memory_cached() / 2 ** 30",
            "+                stats[\"gpu_cached_mem_GB\"] = torch.cuda.max_memory_cached() / 2**30",
            "",
            "self.stats.setdefault(self.epoch, {})[sub_reporter.key] = stats",
            "sub_reporter.finished()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 2008,
        "neg_line": [
            "-torch.cuda.max_memory_reserved() / 2 ** 30",
            "-stats[\"gpu_cached_mem_GB\"] = torch.cuda.max_memory_cached() / 2 ** 30"
        ],
        "pos_line": [
            "+torch.cuda.max_memory_reserved() / 2**30",
            "+stats[\"gpu_cached_mem_GB\"] = torch.cuda.max_memory_cached() / 2**30"
        ],
        "core_change": "-torch.cuda.max_memory_reserved() / 2 ** 30 +torch.cuda.max_memory_reserved() / 2**30 -stats[\"gpu_cached_mem_GB\"] = torch.cuda.max_memory_cached() / 2 ** 30 +stats[\"gpu_cached_mem_GB\"] = torch.cuda.max_memory_cached() / 2**30",
        "core_API": "is_initialized"
    },
    {
        "commit_hash": "0ba7472da92a89af9aca84de5b01a228d18340a2",
        "index": "253e1a35f..66bb04f78 100644",
        "commit_message": "[Testing] Fix LINT/sphinx errors. (#8874)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "from ray.rllib.utils import try_import_torch",
            "_, nn = try_import_torch()",
            "",
            "",
            "-class VisionNetwork(TorchModelV2):",
            "+class VisionNetwork(TorchModelV2, nn.Module):",
            "\"\"\"Generic vision network.\"\"\"",
            "",
            "def __init__(self, obs_space, action_space, num_outputs, model_config,",
            "name):",
            "TorchModelV2.__init__(self, obs_space, action_space, num_outputs,",
            "model_config, name)",
            "+        nn.Module.__init__(self)",
            "",
            "activation = get_activation_fn(",
            "model_config.get(\"conv_activation\"), framework=\"torch\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=6, insert_id=1506474)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1506475)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1506476)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=1506477)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1506478)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1506479)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1506480)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1506481)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Module'), position=2, insert_id=1506482)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1506483)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1506484)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__init__'), position=2, insert_id=1506485)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1506486)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'self'), position=1, insert_id=1506487)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1506488)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1506489)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1506490)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Module'), position=2, insert_id=1506491)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 2009,
        "neg_line": [
            "-class VisionNetwork(TorchModelV2):"
        ],
        "pos_line": [
            "+class VisionNetwork(TorchModelV2, nn.Module):",
            "+nn.Module.__init__(self)"
        ],
        "core_change": "-class VisionNetwork(TorchModelV2): +class VisionNetwork(TorchModelV2, nn.Module): +nn.Module.__init__(self)",
        "core_API": "__init__"
    },
    {
        "commit_hash": "37e98945728f9961fd33d598a954358d79988c28",
        "index": "0ea3558e..52b8a3e3 100644",
        "commit_message": "fix flake8 style in tensorpack/\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def FullyConnected(x, out_dim,",
            "prod = tf.nn.xw_plus_b(x, W, b) if use_bias else tf.matmul(x, W)",
            "if nl is None:",
            "logger.warn(",
            "-            \"[DEPRECATED] Default ReLU nonlinearity for Conv2D and FullyConnected will be deprecated. Please use argscope instead.\")",
            "+            \"[DEPRECATED] Default ReLU nonlinearity for Conv2D and FullyConnected will be deprecated.\"",
            "+            \" Please use argscope instead.\")",
            "nl = tf.nn.relu",
            "return nl(prod, name='output')"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('concatenated_string', None), position=1, insert_id=2308333)",
            "Update(target_node=ASTNode(type=string, text=\"[DEPRECATED] Default ReLU nonlinearity for Conv2D and FullyConnected will be deprecated. Please use argscope instead.\"), value='\"[DEPRECATED] Default ReLU nonlinearity for Conv2D and FullyConnected will be deprecated.\"')",
            "Move(target_node=IN(type=concatenated_string), node=ASTNode(type=string, text=\"[DEPRECATED] Default ReLU nonlinearity for Conv2D and FullyConnected will be deprecated. Please use argscope instead.\"), position=0)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', '\" Please use argscope instead.\"'), position=1, insert_id=2308334)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 2010,
        "neg_line": [
            "-\"[DEPRECATED] Default ReLU nonlinearity for Conv2D and FullyConnected will be deprecated. Please use argscope instead.\")"
        ],
        "pos_line": [
            "+\"[DEPRECATED] Default ReLU nonlinearity for Conv2D and FullyConnected will be deprecated.\"",
            "+\" Please use argscope instead.\")"
        ],
        "core_change": "-\"[DEPRECATED] Default ReLU nonlinearity for Conv2D and FullyConnected will be deprecated. Please use argscope instead.\") +\"[DEPRECATED] Default ReLU nonlinearity for Conv2D and FullyConnected will be deprecated.\" +\" Please use argscope instead.\")",
        "core_API": "xw_plus_b"
    },
    {
        "commit_hash": "688c3e8e402b1a9c49404e5a414daa7452d53139",
        "index": "145157c54..6ff31a4de 100755",
        "commit_message": "Update `max_diff` in `test_save_load_fast_init_to_base` (#19849)\n\n* Fix test_save_load_fast_init_to_base\n\n* Fix test_save_load_fast_init_to_base\n\n* update\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ModelTesterMixin:",
            "model_slow_init = base_class_copy.from_pretrained(tmpdirname, _fast_init=False)",
            "",
            "for key in model_fast_init.state_dict().keys():",
            "-                    max_diff = (model_slow_init.state_dict()[key] - model_fast_init.state_dict()[key]).sum().item()",
            "+                    max_diff = torch.max(",
            "+                        torch.abs(model_slow_init.state_dict()[key] - model_fast_init.state_dict()[key])",
            "+                    ).item()",
            "self.assertLessEqual(max_diff, 1e-3, msg=f\"{key} not identical\")",
            "",
            "def test_initialization(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1187854)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1187855)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1187856)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1187857)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'max'), position=2, insert_id=1187858)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1187859)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=1187860)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1187861)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1187862)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1187863)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1187864)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'abs'), position=2, insert_id=1187865)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=binary_operator), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=parenthesized_expression))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 2012,
        "neg_line": [
            "-max_diff = (model_slow_init.state_dict()[key] - model_fast_init.state_dict()[key]).sum().item()"
        ],
        "pos_line": [
            "+max_diff = torch.max(",
            "+torch.abs(model_slow_init.state_dict()[key] - model_fast_init.state_dict()[key])",
            "+).item()"
        ],
        "core_change": "-max_diff = (model_slow_init.state_dict()[key] - model_fast_init.state_dict()[key]).sum().item() +max_diff = torch.max( +torch.abs(model_slow_init.state_dict()[key] - model_fast_init.state_dict()[key]) +).item()",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "c2e70ca76872daf5c975bf8256a210938eaf3599",
        "index": "2f0c2c20..149b616b 100644",
        "commit_message": "upgrade to pytorch 0.4.1 + make work with python 3.7 (but still 3.6 also) (#1543)\n\n* changes for pytorch 0.4.1\n\n* increase tolerance for srl test\n\n* update versions in setup.py\n\n* add script to check requirements.txt vs setup.py + fix setup.py\n\n* loosen bounds on pytorch version\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class WikiTablesSemanticParser(Model):",
            "entity_type_embeddings = self._type_params(entity_types.float())",
            "projected_neighbor_embeddings = self._neighbor_params(embedded_neighbors.float())",
            "# (batch_size, num_entities, embedding_dim)",
            "-        entity_embeddings = torch.nn.functional.tanh(entity_type_embeddings + projected_neighbor_embeddings)",
            "+        entity_embeddings = torch.tanh(entity_type_embeddings + projected_neighbor_embeddings)",
            "",
            "",
            "# Compute entity and question word similarity.  We tried using cosine distance here, but"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=attribute), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=functional))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 2013,
        "neg_line": [
            "-entity_embeddings = torch.nn.functional.tanh(entity_type_embeddings + projected_neighbor_embeddings)"
        ],
        "pos_line": [
            "+entity_embeddings = torch.tanh(entity_type_embeddings + projected_neighbor_embeddings)"
        ],
        "core_change": "-entity_embeddings = torch.nn.functional.tanh(entity_type_embeddings + projected_neighbor_embeddings) +entity_embeddings = torch.tanh(entity_type_embeddings + projected_neighbor_embeddings)",
        "core_API": "_type_params"
    },
    {
        "commit_hash": "351753aae55894591dafa81814eaa82a59687f09",
        "index": "ef411b047d..4694c96c03 100644",
        "commit_message": "[rllib] Remove dependency on TensorFlow (#4764)\n\n* remove hard tf dep\n\n* add test\n\n* comment fix\n\n* fix test\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "_count = 0",
            "",
            "def run_timeline(sess, ops, debug_name, feed_dict={}, timeline_dir=None):",
            "if timeline_dir:",
            "+        from tensorflow.python.client import timeline",
            "+",
            "run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)",
            "run_metadata = tf.RunMetadata()",
            "start = time.time()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=2454520)",
            "Insert(target_node=IN(type=block), node=('import_from_statement', None), position=0, insert_id=2454521)",
            "Insert(target_node=IN(type=import_from_statement), node=('from', 'from'), position=0, insert_id=2454522)",
            "Insert(target_node=IN(type=import_from_statement), node=('dotted_name', None), position=1, insert_id=2454523)",
            "Insert(target_node=IN(type=import_from_statement), node=('import', 'import'), position=2, insert_id=2454524)",
            "Insert(target_node=IN(type=import_from_statement), node=('dotted_name', None), position=3, insert_id=2454525)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'tensorflow'), position=0, insert_id=2454526)",
            "Insert(target_node=IN(type=dotted_name), node=('.', '.'), position=1, insert_id=2454527)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'python'), position=2, insert_id=2454528)",
            "Insert(target_node=IN(type=dotted_name), node=('.', '.'), position=3, insert_id=2454529)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'client'), position=4, insert_id=2454530)",
            "Insert(target_node=IN(type=dotted_name), node=('identifier', 'timeline'), position=0, insert_id=2454531)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 13,
        "number": 2014,
        "neg_line": [],
        "pos_line": [
            "+from tensorflow.python.client import timeline",
            "+"
        ],
        "core_change": "+from tensorflow.python.client import timeline +",
        "core_API": "RunOptions"
    },
    {
        "commit_hash": "a0f79318c5b16ef5d14bbff8840bc0dc045d078d",
        "index": "0e2aae7..de5d0f6 100644",
        "commit_message": "Culling to frustrum bug fix\n\nSummary:\nWhen `z_clip_value = None` and faces are outside the view frustum the shape of one of the tensors in `clip.py` is incorrect.\n\n`faces_num_clipped_verts` should be (F,) but it was (F,3).  Added a new test to ensure this case is handled.\n\nReviewed By: bottler\n\nDifferential Revision: D29051282\n\nfbshipit-source-id: 5f4172ba4d4a75d928404dde9abf48aef18c68bd\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def clip_faces(",
            "# (F) dim tensor containing the number of clipped vertices in each triangle",
            "faces_num_clipped_verts = faces_clipped_verts.sum(1)",
            "else:",
            "-        faces_num_clipped_verts = torch.zeros([F, 3], device=device)",
            "+        faces_num_clipped_verts = torch.zeros([F], device=device)",
            "",
            "# If no triangles need to be clipped or culled, avoid unnecessary computation",
            "# and return early"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=3))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 2015,
        "neg_line": [
            "-faces_num_clipped_verts = torch.zeros([F, 3], device=device)"
        ],
        "pos_line": [
            "+faces_num_clipped_verts = torch.zeros([F], device=device)"
        ],
        "core_change": "-faces_num_clipped_verts = torch.zeros([F, 3], device=device) +faces_num_clipped_verts = torch.zeros([F], device=device)",
        "core_API": "sum"
    },
    {
        "commit_hash": "a00fe9f1741ba7452b886b4d2fbd79eb9acd6a17",
        "index": "567236138d..0ae460f737 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from . import backend_version",
            "",
            "",
            "@with_unsupported_dtypes({\"1.11.0 and below\": (\"float16\",)}, backend_version)",
            "-def relu(",
            "-    x: torch.Tensor,",
            "-    /,",
            "-    *,",
            "-    out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:",
            "+def relu(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "return torch.relu(x)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 2019,
        "neg_line": [
            "-def relu(",
            "-x: torch.Tensor,",
            "-/,",
            "-*,",
            "-out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def relu(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:"
        ],
        "core_change": "-def relu( -x: torch.Tensor, -/, -*, -out: Optional[torch.Tensor] = None -) -> torch.Tensor: +def relu(x: torch.Tensor, /, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
        "core_API": "relu"
    },
    {
        "commit_hash": "4fc7084875ac672980c4e0207fa325c8c418a88e",
        "index": "57dd424a..2fdf70ce 100644",
        "commit_message": "Fix a dimension bug in Transform2d (#2144)\n\nThe dimension does not match when `inner_dim` is not equal to `in_channels`.\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Transformer2DModel(ModelMixin, ConfigMixin):",
            "if self.is_input_continuous:",
            "# TODO: should use out_channels for continous projections",
            "if use_linear_projection:",
            "-                self.proj_out = nn.Linear(in_channels, inner_dim)",
            "+                self.proj_out = nn.Linear(inner_dim, in_channels)",
            "else:",
            "self.proj_out = nn.Conv2d(inner_dim, in_channels, kernel_size=1, stride=1, padding=0)",
            "elif self.is_input_vectorized:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=identifier, text=in_channels), node=ASTNode(type=argument_list), position=3)",
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=argument_list), position=4)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 2023,
        "neg_line": [
            "-self.proj_out = nn.Linear(in_channels, inner_dim)"
        ],
        "pos_line": [
            "+self.proj_out = nn.Linear(inner_dim, in_channels)"
        ],
        "core_change": "-self.proj_out = nn.Linear(in_channels, inner_dim) +self.proj_out = nn.Linear(inner_dim, in_channels)",
        "core_API": "Linear"
    },
    {
        "commit_hash": "17b14ca8fd3b23985effe07082329308678fa1a3",
        "index": "d175cab37..3860041d2 100644",
        "commit_message": "fix self.init_hidden never used\n\n",
        "file": "tutorials.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "autograd",
        "change": [
            "class BiLSTM_CRF(nn.Module):",
            "def _get_lstm_features(self, sentence):",
            "self.hidden = self.init_hidden()",
            "embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)",
            "-        lstm_out, self.hidden = self.lstm(embeds)",
            "+        lstm_out, self.hidden = self.lstm(embeds, self.hidden)",
            "lstm_out = lstm_out.view(len(sentence), self.hidden_dim)",
            "lstm_feats = self.hidden2tag(lstm_out)",
            "return lstm_feats"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1583374)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=1583375)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1583376)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1583377)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'hidden'), position=2, insert_id=1583378)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2024,
        "neg_line": [
            "-lstm_out, self.hidden = self.lstm(embeds)"
        ],
        "pos_line": [
            "+lstm_out, self.hidden = self.lstm(embeds, self.hidden)"
        ],
        "core_change": "-lstm_out, self.hidden = self.lstm(embeds) +lstm_out, self.hidden = self.lstm(embeds, self.hidden)",
        "core_API": "init_hidden"
    },
    {
        "commit_hash": "f4781a0b27ffb3ea61ecd25b0b87305e0960304e",
        "index": "5e626937..d9c4967b 100644",
        "commit_message": "update expected results of slow tests (#268)\n\n* update expected results of slow tests\n\n* relax sum and mean tests\n\n* Print shapes when reporting exception\n\n* formatting\n\n* fix sentence\n\n* relax test_stable_diffusion_fast_ddim for gpu fp16\n\n* relax flakey tests on GPU\n\n* added comment on large tolerences\n\n* black\n\n* format\n\n* set scheduler seed\n\n* added generator\n\n* use np.isclose\n\n* set num_inference_steps to 50\n\n* fix dep. warning\n\n* update expected_slice\n\n* preprocess if image\n\n* updated expected results\n\n* updated expected from CI\n\n* pass generator to VAE\n\n* undo change back to orig\n\n* use orignal\n\n* revert back the expected on cpu\n\n* revert back values for CPU\n\n* more undo\n\n* update result after using gen\n\n* update mean\n\n* set generator for mps\n\n* update expected on CI server\n\n* undo\n\n* use new seed every time\n\n* cpu manual seed\n\n* reduce num_inference_steps\n\n* style\n\n* use generator for randn\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class UNetLDMModelTests(ModelTesterMixin, unittest.TestCase):",
            "expected_output_slice = torch.tensor([-13.3258, -20.1100, -15.9873, -17.6617, -23.0596, -17.9419, -13.3675, -16.1889, -12.3800])",
            "# fmt: on",
            "",
            "-        self.assertTrue(torch.allclose(output_slice, expected_output_slice, atol=1e-3))",
            "+        self.assertTrue(torch.allclose(output_slice, expected_output_slice, rtol=1e-3))",
            "",
            "",
            "#    TODO(Patrick) - Re-add this test after having cleaned up LDM"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=atol), value='rtol')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2028,
        "neg_line": [
            "-self.assertTrue(torch.allclose(output_slice, expected_output_slice, atol=1e-3))"
        ],
        "pos_line": [
            "+self.assertTrue(torch.allclose(output_slice, expected_output_slice, rtol=1e-3))"
        ],
        "core_change": "-self.assertTrue(torch.allclose(output_slice, expected_output_slice, atol=1e-3)) +self.assertTrue(torch.allclose(output_slice, expected_output_slice, rtol=1e-3))",
        "core_API": "tensor"
    },
    {
        "commit_hash": "f2191b0cdf4305ae3a5ad2b1e404f99764a1a7c6",
        "index": "52541a86f..8533fdf48 100644",
        "commit_message": "fix for pyTorch 1.2 (#549)\n\n* min pytorch 1.2\n\n* fix IterableDataset\n\n* upgrade torchvision\n\n* fix msg\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TrainerDataLoadingMixin(object):",
            "self.get_val_dataloaders()",
            "",
            "# support IterableDataset for train data",
            "-        self.is_iterable_train_dataloader = isinstance(self.get_train_dataloader().dataset, IterableDataset)",
            "+        self.is_iterable_train_dataloader = (",
            "+            EXIST_ITER_DATASET and isinstance(self.get_train_dataloader().dataset, IterableDataset))",
            "if self.is_iterable_train_dataloader and not isinstance(self.val_check_interval, int):",
            "m = '''",
            "When using an iterableDataset for train_dataloader,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('parenthesized_expression', None), position=2, insert_id=587656)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=587657)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('boolean_operator', None), position=1, insert_id=587658)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=587659)",
            "Insert(target_node=IN(type=boolean_operator), node=('identifier', 'EXIST_ITER_DATASET'), position=0, insert_id=587660)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=587661)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=2)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 2029,
        "neg_line": [
            "-self.is_iterable_train_dataloader = isinstance(self.get_train_dataloader().dataset, IterableDataset)"
        ],
        "pos_line": [
            "+self.is_iterable_train_dataloader = (",
            "+EXIST_ITER_DATASET and isinstance(self.get_train_dataloader().dataset, IterableDataset))"
        ],
        "core_change": "-self.is_iterable_train_dataloader = isinstance(self.get_train_dataloader().dataset, IterableDataset) +self.is_iterable_train_dataloader = ( +EXIST_ITER_DATASET and isinstance(self.get_train_dataloader().dataset, IterableDataset))",
        "core_API": "get_val_dataloaders"
    },
    {
        "commit_hash": "91c9691c309e7c256ffdbdd429f0a98903deaaf0",
        "index": "e7503f6..1db3bd6 100644",
        "commit_message": "  fix tensorflow logging\n\n",
        "file": "spleeter.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def configure_logger(verbose: bool) -> None:",
            "verbose (bool):",
            "`True` to use verbose logger, `False` otherwise.",
            "\"\"\"",
            "-    tf_logger = tf_logging.get_logger()",
            "+    tf_logger = tf.get_logger()",
            "tf_logger.handlers = [handler]",
            "if verbose:",
            "-        environ['TF_CPP_MIN_LOG_LEVEL'] = '1'",
            "tf_logging.set_verbosity(tf_logging.INFO)",
            "logger.setLevel(logging.DEBUG)",
            "else:",
            "warnings.filterwarnings('ignore')",
            "-        environ['TF_CPP_MIN_LOG_LEVEL'] = '3'",
            "tf_logging.set_verbosity(tf_logging.ERROR)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('block', ''), position=3, insert_id=2204437)",
            "Update(target_node=ASTNode(type=identifier, text=tf_logging), value='tf')",
            "Delete(target_node=ASTNode(type=identifier, text=environ))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=string, text='TF_CPP_MIN_LOG_LEVEL'))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text='1'))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=identifier, text=environ))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=string, text='TF_CPP_MIN_LOG_LEVEL'))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text='3'))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 2033,
        "neg_line": [
            "-tf_logger = tf_logging.get_logger()",
            "-environ['TF_CPP_MIN_LOG_LEVEL'] = '1'",
            "-environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
        ],
        "pos_line": [
            "+tf_logger = tf.get_logger()"
        ],
        "core_change": "-tf_logger = tf_logging.get_logger() +tf_logger = tf.get_logger() -environ['TF_CPP_MIN_LOG_LEVEL'] = '1' -environ['TF_CPP_MIN_LOG_LEVEL'] = '3'",
        "core_API": "get_logger"
    },
    {
        "commit_hash": "a22de7ec24ba873f699e989533122f36b4f6f693",
        "index": "323ad14..3630ca9 100644",
        "commit_message": "fix to model variable scope\n",
        "file": "bert.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class BertModel(object):",
            "if token_type_ids is None:",
            "token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)",
            "",
            "-    with tf.variable_scope(\"bert\", scope):",
            "+    with tf.variable_scope(scope, \"bert\"):",
            "with tf.variable_scope(\"embeddings\"):",
            "# Perform embedding lookup on the word ids.",
            "(self.embedding_output, self.embedding_table) = embedding_lookup("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=string, text=\"bert\"), node=ASTNode(type=argument_list), position=3)",
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=argument_list), position=4)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 2035,
        "neg_line": [
            "-with tf.variable_scope(\"bert\", scope):"
        ],
        "pos_line": [
            "+with tf.variable_scope(scope, \"bert\"):"
        ],
        "core_change": "-with tf.variable_scope(\"bert\", scope): +with tf.variable_scope(scope, \"bert\"):",
        "core_API": "zeros"
    },
    {
        "commit_hash": "9cb71ababdabd4fbbb60447a09b0707cb5b4907a",
        "index": "9274f99..7a94008 100644",
        "commit_message": "fix mnist example\n\n",
        "file": "examples.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def test(epoch):",
            "output = model(batch_data)",
            "test_loss += criterion(output, batch_targets)",
            "pred = output.data.max(1)[1]",
            "-        correct += pred.long().eq(batch_targets.data.long()).sum()",
            "+        correct += pred.long().eq(batch_targets.data.long()).cpu().sum()",
            "",
            "test_loss = test_loss.data[0]",
            "test_loss /= (test_data.size(0) / TEST_BATCH_SIZE) # criterion averages over batch size"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1347559)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1347560)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1347561)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cpu'), position=2, insert_id=1347562)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1347563)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1347564)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 2036,
        "neg_line": [
            "-correct += pred.long().eq(batch_targets.data.long()).sum()"
        ],
        "pos_line": [
            "+correct += pred.long().eq(batch_targets.data.long()).cpu().sum()"
        ],
        "core_change": "-correct += pred.long().eq(batch_targets.data.long()).sum() +correct += pred.long().eq(batch_targets.data.long()).cpu().sum()",
        "core_API": "max"
    },
    {
        "commit_hash": "9b0b579e876cd7d3ee115a8da94b5ffb182fcf3f",
        "index": "2f116a62..20a4ff96 100644",
        "commit_message": "Adds model configs to ludwig.datasets (#2540)\n\n* Adds README and stub for reading dataset configs.\n\n* Adds __init__.py for configs, moves circular import into function scope in ludwig/datasets/__init__.py\n\n* Print config files in datasets folder.\n\n* First pass at automatic archive extraction.\n\n* Implemented downloading and extract.\n\n* Refactor DatasetConfig into its own file.\n\n* Fixed bugs downloading kaggle dataset.\n\n* Makes registry store dataset instances, not classes. Also comments out import_submodules for testing.\n\n* Typo fix.\n\n* Only pass data files on to load_unprocessed_dataframe, symlink directories.\n\n* Downloading dataset files into existing directory if exists.\n\n* Refactor: make datasets fully config-first, lazy load dataset loaders.\n\n* Implemented agnews custom loader.\n\n* Implements train/validation/test split by files, and globbing support\n\n* Adds _glob_multiple\n\n* Adds adult_census_income, agnews, allstate_claims_severity.\n\n* Implements sha256 verification, adds more datasets up to creditcard_fraud.\n\n* Adds checksums, dbpedia, electricity\n\n* Fixes gzip file name returned as string not list, adds up to forest_cover dataset.\n\n* Adds datasets up to reuters_r8\n\n* Adds all datasets which don't require a custom class.\n\n* Restore dataset import behavior by implementing module __getattr__\n\n* Adds KDD datasets.\n\n* Adds ieee_fraud.\n\n* Adds imbalanced_insurance, insurance_lite.\n\n* Adds mnist.\n\n* Completes implementation of all of the built-in datasets.\n\n* Made cache_dir optional, read from environment variable if set.\n\n* Upgrades datasets tests.\n\n* Adds test for new dataset config API.  Also adds scripts for dataset link checking.\n\n* Fixes loading allstate claims severity dataset.\n\n* Use @lru_cache(1), @cache not supported in python < 3.9\n\n* Deletes dataset registry, updates automl test utils\n\n* Fix imports of datasets API.\n\n* Adds more detail to sha256: docstring and basic README\n\n* Copy-paste link oops.\n\n* Fixes handling of nested archive types like .tar.bz  Also adds a LUDWIG_CACHE and export to the README\n\n* Adds link for twitter bots.\n\n* Fix order of splits in README.md\n\n* typo\n\n* Adds verify as a phase in doc string.\n\n* Support .pqt, .pq extensions for parquet.\n\n* Handle nested archives with longer file extensions like .csv.zip\n\n* Handle nested .gz types properly too.  Check all extensions with .endswith\n\n* Handle all archive types with .endswith\n\n* Update ludwig/datasets/loaders/split_loaders.py\n\nCo-authored-by: Joppe Geluykens <joppe@rvrie.com>\n\n* Adds explanation for export, fixes preserve_paths (should be relative to processed_dataset_dir)\n\n* Resolve preserved paths relative to raw dataset dir before move.\n\n* Catch runtime exception from extracting sub-archives.\n\n* Started adding info to README about dataset model configs.\n\n* Adds method to get model configs for datasets.\n\n* Adds mnist, titanic examples as default configs.\n\n* Export dataset before training.\n\n* Adds multiprocessing version of train_all_model_configs, and adds a few configs mosttly from automl experiments.\n\n* Adds a few more configs, removes AG news, training is too slow.\n\n* Default to only 4 processes due to memory constraints.\n\n* Visualize learning curves.\n\n* Started documenting API functions in readme.\n\n* Adds test for model configs API, updates tests to mock protected _load_dataset_config\n\n* Clear dataset cache after testing with mock datasets.\n\n* Adds best configs, improved README\n\n* higgs_best consistent formatting.\n\n* Update ludwig/datasets/README.md\n\nCo-authored-by: abidwael <103003638+abidwael@users.noreply.github.com>\n\n* Adds model commit hash to results.\n\n* Adds MSE, MAE to metric list.\n\n* Don't printout ludwig commit.\n\n* Increase display width to show more columns in print output.\n\n* Fix error in get_commit_hash\n\nCo-authored-by: Daniel Treiman <daniel@predibase.com>\nCo-authored-by: Joppe Geluykens <joppe@rvrie.com>\nCo-authored-by: abidwael <103003638+abidwael@users.noreply.github.com>\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def test_get_config_and_load(tmpdir):",
            "",
            "",
            "def test_get_config_kaggle(tmpdir):",
            "-    twitter_bots_config = ludwig.datasets.get_dataset_config(\"twitter_bots\")",
            "+    twitter_bots_config = ludwig.datasets._get_dataset_config(\"twitter_bots\")",
            "assert isinstance(twitter_bots_config, DatasetConfig)",
            "",
            "twitter_bots_dataset = ludwig.datasets.get_dataset(\"twitter_bots\", cache_dir=tmpdir)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=get_dataset_config), value='_get_dataset_config')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2039,
        "neg_line": [
            "-twitter_bots_config = ludwig.datasets.get_dataset_config(\"twitter_bots\")"
        ],
        "pos_line": [
            "+twitter_bots_config = ludwig.datasets._get_dataset_config(\"twitter_bots\")"
        ],
        "core_change": "-twitter_bots_config = ludwig.datasets.get_dataset_config(\"twitter_bots\") +twitter_bots_config = ludwig.datasets._get_dataset_config(\"twitter_bots\")",
        "core_API": "get_dataset_config"
    },
    {
        "commit_hash": "292186a3e7e1a819aa591901591673639c752157",
        "index": "e2b7a0fa9..dc52472e8 100644",
        "commit_message": "Adding LM Head to Transfo-XL and first step to fixing problem with Adaptive Embeddings in TransfoXL (#3286)\n\n* first commit\n\n* work in progress\n\n* make language generation task pass\n\n* update to working version for LM\n\n* delete print\n\n* remove dead code\n\n* make style\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class TFModelTesterMixin:",
            "",
            "for model_class in self.all_model_classes:",
            "model = model_class(config)",
            "-            assert isinstance(model.get_input_embeddings(), tf.keras.layers.Layer)",
            "+            assert isinstance(model.get_input_embeddings(), (tf.keras.layers.Layer, TFAdaptiveEmbedding))",
            "x = model.get_output_embeddings()",
            "assert x is None or isinstance(x, tf.keras.layers.Layer)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=3, insert_id=2644653)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=2644654)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2644655)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2644656)",
            "Insert(target_node=IN(type=tuple), node=('identifier', 'TFAdaptiveEmbedding'), position=3, insert_id=2644657)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=), text=)), position=4)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 2041,
        "neg_line": [
            "-assert isinstance(model.get_input_embeddings(), tf.keras.layers.Layer)"
        ],
        "pos_line": [
            "+assert isinstance(model.get_input_embeddings(), (tf.keras.layers.Layer, TFAdaptiveEmbedding))"
        ],
        "core_change": "-assert isinstance(model.get_input_embeddings(), tf.keras.layers.Layer) +assert isinstance(model.get_input_embeddings(), (tf.keras.layers.Layer, TFAdaptiveEmbedding))",
        "core_API": "get_input_embeddings"
    },
    {
        "commit_hash": "1e8f6ad6fb2fc8f51fe661141db04e4c67b015e0",
        "index": "9e84f67..2193746 100644",
        "commit_message": "fix document for add_image (#311)\n\n* organize functions, move the image processing code outside make_np.\n\n* made add_image flexible\n\n* change video API\n\n* fixes #286. fix image_with_boxes\n\n* fixed #285\n\n* closes #208 closes #210\n\n* fix lint\n\n",
        "file": "tensorboardX.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "for n_iter in range(100):",
            "if n_iter % 10 == 0:",
            "x = vutils.make_grid(x, normalize=True, scale_each=True)",
            "writer.add_image('Image', x, n_iter)  # Tensor",
            "-        writer.add_image_with_boxes('imagebox', x, torch.Tensor([[10, 10, 40, 40]]), n_iter)",
            "+        writer.add_image_with_boxes('imagebox', x, torch.Tensor([[10, 10, 40, 40], [40, 40, 60, 60]]), n_iter)",
            "x = torch.zeros(sample_rate * 2)",
            "for i in range(x.size(0)):",
            "# sound amplitude should in [-1, 1]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=2, insert_id=1154807)",
            "Insert(target_node=ASTNode(type=list), node=('list', None), position=3, insert_id=1154808)",
            "Insert(target_node=ASTNode(type=list), node=(']', ']'), position=4, insert_id=1154809)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=1154810)",
            "Insert(target_node=IN(type=list), node=('integer', '40'), position=1, insert_id=1154811)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=1154812)",
            "Insert(target_node=IN(type=list), node=('integer', '40'), position=3, insert_id=1154813)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=4, insert_id=1154814)",
            "Insert(target_node=IN(type=list), node=('integer', '60'), position=5, insert_id=1154815)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=6, insert_id=1154816)",
            "Insert(target_node=IN(type=list), node=('integer', '60'), position=7, insert_id=1154817)",
            "Move(target_node=IN(type=list), node=ASTNode(type=], text=]), position=8)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 2042,
        "neg_line": [
            "-writer.add_image_with_boxes('imagebox', x, torch.Tensor([[10, 10, 40, 40]]), n_iter)"
        ],
        "pos_line": [
            "+writer.add_image_with_boxes('imagebox', x, torch.Tensor([[10, 10, 40, 40], [40, 40, 60, 60]]), n_iter)"
        ],
        "core_change": "-writer.add_image_with_boxes('imagebox', x, torch.Tensor([[10, 10, 40, 40]]), n_iter) +writer.add_image_with_boxes('imagebox', x, torch.Tensor([[10, 10, 40, 40], [40, 40, 60, 60]]), n_iter)",
        "core_API": "make_grid"
    },
    {
        "commit_hash": "1e420bf8c39e886a2373fff315f59fd5d4503026",
        "index": "a740eed14..428b934c9 100644",
        "commit_message": "Fix logging for pytorch>=1.8\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AbsTask(ABC):",
            "f\":{distributed_option.dist_rank}/{distributed_option.dist_world_size}]\"",
            "f\" %(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",",
            ")",
            "+        # Invoking torch.distributed.init_process_group",
            "+        distributed_option.init_torch_distributed()",
            "",
            "# 1. Set random-seed",
            "set_all_random_seed(args.seed)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=140575)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', ''), position=5, insert_id=140576)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=140577)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=140578)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=140579)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'distributed_option'), position=0, insert_id=140580)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=140581)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'init_torch_distributed'), position=2, insert_id=140582)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=140583)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=140584)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 10,
        "number": 2043,
        "neg_line": [],
        "pos_line": [
            "+# Invoking torch.distributed.init_process_group",
            "+distributed_option.init_torch_distributed()"
        ],
        "core_change": "+# Invoking torch.distributed.init_process_group +distributed_option.init_torch_distributed()",
        "core_API": "init_torch_distributed"
    },
    {
        "commit_hash": "023da9a7dbb7dab77a36c246b8244e91415decc6",
        "index": "6d7aa153..3f9f9196 100644",
        "commit_message": "[Feat] 3D volumetric crop implementation (#689)\n\n* Fixed scale error\n\n* Initiated crop 3d\n\n* Added crop3d\n\n* Updated docs\n\n* Added RandomCrop3D and CenterCrop3D.\n\n* Fixed linting issue.\n\n* Fixed mypy check\n\n* Added RandomPerspective3D\n\n* Updated docs\n\n* Renaming 3d operations and added kornia.geometry.crop module.\n\n* Fixed python3.6 tests.\n\n* Updated homography_warper docstring\n\n* Fixed import error\n\n* Moved crop module under transform\n\n* Added tests\n\n* apply stale robot only to bug tags\n\n* Enabled JIT for 3D crops\n\n* Enabled JIT for 2D crop\n\n* Added tests for perspective_transform_3d\n\n* Added missing files\n\n* Added homography tests\n\n* Refined docs\n\n* Allowed only BCHW, BCDHW for cropping functions\n\n* Updated tests\n\n* lint checking\n\n* Updated Augmentation tests\n\n* Fixed float64 error\n\n* Fixed inconsistant merging\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def random_crop_generator(",
            "size = torch.tensor(size).repeat(batch_size, 1)",
            "assert size.shape == torch.Size([batch_size, 2]), \\",
            "f\"If `size` is a tensor, it must be shaped as (B, 2). Got {size.shape}.\"",
            "+    size = size.long()",
            "",
            "x_diff = input_size[1] - size[:, 1] + 1",
            "y_diff = input_size[0] - size[:, 0] + 1"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=435301)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=435302)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'size'), position=0, insert_id=435303)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=435304)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=435305)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=435306)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=435307)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'size'), position=0, insert_id=435308)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=435309)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'long'), position=2, insert_id=435310)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=435311)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=435312)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 12,
        "number": 2044,
        "neg_line": [],
        "pos_line": [
            "+size = size.long()"
        ],
        "core_change": "+size = size.long()",
        "core_API": "tensor"
    },
    {
        "commit_hash": "2aa760b101fe15a7d3e4e511bbf6cc0969e00027",
        "index": "bcebc2c8..d8783d1d 100644",
        "commit_message": "fix shape comparison when reusing placeholders. fix #1329\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def build_or_reuse_placeholder(tensor_spec):",
            "assert \"Placeholder\" in tensor.op.type, \"Tensor {} exists but is not a placeholder!\".format(name)",
            "assert tensor_spec.is_compatible_with(tensor), \\",
            "\"Tensor {} exists but is not compatible with the signature!\".format(tensor)",
            "-        if tensor.shape == tensor_spec.shape:",
            "+        if tensor.shape.as_list() == tensor_spec.shape.as_list():",
            "# It might be desirable to use a placeholder of a different shape in some tower",
            "# (e.g., a less specific shape)",
            "+",
            "+            # Comparing `tensor.shape` directly doesn't work, because",
            "+            # tensorflow thinks `tf.Dimension(None)` and `tf.Dimension(None)` are not equal.",
            "return tensor",
            "except KeyError:",
            "pass"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('call', None), position=0, insert_id=2273990)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('call', None), position=3, insert_id=2273991)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2273992)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2273993)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2273994)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2273995)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2273996)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'as_list'), position=2, insert_id=2273997)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2273998)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2273999)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2274000)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'as_list'), position=2, insert_id=2274001)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2274002)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2274003)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 2048,
        "neg_line": [
            "-if tensor.shape == tensor_spec.shape:"
        ],
        "pos_line": [
            "+if tensor.shape.as_list() == tensor_spec.shape.as_list():",
            "+",
            "+# Comparing `tensor.shape` directly doesn't work, because",
            "+# tensorflow thinks `tf.Dimension(None)` and `tf.Dimension(None)` are not equal."
        ],
        "core_change": "-if tensor.shape == tensor_spec.shape: +if tensor.shape.as_list() == tensor_spec.shape.as_list(): + +# Comparing `tensor.shape` directly doesn't work, because +# tensorflow thinks `tf.Dimension(None)` and `tf.Dimension(None)` are not equal.",
        "core_API": "is_compatible_with"
    },
    {
        "commit_hash": "e26c77a5caff9b47404eca5ff6f50e8c5106719c",
        "index": "f9341efc..ade32c22 100644",
        "commit_message": "fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class InstanceNormalization(Layer):",
            "",
            "reciprocal_stddev = tf.math.rsqrt(x=tf.maximum(x=variance, y=epsilon))",
            "",
            "-        x = (x - mean) * reciprocal_stddev",
            "+        x = (x - tf.stop_gradient(input=mean)) * tf.stop_gradient(input=reciprocal_stddev)",
            "",
            "return x"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=2229822)",
            "Insert(target_node=ASTNode(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=2229823)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2229824)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2229825)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=2229826)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2229827)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2229828)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'stop_gradient'), position=2, insert_id=2229829)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2229830)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2229831)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2229832)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2229833)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2229834)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'input'), position=0, insert_id=2229835)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2229836)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=reciprocal_stddev), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2229837)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2229838)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'stop_gradient'), position=2, insert_id=2229839)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2229840)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2229841)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'input'), position=0, insert_id=2229842)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2229843)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=mean), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 2049,
        "neg_line": [
            "-x = (x - mean) * reciprocal_stddev"
        ],
        "pos_line": [
            "+x = (x - tf.stop_gradient(input=mean)) * tf.stop_gradient(input=reciprocal_stddev)"
        ],
        "core_change": "-x = (x - mean) * reciprocal_stddev +x = (x - tf.stop_gradient(input=mean)) * tf.stop_gradient(input=reciprocal_stddev)",
        "core_API": "rsqrt"
    },
    {
        "commit_hash": "5a5a492c1be81205d74ed555f0f128484d4f4139",
        "index": "8f50cebd..e1e710ca 100755",
        "commit_message": "fixed replay memory\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Replay(Queue):",
            "sequence_indices = tf.boolean_mask(",
            "tensor=sequence_indices, mask=tf.logical_not(x=terminal)",
            ")",
            "-        return self.retrieve_indices(indices=sequence_indices)",
            "",
            "# Retrieve sequence indices",
            "sequences = self.retrieve_indices(indices=sequence_indices)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=retrieve_indices))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=indices))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=sequence_indices))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 2051,
        "neg_line": [
            "-return self.retrieve_indices(indices=sequence_indices)"
        ],
        "pos_line": [],
        "core_change": "-return self.retrieve_indices(indices=sequence_indices)",
        "core_API": "boolean_mask"
    },
    {
        "commit_hash": "086c7f9ea8f0fde5c62e52289604ec5b178da207",
        "index": "889d54a2..2288fa63 100644",
        "commit_message": "Nightly integration tests (#1664)\n\n* [WIP] Nightly integration tests\n\n* initial SD tests\n\n* update SD slow tests\n\n* style\n\n* repaint\n\n* ImageVariations\n\n* style\n\n* finish imgvar\n\n* img2img tests\n\n* debug\n\n* inpaint 1.5\n\n* inpaint legacy\n\n* torch isn't happy about deterministic ops\n\n* allclose -> max diff for shorter logs\n\n* add SD2\n\n* debug\n\n* Update tests/pipelines/stable_diffusion_2/test_stable_diffusion.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* Update tests/pipelines/stable_diffusion/test_stable_diffusion.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix refs\n\n* Update src/diffusers/utils/testing_utils.py\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* fix refs\n\n* remove debug\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RepaintPipelineIntegrationTests(unittest.TestCase):",
            "scheduler = RePaintScheduler.from_pretrained(model_id)",
            "",
            "repaint = RePaintPipeline(unet=unet, scheduler=scheduler).to(torch_device)",
            "+        repaint.set_progress_bar_config(disable=None)",
            "+        repaint.enable_attention_slicing()",
            "",
            "generator = torch.Generator(device=torch_device).manual_seed(0)",
            "output = repaint("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=93320)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=93321)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=93322)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=93323)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=93324)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=93325)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=93326)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=93327)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'repaint'), position=0, insert_id=93328)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=93329)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'set_progress_bar_config'), position=2, insert_id=93330)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=93331)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=93332)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=93333)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'repaint'), position=0, insert_id=93334)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=93335)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'enable_attention_slicing'), position=2, insert_id=93336)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=93337)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=93338)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'disable'), position=0, insert_id=93339)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=93340)",
            "Insert(target_node=IN(type=keyword_argument), node=('none', 'None'), position=2, insert_id=93341)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 22,
        "number": 2054,
        "neg_line": [],
        "pos_line": [
            "+repaint.set_progress_bar_config(disable=None)",
            "+repaint.enable_attention_slicing()"
        ],
        "core_change": "+repaint.set_progress_bar_config(disable=None) +repaint.enable_attention_slicing()",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "a85216a729be99e4abe1f6ec5f81ad646f27d161",
        "index": "06eb6380..b6cb7fa7 100644",
        "commit_message": "fix bugs in vgg test, now the time for numpy2tensor is counted\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def train_step(x_batch, y_batch):",
            "",
            "# begin training",
            "for idx, data in enumerate(gen):",
            "-    x_batch = tf.convert_to_tensor(data[0])",
            "-    y_batch = tf.convert_to_tensor(data[1])",
            "-",
            "start_time = time.time()",
            "",
            "+    x_batch = tf.convert_to_tensor(data[0])",
            "+    y_batch = tf.convert_to_tensor(data[1])",
            "train_step(x_batch, y_batch)",
            "",
            "end_time = time.time()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('for_statement', None), position=1, insert_id=2256797)",
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=4)",
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=5)",
            "Move(target_node=IN(type=for_statement), node=ASTNode(type=for, text=for), position=0)",
            "Move(target_node=IN(type=for_statement), node=ASTNode(type=pattern_list), position=1)",
            "Move(target_node=IN(type=for_statement), node=ASTNode(type=in, text=in), position=2)",
            "Move(target_node=IN(type=for_statement), node=ASTNode(type=call), position=3)",
            "Move(target_node=IN(type=for_statement), node=ASTNode(type=:, text=:), position=4)",
            "Insert(target_node=IN(type=for_statement), node=('block', ''), position=5, insert_id=2256798)",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=for_statement))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 2058,
        "neg_line": [
            "-x_batch = tf.convert_to_tensor(data[0])",
            "-y_batch = tf.convert_to_tensor(data[1])",
            "-"
        ],
        "pos_line": [
            "+x_batch = tf.convert_to_tensor(data[0])",
            "+y_batch = tf.convert_to_tensor(data[1])"
        ],
        "core_change": "-x_batch = tf.convert_to_tensor(data[0]) -y_batch = tf.convert_to_tensor(data[1]) - +x_batch = tf.convert_to_tensor(data[0]) +y_batch = tf.convert_to_tensor(data[1])",
        "core_API": "convert_to_tensor"
    },
    {
        "commit_hash": "96e36a7c913e2433446ff410a4cf60041010a524",
        "index": "22061a1..79e8f24 100644",
        "commit_message": "New CSV Logger (#4148)\n\n* New CSV Logger\n\n* cleanup\n\n* move batch plots into Logger\n\n* rename comment\n\n* Remove total loss from progress bar\n\n* mloss :-1 bug fix\n\n* Update plot_results()\n\n* Update plot_results()\n\n* plot_results bug fix\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ComputeLoss:",
            "lcls *= self.hyp['cls']",
            "bs = tobj.shape[0]  # batch size",
            "",
            "-        loss = lbox + lobj + lcls",
            "-        return loss * bs, torch.cat((lbox, lobj, lcls, loss)).detach()",
            "+        return (lbox + lobj + lcls) * bs, torch.cat((lbox, lobj, lcls)).detach()",
            "",
            "def build_targets(self, p, targets):",
            "# Build targets for compute_loss(), input targets(image,class,x,y,w,h)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=3, insert_id=1297726)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=1297727)",
            "Insert(target_node=IN(type=return_statement), node=('expression_list', None), position=1, insert_id=1297728)",
            "Insert(target_node=IN(type=expression_list), node=('binary_operator', None), position=0, insert_id=1297729)",
            "Move(target_node=IN(type=expression_list), node=ASTNode(type=,, text=,), position=1)",
            "Move(target_node=IN(type=expression_list), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=binary_operator), node=('parenthesized_expression', None), position=0, insert_id=1297730)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=*, text=*), position=1)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=bs), position=2)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1297731)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=binary_operator), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1297732)",
            "Delete(target_node=ASTNode(type=identifier, text=loss))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=loss))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=loss))",
            "Delete(target_node=ASTNode(type=expression_list))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 23,
        "number": 2060,
        "neg_line": [
            "-loss = lbox + lobj + lcls",
            "-return loss * bs, torch.cat((lbox, lobj, lcls, loss)).detach()"
        ],
        "pos_line": [
            "+return (lbox + lobj + lcls) * bs, torch.cat((lbox, lobj, lcls)).detach()"
        ],
        "core_change": "-loss = lbox + lobj + lcls -return loss * bs, torch.cat((lbox, lobj, lcls, loss)).detach() +return (lbox + lobj + lcls) * bs, torch.cat((lbox, lobj, lcls)).detach()",
        "core_API": "cat"
    },
    {
        "commit_hash": "65d1bca5bf1ae3d3b0f1e6c6201ec0340d2aa1cd",
        "index": "265d6ff..4c82245 100644",
        "commit_message": "Fix shapes in conv1d for when input dim == output dim\n\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def conv1d(x, scope, nf, *, w_init_stdev=0.02, params=None, scale=False):",
            "b = mtf.get_variable(x.mesh, 'b', [nf], initializer=tf.constant_initializer(0, dtype=tf.bfloat16), dtype=dt)",
            "# NWC",
            "b = mtf.reshape(b, [singletona, singletonb, nf])",
            "+        b = mtf.broadcast(b, c.shape)",
            "",
            "c += b",
            "return c"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1942923)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1942924)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'b'), position=0, insert_id=1942925)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1942926)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1942927)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1942928)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1942929)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'mtf'), position=0, insert_id=1942930)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1942931)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'broadcast'), position=2, insert_id=1942932)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1942933)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'b'), position=1, insert_id=1942934)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1942935)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=1942936)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1942937)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'c'), position=0, insert_id=1942938)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1942939)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'shape'), position=2, insert_id=1942940)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 2064,
        "neg_line": [],
        "pos_line": [
            "+b = mtf.broadcast(b, c.shape)"
        ],
        "core_change": "+b = mtf.broadcast(b, c.shape)",
        "core_API": "get_variable"
    },
    {
        "commit_hash": "a16fe2b4f917b6b7a6f81c63832429740ff02311",
        "index": "5a525476..4a350541 100644",
        "commit_message": "[Enhance] pytorch nightly fixes (#861)\n\n* Fixed Uniform bug for validation and added extra dimensions\n\n* Fixed geometry tests\n\n* Fixed beta distribution bug\n\n* Fixed linting\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestSpatialSoftArgmax2d:",
            "std = torch.tensor([1.0, 1.0], device=device, dtype=dtype)",
            "",
            "hm = kornia.geometry.dsnt.spatial_softmax2d(input)",
            "-        assert_allclose(hm.sum(-1).sum(-1), torch.tensor(1.0, device=device, dtype=dtype), atol=1e-4, rtol=1e-4)",
            "+        assert_allclose(",
            "+            hm.sum(-1).sum(-1), torch.tensor([[1.0, 1.0]], device=device, dtype=dtype), atol=1e-4, rtol=1e-4)",
            "",
            "pred = kornia.geometry.dsnt.spatial_expectation2d(hm)",
            "assert_allclose("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('list', None), position=1, insert_id=430399)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=430400)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=430401)",
            "Insert(target_node=IN(type=list), node=('list', None), position=1, insert_id=430402)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=2, insert_id=430403)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=430404)",
            "Move(target_node=IN(type=list), node=ASTNode(type=float, text=1.0), position=1)",
            "Move(target_node=IN(type=list), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=IN(type=list), node=('float', '1.0'), position=3, insert_id=430405)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=430406)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 2065,
        "neg_line": [
            "-assert_allclose(hm.sum(-1).sum(-1), torch.tensor(1.0, device=device, dtype=dtype), atol=1e-4, rtol=1e-4)"
        ],
        "pos_line": [
            "+assert_allclose(",
            "+hm.sum(-1).sum(-1), torch.tensor([[1.0, 1.0]], device=device, dtype=dtype), atol=1e-4, rtol=1e-4)"
        ],
        "core_change": "-assert_allclose(hm.sum(-1).sum(-1), torch.tensor(1.0, device=device, dtype=dtype), atol=1e-4, rtol=1e-4) +assert_allclose( +hm.sum(-1).sum(-1), torch.tensor([[1.0, 1.0]], device=device, dtype=dtype), atol=1e-4, rtol=1e-4)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "41750a6cff55e401364568868d619747de3db037",
        "index": "f0df0c1ee..7a63c69f9 100644",
        "commit_message": "Fix typos\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def create_position_ids_from_input_ids(input_ids, padding_idx):",
            "\"\"\"",
            "# The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.",
            "mask = input_ids.ne(padding_idx).int()",
            "-    incremental_indicies = torch.cumsum(mask, dim=1).type_as(mask) * mask",
            "-    return incremental_indicies.long() + padding_idx",
            "+    incremental_indices = torch.cumsum(mask, dim=1).type_as(mask) * mask",
            "+    return incremental_indices.long() + padding_idx",
            "",
            "",
            "def prune_linear_layer(layer, index, dim=0):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=incremental_indicies), value='incremental_indices')",
            "Update(target_node=ASTNode(type=identifier, text=incremental_indicies), value='incremental_indices')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 2067,
        "neg_line": [
            "-incremental_indicies = torch.cumsum(mask, dim=1).type_as(mask) * mask",
            "-return incremental_indicies.long() + padding_idx"
        ],
        "pos_line": [
            "+incremental_indices = torch.cumsum(mask, dim=1).type_as(mask) * mask",
            "+return incremental_indices.long() + padding_idx"
        ],
        "core_change": "-incremental_indicies = torch.cumsum(mask, dim=1).type_as(mask) * mask -return incremental_indicies.long() + padding_idx +incremental_indices = torch.cumsum(mask, dim=1).type_as(mask) * mask +return incremental_indices.long() + padding_idx",
        "core_API": "ne"
    },
    {
        "commit_hash": "133ef0725253db83cfb82a4ed4003df76d189829",
        "index": "192efa0..75fa244 100644",
        "commit_message": "Fix resource not found error in TF AggregationHelper (#3499)\n\nSigned-off-by: Pei-Lun Liao <pliao@linkedin.com>\n",
        "file": "horovod.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class LocalGradientAggregationHelperEager:",
            "# is equal to 0.",
            "self.counter = tf.Variable(initial_value=0)",
            "",
            "-    @tf.function",
            "def compute_gradients(self, grads, vars):",
            "# On steps where allreduce happens, resulting_grads returns the allreduced",
            "# gradients, on other steps it returns the locally aggregated"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=function_definition), position=2)",
            "Delete(target_node=ASTNode(type=@, text=@))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=function))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=decorator))",
            "Delete(target_node=ASTNode(type=decorated_definition))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2072,
        "neg_line": [
            "-@tf.function"
        ],
        "pos_line": [],
        "core_change": "-@tf.function",
        "core_API": "Variable"
    },
    {
        "commit_hash": "a3e55be685fb52ac8c7caafd6835f521bcbcff9d",
        "index": "9975c8f3..9bc09f3e 100644",
        "commit_message": "fixed and improved shape handling for exploration\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class EpsilonAnneal(Exploration):",
            "return self.initial_epsilon + completed_ratio * (self.final_epsilon - self.initial_epsilon)",
            "",
            "pred = tf.logical_or(x=(timestep < self.start_timestep), y=(timestep > self.start_timestep + self.timesteps))",
            "-        return tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn)",
            "+        return tf.constant(dims=shape, value=tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2234356)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2234357)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2234358)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2234359)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'constant'), position=2, insert_id=2234360)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2234361)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2234362)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2234363)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2234364)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2234365)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dims'), position=0, insert_id=2234366)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2234367)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'shape'), position=2, insert_id=2234368)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'value'), position=0, insert_id=2234369)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2234370)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=call), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 2073,
        "neg_line": [
            "-return tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn)"
        ],
        "pos_line": [
            "+return tf.constant(dims=shape, value=tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))"
        ],
        "core_change": "-return tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn) +return tf.constant(dims=shape, value=tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))",
        "core_API": "logical_or"
    },
    {
        "commit_hash": "34f0dd6dbc139f9df4088e6475d4c8c350b5e513",
        "index": "97ad06c5..dfa36502 100755",
        "commit_message": "docs update and fix bug in ResNet-SE\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "l = Conv2D('conv3', l, ch_out * 4, 1)",
            "",
            "squeeze = GlobalAvgPooling('gap', l)",
            "-            squeeze = FullyConnected('fc1', squeeze, ch_out // 4, nl=tf.identity)",
            "+            squeeze = FullyConnected('fc1', squeeze, ch_out // 4, nl=tf.nn.relu)",
            "squeeze = FullyConnected('fc2', squeeze, ch_out * 4, nl=tf.nn.sigmoid)",
            "l = l * tf.reshape(squeeze, [-1, ch_out * 4, 1, 1])",
            "return l + resnet_shortcut(shortcut, ch_out * 4, stride)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2297100)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2297101)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'relu'), position=2, insert_id=2297102)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=identity), value='nn')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=identity), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 2074,
        "neg_line": [
            "-squeeze = FullyConnected('fc1', squeeze, ch_out // 4, nl=tf.identity)"
        ],
        "pos_line": [
            "+squeeze = FullyConnected('fc1', squeeze, ch_out // 4, nl=tf.nn.relu)"
        ],
        "core_change": "-squeeze = FullyConnected('fc1', squeeze, ch_out // 4, nl=tf.identity) +squeeze = FullyConnected('fc1', squeeze, ch_out // 4, nl=tf.nn.relu)",
        "core_API": "reshape"
    },
    {
        "commit_hash": "b6b14a76aff6c5eb87acd7b91b1075f4c5809077",
        "index": "53f7ca7c..91e53da3 100644",
        "commit_message": "Fix VITS stochastic duration predictor\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class StochasticDurationPredictor(nn.Module):",
            "",
            "flows = list(reversed(self.flows))",
            "flows = flows[:-2] + [flows[-1]]  # remove a useless vflow",
            "-        z = torch.rand(x.size(0), 2, x.size(2)).to(device=x.device, dtype=x.dtype) * noise_scale",
            "+        z = torch.randn(x.size(0), 2, x.size(2)).to(device=x.device, dtype=x.dtype) * noise_scale",
            "for flow in flows:",
            "z = torch.flip(z, [1])",
            "z = flow(z, x_mask, g=x, reverse=reverse)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=rand), value='randn')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2076,
        "neg_line": [
            "-z = torch.rand(x.size(0), 2, x.size(2)).to(device=x.device, dtype=x.dtype) * noise_scale"
        ],
        "pos_line": [
            "+z = torch.randn(x.size(0), 2, x.size(2)).to(device=x.device, dtype=x.dtype) * noise_scale"
        ],
        "core_change": "-z = torch.rand(x.size(0), 2, x.size(2)).to(device=x.device, dtype=x.dtype) * noise_scale +z = torch.randn(x.size(0), 2, x.size(2)).to(device=x.device, dtype=x.dtype) * noise_scale",
        "core_API": "rand"
    },
    {
        "commit_hash": "3ab32c31dd2a693682a34489d11605918c8dd6d4",
        "index": "8cdbf46..aa28834 100644",
        "commit_message": "fix eval2 dataset\n\n",
        "file": "deep-voice-conversion.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def train(logdir='logdir/train1', queue=True):",
            "with tf.Graph().as_default():",
            "eval1.eval(logdir=logdir, queue=False)",
            "",
            "+            writer.add_summary(summ, global_step=gs)",
            "+",
            "writer.close()",
            "coord.request_stop()",
            "coord.join(threads)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1920803)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1920804)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1920805)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1920806)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'writer'), position=0, insert_id=1920807)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1920808)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'add_summary'), position=2, insert_id=1920809)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1920810)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'summ'), position=1, insert_id=1920811)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1920812)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1920813)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1920814)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'global_step'), position=0, insert_id=1920815)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1920816)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'gs'), position=2, insert_id=1920817)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 15,
        "number": 2079,
        "neg_line": [],
        "pos_line": [
            "+writer.add_summary(summ, global_step=gs)",
            "+"
        ],
        "core_change": "+writer.add_summary(summ, global_step=gs) +",
        "core_API": "Graph"
    },
    {
        "commit_hash": "455c6390938a5c737fa63e78396cedae41e4e87e",
        "index": "ee039d982..aa7c2b23d 100644",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "BART_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"bart-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large/pytorch_model.bin\",",
            "-    \"bart-large-mnli\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-mnli/pytorch_model.bin\",",
            "-    \"bart-large-cnn\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-cnn/pytorch_model.bin\",",
            "-    \"bart-large-xsum\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-xsum/pytorch_model.bin\",",
            "-    \"mbart-large-en-ro\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/mbart-large-en-ro/pytorch_model.bin\",",
            "+    \"bart-large\": \"https://cdn.huggingface.co/facebook/bart-large/pytorch_model.bin\",",
            "+    \"bart-large-mnli\": \"https://cdn.huggingface.co/facebook/bart-large-mnli/pytorch_model.bin\",",
            "+    \"bart-large-cnn\": \"https://cdn.huggingface.co/facebook/bart-large-cnn/pytorch_model.bin\",",
            "+    \"bart-large-xsum\": \"https://cdn.huggingface.co/facebook/bart-large-xsum/pytorch_model.bin\",",
            "+    \"mbart-large-en-ro\": \"https://cdn.huggingface.co/facebook/mbart-large-en-ro/pytorch_model.bin\",",
            "}",
            "",
            "BART_START_DOCSTRING = r\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=dictionary), position=9)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=2, insert_id=2689753)",
            "Insert(target_node=ASTNode(type=pair), node=(':', ':'), position=1, insert_id=2689754)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large/pytorch_model.bin\"), value='\"https://cdn.huggingface.co/facebook/bart-large/pytorch_model.bin\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-mnli/pytorch_model.bin\"), value='\"https://cdn.huggingface.co/facebook/bart-large-mnli/pytorch_model.bin\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-cnn/pytorch_model.bin\"), value='\"https://cdn.huggingface.co/facebook/bart-large-cnn/pytorch_model.bin\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-xsum/pytorch_model.bin\"), value='\"https://cdn.huggingface.co/facebook/bart-large-xsum/pytorch_model.bin\"')",
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=:, text=:), position=1)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/mbart-large-en-ro/pytorch_model.bin\"), value='\"https://cdn.huggingface.co/facebook/mbart-large-en-ro/pytorch_model.bin\"')",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 5,
        "minus_line": 5,
        "AST_diff_line": 11,
        "number": 2080,
        "neg_line": [
            "-\"bart-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large/pytorch_model.bin\",",
            "-\"bart-large-mnli\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-mnli/pytorch_model.bin\",",
            "-\"bart-large-cnn\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-cnn/pytorch_model.bin\",",
            "-\"bart-large-xsum\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-xsum/pytorch_model.bin\",",
            "-\"mbart-large-en-ro\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/mbart-large-en-ro/pytorch_model.bin\","
        ],
        "pos_line": [
            "+\"bart-large\": \"https://cdn.huggingface.co/facebook/bart-large/pytorch_model.bin\",",
            "+\"bart-large-mnli\": \"https://cdn.huggingface.co/facebook/bart-large-mnli/pytorch_model.bin\",",
            "+\"bart-large-cnn\": \"https://cdn.huggingface.co/facebook/bart-large-cnn/pytorch_model.bin\",",
            "+\"bart-large-xsum\": \"https://cdn.huggingface.co/facebook/bart-large-xsum/pytorch_model.bin\",",
            "+\"mbart-large-en-ro\": \"https://cdn.huggingface.co/facebook/mbart-large-en-ro/pytorch_model.bin\","
        ],
        "core_change": "-\"bart-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large/pytorch_model.bin\", -\"bart-large-mnli\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-mnli/pytorch_model.bin\", -\"bart-large-cnn\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-cnn/pytorch_model.bin\", -\"bart-large-xsum\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-large-xsum/pytorch_model.bin\", -\"mbart-large-en-ro\": \"https://s3.amazonaws.com/models.huggingface.co/bert/facebook/mbart-large-en-ro/pytorch_model.bin\", +\"bart-large\": \"https://cdn.huggingface.co/facebook/bart-large/pytorch_model.bin\", +\"bart-large-mnli\": \"https://cdn.huggingface.co/facebook/bart-large-mnli/pytorch_model.bin\", +\"bart-large-cnn\": \"https://cdn.huggingface.co/facebook/bart-large-cnn/pytorch_model.bin\", +\"bart-large-xsum\": \"https://cdn.huggingface.co/facebook/bart-large-xsum/pytorch_model.bin\", +\"mbart-large-en-ro\": \"https://cdn.huggingface.co/facebook/mbart-large-en-ro/pytorch_model.bin\",",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "fb4236a19f9b85800655695ad5cd237557183c78",
        "index": "671e858..e08632a 100644",
        "commit_message": "Fixed PyTorch MNIST dataset (#2707)\n\nSigned-off-by: Travis Addair <tgaddair@gmail.com>\n",
        "file": "horovod.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "train_loader = torch.utils.data.DataLoader(",
            "train_dataset, batch_size=args.batch_size, sampler=train_sampler, **kwargs)",
            "",
            "test_dataset = \\",
            "-    datasets.MNIST('data-%d' % hvd.rank(), train=False, transform=transforms.Compose([",
            "+    datasets.MNIST(data_dir, train=False, transform=transforms.Compose([",
            "transforms.ToTensor(),",
            "transforms.Normalize((0.1307,), (0.3081,))",
            "]))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'data_dir'), position=1, insert_id=1794880)",
            "Delete(target_node=ASTNode(type=string, text='data-%d'))",
            "Delete(target_node=ASTNode(type=%, text=%))",
            "Delete(target_node=ASTNode(type=identifier, text=hvd))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=rank))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 2082,
        "neg_line": [
            "-datasets.MNIST('data-%d' % hvd.rank(), train=False, transform=transforms.Compose(["
        ],
        "pos_line": [
            "+datasets.MNIST(data_dir, train=False, transform=transforms.Compose(["
        ],
        "core_change": "-datasets.MNIST('data-%d' % hvd.rank(), train=False, transform=transforms.Compose([ +datasets.MNIST(data_dir, train=False, transform=transforms.Compose([",
        "core_API": "DataLoader"
    },
    {
        "commit_hash": "40f4f7eb31bcdb08cbc5d178ea5653563e0ea561",
        "index": "aebac28..8461a62 100644",
        "commit_message": "WIP Fix Avg&Max pool in CellStem0\n\n",
        "file": "pretrained-models.pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class CellStem0(nn.Module):",
            "self.comb_iter_0_left = TwoSeparables(42, 42, 5, 2, 2, bias=False)",
            "self.comb_iter_0_right = TwoSeparables(96, 42, 7, 2, 3, bias=False)",
            "",
            "-        self.comb_iter_1_left = nn.AvgPool2d(3, stride=2, padding=1)",
            "+        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)",
            "self.comb_iter_1_right = TwoSeparables(96, 42, 7, 2, 3, bias=False)",
            "",
            "-        self.comb_iter_2_left = nn.MaxPool2d(3, stride=2, padding=1)",
            "+        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1)",
            "self.comb_iter_2_right = TwoSeparables(96, 42, 5, 2, 2, bias=False)",
            "",
            "self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=3)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=3)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=attribute), position=0)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 2083,
        "neg_line": [
            "-self.comb_iter_1_left = nn.AvgPool2d(3, stride=2, padding=1)",
            "-self.comb_iter_2_left = nn.MaxPool2d(3, stride=2, padding=1)"
        ],
        "pos_line": [
            "+self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)",
            "+self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1)"
        ],
        "core_change": "-self.comb_iter_1_left = nn.AvgPool2d(3, stride=2, padding=1) +self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1) -self.comb_iter_2_left = nn.MaxPool2d(3, stride=2, padding=1) +self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1)",
        "core_API": "AvgPool2d"
    },
    {
        "commit_hash": "de4159a318f9a4f2f474733306c00ac326e643a7",
        "index": "a64f0f0ff..5f6f07a43 100644",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFGPT2PreTrainedModel(TFPreTrainedModel):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "-                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "+                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "}",
            "]",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2357976)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2357977)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2357978)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'TensorSpec'), position=2, insert_id=2357979)",
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=2357980)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=tuple), position=1)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2357981)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=1, insert_id=2357982)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2357983)",
            "Insert(target_node=IN(type=tuple), node=('none', 'None'), position=3, insert_id=2357984)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=4, insert_id=2357985)",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='int32')",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='int32')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=TensorSpec))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 24,
        "number": 2084,
        "neg_line": [
            "-\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "-\"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),"
        ],
        "pos_line": [
            "+\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "+\"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),"
        ],
        "core_change": "-\"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"), -\"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"), +\"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"), +\"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
        "core_API": "function"
    },
    {
        "commit_hash": "053826011260abe92a7f3dfbf33bbde21b7a91b0",
        "index": "8776213bf..8543cb227 100644",
        "commit_message": "various fixes\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TransducerDecoder(AbsDecoder):",
            "dec_states = self.create_batch_states(dec_states, [d[1] for d in done])",
            "",
            "if use_lm:",
            "-            lm_labels = torch.LongTensor([h.yseq[-1] for h in hyps], device=self.device)",
            "+            lm_labels = torch.LongTensor(",
            "+                [h.yseq[-1] for h in hyps], device=self.device",
            "+            ).view(final_batch, 1)",
            "",
            "return dec_out, dec_states, lm_labels"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=131108)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=131109)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=131110)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'view'), position=2, insert_id=131111)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=131112)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'final_batch'), position=1, insert_id=131113)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=131114)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=3, insert_id=131115)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=131116)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 2087,
        "neg_line": [
            "-lm_labels = torch.LongTensor([h.yseq[-1] for h in hyps], device=self.device)"
        ],
        "pos_line": [
            "+lm_labels = torch.LongTensor(",
            "+[h.yseq[-1] for h in hyps], device=self.device",
            "+).view(final_batch, 1)"
        ],
        "core_change": "-lm_labels = torch.LongTensor([h.yseq[-1] for h in hyps], device=self.device) +lm_labels = torch.LongTensor( +[h.yseq[-1] for h in hyps], device=self.device +).view(final_batch, 1)",
        "core_API": "create_batch_states"
    },
    {
        "commit_hash": "bed6408dcce8cf1b04e8dfa41f481500f40e47ca",
        "index": "fa911e5c0..e553365b5 100644",
        "commit_message": "branches, optim cosine fix\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "logger = logging.getLogger(__name__)",
            "def warmup_cosine(x, warmup=0.002):",
            "if x < warmup:",
            "return x/warmup",
            "-    return 0.5 * (1.0 + torch.cos(math.pi * x))",
            "+",
            "+    x_ = (x - warmup) / (1 - warmup)  # progress after warmup",
            "+    return 0.5 * (1. + math.cos(math.pi * x_))",
            "",
            "def warmup_constant(x, warmup=0.002):",
            "\"\"\" Linearly increases learning rate over `warmup`*`t_total` (as provided to BertAdam) training steps."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1249896)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1249897)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'x_'), position=0, insert_id=1249898)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1249899)",
            "Insert(target_node=IN(type=assignment), node=('binary_operator', None), position=2, insert_id=1249900)",
            "Insert(target_node=IN(type=binary_operator), node=('parenthesized_expression', None), position=0, insert_id=1249901)",
            "Insert(target_node=IN(type=binary_operator), node=('/', '/'), position=1, insert_id=1249902)",
            "Insert(target_node=IN(type=binary_operator), node=('parenthesized_expression', None), position=2, insert_id=1249903)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1249904)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('binary_operator', None), position=1, insert_id=1249905)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1249906)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1249907)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('binary_operator', None), position=1, insert_id=1249908)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1249909)",
            "Update(target_node=ASTNode(type=float, text=1.0), value='1.')",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'x'), position=0, insert_id=1249910)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=1249911)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'warmup'), position=2, insert_id=1249912)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=0, insert_id=1249913)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=1249914)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'warmup'), position=2, insert_id=1249915)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='math')",
            "Update(target_node=ASTNode(type=identifier, text=x), value='x_')"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 2093,
        "neg_line": [
            "-return 0.5 * (1.0 + torch.cos(math.pi * x))"
        ],
        "pos_line": [
            "+",
            "+x_ = (x - warmup) / (1 - warmup)  # progress after warmup",
            "+return 0.5 * (1. + math.cos(math.pi * x_))"
        ],
        "core_change": "-return 0.5 * (1.0 + torch.cos(math.pi * x)) + +x_ = (x - warmup) / (1 - warmup)  # progress after warmup +return 0.5 * (1. + math.cos(math.pi * x_))",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "f96d4b92794bf396165fd1ab78a1d7bef366fba9",
        "index": "4968ee6f0..53a003e8f 100644",
        "commit_message": "fix: loss logging tag name\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _train_batches(model: NNModel, iterator: DataLearningIterator, train_config:",
            "tb_train_writer.add_summary(metric_sum, epochs)",
            "",
            "if losses:",
            "-                        loss_sum = tf.Summary(value=[tf.Summary.Value(tag='every_n_batches/' + 'loss',",
            "+                        loss_sum = tf.Summary(value=[tf.Summary.Value(tag='every_n_epochs/' + 'loss',",
            "simple_value=report['loss']), ])",
            "tb_train_writer.add_summary(loss_sum, i)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='every_n_batches/'), value=\"'every_n_epochs/'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2098,
        "neg_line": [
            "-loss_sum = tf.Summary(value=[tf.Summary.Value(tag='every_n_batches/' + 'loss',"
        ],
        "pos_line": [
            "+loss_sum = tf.Summary(value=[tf.Summary.Value(tag='every_n_epochs/' + 'loss',"
        ],
        "core_change": "-loss_sum = tf.Summary(value=[tf.Summary.Value(tag='every_n_batches/' + 'loss', +loss_sum = tf.Summary(value=[tf.Summary.Value(tag='every_n_epochs/' + 'loss',",
        "core_API": "add_summary"
    },
    {
        "commit_hash": "b3b96b254ce9ac253151fa7ee9a0da12d8b14a77",
        "index": "27286b93..12745a9f 100644",
        "commit_message": "[Fix] Minor update improvements to zca, docs for zca whitening, and improve tests. (#566)\n\n* update docs, replace view w/reshape, doc ltransform, fix test\n\n* fix typos\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestZCA:",
            "else:",
            "expected = torch.sqrt(2 * torch.abs(data)) * torch.sign(data)",
            "",
            "-        expected.to(device)",
            "+        expected = expected.to(device)",
            "",
            "actual = kornia.zca_whiten(data, unbiased=unbiased)"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=440204)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'expected'), position=0, insert_id=440205)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=440206)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 2100,
        "neg_line": [
            "-expected.to(device)"
        ],
        "pos_line": [
            "+expected = expected.to(device)"
        ],
        "core_change": "-expected.to(device) +expected = expected.to(device)",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "f7729086cc840a03b41c630e8edb9c310fa903f8",
        "index": "392adfbb..24e7a01e 100755",
        "commit_message": "[FasterRCNN] update docs; fix tf.reverse\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "image, fg_sampled_boxes,",
            "tf.zeros_like(fg_inds_wrt_sample, dtype=tf.int32), 300)",
            "fg_sampled_patches = tf.transpose(fg_sampled_patches, [0, 2, 3, 1])",
            "-                fg_sampled_patches = tf.reverse(fg_sampled_patches, axis=-1)  # BGR->RGB",
            "+                fg_sampled_patches = tf.reverse(fg_sampled_patches, axis=[-1])  # BGR->RGB",
            "tf.summary.image('viz', fg_sampled_patches, max_outputs=30)",
            "",
            "matched_gt_boxes = tf.gather(gt_boxes, fg_inds_wrt_gt)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('list', None), position=2, insert_id=2292462)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=2292463)",
            "Move(target_node=IN(type=list), node=ASTNode(type=unary_operator, text=-1), position=1)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=2, insert_id=2292464)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 2104,
        "neg_line": [
            "-fg_sampled_patches = tf.reverse(fg_sampled_patches, axis=-1)  # BGR->RGB"
        ],
        "pos_line": [
            "+fg_sampled_patches = tf.reverse(fg_sampled_patches, axis=[-1])  # BGR->RGB"
        ],
        "core_change": "-fg_sampled_patches = tf.reverse(fg_sampled_patches, axis=-1)  # BGR->RGB +fg_sampled_patches = tf.reverse(fg_sampled_patches, axis=[-1])  # BGR->RGB",
        "core_API": "zeros_like"
    },
    {
        "commit_hash": "948bcdd21918043f309d763610d51a91afe970d2",
        "index": "e22cf14..659c939 100755",
        "commit_message": "--classes bug fix #17\n\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def non_max_suppression(prediction, conf_thres=0.1, iou_thres=0.6, fast=False, c",
            "",
            "# Filter by class",
            "if classes:",
            "-            x = x[(j.view(-1, 1) == torch.tensor(classes, device=j.device)).any(1)]",
            "+            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]",
            "",
            "# Apply finite constraint",
            "# if not torch.isfinite(x).all():"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('subscript', None), position=0, insert_id=1306930)",
            "Update(target_node=ASTNode(type=identifier, text=j), value='x')",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=j), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1306931)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=2, insert_id=1306932)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=1306933)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=4, insert_id=1306934)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=5, insert_id=1306935)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=1306936)",
            "Insert(target_node=IN(type=slice), node=('integer', '5'), position=0, insert_id=1306937)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=1, insert_id=1306938)",
            "Update(target_node=ASTNode(type=integer, text=1), value='6')",
            "Move(target_node=IN(type=slice), node=ASTNode(type=integer, text=1), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=j), value='x')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=view))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 2106,
        "neg_line": [
            "-x = x[(j.view(-1, 1) == torch.tensor(classes, device=j.device)).any(1)]"
        ],
        "pos_line": [
            "+x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]"
        ],
        "core_change": "-x = x[(j.view(-1, 1) == torch.tensor(classes, device=j.device)).any(1)] +x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]",
        "core_API": "view"
    },
    {
        "commit_hash": "44dda1af649ebabf18859222d615481cba97aa7b",
        "index": "7321d5cb..631e3659 100644",
        "commit_message": "fix indent error\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ThresholdsTest(tf.test.TestCase, parameterized.TestCase):",
            "self.assertAllClose(v1, v2)",
            "",
            "",
            "-if __name__ == \"__main__\":",
            "-    tf.test.main()",
            "+if __name__ == '__main__':",
            "+  tf.test.main()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"__main__\"), value=\"'__main__'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2110,
        "neg_line": [
            "-if __name__ == \"__main__\":",
            "-tf.test.main()"
        ],
        "pos_line": [
            "+if __name__ == '__main__':",
            "+tf.test.main()"
        ],
        "core_change": "-if __name__ == \"__main__\": -tf.test.main() +if __name__ == '__main__': +tf.test.main()",
        "core_API": "assertAllClose"
    },
    {
        "commit_hash": "c4c532d25e012dbe6ab1ac14bca75e53e0acc621",
        "index": "372d0e1d..dbba718a 100644",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestEvaluate(AllenNlpTestCase):",
            "archive_path = str(self.FIXTURES_ROOT / \"decomposable_attention\" / \"serialization\" / \"model.tar.gz\")",
            "# snli2 has a extra token (\"seahorse\") in it.",
            "evaluate_data_path = str(self.FIXTURES_ROOT / 'data' / 'snli2.jsonl')",
            "-        embeddings_filename = str(self.FIXTURES_ROOT / 'data' / 'seahorse_embeddings.gz') #has only seahorse vector",
            "+        embeddings_filename = str(self.FIXTURES_ROOT / 'data' / 'seahorse_embeddings.gz')  # has only seahorse vector",
            "embedding_sources_mapping = json.dumps({\"_text_field_embedder.token_embedder_tokens\": embeddings_filename})",
            "kebab_args = [\"evaluate\", archive_path, evaluate_data_path, \"--cuda-device\", \"-1\"]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2111,
        "neg_line": [
            "-embeddings_filename = str(self.FIXTURES_ROOT / 'data' / 'seahorse_embeddings.gz') #has only seahorse vector"
        ],
        "pos_line": [
            "+embeddings_filename = str(self.FIXTURES_ROOT / 'data' / 'seahorse_embeddings.gz')  # has only seahorse vector"
        ],
        "core_change": "-embeddings_filename = str(self.FIXTURES_ROOT / 'data' / 'seahorse_embeddings.gz') #has only seahorse vector +embeddings_filename = str(self.FIXTURES_ROOT / 'data' / 'seahorse_embeddings.gz')  # has only seahorse vector",
        "core_API": "dumps"
    },
    {
        "commit_hash": "19bd42de61015a465d2b00b13e28634963edffc0",
        "index": "39e57516..761c6fa5 100644",
        "commit_message": "fix: ci (#2306)\n\nFixed: keras, onnx, paddle, spacy\n",
        "file": "BentoML.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "def test_statsmodels_save_load(metadata, holt_model):  # noqa # pylint: disable",
            "",
            "",
            "@pytest.mark.parametrize(\"exc\", [BentoMLException])",
            "-def test_get_model_info_exc(exc, holt_model):",
            "+def test_load_model_exc(exc, holt_model):",
            "tag = wrong_module(holt_model)",
            "with pytest.raises(exc):",
            "-        bentoml._internal.frameworks.statsmodels._get_model_info(tag)",
            "+        bentoml._internal.frameworks.statsmodels.load(tag)",
            "",
            "",
            "def test_statsmodels_runner_setup_run_batch(save_proc, holt_model):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=test_get_model_info_exc), value='test_load_model_exc')",
            "Update(target_node=ASTNode(type=identifier, text=_get_model_info), value='load')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 2112,
        "neg_line": [
            "-def test_get_model_info_exc(exc, holt_model):",
            "-bentoml._internal.frameworks.statsmodels._get_model_info(tag)"
        ],
        "pos_line": [
            "+def test_load_model_exc(exc, holt_model):",
            "+bentoml._internal.frameworks.statsmodels.load(tag)"
        ],
        "core_change": "-def test_get_model_info_exc(exc, holt_model): +def test_load_model_exc(exc, holt_model): -bentoml._internal.frameworks.statsmodels._get_model_info(tag) +bentoml._internal.frameworks.statsmodels.load(tag)",
        "core_API": "parametrize"
    },
    {
        "commit_hash": "c6d4386c1574d715263e1cd8200f94a26f74ec13",
        "index": "79a66acd..54d59004 100644",
        "commit_message": "Fix embedding initialization for padding\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def PositionalEmbedding(num_embeddings, embedding_dim, padding_idx, left_pad, le",
            "if learned:",
            "m = LearnedPositionalEmbedding(num_embeddings, embedding_dim, padding_idx, left_pad)",
            "nn.init.normal(m.weight, mean=0, std=embedding_dim**-0.5)",
            "+        nn.init.constant(m.weight[padding_idx], 0)",
            "else:",
            "m = SinusoidalPositionalEmbedding(embedding_dim, padding_idx, left_pad, init_size=num_embeddings)",
            "return m"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1358555)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1358556)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1358557)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1358558)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'constant'), position=2, insert_id=1358559)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1358560)",
            "Insert(target_node=IN(type=argument_list), node=('subscript', None), position=1, insert_id=1358561)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1358562)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '0'), position=3, insert_id=1358563)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1358564)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('ERROR', None), position=1, insert_id=1358565)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=2, insert_id=1358566)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'init'), position=3, insert_id=1358567)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=0, insert_id=1358568)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1358569)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'padding_idx'), position=2, insert_id=1358570)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1358571)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'nn'), position=0, insert_id=1358572)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'm'), position=0, insert_id=1358573)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1358574)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'weight'), position=2, insert_id=1358575)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 22,
        "number": 2113,
        "neg_line": [],
        "pos_line": [
            "+nn.init.constant(m.weight[padding_idx], 0)"
        ],
        "core_change": "+nn.init.constant(m.weight[padding_idx], 0)",
        "core_API": "normal"
    },
    {
        "commit_hash": "66cffe5d69f86a3f678f41c1db11b41c2719e77d",
        "index": "7f21ce3509..f7f6749264 100644",
        "commit_message": "Searching Fix Up (#2349)\n\nCo-authored-by: jiahanxie353<765130715@qq.com>\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def argmin(",
            "return ret",
            "",
            "",
            "-def nonzero(",
            "-    x: Union[tf.Tensor, tf.Variable],",
            "-) -> Union[tf.Tensor, tf.Variable]:",
            "-    return tf.experimental.numpy.nonzero(x)",
            "+def nonzero(x: Union[tf.Tensor, tf.Variable]) -> Tuple[Union[tf.Tensor, tf.Variable]]:",
            "+    return tuple(tf.experimental.numpy.nonzero(x))",
            "",
            "",
            "def where("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=type), node=('subscript', None), position=0, insert_id=2004049)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Tuple'), position=0, insert_id=2004050)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2004051)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=subscript), position=2)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2004052)",
            "Insert(target_node=ASTNode(type=return_statement), node=('call', None), position=1, insert_id=2004053)",
            "Insert(target_node=IN(type=call), node=('identifier', 'tuple'), position=0, insert_id=2004054)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2004055)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2004056)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2004057)",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 2114,
        "neg_line": [
            "-def nonzero(",
            "-x: Union[tf.Tensor, tf.Variable],",
            "-) -> Union[tf.Tensor, tf.Variable]:",
            "-return tf.experimental.numpy.nonzero(x)"
        ],
        "pos_line": [
            "+def nonzero(x: Union[tf.Tensor, tf.Variable]) -> Tuple[Union[tf.Tensor, tf.Variable]]:",
            "+return tuple(tf.experimental.numpy.nonzero(x))"
        ],
        "core_change": "-def nonzero( -x: Union[tf.Tensor, tf.Variable], -) -> Union[tf.Tensor, tf.Variable]: -return tf.experimental.numpy.nonzero(x) +def nonzero(x: Union[tf.Tensor, tf.Variable]) -> Tuple[Union[tf.Tensor, tf.Variable]]: +return tuple(tf.experimental.numpy.nonzero(x))",
        "core_API": "nonzero"
    },
    {
        "commit_hash": "5f47b7f7482f8c29ddeefc0b75498c84fa70d5b0",
        "index": "45b1dd0..ae82e72 100644",
        "commit_message": "Fixed problem with continuing training of a model.\n",
        "file": "facenet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def l2_loss(tensor, weight=1.0, scope=None):",
            "Returns:",
            "the L2 loss op.",
            "\"\"\"",
            "-    with tf.op_scope([tensor], scope, 'l2_loss'):",
            "+    with tf.name_scope(scope):",
            "weight = tf.convert_to_tensor(weight,",
            "dtype=tensor.dtype.base_dtype,",
            "name='loss_weight')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=op_scope), value='name_scope')",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=string, text='l2_loss'))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2115,
        "neg_line": [
            "-with tf.op_scope([tensor], scope, 'l2_loss'):"
        ],
        "pos_line": [
            "+with tf.name_scope(scope):"
        ],
        "core_change": "-with tf.op_scope([tensor], scope, 'l2_loss'): +with tf.name_scope(scope):",
        "core_API": "op_scope"
    },
    {
        "commit_hash": "14bb2c40a2f8454353cb5cda88d522175d4f66ed",
        "index": "740af4a..9d25298 100755",
        "commit_message": "Fix yolov3 example (#577)\n\n\n",
        "file": "tinygrad.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "if __name__ == \"__main__\":",
            "cv2.destroyAllWindows()",
            "elif url.startswith('http'):",
            "img_stream = io.BytesIO(fetch(url))",
            "-    img = cv2.imdecode(np.fromstring(img_stream.read(), np.uint8), 1)",
            "+    img = cv2.imdecode(np.frombuffer(img_stream.read(), np.uint8), 1)",
            "else:",
            "img = cv2.imread(url)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=fromstring), value='frombuffer')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2117,
        "neg_line": [
            "-img = cv2.imdecode(np.fromstring(img_stream.read(), np.uint8), 1)"
        ],
        "pos_line": [
            "+img = cv2.imdecode(np.frombuffer(img_stream.read(), np.uint8), 1)"
        ],
        "core_change": "-img = cv2.imdecode(np.fromstring(img_stream.read(), np.uint8), 1) +img = cv2.imdecode(np.frombuffer(img_stream.read(), np.uint8), 1)",
        "core_API": "destroyAllWindows"
    },
    {
        "commit_hash": "d673f0ff6eb46b8dafeb9981168f8bbf36ea2e3e",
        "index": "e021dd22..6e8c5a90 100644",
        "commit_message": "Add ``undistort_image`` function (#1303)\n\n* Moving tiltProjection to distort.py\n\nAdding support for tilt and inverse tilt projection matrices. Distortion\nand undistortion models are separated into two different files.\n\n* Adding distort_points function\n\n* Adding function to undistort an image\n\n* Adding lens distortion math model to docs\n\n* Test for distort_points function\n\n* Adding test for undistort_image\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Replace torch.meshgrid by kornia.utils.create_meshgrid\n\n* Replace tiltProjection with tilt_projection\n\n* Raise an error if image shape isn't suitable\n\n* raise error message, change some variable names\n\n* Adapted to pep8\n\n* Adding option to return image as float 0-1 or uint8 0-255\n\n* Typos\n\n* Check if input image isn't float instead of checking if it isn't float32\n\n* Adding test_opencv for undistort_image\n\n* Creating grid of the same dtype of image\n\n* Removing optional uint8 output tensor\n\n* Replacing cat by reshape for 2D matrix of pixel points\n\n* Apply suggestions from code review\n\n* Input image is expected to be [0,1] float\n\nWe don't convert the input image to float. Instead if image isn't float\nand error arises. Then, we don't normalize the output tensor by dividing\nby 255.\n\n* Return when return_inverse is false\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* some fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* if/else fix\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def unproject_points(",
            "tensor of (x, y, z) world coordinates with shape :math:`(*, 3)`.",
            "",
            "Example:",
            "+        >>> _ = torch.manual_seed(0)",
            ">>> x = torch.rand(1, 2)",
            ">>> depth = torch.ones(1, 1)",
            ">>> K = torch.eye(3)[None]",
            ">>> unproject_points(x, depth, K)",
            "-        tensor([[0.2711, 0.6923, 1.0000]])",
            "+        tensor([[0.4963, 0.7682, 1.0000]])",
            "\"\"\"",
            "if not isinstance(point_2d, torch.Tensor):",
            "raise TypeError(f\"Input point_2d type is not a torch.Tensor. Got {type(point_2d)}\")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('binary_operator', None), position=1, insert_id=414640)",
            "Insert(target_node=ASTNode(type=ERROR), node=('=', '='), position=2, insert_id=414641)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('ERROR', None), position=3, insert_id=414642)",
            "Update(target_node=ASTNode(type=identifier, text=x), value='_')",
            "Insert(target_node=IN(type=binary_operator), node=('call', None), position=0, insert_id=414643)",
            "Insert(target_node=IN(type=binary_operator), node=('>>', '>>'), position=1, insert_id=414644)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=ERROR), position=2)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'x'), position=3, insert_id=414645)",
            "Insert(target_node=IN(type=ERROR), node=('>', '>'), position=0, insert_id=414646)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=414647)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=414648)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=414649)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=414650)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'manual_seed'), position=2, insert_id=414651)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=414652)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '0'), position=1, insert_id=414653)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=414654)",
            "Update(target_node=ASTNode(type=float, text=0.2711), value='0.4963')",
            "Update(target_node=ASTNode(type=float, text=0.6923), value='0.7682')"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 2118,
        "neg_line": [
            "-tensor([[0.2711, 0.6923, 1.0000]])"
        ],
        "pos_line": [
            "+>>> _ = torch.manual_seed(0)",
            "+tensor([[0.4963, 0.7682, 1.0000]])"
        ],
        "core_change": "+>>> _ = torch.manual_seed(0) -tensor([[0.2711, 0.6923, 1.0000]]) +tensor([[0.4963, 0.7682, 1.0000]])",
        "core_API": "manual_seed"
    },
    {
        "commit_hash": "23efadd9a394d9d69e10e8539a7332665b118df0",
        "index": "c5ff10a4..a65459c7 100644",
        "commit_message": "upgrade to pytorch 1.2 (#3182)\n\n* first attempt at pytorch 1.2\n\n* explicit is better than implicit\n\n* more explicit\n\n* attempt to fix flaky tests\n\n* pylint\n\n* no disable dropout\n\n* disable dropout by default\n\n* restore dropout, don't deepcopy\n\n* change batch size for biaffine_dependency_parser_multilang_test, maybe that will make it pass? :(\n\n* try batch size 10\n\n* ignore bad gradient parameter\n\n* cleanup\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BagOfWordCountsTokenEmbedder(TokenEmbedder):",
            "# also mask out positions corresponding to oov",
            "mask *= (inputs != self._oov_idx).long()",
            "for document, doc_mask in zip(inputs, mask):",
            "-            document = torch.masked_select(document, doc_mask.byte())",
            "+            document = torch.masked_select(document, doc_mask.to(dtype=torch.bool))",
            "vec = torch.bincount(document, minlength=self.vocab_size).float()",
            "vec = vec.view(1, -1)",
            "bag_of_words_vectors.append(vec)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=byte), value='to')",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=24352)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=24353)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=24354)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=24355)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=24356)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=24357)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=24358)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2119,
        "neg_line": [
            "-document = torch.masked_select(document, doc_mask.byte())"
        ],
        "pos_line": [
            "+document = torch.masked_select(document, doc_mask.to(dtype=torch.bool))"
        ],
        "core_change": "-document = torch.masked_select(document, doc_mask.byte()) +document = torch.masked_select(document, doc_mask.to(dtype=torch.bool))",
        "core_API": "masked_select"
    },
    {
        "commit_hash": "f5516805c2d0ea39797d46d9433dab43f769bea1",
        "index": "ef8932618..29b459fd8 100644",
        "commit_message": "Fix bart slow test\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BartModelIntegrationTest(unittest.TestCase):",
            "output = model.forward(**inputs_dict)[0]",
            "expected_shape = torch.Size((1, 11, 1024))",
            "self.assertEqual(output.shape, expected_shape)",
            "-        expected_slice = torch.Tensor(",
            "+        expected_slice = torch.tensor(",
            "[[0.7144, 0.8143, -1.2813], [0.7144, 0.8143, -1.2813], [-0.0467, 2.5911, -2.1845]], device=torch_device",
            ")",
            "self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=TOLERANCE))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='tensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2123,
        "neg_line": [
            "-expected_slice = torch.Tensor("
        ],
        "pos_line": [
            "+expected_slice = torch.tensor("
        ],
        "core_change": "-expected_slice = torch.Tensor( +expected_slice = torch.tensor(",
        "core_API": "forward"
    },
    {
        "commit_hash": "f162fd00152cf65dd7d99c3de6be90f3ce3f6563",
        "index": "ead4eac5..34954258 100644",
        "commit_message": "Merge PyG master (#48)\n\n* avoid the 'inf'\n\n* Create GATv2Conv\n\n* Update gatv2_conv.py\n\n* Update gatv2_conv.py\n\n* Update gatv2_conv.py\n\n* More doc\n\n* Update gatv2_conv.py\n\n* Update gatv2_conv.py\n\n* Update README.md\n\n* Update README.md\n\n* Create test_gatv2_conv.py\n\n* Update test_gatv2_conv.py\n\n* fixed gatv2 test\n\n* Fixed types\n\n* Update gatv2_conv.py\n\n* fix types\n\n* remove script folder\n\n* update test CI\n\n* fixed comments\n\n* lint + type\n\n* lint\n\n* Update test_gatv2_conv.py\n\n* Update test_gatv2_conv.py\n\n* fixed gatv2 test+ types\n\n* Update gatv2_conv.py\n\n* Update gatv2_conv.py\n\n* pytorch 1.9.0 support\n\n* typo\n\n* The dataset was introduced in the MUSAE paper\n\nDear Matthias,\n\nThese datasets were introduced in the Multi-scale Attributed Node Embedding paper.\n\nhttps://arxiv.org/abs/1909.13021\n\nBenedek\n\n* Github Dataset\n\n* Github Dataset\n\n* Github Dataset\n\n* Revert \"Merge branch 'master' into master\"\n\nThis reverts commit ef38f142465f736692c7c251a315ada287d7f104, reversing\nchanges made to d86de00a98173653a6158fc40238d34d0fb57cc1.\n\n* clean up\n\n* fix doc\n\n* fix gnn explainer\n\n* remove OGB-LSC\n\nCo-authored-by: Ethanzjp <13810907+Ethanzjp@users.noreply.github.com>\nCo-authored-by: shakedbr <shakedbr@campus.technion.ac.il>\nCo-authored-by: Uri Alon <urialon1@gmail.com>\nCo-authored-by: Shaked Brody <shakedbr@gmail.com>\nCo-authored-by: Benedek Rozemberczki <benedek.rozemberczki@gmail.com>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ClusterLoader(torch.utils.data.DataLoader):",
            "node_idx = torch.cat([torch.arange(s, e) for s, e in zip(start, end)])",
            "",
            "data = copy.copy(self.cluster_data.data)",
            "-        del data.num_nodes",
            "+        if hasattr(data, '__num_nodes__'):",
            "+            del data.__num_nodes__",
            "adj, data.adj = self.cluster_data.data.adj, None",
            "adj = cat([adj.narrow(0, s, e - s) for s, e in zip(start, end)], dim=0)",
            "adj = adj.index_select(1, node_idx)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=1002557)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1002558)",
            "Insert(target_node=IN(type=if_statement), node=('call', None), position=1, insert_id=1002559)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1002560)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1002561)",
            "Insert(target_node=IN(type=call), node=('identifier', 'hasattr'), position=0, insert_id=1002562)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1002563)",
            "Move(target_node=IN(type=block), node=ASTNode(type=delete_statement), position=0)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1002564)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'data'), position=1, insert_id=1002565)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1002566)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'__num_nodes__'\"), position=3, insert_id=1002567)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1002568)",
            "Update(target_node=ASTNode(type=identifier, text=num_nodes), value='__num_nodes__')"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 2125,
        "neg_line": [
            "-del data.num_nodes"
        ],
        "pos_line": [
            "+if hasattr(data, '__num_nodes__'):",
            "+del data.__num_nodes__"
        ],
        "core_change": "-del data.num_nodes +if hasattr(data, '__num_nodes__'): +del data.__num_nodes__",
        "core_API": "cat"
    },
    {
        "commit_hash": "4a2a59040c0e59fe6c6a106877ae7d945fbe2604",
        "index": "77b57e3e..5888840f 100644",
        "commit_message": "[Fix] fixes windows issues with augmentation smoke tests (#766)\n\n* add set_printoptions to fix windows issues\n\n* mark augmentation smoke tests as xfail\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestRandomMotionBlur:",
            "",
            "",
            "class TestRandomMotionBlur3D:",
            "+    # TODO: improve and implement more meaningful smoke tests e.g check for a consistent",
            "+    # return values such a torch.Tensor variable.",
            "+    @pytest.mark.xfail(reason=\"might fail under windows OS due to printing preicision.\")",
            "def test_smoke(self, device, dtype):",
            "f = RandomMotionBlur3D(kernel_size=(3, 5), angle=(10, 30), direction=0.5)",
            "repr = \"RandomMotionBlur3D(kernel_size=(3, 5), angle=tensor([[10., 30.],\"\\"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=class_definition), node=('ERROR', None), position=3, insert_id=434384)",
            "Insert(target_node=IN(type=ERROR), node=('decorator', None), position=0, insert_id=434385)",
            "Insert(target_node=IN(type=decorator), node=('@', '@'), position=0, insert_id=434386)",
            "Insert(target_node=IN(type=decorator), node=('call', None), position=1, insert_id=434387)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=434388)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=434389)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=434390)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=434391)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'xfail'), position=2, insert_id=434392)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=434393)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=434394)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=434395)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'pytest'), position=0, insert_id=434396)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=434397)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'mark'), position=2, insert_id=434398)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'reason'), position=0, insert_id=434399)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=434400)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"might fail under windows OS due to printing preicision.\"'), position=2, insert_id=434401)"
        ],
        "plus_line": 3,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 2130,
        "neg_line": [],
        "pos_line": [
            "+# TODO: improve and implement more meaningful smoke tests e.g check for a consistent",
            "+# return values such a torch.Tensor variable.",
            "+@pytest.mark.xfail(reason=\"might fail under windows OS due to printing preicision.\")"
        ],
        "core_change": "+# TODO: improve and implement more meaningful smoke tests e.g check for a consistent +# return values such a torch.Tensor variable. +@pytest.mark.xfail(reason=\"might fail under windows OS due to printing preicision.\")",
        "core_API": "xfail"
    },
    {
        "commit_hash": "e4fc06f986e03919d9aef3ab55c05fee5a6b9d3a",
        "index": "e9d7d76f..4c87aa49 100644",
        "commit_message": "Config-first Datasets API (ludwig.datasets refactor) (#2479)\n\n* Adds README and stub for reading dataset configs.\n\n* Adds __init__.py for configs, moves circular import into function scope in ludwig/datasets/__init__.py\n\n* Print config files in datasets folder.\n\n* First pass at automatic archive extraction.\n\n* Implemented downloading and extract.\n\n* Refactor DatasetConfig into its own file.\n\n* Fixed bugs downloading kaggle dataset.\n\n* Makes registry store dataset instances, not classes. Also comments out import_submodules for testing.\n\n* Typo fix.\n\n* Only pass data files on to load_unprocessed_dataframe, symlink directories.\n\n* Downloading dataset files into existing directory if exists.\n\n* Refactor: make datasets fully config-first, lazy load dataset loaders.\n\n* Implemented agnews custom loader.\n\n* Implements train/validation/test split by files, and globbing support\n\n* Adds _glob_multiple\n\n* Adds adult_census_income, agnews, allstate_claims_severity.\n\n* Implements sha256 verification, adds more datasets up to creditcard_fraud.\n\n* Adds checksums, dbpedia, electricity\n\n* Fixes gzip file name returned as string not list, adds up to forest_cover dataset.\n\n* Adds datasets up to reuters_r8\n\n* Adds all datasets which don't require a custom class.\n\n* Restore dataset import behavior by implementing module __getattr__\n\n* Adds KDD datasets.\n\n* Adds ieee_fraud.\n\n* Adds imbalanced_insurance, insurance_lite.\n\n* Adds mnist.\n\n* Completes implementation of all of the built-in datasets.\n\n* Made cache_dir optional, read from environment variable if set.\n\n* Upgrades datasets tests.\n\n* Adds test for new dataset config API.  Also adds scripts for dataset link checking.\n\n* Fixes loading allstate claims severity dataset.\n\n* Use @lru_cache(1), @cache not supported in python < 3.9\n\n* Deletes dataset registry, updates automl test utils\n\n* Fix imports of datasets API.\n\n* Adds more detail to sha256: docstring and basic README\n\n* Copy-paste link oops.\n\n* Fixes handling of nested archive types like .tar.bz  Also adds a LUDWIG_CACHE and export to the README\n\n* Adds link for twitter bots.\n\n* Fix order of splits in README.md\n\n* typo\n\n* Adds verify as a phase in doc string.\n\n* Support .pqt, .pq extensions for parquet.\n\n* Handle nested archives with longer file extensions like .csv.zip\n\n* Handle nested .gz types properly too.  Check all extensions with .endswith\n\n* Handle all archive types with .endswith\n\n* Update ludwig/datasets/loaders/split_loaders.py\n\nCo-authored-by: Joppe Geluykens <joppe@rvrie.com>\n\n* Adds explanation for export, fixes preserve_paths (should be relative to processed_dataset_dir)\n\n* Resolve preserved paths relative to raw dataset dir before move.\n\n* Catch runtime exception from extracting sub-archives.\n\nCo-authored-by: Daniel Treiman <daniel@predibase.com>\nCo-authored-by: Joppe Geluykens <joppe@rvrie.com>\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def get_dataset_golden_types_path(dataset_name: str) -> str:",
            "return str(Path(__file__).resolve().parent / \"golden\" / f\"{dataset_name}.types.json\")",
            "",
            "",
            "-def get_dataset_object(dataset_name: str) -> BaseDataset:",
            "+def get_dataset_object(dataset_name: str) -> DatasetLoader:",
            "\"\"\"Returns a Ludwig dataset instance for the given dataset.\"\"\"",
            "-    return dataset_registry[dataset_name]()",
            "+    return ludwig.datasets.get_dataset(dataset_name)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=BaseDataset), value='DatasetLoader')",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1796105)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1796106)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1796107)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get_dataset'), position=2, insert_id=1796108)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'dataset_name'), position=1, insert_id=1796109)",
            "Update(target_node=ASTNode(type=identifier, text=dataset_registry), value='ludwig')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=dataset_registry), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1796110)",
            "Update(target_node=ASTNode(type=identifier, text=dataset_name), value='datasets')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=dataset_name), position=2)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 2132,
        "neg_line": [
            "-def get_dataset_object(dataset_name: str) -> BaseDataset:",
            "-return dataset_registry[dataset_name]()"
        ],
        "pos_line": [
            "+def get_dataset_object(dataset_name: str) -> DatasetLoader:",
            "+return ludwig.datasets.get_dataset(dataset_name)"
        ],
        "core_change": "-def get_dataset_object(dataset_name: str) -> BaseDataset: +def get_dataset_object(dataset_name: str) -> DatasetLoader: -return dataset_registry[dataset_name]() +return ludwig.datasets.get_dataset(dataset_name)",
        "core_API": "get_dataset"
    },
    {
        "commit_hash": "2f92db8a16294ed72ab0697d208d7415dd44d88e",
        "index": "7daef2611..9e9f39bd9 100644",
        "commit_message": "Fix code according to the coding standards\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Decoder(torch.nn.Module):",
            "else:",
            "local_scores = functional.log_softmax(self.output(z_list[-1]), dim=1).data",
            "if lpz is not None:",
            "-                    local_att_best_scores, local_att_best_ids = torch.topk(local_scores, int(beam*1.5), dim=1)",
            "+                    local_att_best_scores, local_att_best_ids = torch.topk(local_scores, int(beam * CTC_SCORING_RATIO), dim=1)",
            "ctc_scores, ctc_states = ctc_prefix_score(hyp['yseq'], local_att_best_ids[0], hyp['ctc_prev'])",
            "-                    joint_scores = (1. - ctc_weight) * (local_att_best_scores[0].numpy() + hyp['score']) + ctc_weight * ctc_scores",
            "-                    joint_best_ids = np.argsort(joint_scores)[:-beam-1:-1]",
            "+                    joint_scores = (1. - ctc_weight) * \\",
            "+                        (local_att_best_scores[0].numpy() + hyp['score']) + ctc_weight * ctc_scores",
            "+                    joint_best_ids = np.argsort(joint_scores)[:-beam - 1:-1]",
            "local_best_ids = local_att_best_ids.numpy()[:, joint_best_ids]",
            "local_best_scores = local_att_best_scores.numpy()[:, joint_best_ids]",
            "else:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('identifier', 'CTC_SCORING_RATIO'), position=2, insert_id=186939)",
            "Delete(target_node=ASTNode(type=float, text=1.5))"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 2,
        "number": 2133,
        "neg_line": [
            "-local_att_best_scores, local_att_best_ids = torch.topk(local_scores, int(beam*1.5), dim=1)",
            "-joint_scores = (1. - ctc_weight) * (local_att_best_scores[0].numpy() + hyp['score']) + ctc_weight * ctc_scores",
            "-joint_best_ids = np.argsort(joint_scores)[:-beam-1:-1]"
        ],
        "pos_line": [
            "+local_att_best_scores, local_att_best_ids = torch.topk(local_scores, int(beam * CTC_SCORING_RATIO), dim=1)",
            "+joint_scores = (1. - ctc_weight) * \\",
            "+(local_att_best_scores[0].numpy() + hyp['score']) + ctc_weight * ctc_scores",
            "+joint_best_ids = np.argsort(joint_scores)[:-beam - 1:-1]"
        ],
        "core_change": "-local_att_best_scores, local_att_best_ids = torch.topk(local_scores, int(beam*1.5), dim=1) +local_att_best_scores, local_att_best_ids = torch.topk(local_scores, int(beam * CTC_SCORING_RATIO), dim=1) -joint_scores = (1. - ctc_weight) * (local_att_best_scores[0].numpy() + hyp['score']) + ctc_weight * ctc_scores -joint_best_ids = np.argsort(joint_scores)[:-beam-1:-1] +joint_scores = (1. - ctc_weight) * \\ +(local_att_best_scores[0].numpy() + hyp['score']) + ctc_weight * ctc_scores +joint_best_ids = np.argsort(joint_scores)[:-beam - 1:-1]",
        "core_API": "log_softmax"
    },
    {
        "commit_hash": "0db2046b0a35d5ce7a411489a9d7d14ee254d15d",
        "index": "c4342df78..59e112635 100644",
        "commit_message": "[RLlib] Policy.compute_log_likelihoods() and SAC refactor. (issue #7107) (#7124)\n\n* Exploration API (+EpsilonGreedy sub-class).\n\n* Exploration API (+EpsilonGreedy sub-class).\n\n* Cleanup/LINT.\n\n* Add `deterministic` to generic Trainer config (NOTE: this is still ignored by most Agents).\n\n* Add `error` option to deprecation_warning().\n\n* WIP.\n\n* Bug fix: Get exploration-info for tf framework.\nBug fix: Properly deprecate some DQN config keys.\n\n* WIP.\n\n* LINT.\n\n* WIP.\n\n* Split PerWorkerEpsilonGreedy out of EpsilonGreedy.\nDocstrings.\n\n* Fix bug in sampler.py in case Policy has self.exploration = None\n\n* Update rllib/agents/dqn/dqn.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* Update rllib/agents/trainer.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* Change requests.\n\n* LINT\n\n* In tune/utils/util.py::deep_update() Only keep deep_updat'ing if both original and value are dicts. If value is not a dict, set\n\n* Completely obsolete syn_replay_optimizer.py's parameters schedule_max_timesteps AND beta_annealing_fraction (replaced with prioritized_replay_beta_annealing_timesteps).\n\n* Update rllib/evaluation/worker_set.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Review fixes.\n\n* Fix default value for DQN's exploration spec.\n\n* LINT\n\n* Fix recursion bug (wrong parent c'tor).\n\n* Do not pass timestep to get_exploration_info.\n\n* Update tf_policy.py\n\n* Fix some remaining issues with test cases and remove more deprecated DQN/APEX exploration configs.\n\n* Bug fix tf-action-dist\n\n* DDPG incompatibility bug fix with new DQN exploration handling (which is imported by DDPG).\n\n* Switch off exploration when getting action probs from off-policy-estimator's policy.\n\n* LINT\n\n* Fix test_checkpoint_restore.py.\n\n* Deprecate all SAC exploration (unused) configs.\n\n* Properly use `model.last_output()` everywhere. Instead of `model._last_output`.\n\n* WIP.\n\n* Take out set_epsilon from multi-agent-env test (not needed, decays anyway).\n\n* WIP.\n\n* Trigger re-test (flaky checkpoint-restore test).\n\n* WIP.\n\n* WIP.\n\n* Add test case for deterministic action sampling in PPO.\n\n* bug fix.\n\n* Added deterministic test cases for different Agents.\n\n* Fix problem with TupleActions in dynamic-tf-policy.\n\n* Separate supported_spaces tests so they can be run separately for easier debugging.\n\n* LINT.\n\n* Fix autoregressive_action_dist.py test case.\n\n* Re-test.\n\n* Fix.\n\n* Remove duplicate py_test rule from bazel.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* SAC fix.\n\n* SAC fix.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* FIX 2 examples tests.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* Renamed test file.\n\n* WIP.\n\n* Add unittest.main.\n\n* Make action_dist_class mandatory.\n\n* fix\n\n* FIX.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix explorations test case (contextlib cannot find its own nullcontext??).\n\n* Force torch to be installed for QMIX.\n\n* LINT.\n\n* Fix determine_tests_to_run.py.\n\n* Fix determine_tests_to_run.py.\n\n* WIP\n\n* Add Random exploration component to tests (fixed issue with \"static-graph randomness\" via py_function).\n\n* Add Random exploration component to tests (fixed issue with \"static-graph randomness\" via py_function).\n\n* Rename some stuff.\n\n* Rename some stuff.\n\n* WIP.\n\n* WIP.\n\n* Fix SAC.\n\n* Fix SAC.\n\n* Fix strange tf-error in ray core tests.\n\n* Fix strange ray-core tf-error in test_memory_scheduling test case.\n\n* Fix test_io.py.\n\n* LINT.\n\n* Update SAC yaml files' config.\n\nCo-authored-by: Eric Liang <ekhliang@gmail.com>\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Random(Exploration):",
            "if explore:",
            "# Unsqueeze will be unnecessary, once we support batch/time-aware",
            "# Spaces.",
            "-            action = torch.IntTensor(self.action_space.sample()).unsqueeze(0)",
            "+            action = torch.LongTensor(self.action_space.sample()).unsqueeze(0)",
            "else:",
            "-            action = torch.IntTensor(action_dist.deterministic_sample())",
            "+            action = torch.LongTensor(action_dist.deterministic_sample())",
            "logp = torch.zeros((action.size()[0], ), dtype=torch.float32)",
            "return action, logp"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=IntTensor), value='LongTensor')",
            "Update(target_node=ASTNode(type=identifier, text=IntTensor), value='LongTensor')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 2134,
        "neg_line": [
            "-action = torch.IntTensor(self.action_space.sample()).unsqueeze(0)",
            "-action = torch.IntTensor(action_dist.deterministic_sample())"
        ],
        "pos_line": [
            "+action = torch.LongTensor(self.action_space.sample()).unsqueeze(0)",
            "+action = torch.LongTensor(action_dist.deterministic_sample())"
        ],
        "core_change": "-action = torch.IntTensor(self.action_space.sample()).unsqueeze(0) +action = torch.LongTensor(self.action_space.sample()).unsqueeze(0) -action = torch.IntTensor(action_dist.deterministic_sample()) +action = torch.LongTensor(action_dist.deterministic_sample())",
        "core_API": "IntTensor"
    },
    {
        "commit_hash": "c17169dc110ed0699eefab04ca17537eb68ce713",
        "index": "c377608d3..97ebc21d5 100644",
        "commit_message": "[RLlib] Fix all example scripts to run on GPUs. (#11105)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchFastModel(TorchModelV2, nn.Module):",
            "",
            "@override(ModelV2)",
            "def forward(self, input_dict, state, seq_lens):",
            "-        self._output = self.bias + \\",
            "-            torch.zeros(size=(input_dict[\"obs\"].shape[0], self.num_outputs))",
            "+        self._output = self.bias + torch.zeros(",
            "+            size=(input_dict[\"obs\"].shape[0], self.num_outputs)).to(",
            "+                self.bias.device)",
            "return self._output, []",
            "",
            "@override(ModelV2)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1121611)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1121612)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1121613)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1121614)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1121615)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1121616)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1121617)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1121618)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1121619)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1121620)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1121621)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1121622)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bias'), position=2, insert_id=1121623)"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 2136,
        "neg_line": [
            "-self._output = self.bias + \\",
            "-torch.zeros(size=(input_dict[\"obs\"].shape[0], self.num_outputs))"
        ],
        "pos_line": [
            "+self._output = self.bias + torch.zeros(",
            "+size=(input_dict[\"obs\"].shape[0], self.num_outputs)).to(",
            "+self.bias.device)"
        ],
        "core_change": "-self._output = self.bias + \\ -torch.zeros(size=(input_dict[\"obs\"].shape[0], self.num_outputs)) +self._output = self.bias + torch.zeros( +size=(input_dict[\"obs\"].shape[0], self.num_outputs)).to( +self.bias.device)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "3c5ee5224f035773b8b79e7f744631cdfa6f8863",
        "index": "35ebb8db..51e2cf52 100644",
        "commit_message": "Fix typo error of tf.compat.v1.keras.experimental for export_saved_model and load_from_saved_model\n\nThis PR fixes the Typo error for tf.compat.v1.keras.experimental.export_saved_model and tf.compat.v1.keras.experimental.load_from_saved_model\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def load_from_saved_model(saved_model_path, custom_objects=None):",
            "",
            "# Save the tf.keras model in the SavedModel format.",
            "path = '/tmp/simple_keras_model'",
            "-    tf.keras.experimental.export_saved_model(model, path)",
            "+    tf.compat.v1.keras.experimental.export_saved_model(model, path)",
            "",
            "# Load the saved keras model back.",
            "-    new_model = tf.keras.experimental.load_from_saved_model(path)",
            "+    new_model = tf.compat.v1.keras.experimental.load_from_saved_model(path)",
            "new_model.summary()",
            "```"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2056551)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2056552)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2056553)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2056554)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'v1'), position=2, insert_id=2056555)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2056556)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2056557)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'compat'), position=2, insert_id=2056558)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2056559)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2056560)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'v1'), position=2, insert_id=2056561)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'compat'), position=2, insert_id=2056562)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 2139,
        "neg_line": [
            "-tf.keras.experimental.export_saved_model(model, path)",
            "-new_model = tf.keras.experimental.load_from_saved_model(path)"
        ],
        "pos_line": [
            "+tf.compat.v1.keras.experimental.export_saved_model(model, path)",
            "+new_model = tf.compat.v1.keras.experimental.load_from_saved_model(path)"
        ],
        "core_change": "-tf.keras.experimental.export_saved_model(model, path) +tf.compat.v1.keras.experimental.export_saved_model(model, path) -new_model = tf.keras.experimental.load_from_saved_model(path) +new_model = tf.compat.v1.keras.experimental.load_from_saved_model(path)",
        "core_API": "export_saved_model"
    },
    {
        "commit_hash": "45a5606fca90380d6fa122f8596ab56df048aa2c",
        "index": "50de36e9..f8c58c79 100644",
        "commit_message": "bugfixes\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from torch_geometric.nn.functional.random_walk import random_walk",
            "def test_random_walk():",
            "edge_index = torch.LongTensor([[0, 0, 1, 1, 2], [0, 1, 1, 2, 2]])",
            "edge_attr = torch.Tensor([0.5, 0.5, 0.5, 0.5, 1])",
            "-    target = torch.LongTensor([1, 0, 1])",
            "+    one_hot = torch.Tensor([[0, 1], [1, 0], [0, 1]])",
            "weight = torch.Tensor(2, 4).fill_(1)  # 2 classes, 4 steps.",
            "",
            "-    random_walk(edge_index, edge_attr, target, weight)",
            "-    random_walk(edge_index, Var(edge_attr), Var(target), Var(weight))",
            "+    random_walk(edge_index, edge_attr, one_hot, weight)",
            "+    random_walk(edge_index, Var(edge_attr), Var(one_hot), Var(weight))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=target), value='one_hot')",
            "Update(target_node=ASTNode(type=identifier, text=target), value='one_hot')",
            "Update(target_node=ASTNode(type=identifier, text=LongTensor), value='Tensor')",
            "Insert(target_node=ASTNode(type=list), node=('list', None), position=1, insert_id=1076746)",
            "Insert(target_node=ASTNode(type=list), node=('list', None), position=4, insert_id=1076747)",
            "Insert(target_node=ASTNode(type=list), node=('list', None), position=6, insert_id=1076748)",
            "Insert(target_node=ASTNode(type=list), node=(']', ']'), position=7, insert_id=1076749)",
            "Update(target_node=ASTNode(type=identifier, text=target), value='one_hot')",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=1076750)",
            "Insert(target_node=IN(type=list), node=('integer', '0'), position=1, insert_id=1076751)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=1076752)",
            "Move(target_node=IN(type=list), node=ASTNode(type=integer, text=1), position=3)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=1076753)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=1076754)",
            "Insert(target_node=IN(type=list), node=('integer', '1'), position=1, insert_id=1076755)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=1076756)",
            "Move(target_node=IN(type=list), node=ASTNode(type=integer, text=0), position=3)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=1076757)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=1076758)",
            "Insert(target_node=IN(type=list), node=('integer', '0'), position=1, insert_id=1076759)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=1076760)",
            "Move(target_node=IN(type=list), node=ASTNode(type=integer, text=1), position=3)",
            "Move(target_node=IN(type=list), node=ASTNode(type=], text=]), position=4)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 23,
        "number": 2141,
        "neg_line": [
            "-target = torch.LongTensor([1, 0, 1])",
            "-random_walk(edge_index, edge_attr, target, weight)",
            "-random_walk(edge_index, Var(edge_attr), Var(target), Var(weight))"
        ],
        "pos_line": [
            "+one_hot = torch.Tensor([[0, 1], [1, 0], [0, 1]])",
            "+random_walk(edge_index, edge_attr, one_hot, weight)",
            "+random_walk(edge_index, Var(edge_attr), Var(one_hot), Var(weight))"
        ],
        "core_change": "-target = torch.LongTensor([1, 0, 1]) +one_hot = torch.Tensor([[0, 1], [1, 0], [0, 1]]) -random_walk(edge_index, edge_attr, target, weight) -random_walk(edge_index, Var(edge_attr), Var(target), Var(weight)) +random_walk(edge_index, edge_attr, one_hot, weight) +random_walk(edge_index, Var(edge_attr), Var(one_hot), Var(weight))",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "9c21696d060924799fab354321596031857dfff7",
        "index": "a57bf728..8fa92dc7 100644",
        "commit_message": "fixing a bug in trainer for histograms (#1498)\n\notherwise it would raise an error in line 505 (expected object of type torch.FloatTensor but found type torch.cuda.FloatTensor for argument #2 'other')\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer:",
            "for name, param in self._model.named_parameters():",
            "param_updates[name].sub_(param.detach().cpu())",
            "update_norm = torch.norm(param_updates[name].view(-1, ))",
            "-                    param_norm = torch.norm(param.view(-1, ))",
            "+                    param_norm = torch.norm(param.view(-1, )).cpu()",
            "self._tensorboard.add_train_scalar(\"gradient_update/\" + name,",
            "update_norm / (param_norm + 1e-7),",
            "batch_num_total)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=35185)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=35186)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=35187)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cpu'), position=2, insert_id=35188)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=35189)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=35190)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 2142,
        "neg_line": [
            "-param_norm = torch.norm(param.view(-1, ))"
        ],
        "pos_line": [
            "+param_norm = torch.norm(param.view(-1, )).cpu()"
        ],
        "core_change": "-param_norm = torch.norm(param.view(-1, )) +param_norm = torch.norm(param.view(-1, )).cpu()",
        "core_API": "named_parameters"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "6fa49031..5fe8d86c 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SOSNet(nn.Module):",
            "# load pretrained model",
            "if pretrained:",
            "storage_fcn: Callable = lambda storage, loc: storage",
            "-            pretrained_dict = torch.hub.load_state_dict_from_url(",
            "-                urls['lib'], map_location=storage_fcn",
            "-            )",
            "+            pretrained_dict = torch.hub.load_state_dict_from_url(urls['lib'], map_location=storage_fcn)",
            "self.load_state_dict(pretrained_dict, strict=True)",
            "self.eval()",
            "return"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 2143,
        "neg_line": [
            "-pretrained_dict = torch.hub.load_state_dict_from_url(",
            "-urls['lib'], map_location=storage_fcn",
            "-)"
        ],
        "pos_line": [
            "+pretrained_dict = torch.hub.load_state_dict_from_url(urls['lib'], map_location=storage_fcn)"
        ],
        "core_change": "-pretrained_dict = torch.hub.load_state_dict_from_url( -urls['lib'], map_location=storage_fcn -) +pretrained_dict = torch.hub.load_state_dict_from_url(urls['lib'], map_location=storage_fcn)",
        "core_API": "load_state_dict_from_url"
    },
    {
        "commit_hash": "293fe2cb182b15499672c9cf50f79c8a9857dfb4",
        "index": "90846eb84..196051971 100644",
        "commit_message": "[release] minor fix to pytorch_pbt_failure test when using gpu. (#32070)\n\nSigned-off-by: xwjiang2010 <xwjiang2010@gmail.com>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train_func(config):",
            "checkpoint_epoch = checkpoint_dict[\"epoch\"]",
            "starting_epoch = checkpoint_epoch + 1",
            "",
            "-    model = train.torch.prepare_model(model)",
            "-",
            "# Load in training and validation data.",
            "transform_train = transforms.Compose(",
            "["
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=model))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=train))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=prepare_model))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=model))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 2144,
        "neg_line": [
            "-model = train.torch.prepare_model(model)",
            "-"
        ],
        "pos_line": [],
        "core_change": "-model = train.torch.prepare_model(model) -",
        "core_API": "prepare_model"
    },
    {
        "commit_hash": "22458ac52956bb63c5f316bc5edeff21b5fa9810",
        "index": "7579723202..4b43e9f437 100644",
        "commit_message": "Changing tf array_equal() to return bool; add() and roll() docstrings tests passing with formatting fix in functional/ivy files.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def copy_array(x: Tensor) -> Tensor:",
            "",
            "",
            "def array_equal(x0: Tensor, x1: Tensor) -> bool:",
            "-    return tf.experimental.numpy.array_equal(x0, x1)",
            "+    return bool((tf.experimental.numpy.array_equal(x0, x1)))",
            "",
            "",
            "def to_numpy(x: Tensor) -> _np.ndarray:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('identifier', 'bool'), position=0, insert_id=2009030)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2009031)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2009032)",
            "Insert(target_node=IN(type=argument_list), node=('parenthesized_expression', None), position=1, insert_id=2009033)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2009034)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=2009035)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=2009036)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2145,
        "neg_line": [
            "-return tf.experimental.numpy.array_equal(x0, x1)"
        ],
        "pos_line": [
            "+return bool((tf.experimental.numpy.array_equal(x0, x1)))"
        ],
        "core_change": "-return tf.experimental.numpy.array_equal(x0, x1) +return bool((tf.experimental.numpy.array_equal(x0, x1)))",
        "core_API": "array_equal"
    },
    {
        "commit_hash": "4984cf54be4eb88f00ccf33a05f57681b2a770ab",
        "index": "5055560..142268b 100644",
        "commit_message": "train.py GPU memory fix (#3590)\n\n* train.py GPU memory fix\n\n* ema\n\n* cuda\n\n* cuda\n\n* zeros input\n\n* to device\n\n* batch index 0\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train(hyp, opt, device, tb_writer=None):",
            "if tb_writer and ni == 0:",
            "with warnings.catch_warnings():",
            "warnings.simplefilter('ignore')  # suppress jit trace warning",
            "-                            tb_writer.add_graph(torch.jit.trace(de_parallel(model), imgs, strict=False), [])  # graph",
            "+                            tb_writer.add_graph(torch.jit.trace(de_parallel(model), imgs[0:1], strict=False), [])",
            "elif plots and ni == 10 and wandb_logger.wandb:",
            "wandb_logger.log({'Mosaics': [wandb_logger.wandb.Image(str(x), caption=x.name) for x in",
            "save_dir.glob('train*.jpg') if x.exists()]})"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('subscript', None), position=3, insert_id=1298429)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=imgs), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1298430)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=2, insert_id=1298431)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1298432)",
            "Insert(target_node=IN(type=slice), node=('integer', '0'), position=0, insert_id=1298433)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=1, insert_id=1298434)",
            "Insert(target_node=IN(type=slice), node=('integer', '1'), position=2, insert_id=1298435)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2147,
        "neg_line": [
            "-tb_writer.add_graph(torch.jit.trace(de_parallel(model), imgs, strict=False), [])  # graph"
        ],
        "pos_line": [
            "+tb_writer.add_graph(torch.jit.trace(de_parallel(model), imgs[0:1], strict=False), [])"
        ],
        "core_change": "-tb_writer.add_graph(torch.jit.trace(de_parallel(model), imgs, strict=False), [])  # graph +tb_writer.add_graph(torch.jit.trace(de_parallel(model), imgs[0:1], strict=False), [])",
        "core_API": "catch_warnings"
    },
    {
        "commit_hash": "5593b6f772c75ae016580e2579c2af9f86b19a39",
        "index": "a54d00a98..a507afa6b 100644",
        "commit_message": "Merge pull request #7872 from PyTorchLightning/refactor/logger-poc-changes\n\nRandom fixes for logger connector PoC\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def sync_ddp_if_available(",
            "Return:",
            "reduced value",
            "\"\"\"",
            "-    if torch.distributed.is_available() and torch.distributed.is_initialized():",
            "+    if torch.distributed.is_available() and torch.distributed.is_initialized() or tpu_distributed():",
            "return sync_ddp(result, group=group, reduce_op=reduce_op)",
            "return result"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=boolean_operator), node=('boolean_operator', None), position=0, insert_id=533318)",
            "Insert(target_node=ASTNode(type=boolean_operator), node=('or', 'or'), position=1, insert_id=533319)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=0)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=and, text=and), position=1)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=call), node=('call', None), position=0, insert_id=533320)",
            "Insert(target_node=IN(type=call), node=('identifier', 'tpu_distributed'), position=0, insert_id=533321)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=533322)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=533323)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=533324)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 2150,
        "neg_line": [
            "-if torch.distributed.is_available() and torch.distributed.is_initialized():"
        ],
        "pos_line": [
            "+if torch.distributed.is_available() and torch.distributed.is_initialized() or tpu_distributed():"
        ],
        "core_change": "-if torch.distributed.is_available() and torch.distributed.is_initialized(): +if torch.distributed.is_available() and torch.distributed.is_initialized() or tpu_distributed():",
        "core_API": "is_available"
    },
    {
        "commit_hash": "709f89a964ffbd7f3a0bd7fdd447049917785f89",
        "index": "4dd62a51..8b9a013e 100644",
        "commit_message": "fix old usage of leakyrelu\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def LeakyReLU(x, alpha, name='output'):",
            "x (tf.Tensor): input",
            "alpha (float): the slope.",
            "\"\"\"",
            "+    log_deprecated(\"LeakyReLU\", \"Use tf.nn.leaky_relu in TF 1.4 instead!\", \"2018-03-30\")",
            "return tf.maximum(x, alpha * x, name=name)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=(')', ')'), position=4, insert_id=2290910)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 1,
        "number": 2151,
        "neg_line": [],
        "pos_line": [
            "+log_deprecated(\"LeakyReLU\", \"Use tf.nn.leaky_relu in TF 1.4 instead!\", \"2018-03-30\")"
        ],
        "core_change": "+log_deprecated(\"LeakyReLU\", \"Use tf.nn.leaky_relu in TF 1.4 instead!\", \"2018-03-30\")",
        "core_API": "maximum"
    },
    {
        "commit_hash": "9f4639200d87195dca3e97a96067e63c813a7d16",
        "index": "96a61063..6d8b4a7f 100644",
        "commit_message": "Fix flaky ray nightly image test (#2493)\n\n\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ImageFeatureMixin(BaseFeatureMixin):",
            "if isinstance(img_entry, bytes):",
            "img = read_image_from_bytes_obj(img_entry, num_channels)",
            "elif isinstance(img_entry, np.ndarray):",
            "-            img = torch.from_numpy(img_entry).permute(2, 0, 1)",
            "+            img = torch.from_numpy(np.array(img_entry, copy=True)).permute(2, 0, 1)",
            "else:",
            "img = img_entry"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=598765)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=598766)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=598767)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=598768)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'np'), position=0, insert_id=598769)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=598770)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'array'), position=2, insert_id=598771)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=598772)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=img_entry), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=598773)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=598774)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'copy'), position=0, insert_id=598775)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=598776)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=598777)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 2159,
        "neg_line": [
            "-img = torch.from_numpy(img_entry).permute(2, 0, 1)"
        ],
        "pos_line": [
            "+img = torch.from_numpy(np.array(img_entry, copy=True)).permute(2, 0, 1)"
        ],
        "core_change": "-img = torch.from_numpy(img_entry).permute(2, 0, 1) +img = torch.from_numpy(np.array(img_entry, copy=True)).permute(2, 0, 1)",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "9e5a469c64ca7121d3558f3ddf40b1a3e993ffcc",
        "index": "40cd2d3d..f7751931 100644",
        "commit_message": "d-vector handling (#1945)\n\n* Update BaseDatasetConfig\n\n- Add dataset_name\n- Chane name to formatter_name\n\n* Update compute_embedding\n\n- Allow entering dataset by args\n- Use released model by default\n- Use the new key format\n\n* Update loading\n\n* Update recipes\n\n* Update other dep code\n\n* Update tests\n\n* Fixup\n\n* Load multiple embedding files\n\n* Fix argument names in dep code\n\n* Update docs\n\n* Fix argument name\n\n* Fix linter\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "config.save_json(config_path)",
            "command_train = (",
            "f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"",
            "f\"--coqpit.output_path {output_path} \"",
            "-    \"--coqpit.datasets.0.name ljspeech \"",
            "+    \"--coqpit.datasets.0.formatter ljspeech \"",
            "\"--coqpit.datasets.0.meta_file_train metadata.csv \"",
            "\"--coqpit.datasets.0.meta_file_val metadata.csv \"",
            "\"--coqpit.datasets.0.path tests/data/ljspeech \""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"--coqpit.datasets.0.name ljspeech \"), value='\"--coqpit.datasets.0.formatter ljspeech \"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2160,
        "neg_line": [
            "-\"--coqpit.datasets.0.name ljspeech \""
        ],
        "pos_line": [
            "+\"--coqpit.datasets.0.formatter ljspeech \""
        ],
        "core_change": "-\"--coqpit.datasets.0.name ljspeech \" +\"--coqpit.datasets.0.formatter ljspeech \"",
        "core_API": "save_json"
    },
    {
        "commit_hash": "008c0edb3b7b96543930c47bcaed03429503971a",
        "index": "8edf072..5f789b3 100644",
        "commit_message": "Data Input Refactoring\n\n- Data input pipeline can now deal properly with missing target data.\nFixes #101\n- Moved some of the featurizer functionality into the data reader and\nmade featurizer implicit part of the model.\n- Misc: Replace concat with concat_v2\n\n",
        "file": "seq2seq.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def beam_search_step(logits, beam_state, config):",
            "",
            "# Append new ids to current predictions",
            "next_predictions = tf.gather(beam_state.predictions, next_beam_ids)",
            "-  next_predictions = tf.concat(1, [",
            "+  next_predictions = tf.concat_v2([",
            "next_predictions[:, 0:time_ - 1],",
            "tf.to_int32(tf.expand_dims(next_word_ids, 1)), next_predictions[:, time_:]",
            "-  ])",
            "+  ], 1)",
            "",
            "next_beam_state = BeamState(",
            "time=time_,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=concat), value='concat_v2')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=3, insert_id=2166968)",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '1'), position=4, insert_id=2166969)",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 5,
        "number": 2162,
        "neg_line": [
            "-next_predictions = tf.concat(1, [",
            "-])"
        ],
        "pos_line": [
            "+next_predictions = tf.concat_v2([",
            "+], 1)"
        ],
        "core_change": "-next_predictions = tf.concat(1, [ +next_predictions = tf.concat_v2([ -]) +], 1)",
        "core_API": "gather"
    },
    {
        "commit_hash": "290633b882a706c862f3407cb9779063ce443e1c",
        "index": "2c82bcd4c..b5290afd1 100644",
        "commit_message": "Fix `args.gradient_accumulation_steps` used before assigment.\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "",
            "model = BertForSequenceClassification(bert_config, len(label_list))",
            "if args.init_checkpoint is not None:",
            "-        model.load_state_dict(torch.load(args.init_checkpoint, map_location='cpu'))",
            "+        model.bert.load_state_dict(torch.load(args.init_checkpoint, map_location='cpu'))",
            "model.to(device)",
            "",
            "if args.local_rank != -1:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=1251461)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1251462)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=model), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bert'), position=2, insert_id=1251463)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2163,
        "neg_line": [
            "-model.load_state_dict(torch.load(args.init_checkpoint, map_location='cpu'))"
        ],
        "pos_line": [
            "+model.bert.load_state_dict(torch.load(args.init_checkpoint, map_location='cpu'))"
        ],
        "core_change": "-model.load_state_dict(torch.load(args.init_checkpoint, map_location='cpu')) +model.bert.load_state_dict(torch.load(args.init_checkpoint, map_location='cpu'))",
        "core_API": "load_state_dict"
    },
    {
        "commit_hash": "66641e8e683757c05148373cd43a00f034df302e",
        "index": "e64724e2..5fb9d70d 100644",
        "commit_message": "Clarify and fix for ResNeXt\n\n",
        "file": "d2l-en.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class ResNeXtBlock(nn.Block):",
            "use_1x1conv=False, strides=1, **kwargs):",
            "super().__init__(**kwargs)",
            "bot_channels = int(round(num_channels * bot_mul))",
            "-        self.conv1 = nn.Conv2D(bot_channels, kernel_size=1, padding=0, strides=1)",
            "+        self.conv1 = nn.Conv2D(bot_channels, kernel_size=1, padding=0,",
            "+                               strides=1)",
            "self.conv2 = nn.Conv2D(bot_channels, kernel_size=3, padding=1,",
            "strides=strides, groups=bot_channels//groups)",
            "-        self.conv3 = nn.Conv2D(num_channels, kernel_size=1, padding=0, strides=1)",
            "+        self.conv3 = nn.Conv2D(num_channels, kernel_size=1, padding=0,",
            "+                               strides=1)",
            "self.bn1 = nn.BatchNorm()",
            "self.bn2 = nn.BatchNorm()",
            "self.bn3 = nn.BatchNorm()",
            "if use_1x1conv:",
            "-            self.conv4 = nn.Conv2D(num_channels, kernel_size=1, strides=strides)",
            "+            self.conv4 = nn.Conv2D(num_channels, kernel_size=1,",
            "+                                   strides=strides)",
            "self.bn4 = nn.BatchNorm()",
            "else:",
            "self.conv4 = None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 6,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 2164,
        "neg_line": [
            "-self.conv1 = nn.Conv2D(bot_channels, kernel_size=1, padding=0, strides=1)",
            "-self.conv3 = nn.Conv2D(num_channels, kernel_size=1, padding=0, strides=1)",
            "-self.conv4 = nn.Conv2D(num_channels, kernel_size=1, strides=strides)"
        ],
        "pos_line": [
            "+self.conv1 = nn.Conv2D(bot_channels, kernel_size=1, padding=0,",
            "+strides=1)",
            "+self.conv3 = nn.Conv2D(num_channels, kernel_size=1, padding=0,",
            "+strides=1)",
            "+self.conv4 = nn.Conv2D(num_channels, kernel_size=1,",
            "+strides=strides)"
        ],
        "core_change": "-self.conv1 = nn.Conv2D(bot_channels, kernel_size=1, padding=0, strides=1) +self.conv1 = nn.Conv2D(bot_channels, kernel_size=1, padding=0, +strides=1) -self.conv3 = nn.Conv2D(num_channels, kernel_size=1, padding=0, strides=1) +self.conv3 = nn.Conv2D(num_channels, kernel_size=1, padding=0, +strides=1) -self.conv4 = nn.Conv2D(num_channels, kernel_size=1, strides=strides) +self.conv4 = nn.Conv2D(num_channels, kernel_size=1, +strides=strides)",
        "core_API": "Conv2D"
    },
    {
        "commit_hash": "39e620c134efc5a8ce3e93279ca5ee6d0942ba8f",
        "index": "1e27690bd..71c317cd1 100644",
        "commit_message": "Update `HubertModelIntegrationTest.test_inference_keyword_spotting` (#20863)\n\nfix ci\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class HubertModelIntegrationTest(unittest.TestCase):",
            "expected_logits = torch.tensor([7.6692, 17.7795, 11.1562, 11.8232], dtype=torch.float16, device=torch_device)",
            "",
            "self.assertListEqual(predicted_ids.tolist(), expected_labels)",
            "-        self.assertTrue(torch.allclose(predicted_logits, expected_logits, atol=2e-2))",
            "+        self.assertTrue(torch.allclose(predicted_logits, expected_logits, atol=3e-2))",
            "",
            "def test_inference_intent_classification(self):",
            "model = HubertForSequenceClassification.from_pretrained("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=2e-2), value='3e-2')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2165,
        "neg_line": [
            "-self.assertTrue(torch.allclose(predicted_logits, expected_logits, atol=2e-2))"
        ],
        "pos_line": [
            "+self.assertTrue(torch.allclose(predicted_logits, expected_logits, atol=3e-2))"
        ],
        "core_change": "-self.assertTrue(torch.allclose(predicted_logits, expected_logits, atol=2e-2)) +self.assertTrue(torch.allclose(predicted_logits, expected_logits, atol=3e-2))",
        "core_API": "tensor"
    },
    {
        "commit_hash": "ba61ac8822e75b551debe7686bfd7470060d7a5c",
        "index": "038c55d97..0a0f94638 100644",
        "commit_message": "Fix dtype issues in AST (#3679)\n\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AdditiveSharingTensor(AbstractTensor):",
            "mask_pos = x > self.max_value",
            "mask_neg = x < self.min_value",
            "if mask_pos.any():",
            "-                mask_pos = mask_pos.long()",
            "+                mask_pos = mask_pos.type(self.torch_dtype)",
            "return self.modulo(x - (mask_pos * self.field))",
            "elif mask_neg.any():",
            "-                mask_neg = mask_neg.long()",
            "+                mask_neg = mask_neg.type(self.torch_dtype)",
            "return self.modulo(x + (mask_neg * self.field))",
            "else:",
            "return x.type(self.torch_dtype)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=long), value='type')",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=790687)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=790688)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=790689)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch_dtype'), position=2, insert_id=790690)",
            "Update(target_node=ASTNode(type=identifier, text=long), value='type')",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=790691)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=790692)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=790693)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch_dtype'), position=2, insert_id=790694)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 2166,
        "neg_line": [
            "-mask_pos = mask_pos.long()",
            "-mask_neg = mask_neg.long()"
        ],
        "pos_line": [
            "+mask_pos = mask_pos.type(self.torch_dtype)",
            "+mask_neg = mask_neg.type(self.torch_dtype)"
        ],
        "core_change": "-mask_pos = mask_pos.long() +mask_pos = mask_pos.type(self.torch_dtype) -mask_neg = mask_neg.long() +mask_neg = mask_neg.type(self.torch_dtype)",
        "core_API": "any"
    },
    {
        "commit_hash": "6013d3f1f6ed341727206ed3585182de1cfa8023",
        "index": "019ee6ed..310de1ea 100644",
        "commit_message": "fixed problem with a3c\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DistributedModel(object):",
            "",
            "self.gradients = tf.gradients(self.loss, self.local_network.get_variables())",
            "",
            "-            grad_var_list = list(zip(self.gradients, self.local_network.get_variables()))",
            "+            grad_var_list = list(zip(self.gradients, self.global_network.get_variables()))",
            "",
            "global_step_inc = self.global_step.assign_add(self.batch_size)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=local_network), value='global_network')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2168,
        "neg_line": [
            "-grad_var_list = list(zip(self.gradients, self.local_network.get_variables()))"
        ],
        "pos_line": [
            "+grad_var_list = list(zip(self.gradients, self.global_network.get_variables()))"
        ],
        "core_change": "-grad_var_list = list(zip(self.gradients, self.local_network.get_variables())) +grad_var_list = list(zip(self.gradients, self.global_network.get_variables()))",
        "core_API": "gradients"
    },
    {
        "commit_hash": "3dda5ac9fcf390d8b83eb855249360711707c11c",
        "index": "88a546a9..5a08f404 100644",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MentionRecall(Metric):",
            "if self._num_gold_mentions == 0:",
            "recall = 0.0",
            "else:",
            "-            recall = self._num_recalled_mentions/float(self._num_gold_mentions)",
            "+            recall = self._num_recalled_mentions / float(self._num_gold_mentions)",
            "if reset:",
            "self.reset()",
            "return recall"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2170,
        "neg_line": [
            "-recall = self._num_recalled_mentions/float(self._num_gold_mentions)"
        ],
        "pos_line": [
            "+recall = self._num_recalled_mentions / float(self._num_gold_mentions)"
        ],
        "core_change": "-recall = self._num_recalled_mentions/float(self._num_gold_mentions) +recall = self._num_recalled_mentions / float(self._num_gold_mentions)",
        "core_API": "reset"
    },
    {
        "commit_hash": "5f4c6e830f603830117877b8990a0e65a2386aa6",
        "index": "73a071fb8..610d3c97e 100644",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class FeaturesTest(TestCase):",
            "casted_obj = cast_to_python_objects(obj)",
            "self.assertDictEqual(casted_obj, expected_obj)",
            "",
            "-    @patch(\"nlp.features._cast_to_python_objects\", side_effect=_cast_to_python_objects)",
            "+    @patch(\"datasets.features._cast_to_python_objects\", side_effect=_cast_to_python_objects)",
            "def test_dont_iterate_over_each_element_in_a_list(self, mocked_cast):",
            "obj = {\"col_1\": [[1, 2], [3, 4], [5, 6]]}",
            "cast_to_python_objects(obj)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"nlp.features._cast_to_python_objects\"), value='\"datasets.features._cast_to_python_objects\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2171,
        "neg_line": [
            "-@patch(\"nlp.features._cast_to_python_objects\", side_effect=_cast_to_python_objects)"
        ],
        "pos_line": [
            "+@patch(\"datasets.features._cast_to_python_objects\", side_effect=_cast_to_python_objects)"
        ],
        "core_change": "-@patch(\"nlp.features._cast_to_python_objects\", side_effect=_cast_to_python_objects) +@patch(\"datasets.features._cast_to_python_objects\", side_effect=_cast_to_python_objects)",
        "core_API": "assertDictEqual"
    },
    {
        "commit_hash": "8fcec728bb070ae57ad425a69c45cfd8ac9501e3",
        "index": "1229a4f470..931395dfcb 100644",
        "commit_message": "fixed failing test_to_dev for torch.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "torch_scatter = None",
            "def dev(x: torch.Tensor, as_native: bool = False) -> Union[ivy.Device, torch.device]:",
            "dv = x.device",
            "if as_native:",
            "-        return dv",
            "+        return torch.device(dv)",
            "return as_ivy_dev(dv)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('call', None), position=1, insert_id=352594)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=352595)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=352596)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=352597)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=352598)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=352599)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=352600)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=dv), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=352601)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 2172,
        "neg_line": [
            "-return dv"
        ],
        "pos_line": [
            "+return torch.device(dv)"
        ],
        "core_change": "-return dv +return torch.device(dv)",
        "core_API": "device"
    },
    {
        "commit_hash": "6fdc016dc651550c9f41dff186b15fa0051636bf",
        "index": "0258f15..223dca0 100644",
        "commit_message": "fix small bug\n\n",
        "file": "pytorch-CycleGAN-and-pix2pix.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "class CycleGANModel(BaseModel):",
            "# initialize optimizers",
            "self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()),",
            "lr=opt.lr, betas=(opt.beta1, 0.999))",
            "-            self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))",
            "+            self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()),",
            "+                                                lr=opt.lr, betas=(opt.beta1, 0.999))",
            "self.optimizers = []",
            "self.schedulers = []",
            "self.optimizers.append(self.optimizer_G)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2173,
        "neg_line": [
            "-self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))"
        ],
        "pos_line": [
            "+self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()),",
            "+lr=opt.lr, betas=(opt.beta1, 0.999))"
        ],
        "core_change": "-self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999)) +self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()), +lr=opt.lr, betas=(opt.beta1, 0.999))",
        "core_API": "Adam"
    },
    {
        "commit_hash": "ada7a73acc315258337b72e3d80597159487a8e7",
        "index": "20b12c26..5e73981e 100644",
        "commit_message": "fix D210\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def retrieve_seq_length_op3(data, pad_val=0):  # HangSheng: return tensor for se",
            "",
            "",
            "def target_mask_op(data, pad_val=0):  # HangSheng: return tensor for mask,if input is tf.string",
            "-    \"\"\" Return tensor for mask, if input is ``tf.string``. \"\"\"",
            "+    \"\"\"Return tensor for mask, if input is ``tf.string``.\"\"\"",
            "data_shape_size = data.get_shape().ndims",
            "if data_shape_size == 3:",
            "return tf.cast(tf.reduce_any(tf.not_equal(data, pad_val), axis=2), dtype=tf.int32)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\" Return tensor for mask, if input is ``tf.string``. \"\"\"), value='\"\"\"Return tensor for mask, if input is ``tf.string``.\"\"\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2175,
        "neg_line": [
            "-\"\"\" Return tensor for mask, if input is ``tf.string``. \"\"\""
        ],
        "pos_line": [
            "+\"\"\"Return tensor for mask, if input is ``tf.string``.\"\"\""
        ],
        "core_change": "-\"\"\" Return tensor for mask, if input is ``tf.string``. \"\"\" +\"\"\"Return tensor for mask, if input is ``tf.string``.\"\"\"",
        "core_API": "get_shape"
    },
    {
        "commit_hash": "fafd4c86ecb63bb90b095bbd23453553e33fe99d",
        "index": "ed8fdb74c..8d010e589 100644",
        "commit_message": "fix TF 2.0 version of T5 - update conversion script\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFPreTrainedModel(tf.keras.Model):",
            "Returns:",
            "tf.Tensor with dummy inputs",
            "\"\"\"",
            "-        return tf.constant(DUMMY_INPUTS)",
            "+        return {'input_ids': tf.constant(DUMMY_INPUTS)}",
            "",
            "def __init__(self, config, *inputs, **kwargs):",
            "super(TFPreTrainedModel, self).__init__(*inputs, **kwargs)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('dictionary', None), position=4, insert_id=2384850)",
            "Insert(target_node=IN(type=dictionary), node=('{', '{'), position=0, insert_id=2384851)",
            "Insert(target_node=IN(type=dictionary), node=('pair', None), position=1, insert_id=2384852)",
            "Insert(target_node=IN(type=dictionary), node=('}', '}'), position=2, insert_id=2384853)",
            "Insert(target_node=IN(type=pair), node=('string', \"'input_ids'\"), position=0, insert_id=2384854)",
            "Insert(target_node=IN(type=pair), node=(':', ':'), position=1, insert_id=2384855)",
            "Move(target_node=IN(type=pair), node=ASTNode(type=call), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 2178,
        "neg_line": [
            "-return tf.constant(DUMMY_INPUTS)"
        ],
        "pos_line": [
            "+return {'input_ids': tf.constant(DUMMY_INPUTS)}"
        ],
        "core_change": "-return tf.constant(DUMMY_INPUTS) +return {'input_ids': tf.constant(DUMMY_INPUTS)}",
        "core_API": "constant"
    },
    {
        "commit_hash": "aaa6aa75e9e10de8857324e2142e137e7d7a43b2",
        "index": "6741bf59b..4993a10c8 100644",
        "commit_message": "Fix converting only float type tensors in Lite (#10429)\n\n* fix\n\n* less code\n\n* add test case\n\n* add test cases\n\n* update input\n\n* add test cases\n\n* add type hint\n\n* add changelog note\n\nCo-authored-by: Kaushik B <45285388+kaushikb11@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_lite_module_forward_conversion(precision, input_type, expected_type):",
            "assert precision != 16 or torch.is_autocast_enabled()",
            "return forward_input",
            "",
            "-    module = Mock(wraps=torch.nn.Linear(1, 1), side_effect=check_autocast)",
            "+    module = Mock(wraps=torch.nn.Identity(), side_effect=check_autocast)",
            "lite_module = _LiteModule(module, lite._precision_plugin).to(device)",
            "-    out = lite_module(torch.rand(1, dtype=input_type, device=device))",
            "+    out = lite_module(torch.tensor([1, 2, 3], dtype=input_type, device=device))",
            "assert module.call_args[0][0].dtype == expected_type",
            "-    assert out.dtype == torch.get_default_dtype()",
            "+    assert out.dtype == input_type or out.dtype == torch.get_default_dtype()",
            "",
            "",
            "def test_lite_dataloader_iterator():"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assert_statement), node=('boolean_operator', None), position=1, insert_id=521287)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=0, insert_id=521288)",
            "Insert(target_node=IN(type=boolean_operator), node=('or', 'or'), position=1, insert_id=521289)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=2)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=521290)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=521291)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'input_type'), position=2, insert_id=521292)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'out'), position=0, insert_id=521293)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=521294)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=521295)",
            "Update(target_node=ASTNode(type=identifier, text=rand), value='tensor')",
            "Insert(target_node=ASTNode(type=argument_list), node=('list', None), position=1, insert_id=521296)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=521297)",
            "Update(target_node=ASTNode(type=identifier, text=Linear), value='Identity')",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=521298)",
            "Move(target_node=IN(type=list), node=ASTNode(type=integer, text=1), position=1)",
            "Move(target_node=IN(type=list), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=IN(type=list), node=('integer', '2'), position=3, insert_id=521299)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=4, insert_id=521300)",
            "Insert(target_node=IN(type=list), node=('integer', '3'), position=5, insert_id=521301)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=6, insert_id=521302)",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 24,
        "number": 2179,
        "neg_line": [
            "-module = Mock(wraps=torch.nn.Linear(1, 1), side_effect=check_autocast)",
            "-out = lite_module(torch.rand(1, dtype=input_type, device=device))",
            "-assert out.dtype == torch.get_default_dtype()"
        ],
        "pos_line": [
            "+module = Mock(wraps=torch.nn.Identity(), side_effect=check_autocast)",
            "+out = lite_module(torch.tensor([1, 2, 3], dtype=input_type, device=device))",
            "+assert out.dtype == input_type or out.dtype == torch.get_default_dtype()"
        ],
        "core_change": "-module = Mock(wraps=torch.nn.Linear(1, 1), side_effect=check_autocast) +module = Mock(wraps=torch.nn.Identity(), side_effect=check_autocast) -out = lite_module(torch.rand(1, dtype=input_type, device=device)) +out = lite_module(torch.tensor([1, 2, 3], dtype=input_type, device=device)) -assert out.dtype == torch.get_default_dtype() +assert out.dtype == input_type or out.dtype == torch.get_default_dtype()",
        "core_API": "is_autocast_enabled"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "c555796421..2065f226e3 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TestModules(unittest.TestCase):",
            "that it trains in a supervised setting.\"\"\"",
            "",
            "# Checks that torch and tf embedding matrices are the same",
            "-        with tf.Session().as_default() as sess:",
            "+        with tf1.Session().as_default() as sess:",
            "assert np.allclose(",
            "relative_position_embedding(20, 15).eval(session=sess),",
            "relative_position_embedding_torch(20, 15).numpy())"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2180,
        "neg_line": [
            "-with tf.Session().as_default() as sess:"
        ],
        "pos_line": [
            "+with tf1.Session().as_default() as sess:"
        ],
        "core_change": "-with tf.Session().as_default() as sess: +with tf1.Session().as_default() as sess:",
        "core_API": "Session"
    },
    {
        "commit_hash": "c26e81a22038a84a054c630a4435bcdb995c9e21",
        "index": "67f29220..2f0d905b 100644",
        "commit_message": "enh: Implements `InferenceModule` as a pipelined module with separate preprocessor, predictor, and postprocessor modules (#2105)\n\n* Adding inference pipeline with seperate pre-processing, predict and post-processing modules\n\n* Update to flatten outputs from predict consistent to support triton\n\n* inference module refactor\n\n* add back InferenceLudwigModel\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* unify modules into inference.py\n\n* cleaned up inaccurate documentation\n\n* clean up\n\n* clean up type hints and update InferenceLudwigModel\n\n* clean up type hint; passes test_torchscript.py\n\n* added typing to inference module for clarity\n\n* remove inference_module_file_name constant\n\n* unified predict module with postproc\n\n* removed InferencePredictor entirely\n\n* add back the old inference module\n\n* add back training set metadata\n\n* revert change to predict module, move feature filtering to postproc\n\n* cleanup inference_module_v0\n\n* cleanup\n\n* adds device placement to InferenceLudwigModel\n\n* adds ability to save/load torchscript on particular devices\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* allows saving torchscript with dict of devices from api.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* correct device inputs\n\n* refactor to expose inference stages (prep for triton refactor)\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove magic 'cpu' string\n\n* remove extraneous constants\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add from_directory classmethod for e2e users\n\n* merge\n\n* merge InferenceModule and InferenceLudwigModel\n\n* add comment\n\n* revert small change\n\n* cleanup\n\n* add to_torchscript functionality\n\n* cleanup\n\n* pushes device logic down into inference stages\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* move device placement upstream to inference module to ensure stage modules are performant\n\n* adds logs for device placement experiments\n\n* removes logs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove stage_to_dict\n\n* clean up how we get input device in predictor_forward\n\n* first commit\n\n* wip\n\n* updated interfaces\n\n* postproc GPU\n\n* add intelligent device placement\n\n* clean up device api\n\n* revert flatten op in inference_module_v0\n\n* remove dtype workaround\n\n* benchmarking code\n\n* add DEVICE constant as good default for loading/saving\n\n* added helpful logging and style\n\n* cleanup\n\n* cleanup, adding docstrings\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* docstring\n\nCo-authored-by: Geoffrey Angus <geoffrey@predibase.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class _SequencePostprocessing(torch.nn.Module):",
            "sequence_predictions.append(unit_prediction)",
            "predictions.append(sequence_predictions)",
            "",
            "-        pred_probabilities = preds[self.probabilities_key]",
            "probabilities, _ = torch.max(pred_probabilities, dim=-1)",
            "probability = torch.sum(torch.log(probabilities), dim=-1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=pred_probabilities))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=preds))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=probabilities_key))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 2184,
        "neg_line": [
            "-pred_probabilities = preds[self.probabilities_key]"
        ],
        "pos_line": [],
        "core_change": "-pred_probabilities = preds[self.probabilities_key]",
        "core_API": "append"
    },
    {
        "commit_hash": "7b83a847cc61a25b468f29063cb6425beef6fff6",
        "index": "9b11745..ebcc008 100755",
        "commit_message": "Various fixes\n\n",
        "file": "cnn-text-classification-tf.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "with tf.Graph().as_default():",
            "num_filters=FLAGS.num_filters)",
            "",
            "# Define Training procedure",
            "-        global_step = tf.Variable(0, name=\"global_step\")",
            "+        global_step = tf.Variable(0, name=\"global_step\", trainable=False)",
            "optimizer = tf.train.AdamOptimizer(1e-4)",
            "grads_and_vars = optimizer.compute_gradients(cnn.loss)",
            "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1910919)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1910920)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'trainable'), position=0, insert_id=1910921)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1910922)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=1910923)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2186,
        "neg_line": [
            "-global_step = tf.Variable(0, name=\"global_step\")"
        ],
        "pos_line": [
            "+global_step = tf.Variable(0, name=\"global_step\", trainable=False)"
        ],
        "core_change": "-global_step = tf.Variable(0, name=\"global_step\") +global_step = tf.Variable(0, name=\"global_step\", trainable=False)",
        "core_API": "Graph"
    },
    {
        "commit_hash": "2cdfbd18a750cabaff1499fad7473dd91f3c7fa7",
        "index": "19c52fa2..3be22f91 100644",
        "commit_message": "Previously, many unit test files started with `enable_v2_behavior`, which would have caused them to run in V2 mode when executing with a V1 test flag. The correct behavior would in fact be to skip such tests when executing with a V1 test flag.\n\nThis fix significantly reduces the total V1 + V2 test load by eliminating redundancy.\n\nPiperOrigin-RevId: 424734850\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class IntegerLookupSavingTest(keras_parameterized.TestCase,",
            "",
            "",
            "if __name__ == \"__main__\":",
            "-  # IntegerLookup is only exported as a TF2 API.",
            "-  tf.compat.v1.enable_v2_behavior()",
            "tf.test.main()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', ''), position=0, insert_id=2069041)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=compat))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=v1))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=enable_v2_behavior))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 2187,
        "neg_line": [
            "-# IntegerLookup is only exported as a TF2 API.",
            "-tf.compat.v1.enable_v2_behavior()"
        ],
        "pos_line": [],
        "core_change": "-# IntegerLookup is only exported as a TF2 API. -tf.compat.v1.enable_v2_behavior()",
        "core_API": "enable_v2_behavior"
    },
    {
        "commit_hash": "23efadd9a394d9d69e10e8539a7332665b118df0",
        "index": "97f7c7e6..ad0ab0b3 100644",
        "commit_message": "upgrade to pytorch 1.2 (#3182)\n\n* first attempt at pytorch 1.2\n\n* explicit is better than implicit\n\n* more explicit\n\n* attempt to fix flaky tests\n\n* pylint\n\n* no disable dropout\n\n* disable dropout by default\n\n* restore dropout, don't deepcopy\n\n* change batch size for biaffine_dependency_parser_multilang_test, maybe that will make it pass? :(\n\n* try batch size 10\n\n* ignore bad gradient parameter\n\n* cleanup\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Auc(Metric):",
            "if mask is None:",
            "batch_size = gold_labels.shape[0]",
            "mask = torch.ones(batch_size)",
            "-        mask = mask.byte()",
            "+        mask = mask.to(dtype=torch.bool)",
            "",
            "self._all_predictions = torch.cat([self._all_predictions,",
            "torch.masked_select(predictions, mask).float()], dim=0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=byte), value='to')",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=24361)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=24362)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=24363)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=24364)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=24365)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=24366)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=24367)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2189,
        "neg_line": [
            "-mask = mask.byte()"
        ],
        "pos_line": [
            "+mask = mask.to(dtype=torch.bool)"
        ],
        "core_change": "-mask = mask.byte() +mask = mask.to(dtype=torch.bool)",
        "core_API": "ones"
    },
    {
        "commit_hash": "0dfa0d52ebc7c94f17d2d6140bc993176072aa5c",
        "index": "4e82f54f..c0d4884b 100644",
        "commit_message": "Use Empirical Distribution in TracePosterior (#1019)\n\n* Use Empirical Distribution in TracePosterior\n\n* address comments\n\n* fix mcmc test\n\n* Remove skipif; not needed\n\n* fix inclined plane\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MCMC(TracePosterior):",
            "if t == self.warmup_steps:",
            "self.kernel.end_warmup()",
            "continue",
            "-            yield (trace, torch.tensor([1.0]))",
            "+            yield (trace, 1.0)",
            "self.kernel.cleanup()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=float, text=1.0), position=3)",
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=), text=)), position=4)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 2190,
        "neg_line": [
            "-yield (trace, torch.tensor([1.0]))"
        ],
        "pos_line": [
            "+yield (trace, 1.0)"
        ],
        "core_change": "-yield (trace, torch.tensor([1.0])) +yield (trace, 1.0)",
        "core_API": "end_warmup"
    },
    {
        "commit_hash": "0973cc578e3a96346f6ac5c151491684b956983e",
        "index": "9c70a2867..ec09b7723 100644",
        "commit_message": "Fix import error when `distributed` module not available (#12794)\n\n* add distributed.is_available checks to avoid errors when not available\n\n* Update CHANGELOG\n\n* Update pytorch_lightning/strategies/ddp.py\n\nCo-authored-by: Akihiro Nitta <nitta@akihironitta.com>\n\n* Update pytorch_lightning/strategies/ddp.py\n\nCo-authored-by: Rohit Gupta <rohitgr1998@gmail.com>\n\nCo-authored-by: Akihiro Nitta <nitta@akihironitta.com>\nCo-authored-by: Rohit Gupta <rohitgr1998@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "if torch.distributed.is_available():",
            "",
            "",
            "# Taken from https://github.com/pytorch/pytorch/blob/3466c1b6901f06a563b8cbfa3c942fa50bda835b/torch/distributed/distributed_c10d.py#L267 # noqa: E501",
            "-def _rank_not_in_group(group: ProcessGroup):",
            "+def _rank_not_in_group(group: \"ProcessGroup\"):",
            "\"\"\"Helper that checks if the current process's rank is not in a given group.\"\"\"",
            "if group is None:",
            "return False"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('type', None), position=0, insert_id=509387)",
            "Insert(target_node=IN(type=type), node=('string', '\"ProcessGroup\"'), position=0, insert_id=509388)",
            "Delete(target_node=ASTNode(type=identifier, text=ProcessGroup))",
            "Delete(target_node=ASTNode(type=type))"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 4,
        "number": 2192,
        "neg_line": [
            "-def _rank_not_in_group(group: ProcessGroup):"
        ],
        "pos_line": [
            "+def _rank_not_in_group(group: \"ProcessGroup\"):"
        ],
        "core_change": "-def _rank_not_in_group(group: ProcessGroup): +def _rank_not_in_group(group: \"ProcessGroup\"):",
        "core_API": "is_available"
    },
    {
        "commit_hash": "1701b76a31e3e8c97d51b49dfcaa060762ab3764",
        "index": "1b81c38..74a8dd6 100644",
        "commit_message": "another meshgrid fix for old pytorch\n\nSummary: Try to fix circleci again.\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D34752188\n\nfbshipit-source-id: 5966c585b61d77df1d8dd97c24383cf74dfb1fae\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def meshgrid_ij(",
            "\"\"\"",
            "Like torch.meshgrid was before PyTorch 1.10.0, i.e. with indexing set to ij",
            "\"\"\"",
            "-    if \"indexing\" in torch.meshgrid.__kwdefaults__:",
            "+    if (",
            "+        torch.meshgrid.__kwdefaults__ is not None",
            "+        and \"indexing\" in torch.meshgrid.__kwdefaults__",
            "+    ):",
            "# PyTorch >= 1.10.0",
            "return torch.meshgrid(*A, indexing=\"ij\")",
            "return torch.meshgrid(*A)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('parenthesized_expression', None), position=1, insert_id=914574)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=914575)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('boolean_operator', None), position=1, insert_id=914576)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=914577)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=0, insert_id=914578)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=914579)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=2)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=914580)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=914581)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=914582)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=914583)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=914584)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=914585)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__kwdefaults__'), position=2, insert_id=914586)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=914587)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=914588)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'meshgrid'), position=2, insert_id=914589)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2193,
        "neg_line": [
            "-if \"indexing\" in torch.meshgrid.__kwdefaults__:"
        ],
        "pos_line": [
            "+if (",
            "+torch.meshgrid.__kwdefaults__ is not None",
            "+and \"indexing\" in torch.meshgrid.__kwdefaults__",
            "+):"
        ],
        "core_change": "-if \"indexing\" in torch.meshgrid.__kwdefaults__: +if ( +torch.meshgrid.__kwdefaults__ is not None +and \"indexing\" in torch.meshgrid.__kwdefaults__ +):",
        "core_API": "meshgrid"
    },
    {
        "commit_hash": "0bcf64874ff6cbcd4c46b1ddd4995e1fcf8de08a",
        "index": "162d231..1b75e8f 100644",
        "commit_message": "Fix no detection case\n\n",
        "file": "face-alignment.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def detect(net, img, device):",
            "# Creates a batch of 1",
            "img = img.reshape((1,) + img.shape)",
            "",
            "-    if 'cuda' in device:",
            "-        torch.backends.cudnn.benchmark = True",
            "-",
            "img = torch.from_numpy(img).float().to(device)",
            "",
            "return batch_detect(net, img, device)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=string, text='cuda'))",
            "Delete(target_node=ASTNode(type=in, text=in))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=backends))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cudnn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=benchmark))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 2194,
        "neg_line": [
            "-if 'cuda' in device:",
            "-torch.backends.cudnn.benchmark = True",
            "-"
        ],
        "pos_line": [],
        "core_change": "-if 'cuda' in device: -torch.backends.cudnn.benchmark = True -",
        "core_API": "reshape"
    },
    {
        "commit_hash": "090bbc8605ed6aca80a509185ff341b5895275db",
        "index": "2916d8b07..5086583d8 100644",
        "commit_message": "Fix mypy errors attributed to `pytorch_lightning.core.module.py` (#13603)\n\n\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\nCo-authored-by: Carlos Mochol <carlossmocholi@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DeviceDtypeModuleMixin(Module):",
            "raise RuntimeError(\"Cannot set the dtype explicitly. Please use module.to(new_dtype).\")",
            "",
            "@property",
            "-    def device(self) -> Union[str, torch.device]:",
            "+    def device(self) -> torch.device:",
            "device = self._device",
            "",
            "# make this more explicit to always include the index"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=type), node=ASTNode(type=attribute), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=Union))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=str))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 2195,
        "neg_line": [
            "-def device(self) -> Union[str, torch.device]:"
        ],
        "pos_line": [
            "+def device(self) -> torch.device:"
        ],
        "core_change": "-def device(self) -> Union[str, torch.device]: +def device(self) -> torch.device:",
        "core_API": "to"
    },
    {
        "commit_hash": "7732d0fe7a759c9844215920e9f1c5540eafb1a6",
        "index": "b72040801..b3d9a8506 100644",
        "commit_message": "Upgrade black to version ~=22.0 (#15565)\n\n* Upgrade black to version ~=22.0\n\n* Check copies\n\n* Fix code\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFFunnelRelMultiheadAttention(tf.keras.layers.Layer):",
            "",
            "self.post_proj = tf.keras.layers.Dense(d_model, kernel_initializer=initializer, name=\"post_proj\")",
            "self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")",
            "-        self.scale = 1.0 / (d_head ** 0.5)",
            "+        self.scale = 1.0 / (d_head**0.5)",
            "",
            "def build(self, input_shape):",
            "n_head, d_head, d_model = self.n_head, self.d_head, self.d_model"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2199,
        "neg_line": [
            "-self.scale = 1.0 / (d_head ** 0.5)"
        ],
        "pos_line": [
            "+self.scale = 1.0 / (d_head**0.5)"
        ],
        "core_change": "-self.scale = 1.0 / (d_head ** 0.5) +self.scale = 1.0 / (d_head**0.5)",
        "core_API": "Dense"
    },
    {
        "commit_hash": "d14954eec4ba2350f0dd7d728f732af369654c26",
        "index": "30a7c673..e42e28da 100644",
        "commit_message": "test_dataset pytorch tensorflow tests fixed\n\n",
        "file": "deeplake.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def test_to_backend_with_tf_and_pytorch():",
            "break",
            "",
            "",
            "-def test_to_backend_with_tf_and_pytorch():",
            "+def test_to_backend_with_tf_and_pytorch_multiworker():",
            "try:",
            "import torch",
            "-        import tensorflow",
            "+        import tensorflow as tf",
            "except ImportError:",
            "print(\"Pytorch hasn't been imported and tested\")",
            "return",
            "",
            "+    tf.compat.v1.enable_eager_execution()",
            "ds = dataset.load(\"mnist/mnist\")",
            "",
            "tfds = ds.to_tensorflow().batch(8)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=9, insert_id=1922755)",
            "Update(target_node=ASTNode(type=identifier, text=test_to_backend_with_tf_and_pytorch), value='test_to_backend_with_tf_and_pytorch_multiworker')",
            "Insert(target_node=ASTNode(type=import_statement), node=('aliased_import', None), position=1, insert_id=1922756)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1922757)",
            "Move(target_node=IN(type=aliased_import), node=ASTNode(type=dotted_name), position=0)",
            "Insert(target_node=IN(type=aliased_import), node=('as', 'as'), position=1, insert_id=1922758)",
            "Insert(target_node=IN(type=aliased_import), node=('identifier', 'tf'), position=2, insert_id=1922759)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1922760)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1922761)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1922762)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1922763)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'enable_eager_execution'), position=2, insert_id=1922764)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1922765)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1922766)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1922767)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1922768)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'v1'), position=2, insert_id=1922769)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1922770)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1922771)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'compat'), position=2, insert_id=1922772)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 2201,
        "neg_line": [
            "-def test_to_backend_with_tf_and_pytorch():",
            "-import tensorflow"
        ],
        "pos_line": [
            "+def test_to_backend_with_tf_and_pytorch_multiworker():",
            "+import tensorflow as tf",
            "+tf.compat.v1.enable_eager_execution()"
        ],
        "core_change": "-def test_to_backend_with_tf_and_pytorch(): +def test_to_backend_with_tf_and_pytorch_multiworker(): -import tensorflow +import tensorflow as tf +tf.compat.v1.enable_eager_execution()",
        "core_API": "enable_eager_execution"
    },
    {
        "commit_hash": "3bb1128c53ac7fa39d065297ca8a370571237e7e",
        "index": "e009ded524..8a87d9db32 100644",
        "commit_message": "docstring fix\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def fft(",
            "*,",
            "norm: Optional[str] = \"backward\",",
            "n: Union[int, Tuple[int]] = None,",
            "-    out: Optional[Union[tf.Tensor, tf.Variable]] = None",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "if not isinstance(dim, int):",
            "raise ivy.exceptions.IvyError(f\"Expecting <class 'int'> instead of {type(dim)}\")",
            "if n is None:",
            "n = x.shape[dim]",
            "-    if n < -len(x.shape) :",
            "+    if n < -len(x.shape):",
            "raise ivy.exceptions.IvyError(",
            "f\"Invalid dim {dim}, expecting ranging\"",
            "\" from {-len(x.shape)} to {len(x.shape)-1}  \"",
            ")",
            "if not isinstance(n, int):",
            "raise ivy.exceptions.IvyError(f\"Expecting <class 'int'> instead of {type(n)}\")",
            "-    if n <= 1 :",
            "+    if n <= 1:",
            "raise ivy.exceptions.IvyError(f\"Invalid data points {n}, expecting more than 1\")",
            "if norm != \"backward\" and norm != \"ortho\" and norm != \"forward\":",
            "raise ivy.exceptions.IvyError(f\"Unrecognized normalization mode {norm}\")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=1974775)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 1,
        "number": 2204,
        "neg_line": [
            "-out: Optional[Union[tf.Tensor, tf.Variable]] = None",
            "-if n < -len(x.shape) :",
            "-if n <= 1 :"
        ],
        "pos_line": [
            "+out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            "+if n < -len(x.shape):",
            "+if n <= 1:"
        ],
        "core_change": "-out: Optional[Union[tf.Tensor, tf.Variable]] = None +out: Optional[Union[tf.Tensor, tf.Variable]] = None, -if n < -len(x.shape) : +if n < -len(x.shape): -if n <= 1 : +if n <= 1:",
        "core_API": "IvyError"
    },
    {
        "commit_hash": "a5184744689a2e5b82d12ce0fc6d1fe5e31b3be5",
        "index": "3a22bc5ac..0becd47b8 100644",
        "commit_message": "fix docstring\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "dataloader",
        "change": [
            "class TransformDataset(torch.utils.data.Dataset):",
            "",
            "",
            "class ChainerDataLoader(object):",
            "+    \"\"\"Pytorch dataloader in chainer style.",
            "+",
            "+    Args:",
            "+        all args for torch.utils.data.dataloader.Dataloader",
            "+",
            "+    \"\"\"",
            "+",
            "def __init__(self, **kwargs):",
            "self.loader = torch.utils.data.dataloader.DataLoader(**kwargs)",
            "self.len = len(kwargs['dataset'])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=1761333)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1761334)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"Pytorch dataloader in chainer style.\\n\\n    Args:\\n        all args for torch.utils.data.dataloader.Dataloader\\n\\n    \"\"\"'), position=0, insert_id=1761335)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 4,
        "minus_line": 0,
        "AST_diff_line": 4,
        "number": 2205,
        "neg_line": [],
        "pos_line": [
            "+\"\"\"Pytorch dataloader in chainer style.",
            "+",
            "+Args:",
            "+all args for torch.utils.data.dataloader.Dataloader",
            "+",
            "+\"\"\"",
            "+"
        ],
        "core_change": "+\"\"\"Pytorch dataloader in chainer style. + +Args: +all args for torch.utils.data.dataloader.Dataloader + +\"\"\" +",
        "core_API": "DataLoader"
    },
    {
        "commit_hash": "0dfd6b624081fc4e1c72fc74ae0cd2de199c334c",
        "index": "3a785e18..4d6073a9 100644",
        "commit_message": "Add linting with black (#2678)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2678\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D32653381\n\nPulled By: dianaml0\n\nfbshipit-source-id: 2810d14867cd7d64f4d340740e2b590b82de47fe\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestGradientScalingAMP(unittest.TestCase):",
            "self.scaler.update()",
            "self.assertEqual(",
            "model.weight,",
            "-            torch.tensor(",
            "-                [[3.1]], device=\"cuda:0\", requires_grad=True",
            "-            ),",
            "+            torch.tensor([[3.1]], device=\"cuda:0\", requires_grad=True),",
            ")",
            "self.assertEqual(",
            "model.bias,",
            "-            torch.tensor(",
            "-                [5.1], device=\"cuda:0\", requires_grad=True",
            "-            ),",
            "+            torch.tensor([5.1], device=\"cuda:0\", requires_grad=True),",
            ")",
            "self.assertEqual(self.scaler.get_scale(), 2.0)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 6,
        "AST_diff_line": 17,
        "number": 2206,
        "neg_line": [
            "-torch.tensor(",
            "-[[3.1]], device=\"cuda:0\", requires_grad=True",
            "-),",
            "-torch.tensor(",
            "-[5.1], device=\"cuda:0\", requires_grad=True",
            "-),"
        ],
        "pos_line": [
            "+torch.tensor([[3.1]], device=\"cuda:0\", requires_grad=True),",
            "+torch.tensor([5.1], device=\"cuda:0\", requires_grad=True),"
        ],
        "core_change": "-torch.tensor( -[[3.1]], device=\"cuda:0\", requires_grad=True -), +torch.tensor([[3.1]], device=\"cuda:0\", requires_grad=True), -torch.tensor( -[5.1], device=\"cuda:0\", requires_grad=True -), +torch.tensor([5.1], device=\"cuda:0\", requires_grad=True),",
        "core_API": "update"
    },
    {
        "commit_hash": "e386caa07107a77c7dfb2301c8ba60f05d0211da",
        "index": "656d9575..2fc9e86a 100644",
        "commit_message": "mass linter fix\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Decoder(nn.Module):",
            "if t > inputs.shape[1] / 4 and (stop_token > 0.6",
            "or attention[:, -1].item() > 0.6):",
            "break",
            "-            elif t > self.max_decoder_steps:",
            "+            if t > self.max_decoder_steps:",
            "print(\"   | > Decoder stopped with 'max_decoder_steps\")",
            "break",
            "return self._parse_outputs(outputs, attentions, stop_tokens)"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=2)",
            "Insert(target_node=ASTNode(type=module), node=('break_statement', None), position=2, insert_id=1268607)",
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=1268608)",
            "Insert(target_node=IN(type=break_statement), node=('break', 'break'), position=0, insert_id=1268609)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1268610)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1268611)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=:, text=:), position=2)",
            "Insert(target_node=IN(type=if_statement), node=('block', ''), position=3, insert_id=1268612)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Move(target_node=IN(type=comparison_operator), node=ASTNode(type=identifier, text=t), position=0)",
            "Move(target_node=IN(type=comparison_operator), node=ASTNode(type=>, text=>), position=1)",
            "Move(target_node=IN(type=comparison_operator), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=elif))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=break, text=break))",
            "Delete(target_node=ASTNode(type=break_statement))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 2208,
        "neg_line": [
            "-elif t > self.max_decoder_steps:"
        ],
        "pos_line": [
            "+if t > self.max_decoder_steps:"
        ],
        "core_change": "-elif t > self.max_decoder_steps: +if t > self.max_decoder_steps:",
        "core_API": "_parse_outputs"
    },
    {
        "commit_hash": "b6204c9e9b0c03a5a1209842d77a05eadbeb007e",
        "index": "b5dda8762..f51b88856 100644",
        "commit_message": "fix warnings in deberta (#19458)\n\n* fix warnings in deberta\n\n* fix copies\n\n* Revert \"fix copies\"\n\nThis reverts commit 324cb3fed11e04190ba7b4662644baa8143b60be.\n\n* fix copies\n\n* fix copies again\n\n* revert changes to whitespace that make style did since it results in an infinite chain of fix-copies\n\n* argh\n\nCo-authored-by: Sander Land <sander@chatdesk.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DisentangledSelfAttention(nn.Module):",
            "dim=-1,",
            "index=p2c_pos.squeeze(0).expand([query_layer.size(0), key_layer.size(-2), key_layer.size(-2)]),",
            ").transpose(-1, -2)",
            "-            score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)",
            "+            score += p2c_att / scale.to(dtype=p2c_att.dtype)",
            "",
            "return score"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='scale')",
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='to')",
            "Delete(target_node=ASTNode(type=identifier, text=scale))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 2209,
        "neg_line": [
            "-score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)"
        ],
        "pos_line": [
            "+score += p2c_att / scale.to(dtype=p2c_att.dtype)"
        ],
        "core_change": "-score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype) +score += p2c_att / scale.to(dtype=p2c_att.dtype)",
        "core_API": "squeeze"
    },
    {
        "commit_hash": "00b5688ecb1535f844ee4575ec590f2166dd8a62",
        "index": "8917824b..293a9be3 100644",
        "commit_message": "Revise Dirichlet distribution (#163)\n\n* Revise Dirichlet distribution\n\n* Updates per review\n\n* Fix indexing bug using scipy.stats.dirichlet.rvs\n\n* Remove egregious assertion\n\n* Fix expansion bug\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def log_gamma(xx):",
            "",
            "",
            "def log_beta(t):",
            "+    \"\"\"",
            "+    Computes log Beta function.",
            "+",
            "+    :param t:",
            "+    :type t: torch.autograd.Variable of dimension 1 or 2",
            "+    :rtype: torch.autograd.Variable of float (if t.dim() == 1) or torch.Tensor (if t.dim() == 2)",
            "+    \"\"\"",
            "+    assert t.dim() in (1, 2)",
            "if t.dim() == 1:",
            "numer = torch.sum(log_gamma(t))",
            "denom = log_gamma(torch.sum(t))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=765443)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=765444)",
            "Insert(target_node=IN(type=block), node=('assert_statement', None), position=1, insert_id=765445)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"\\n    Computes log Beta function.\\n\\n    :param t:\\n    :type t: torch.autograd.Variable of dimension 1 or 2\\n    :rtype: torch.autograd.Variable of float (if t.dim() == 1) or torch.Tensor (if t.dim() == 2)\\n    \"\"\"'), position=0, insert_id=765446)",
            "Insert(target_node=IN(type=assert_statement), node=('assert', 'assert'), position=0, insert_id=765447)",
            "Insert(target_node=IN(type=assert_statement), node=('comparison_operator', None), position=1, insert_id=765448)",
            "Insert(target_node=IN(type=comparison_operator), node=('call', None), position=0, insert_id=765449)",
            "Insert(target_node=IN(type=comparison_operator), node=('in', 'in'), position=1, insert_id=765450)",
            "Insert(target_node=IN(type=comparison_operator), node=('tuple', None), position=2, insert_id=765451)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=765452)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=765453)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=765454)",
            "Insert(target_node=IN(type=tuple), node=('integer', '1'), position=1, insert_id=765455)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=765456)",
            "Insert(target_node=IN(type=tuple), node=('integer', '2'), position=3, insert_id=765457)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=4, insert_id=765458)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 't'), position=0, insert_id=765459)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=765460)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dim'), position=2, insert_id=765461)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=765462)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=765463)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 7,
        "minus_line": 0,
        "AST_diff_line": 22,
        "number": 2211,
        "neg_line": [],
        "pos_line": [
            "+\"\"\"",
            "+Computes log Beta function.",
            "+",
            "+:param t:",
            "+:type t: torch.autograd.Variable of dimension 1 or 2",
            "+:rtype: torch.autograd.Variable of float (if t.dim() == 1) or torch.Tensor (if t.dim() == 2)",
            "+\"\"\"",
            "+assert t.dim() in (1, 2)"
        ],
        "core_change": "+\"\"\" +Computes log Beta function. + +:param t: +:type t: torch.autograd.Variable of dimension 1 or 2 +:rtype: torch.autograd.Variable of float (if t.dim() == 1) or torch.Tensor (if t.dim() == 2) +\"\"\" +assert t.dim() in (1, 2)",
        "core_API": "dim"
    },
    {
        "commit_hash": "c4c532d25e012dbe6ab1ac14bca75e53e0acc621",
        "index": "f41ba8e9..64fd5cf4 100644",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DepLabelIndexer(TokenIndexer[int]):",
            "return {index_name: [vocabulary.get_token_index(dep_label, self.namespace) for dep_label in dep_labels]}",
            "",
            "@overrides",
            "-    def get_padding_lengths(self, token: int) -> Dict[str, int]:  # pylint: disable=unused-argument",
            "+    def get_padding_lengths(self, token: int) -> Dict[str, int]:",
            "return {}",
            "",
            "@overrides",
            "def as_padded_tensor(self,",
            "tokens: Dict[str, List[int]],",
            "desired_num_tokens: Dict[str, int],",
            "-                         padding_lengths: Dict[str, int]) -> Dict[str, torch.Tensor]:  # pylint: disable=unused-argument",
            "+                         padding_lengths: Dict[str, int]) -> Dict[str, torch.Tensor]:",
            "return {key: torch.LongTensor(pad_sequence_to_length(val, desired_num_tokens[key]))",
            "for key, val in tokens.items()}"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 2212,
        "neg_line": [
            "-def get_padding_lengths(self, token: int) -> Dict[str, int]:  # pylint: disable=unused-argument",
            "-padding_lengths: Dict[str, int]) -> Dict[str, torch.Tensor]:  # pylint: disable=unused-argument"
        ],
        "pos_line": [
            "+def get_padding_lengths(self, token: int) -> Dict[str, int]:",
            "+padding_lengths: Dict[str, int]) -> Dict[str, torch.Tensor]:"
        ],
        "core_change": "-def get_padding_lengths(self, token: int) -> Dict[str, int]:  # pylint: disable=unused-argument +def get_padding_lengths(self, token: int) -> Dict[str, int]: -padding_lengths: Dict[str, int]) -> Dict[str, torch.Tensor]:  # pylint: disable=unused-argument +padding_lengths: Dict[str, int]) -> Dict[str, torch.Tensor]:",
        "core_API": "get_token_index"
    },
    {
        "commit_hash": "6e9035d42eea31cad87a7c8b87fc79635a6df7c2",
        "index": "4bfeeb96c..3a7b1e079 100644",
        "commit_message": "fix\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def make_pad_mask(lengths, xs=None, length_dim=-1, maxlen=None):",
            "if not isinstance(lengths, list):",
            "lengths = lengths.tolist()",
            "else:",
            "-        assert isinstance(lengths, torch.tensor), type(lengths)",
            "+        assert isinstance(lengths, torch.Tensor), type(lengths)",
            "lengths = lengths.long()",
            "",
            "bs = int(len(lengths))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='Tensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2214,
        "neg_line": [
            "-assert isinstance(lengths, torch.tensor), type(lengths)"
        ],
        "pos_line": [
            "+assert isinstance(lengths, torch.Tensor), type(lengths)"
        ],
        "core_change": "-assert isinstance(lengths, torch.tensor), type(lengths) +assert isinstance(lengths, torch.Tensor), type(lengths)",
        "core_API": "tolist"
    },
    {
        "commit_hash": "a36efd3cbf80d79af60248bbb68a3f77aa99ef32",
        "index": "4e938aa..2869590 100644",
        "commit_message": "Fixes #32 address accuracy issue\n\n",
        "file": "face-alignment.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class FAN(nn.Module):",
            "",
            "def forward(self, x):",
            "x = F.relu(self.bn1(self.conv1(x)), True)",
            "-        x = F.max_pool2d(self.conv2(x), 2)",
            "+        x = F.avg_pool2d(self.conv2(x), 2, stride=2)",
            "x = self.conv3(x)",
            "x = self.conv4(x)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=max_pool2d), value='avg_pool2d')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1355418)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1355419)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'stride'), position=0, insert_id=1355420)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1355421)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '2'), position=2, insert_id=1355422)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 2215,
        "neg_line": [
            "-x = F.max_pool2d(self.conv2(x), 2)"
        ],
        "pos_line": [
            "+x = F.avg_pool2d(self.conv2(x), 2, stride=2)"
        ],
        "core_change": "-x = F.max_pool2d(self.conv2(x), 2) +x = F.avg_pool2d(self.conv2(x), 2, stride=2)",
        "core_API": "relu"
    },
    {
        "commit_hash": "671e3e13fc4ead96a340d0fa09e8032584d1fce8",
        "index": "b4d40b9c..c0175606 100644",
        "commit_message": "Fix unit tests\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DefaultClassifier(Classifier[DT], typing.Generic[DT, DT2]):",
            "else:",
            "self._multi_label_threshold = {\"default\": x}",
            "",
            "-    # def get_scores_and_labels(self, batch: List[DT]) -> Tuple[torch.Tensor, List[List[str]]]:",
            "-    #     batch = [dp for dp in batch if self._filter_data_point(dp)]",
            "-    #     predict_data_points = self._get_prediction_data_points(batch)",
            "-    #     labels = [self._get_label_of_datapoint(pdp) for pdp in predict_data_points]",
            "-    #     embedded_tensor = self._prepare_tensors(batch)",
            "-    #     logits = self._transform_embeddings(*embedded_tensor)",
            "-    #     return logits, labels",
            "-",
            "def _prepare_label_tensor(self, prediction_data_points: List[DT2]) -> torch.Tensor:",
            "labels = [self._get_label_of_datapoint(dp) for dp in prediction_data_points]",
            "if self.multi_label:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 7,
        "AST_diff_line": 17,
        "number": 2216,
        "neg_line": [
            "-# def get_scores_and_labels(self, batch: List[DT]) -> Tuple[torch.Tensor, List[List[str]]]:",
            "-#     batch = [dp for dp in batch if self._filter_data_point(dp)]",
            "-#     predict_data_points = self._get_prediction_data_points(batch)",
            "-#     labels = [self._get_label_of_datapoint(pdp) for pdp in predict_data_points]",
            "-#     embedded_tensor = self._prepare_tensors(batch)",
            "-#     logits = self._transform_embeddings(*embedded_tensor)",
            "-#     return logits, labels",
            "-"
        ],
        "pos_line": [],
        "core_change": "-# def get_scores_and_labels(self, batch: List[DT]) -> Tuple[torch.Tensor, List[List[str]]]: -#     batch = [dp for dp in batch if self._filter_data_point(dp)] -#     predict_data_points = self._get_prediction_data_points(batch) -#     labels = [self._get_label_of_datapoint(pdp) for pdp in predict_data_points] -#     embedded_tensor = self._prepare_tensors(batch) -#     logits = self._transform_embeddings(*embedded_tensor) -#     return logits, labels -",
        "core_API": "_filter_data_point"
    },
    {
        "commit_hash": "32484500befa825b3def2cdc15cb12eb2251f681",
        "index": "f57cd1a..3c4c825 100644",
        "commit_message": "tutorial fixes from #336. Wheels with cuda10.1.\n\nSummary:\nAdd a document to explain how to run the tutorials.\nFix API of TexturesVertex in fit_textured_mesh.\nPrepare cuda 10.1 wheels (not 10.2) for linux to be available on pypi - this matches what colab has.\nChange the tutorials to use these new wheels.\n\nReviewed By: gkioxari\n\nDifferential Revision: D23324479\n\nfbshipit-source-id: 60e92a3f46a2d878f811b7703638f8d1dae143d9\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "\"    new_src_mesh = src_mesh.offset_verts(deform_verts)\\n\",",
            "\"    \\n\",",
            "\"    # Add per vertex colors to texture the mesh\\n\",",
            "-    \"    new_src_mesh.textures = TexturesVertex(verts_rgb=sphere_verts_rgb) \\n\",",
            "+    \"    new_src_mesh.textures = TexturesVertex(verts_features=sphere_verts_rgb) \\n\",",
            "\"    \\n\",",
            "\"    # Losses to smooth /regularize the mesh shape\\n\",",
            "\"    loss = {k: torch.tensor(0.0, device=device) for k in losses}\\n\","
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"    new_src_mesh.textures = TexturesVertex(verts_rgb=sphere_verts_rgb) \\n\"), value='\"    new_src_mesh.textures = TexturesVertex(verts_features=sphere_verts_rgb) \\\\n\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2218,
        "neg_line": [
            "-\"    new_src_mesh.textures = TexturesVertex(verts_rgb=sphere_verts_rgb) \\n\","
        ],
        "pos_line": [
            "+\"    new_src_mesh.textures = TexturesVertex(verts_features=sphere_verts_rgb) \\n\","
        ],
        "core_change": "-\"    new_src_mesh.textures = TexturesVertex(verts_rgb=sphere_verts_rgb) \\n\", +\"    new_src_mesh.textures = TexturesVertex(verts_features=sphere_verts_rgb) \\n\",",
        "core_API": "offset_verts"
    },
    {
        "commit_hash": "bc67689068a0db11adaf10b32a41bcd33b8ae88e",
        "index": "73da794f7..ccb50ef3e 100644",
        "commit_message": "clean v2 docs (#691)\n\n* updated gitignore\n\n* Update README.md\n\n* updated gitignore\n\n* updated links in ninja file\n\n* updated docs\n\n* Update README.md\n\n* Update README.md\n\n* finished callbacks\n\n* finished callbacks\n\n* finished callbacks\n\n* fixed left menu\n\n* added callbacks to menu\n\n* added direct links to docs\n\n* added direct links to docs\n\n* added direct links to docs\n\n* added direct links to docs\n\n* added direct links to docs\n\n* fixing TensorBoard (#687)\n\n* flake8\n\n* fix typo\n\n* fix tensorboardlogger\ndrop test_tube dependence\n\n* formatting\n\n* fix tensorboard & tests\n\n* upgrade Tensorboard\n\n* test formatting separately\n\n* try to fix JIT issue\n\n* add tests for 1.4\n\n* added direct links to docs\n\n* updated gitignore\n\n* updated links in ninja file\n\n* updated docs\n\n* finished callbacks\n\n* finished callbacks\n\n* finished callbacks\n\n* fixed left menu\n\n* added callbacks to menu\n\n* added direct links to docs\n\n* added direct links to docs\n\n* added direct links to docs\n\n* added direct links to docs\n\n* added direct links to docs\n\n* added direct links to docs\n\n* finished rebase\n\n* making private  members\n\n* making private  members\n\n* making private  members\n\n* working on trainer docs\n\n* working on trainer docs\n\n* working on trainer docs\n\n* working on trainer docs\n\n* working on trainer docs\n\n* working on trainer docs\n\n* set auto dp if no backend\n\n* working on trainer docs\n\n* working on trainer docs\n\n* working on trainer docs\n\n* working on trainer docs\n\n* working on trainer docs\n\n* working on trainer docs\n\n* working on trainer docs\n\n* working on trainer docs\n\n* fixed lightning import\n\n* cleared  spaces\n\n* cleared  spaces\n\n* cleared  spaces\n\n* cleared  spaces\n\n* cleared  spaces\n\n* cleared  spaces\n\n* cleared  spaces\n\n* cleared  spaces\n\n* cleared  spaces\n\n* cleared  spaces\n\n* finished lightning module\n\n* finished lightning module\n\n* finished lightning module\n\n* finished lightning module\n\n* added callbacks\n\n* added loggers\n\n* added loggers\n\n* added loggers\n\n* added loggers\n\n* added loggers\n\n* added loggers\n\n* added loggers\n\n* added loggers\n\n* set auto dp if no backend\n\n* added loggers\n\n* added loggers\n\n* added loggers\n\n* added loggers\n\n* added loggers\n\n* added loggers\n\n* flake 8\n\n* flake 8\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestTubeLogger(LightningLoggerBase):",
            "",
            "@property",
            "def experiment(self):",
            "+        r\"\"\"",
            "+",
            "+          Actual test-tube object. To use test-tube features do the following.",
            "+",
            "+          Example::",
            "+",
            "+              self.logger.experiment.some_test_tube_function()",
            "+",
            "+          \"\"\"",
            "+",
            "if self._experiment is not None:",
            "return self._experiment"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=587399)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=587400)",
            "Insert(target_node=IN(type=expression_statement), node=('string', 'r\"\"\"\\n\\n          Actual test-tube object. To use test-tube features do the following.\\n\\n          Example::\\n\\n              self.logger.experiment.some_test_tube_function()\\n\\n          \"\"\"'), position=0, insert_id=587401)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 5,
        "minus_line": 0,
        "AST_diff_line": 4,
        "number": 2224,
        "neg_line": [],
        "pos_line": [
            "+r\"\"\"",
            "+",
            "+Actual test-tube object. To use test-tube features do the following.",
            "+",
            "+Example::",
            "+",
            "+self.logger.experiment.some_test_tube_function()",
            "+",
            "+\"\"\"",
            "+"
        ],
        "core_change": "+r\"\"\" + +Actual test-tube object. To use test-tube features do the following. + +Example:: + +self.logger.experiment.some_test_tube_function() + +\"\"\" +",
        "core_API": "some_test_tube_function"
    },
    {
        "commit_hash": "34263ab196cbfab65f2afa1150e6cf6db5ffd8b6",
        "index": "71ec38513..ddda3309f 100755",
        "commit_message": "rnn->encoders, removes _th suffix, same filenames for chainer and pytorch, modify tests to not rely on _th\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "np.random.seed(args.seed)",
            "",
            "if args.backend == \"pytorch\":",
            "-        from espnet.tts.pytorch.tts_pytorch import train",
            "+        from espnet.tts.pytorch.tts import train",
            "train(args)",
            "else:",
            "raise NotImplementedError(\"Only pytorch is supported.\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tts_pytorch), value='tts')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2225,
        "neg_line": [
            "-from espnet.tts.pytorch.tts_pytorch import train"
        ],
        "pos_line": [
            "+from espnet.tts.pytorch.tts import train"
        ],
        "core_change": "-from espnet.tts.pytorch.tts_pytorch import train +from espnet.tts.pytorch.tts import train",
        "core_API": "seed"
    },
    {
        "commit_hash": "a3eb618014a93390b2506d53ebbba11574ba98d3",
        "index": "7fdc5ed3..5cd17e63 100644",
        "commit_message": "fix mypy\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DefaultClassifier(Classifier[DT], typing.Generic[DT]):",
            "progress_bar.set_description(\"Batch inference\")",
            "dataloader = progress_bar",
            "",
            "-            overall_loss = 0",
            "+            overall_loss = torch.zeros(1, device=flair.device)",
            "label_count = 0",
            "for batch in dataloader:",
            "# stop if all sentences are empty"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=234726)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=234727)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=234728)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=234729)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=234730)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'zeros'), position=2, insert_id=234731)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=234732)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=1, insert_id=234733)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=234734)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=234735)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=234736)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=234737)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=234738)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=234739)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'flair'), position=0, insert_id=234740)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=234741)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=234742)",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 2227,
        "neg_line": [
            "-overall_loss = 0"
        ],
        "pos_line": [
            "+overall_loss = torch.zeros(1, device=flair.device)"
        ],
        "core_change": "-overall_loss = 0 +overall_loss = torch.zeros(1, device=flair.device)",
        "core_API": "set_description"
    },
    {
        "commit_hash": "57dce7244c1eadded22e8ff0deb715cdd7662d3e",
        "index": "86177c550..064c65b50 100644",
        "commit_message": "Fix double precision casting complex buffers (#8208)\n\n* Fix double precision casting complex buffers\n\n* Update CHANGELOG.md\n\n* Fixes\n\n* Fixes\n\n* Fix\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DoublePrecisionPlugin(PrecisionPlugin):",
            "incoming floating point data to double (``torch.float64``) precision. Does not alter `optimizers` or",
            "`lr_schedulers`.",
            "\"\"\"",
            "-        model = cast(pl.LightningModule, model.to(dtype=torch.float64))",
            "+        model = cast(pl.LightningModule, model.double())",
            "model = LightningDoublePrecisionModule(model)",
            "",
            "return super().connect(model, optimizers, lr_schedulers)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=to), value='double')",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float64))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2228,
        "neg_line": [
            "-model = cast(pl.LightningModule, model.to(dtype=torch.float64))"
        ],
        "pos_line": [
            "+model = cast(pl.LightningModule, model.double())"
        ],
        "core_change": "-model = cast(pl.LightningModule, model.to(dtype=torch.float64)) +model = cast(pl.LightningModule, model.double())",
        "core_API": "to"
    },
    {
        "commit_hash": "b449e2295d698138ecc2dfe0618b48a133ac7fa6",
        "index": "a82415e..0a2b499 100644",
        "commit_message": "fix a bug\n\n",
        "file": "text-detection-ctpn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SolverWrapper(object):",
            "",
            "last_snapshot_iter = -1",
            "timer = Timer()",
            "-        tf.Graph.finalize(tf.get_default_graph())",
            "+        #tf.Graph.finalize(tf.get_default_graph())",
            "# for iter in range(max_iters):",
            "for iter in range(restore_iter, max_iters):",
            "timer.tic()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Graph))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=finalize))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=get_default_graph))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 2229,
        "neg_line": [
            "-tf.Graph.finalize(tf.get_default_graph())"
        ],
        "pos_line": [
            "+#tf.Graph.finalize(tf.get_default_graph())"
        ],
        "core_change": "-tf.Graph.finalize(tf.get_default_graph()) +#tf.Graph.finalize(tf.get_default_graph())",
        "core_API": "finalize"
    },
    {
        "commit_hash": "8ad9c0584a6aab1b155f34d92ef3fc2749243ea3",
        "index": "a039c315..22875c04 100644",
        "commit_message": "fix: revert back to tf.nn.rnn_cell_BasicRNNCell\n\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "logger = logging.getLogger(__name__)",
            "",
            "def get_cell_fun(cell_type):",
            "if cell_type == 'rnn':",
            "-        cell_fn = tf2.keras.layers.SimpleRNNCell   # todo tf2 remove obsolete #tf.nn.rnn_cell.BasicRNNCell",
            "+        cell_fn = tf.nn.rnn_cell.BasicRNNCell  # todo tf2: do we eventually need tf2.keras.layers.SimpleRNNCell",
            "elif cell_type == 'lstm':",
            "# allows for optional peephole connections and cell clipping",
            "cell_fn = tf.nn.rnn_cell.LSTMCell"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=SimpleRNNCell), value='BasicRNNCell')",
            "Update(target_node=ASTNode(type=identifier, text=layers), value='rnn_cell')",
            "Update(target_node=ASTNode(type=identifier, text=tf2), value='tf')",
            "Update(target_node=ASTNode(type=identifier, text=keras), value='nn')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 2230,
        "neg_line": [
            "-cell_fn = tf2.keras.layers.SimpleRNNCell   # todo tf2 remove obsolete #tf.nn.rnn_cell.BasicRNNCell"
        ],
        "pos_line": [
            "+cell_fn = tf.nn.rnn_cell.BasicRNNCell  # todo tf2: do we eventually need tf2.keras.layers.SimpleRNNCell"
        ],
        "core_change": "-cell_fn = tf2.keras.layers.SimpleRNNCell   # todo tf2 remove obsolete #tf.nn.rnn_cell.BasicRNNCell +cell_fn = tf.nn.rnn_cell.BasicRNNCell  # todo tf2: do we eventually need tf2.keras.layers.SimpleRNNCell",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "96d468a1..bd38a7dd 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestLegacyMatrixAttention(AllenNlpTestCase):",
            "",
            "def test_forward_works_on_simple_input(self):",
            "attention = LegacyMatrixAttention(DotProductSimilarity())",
            "-        sentence_1_tensor = Variable(torch.FloatTensor([[[1, 1, 1], [-1, 0, 1]]]))",
            "-        sentence_2_tensor = Variable(torch.FloatTensor([[[1, 1, 1], [-1, 0, 1], [-1, -1, -1]]]))",
            "+        sentence_1_tensor = torch.FloatTensor([[[1, 1, 1], [-1, 0, 1]]])",
            "+        sentence_2_tensor = torch.FloatTensor([[[1, 1, 1], [-1, 0, 1], [-1, -1, -1]]])",
            "result = attention(sentence_1_tensor, sentence_2_tensor).data.numpy()",
            "assert result.shape == (1, 2, 3)",
            "assert_allclose(result, [[[3, 0, -3], [0, 2, 0]]])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 2231,
        "neg_line": [
            "-sentence_1_tensor = Variable(torch.FloatTensor([[[1, 1, 1], [-1, 0, 1]]]))",
            "-sentence_2_tensor = Variable(torch.FloatTensor([[[1, 1, 1], [-1, 0, 1], [-1, -1, -1]]]))"
        ],
        "pos_line": [
            "+sentence_1_tensor = torch.FloatTensor([[[1, 1, 1], [-1, 0, 1]]])",
            "+sentence_2_tensor = torch.FloatTensor([[[1, 1, 1], [-1, 0, 1], [-1, -1, -1]]])"
        ],
        "core_change": "-sentence_1_tensor = Variable(torch.FloatTensor([[[1, 1, 1], [-1, 0, 1]]])) -sentence_2_tensor = Variable(torch.FloatTensor([[[1, 1, 1], [-1, 0, 1], [-1, -1, -1]]])) +sentence_1_tensor = torch.FloatTensor([[[1, 1, 1], [-1, 0, 1]]]) +sentence_2_tensor = torch.FloatTensor([[[1, 1, 1], [-1, 0, 1], [-1, -1, -1]]])",
        "core_API": "FloatTensor"
    },
    {
        "commit_hash": "df3e2cb4cceb47a36ceed0a261cceeb27e7a6e00",
        "index": "1c13e216..0dff4c40 100644",
        "commit_message": "Layer API Refactoring (#675)\n\n* Layer API Refactoring\n\n* private method decorator added\n\n* Input Layers added\n\n* Lazy Import on ROI pooling\n\n* @zsdonghao change proposal added\n\n* UTF-8 header added\n\n* gitignore updated\n\n* Python2 error correction\n\n* Changelog Updated\n\n* Python Headers Uniformized + Codacy Error Fix + LSTMStateTuple missing import\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class TileLayer(Layer):",
            "",
            "@deprecated_alias(layer='prev_layer', end_support_version=1.9)  # TODO remove this line for the 1.9 release",
            "def __init__(self, prev_layer, multiples=None, name='tile'):",
            "+",
            "super(TileLayer, self).__init__(prev_layer=prev_layer, name=name)",
            "",
            "logging.info(\"TileLayer  %s: multiples:%s\" % (name, multiples))",
            "",
            "-        self.inputs = prev_layer.outputs",
            "-",
            "with tf.variable_scope(name):",
            "self.outputs = tf.tile(self.inputs, multiples=multiples)",
            "",
            "-        self.all_layers.append(self.outputs)",
            "+        self._add_layers(self.outputs)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=all_layers), value='_add_layers')",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=inputs))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=prev_layer))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=outputs))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=append))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 2233,
        "neg_line": [
            "-self.inputs = prev_layer.outputs",
            "-",
            "-self.all_layers.append(self.outputs)"
        ],
        "pos_line": [
            "+",
            "+self._add_layers(self.outputs)"
        ],
        "core_change": "+ -self.inputs = prev_layer.outputs - -self.all_layers.append(self.outputs) +self._add_layers(self.outputs)",
        "core_API": "info"
    },
    {
        "commit_hash": "384e124490f7a629dc677fc5b658b69afade0a04",
        "index": "3f6d97091..826bd4ede 100644",
        "commit_message": "ReduceLROnPlateau bug fix (#1126)\n\n* bug fix and test\n\n* update CHANGELOG.md\n\nCo-authored-by: Nicki Skafte <nugginea@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "class Trainer(",
            "if 'scheduler' not in scheduler:",
            "raise ValueError(f'Lr scheduler should have key `scheduler`',",
            "' with item being a lr scheduler')",
            "-                scheduler['reduce_on_plateau'] = \\",
            "-                    isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau)",
            "+                scheduler['reduce_on_plateau'] = isinstance(",
            "+                    scheduler['scheduler'], optim.lr_scheduler.ReduceLROnPlateau)",
            "",
            "lr_schedulers.append({**default_config, **scheduler})"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('subscript', None), position=1, insert_id=1600673)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=scheduler), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1600674)",
            "Insert(target_node=IN(type=subscript), node=('string', \"'scheduler'\"), position=2, insert_id=1600675)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1600676)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 5,
        "number": 2234,
        "neg_line": [
            "-scheduler['reduce_on_plateau'] = \\",
            "-isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau)"
        ],
        "pos_line": [
            "+scheduler['reduce_on_plateau'] = isinstance(",
            "+scheduler['scheduler'], optim.lr_scheduler.ReduceLROnPlateau)"
        ],
        "core_change": "-scheduler['reduce_on_plateau'] = \\ -isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau) +scheduler['reduce_on_plateau'] = isinstance( +scheduler['scheduler'], optim.lr_scheduler.ReduceLROnPlateau)",
        "core_API": "append"
    },
    {
        "commit_hash": "3efce3aeab03a626daf943d6b28218b5ad7c078b",
        "index": "139db324..5cc74d02 100644",
        "commit_message": "fix tower-summary bug\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class QueueInputTrainer(Trainer):",
            "kept_summaries[k] = copy.copy(tf.get_collection(k))",
            "logger.info(\"Graph built for tower {}.\".format(i))",
            "for k in coll_keys:",
            "-                del tf.get_collection(k)[:]",
            "-                tf.get_collection(k).extend(kept_summaries[k])",
            "+                del tf.get_collection_ref(k)[:]",
            "+                tf.get_collection_ref(k).extend(kept_summaries[k])",
            "grads = QueueInputTrainer._average_grads(grad_list)",
            "cost_var = cost_var_t0",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2317234)",
            "Update(target_node=ASTNode(type=identifier, text=get_collection), value='get_collection_ref')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2317235)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'k'), position=1, insert_id=2317236)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2317237)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=get_collection), value='get_collection_ref')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=k))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 2237,
        "neg_line": [
            "-del tf.get_collection(k)[:]",
            "-tf.get_collection(k).extend(kept_summaries[k])"
        ],
        "pos_line": [
            "+del tf.get_collection_ref(k)[:]",
            "+tf.get_collection_ref(k).extend(kept_summaries[k])"
        ],
        "core_change": "-del tf.get_collection(k)[:] -tf.get_collection(k).extend(kept_summaries[k]) +del tf.get_collection_ref(k)[:] +tf.get_collection_ref(k).extend(kept_summaries[k])",
        "core_API": "copy"
    },
    {
        "commit_hash": "1daab8714463e63c8f4b4ba4caed3cc1ef97a195",
        "index": "f6614948..7fdf92c8 100644",
        "commit_message": "[torch] Enable GPU training (#1508)\n\n* Added more descriptive description_summary, fixed bug in TabNet combiner\n\n* Updated modules to do proper GPU conversion\n\n* Functional GPU training\n\n* Tabnet working, removed RayRemoteTrainer\n\n* Temporary changes to unblock GPU training\n\n* Functional test_seq_encoder\n\n* Cleaned up PR\n\n* Fixed failing CPU tests\n\n* Responded to comments\n\n* Fix failing GPU tests in test_api.py\n\n* [torch] Fix GPU tests for test_visualizations (#1539)\n\nUpdated torch.aranges to registered buffers in embedding_modules.py\n\n* [torch] Fix test_encoder.py for GPU (#1544)\n\n* Accept and move model to GPU as part of Predictor init (#1559)\n\n* [torch] Fixed tests in test_experiment.py (#1562)\n\n* Fixed tests in tests/integration_tests/test_experiment.py\n\n* Moved set embed and inputs to correct device in tests/ludwig/encoders/test_set_encoders.py\n\n* Moved set date and inputs to correct device in tests/ludwig/encoders/test_date_encoders.py\n\n* Moved bag embed and inputs to correct device in tests/ludwig/encoders/test_bag_encoders.py\n\n* Moved sequence embed and inputs to correct device in tests/ludwig/features/test_sequence_features.py\n\n* [torch] Fixed GPU issues in combiners.py (#1568)\n\n* Fix GPU tests in test_collect.py (#1569)\n\n* Added model to Predictor initialization\n\n* Fixed embedding modules tests (#1554)\n\n* Fixed embedding modules tests\n* Moved embedding modules to device in tests, removed duplicate argument\n\nCo-authored-by: Shreya Rajpal <shreya.rajpal@gmail.com>\n\n* Fixed encoders tests except test_h3_rnn_embed (#1556)\n\n* Fixed encoders tests except test_h3_rnn_embed\n\n* Addressed Shreya's comments\n\n* Moved embedding modules to device, used registered buffers\n\nCo-authored-by: Shreya Rajpal <shreya.rajpal@gmail.com>\n\n* Pass ECD object to Predictor\n\n* Removed model arg from predict function\n\nCo-authored-by: Jeffrey Tang <jeff@predibase.com>\nCo-authored-by: Jeffrey Tang <810895+jeffreyftang@users.noreply.github.com>\nCo-authored-by: Animesh Kumar <anmshkmr@users.noreply.github.com>\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def run_api_experiment(input_features, output_features, data_csv):",
            "model_weights = get_weights(model.model)",
            "loaded_weights = get_weights(loaded_model.model)",
            "for model_weight, loaded_weight in zip(model_weights, loaded_weights):",
            "-            assert np.allclose(model_weight, loaded_weight)",
            "+            assert torch.allclose(model_weight, loaded_weight)",
            "finally:",
            "# Remove results/intermediate data saved to disk",
            "shutil.rmtree(output_dir, ignore_errors=True)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=np), value='torch')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2239,
        "neg_line": [
            "-assert np.allclose(model_weight, loaded_weight)"
        ],
        "pos_line": [
            "+assert torch.allclose(model_weight, loaded_weight)"
        ],
        "core_change": "-assert np.allclose(model_weight, loaded_weight) +assert torch.allclose(model_weight, loaded_weight)",
        "core_API": "allclose"
    },
    {
        "commit_hash": "9abc5250be502b6a319b2273aa226e68ae2b7a45",
        "index": "f036c8a19e..162711d6fc 100644",
        "commit_message": "frontends.tensorflow.nn: fix function name\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "def test_tensorflow_dropout(",
            "),",
            "name=st.sampled_from([\"sigmoid_cross_entropy_with_logits\"]),",
            "num_positional_args=helpers.num_positional_args(",
            "-        fn_name=\"ivy.functional.frontends.tensorflow.sigmoid_cross_entropy_with_logits\",",
            "+        fn_name=\"ivy.functional.frontends.tensorflow.nn.sigmoid_cross_entropy_with_logits\",  # noqa",
            "),",
            ")",
            "def test_tensorflow_sigmoid_cross_entropy_with_logits("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"ivy.functional.frontends.tensorflow.sigmoid_cross_entropy_with_logits\"), value='\"ivy.functional.frontends.tensorflow.nn.sigmoid_cross_entropy_with_logits\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2240,
        "neg_line": [
            "-fn_name=\"ivy.functional.frontends.tensorflow.sigmoid_cross_entropy_with_logits\","
        ],
        "pos_line": [
            "+fn_name=\"ivy.functional.frontends.tensorflow.nn.sigmoid_cross_entropy_with_logits\",  # noqa"
        ],
        "core_change": "-fn_name=\"ivy.functional.frontends.tensorflow.sigmoid_cross_entropy_with_logits\", +fn_name=\"ivy.functional.frontends.tensorflow.nn.sigmoid_cross_entropy_with_logits\",  # noqa",
        "core_API": "sampled_from"
    },
    {
        "commit_hash": "e11fe19673b3e7035f9625345924d90d9dc1d39e",
        "index": "32ec0282c..2d13855f9 100644",
        "commit_message": "Remove unnecessary use of comprehension (#8149)\n\nCo-authored-by: deepsource-autofix[bot] <62050782+deepsource-autofix[bot]@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_trainer_with_gpus_options_combination_at_available_gpus_env(auto_select",
            "[\"nb\", \"expected_gpu_idxs\", \"expected_error\"],",
            "[",
            "(0, [], MisconfigurationException),",
            "-        (-1, [i for i in range(torch.cuda.device_count())], None),",
            "+        (-1, list(range(torch.cuda.device_count())), None),",
            "(1, [0], None),",
            "],",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=tuple), node=('call', None), position=3, insert_id=531652)",
            "Insert(target_node=IN(type=call), node=('identifier', 'list'), position=0, insert_id=531653)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=531654)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=531655)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=531656)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=i))",
            "Delete(target_node=ASTNode(type=for, text=for))",
            "Delete(target_node=ASTNode(type=identifier, text=i))",
            "Delete(target_node=ASTNode(type=in, text=in))",
            "Delete(target_node=ASTNode(type=for_in_clause))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list_comprehension))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 2241,
        "neg_line": [
            "-(-1, [i for i in range(torch.cuda.device_count())], None),"
        ],
        "pos_line": [
            "+(-1, list(range(torch.cuda.device_count())), None),"
        ],
        "core_change": "-(-1, [i for i in range(torch.cuda.device_count())], None), +(-1, list(range(torch.cuda.device_count())), None),",
        "core_API": "device_count"
    },
    {
        "commit_hash": "2c68e545fa5c1b8b42a99880ebbcec961554e2b4",
        "index": "88d796fc..96247b04 100644",
        "commit_message": "api version fix\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def anchor_target_single(flat_anchors,",
            "num_valid_anchors = anchors.shape[0]",
            "bbox_targets = torch.zeros_like(anchors)",
            "bbox_weights = torch.zeros_like(anchors)",
            "-    labels = anchors.new_zeros((num_valid_anchors, ))",
            "-    label_weights = anchors.new_zeros((num_valid_anchors, ))",
            "+    labels = gt_labels.new_zeros(num_valid_anchors)",
            "+    label_weights = gt_labels.new_zeros(num_valid_anchors, dtype=torch.float)",
            "",
            "pos_inds = sampling_result.pos_inds",
            "neg_inds = sampling_result.neg_inds"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=num_valid_anchors), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=647090)",
            "Update(target_node=ASTNode(type=identifier, text=anchors), value='gt_labels')",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=647091)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=647092)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=647093)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=num_valid_anchors), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=647094)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=647095)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float'), position=2, insert_id=647096)",
            "Update(target_node=ASTNode(type=identifier, text=anchors), value='gt_labels')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=tuple))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 2242,
        "neg_line": [
            "-labels = anchors.new_zeros((num_valid_anchors, ))",
            "-label_weights = anchors.new_zeros((num_valid_anchors, ))"
        ],
        "pos_line": [
            "+labels = gt_labels.new_zeros(num_valid_anchors)",
            "+label_weights = gt_labels.new_zeros(num_valid_anchors, dtype=torch.float)"
        ],
        "core_change": "-labels = anchors.new_zeros((num_valid_anchors, )) -label_weights = anchors.new_zeros((num_valid_anchors, )) +labels = gt_labels.new_zeros(num_valid_anchors) +label_weights = gt_labels.new_zeros(num_valid_anchors, dtype=torch.float)",
        "core_API": "zeros_like"
    },
    {
        "commit_hash": "574c7a20b90cf5a1012db504e19877148a3145a9",
        "index": "ae0a70e1..6374bf52 100644",
        "commit_message": "Fix bug with `save_spec` when there is a `training` positional arg.\n\nPiperOrigin-RevId: 445236065\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Layer(tf.Module, version_utils.LayerVersionSelector):",
            "kwargs_spec[key] = tf.nest.pack_sequence_as(kwarg, flat_specs)",
            "",
            "self._saved_model_inputs_spec = inputs_spec",
            "-    self._saved_model_arg_spec = ([inputs_spec] + args_spec, kwargs_spec)",
            "+    self._saved_model_arg_spec = ([inputs_spec] + list(args_spec), kwargs_spec)",
            "",
            "def _get_save_spec(self, dynamic_batch=True, inputs_only=True):",
            "if self._saved_model_inputs_spec is None:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=2058603)",
            "Insert(target_node=IN(type=call), node=('identifier', 'list'), position=0, insert_id=2058604)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2058605)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2058606)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=args_spec), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2058607)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 2243,
        "neg_line": [
            "-self._saved_model_arg_spec = ([inputs_spec] + args_spec, kwargs_spec)"
        ],
        "pos_line": [
            "+self._saved_model_arg_spec = ([inputs_spec] + list(args_spec), kwargs_spec)"
        ],
        "core_change": "-self._saved_model_arg_spec = ([inputs_spec] + args_spec, kwargs_spec) +self._saved_model_arg_spec = ([inputs_spec] + list(args_spec), kwargs_spec)",
        "core_API": "pack_sequence_as"
    },
    {
        "commit_hash": "a533c6ff92980e54b094b9ab42e8db87b21d0547",
        "index": "8c9f4b2..81e2b9a 100644",
        "commit_message": "Updated README fixed python2 issues\n\n",
        "file": "face-alignment.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def transform(point, center, scale, resolution, invert=False):",
            "",
            "if invert:",
            "t = torch.inverse(t)",
            "-    new_point = (t @ _pt)[0:2]",
            "+",
            "+    new_point = (torch.matmul(t,_pt))[0:2]",
            "",
            "return new_point.int()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parenthesized_expression), node=('(', '('), position=0, insert_id=201869)",
            "Insert(target_node=ASTNode(type=parenthesized_expression), node=('call', None), position=1, insert_id=201870)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=201871)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=201872)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=201873)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=201874)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'matmul'), position=2, insert_id=201875)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=t), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=201876)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=_pt), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=201877)",
            "Delete(target_node=ASTNode(type=@, text=@))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 2244,
        "neg_line": [
            "-new_point = (t @ _pt)[0:2]"
        ],
        "pos_line": [
            "+",
            "+new_point = (torch.matmul(t,_pt))[0:2]"
        ],
        "core_change": "-new_point = (t @ _pt)[0:2] + +new_point = (torch.matmul(t,_pt))[0:2]",
        "core_API": "inverse"
    },
    {
        "commit_hash": "828ca68605cad6d62b20fdbf6903fc13ff7d66c2",
        "index": "8576c6b141..a7e865c6f5 100644",
        "commit_message": "lint fixes (#5332)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def einsum(",
            "*operands: Union[tf.Tensor, tf.Variable],",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    return tf.einsum(equation, *operands)",
            "\\ No newline at end of file",
            "+    return tf.einsum(equation, *operands)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=return_statement), position=1)",
            "Move(target_node=ASTNode(type=function_definition), node=ASTNode(type=ERROR), position=6)",
            "Insert(target_node=ASTNode(type=function_definition), node=('block', None), position=7, insert_id=1989402)",
            "Move(target_node=ASTNode(type=return_statement), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1989403)",
            "Move(target_node=IN(type=expression_statement), node=ASTNode(type=identifier, text=file), position=0)",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 2246,
        "neg_line": [
            "-return tf.einsum(equation, *operands)"
        ],
        "pos_line": [
            "+return tf.einsum(equation, *operands)"
        ],
        "core_change": "-return tf.einsum(equation, *operands) +return tf.einsum(equation, *operands)",
        "core_API": "einsum"
    },
    {
        "commit_hash": "8ed7b0508c2c4bbba5df3a450c096a784ab268f2",
        "index": "b88cdbf2ab..ccad08a2fb 100644",
        "commit_message": "fix docstring failure\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "def avg_pool1d(",
            "x = x.permute(0, 2, 1)",
            "x_shape = x.shape[2]",
            "pad_w = ivy.handle_padding(x_shape, strides[0], kernel[0], padding)",
            "-    x = torch.nn.functional.pad(",
            "-        x, [pad_w // 2, pad_w - pad_w // 2], mode=\"replicate\"",
            "-    )",
            "+    x = torch.nn.functional.pad(x, [pad_w // 2, pad_w - pad_w // 2], mode=\"replicate\")",
            "",
            "res = torch.nn.functional.avg_pool1d(x, kernel, strides, 0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 2250,
        "neg_line": [
            "-x = torch.nn.functional.pad(",
            "-x, [pad_w // 2, pad_w - pad_w // 2], mode=\"replicate\"",
            "-)"
        ],
        "pos_line": [
            "+x = torch.nn.functional.pad(x, [pad_w // 2, pad_w - pad_w // 2], mode=\"replicate\")"
        ],
        "core_change": "-x = torch.nn.functional.pad( -x, [pad_w // 2, pad_w - pad_w // 2], mode=\"replicate\" -) +x = torch.nn.functional.pad(x, [pad_w // 2, pad_w - pad_w // 2], mode=\"replicate\")",
        "core_API": "permute"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "4c392bc9..067393dc 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_pow():",
            "def test_tensor_ops():",
            "pi = 3.141592654",
            "X = Uniform(0, 1).expand([5, 5]).rv",
            "-    a = tt([[1, 2, 3, 4, 5]])",
            "+    a = torch.tensor([[1, 2, 3, 4, 5]])",
            "b = a.T",
            "X = abs(pi*(-X + a - 3*b))",
            "x = X.dist.sample()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=677062)",
            "Update(target_node=ASTNode(type=identifier, text=tt), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tt), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=677063)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=677064)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2251,
        "neg_line": [
            "-a = tt([[1, 2, 3, 4, 5]])"
        ],
        "pos_line": [
            "+a = torch.tensor([[1, 2, 3, 4, 5]])"
        ],
        "core_change": "-a = tt([[1, 2, 3, 4, 5]]) +a = torch.tensor([[1, 2, 3, 4, 5]])",
        "core_API": "tensor"
    },
    {
        "commit_hash": "3c8f91ee3d32222633c125dd97b669dbf740a1eb",
        "index": "e7dbf33d..040693d7 100644",
        "commit_message": "Style fix\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _convert_string_dtype(dtype):",
            "",
            "",
            "def _to_tensor(x, dtype):",
            "-    x = tf.python.framework.ops.convert_to_tensor(x)",
            "+    x = tf.convert_to_tensor(x)",
            "if x.dtype != dtype:",
            "x = tf.cast(x, dtype)",
            "return x"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=python))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=framework))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=ops))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2256,
        "neg_line": [
            "-x = tf.python.framework.ops.convert_to_tensor(x)"
        ],
        "pos_line": [
            "+x = tf.convert_to_tensor(x)"
        ],
        "core_change": "-x = tf.python.framework.ops.convert_to_tensor(x) +x = tf.convert_to_tensor(x)",
        "core_API": "convert_to_tensor"
    },
    {
        "commit_hash": "a9baca376616bed56e5df5115d7adf8059c0d296",
        "index": "a4a09852..caaef8f7 100644",
        "commit_message": "add supports_fetch_outside_dataloader property to avoid creating real first batches for datasets that only expect to be used inside dataloader workers\n\nSummary:\nD24093421 (https://github.com/pytorch/fairseq/commit/e056de1fb641425974b0941704e63b9b42fe8a1f) added `first_batch` to iterators in fairseq. This means that FairseqDataset objects now might need to fetch data outside the dataloader workers. This causes issues with certain datasets, in particular datasets that fetch data via everstore/memcache, since these clients open a ton of file descriptors based on how many items are fetched. Opening too many file descriptors causes forking to fail in python multiprocessing.\n\nTo fix this, lets have a property `supports_fetch_outside_dataloader` in the FairseqDataset that allows us to decide if it is safe to fetch the first batch. If it is not, we will revert back to the original behavior before D24093421 (https://github.com/pytorch/fairseq/commit/e056de1fb641425974b0941704e63b9b42fe8a1f) which is to just use \"DUMMY\", and set this as a real batch late.r\n\nReviewed By: yqwangustc\n\nDifferential Revision: D24234470\n\nfbshipit-source-id: 7ad66a6de622ce26f59f00d00b19700fbd992921\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "dataloader",
        "change": [
            "class FairseqDataset(torch.utils.data.Dataset, EpochListening):",
            "indices, ignored = data_utils._filter_by_size_dynamic(indices, self.size, max_sizes)",
            "return indices, ignored",
            "",
            "+    @property",
            "+    def supports_fetch_outside_dataloader(self):",
            "+        \"\"\"Whether this dataset supports fetching outside the workers of the dataloader.\"\"\"",
            "+        return True",
            "+",
            "",
            "class FairseqIterableDataset(torch.utils.data.IterableDataset, EpochListening):",
            "\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('decorated_definition', None), position=3, insert_id=1762115)",
            "Insert(target_node=IN(type=decorated_definition), node=('decorator', None), position=0, insert_id=1762116)",
            "Insert(target_node=IN(type=decorated_definition), node=('function_definition', None), position=1, insert_id=1762117)",
            "Insert(target_node=IN(type=decorator), node=('@', '@'), position=0, insert_id=1762118)",
            "Insert(target_node=IN(type=decorator), node=('identifier', 'property'), position=1, insert_id=1762119)",
            "Insert(target_node=IN(type=function_definition), node=('def', 'def'), position=0, insert_id=1762120)",
            "Insert(target_node=IN(type=function_definition), node=('identifier', 'supports_fetch_outside_dataloader'), position=1, insert_id=1762121)",
            "Insert(target_node=IN(type=function_definition), node=('parameters', None), position=2, insert_id=1762122)",
            "Insert(target_node=IN(type=function_definition), node=(':', ':'), position=3, insert_id=1762123)",
            "Insert(target_node=IN(type=function_definition), node=('block', None), position=4, insert_id=1762124)",
            "Insert(target_node=IN(type=parameters), node=('(', '('), position=0, insert_id=1762125)",
            "Insert(target_node=IN(type=parameters), node=('identifier', 'self'), position=1, insert_id=1762126)",
            "Insert(target_node=IN(type=parameters), node=(')', ')'), position=2, insert_id=1762127)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1762128)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=1, insert_id=1762129)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"Whether this dataset supports fetching outside the workers of the dataloader.\"\"\"'), position=0, insert_id=1762130)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=1762131)",
            "Insert(target_node=IN(type=return_statement), node=('true', 'True'), position=1, insert_id=1762132)"
        ],
        "plus_line": 4,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 2257,
        "neg_line": [],
        "pos_line": [
            "+@property",
            "+def supports_fetch_outside_dataloader(self):",
            "+\"\"\"Whether this dataset supports fetching outside the workers of the dataloader.\"\"\"",
            "+return True",
            "+"
        ],
        "core_change": "+@property +def supports_fetch_outside_dataloader(self): +\"\"\"Whether this dataset supports fetching outside the workers of the dataloader.\"\"\" +return True +",
        "core_API": "_filter_by_size_dynamic"
    },
    {
        "commit_hash": "e94abf66783f5e24ab37adc754ac902e827a4b4d",
        "index": "dfbd1e86..403f0006 100644",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "__all__ = ['ConcatWith']",
            "@layer_register(use_scope=False, log_shape=False)",
            "def ConcatWith(x, dim, tensor):",
            "\"\"\"",
            "-    A wrapper around `tf.concat` to support `LinearWrap`",
            "+    A wrapper around `tf.concat_v2` to support `LinearWrap`",
            ":param x: the input tensor",
            ":param dim: the dimension along which to concatenate",
            ":param tensor: a tensor or list of tensor to concatenate with x.",
            "x will be at the beginning",
            "-    :return: tf.concat(dim, [x] + [tensor])",
            "+    :return: tf.concat_v2([x] + [tensor], dim)",
            "\"\"\"",
            "if type(tensor) != list:",
            "tensor = [tensor]",
            "-    return tf.concat(dim, [x] + tensor)",
            "+    return tf.concat_v2([x] + tensor, dim)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n    A wrapper around `tf.concat` to support `LinearWrap`\n:param x: the input tensor\n:param dim: the dimension along which to concatenate\n:param tensor: a tensor or list of tensor to concatenate with x.\nx will be at the beginning\n    :return: tf.concat(dim, [x] + [tensor])\n\"\"\"), value='\"\"\"\\n    A wrapper around `tf.concat_v2` to support `LinearWrap`\\n:param x: the input tensor\\n:param dim: the dimension along which to concatenate\\n:param tensor: a tensor or list of tensor to concatenate with x.\\nx will be at the beginning\\n    :return: tf.concat_v2([x] + [tensor], dim)\\n\"\"\"')",
            "Update(target_node=ASTNode(type=identifier, text=concat), value='concat_v2')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=3, insert_id=2308304)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'dim'), position=4, insert_id=2308305)",
            "Delete(target_node=ASTNode(type=identifier, text=dim))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 6,
        "number": 2258,
        "neg_line": [
            "-A wrapper around `tf.concat` to support `LinearWrap`",
            "-:return: tf.concat(dim, [x] + [tensor])",
            "-return tf.concat(dim, [x] + tensor)"
        ],
        "pos_line": [
            "+A wrapper around `tf.concat_v2` to support `LinearWrap`",
            "+:return: tf.concat_v2([x] + [tensor], dim)",
            "+return tf.concat_v2([x] + tensor, dim)"
        ],
        "core_change": "-A wrapper around `tf.concat` to support `LinearWrap` +A wrapper around `tf.concat_v2` to support `LinearWrap` -:return: tf.concat(dim, [x] + [tensor]) +:return: tf.concat_v2([x] + [tensor], dim) -return tf.concat(dim, [x] + tensor) +return tf.concat_v2([x] + tensor, dim)",
        "core_API": "concat"
    },
    {
        "commit_hash": "726aba089d12503249d824bbaf4070f47d0fe44d",
        "index": "63e14002..f8a7d9fe 100644",
        "commit_message": "[Pytorch] pytorch only timesteps (#724)\n\n* pytorch timesteps\n\n* style\n\n* get rid of if-else\n\n* fix test\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class KarrasVeScheduler(SchedulerMixin, ConfigMixin):",
            ")",
            "for i in self.timesteps",
            "]",
            "-        self.schedule = torch.tensor(schedule, dtype=torch.float32)",
            "+        self.schedule = torch.tensor(schedule, dtype=torch.float32, device=device)",
            "",
            "def add_noise_to_input(",
            "self, sample: torch.FloatTensor, sigma: float, generator: Optional[torch.Generator] = None"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=104029)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=104030)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=104031)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=104032)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=104033)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2259,
        "neg_line": [
            "-self.schedule = torch.tensor(schedule, dtype=torch.float32)"
        ],
        "pos_line": [
            "+self.schedule = torch.tensor(schedule, dtype=torch.float32, device=device)"
        ],
        "core_change": "-self.schedule = torch.tensor(schedule, dtype=torch.float32) +self.schedule = torch.tensor(schedule, dtype=torch.float32, device=device)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "962f9fd2313aefd4025ede24d3e55e41285b684b",
        "index": "519c816673..a94a691b6f 100644",
        "commit_message": "fixes for vector_norm with tensorflow backend.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def vector_norm(",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "if ord == -float(\"inf\"):",
            "tn_normalized_vector = tf.reduce_min(tf.abs(x), axis, keepdims)",
            "-    elif ord == -1:",
            "+    elif ord < 1:",
            "tn_normalized_vector = tf.reduce_sum(tf.abs(x) ** ord, axis, keepdims) ** (",
            "1.0 / ord",
            ")",
            "",
            "elif ord == 0:",
            "-        tn_normalized_vector = tf.reduce_sum(",
            "-            tf.cast(x != 0, \"float32\"), axis, keepdims",
            "-        ).numpy()",
            "+        tn_normalized_vector = tf.reduce_sum(tf.cast(x != 0, x.dtype), axis, keepdims)",
            "",
            "else:",
            "tn_normalized_vector = tf.linalg.norm(x, ord, axis, keepdims)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('<', '<'), position=2, insert_id=2005480)",
            "Move(target_node=ASTNode(type=comparison_operator), node=ASTNode(type=ERROR), position=3)",
            "Move(target_node=ASTNode(type=comparison_operator), node=ASTNode(type=identifier, text=tn_normalized_vector), position=4)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2005481)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x'), position=0, insert_id=2005482)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2005483)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=2005484)",
            "Delete(target_node=ASTNode(type===, text===))",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=unary_operator))",
            "Delete(target_node=ASTNode(type=string, text=\"float32\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=numpy))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 4,
        "AST_diff_line": 19,
        "number": 2261,
        "neg_line": [
            "-elif ord == -1:",
            "-tn_normalized_vector = tf.reduce_sum(",
            "-tf.cast(x != 0, \"float32\"), axis, keepdims",
            "-).numpy()"
        ],
        "pos_line": [
            "+elif ord < 1:",
            "+tn_normalized_vector = tf.reduce_sum(tf.cast(x != 0, x.dtype), axis, keepdims)"
        ],
        "core_change": "-elif ord == -1: +elif ord < 1: -tn_normalized_vector = tf.reduce_sum( -tf.cast(x != 0, \"float32\"), axis, keepdims -).numpy() +tn_normalized_vector = tf.reduce_sum(tf.cast(x != 0, x.dtype), axis, keepdims)",
        "core_API": "reduce_min"
    },
    {
        "commit_hash": "d3cb28886ac68beba9a6646b422a4d727b056c0c",
        "index": "cc6871d93..169f1faeb 100755",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AlbertModel(AlbertPreTrainedModel):",
            "",
            "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)",
            "extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility",
            "-        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0",
            "+        extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(self.dtype).min",
            "head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)",
            "",
            "embedding_output = self.embeddings("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('attribute', None), position=2, insert_id=1195752)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=1195753)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1195754)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'min'), position=2, insert_id=1195755)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1195756)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1195757)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1195758)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1195759)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'finfo'), position=2, insert_id=1195760)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1195761)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1195762)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1195763)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1195764)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1195765)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1195766)",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=float, text=10000.0))",
            "Delete(target_node=ASTNode(type=unary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 2263,
        "neg_line": [
            "-extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0"
        ],
        "pos_line": [
            "+extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(self.dtype).min"
        ],
        "core_change": "-extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0 +extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(self.dtype).min",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "cea17acd8cc5190684da944924116fe10742ad81",
        "index": "066fab894..b575a975c 100644",
        "commit_message": "[Generate] Fix generate with inputs_embeds on GPU (#14564)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GenerationMixin:",
            "",
            "# First if `inputs_embeds` are given, but no `attention_mask` assume that full attention_mask is used",
            "if inputs_embeds is not None:",
            "-            return torch.ones((inputs_embeds.shape[0], inputs_embeds.shape[1]), dtype=torch.long)",
            "+            return torch.ones((inputs_embeds.shape[0], inputs_embeds.shape[1]), dtype=torch.long, device=self.device)",
            "",
            "# Otherwise, use `input_ids`",
            "is_pad_token_in_inputs_ids = (pad_token_id is not None) and (pad_token_id in input_ids)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1209319)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1209320)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1209321)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1209322)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1209323)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1209324)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1209325)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1209326)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2264,
        "neg_line": [
            "-return torch.ones((inputs_embeds.shape[0], inputs_embeds.shape[1]), dtype=torch.long)"
        ],
        "pos_line": [
            "+return torch.ones((inputs_embeds.shape[0], inputs_embeds.shape[1]), dtype=torch.long, device=self.device)"
        ],
        "core_change": "-return torch.ones((inputs_embeds.shape[0], inputs_embeds.shape[1]), dtype=torch.long) +return torch.ones((inputs_embeds.shape[0], inputs_embeds.shape[1]), dtype=torch.long, device=self.device)",
        "core_API": "ones"
    },
    {
        "commit_hash": "1733946a819b1f0a756aaee75a1e4d71addf08d9",
        "index": "bfeb247..9297c8c 100644",
        "commit_message": "Change to Multihead Attention to allow Batched GEMMs larger than 64K. (#728)\n\n* Adding C++ Multihead Attention implementation to contrib.\n\n* Add reference test that at least works for forward.\n\n* Remove CublasLt support from multihead attention.\n\n* Add new Python version of self attention.\n\n* Update python model of MHA with backward pass.\n\n* Fixed Output Linear connection in MHA.\n\n* Clean up compiles and add documentation to PySelfAttention.\n\n* Add Encdec Python version of multihead attention.  Cleanup files.\n\n* Tests for self and encdec multihead attention.\n\n* Add reference pytorch implementation of attention with norm and add.\n\n* Add cutlass branch definition.\n\n* Add cutlass download to compile.\n\n* Add norm/add tests.\n\n* Add biases to pytorch python versions.\n\n* Add tests and fix issues with python version of attention masking.\n\n* Create README.md\n\n* Update README.md\n\n* Update README.md\n\n* Update perf test parameters.\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Add files via upload\n\n* Update README.md\n\n* Update README.md\n\n* Update README.md\n\n* Fix matmul1 output tensor size.  Fix tests that missed issue.\n\n* Allow for Z dimensions of 64K and greater on batched GEMMs.\n\n* remove redundant imports\n\n* general cleanup, remove deprecated or unused functions\n\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FastSelfAttnFunc(torch.autograd.Function) :",
            "return outputs.detach()",
            "",
            "@staticmethod",
            "-    def backward(ctx, output_grads) :",
            "+    def backward(ctx, output_grads):",
            "heads_t,                                                        \\",
            "matmul2_results,                                                \\",
            "dropout_results,                                                \\"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2268,
        "neg_line": [
            "-def backward(ctx, output_grads) :"
        ],
        "pos_line": [
            "+def backward(ctx, output_grads):"
        ],
        "core_change": "-def backward(ctx, output_grads) : +def backward(ctx, output_grads):",
        "core_API": "detach"
    },
    {
        "commit_hash": "f04257fdbcb6ecb5a9bef75f4c2a8d2e8b5a6209",
        "index": "d2da06446..29cb63c3a 100644",
        "commit_message": "Add test to ensure models can take int64 inputs (#17210)\n\n* Add test to ensure models can take int64 inputs\n\n* is_integer is an attribute, not a method\n\n* Fix test when some inputs aren't tensors\n\n* Add casts to blenderbot and blenderbot-small\n\n* Add casts to the other failing models\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def flatten(index, name=\"segmented_flatten\"):",
            "for _ in range(index.batch_dims, index.indices.shape.rank):",
            "offset = tf.expand_dims(offset, -1)",
            "",
            "-    indices = offset + index.indices",
            "+    indices = tf.cast(offset, index.indices.dtype) + index.indices",
            "return IndexMap(indices=tf.reshape(indices, [-1]), num_segments=index.num_segments * batch_size, batch_dims=0)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=0, insert_id=2364410)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('attribute', None), position=3, insert_id=2364411)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2364412)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2364413)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'index'), position=0, insert_id=2364414)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2364415)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'indices'), position=2, insert_id=2364416)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2364417)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2364418)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=2, insert_id=2364419)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2364420)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=offset), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2364421)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=2364422)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2364423)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2364424)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=2364425)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 2270,
        "neg_line": [
            "-indices = offset + index.indices"
        ],
        "pos_line": [
            "+indices = tf.cast(offset, index.indices.dtype) + index.indices"
        ],
        "core_change": "-indices = offset + index.indices +indices = tf.cast(offset, index.indices.dtype) + index.indices",
        "core_API": "expand_dims"
    },
    {
        "commit_hash": "98cc35b6a8e53e15829ce64fd8da835db0c61da9",
        "index": "53dfde99..f86ae86f 100644",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class ZeroOneAdam(torch.optim.Optimizer):",
            "(self.size * self.divider)))",
            "state['server_chunk_size'] = state[",
            "'corrected_tensor_size'] // self.size",
            "-                    torch.cuda.empty_cache()",
            "+                    get_accelerator().empty_cache()",
            "state['worker_error'] = torch.zeros(state['corrected_tensor_size'],",
            "device=p.device)",
            "state['server_error'] = torch.zeros(state['server_chunk_size'],",
            "device=p.device)",
            "# Accumulation of momentum, i.e., the u variable in the 0/1 Adam paper",
            "state['momentum_accumulator'] = torch.zeros_like(p.data)",
            "-                    torch.cuda.empty_cache()",
            "+                    get_accelerator().empty_cache()",
            "# self.freeze_key = True",
            "if not self.initialize and dist.get_rank() == 0:",
            "print(\"Cupy Buffers Initialized Successfully.\")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1816411)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=1816412)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1816413)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1816414)",
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=1816415)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='get_accelerator')",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1816416)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='get_accelerator')",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1816417)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1816418)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1816419)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1816420)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1816421)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 25,
        "number": 2271,
        "neg_line": [
            "-torch.cuda.empty_cache()",
            "-torch.cuda.empty_cache()"
        ],
        "pos_line": [
            "+get_accelerator().empty_cache()",
            "+get_accelerator().empty_cache()"
        ],
        "core_change": "-torch.cuda.empty_cache() +get_accelerator().empty_cache() -torch.cuda.empty_cache() +get_accelerator().empty_cache()",
        "core_API": "empty_cache"
    },
    {
        "commit_hash": "d309f3eccff700b3b01f1cc2b922a522163232b0",
        "index": "e773842f..bbf860e7 100644",
        "commit_message": "fix DiffPool benchmark\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_dataset(name, sparse=True, cleaned=False):",
            "for i, data in enumerate(dataset):",
            "if data.num_nodes <= num_nodes:",
            "indices.append(i)",
            "-        dataset = dataset[torch.tensor(indices)]",
            "+        dataset = dataset.copy(torch.tensor(indices))",
            "",
            "if dataset.transform is None:",
            "dataset.transform = T.ToDense(num_nodes)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=1000008)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1000009)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1000010)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=dataset), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1000011)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'copy'), position=2, insert_id=1000012)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1000013)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1000014)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 2272,
        "neg_line": [
            "-dataset = dataset[torch.tensor(indices)]"
        ],
        "pos_line": [
            "+dataset = dataset.copy(torch.tensor(indices))"
        ],
        "core_change": "-dataset = dataset[torch.tensor(indices)] +dataset = dataset.copy(torch.tensor(indices))",
        "core_API": "append"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "eed5e62b..768b6ce3 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CholeskyTransform(Transform):",
            "return isinstance(other, CholeskyTransform)",
            "",
            "def _call(self, x):",
            "-        return torch.cholesky(x)",
            "+        return torch.linalg.cholesky(x)",
            "",
            "def _inverse(self, y):",
            "return torch.matmul(y, torch.transpose(y, -2, -1))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=676927)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=676928)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=676929)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2273,
        "neg_line": [
            "-return torch.cholesky(x)"
        ],
        "pos_line": [
            "+return torch.linalg.cholesky(x)"
        ],
        "core_change": "-return torch.cholesky(x) +return torch.linalg.cholesky(x)",
        "core_API": "cholesky"
    },
    {
        "commit_hash": "02b176c4ce14340d26d42825523f406959c6c202",
        "index": "fad8d1061..5d413bba7 100644",
        "commit_message": "Fix torch version comparisons (#18460)\n\nComparisons like\nversion.parse(torch.__version__) > version.parse(\"1.6\")\nare True for torch==1.6.0+cu101 or torch==1.6.0+cpu\n\nversion.parse(version.parse(torch.__version__).base_version) are preferred (and available in pytorch_utils.py\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MishActivation(nn.Module):",
            "",
            "def __init__(self):",
            "super().__init__()",
            "-        if version.parse(torch.__version__) < version.parse(\"1.9\"):",
            "+        if version.parse(version.parse(torch.__version__).base_version) < version.parse(\"1.9\"):",
            "self.act = self._mish_python",
            "else:",
            "self.act = nn.functional.mish"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=1193442)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1193443)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'base_version'), position=2, insert_id=1193444)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1193445)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1193446)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'version'), position=0, insert_id=1193447)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1193448)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'parse'), position=2, insert_id=1193449)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1193450)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1193451)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 2277,
        "neg_line": [
            "-if version.parse(torch.__version__) < version.parse(\"1.9\"):"
        ],
        "pos_line": [
            "+if version.parse(version.parse(torch.__version__).base_version) < version.parse(\"1.9\"):"
        ],
        "core_change": "-if version.parse(torch.__version__) < version.parse(\"1.9\"): +if version.parse(version.parse(torch.__version__).base_version) < version.parse(\"1.9\"):",
        "core_API": "parse"
    },
    {
        "commit_hash": "26df10c0b737adc5419940a098a89e8e1a733e0c",
        "index": "b4ee105..a98895f 100644",
        "commit_message": "fix max pool in nest\n\n",
        "file": "vit-pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def Aggregate(dim, dim_out):",
            "return nn.Sequential(",
            "nn.Conv2d(dim, dim_out, 3, padding = 1),",
            "ChanNorm(dim_out),",
            "-        nn.MaxPool2d(2)",
            "+        nn.MaxPool2d(3, stride = 2, padding = 1)",
            ")",
            "",
            "class Transformer(nn.Module):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '3'), position=1, insert_id=1562295)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1562296)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1562297)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1562298)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1562299)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'stride'), position=0, insert_id=1562300)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1562301)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=integer, text=2), position=2)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'padding'), position=0, insert_id=1562302)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1562303)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '1'), position=2, insert_id=1562304)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 2278,
        "neg_line": [
            "-nn.MaxPool2d(2)"
        ],
        "pos_line": [
            "+nn.MaxPool2d(3, stride = 2, padding = 1)"
        ],
        "core_change": "-nn.MaxPool2d(2) +nn.MaxPool2d(3, stride = 2, padding = 1)",
        "core_API": "Sequential"
    },
    {
        "commit_hash": "c317aaa3de289fc0a69e6b1001ffc586bb4d9c9a",
        "index": "5ff9eb8c..a921b5bd 100644",
        "commit_message": "Refactor library namespaces [pre-release][0.6-rc1] (#1412)\n\n* flake fixes\n\n* initial flake8 fixeS\n\n* remove top level from kornia.color\n\n* kornia filters\n\n* kornia losses\n\n* kornia features\n\n* geomtry and all ok\n\n* removed jit module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* apply formatting and few fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add keep block for isort\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* skip init\n\n* fix the docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove old code\n\n* simplify ci workflow\n\n* fix circular dependency\n\n* few format fixes\n\n* fix code format test\n\n* remove kornia.jit imports\n\n* final fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* ci fixes\n\n* add versioneer\n\n* fix pnp import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* exclude version files from pre-commit\n\n* exclude files precommit\n\n* update to pytorch 1.10 and add fixes\n\n* Update tests_cpu.yml\n\n* Update setup_dev_env.sh\n\n* Update tests_cpu.yml\n\n* undo versioneer\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix no_grad\n\n* Apply suggestions from code review\n\n* fix skip windows tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update test_integrated.py\n\n* Apply suggestions from code review\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class VonMisesKernel(nn.Module):",
            "frange = frange.reshape(-1, 1, 1)",
            "weights = torch.zeros([2 * n + 1])",
            "weights[: n + 1] = torch.sqrt(b_coeffs)",
            "-        weights[n + 1 :] = torch.sqrt(b_coeffs[1:])",
            "+        weights[n + 1:] = torch.sqrt(b_coeffs[1:])",
            "weights = weights.reshape(-1, 1, 1)",
            "self.register_buffer('emb0', emb0)",
            "self.register_buffer('frange', frange)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2280,
        "neg_line": [
            "-weights[n + 1 :] = torch.sqrt(b_coeffs[1:])"
        ],
        "pos_line": [
            "+weights[n + 1:] = torch.sqrt(b_coeffs[1:])"
        ],
        "core_change": "-weights[n + 1 :] = torch.sqrt(b_coeffs[1:]) +weights[n + 1:] = torch.sqrt(b_coeffs[1:])",
        "core_API": "reshape"
    },
    {
        "commit_hash": "093e901c979c0f8b0af92f79010b53db1ddcc972",
        "index": "e80a4c3..fcad48d 100644",
        "commit_message": "fix deprecation warnings; remove pydoop dependency; update README\n\n",
        "file": "TensorFlowOnSpark.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def evaluate(dataset):",
            "saver = tf.train.Saver(variables_to_restore)",
            "",
            "# Build the summary operation based on the TF collection of Summaries.",
            "-    summary_op = tf.merge_all_summaries()",
            "+    summary_op = tf.summary.merge_all()",
            "",
            "graph_def = tf.get_default_graph().as_graph_def()",
            "-    summary_writer = tf.train.SummaryWriter(FLAGS.eval_dir,",
            "+    summary_writer = tf.summary.FileWriter(FLAGS.eval_dir,",
            "graph_def=graph_def)",
            "",
            "while True:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2213644)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2213645)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'merge_all'), position=2, insert_id=2213646)",
            "Update(target_node=ASTNode(type=identifier, text=SummaryWriter), value='FileWriter')",
            "Update(target_node=ASTNode(type=identifier, text=merge_all_summaries), value='summary')",
            "Update(target_node=ASTNode(type=identifier, text=train), value='summary')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 2281,
        "neg_line": [
            "-summary_op = tf.merge_all_summaries()",
            "-summary_writer = tf.train.SummaryWriter(FLAGS.eval_dir,"
        ],
        "pos_line": [
            "+summary_op = tf.summary.merge_all()",
            "+summary_writer = tf.summary.FileWriter(FLAGS.eval_dir,"
        ],
        "core_change": "-summary_op = tf.merge_all_summaries() +summary_op = tf.summary.merge_all() -summary_writer = tf.train.SummaryWriter(FLAGS.eval_dir, +summary_writer = tf.summary.FileWriter(FLAGS.eval_dir,",
        "core_API": "Saver"
    },
    {
        "commit_hash": "8f59f13b8fa96bc841d6f154d47b1d63359c8ce4",
        "index": "6edf670a..6730b655 100644",
        "commit_message": "Fix a specific test on Windows (AFTER corenlp is downloaded and the setup script is run)\n\n",
        "file": "stanza.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def test_annotators_and_output_format(corenlp_client):",
            "\"\"\" Test setting the annotators and output_format \"\"\"",
            "ann = corenlp_client.annotate(FRENCH_DOC, properties=FRENCH_EXTRA_PROPS,",
            "annotators=\"tokenize,ssplit,mwt,pos\", output_format=\"json\")",
            "-    assert FRENCH_JSON_GOLD == ann",
            "+    assert ann == FRENCH_JSON_GOLD"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Move(target_node=ASTNode(type=identifier, text=FRENCH_JSON_GOLD), node=ASTNode(type=comparison_operator), position=2)",
            "Move(target_node=ASTNode(type===, text===), node=ASTNode(type=comparison_operator), position=3)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 2282,
        "neg_line": [
            "-assert FRENCH_JSON_GOLD == ann"
        ],
        "pos_line": [
            "+assert ann == FRENCH_JSON_GOLD"
        ],
        "core_change": "-assert FRENCH_JSON_GOLD == ann +assert ann == FRENCH_JSON_GOLD",
        "core_API": "annotate"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "489b0bc1..6a5cc119 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ScalarMix(torch.nn.Module):",
            "return self.gamma * sum(pieces)",
            "",
            "else:",
            "-            mask_float = mask.float()",
            "-            broadcast_mask = mask_float.unsqueeze(-1)",
            "+            broadcast_mask = mask.unsqueeze(-1)",
            "input_dim = tensors[0].size(-1)",
            "-            num_elements_not_masked = torch.sum(mask_float) * input_dim",
            "+            num_elements_not_masked = torch.sum(mask) * input_dim",
            "",
            "pieces = []",
            "for weight, tensor in zip(normed_weights, tensors):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=mask_float), value='broadcast_mask')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=float), value='unsqueeze')",
            "Update(target_node=ASTNode(type=identifier, text=mask_float), value='mask')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=identifier, text=broadcast_mask))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=mask_float))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=unsqueeze))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 2283,
        "neg_line": [
            "-mask_float = mask.float()",
            "-broadcast_mask = mask_float.unsqueeze(-1)",
            "-num_elements_not_masked = torch.sum(mask_float) * input_dim"
        ],
        "pos_line": [
            "+broadcast_mask = mask.unsqueeze(-1)",
            "+num_elements_not_masked = torch.sum(mask) * input_dim"
        ],
        "core_change": "-mask_float = mask.float() -broadcast_mask = mask_float.unsqueeze(-1) +broadcast_mask = mask.unsqueeze(-1) -num_elements_not_masked = torch.sum(mask_float) * input_dim +num_elements_not_masked = torch.sum(mask) * input_dim",
        "core_API": "float"
    },
    {
        "commit_hash": "e3c052382bd76dcd1184fdd33c5e8932aea793e6",
        "index": "6a4c3da5..053993ca 100644",
        "commit_message": "fix loading always best_model when continue\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_last_checkpoint(path):",
            "key_file_names = [fn for fn in file_names if key in fn]",
            "if last_model is None and len(key_file_names) > 0:",
            "last_model = max(key_file_names, key=os.path.getctime)",
            "-            last_model_num = os.path.getctime(last_model)",
            "+            last_model_num = torch.load(last_model)['step']",
            "",
            "if last_model is not None:",
            "last_models[key] = last_model"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('subscript', None), position=2, insert_id=1264261)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1264262)",
            "Insert(target_node=IN(type=subscript), node=('string', \"'step'\"), position=2, insert_id=1264263)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1264264)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=os), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=path), value='load')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=getctime))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 2285,
        "neg_line": [
            "-last_model_num = os.path.getctime(last_model)"
        ],
        "pos_line": [
            "+last_model_num = torch.load(last_model)['step']"
        ],
        "core_change": "-last_model_num = os.path.getctime(last_model) +last_model_num = torch.load(last_model)['step']",
        "core_API": "getctime"
    },
    {
        "commit_hash": "dda669a12c4df7b282a1378e251f8314e6179bcb",
        "index": "b93ad16..38868cc 100644",
        "commit_message": "Fix Flask REST API (#7210)\n\n* Update restapi.py\n\n* Update restapi.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Cleanup\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def predict():",
            "if __name__ == \"__main__\":",
            "parser = argparse.ArgumentParser(description=\"Flask API exposing YOLOv5 model\")",
            "parser.add_argument(\"--port\", default=5000, type=int, help=\"port number\")",
            "-    args = parser.parse_args()",
            "+    opt = parser.parse_args()",
            "+",
            "+    # Fix known issue urllib.error.HTTPError 403: rate limit exceeded https://github.com/ultralytics/yolov5/pull/7210",
            "+    torch.hub._validate_not_a_forked_repo = lambda a, b, c: True",
            "",
            "model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", force_reload=True)  # force_reload to recache",
            "-    app.run(host=\"0.0.0.0\", port=args.port)  # debug=True causes Restarting with stat",
            "+    app.run(host=\"0.0.0.0\", port=opt.port)  # debug=True causes Restarting with stat"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=5, insert_id=1295079)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1295080)",
            "Update(target_node=ASTNode(type=identifier, text=args), value='opt')",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=1295081)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1295082)",
            "Insert(target_node=IN(type=assignment), node=('lambda', None), position=2, insert_id=1295083)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1295084)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1295085)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '_validate_not_a_forked_repo'), position=2, insert_id=1295086)",
            "Insert(target_node=IN(type=lambda), node=('lambda', 'lambda'), position=0, insert_id=1295087)",
            "Insert(target_node=IN(type=lambda), node=('lambda_parameters', None), position=1, insert_id=1295088)",
            "Insert(target_node=IN(type=lambda), node=(':', ':'), position=2, insert_id=1295089)",
            "Insert(target_node=IN(type=lambda), node=('true', 'True'), position=3, insert_id=1295090)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1295091)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1295092)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'hub'), position=2, insert_id=1295093)",
            "Insert(target_node=IN(type=lambda_parameters), node=('identifier', 'a'), position=0, insert_id=1295094)",
            "Insert(target_node=IN(type=lambda_parameters), node=(',', ','), position=1, insert_id=1295095)",
            "Insert(target_node=IN(type=lambda_parameters), node=('identifier', 'b'), position=2, insert_id=1295096)",
            "Insert(target_node=IN(type=lambda_parameters), node=(',', ','), position=3, insert_id=1295097)",
            "Insert(target_node=IN(type=lambda_parameters), node=('identifier', 'c'), position=4, insert_id=1295098)",
            "Update(target_node=ASTNode(type=identifier, text=args), value='opt')"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 2286,
        "neg_line": [
            "-args = parser.parse_args()",
            "-app.run(host=\"0.0.0.0\", port=args.port)  # debug=True causes Restarting with stat"
        ],
        "pos_line": [
            "+opt = parser.parse_args()",
            "+",
            "+# Fix known issue urllib.error.HTTPError 403: rate limit exceeded https://github.com/ultralytics/yolov5/pull/7210",
            "+torch.hub._validate_not_a_forked_repo = lambda a, b, c: True",
            "+app.run(host=\"0.0.0.0\", port=opt.port)  # debug=True causes Restarting with stat"
        ],
        "core_change": "-args = parser.parse_args() +opt = parser.parse_args() + +# Fix known issue urllib.error.HTTPError 403: rate limit exceeded https://github.com/ultralytics/yolov5/pull/7210 +torch.hub._validate_not_a_forked_repo = lambda a, b, c: True -app.run(host=\"0.0.0.0\", port=args.port)  # debug=True causes Restarting with stat +app.run(host=\"0.0.0.0\", port=opt.port)  # debug=True causes Restarting with stat",
        "core_API": "ArgumentParser"
    },
    {
        "commit_hash": "0c1f7cba1acf3ecdecea47b914072dfef1878beb",
        "index": "0966a3dab5..4d12f36fa7 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def multinomial(",
            "samples_stack.append(indices)",
            "samples_flat = tf.stack(samples_stack)",
            "return tf.convert_to_tensor(",
            "-                tf.reshape(samples_flat, orig_probs_shape[:-1] + [num_samples]))",
            "+                tf.reshape(samples_flat, orig_probs_shape[:-1] + [num_samples])",
            "+            )",
            "else:",
            "if len(probs.numpy().shape) == 1:",
            "probs = tf.expand_dims(probs, axis=0)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2288,
        "neg_line": [
            "-tf.reshape(samples_flat, orig_probs_shape[:-1] + [num_samples]))"
        ],
        "pos_line": [
            "+tf.reshape(samples_flat, orig_probs_shape[:-1] + [num_samples])",
            "+)"
        ],
        "core_change": "-tf.reshape(samples_flat, orig_probs_shape[:-1] + [num_samples])) +tf.reshape(samples_flat, orig_probs_shape[:-1] + [num_samples]) +)",
        "core_API": "append"
    },
    {
        "commit_hash": "05ff8801d1591cc13a4ece52fadb59c5443d32ab",
        "index": "deb07a5c..e5d0ff47 100644",
        "commit_message": "config, benchmark notebook, synthesis fixed\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def synthesis(model,",
            "style_mel = compute_style_mel(style_wav, ap, use_cuda)",
            "# preprocess the given text",
            "inputs = text_to_seqvec(text, CONFIG, use_cuda)",
            "-    speaker_id = speaker_id_var = torch.from_numpy(speaker_id).unsqueeze(0)",
            "+    speaker_id = np.asarray(speaker_id)",
            "+    speaker_id = torch.from_numpy(speaker_id).unsqueeze(0)",
            "if use_cuda:",
            "speaker_id.cuda()",
            "# synthesize voice",
            "decoder_output, postnet_output, alignments, stop_tokens = run_model(",
            "-        model, inputs, CONFIG, truncated, style_mel)",
            "+        model, inputs, speaker_id, CONFIG, truncated, style_mel)",
            "# convert outputs to numpy",
            "postnet_output, decoder_output, alignment = parse_outputs(",
            "postnet_output, decoder_output, alignments)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=1272670)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1272671)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'speaker_id'), position=5, insert_id=1272672)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=1272673)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('ERROR', None), position=1, insert_id=1272674)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=2, insert_id=1272675)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'from_numpy'), position=3, insert_id=1272676)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1272677)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'speaker_id'), position=0, insert_id=1272678)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type==, text==), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=torch), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=from_numpy), value='asarray')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1272679)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'speaker_id'), position=1, insert_id=1272680)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1272681)",
            "Update(target_node=ASTNode(type=identifier, text=speaker_id_var), value='np')"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 2289,
        "neg_line": [
            "-speaker_id = speaker_id_var = torch.from_numpy(speaker_id).unsqueeze(0)",
            "-model, inputs, CONFIG, truncated, style_mel)"
        ],
        "pos_line": [
            "+speaker_id = np.asarray(speaker_id)",
            "+speaker_id = torch.from_numpy(speaker_id).unsqueeze(0)",
            "+model, inputs, speaker_id, CONFIG, truncated, style_mel)"
        ],
        "core_change": "-speaker_id = speaker_id_var = torch.from_numpy(speaker_id).unsqueeze(0) +speaker_id = np.asarray(speaker_id) +speaker_id = torch.from_numpy(speaker_id).unsqueeze(0) -model, inputs, CONFIG, truncated, style_mel) +model, inputs, speaker_id, CONFIG, truncated, style_mel)",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "7d1e2acfea6134d18dd5f3db366b23e2d3f6e3a8",
        "index": "401297a8..029b3b7d 100644",
        "commit_message": "minor fix, gpu allow memory growth\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "for idx, data in enumerate(gen):",
            "avg_mem_usage += cur_usage",
            "count += 1",
            "tl.logging.info(",
            "-            \"[*] {} iteration: memory usage {:.2f}MB, consume time {:.4f}s\".format(",
            "-                idx, cur_usage / (1024 * 1024), consume_time",
            "+            \"[*] {} iteration: memory usage {:.2f}MB, consume time {:.4f}s, loss {:.4f}\".format(",
            "+                idx, cur_usage / (1024 * 1024), consume_time, loss",
            ")",
            ")"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"[*] {} iteration: memory usage {:.2f}MB, consume time {:.4f}s\"), value='\"[*] {} iteration: memory usage {:.2f}MB, consume time {:.4f}s, loss {:.4f}\"')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=2252105)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'loss'), position=7, insert_id=2252106)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 2290,
        "neg_line": [
            "-\"[*] {} iteration: memory usage {:.2f}MB, consume time {:.4f}s\".format(",
            "-idx, cur_usage / (1024 * 1024), consume_time"
        ],
        "pos_line": [
            "+\"[*] {} iteration: memory usage {:.2f}MB, consume time {:.4f}s, loss {:.4f}\".format(",
            "+idx, cur_usage / (1024 * 1024), consume_time, loss"
        ],
        "core_change": "-\"[*] {} iteration: memory usage {:.2f}MB, consume time {:.4f}s\".format( -idx, cur_usage / (1024 * 1024), consume_time +\"[*] {} iteration: memory usage {:.2f}MB, consume time {:.4f}s, loss {:.4f}\".format( +idx, cur_usage / (1024 * 1024), consume_time, loss",
        "core_API": "info"
    },
    {
        "commit_hash": "cde623fbbd0da889cbe4b4aa03bbc905911030a9",
        "index": "5d4abf0dd..edb4a513a 100644",
        "commit_message": "Apply fix from PyTorch in code for older versions and clean Part 8\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TorchHook:",
            "# 5. Put instead the hooked one",
            "setattr(torch_module, func, new_func)",
            "",
            "+        # Hard fix for PyTorch versions < 1.0.2",
            "+        syft.torch.apply_fix16922(self.torch)",
            "+",
            "torch_modules = syft.torch.torch_modules",
            "-        # torch_modules = {\"torch.nn.functional\": torch.nn.functional}",
            "",
            "for module_name, torch_module in torch_modules.items():",
            "for func in dir(torch_module):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=1462807)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1462808)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1462809)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1462810)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1462811)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1462812)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'apply_fix16922'), position=2, insert_id=1462813)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1462814)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1462815)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1462816)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'syft'), position=0, insert_id=1462817)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1462818)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=2, insert_id=1462819)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1462820)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1462821)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=2, insert_id=1462822)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 2292,
        "neg_line": [
            "-# torch_modules = {\"torch.nn.functional\": torch.nn.functional}"
        ],
        "pos_line": [
            "+# Hard fix for PyTorch versions < 1.0.2",
            "+syft.torch.apply_fix16922(self.torch)",
            "+"
        ],
        "core_change": "+# Hard fix for PyTorch versions < 1.0.2 +syft.torch.apply_fix16922(self.torch) + -# torch_modules = {\"torch.nn.functional\": torch.nn.functional}",
        "core_API": "apply_fix16922"
    },
    {
        "commit_hash": "e8b41cfeec0b70b692e1a042e6f1e542f1fe446d",
        "index": "26a42e95..497a7e42 100644",
        "commit_message": "Fix `DBLP` download, move `get_edge_index` to `testing` package (#6901)\n\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_num_nodes_size(FeatureStore, GraphStore):",
            "assert num_nodes(feature_store, graph_store, 'x') == 100",
            "",
            "# Infer num nodes and size from edges:",
            "-    xy = get_edge_index(100, 50, 20)",
            "+    xy = get_random_edge_index(100, 50, 20)",
            "graph_store.put_edge_index(xy, edge_type=('x', 'to', 'y'), layout='coo',",
            "size=(100, 50))",
            "assert num_nodes(feature_store, graph_store, 'y') == 50"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=get_edge_index), value='get_random_edge_index')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2297,
        "neg_line": [
            "-xy = get_edge_index(100, 50, 20)"
        ],
        "pos_line": [
            "+xy = get_random_edge_index(100, 50, 20)"
        ],
        "core_change": "-xy = get_edge_index(100, 50, 20) +xy = get_random_edge_index(100, 50, 20)",
        "core_API": "put_edge_index"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "cd1b7a7f..3cd31103 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ArrayField(Field[numpy.ndarray]):",
            "slicing_shape = slicing_shape + [0 for _ in range(len(max_shape) - len(self.array.shape))]",
            "slices = [slice(0, x) for x in slicing_shape]",
            "return_array[slices] = self.array",
            "-        tensor = Variable(torch.from_numpy(return_array), volatile=not for_training)",
            "+        tensor = torch.from_numpy(return_array)",
            "return tensor if cuda_device == -1 else tensor.cuda(cuda_device)",
            "",
            "@overrides"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=volatile))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=not, text=not))",
            "Delete(target_node=ASTNode(type=identifier, text=for_training))",
            "Delete(target_node=ASTNode(type=not_operator))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 2299,
        "neg_line": [
            "-tensor = Variable(torch.from_numpy(return_array), volatile=not for_training)"
        ],
        "pos_line": [
            "+tensor = torch.from_numpy(return_array)"
        ],
        "core_change": "-tensor = Variable(torch.from_numpy(return_array), volatile=not for_training) +tensor = torch.from_numpy(return_array)",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "918c8a74b095a9ca65e3802bceb12d48b1208102",
        "index": "893d8e4bc2..2a819d424c 100644",
        "commit_message": "small fix\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def linspace_helper(start, stop, num, axis=None, *, dtype=None, device):",
            "else:",
            "res = [linspace_method(start, stp, num, device=device) for stp in stop]",
            "else:",
            "-        return linspace_method(start, stop, num, dtype=torch.float64, device=device)",
            "+        return linspace_method(start, stop, num, dtype=dtype, device=device)",
            "res = torch.cat(res, -1).reshape(sos_shape + [num])",
            "if axis is not None:",
            "res = torch.transpose(res, axis, -1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='dtype')",
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=identifier, text=torch), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float64))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2300,
        "neg_line": [
            "-return linspace_method(start, stop, num, dtype=torch.float64, device=device)"
        ],
        "pos_line": [
            "+return linspace_method(start, stop, num, dtype=dtype, device=device)"
        ],
        "core_change": "-return linspace_method(start, stop, num, dtype=torch.float64, device=device) +return linspace_method(start, stop, num, dtype=dtype, device=device)",
        "core_API": "cat"
    },
    {
        "commit_hash": "0a5c555c33c1204bc4404bf6a6bc3c5690a804e6",
        "index": "b318ebc0..5ef2c9ee 100644",
        "commit_message": "Fixing pep8 lint errors\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Categorical(Distribution):",
            "# vs is an array, so the support must be of type array",
            "r_np = _vs.shape[0]",
            "c_np = _vs.shape[1]",
            "-                ix = np.expand_dims(np.arange(r_np), axis=1)",
            "-                b = torch.ones(r_np, 1)",
            "+                np.expand_dims(np.arange(r_np), axis=1)",
            "+                torch.ones(r_np, 1)",
            "return (_vs[np.arange(r_np), torch.Tensor(list(x)).numpy().astype(int)]",
            ".reshape(r_np, 1).tolist()",
            "for x in itertools.product(torch.arange(0, c_np), repeat=r_np))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=ix))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=identifier, text=b))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 2302,
        "neg_line": [
            "-ix = np.expand_dims(np.arange(r_np), axis=1)",
            "-b = torch.ones(r_np, 1)"
        ],
        "pos_line": [
            "+np.expand_dims(np.arange(r_np), axis=1)",
            "+torch.ones(r_np, 1)"
        ],
        "core_change": "-ix = np.expand_dims(np.arange(r_np), axis=1) -b = torch.ones(r_np, 1) +np.expand_dims(np.arange(r_np), axis=1) +torch.ones(r_np, 1)",
        "core_API": "expand_dims"
    },
    {
        "commit_hash": "353f6231addb10e1d6b4ed4b62ab4ea89a9e909a",
        "index": "058ca045..d44bcaa8 100644",
        "commit_message": "Fix atss uint8 (#2269)\n\n* fix atss uint8 warning\n\n* fix bool sum error\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ATSSHead(AnchorHead):",
            "",
            "# map up to original set of anchors",
            "if unmap_outputs:",
            "+            inside_flags = inside_flags.type(torch.bool)",
            "num_total_anchors = flat_anchors.size(0)",
            "anchors = unmap(anchors, num_total_anchors, inside_flags)",
            "labels = unmap(labels, num_total_anchors, inside_flags)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=640492)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=640493)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=640494)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'inside_flags'), position=0, insert_id=640495)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=640496)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=640497)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=640498)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=640499)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'inside_flags'), position=0, insert_id=640500)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=640501)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type'), position=2, insert_id=640502)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=640503)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=640504)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=640505)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=640506)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=640507)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=640508)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 2303,
        "neg_line": [],
        "pos_line": [
            "+inside_flags = inside_flags.type(torch.bool)"
        ],
        "core_change": "+inside_flags = inside_flags.type(torch.bool)",
        "core_API": "type"
    },
    {
        "commit_hash": "02a79050aee3e3385cda5b6a900df8bf49371606",
        "index": "e03baacb..7f4d017b 100644",
        "commit_message": "fix: add get_value in tests\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class AttentionTest(tf.test.TestCase, parameterized.TestCase):",
            "attention_layer.concat_score_weight = 1",
            "attention_layer.build(input_shape=([1, 1, 1], [1, 1, 1]))",
            "attention_layer.scale = 2.",
            "-    actual = attention_layer._calculate_scores(query=q, key=k)",
            "+    actual = keras.backend.get_value(",
            "+            attention_layer._calculate_scores(query=q, key=k))",
            "",
            "# Expected tensor of shape [1, 1, 1].",
            "# expected000 = tanh(2*(1.1+1.6)) = 0.9999592018254402"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2513301)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2513302)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2513303)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2513304)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get_value'), position=2, insert_id=2513305)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2513306)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2513307)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'keras'), position=0, insert_id=2513308)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2513309)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'backend'), position=2, insert_id=2513310)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 2305,
        "neg_line": [
            "-actual = attention_layer._calculate_scores(query=q, key=k)"
        ],
        "pos_line": [
            "+actual = keras.backend.get_value(",
            "+attention_layer._calculate_scores(query=q, key=k))"
        ],
        "core_change": "-actual = attention_layer._calculate_scores(query=q, key=k) +actual = keras.backend.get_value( +attention_layer._calculate_scores(query=q, key=k))",
        "core_API": "build"
    },
    {
        "commit_hash": "f626a38fd7c3ba279bcd534a537e0ab638c18727",
        "index": "d929d38f3..3b06a53e6 100644",
        "commit_message": "fix sharded_ddp mode\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Trainer:",
            "iterator: Iterable[Dict[str, torch.Tensor]],",
            "reporter: SubReporter,",
            "options: TrainerOptions,",
            "+        distributed_option: DistributedOption,",
            ") -> None:",
            "assert check_argument_types()",
            "ngpu = options.ngpu",
            "no_forward_run = options.no_forward_run",
            "-        distributed = isinstance(model, torch.nn.parallel.DistributedDataParallel)",
            "+        distributed = distributed_option.distributed",
            "",
            "model.eval()"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=attribute), position=2)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'distributed_option'), position=6, insert_id=1336572)",
            "Insert(target_node=ASTNode(type=ERROR), node=(':', ':'), position=7, insert_id=1336573)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'DistributedOption'), position=8, insert_id=1336574)",
            "Insert(target_node=ASTNode(type=ERROR), node=(',', ','), position=9, insert_id=1336575)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='distributed_option')",
            "Update(target_node=ASTNode(type=identifier, text=nn), value='distributed')",
            "Delete(target_node=ASTNode(type=identifier, text=isinstance))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=model))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=parallel))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=DistributedDataParallel))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 2308,
        "neg_line": [
            "-distributed = isinstance(model, torch.nn.parallel.DistributedDataParallel)"
        ],
        "pos_line": [
            "+distributed_option: DistributedOption,",
            "+distributed = distributed_option.distributed"
        ],
        "core_change": "+distributed_option: DistributedOption, -distributed = isinstance(model, torch.nn.parallel.DistributedDataParallel) +distributed = distributed_option.distributed",
        "core_API": "eval"
    },
    {
        "commit_hash": "b8caa301299a3ca743c97bb9299386467b39044e",
        "index": "7dd91dd..6a891a4 100644",
        "commit_message": "fix path to allow for restoring saved models\n",
        "file": "skflow.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TensorFlowEstimator(BaseEstimator):",
            "if not os.path.exists(saver_filename):",
            "raise ValueError(\"Restore folder doesn't contain saver defintion.\")",
            "with open(saver_filename) as fsaver:",
            "-                saver_def = tf.python.training.saver_pb2.SaverDef()",
            "+                saver_def = tf.python.training.saver.saver_pb2.SaverDef()",
            "text_format.Merge(fsaver.read(), saver_def)",
            "self._saver = tf.train.Saver(saver_def=saver_def)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2170236)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'saver'), position=2, insert_id=2170237)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 2309,
        "neg_line": [
            "-saver_def = tf.python.training.saver_pb2.SaverDef()"
        ],
        "pos_line": [
            "+saver_def = tf.python.training.saver.saver_pb2.SaverDef()"
        ],
        "core_change": "-saver_def = tf.python.training.saver_pb2.SaverDef() +saver_def = tf.python.training.saver.saver_pb2.SaverDef()",
        "core_API": "exists"
    },
    {
        "commit_hash": "fdc907675050cb9e3aa887bf89d6c92519178349",
        "index": "1ee2157e..0d7ca202 100644",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def class_balanced_cross_entropy(pred, label, name='cross_entropy_loss'):",
            "eps = 1e-12",
            "loss_pos = -beta * tf.reduce_mean(y * tf.log(z + eps))",
            "loss_neg = (1. - beta) * tf.reduce_mean((1. - y) * tf.log(1. - z + eps))",
            "-    cost = tf.sub(loss_pos, loss_neg, name=name)",
            "+    cost = tf.subtract(loss_pos, loss_neg, name=name)",
            "return cost"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=sub), value='subtract')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2310,
        "neg_line": [
            "-cost = tf.sub(loss_pos, loss_neg, name=name)"
        ],
        "pos_line": [
            "+cost = tf.subtract(loss_pos, loss_neg, name=name)"
        ],
        "core_change": "-cost = tf.sub(loss_pos, loss_neg, name=name) +cost = tf.subtract(loss_pos, loss_neg, name=name)",
        "core_API": "reduce_mean"
    },
    {
        "commit_hash": "d4c0895cb98f5aa6f112a6f28533a49ad9d6312a",
        "index": "2936cbbf..219eda9c 100644",
        "commit_message": "Bump torch, torchvision, python versions (#2663)\n\n* Bump torch, torchvision, python versions\n\n* Fix .nonzero() usage\n\n* Add missing licenses\n\n* Fix more .nonzero() errors\n\n* Fix errors\n\n* Fix pyplot.hist invocation\n\n* Work around torch 1.6 incompatibility with pyplot.hist\n\n* Fix more pyplot.hist() errors\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def compute_posterior_stats(X, Y, msq, lam, eta1, xisq, c, sigma, jitter=1.0e-4)",
            "std = ((var * vec.unsqueeze(-1)).sum(-2) * vec.unsqueeze(-1)).sum(-2).clamp(min=0.0).sqrt()",
            "",
            "active_quad_dims = (((mu - 4.0 * std) > 0.0) | ((mu + 4.0 * std) < 0.0)) & (mu.abs() > 1.0e-4).bool()",
            "-    active_quad_dims = active_quad_dims.nonzero()",
            "+    active_quad_dims = active_quad_dims.nonzero(as_tuple=False)",
            "",
            "active_quadratic_dims = np.stack([left_dims[active_quad_dims].data.numpy().flatten(),",
            "right_dims[active_quad_dims].data.numpy().flatten()], axis=1)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=1, insert_id=690906)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'as_tuple'), position=0, insert_id=690907)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=690908)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=690909)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 2311,
        "neg_line": [
            "-active_quad_dims = active_quad_dims.nonzero()"
        ],
        "pos_line": [
            "+active_quad_dims = active_quad_dims.nonzero(as_tuple=False)"
        ],
        "core_change": "-active_quad_dims = active_quad_dims.nonzero() +active_quad_dims = active_quad_dims.nonzero(as_tuple=False)",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "c60ccbad4629fa3d5b252ab8036a382f722a7c7b",
        "index": "a2f5377cff..f311d7461a 100644",
        "commit_message": "[carla] [rllib] Add support for carla nav planner and scenarios from paper (#1382)\n\n* wip\n\n* Sat Dec 30 15:07:28 PST 2017\n\n* log video\n\n* video doesn't work well\n\n* scenario integration\n\n* Sat Dec 30 17:30:22 PST 2017\n\n* Sat Dec 30 17:31:05 PST 2017\n\n* Sat Dec 30 17:31:32 PST 2017\n\n* Sat Dec 30 17:32:16 PST 2017\n\n* Sat Dec 30 17:34:11 PST 2017\n\n* Sat Dec 30 17:34:50 PST 2017\n\n* Sat Dec 30 17:35:34 PST 2017\n\n* Sat Dec 30 17:38:49 PST 2017\n\n* Sat Dec 30 17:40:39 PST 2017\n\n* Sat Dec 30 17:43:00 PST 2017\n\n* Sat Dec 30 17:43:04 PST 2017\n\n* Sat Dec 30 17:45:56 PST 2017\n\n* Sat Dec 30 17:46:26 PST 2017\n\n* Sat Dec 30 17:47:02 PST 2017\n\n* Sat Dec 30 17:51:53 PST 2017\n\n* Sat Dec 30 17:52:54 PST 2017\n\n* Sat Dec 30 17:56:43 PST 2017\n\n* Sat Dec 30 18:27:07 PST 2017\n\n* Sat Dec 30 18:27:52 PST 2017\n\n* fix train\n\n* Sat Dec 30 18:41:51 PST 2017\n\n* Sat Dec 30 18:54:11 PST 2017\n\n* Sat Dec 30 18:56:22 PST 2017\n\n* Sat Dec 30 19:05:04 PST 2017\n\n* Sat Dec 30 19:05:23 PST 2017\n\n* Sat Dec 30 19:11:53 PST 2017\n\n* Sat Dec 30 19:14:31 PST 2017\n\n* Sat Dec 30 19:16:20 PST 2017\n\n* Sat Dec 30 19:18:05 PST 2017\n\n* Sat Dec 30 19:18:45 PST 2017\n\n* Sat Dec 30 19:22:44 PST 2017\n\n* Sat Dec 30 19:24:41 PST 2017\n\n* Sat Dec 30 19:26:57 PST 2017\n\n* Sat Dec 30 19:40:37 PST 2017\n\n* wip models\n\n* reward bonus\n\n* test prep\n\n* Sun Dec 31 18:45:25 PST 2017\n\n* Sun Dec 31 18:58:28 PST 2017\n\n* Sun Dec 31 18:59:34 PST 2017\n\n* Sun Dec 31 19:03:33 PST 2017\n\n* Sun Dec 31 19:05:05 PST 2017\n\n* Sun Dec 31 19:09:25 PST 2017\n\n* fix train\n\n* kill\n\n* add tuple preprocessor\n\n* Sun Dec 31 20:38:33 PST 2017\n\n* Sun Dec 31 22:51:24 PST 2017\n\n* Sun Dec 31 23:14:13 PST 2017\n\n* Sun Dec 31 23:16:04 PST 2017\n\n* Mon Jan  1 00:08:35 PST 2018\n\n* Mon Jan  1 00:10:48 PST 2018\n\n* Mon Jan  1 01:08:31 PST 2018\n\n* Mon Jan  1 14:45:44 PST 2018\n\n* Mon Jan  1 14:54:56 PST 2018\n\n* Mon Jan  1 17:29:29 PST 2018\n\n* switch to euclidean dists\n\n* Mon Jan  1 17:39:27 PST 2018\n\n* Mon Jan  1 17:41:47 PST 2018\n\n* Mon Jan  1 17:44:18 PST 2018\n\n* Mon Jan  1 17:47:09 PST 2018\n\n* Mon Jan  1 20:31:02 PST 2018\n\n* Mon Jan  1 20:39:33 PST 2018\n\n* Mon Jan  1 20:40:55 PST 2018\n\n* Mon Jan  1 20:55:06 PST 2018\n\n* Mon Jan  1 21:05:52 PST 2018\n\n* fix env path\n\n* merge richards fix\n\n* fix hash\n\n* Mon Jan  1 22:04:00 PST 2018\n\n* Mon Jan  1 22:25:29 PST 2018\n\n* Mon Jan  1 22:30:42 PST 2018\n\n* simplified reward function\n\n* add framestack\n\n* add env configs\n\n* simplify speed reward\n\n* Tue Jan  2 17:36:15 PST 2018\n\n* Tue Jan  2 17:49:16 PST 2018\n\n* Tue Jan  2 18:10:38 PST 2018\n\n* add lane keeping simple mode\n\n* Tue Jan  2 20:25:26 PST 2018\n\n* Tue Jan  2 20:30:30 PST 2018\n\n* Tue Jan  2 20:33:26 PST 2018\n\n* Tue Jan  2 20:41:42 PST 2018\n\n* ppo lane keep\n\n* simplify discrete actions\n\n* Tue Jan  2 21:41:05 PST 2018\n\n* Tue Jan  2 21:49:03 PST 2018\n\n* Tue Jan  2 22:12:23 PST 2018\n\n* Tue Jan  2 22:14:42 PST 2018\n\n* Tue Jan  2 22:20:59 PST 2018\n\n* Tue Jan  2 22:23:43 PST 2018\n\n* Tue Jan  2 22:26:27 PST 2018\n\n* Tue Jan  2 22:27:20 PST 2018\n\n* Tue Jan  2 22:44:00 PST 2018\n\n* Tue Jan  2 22:57:58 PST 2018\n\n* Tue Jan  2 23:08:51 PST 2018\n\n* Tue Jan  2 23:11:32 PST 2018\n\n* update dqn reward\n\n* Thu Jan  4 12:29:40 PST 2018\n\n* Thu Jan  4 12:30:26 PST 2018\n\n* Update train_dqn.py\n\n* fix\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFPolicy(Policy):",
            "",
            "# TODO(rliaw): Can consider exposing these parameters",
            "self.sess = tf.Session(graph=self.g, config=tf.ConfigProto(",
            "-            intra_op_parallelism_threads=1, inter_op_parallelism_threads=2))",
            "+            intra_op_parallelism_threads=1, inter_op_parallelism_threads=2,",
            "+            gpu_options=tf.GPUOptions(allow_growth=True)))",
            "self.variables = ray.experimental.TensorFlowVariables(self.loss,",
            "self.sess)",
            "self.sess.run(tf.global_variables_initializer())"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2155100)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2155101)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=6, insert_id=2155102)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'gpu_options'), position=0, insert_id=2155103)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2155104)",
            "Insert(target_node=IN(type=keyword_argument), node=('call', None), position=2, insert_id=2155105)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2155106)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2155107)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2155108)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2155109)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'GPUOptions'), position=2, insert_id=2155110)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2155111)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2155112)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'allow_growth'), position=0, insert_id=2155113)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2155114)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=2155115)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2312,
        "neg_line": [
            "-intra_op_parallelism_threads=1, inter_op_parallelism_threads=2))"
        ],
        "pos_line": [
            "+intra_op_parallelism_threads=1, inter_op_parallelism_threads=2,",
            "+gpu_options=tf.GPUOptions(allow_growth=True)))"
        ],
        "core_change": "-intra_op_parallelism_threads=1, inter_op_parallelism_threads=2)) +intra_op_parallelism_threads=1, inter_op_parallelism_threads=2, +gpu_options=tf.GPUOptions(allow_growth=True)))",
        "core_API": "Session"
    },
    {
        "commit_hash": "a41f9e83173c72317aa8909e8e7634cc2f33773f",
        "index": "57e625d3..87f61d3b 100755",
        "commit_message": "update and fix grad bug\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "l = FullyConnected('fc1', l, out_dim=512,",
            "b_init=tf.constant_initializer(0.1))",
            "# fc will have activation summary by default. disable for the output layer",
            "-        logits = FullyConnected('linear', l, out_dim=10, summary_activation=False,",
            "-                                nl=tf.identity)",
            "+        logits = FullyConnected('linear', l, out_dim=10, nl=tf.identity)",
            "prob = tf.nn.softmax(logits, name='output')",
            "",
            "y = one_hot(label, 10)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=summary_activation))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 5,
        "number": 2315,
        "neg_line": [
            "-logits = FullyConnected('linear', l, out_dim=10, summary_activation=False,",
            "-nl=tf.identity)"
        ],
        "pos_line": [
            "+logits = FullyConnected('linear', l, out_dim=10, nl=tf.identity)"
        ],
        "core_change": "-logits = FullyConnected('linear', l, out_dim=10, summary_activation=False, -nl=tf.identity) +logits = FullyConnected('linear', l, out_dim=10, nl=tf.identity)",
        "core_API": "constant_initializer"
    },
    {
        "commit_hash": "47d5c244706ab01859a3931f66066407a9d5efbb",
        "index": "213777a8..58d156e0 100755",
        "commit_message": "Summaries completely changed, distributed mode incompletely changed, various fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class QModel(DistributionModel):",
            "optimization = super(QModel, self).tf_optimization(states, internals, actions, terminal, reward)",
            "",
            "target_optimization = self.target_optimizer.minimize(",
            "-            time=self.time,",
            "+            time=self.timestep,",
            "variables=self.target_network.get_variables(),",
            "source_variables=self.network.get_variables()",
            ")",
            "",
            "-        return tf.group(optimization, target_optimization)",
            "\\ No newline at end of file",
            "+        return tf.group(optimization, target_optimization)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=return_statement), node=ASTNode(type=call), position=1)",
            "Insert(target_node=ASTNode(type=assignment), node=('ERROR', None), position=2, insert_id=2242699)",
            "Insert(target_node=ASTNode(type=assignment), node=('identifier', 'file'), position=3, insert_id=2242700)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=ERROR), node=('ERROR', ' '), position=1, insert_id=2242701)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'No'), position=2, insert_id=2242702)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'newline'), position=3, insert_id=2242703)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'at'), position=4, insert_id=2242704)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'end'), position=5, insert_id=2242705)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'of'), position=6, insert_id=2242706)",
            "Update(target_node=ASTNode(type=identifier, text=time), value='timestep')",
            "Delete(target_node=ASTNode(type=ERROR, text= ))",
            "Delete(target_node=ASTNode(type=identifier, text=No))",
            "Delete(target_node=ASTNode(type=identifier, text=newline))",
            "Delete(target_node=ASTNode(type=identifier, text=at))",
            "Delete(target_node=ASTNode(type=identifier, text=end))",
            "Delete(target_node=ASTNode(type=identifier, text=of))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=file))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 2316,
        "neg_line": [
            "-time=self.time,",
            "-return tf.group(optimization, target_optimization)"
        ],
        "pos_line": [
            "+time=self.timestep,",
            "+return tf.group(optimization, target_optimization)"
        ],
        "core_change": "-time=self.time, +time=self.timestep, -return tf.group(optimization, target_optimization) +return tf.group(optimization, target_optimization)",
        "core_API": "minimize"
    },
    {
        "commit_hash": "1f2bcbef7a1286bda285d82f32c2fed2bacaea4e",
        "index": "9c31ddf0..3e49e65c 100644",
        "commit_message": "fix cuda init (#1953)\n\n* fix cuda init\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_adalam_default_config():",
            "'refit': True,  # Whether to perform refitting at the end of the RANSACs. Generally improves accuracy at the cost of runtime.   # noqa: E501",
            "'force_seed_mnn': True,  # Whether to consider only MNN for the purpose of selecting seeds. Generally improves accuracy at the cost of runtime.    # noqa: E501",
            "# You can provide a MNN mask in input to skip MNN computation and still get the improvement.",
            "-        'device': get_cuda_device_if_available(),  # Device to be used for running AdaLAM. Use GPU if available.   # noqa: E501",
            "+        'device': torch.device('cpu'),  # Device to be used for running AdaLAM. Use GPU if available.",
            "}",
            "return DEFAULT_CONFIG"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=396353)",
            "Update(target_node=ASTNode(type=identifier, text=get_cuda_device_if_available), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=get_cuda_device_if_available), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=396354)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=396355)",
            "Insert(target_node=ASTNode(type=argument_list), node=('string', \"'cpu'\"), position=1, insert_id=396356)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 2317,
        "neg_line": [
            "-'device': get_cuda_device_if_available(),  # Device to be used for running AdaLAM. Use GPU if available.   # noqa: E501"
        ],
        "pos_line": [
            "+'device': torch.device('cpu'),  # Device to be used for running AdaLAM. Use GPU if available."
        ],
        "core_change": "-'device': get_cuda_device_if_available(),  # Device to be used for running AdaLAM. Use GPU if available.   # noqa: E501 +'device': torch.device('cpu'),  # Device to be used for running AdaLAM. Use GPU if available.",
        "core_API": "device"
    },
    {
        "commit_hash": "a6bcfb80156fac34c40a3b8dcd973c4a990d75ca",
        "index": "5744537cb..7bcb7cafd 100644",
        "commit_message": "fix tests\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "if is_torch_available():",
            "",
            "# TensorFlow",
            "if is_tf_available():",
            "-    logger.info(\"TensorFlow version {} available.\".format(tf.__version__))",
            "-",
            "from .modeling_tf_utils import TFPreTrainedModel, TFSharedEmbeddings, TFSequenceSummary",
            "from .modeling_tf_auto import (TFAutoModel, TFAutoModelForSequenceClassification, TFAutoModelForQuestionAnswering,",
            "TFAutoModelWithLMHead)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', ''), position=0, insert_id=1246393)",
            "Delete(target_node=ASTNode(type=identifier, text=logger))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=info))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"TensorFlow version {} available.\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=__version__))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 2319,
        "neg_line": [
            "-logger.info(\"TensorFlow version {} available.\".format(tf.__version__))",
            "-"
        ],
        "pos_line": [],
        "core_change": "-logger.info(\"TensorFlow version {} available.\".format(tf.__version__)) -",
        "core_API": "info"
    },
    {
        "commit_hash": "046db20e25636d6334ca4d2c8180a838afb11fca",
        "index": "31a45d1f..7bb7160c 100644",
        "commit_message": "fix bug of gaussian_target, update unittest of heatmap (#3543)\n\n* fix bug of gaussian_target, update unittest\n\n* fix AELoss's weight; fix Conv's init in CornerHead; now the loss and mAP from 1 to 40 epochs are correct, still training\n\n* add some comments\n\n* fix cases\n\n* update comments\n\n* fix yapf\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def gen_gaussian_target(heatmap, center, radius, k=1):",
            "masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]",
            "masked_gaussian = gaussian_kernel[radius - top:radius + bottom,",
            "radius - left:radius + right]",
            "-    out_heatmap = torch.zeros_like(heatmap)",
            "+    out_heatmap = heatmap",
            "torch.max(",
            "masked_heatmap,",
            "masked_gaussian * k,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=heatmap), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=zeros_like))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 2321,
        "neg_line": [
            "-out_heatmap = torch.zeros_like(heatmap)"
        ],
        "pos_line": [
            "+out_heatmap = heatmap"
        ],
        "core_change": "-out_heatmap = torch.zeros_like(heatmap) +out_heatmap = heatmap",
        "core_API": "zeros_like"
    },
    {
        "commit_hash": "f2399a39aa272e0c380165d7c1133ba0583f4c30",
        "index": "969378cf..dcba06e8 100644",
        "commit_message": "samplified-example-code (#385)\n\n* Update tutorial_mlp_dropout2.py\n\n* Update tutorial_mlp_dropout1.py\n\n* Update tutorial_mnist_simple.py\n\n* Update tutorial_mnist_float16.py\n\n* fix format\n\n* fix format\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "X_train, y_train, X_val, y_val, X_test, y_test = \\",
            "tl.files.load_mnist_dataset(shape=(-1,784))",
            "# define placeholder",
            "x = tf.placeholder(tf.float32, shape=[None, 784], name='x')",
            "-y_ = tf.placeholder(",
            "-    tf.int64, shape=[",
            "-        None,",
            "-    ], name='y_')",
            "+y_ = tf.placeholder(tf.int64, shape=[None], name='y_')",
            "",
            "# define the network",
            "network = tl.layers.InputLayer(x, name='input')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 0,
        "minus_line": 3,
        "AST_diff_line": 1,
        "number": 2327,
        "neg_line": [
            "-y_ = tf.placeholder(",
            "-tf.int64, shape=[",
            "-None,",
            "-], name='y_')"
        ],
        "pos_line": [
            "+y_ = tf.placeholder(tf.int64, shape=[None], name='y_')"
        ],
        "core_change": "-y_ = tf.placeholder( -tf.int64, shape=[ -None, -], name='y_') +y_ = tf.placeholder(tf.int64, shape=[None], name='y_')",
        "core_API": "load_mnist_dataset"
    },
    {
        "commit_hash": "797b290ed09a84091a4c23884b7c104f8e94b128",
        "index": "dc8a9116..d4cb367e 100644",
        "commit_message": "support bf16 for stable diffusion (#792)\n\n* support bf16 for stable diffusion\n\n* fix typo\n\n* address review comments\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Upsample2D(nn.Module):",
            "else:",
            "hidden_states = F.interpolate(hidden_states, size=output_size, mode=\"nearest\")",
            "",
            "+        # If the input is bfloat16, we cast back to bfloat16",
            "+        if dtype == torch.bfloat16:",
            "+            hidden_states = hidden_states.to(dtype)",
            "+",
            "# TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed",
            "if self.use_conv:",
            "if self.name == \"conv\":"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=2, insert_id=102883)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=102884)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=102885)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=102886)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=102887)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'dtype'), position=0, insert_id=102888)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=102889)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=2, insert_id=102890)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=102891)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=102892)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=102893)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bfloat16'), position=2, insert_id=102894)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=102895)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'hidden_states'), position=0, insert_id=102896)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=102897)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=102898)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=102899)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=102900)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'hidden_states'), position=0, insert_id=102901)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=102902)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=102903)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=102904)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'dtype'), position=1, insert_id=102905)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=102906)"
        ],
        "plus_line": 3,
        "minus_line": 0,
        "AST_diff_line": 24,
        "number": 2328,
        "neg_line": [],
        "pos_line": [
            "+# If the input is bfloat16, we cast back to bfloat16",
            "+if dtype == torch.bfloat16:",
            "+hidden_states = hidden_states.to(dtype)",
            "+"
        ],
        "core_change": "+# If the input is bfloat16, we cast back to bfloat16 +if dtype == torch.bfloat16: +hidden_states = hidden_states.to(dtype) +",
        "core_API": "interpolate"
    },
    {
        "commit_hash": "0113e0b349f12ddd29d4163802eeee01bdd77b42",
        "index": "ba09aa64..cc36193b 100644",
        "commit_message": "CUDA fixes for MCMC (#1954)\n\n* CUDA related fixes for MCMC\n\n* resolve cuda failures by setting mp context\n\n* resolve failure due to pickling issue\n\n* add uniform initialization as in Stan\n\n* fix hmc tests\n\n* fix lint\n\n* fix tests; add docstring\n\n* skip gamma tests on cuda\n\n* reinstate use of mp.event\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "if __name__ == \"__main__\":",
            "",
            "if args.cuda:",
            "torch.set_default_tensor_type(torch.cuda.FloatTensor)",
            "-        torch.multiprocessing.set_start_method(\"spawn\", force=True)",
            "",
            "main(args)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=multiprocessing))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=set_start_method))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"spawn\"))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=force))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 2329,
        "neg_line": [
            "-torch.multiprocessing.set_start_method(\"spawn\", force=True)"
        ],
        "pos_line": [],
        "core_change": "-torch.multiprocessing.set_start_method(\"spawn\", force=True)",
        "core_API": "set_default_tensor_type"
    },
    {
        "commit_hash": "68fef62d2788cea69f75101447af2bcb3405186c",
        "index": "d364f569..0a2b8fee 100644",
        "commit_message": "Fix non-float32 efficientnet calls\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "def EfficientNet(",
            "# normalize the input, we need to divide another sqrt(var) to match the",
            "# original implementation.",
            "# See https://github.com/tensorflow/tensorflow/issues/49930 for more details",
            "-    x = x / tf.math.sqrt(IMAGENET_STDDEV_RGB)",
            "+    x = layers.Rescaling(1. / tf.math.sqrt(IMAGENET_STDDEV_RGB))(x)",
            "",
            "x = layers.ZeroPadding2D(",
            "padding=imagenet_utils.correct_pad(x, 3),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=2582354)",
            "Insert(target_node=IN(type=call), node=('call', None), position=0, insert_id=2582355)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2582356)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2582357)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2582358)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2582359)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'x'), position=1, insert_id=2582360)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2582361)",
            "Update(target_node=ASTNode(type=identifier, text=x), value='layers')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=x), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2582362)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Rescaling'), position=2, insert_id=2582363)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2582364)",
            "Insert(target_node=IN(type=argument_list), node=('binary_operator', None), position=1, insert_id=2582365)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2582366)",
            "Insert(target_node=IN(type=binary_operator), node=('float', '1.'), position=0, insert_id=2582367)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=/, text=/), position=1)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 2330,
        "neg_line": [
            "-x = x / tf.math.sqrt(IMAGENET_STDDEV_RGB)"
        ],
        "pos_line": [
            "+x = layers.Rescaling(1. / tf.math.sqrt(IMAGENET_STDDEV_RGB))(x)"
        ],
        "core_change": "-x = x / tf.math.sqrt(IMAGENET_STDDEV_RGB) +x = layers.Rescaling(1. / tf.math.sqrt(IMAGENET_STDDEV_RGB))(x)",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "94d20f71a3b9be4ce6764240fa34767114ad616b",
        "index": "d920f447..1813cd1b 100644",
        "commit_message": "Remove tl.layers.initialize_global_variables(sess) (#931)\n\n* update sampling layers\n\n* upadte zoom\n\n* fix bug zoom\n\n* typo\n\n* fix bug affine_transform_cv2 x and y\n\n* fix bug crop when crop size equal to image size\n\n* fix file docs typo\n\n* fix bug instance norm\n\n* fix docs\n\n* update examples , init variables\n\n* changelog\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "def main_word2vec_basic():",
            "# transpose_b=True, normalized_embeddings is transposed before multiplication.",
            "",
            "# Step 5: Start training.",
            "-    print()",
            "-",
            "-    tl.layers.initialize_global_variables(sess)",
            "+    sess.run(tf.global_variables_initializer())",
            "if resume:",
            "print(\"Load existing model\" + \"!\" * 10)",
            "# Load from ckpt or npz file"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=2633353)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2633354)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2633355)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2633356)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sess'), position=0, insert_id=2633357)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2633358)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'run'), position=2, insert_id=2633359)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2633360)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2633361)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=print), value='tf')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=print), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=layers), value='global_variables_initializer')",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=identifier, text=tl))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=initialize_global_variables))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=sess))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 25,
        "number": 2336,
        "neg_line": [
            "-print()",
            "-",
            "-tl.layers.initialize_global_variables(sess)"
        ],
        "pos_line": [
            "+sess.run(tf.global_variables_initializer())"
        ],
        "core_change": "-print() - -tl.layers.initialize_global_variables(sess) +sess.run(tf.global_variables_initializer())",
        "core_API": "initialize_global_variables"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "8f2edd98..de207432 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpearmanCorrelationTest(AllenNlpTestCase):",
            "predictions_labels_ = [(predictions1, labels1), (predictions2, labels2)]",
            "",
            "# Random binary mask",
            "-        mask = torch.randint(0, 2, size=(batch_size, num_labels), device=device)",
            "+        mask = torch.randint(0, 2, size=(batch_size, num_labels), device=device).bool()",
            "",
            "for predictions, labels in predictions_labels_:",
            "spearman_correlation.reset()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=20149)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=20150)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=20151)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=20152)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=20153)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=20154)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 2338,
        "neg_line": [
            "-mask = torch.randint(0, 2, size=(batch_size, num_labels), device=device)"
        ],
        "pos_line": [
            "+mask = torch.randint(0, 2, size=(batch_size, num_labels), device=device).bool()"
        ],
        "core_change": "-mask = torch.randint(0, 2, size=(batch_size, num_labels), device=device) +mask = torch.randint(0, 2, size=(batch_size, num_labels), device=device).bool()",
        "core_API": "randint"
    },
    {
        "commit_hash": "c64a03db779d516b3bb80e5eaacab6c7107af791",
        "index": "b1ce33ff..307bb732 100755",
        "commit_message": "Fix docstrings for lr_schedules.py (#1455)\n\n\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "class WarmupLR(object):",
            "last_batch_iteration (int): The index of the last batch. Default: -1.",
            "Example:",
            ">>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)",
            "-            >>> scheduler = torch.optim.WarmupLR(optimizer)",
            "+            >>> scheduler = WarmupLR(optimizer)",
            ">>> data_loader = torch.utils.data.DataLoader(...)",
            ">>> for epoch in range(10):",
            ">>>     for batch in data_loader:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=WarmupLR), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=optim))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 2340,
        "neg_line": [
            "->>> scheduler = torch.optim.WarmupLR(optimizer)"
        ],
        "pos_line": [
            "+>>> scheduler = WarmupLR(optimizer)"
        ],
        "core_change": "->>> scheduler = torch.optim.WarmupLR(optimizer) +>>> scheduler = WarmupLR(optimizer)",
        "core_API": "SGD"
    },
    {
        "commit_hash": "737b07d827bc8fb1ffac0aecd81ee7e9ec98ae71",
        "index": "710501f8c2..01b2d6a7dd 100644",
        "commit_message": "fixed torch subtract\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def subtract(x1: torch.Tensor,",
            "promoted_type = torch.promote_types(x1.dtype, x2.dtype)",
            "x1 = x1.to(promoted_type)",
            "x2 = x2.to(promoted_type)",
            "-    return torch.subtract(x1, x2, out=out)",
            "+        return torch.subtract(x1, x2, out=out)",
            "+    return torch.subtract(x1, x2)",
            "",
            "",
            "def remainder(x1: torch.Tensor,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=359950)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=359951)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('ERROR', None), position=1, insert_id=359952)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=2, insert_id=359953)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'subtract'), position=3, insert_id=359954)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=359955)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'x1'), position=1, insert_id=359956)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=359957)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'x2'), position=3, insert_id=359958)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=359959)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'return'), position=0, insert_id=359960)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'torch'), position=1, insert_id=359961)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 2341,
        "neg_line": [
            "-return torch.subtract(x1, x2, out=out)"
        ],
        "pos_line": [
            "+return torch.subtract(x1, x2, out=out)",
            "+return torch.subtract(x1, x2)"
        ],
        "core_change": "-return torch.subtract(x1, x2, out=out) +return torch.subtract(x1, x2, out=out) +return torch.subtract(x1, x2)",
        "core_API": "promote_types"
    },
    {
        "commit_hash": "6669cc82d1ed4b59df80ff43a5da8c87fe2fcb90",
        "index": "79e93e3..02a823d 100644",
        "commit_message": "fixed error on summaries (#846)\n\n\n",
        "file": "tflearn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "with tf.Graph().as_default():",
            "with tf.name_scope('CustomMonitor'):",
            "test_var = tf.reduce_sum(tf.cast(net, tf.float32), name=\"test_var\")",
            "test_const = tf.constant(32.0, name=\"custom_constant\")",
            "-",
            "-    # Define a train op",
            "+        # Define a train op",
            "trainop = tflearn.TrainOp(loss=loss, optimizer=optimizer,",
            "-                              validation_monitors=[test_var, test_const],",
            "-                              metric=accuracy, batch_size=128)",
            "+                            validation_monitors=[test_var, test_const],",
            "+                            metric=accuracy, batch_size=128)",
            "",
            "# Tensorboard logs stored in /tmp/tflearn_logs/. Using verbose level 2.",
            "trainer = tflearn.Trainer(train_ops=trainop,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 2343,
        "neg_line": [
            "-",
            "-# Define a train op",
            "-validation_monitors=[test_var, test_const],",
            "-metric=accuracy, batch_size=128)"
        ],
        "pos_line": [
            "+# Define a train op",
            "+validation_monitors=[test_var, test_const],",
            "+metric=accuracy, batch_size=128)"
        ],
        "core_change": "- -# Define a train op +# Define a train op -validation_monitors=[test_var, test_const], -metric=accuracy, batch_size=128) +validation_monitors=[test_var, test_const], +metric=accuracy, batch_size=128)",
        "core_API": "Graph"
    },
    {
        "commit_hash": "d0fa2d7086f7fdf506987c4f28b6139ffb0cd802",
        "index": "f29167c2d..22f848027 100644",
        "commit_message": "fix: remove cast\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFModel(Trainable, Inferable, metaclass=TfModelMeta):",
            "print('model saved')",
            "",
            "def get_checkpoint_state(self):",
            "-        return tf.train.get_checkpoint_state(Path(self.model_path).parent)",
            "+        return tf.train.get_checkpoint_state(self.model_path.parent)",
            "",
            "@check_path_exists('dir')",
            "@overrides"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=attribute), position=0)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2344,
        "neg_line": [
            "-return tf.train.get_checkpoint_state(Path(self.model_path).parent)"
        ],
        "pos_line": [
            "+return tf.train.get_checkpoint_state(self.model_path.parent)"
        ],
        "core_change": "-return tf.train.get_checkpoint_state(Path(self.model_path).parent) +return tf.train.get_checkpoint_state(self.model_path.parent)",
        "core_API": "get_checkpoint_state"
    },
    {
        "commit_hash": "0fe17f375a4f0fdd9aea260d0645ccfd4896e958",
        "index": "7176cfa79..c516ca57a 100755",
        "commit_message": "FX tracing improvement (#14321)\n\n* Change the way tracing happens, enabling dynamic axes out of the box\n\n* Update the tests and modeling xlnet\n\n* Add the non recoding of leaf modules to avoid recording more values for the methods to record than what will be seen at tracing time (which would otherwise desynchronize the recorded values and the values that need to be given to the proxies during tracing, causing errors).\n\n* Comments and making tracing work for gpt-j and xlnet\n\n* Refactore things related to num_choices (and batch_size, sequence_length)\n\n* Update fx to work on PyTorch 1.10\n\n* Postpone autowrap_function feature usage for later\n\n* Add copyrights\n\n* Remove unnecessary file\n\n* Fix issue with add_new_model_like\n\n* Apply suggestions\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GPTNeoForSequenceClassification(GPTNeoPreTrainedModel):",
            "f\"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"",
            ")",
            "",
            "-        pooled_logits = logits[torch.arange(batch_size), sequence_lengths]",
            "+        pooled_logits = logits[torch.arange(batch_size, device=self.device), sequence_lengths]",
            "",
            "loss = None",
            "if labels is not None:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1207162)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1207163)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1207164)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1207165)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1207166)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1207167)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1207168)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1207169)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2345,
        "neg_line": [
            "-pooled_logits = logits[torch.arange(batch_size), sequence_lengths]"
        ],
        "pos_line": [
            "+pooled_logits = logits[torch.arange(batch_size, device=self.device), sequence_lengths]"
        ],
        "core_change": "-pooled_logits = logits[torch.arange(batch_size), sequence_lengths] +pooled_logits = logits[torch.arange(batch_size, device=self.device), sequence_lengths]",
        "core_API": "arange"
    },
    {
        "commit_hash": "da3aceb876e0f92b65405a20198836c08ee80d10",
        "index": "9e9bb8b3..fce7f201 100644",
        "commit_message": "Add fp16 support (#963)\n\n* Update imgwarp.py\n\n* update fix\n\n* move _torch_inverse_cast under utils\n\n* fix some tests in pyramid\n\n* fix issues\n\n* implement other autocasting functions\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestHomographyWarper:",
            "",
            "# check functional api",
            "patch_dst_to_src_functional = kornia.homography_warp(",
            "-                patch_dst, torch.inverse(dst_homo_src_i), (height, width), align_corners=True)",
            "+                patch_dst, _torch_inverse_cast(dst_homo_src_i), (height, width), align_corners=True)",
            "",
            "assert_allclose(",
            "patch_dst_to_src, patch_dst_to_src_functional, atol=1e-4, rtol=1e-4)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='_torch_inverse_cast')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=inverse))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2348,
        "neg_line": [
            "-patch_dst, torch.inverse(dst_homo_src_i), (height, width), align_corners=True)"
        ],
        "pos_line": [
            "+patch_dst, _torch_inverse_cast(dst_homo_src_i), (height, width), align_corners=True)"
        ],
        "core_change": "-patch_dst, torch.inverse(dst_homo_src_i), (height, width), align_corners=True) +patch_dst, _torch_inverse_cast(dst_homo_src_i), (height, width), align_corners=True)",
        "core_API": "homography_warp"
    },
    {
        "commit_hash": "e669aaf55b166a5adc3e33aa92c6d81662fdf908",
        "index": "ac35a9fa..03e1c413 100644",
        "commit_message": "Trajepl/nebula ckpt engine (#2085)\n\n* enable checkpoint engine\n\n* seprated nebula config\n\n* add __init__.py for nebula importing\n\n* linter fix\n\n* fix: ds_config is None\n\n* fix: ds config\n\n* fix: get sd loader fix\n\n* align the API with torch raw code\n\n* linter fix\n\n* remove duplicate tag params\n\n* make checkpoint_engine as required args\n\n* fix args\n\n* extract parameters out to config\n\n* fix: load state dict\n\n* separate load engine\n\n* linter fix\n\n* extract checkpoint engine to abstract calss\n\n* linter fix\n\n* construct function args fix\n\n* add docs for dev/customers\n\n* linter fix\n\n* remove load engine\n\n* print->log_dist\n\n* linter fix\n\n* add tag flag to distinguish the loading order\n\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\nCo-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>\nCo-authored-by: Jeff Rasley <jerasley@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PipelineModule(nn.Module):",
            "mp_rank = self._grid.get_slice_parallel_rank()",
            "mp_world_size = self._grid.get_slice_parallel_world_size()",
            "",
            "-            sd_loader = SDLoaderFactory.get_sd_loader(model_ckpt_list, version=2.0)",
            "+            sd_loader = SDLoaderFactory.get_sd_loader(",
            "+                model_ckpt_list,",
            "+                version=2.0,",
            "+                checkpoint_engine=checkpoint_engine)",
            "load_path, checkpoint, _ = sd_loader.load(mp_world_size, mp_rank, module_key=None, is_pipe_parallel=True)",
            "",
            "layer.load_state_dict(checkpoint)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=77408)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=77409)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'checkpoint_engine'), position=0, insert_id=77410)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=77411)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'checkpoint_engine'), position=2, insert_id=77412)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2350,
        "neg_line": [
            "-sd_loader = SDLoaderFactory.get_sd_loader(model_ckpt_list, version=2.0)"
        ],
        "pos_line": [
            "+sd_loader = SDLoaderFactory.get_sd_loader(",
            "+model_ckpt_list,",
            "+version=2.0,",
            "+checkpoint_engine=checkpoint_engine)"
        ],
        "core_change": "-sd_loader = SDLoaderFactory.get_sd_loader(model_ckpt_list, version=2.0) +sd_loader = SDLoaderFactory.get_sd_loader( +model_ckpt_list, +version=2.0, +checkpoint_engine=checkpoint_engine)",
        "core_API": "get_slice_parallel_rank"
    },
    {
        "commit_hash": "de8c2ee5eb06e64efcde83e92b74eb1b92d64563",
        "index": "4a205b6f..17dc5230 100644",
        "commit_message": "Support sample_shape in Rejector (#738)\n\n* Support sample_shape in Rejector\n\n* Fix typo in docs\n\n* Fix typo\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ShapeAugmentedGamma(Gamma):",
            "self._unboost_x_cache = None, None",
            "",
            "def sample(self, sample_shape=torch.Size()):",
            "-        if sample_shape:",
            "-            raise ValueError(\"Arbitrary `sample_shape` not supported by ShapeAugmentedGamma class.\")",
            "-        x = self._rejection_gamma.sample()",
            "+        x = self._rejection_gamma.sample(sample_shape)",
            "boosted_x = x.clone()",
            "for i in range(self._boost):",
            "boosted_x *= (1 - x.new(x.shape).uniform_()) ** (1 / (i + self.alpha))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'sample_shape'), position=1, insert_id=756057)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=sample_shape))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=raise, text=raise))",
            "Delete(target_node=ASTNode(type=identifier, text=ValueError))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"Arbitrary `sample_shape` not supported by ShapeAugmentedGamma class.\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=raise_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 14,
        "number": 2354,
        "neg_line": [
            "-if sample_shape:",
            "-raise ValueError(\"Arbitrary `sample_shape` not supported by ShapeAugmentedGamma class.\")",
            "-x = self._rejection_gamma.sample()"
        ],
        "pos_line": [
            "+x = self._rejection_gamma.sample(sample_shape)"
        ],
        "core_change": "-if sample_shape: -raise ValueError(\"Arbitrary `sample_shape` not supported by ShapeAugmentedGamma class.\") -x = self._rejection_gamma.sample() +x = self._rejection_gamma.sample(sample_shape)",
        "core_API": "Size"
    },
    {
        "commit_hash": "0a3a987eaeefbbe1421574f82ebe24e71e9b7883",
        "index": "7ea39ecb1..98b4aaaef 100755",
        "commit_message": "Really fix import\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "np.random.seed(args.seed)",
            "",
            "if args.backend == \"pytorch\":",
            "-        fromespnet.lmpytorch.tts_pytorch import train",
            "+        from espnet.lmpytorch.tts_pytorch import train",
            "train(args)",
            "else:",
            "raise NotImplementedError(\"Only pytorch is supported.\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('block', None), position=3, insert_id=178866)",
            "Insert(target_node=IN(type=block), node=('import_from_statement', None), position=0, insert_id=178867)",
            "Insert(target_node=IN(type=import_from_statement), node=('from', 'from'), position=0, insert_id=178868)",
            "Insert(target_node=IN(type=import_from_statement), node=('dotted_name', None), position=1, insert_id=178869)",
            "Move(target_node=IN(type=import_from_statement), node=ASTNode(type=import, text=import), position=2)",
            "Move(target_node=IN(type=import_from_statement), node=ASTNode(type=dotted_name), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=fromespnet), value='espnet')",
            "Move(target_node=IN(type=dotted_name), node=ASTNode(type=identifier, text=fromespnet), position=0)",
            "Move(target_node=IN(type=dotted_name), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=dotted_name), node=ASTNode(type=identifier, text=lmpytorch), position=2)",
            "Move(target_node=IN(type=dotted_name), node=ASTNode(type=., text=.), position=3)",
            "Move(target_node=IN(type=dotted_name), node=ASTNode(type=identifier, text=tts_pytorch), position=4)",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=import_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2358,
        "neg_line": [
            "-fromespnet.lmpytorch.tts_pytorch import train"
        ],
        "pos_line": [
            "+from espnet.lmpytorch.tts_pytorch import train"
        ],
        "core_change": "-fromespnet.lmpytorch.tts_pytorch import train +from espnet.lmpytorch.tts_pytorch import train",
        "core_API": "seed"
    },
    {
        "commit_hash": "8a96eedc7f78aa3e70dc2ebd0d1eba301bf2663a",
        "index": "2be5283b..401ae9a8 100644",
        "commit_message": "Fix imports & docstrings\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "def serialize_model_as_bytecode(model):",
            "archive.addfile(tarinfo=info, fileobj=f)",
            "tf.io.gfile.rmtree(temp_dir)",
            "b.seek(0)",
            "-  return (asarray(memoryview(b.read())), )",
            "+  return (numpy.asarray(memoryview(b.read())), )"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2521430)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'numpy'), position=0, insert_id=2521431)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2521432)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=asarray), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 2359,
        "neg_line": [
            "-return (asarray(memoryview(b.read())), )"
        ],
        "pos_line": [
            "+return (numpy.asarray(memoryview(b.read())), )"
        ],
        "core_change": "-return (asarray(memoryview(b.read())), ) +return (numpy.asarray(memoryview(b.read())), )",
        "core_API": "addfile"
    },
    {
        "commit_hash": "cec92098641d3f4c395cd51d84ba93b691d1cdf3",
        "index": "fe326e47..997d72db 100644",
        "commit_message": "Several micro optimizations (#4833)\n\n* benchmark transfers\n\n* create tensors directl on device when possible\n\n* fix\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Entropy(Metric):",
            "_count = 1",
            "",
            "if is_distributed():",
            "-            count = torch.tensor(_count).to(device)",
            "+            count = torch.tensor(_count, device=device)",
            "dist.all_reduce(_entropy, op=dist.ReduceOp.SUM)",
            "dist.all_reduce(count, op=dist.ReduceOp.SUM)",
            "_count = count.item()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=5403)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=_count), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=5404)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=5405)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=4)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=5406)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=5407)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=device), position=2)",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 2360,
        "neg_line": [
            "-count = torch.tensor(_count).to(device)"
        ],
        "pos_line": [
            "+count = torch.tensor(_count, device=device)"
        ],
        "core_change": "-count = torch.tensor(_count).to(device) +count = torch.tensor(_count, device=device)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "be1a382d04c26a735026d10c04a395c771320d7e",
        "index": "dd39f2ea..325f98a1 100644",
        "commit_message": "pyrdown fix\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PyrDown(nn.Module):",
            "input, self.kernel, self.border_type)",
            "",
            "# reject even rows and columns.",
            "-        out: torch.Tensor = x_blur[..., ::2, ::2]",
            "+        out: torch.Tensor = F.avg_pool2d(x_blur, 2,2)",
            "return out"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=4, insert_id=464900)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=464901)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=464902)",
            "Update(target_node=ASTNode(type=identifier, text=x_blur), value='F')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=x_blur), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=464903)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'avg_pool2d'), position=2, insert_id=464904)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=464905)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'x_blur'), position=1, insert_id=464906)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=464907)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=integer, text=2), position=3)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=,, text=,), position=4)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=integer, text=2), position=5)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=6, insert_id=464908)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=ellipsis, text=...))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=slice))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=slice))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 2361,
        "neg_line": [
            "-out: torch.Tensor = x_blur[..., ::2, ::2]"
        ],
        "pos_line": [
            "+out: torch.Tensor = F.avg_pool2d(x_blur, 2,2)"
        ],
        "core_change": "-out: torch.Tensor = x_blur[..., ::2, ::2] +out: torch.Tensor = F.avg_pool2d(x_blur, 2,2)",
        "core_API": "avg_pool2d"
    },
    {
        "commit_hash": "668ea0bc261d85d4ac2d6474974b02e3ff136bc0",
        "index": "a42707e4d..22077202e 100644",
        "commit_message": "Fix typo RMSProp -> RMSprop (#13063)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "def choose_optimizer(policy, config):",
            "return torch.optim.Adam(",
            "params=policy.model.parameters(), lr=policy.cur_lr)",
            "else:",
            "-        return torch.optim.RMSProp(",
            "+        return torch.optim.RMSprop(",
            "params=policy.model.parameters(),",
            "lr=policy.cur_lr,",
            "weight_decay=config[\"decay\"],"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=RMSProp), value='RMSprop')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2363,
        "neg_line": [
            "-return torch.optim.RMSProp("
        ],
        "pos_line": [
            "+return torch.optim.RMSprop("
        ],
        "core_change": "-return torch.optim.RMSProp( +return torch.optim.RMSprop(",
        "core_API": "Adam"
    },
    {
        "commit_hash": "415970ecbbd95fd1514f04f99c661dd2b4c1e40b",
        "index": "8a62262b..69f53655 100644",
        "commit_message": "more consistent assigner (#2536)\n\n* added background label to assign function\n\n* fixed the background label as -1 in all assigners\n\nCo-authored-by: wangxinjiang <wangxinjiang@sensetime.com>\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ATSSAssigner(BaseAssigner):",
            "max_overlaps != -INF] = argmax_overlaps[max_overlaps != -INF] + 1",
            "",
            "if gt_labels is not None:",
            "-            assigned_labels = assigned_gt_inds.new_zeros((num_bboxes, ))",
            "+            assigned_labels = assigned_gt_inds.new_full((num_bboxes, ), -1)",
            "pos_inds = torch.nonzero(assigned_gt_inds > 0).squeeze()",
            "if pos_inds.numel() > 0:",
            "assigned_labels[pos_inds] = gt_labels["
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=new_zeros), value='new_full')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=639076)",
            "Insert(target_node=ASTNode(type=argument_list), node=('unary_operator', '-1'), position=3, insert_id=639077)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 2368,
        "neg_line": [
            "-assigned_labels = assigned_gt_inds.new_zeros((num_bboxes, ))"
        ],
        "pos_line": [
            "+assigned_labels = assigned_gt_inds.new_full((num_bboxes, ), -1)"
        ],
        "core_change": "-assigned_labels = assigned_gt_inds.new_zeros((num_bboxes, )) +assigned_labels = assigned_gt_inds.new_full((num_bboxes, ), -1)",
        "core_API": "new_zeros"
    },
    {
        "commit_hash": "6776e5672c7635723fed05130b09272e9ee1cf83",
        "index": "fe652f0e..256b5213 100644",
        "commit_message": "FIX yapf\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class Seq2seq(Model):",
            "for i in range(n_layer):",
            "if (i == 0):",
            "self.dec_layers.append(",
            "-                    tl.layers.",
            "-                    RNN(cell=cell_dec(units=n_units), in_channels=self.embedding_size, return_last_state=True)",
            "+                    tl.layers.RNN(",
            "+                        cell=cell_dec(units=n_units), in_channels=self.embedding_size, return_last_state=True",
            "+                    )",
            ")",
            "else:",
            "self.dec_layers.append("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 2372,
        "neg_line": [
            "-tl.layers.",
            "-RNN(cell=cell_dec(units=n_units), in_channels=self.embedding_size, return_last_state=True)"
        ],
        "pos_line": [
            "+tl.layers.RNN(",
            "+cell=cell_dec(units=n_units), in_channels=self.embedding_size, return_last_state=True",
            "+)"
        ],
        "core_change": "-tl.layers. -RNN(cell=cell_dec(units=n_units), in_channels=self.embedding_size, return_last_state=True) +tl.layers.RNN( +cell=cell_dec(units=n_units), in_channels=self.embedding_size, return_last_state=True +)",
        "core_API": "append"
    },
    {
        "commit_hash": "a0d99fdb2cfcc418809dde975f51097c3d6010ca",
        "index": "0811af0..6b9714b 100644",
        "commit_message": "Fixed weight init for fused weight matrices in fused MHA by adding correct gain factor.\n\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class SelfMultiheadAttn(nn.Module):",
            "nn.init.xavier_uniform_(self.k_weight)",
            "nn.init.xavier_uniform_(self.v_weight)",
            "else:",
            "-            nn.init.xavier_uniform_(self.in_proj_weight)",
            "+            # in_proj_weight has shape [3 * hidden, hidden] but it should be",
            "+            # initialized like a [hidden, hidden] matrix.",
            "+            # sqrt(6 / (hidden + hidden)) / sqrt(6 / (3 * hidden + hidden)) = sqrt(2)",
            "+            # therefore xavier_uniform gain should be set to sqrt(2).",
            "+            nn.init.xavier_uniform_(self.in_proj_weight, gain=math.sqrt(2))",
            "nn.init.xavier_uniform_(self.out_proj_weight)",
            "if self.bias:",
            "if self.separate_qkv_params:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1314517)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1314518)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=1314519)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'gain'), position=0, insert_id=1314520)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1314521)",
            "Insert(target_node=IN(type=keyword_argument), node=('call', None), position=2, insert_id=1314522)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1314523)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1314524)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=0, insert_id=1314525)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1314526)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sqrt'), position=2, insert_id=1314527)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1314528)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '2'), position=1, insert_id=1314529)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 2373,
        "neg_line": [
            "-nn.init.xavier_uniform_(self.in_proj_weight)"
        ],
        "pos_line": [
            "+# in_proj_weight has shape [3 * hidden, hidden] but it should be",
            "+# initialized like a [hidden, hidden] matrix.",
            "+# sqrt(6 / (hidden + hidden)) / sqrt(6 / (3 * hidden + hidden)) = sqrt(2)",
            "+# therefore xavier_uniform gain should be set to sqrt(2).",
            "+nn.init.xavier_uniform_(self.in_proj_weight, gain=math.sqrt(2))"
        ],
        "core_change": "-nn.init.xavier_uniform_(self.in_proj_weight) +# in_proj_weight has shape [3 * hidden, hidden] but it should be +# initialized like a [hidden, hidden] matrix. +# sqrt(6 / (hidden + hidden)) / sqrt(6 / (3 * hidden + hidden)) = sqrt(2) +# therefore xavier_uniform gain should be set to sqrt(2). +nn.init.xavier_uniform_(self.in_proj_weight, gain=math.sqrt(2))",
        "core_API": "xavier_uniform_"
    },
    {
        "commit_hash": "04dbea31a9b2e1e0bbb44ea6c2e0074d90cf0ba9",
        "index": "99606fd90..a652d0727 100755",
        "commit_message": "[Examples] Added context manager to datasets map (#12367)\n\n* added cotext manager to datasets map\n\n* fixed style and spaces\n\n* fixed warning of deprecation\n\n* changed desc\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def main():",
            "",
            "for predict_dataset, task in zip(predict_datasets, tasks):",
            "# Removing the `label` columns because it contains -1 and Trainer won't like that.",
            "-            predict_dataset.remove_columns_(\"label\")",
            "+            predict_dataset = predict_dataset.remove_columns(\"label\")",
            "predictions = trainer.predict(predict_dataset, metric_key_prefix=\"predict\").predictions",
            "predictions = np.squeeze(predictions) if is_regression else np.argmax(predictions, axis=1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=1805512)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'predict_dataset'), position=0, insert_id=1805513)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1805514)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=remove_columns_), value='remove_columns')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2374,
        "neg_line": [
            "-predict_dataset.remove_columns_(\"label\")"
        ],
        "pos_line": [
            "+predict_dataset = predict_dataset.remove_columns(\"label\")"
        ],
        "core_change": "-predict_dataset.remove_columns_(\"label\") +predict_dataset = predict_dataset.remove_columns(\"label\")",
        "core_API": "remove_columns_"
    },
    {
        "commit_hash": "64326dccfba46890edc93b5bf7be6974b71984d5",
        "index": "245aee0ff..39e9c5019 100644",
        "commit_message": "Fix it to run properly even if without `--do_train` param.\n\nIt was modified similar to `run_classifier.py`, and Fixed to run properly even if without `--do_train` param.\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "# Save a trained model",
            "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self",
            "output_model_file = os.path.join(args.output_dir, \"pytorch_model.bin\")",
            "-    torch.save(model_to_save.state_dict(), output_model_file)",
            "+    if args.do_train:",
            "+        torch.save(model_to_save.state_dict(), output_model_file)",
            "",
            "# Load a trained model that you have fine-tuned",
            "model_state_dict = torch.load(output_model_file)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=1250455)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1250456)",
            "Insert(target_node=IN(type=if_statement), node=('attribute', None), position=1, insert_id=1250457)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1250458)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1250459)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'args'), position=0, insert_id=1250460)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1250461)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'do_train'), position=2, insert_id=1250462)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 2375,
        "neg_line": [
            "-torch.save(model_to_save.state_dict(), output_model_file)"
        ],
        "pos_line": [
            "+if args.do_train:",
            "+torch.save(model_to_save.state_dict(), output_model_file)"
        ],
        "core_change": "-torch.save(model_to_save.state_dict(), output_model_file) +if args.do_train: +torch.save(model_to_save.state_dict(), output_model_file)",
        "core_API": "join"
    },
    {
        "commit_hash": "ba6f3f974bfc4a2968964dbe5eedea73c9f5efcb",
        "index": "afa7879..d316b18 100644",
        "commit_message": "Enable direct `--weights URL` definition (#3373)\n\n* Enable direct `--weights URL` definition\n\n@KalenMike this PR will enable direct --weights URL definition. Example use case:\n```\npython train.py --weights https://storage.googleapis.com/bucket/dir/model.pt\n```\n\n* cleanup\n\n* bug fixes\n\n* weights = attempt_download(weights)\n\n* Update experimental.py\n\n* Update hubconf.py\n\n* return bug fix\n\n* comment mirror\n\n* min_bytes\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def attempt_load(weights, map_location=None, inplace=True):",
            "# Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a",
            "model = Ensemble()",
            "for w in weights if isinstance(weights, list) else [weights]:",
            "-        attempt_download(w)",
            "-        ckpt = torch.load(w, map_location=map_location)  # load",
            "+        ckpt = torch.load(attempt_download(w), map_location=map_location)  # load",
            "model.append(ckpt['ema' if ckpt.get('ema') else 'model'].float().fuse().eval())  # FP32 model",
            "",
            "# Compatibility updates"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=call), position=1)",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=identifier, text=w))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 2376,
        "neg_line": [
            "-attempt_download(w)",
            "-ckpt = torch.load(w, map_location=map_location)  # load"
        ],
        "pos_line": [
            "+ckpt = torch.load(attempt_download(w), map_location=map_location)  # load"
        ],
        "core_change": "-attempt_download(w) -ckpt = torch.load(w, map_location=map_location)  # load +ckpt = torch.load(attempt_download(w), map_location=map_location)  # load",
        "core_API": "load"
    },
    {
        "commit_hash": "233bad9152aa6c08efd87c4ef03efc0dc46a3597",
        "index": "24bdb3b5..3b3ed44d 100644",
        "commit_message": "Fix unused variable found 2/n (#1252)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Fix unused variable found\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def decompose_essential_matrix(E_mat: torch.Tensor) -> Tuple[torch.Tensor, torch",
            "raise AssertionError(E_mat.shape)",
            "",
            "# decompose matrix by its singular values",
            "-    U, S, V = torch.svd(E_mat)",
            "+    U, _, V = torch.svd(E_mat)",
            "Vt = V.transpose(-2, -1)",
            "",
            "mask = torch.ones_like(E_mat)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=S), value='_')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2377,
        "neg_line": [
            "-U, S, V = torch.svd(E_mat)"
        ],
        "pos_line": [
            "+U, _, V = torch.svd(E_mat)"
        ],
        "core_change": "-U, S, V = torch.svd(E_mat) +U, _, V = torch.svd(E_mat)",
        "core_API": "svd"
    },
    {
        "commit_hash": "b991a9caf28a23457a5ba74fa14ab0e23d5a1ebf",
        "index": "f3b67e975..cdf5cd630 100644",
        "commit_message": "Remove fix for torch 1.0.1 as requirement is now >=1.1\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchHook:",
            "# 5. Put instead the hooked one",
            "setattr(torch_module, func, new_func)",
            "",
            "-        # Hard fix for PyTorch versions < 1.0.2",
            "-        syft.torch.apply_fix16922(self.torch)",
            "-",
            "torch_modules = syft.torch.torch_modules",
            "",
            "for module_name, torch_module in torch_modules.items():"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=syft))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=apply_fix16922))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 2379,
        "neg_line": [
            "-# Hard fix for PyTorch versions < 1.0.2",
            "-syft.torch.apply_fix16922(self.torch)",
            "-"
        ],
        "pos_line": [],
        "core_change": "-# Hard fix for PyTorch versions < 1.0.2 -syft.torch.apply_fix16922(self.torch) -",
        "core_API": "apply_fix16922"
    },
    {
        "commit_hash": "a41f9e83173c72317aa8909e8e7634cc2f33773f",
        "index": "2844c760..21d5a3a4 100755",
        "commit_message": "update and fix grad bug\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "l = tf.nn.dropout(l, keep_prob)",
            "l = FullyConnected('fc7', l, 4096)",
            "l = tf.nn.dropout(l, keep_prob)",
            "-        logits = FullyConnected('fc8', l, out_dim=1000, summary_activation=False, nl=tf.identity)",
            "+        logits = FullyConnected('fc8', l, out_dim=1000, nl=tf.identity)",
            "prob = tf.nn.softmax(logits, name='output')",
            "",
            "y = one_hot(label, 1000)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=summary_activation))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2380,
        "neg_line": [
            "-logits = FullyConnected('fc8', l, out_dim=1000, summary_activation=False, nl=tf.identity)"
        ],
        "pos_line": [
            "+logits = FullyConnected('fc8', l, out_dim=1000, nl=tf.identity)"
        ],
        "core_change": "-logits = FullyConnected('fc8', l, out_dim=1000, summary_activation=False, nl=tf.identity) +logits = FullyConnected('fc8', l, out_dim=1000, nl=tf.identity)",
        "core_API": "dropout"
    },
    {
        "commit_hash": "4fd8977eafd658056c92ffa6294027e77cdc46c0",
        "index": "61816ff397..a8baf9bbff 100644",
        "commit_message": "[RLlib] Minor cleanup in preparation to tf2.x support. (#9130)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* Fixes.\n\n* Fixes and LINT.\n\n* WIP.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def stats(policy, train_batch):",
            "",
            "def grad_stats(policy, train_batch, grads):",
            "return {",
            "-        \"grad_gnorm\": tf.global_norm(grads),",
            "+        \"grad_gnorm\": tf.linalg.global_norm(grads),",
            "}"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2145856)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2145857)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=2145858)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2381,
        "neg_line": [
            "-\"grad_gnorm\": tf.global_norm(grads),"
        ],
        "pos_line": [
            "+\"grad_gnorm\": tf.linalg.global_norm(grads),"
        ],
        "core_change": "-\"grad_gnorm\": tf.global_norm(grads), +\"grad_gnorm\": tf.linalg.global_norm(grads),",
        "core_API": "global_norm"
    },
    {
        "commit_hash": "093e901c979c0f8b0af92f79010b53db1ddcc972",
        "index": "1057d46..fd28be0 100644",
        "commit_message": "fix deprecation warnings; remove pydoop dependency; update README\n\n",
        "file": "TensorFlowOnSpark.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def preprocess_for_eval(image, output_height, output_width):",
            "resized_image = tf.image.resize_image_with_crop_or_pad(image,",
            "output_width,",
            "output_height)",
            "-  tf.image_summary('resized_image', tf.expand_dims(resized_image, 0))",
            "+  tf.summary.image('resized_image', tf.expand_dims(resized_image, 0))",
            "",
            "# Subtract off the mean and divide by the variance of the pixels.",
            "return tf.image.per_image_whitening(resized_image)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2213724)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2213725)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'image'), position=2, insert_id=2213726)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=image_summary), value='summary')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=image_summary), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 2384,
        "neg_line": [
            "-tf.image_summary('resized_image', tf.expand_dims(resized_image, 0))"
        ],
        "pos_line": [
            "+tf.summary.image('resized_image', tf.expand_dims(resized_image, 0))"
        ],
        "core_change": "-tf.image_summary('resized_image', tf.expand_dims(resized_image, 0)) +tf.summary.image('resized_image', tf.expand_dims(resized_image, 0))",
        "core_API": "resize_image_with_crop_or_pad"
    },
    {
        "commit_hash": "241c51a23dfa643f7c45aee223ab57d420044ef4",
        "index": "635c334eda..b3f7950e6b 100644",
        "commit_message": "reduced redundant arguments to reptile method, and other small fixes.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "autograd",
        "change": [
            "def execute_with_gradients(func, xs, retain_grads=False):",
            "else:",
            "y = func_ret",
            "rest = tuple()",
            "-    x_grads_flat = _mx.autograd.grad(y, retain_graph=retain_grads, variables=[v for k, v in xs.to_iterator()])",
            "+    x_grads_flat = _mx.autograd.grad(y, [v for k, v in xs.to_iterator()], retain_graph=retain_grads,",
            "+                                     create_graph=retain_grads)",
            "return (y, xs.from_flat_list(x_grads_flat), *rest)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=list_comprehension), position=3)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1577083)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=1577084)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=1577085)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'create_graph'), position=0, insert_id=1577086)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1577087)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'retain_grads'), position=2, insert_id=1577088)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=variables))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 2385,
        "neg_line": [
            "-x_grads_flat = _mx.autograd.grad(y, retain_graph=retain_grads, variables=[v for k, v in xs.to_iterator()])"
        ],
        "pos_line": [
            "+x_grads_flat = _mx.autograd.grad(y, [v for k, v in xs.to_iterator()], retain_graph=retain_grads,",
            "+create_graph=retain_grads)"
        ],
        "core_change": "-x_grads_flat = _mx.autograd.grad(y, retain_graph=retain_grads, variables=[v for k, v in xs.to_iterator()]) +x_grads_flat = _mx.autograd.grad(y, [v for k, v in xs.to_iterator()], retain_graph=retain_grads, +create_graph=retain_grads)",
        "core_API": "grad"
    },
    {
        "commit_hash": "f42ca2b73fae96d0f6598d7cfecf5c4729c52c1b",
        "index": "18562d10..da491771 100644",
        "commit_message": "Update wavegrad.py\n\nThis should fix the issue https://github.com/mozilla/TTS/issues/581\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Wavegrad(nn.Module):",
            "self.noise_level = self.noise_level.to(y_0)",
            "if len(y_0.shape) == 3:",
            "y_0 = y_0.squeeze(1)",
            "-        s = torch.randint(1, self.num_steps + 1, [y_0.shape[0]])",
            "-        l_a, l_b = self.noise_level[s-1], self.noise_level[s]",
            "+        s = torch.randint(0, self.num_steps - 1, [y_0.shape[0]])",
            "+        l_a, l_b = self.noise_level[s], self.noise_level[s+1]",
            "noise_scale = l_a + torch.rand(y_0.shape[0]).to(y_0) * (l_b - l_a)",
            "noise_scale = noise_scale.unsqueeze(1)",
            "noise = torch.randn_like(y_0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=subscript), node=ASTNode(type=expression_list), position=0)",
            "Insert(target_node=ASTNode(type=expression_list), node=('subscript', None), position=3, insert_id=1266310)",
            "Update(target_node=ASTNode(type=integer, text=1), value='0')",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1266311)",
            "Insert(target_node=IN(type=subscript), node=('binary_operator', None), position=2, insert_id=1266312)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1266313)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('-', '-'), position=1, insert_id=1266314)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 's'), position=0, insert_id=1266315)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=1266316)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=2, insert_id=1266317)",
            "Delete(target_node=ASTNode(type=+, text=+))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=s))",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 2386,
        "neg_line": [
            "-s = torch.randint(1, self.num_steps + 1, [y_0.shape[0]])",
            "-l_a, l_b = self.noise_level[s-1], self.noise_level[s]"
        ],
        "pos_line": [
            "+s = torch.randint(0, self.num_steps - 1, [y_0.shape[0]])",
            "+l_a, l_b = self.noise_level[s], self.noise_level[s+1]"
        ],
        "core_change": "-s = torch.randint(1, self.num_steps + 1, [y_0.shape[0]]) -l_a, l_b = self.noise_level[s-1], self.noise_level[s] +s = torch.randint(0, self.num_steps - 1, [y_0.shape[0]]) +l_a, l_b = self.noise_level[s], self.noise_level[s+1]",
        "core_API": "to"
    },
    {
        "commit_hash": "41c99fbf385a8c875fb6181ce7301e4bc218535b",
        "index": "c540da1..1fa009b 100644",
        "commit_message": "Use preprocessing layers for categorical encoding (#1090)\n\n* removed sigmoid layer\n\n* added lookup\n\n* bug fix\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class CategoricalToNumerical(block_module.Block):",
            "encoding.append(keras_layers.INT)",
            "else:",
            "encoding.append(keras_layers.NONE)",
            "-        return keras_layers.CategoricalEncoding(encoding)(input_node)",
            "+        return keras_layers.MultiColumnCategoricalEncoding(encoding)(input_node)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=CategoricalEncoding), value='MultiColumnCategoricalEncoding')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2387,
        "neg_line": [
            "-return keras_layers.CategoricalEncoding(encoding)(input_node)"
        ],
        "pos_line": [
            "+return keras_layers.MultiColumnCategoricalEncoding(encoding)(input_node)"
        ],
        "core_change": "-return keras_layers.CategoricalEncoding(encoding)(input_node) +return keras_layers.MultiColumnCategoricalEncoding(encoding)(input_node)",
        "core_API": "append"
    },
    {
        "commit_hash": "dc41769f02a4fd03ae909c53e4b66028fb15be4f",
        "index": "e60a0d95c..ddaca18c2 100644",
        "commit_message": "fixed clip grad warning\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer(TrainerIO):",
            "# clip gradients",
            "if self.gradient_clip > 0:",
            "model = self.__get_model()",
            "-                torch.nn.utils.clip_grad_norm(model.parameters(), self.gradient_clip)",
            "+                torch.nn.utils.clip_grad_norm_(model.parameters(), self.gradient_clip)",
            "",
            "# update gradients across all optimizers",
            "for optimizer in self.optimizers:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=clip_grad_norm), value='clip_grad_norm_')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2388,
        "neg_line": [
            "-torch.nn.utils.clip_grad_norm(model.parameters(), self.gradient_clip)"
        ],
        "pos_line": [
            "+torch.nn.utils.clip_grad_norm_(model.parameters(), self.gradient_clip)"
        ],
        "core_change": "-torch.nn.utils.clip_grad_norm(model.parameters(), self.gradient_clip) +torch.nn.utils.clip_grad_norm_(model.parameters(), self.gradient_clip)",
        "core_API": "__get_model"
    },
    {
        "commit_hash": "bb76d32bfee24225869176df4ff9ceb1352c3e00",
        "index": "0311aba..c7b0f1a 100644",
        "commit_message": "Fixed warning in scope declaration\n",
        "file": "facenet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def conv(inpOp, nIn, nOut, kH, kW, dH, dW, padType, prefix, phase_train=True, us",
            "global parameters",
            "name = prefix + '_' + str(conv_counter)",
            "conv_counter += 1",
            "-  with tf.variable_scope(name) as scope:",
            "+  with tf.variable_scope(name):",
            "l2_regularizer = lambda t: l2_loss(t, weight=4e-5)",
            "kernel = tf.get_variable(\"weights\", [kH, kW, nIn, nOut],",
            "initializer=tf.truncated_normal_initializer(stddev=1e-1),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=binary_operator), position=0)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=:, text=:), position=1)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=l2_regularizer), position=2)",
            "Delete(target_node=ASTNode(type=as, text=as))",
            "Delete(target_node=ASTNode(type=identifier, text=scope))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=as_pattern_target))",
            "Delete(target_node=ASTNode(type=as_pattern))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2390,
        "neg_line": [
            "-with tf.variable_scope(name) as scope:"
        ],
        "pos_line": [
            "+with tf.variable_scope(name):"
        ],
        "core_change": "-with tf.variable_scope(name) as scope: +with tf.variable_scope(name):",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "1d199c0b5ba02630e39b3937af37e05005eb1774",
        "index": "3ec169775..a1cc9cef1 100644",
        "commit_message": "fix resuming when multigpu mode:   espnet2/tasks/abs_task.py\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AbsTask(ABC):",
            ")",
            "model.to(device)",
            "if model_file is not None:",
            "+            if device == \"cuda\":",
            "+                # NOTE(kamo): \"cuda\" for torch.load always indicates cuda:0",
            "+                #   in PyTorch<=1.4",
            "+                device = f\"cuda:{torch.cuda.current_device()}\"",
            "model.load_state_dict(torch.load(model_file, map_location=device))",
            "",
            "return model, args"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=158191)",
            "Insert(target_node=IN(type=block), node=('if_statement', None), position=0, insert_id=158192)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=158193)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=158194)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=158195)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=158196)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'device'), position=0, insert_id=158197)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=158198)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', '\"cuda\"'), position=2, insert_id=158199)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=158200)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=158201)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'device'), position=0, insert_id=158202)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=158203)",
            "Insert(target_node=IN(type=assignment), node=('string', 'f\"cuda:{torch.cuda.current_device()}\"'), position=2, insert_id=158204)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 4,
        "minus_line": 0,
        "AST_diff_line": 15,
        "number": 2391,
        "neg_line": [],
        "pos_line": [
            "+if device == \"cuda\":",
            "+# NOTE(kamo): \"cuda\" for torch.load always indicates cuda:0",
            "+#   in PyTorch<=1.4",
            "+device = f\"cuda:{torch.cuda.current_device()}\""
        ],
        "core_change": "+if device == \"cuda\": +# NOTE(kamo): \"cuda\" for torch.load always indicates cuda:0 +#   in PyTorch<=1.4 +device = f\"cuda:{torch.cuda.current_device()}\"",
        "core_API": "to"
    },
    {
        "commit_hash": "c4ede069c33adc380ce5c3223f1acca035a2756e",
        "index": "f28198ed7..9a9c9c1e4 100755",
        "commit_message": "attempt to fix media types test in pytoch (#2339)\n\n\n",
        "file": "wandb.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "def main():",
            "'tensorflow-variable-single-summary': tensorflow_variable_single,",
            "'tensorflow-variable-multi-summary': tensorflow_variable_multi,",
            "",
            "-            'graph-summary': graph,",
            "+            #'graph-summary': graph,",
            "})",
            "",
            "#history.add({"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=string, text='graph-summary'))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=graph))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 2392,
        "neg_line": [
            "-'graph-summary': graph,"
        ],
        "pos_line": [
            "+#'graph-summary': graph,"
        ],
        "core_change": "-'graph-summary': graph, +#'graph-summary': graph,",
        "core_API": "add"
    },
    {
        "commit_hash": "7db18ce0c28c0b7c93e9a61af55433f2d88d4213",
        "index": "ebc53241..8e70d140 100644",
        "commit_message": "fix mypy errors\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpatialSoftArgmax2d(nn.Module):",
            "",
            "# compute softmax with max substraction trick",
            "exp_x = torch.exp(x - torch.max(x, dim=-1, keepdim=True)[0])",
            "-        exp_x_sum = 1.0 / (exp_x.sum(dim=-1, keepdim=True) + self.eps)",
            "+        exp_x_sum = torch.tensor(",
            "+            1.0) / (exp_x.sum(dim=-1, keepdim=True) + self.eps)",
            "",
            "# create coordinates grid",
            "pos_y, pos_x = create_meshgrid(input, self.normalized_coordinates)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=0, insert_id=470359)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=470360)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=470361)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=470362)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=470363)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=470364)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=470365)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=float, text=1.0), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=470366)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 2395,
        "neg_line": [
            "-exp_x_sum = 1.0 / (exp_x.sum(dim=-1, keepdim=True) + self.eps)"
        ],
        "pos_line": [
            "+exp_x_sum = torch.tensor(",
            "+1.0) / (exp_x.sum(dim=-1, keepdim=True) + self.eps)"
        ],
        "core_change": "-exp_x_sum = 1.0 / (exp_x.sum(dim=-1, keepdim=True) + self.eps) +exp_x_sum = torch.tensor( +1.0) / (exp_x.sum(dim=-1, keepdim=True) + self.eps)",
        "core_API": "exp"
    },
    {
        "commit_hash": "ec3aace0ae6c89e9d7d6ed33425531db3b683485",
        "index": "a98e6e969..2be3b3671 100755",
        "commit_message": "Add type annotations for Rembert/Splinter and copies (#16338)\n\n* undo black autoformat\n\n* minor fix to rembert forward with default\n\n* make fix-copies, make quality\n\n* Adding types to template model\n\n* Removing List from the template types\n\n* Remove `Optional` from a couple of types that don't accept `None`\n\nCo-authored-by: matt <rocketknight1@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LayoutLMv2Output(nn.Module):",
            "self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)",
            "self.dropout = nn.Dropout(config.hidden_dropout_prob)",
            "",
            "-    def forward(self, hidden_states, input_tensor):",
            "+    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:",
            "hidden_states = self.dense(hidden_states)",
            "hidden_states = self.dropout(hidden_states)",
            "hidden_states = self.LayerNorm(hidden_states + input_tensor)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('->', '->'), position=3, insert_id=1202703)",
            "Insert(target_node=ASTNode(type=function_definition), node=('type', None), position=4, insert_id=1202704)",
            "Insert(target_node=ASTNode(type=function_definition), node=(':', ':'), position=5, insert_id=1202705)",
            "Insert(target_node=ASTNode(type=parameters), node=('typed_parameter', None), position=3, insert_id=1202706)",
            "Insert(target_node=ASTNode(type=parameters), node=('typed_parameter', None), position=6, insert_id=1202707)",
            "Insert(target_node=IN(type=type), node=('attribute', None), position=0, insert_id=1202708)",
            "Move(target_node=IN(type=typed_parameter), node=ASTNode(type=identifier, text=hidden_states), position=0)",
            "Insert(target_node=IN(type=typed_parameter), node=(':', ':'), position=1, insert_id=1202709)",
            "Insert(target_node=IN(type=typed_parameter), node=('type', None), position=2, insert_id=1202710)",
            "Move(target_node=IN(type=typed_parameter), node=ASTNode(type=identifier, text=input_tensor), position=0)",
            "Move(target_node=IN(type=typed_parameter), node=ASTNode(type=:, text=:), position=1)",
            "Insert(target_node=IN(type=typed_parameter), node=('type', None), position=2, insert_id=1202711)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1202712)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1202713)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=1202714)",
            "Insert(target_node=IN(type=type), node=('attribute', None), position=0, insert_id=1202715)",
            "Insert(target_node=IN(type=type), node=('attribute', None), position=0, insert_id=1202716)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1202717)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1202718)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=1202719)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1202720)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1202721)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=1202722)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 2396,
        "neg_line": [
            "-def forward(self, hidden_states, input_tensor):"
        ],
        "pos_line": [
            "+def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:"
        ],
        "core_change": "-def forward(self, hidden_states, input_tensor): +def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:",
        "core_API": "LayerNorm"
    },
    {
        "commit_hash": "c5b6ceea4a0a151ab78b9c065584bc416d1ee275",
        "index": "de643d70..11cfade4 100644",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def model_batch_norm(x, y_, reuse, is_train):",
            "net = FlattenLayer(net, name='flatten')  # output: (batch_size, 2304)",
            "net = DenseLayer(net, 384, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d1relu')",
            "net = DenseLayer(net, 192, act=tf.nn.relu, W_init=W_init2, b_init=b_init2, name='d2relu')",
            "-        net = DenseLayer(net, 10, act=tf.identity, W_init=W_init2, name='output')",
            "+        net = DenseLayer(net, 10, act=None, W_init=W_init2, name='output')",
            "y = net.outputs",
            "",
            "ce = tl.cost.cross_entropy(y, y_, name='cost')"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('none', 'None'), position=2, insert_id=2262253)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=identity))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2397,
        "neg_line": [
            "-net = DenseLayer(net, 10, act=tf.identity, W_init=W_init2, name='output')"
        ],
        "pos_line": [
            "+net = DenseLayer(net, 10, act=None, W_init=W_init2, name='output')"
        ],
        "core_change": "-net = DenseLayer(net, 10, act=tf.identity, W_init=W_init2, name='output') +net = DenseLayer(net, 10, act=None, W_init=W_init2, name='output')",
        "core_API": "cross_entropy"
    },
    {
        "commit_hash": "4686c7cb78ab9e4a4f8880a5f23505bd58edc8f6",
        "index": "2fd73a8..577883a 100644",
        "commit_message": "fix manga mask\n\n",
        "file": "lama-cleaner.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Manga(InpaintModel):",
            "",
            "mask = torch.from_numpy(mask[np.newaxis, :, :, :]).to(self.device)",
            "mask = mask.permute(0, 3, 1, 2)",
            "-        mask = torch.where(mask > 0.5, torch.tensor(1.0), torch.tensor(0.0))",
            "+        mask = torch.where(mask > 0.5, 1.0, 0.0)",
            "noise = torch.randn_like(mask)",
            "",
            "gray_img = gray_img / 255 * 2 - 1.0"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=float, text=1.0), position=3)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=float, text=0.0), position=6)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=7)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 2399,
        "neg_line": [
            "-mask = torch.where(mask > 0.5, torch.tensor(1.0), torch.tensor(0.0))"
        ],
        "pos_line": [
            "+mask = torch.where(mask > 0.5, 1.0, 0.0)"
        ],
        "core_change": "-mask = torch.where(mask > 0.5, torch.tensor(1.0), torch.tensor(0.0)) +mask = torch.where(mask > 0.5, 1.0, 0.0)",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "b5e2b183af5e40e33a4dc7659e697d137259d56e",
        "index": "86f6698c9..f4440d7c2 100644",
        "commit_message": "Doc styler examples (#14953)\n\n* Fix bad examples\n\n* Add black formatting to style_doc\n\n* Use first nonempty line\n\n* Put it at the right place\n\n* Don't add spaces to empty lines\n\n* Better templates\n\n* Deal with triple quotes in docstrings\n\n* Result of style_doc\n\n* Enable mdx treatment and fix code examples in MDXs\n\n* Result of doc styler on doc source files\n\n* Last fixes\n\n* Break copy from\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFAlbertForPreTraining(TFAlbertPreTrainedModel, TFAlbertPreTrainingLoss):",
            ">>> import tensorflow as tf",
            ">>> from transformers import AlbertTokenizer, TFAlbertForPreTraining",
            "",
            "-        >>> tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')",
            "-        >>> model = TFAlbertForPreTraining.from_pretrained('albert-base-v2')",
            "+        >>> tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")",
            "+        >>> model = TFAlbertForPreTraining.from_pretrained(\"albert-base-v2\")",
            "",
            "-        >>> input_ids = tf.constant(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True))[None, :]  # Batch size 1",
            "+        >>> input_ids = tf.constant(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True))[",
            "+        ...     None, :",
            "+        >>> ]  # Batch size 1",
            ">>> outputs = model(input_ids)",
            "",
            ">>> prediction_logits = outputs.prediction_logits"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=module), position=12)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=1, insert_id=2367955)",
            "Insert(target_node=IN(type=ERROR), node=('>>', '>>'), position=0, insert_id=2367956)",
            "Insert(target_node=IN(type=ERROR), node=('>', '>'), position=1, insert_id=2367957)",
            "Insert(target_node=ASTNode(type=subscript), node=('ERROR', None), position=2, insert_id=2367958)",
            "Insert(target_node=ASTNode(type=subscript), node=('ERROR', None), position=6, insert_id=2367959)",
            "Update(target_node=ASTNode(type=string, text='albert-base-v2'), value='\"albert-base-v2\"')",
            "Update(target_node=ASTNode(type=string, text='albert-base-v2'), value='\"albert-base-v2\"')",
            "Insert(target_node=IN(type=ERROR), node=('ellipsis', '...'), position=0, insert_id=2367960)",
            "Insert(target_node=IN(type=ERROR), node=('>', '>'), position=0, insert_id=2367961)",
            "Insert(target_node=IN(type=ERROR), node=('>>', '>>'), position=1, insert_id=2367962)",
            "Delete(target_node=ASTNode(type=>>, text=>>))",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 5,
        "minus_line": 3,
        "AST_diff_line": 14,
        "number": 2402,
        "neg_line": [
            "->>> tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')",
            "->>> model = TFAlbertForPreTraining.from_pretrained('albert-base-v2')",
            "->>> input_ids = tf.constant(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True))[None, :]  # Batch size 1"
        ],
        "pos_line": [
            "+>>> tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")",
            "+>>> model = TFAlbertForPreTraining.from_pretrained(\"albert-base-v2\")",
            "+>>> input_ids = tf.constant(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True))[",
            "+...     None, :",
            "+>>> ]  # Batch size 1"
        ],
        "core_change": "->>> tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2') ->>> model = TFAlbertForPreTraining.from_pretrained('albert-base-v2') +>>> tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\") +>>> model = TFAlbertForPreTraining.from_pretrained(\"albert-base-v2\") ->>> input_ids = tf.constant(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True))[None, :]  # Batch size 1 +>>> input_ids = tf.constant(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True))[ +...     None, : +>>> ]  # Batch size 1",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "7e56ba28641d4901194021d3c61ba661c1e8fd90",
        "index": "c9012dcc9..263eb3c80 100644",
        "commit_message": "Fix spurious warning in TF TokenClassification models (#15435)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFTokenClassificationLoss:",
            "# make sure only labels that are not equal to -100",
            "# are taken into account as loss",
            "if tf.math.reduce_any(labels == -1):",
            "-            warnings.warn(\"Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\")",
            "+            tf.print(\"Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\")",
            "active_loss = tf.reshape(labels, (-1,)) != -1",
            "else:",
            "active_loss = tf.reshape(labels, (-1,)) != -100"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=warnings), value='tf')",
            "Update(target_node=ASTNode(type=identifier, text=warn), value='print')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 2409,
        "neg_line": [
            "-warnings.warn(\"Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\")"
        ],
        "pos_line": [
            "+tf.print(\"Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\")"
        ],
        "core_change": "-warnings.warn(\"Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\") +tf.print(\"Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\")",
        "core_API": "reduce_any"
    },
    {
        "commit_hash": "eb1217f3ac93fa906761216b16ae1b9435cbb850",
        "index": "e53b478..1100945 100644",
        "commit_message": "Add PyTorch AMP check (#7917)\n\n* Add PyTorch AMP check\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Cleanup\n\n* Cleanup\n\n* Cleanup\n\n* Robust for DDP\n\n* Fixes\n\n* Add amp enabled boolean to check_train_batch_size\n\n* Simplify\n\n* space to prefix\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from copy import deepcopy",
            "",
            "import numpy as np",
            "import torch",
            "-from torch.cuda import amp",
            "",
            "from utils.general import LOGGER, colorstr",
            "from utils.torch_utils import profile",
            "",
            "",
            "-def check_train_batch_size(model, imgsz=640):",
            "+def check_train_batch_size(model, imgsz=640, amp=True):",
            "# Check YOLOv5 training batch size",
            "-    with amp.autocast():",
            "+    with torch.cuda.amp.autocast(amp):",
            "return autobatch(deepcopy(model).train(), imgsz)  # compute optimal batch size"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=4, insert_id=1294147)",
            "Insert(target_node=ASTNode(type=parameters), node=('default_parameter', None), position=5, insert_id=1294148)",
            "Insert(target_node=IN(type=default_parameter), node=('identifier', 'amp'), position=0, insert_id=1294149)",
            "Insert(target_node=IN(type=default_parameter), node=('=', '='), position=1, insert_id=1294150)",
            "Insert(target_node=IN(type=default_parameter), node=('true', 'True'), position=2, insert_id=1294151)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=1294152)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'amp'), position=1, insert_id=1294153)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1294154)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1294155)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=amp), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1294156)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1294157)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=1294158)",
            "Delete(target_node=ASTNode(type=from, text=from))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=dotted_name))",
            "Delete(target_node=ASTNode(type=import, text=import))",
            "Delete(target_node=ASTNode(type=identifier, text=amp))",
            "Delete(target_node=ASTNode(type=dotted_name))",
            "Delete(target_node=ASTNode(type=import_from_statement))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 2410,
        "neg_line": [
            "-from torch.cuda import amp",
            "-def check_train_batch_size(model, imgsz=640):",
            "-with amp.autocast():"
        ],
        "pos_line": [
            "+def check_train_batch_size(model, imgsz=640, amp=True):",
            "+with torch.cuda.amp.autocast(amp):"
        ],
        "core_change": "-from torch.cuda import amp -def check_train_batch_size(model, imgsz=640): +def check_train_batch_size(model, imgsz=640, amp=True): -with amp.autocast(): +with torch.cuda.amp.autocast(amp):",
        "core_API": "autocast"
    },
    {
        "commit_hash": "2b8de8d96a38448fe3b92b2283d1988b46f2a801",
        "index": "3b9ba41..a4ecc42 100644",
        "commit_message": "Add Speaker embedding, fix tacotron-2 missing code.\n\n* add speaker embedding for fastspeech encoder and decoder.\n* use softplus activation function for speaker embedding.\n* training always = True for prenet.\n\n",
        "file": "TensorFlowTTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def test_tacotron2_trainable(n_speakers, n_chars, max_input_length, max_mel_leng",
            "post_mel_preds, \\",
            "stop_preds, \\",
            "alignment_history = model(input_ids,",
            "-                                          tf.constant([max_mel_length, max_mel_length]),",
            "+                                          tf.constant([max_input_length, max_input_length]),",
            "speaker_ids,",
            "mel_outputs,",
            "-                                          mel_lengths)",
            "+                                          mel_lengths,",
            "+                                          training=True)",
            "loss_before = tf.keras.losses.MeanSquaredError()(mel_outputs, mel_preds)",
            "loss_after = tf.keras.losses.MeanSquaredError()(mel_outputs, post_mel_preds)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=10, insert_id=2216380)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=11, insert_id=2216381)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'training'), position=0, insert_id=2216382)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2216383)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=2216384)",
            "Update(target_node=ASTNode(type=identifier, text=max_mel_length), value='max_input_length')",
            "Update(target_node=ASTNode(type=identifier, text=max_mel_length), value='max_input_length')"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 2411,
        "neg_line": [
            "-tf.constant([max_mel_length, max_mel_length]),",
            "-mel_lengths)"
        ],
        "pos_line": [
            "+tf.constant([max_input_length, max_input_length]),",
            "+mel_lengths,",
            "+training=True)"
        ],
        "core_change": "-tf.constant([max_mel_length, max_mel_length]), +tf.constant([max_input_length, max_input_length]), -mel_lengths) +mel_lengths, +training=True)",
        "core_API": "constant"
    },
    {
        "commit_hash": "4cbe13cdfded336dbb814a7c10eba19075b9ba13",
        "index": "eef1fb978..287206745 100644",
        "commit_message": "[RLlib] CQL loss fn fixes, MuJoCo + Pendulum benchmarks, offline-RL example script w/ json file. (#15603)\n\nCo-authored-by: Sven Mika <sven@anyscale.io>\nCo-authored-by: sven1977 <svenmika1977@gmail.com>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def apply_grad_clipping(policy, optimizer, loss):",
            "",
            "",
            "def atanh(x):",
            "-    return 0.5 * torch.log((1 + x) / (1 - x))",
            "+    return 0.5 * torch.log(",
            "+        (1 + x).clamp(min=SMALL_NUMBER) / (1 - x).clamp(min=SMALL_NUMBER))",
            "",
            "",
            "def convert_to_non_torch_type(stats):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=0, insert_id=1117296)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=3, insert_id=1117297)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1117298)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1117299)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1117300)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1117301)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=parenthesized_expression), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1117302)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'clamp'), position=2, insert_id=1117303)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1117304)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=1117305)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1117306)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=parenthesized_expression), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1117307)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'clamp'), position=2, insert_id=1117308)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1117309)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=1117310)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1117311)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'min'), position=0, insert_id=1117312)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1117313)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'SMALL_NUMBER'), position=2, insert_id=1117314)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'min'), position=0, insert_id=1117315)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1117316)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'SMALL_NUMBER'), position=2, insert_id=1117317)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 2412,
        "neg_line": [
            "-return 0.5 * torch.log((1 + x) / (1 - x))"
        ],
        "pos_line": [
            "+return 0.5 * torch.log(",
            "+(1 + x).clamp(min=SMALL_NUMBER) / (1 - x).clamp(min=SMALL_NUMBER))"
        ],
        "core_change": "-return 0.5 * torch.log((1 + x) / (1 - x)) +return 0.5 * torch.log( +(1 + x).clamp(min=SMALL_NUMBER) / (1 - x).clamp(min=SMALL_NUMBER))",
        "core_API": "log"
    },
    {
        "commit_hash": "f58bb33624521682f58a01471ac16ddf1a21d835",
        "index": "c955fa6b49..f638674036 100644",
        "commit_message": "fixed lint errors.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def randint(",
            "device: Optional[Union[ivy.Device, str]] = None,",
            ") -> Tensor:",
            "device = default_device(device)",
            "-    low = tf.cast(low, 'int64')",
            "-    high = tf.cast(high, 'int64')",
            "+    low = tf.cast(low, \"int64\")",
            "+    high = tf.cast(high, \"int64\")",
            "with tf.device(\"/\" + device.upper()):",
            "return tf.random.uniform(shape=shape, minval=low, maxval=high, dtype=tf.int64)",
            "",
            "-",
            "+",
            "def seed(seed_value: int = 0) -> None:",
            "tf.random.set_seed(seed_value)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2008570)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2008571)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2008572)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=2, insert_id=2008573)",
            "Update(target_node=ASTNode(type=string, text='int64'), value='\"int64\"')",
            "Update(target_node=ASTNode(type=string, text='int64'), value='\"int64\"')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cast))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 2413,
        "neg_line": [
            "-low = tf.cast(low, 'int64')",
            "-high = tf.cast(high, 'int64')",
            "-"
        ],
        "pos_line": [
            "+low = tf.cast(low, \"int64\")",
            "+high = tf.cast(high, \"int64\")",
            "+"
        ],
        "core_change": "-low = tf.cast(low, 'int64') -high = tf.cast(high, 'int64') +low = tf.cast(low, \"int64\") +high = tf.cast(high, \"int64\") - +",
        "core_API": "cast"
    },
    {
        "commit_hash": "92ccfe0a46eeb6545336cbe239094aa10e93a9b0",
        "index": "6de8395e..d68e72cd 100644",
        "commit_message": "Fix tensor name for batch norm (#663); plus some docs update.\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def update_bn_ema(xn, batch_mean, batch_var,",
            "else:",
            "tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_op1)",
            "tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_op2)",
            "-        return xn",
            "+        return tf.identity(xn, name='output')",
            "",
            "",
            "def reshape_for_bn(param, ndims, chan, data_format):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('call', None), position=0, insert_id=2289590)",
            "Insert(target_node=ASTNode(type=call), node=('ERROR', None), position=1, insert_id=2289591)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2289592)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2289593)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'def'), position=0, insert_id=2289594)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=reshape_for_bn), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('ERROR', None), position=1, insert_id=2289595)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=2, insert_id=2289596)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'identity'), position=3, insert_id=2289597)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2289598)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=xn), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2289599)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2289600)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2289601)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=return), position=0)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'tf'), position=1, insert_id=2289602)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'name'), position=0, insert_id=2289603)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2289604)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'output'\"), position=2, insert_id=2289605)",
            "Delete(target_node=ASTNode(type=def, text=def))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 2414,
        "neg_line": [
            "-return xn"
        ],
        "pos_line": [
            "+return tf.identity(xn, name='output')"
        ],
        "core_change": "-return xn +return tf.identity(xn, name='output')",
        "core_API": "add_to_collection"
    },
    {
        "commit_hash": "1eb977007829d34ab7b5a169479b9de45cca9bb8",
        "index": "386bd38f..4e6f0159 100644",
        "commit_message": "Fix logging when loss is in FP16 (#1047)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1047\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D20068167\n\nPulled By: myleott\n\nfbshipit-source-id: 46ccca05ed225103a1b2aa6fa80f8fbfe91b040b\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer(object):",
            "# convert logging_outputs to CPU to avoid unnecessary",
            "# device-to-host transfers in reduce_metrics",
            "logging_outputs = utils.apply_to_sample(",
            "-                lambda t: t.to(device='cpu', non_blocking=True),",
            "+                lambda t: t.to(device='cpu', non_blocking=True, dtype=torch.double),",
            "logging_outputs",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=215866)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=215867)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=215868)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=215869)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=215870)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=215871)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=215872)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'double'), position=2, insert_id=215873)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2415,
        "neg_line": [
            "-lambda t: t.to(device='cpu', non_blocking=True),"
        ],
        "pos_line": [
            "+lambda t: t.to(device='cpu', non_blocking=True, dtype=torch.double),"
        ],
        "core_change": "-lambda t: t.to(device='cpu', non_blocking=True), +lambda t: t.to(device='cpu', non_blocking=True, dtype=torch.double),",
        "core_API": "apply_to_sample"
    },
    {
        "commit_hash": "5d62c95cd94df4b2a303309d90cec75424c05b3d",
        "index": "c95a0b3..e10f000 100644",
        "commit_message": "fix docs of customized (#1508)\n\n\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "\"class SingleDenseLayerBlock(ak.Block):\\n\",",
            "\"    def build(self, hp, inputs=None):\\n\",",
            "\"        # Get the input_node from inputs.\\n\",",
            "-    \"        input_node = tf.python.util.nest.flatten(inputs)[0]\\n\",",
            "+    \"        input_node = tf.nest.flatten(inputs)[0]\\n\",",
            "\"        layer = tf.keras.layers.Dense(\\n\",",
            "\"            hp.Int(\\\"num_units\\\", min_value=32, max_value=512, step=32)\\n\",",
            "\"        )\\n\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"        input_node = tf.python.util.nest.flatten(inputs)[0]\\n\"), value='\"        input_node = tf.nest.flatten(inputs)[0]\\\\n\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2416,
        "neg_line": [
            "-\"        input_node = tf.python.util.nest.flatten(inputs)[0]\\n\","
        ],
        "pos_line": [
            "+\"        input_node = tf.nest.flatten(inputs)[0]\\n\","
        ],
        "core_change": "-\"        input_node = tf.python.util.nest.flatten(inputs)[0]\\n\", +\"        input_node = tf.nest.flatten(inputs)[0]\\n\",",
        "core_API": "flatten"
    },
    {
        "commit_hash": "1ed2ebf60d87ef12bd063c7c58e484e19189c754",
        "index": "d3d8085d3..09d3d0db8 100755",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class ConvBertSelfAttention(nn.Module):",
            "attention_scores = attention_scores + attention_mask",
            "",
            "# Normalize the attention scores to probabilities.",
            "-        attention_probs = torch.nn.functional.softmax(attention_scores, dim=-1)",
            "+        attention_probs = nn.functional.softmax(attention_scores, dim=-1)",
            "",
            "# This is actually dropping out entire tokens to attend to, which might",
            "# seem a bit unusual, but is taken from the original Transformer paper."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 2417,
        "neg_line": [
            "-attention_probs = torch.nn.functional.softmax(attention_scores, dim=-1)"
        ],
        "pos_line": [
            "+attention_probs = nn.functional.softmax(attention_scores, dim=-1)"
        ],
        "core_change": "-attention_probs = torch.nn.functional.softmax(attention_scores, dim=-1) +attention_probs = nn.functional.softmax(attention_scores, dim=-1)",
        "core_API": "softmax"
    },
    {
        "commit_hash": "26edfabeb2e2933d198376813d2c9f0a879ca6d6",
        "index": "49916d07..5c3c2b68 100644",
        "commit_message": "misc fix\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def batch_flatten(x):",
            "return tf.reshape(x, [-1, total_dim])",
            "",
            "def logSoftmax(x):",
            "-    with tf.variable_scope('logSoftmax'):",
            "+    with tf.op_scope([x], 'logSoftmax'):",
            "z = x - tf.reduce_max(x, 1, keep_dims=True)",
            "logprob = z - tf.log(tf.reduce_sum(tf.exp(z), 1, keep_dims=True))",
            "return logprob"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=variable_scope), value='op_scope')",
            "Insert(target_node=ASTNode(type=argument_list), node=('list', None), position=1, insert_id=2321650)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2321651)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=2321652)",
            "Insert(target_node=IN(type=list), node=('identifier', 'x'), position=1, insert_id=2321653)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=2, insert_id=2321654)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 2418,
        "neg_line": [
            "-with tf.variable_scope('logSoftmax'):"
        ],
        "pos_line": [
            "+with tf.op_scope([x], 'logSoftmax'):"
        ],
        "core_change": "-with tf.variable_scope('logSoftmax'): +with tf.op_scope([x], 'logSoftmax'):",
        "core_API": "reshape"
    },
    {
        "commit_hash": "c506e45b078d86f8bf4743bbdc3bf59351904b8b",
        "index": "ff00040..a45ea9b 100644",
        "commit_message": "Fix AttributeError: 'Tensor' object has no attribute 'numpy'\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def load_tf_weights_in_gpt2(model, gpt2_checkpoint_path):",
            "import re",
            "",
            "import tensorflow as tf",
            "+        tf.enable_eager_execution()",
            "except ImportError:",
            "logger.error(",
            "\"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1933911)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1933912)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1933913)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1933914)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1933915)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1933916)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'enable_eager_execution'), position=2, insert_id=1933917)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1933918)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1933919)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 9,
        "number": 2419,
        "neg_line": [],
        "pos_line": [
            "+tf.enable_eager_execution()"
        ],
        "core_change": "+tf.enable_eager_execution()",
        "core_API": "enable_eager_execution"
    },
    {
        "commit_hash": "e94abf66783f5e24ab37adc754ac902e827a4b4d",
        "index": "eab9735e..59693a01 100644",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "from ...utils.argtools import memoized_ignoreargs",
            "try:",
            "from tensorflow.models.rnn.ptb import reader as tfreader",
            "except ImportError:",
            "-    logger.warn_dependency('PennTreeBank', 'tensorflow')",
            "+    logger.warn_dependency('PennTreeBank', 'tensorflow.models.rnn.ptb.reader')",
            "__all__ = []",
            "else:",
            "__all__ = ['get_PennTreeBank']"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='tensorflow'), value=\"'tensorflow.models.rnn.ptb.reader'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2420,
        "neg_line": [
            "-logger.warn_dependency('PennTreeBank', 'tensorflow')"
        ],
        "pos_line": [
            "+logger.warn_dependency('PennTreeBank', 'tensorflow.models.rnn.ptb.reader')"
        ],
        "core_change": "-logger.warn_dependency('PennTreeBank', 'tensorflow') +logger.warn_dependency('PennTreeBank', 'tensorflow.models.rnn.ptb.reader')",
        "core_API": "warn_dependency"
    },
    {
        "commit_hash": "8777de3c4b8a84bfce94036bd95ebc2afbfce3e8",
        "index": "86623a5e..570a3e9a 100755",
        "commit_message": "Fix import order in model, reformatting.\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class NaturalGradient(Optimizer):",
            "# [delta*lambda] / lambda",
            "estimated_diffs = [diff / lagrange_multiplier for diff in diffs]",
            "# deriv(loss)^T * sum(delta)",
            "-            estimated_improvement = tf.add_n(inputs=[tf.reduce_sum(input_tensor=(grad * diff)) for grad, diff in zip(loss_gradient, estimated_diffs)])",
            "+            estimated_improvement = tf.add_n(inputs=[tf.reduce_sum(input_tensor=(grad * diff))",
            "+                                                     for grad, diff in zip(loss_gradient, estimated_diffs)])",
            "",
            "applied = self.apply_step(variables=variables, diffs=estimated_diffs)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2422,
        "neg_line": [
            "-estimated_improvement = tf.add_n(inputs=[tf.reduce_sum(input_tensor=(grad * diff)) for grad, diff in zip(loss_gradient, estimated_diffs)])"
        ],
        "pos_line": [
            "+estimated_improvement = tf.add_n(inputs=[tf.reduce_sum(input_tensor=(grad * diff))",
            "+for grad, diff in zip(loss_gradient, estimated_diffs)])"
        ],
        "core_change": "-estimated_improvement = tf.add_n(inputs=[tf.reduce_sum(input_tensor=(grad * diff)) for grad, diff in zip(loss_gradient, estimated_diffs)]) +estimated_improvement = tf.add_n(inputs=[tf.reduce_sum(input_tensor=(grad * diff)) +for grad, diff in zip(loss_gradient, estimated_diffs)])",
        "core_API": "add_n"
    },
    {
        "commit_hash": "88e7d2586b33fab1ade96a5a2390daeab368c57b",
        "index": "9144083a..ad1e14e0 100644",
        "commit_message": "fix flake8 issues (#2570)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\n- [x] applies flake8 fixes to main branch (https://github.com/fairinternal/fairseq-py/issues/2546) - still more to be fixed\n\nFix GPU tests:\n- [x] when torch.ao.quantization import doesn't work use torch.quantization\n- [x] build apex from earlier commit in circleci so that its compatible with pytorch 1.8 and 1.9\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2570\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D32955312\n\nPulled By: dianaml0\n\nfbshipit-source-id: e163cbd4998f171f819e31b0682c1c0f1986f9e1\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def emulate_int8_channel(w, scale=None, zero_point=None, bits=8):",
            "",
            "def emulate_int8_tensor(w, scale=None, zero_point=None, bits=8):",
            "if scale is None:",
            "-        obs = torch.ao.quantization.observer.MinMaxObserver()",
            "+        obs = quantization.observer.MinMaxObserver()",
            "obs.to(device=w.device)",
            "_ = obs(w)",
            "scale, zero_point = obs.calculate_qparams()"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=quantization), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=ao))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2423,
        "neg_line": [
            "-obs = torch.ao.quantization.observer.MinMaxObserver()"
        ],
        "pos_line": [
            "+obs = quantization.observer.MinMaxObserver()"
        ],
        "core_change": "-obs = torch.ao.quantization.observer.MinMaxObserver() +obs = quantization.observer.MinMaxObserver()",
        "core_API": "MinMaxObserver"
    },
    {
        "commit_hash": "e8b41cfeec0b70b692e1a042e6f1e542f1fe446d",
        "index": "5c6f483f..8bf424f1 100644",
        "commit_message": "Fix `DBLP` download, move `get_edge_index` to `testing` package (#6901)\n\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_heterogeneous_dataloader(num_workers):",
            "data = HeteroData()",
            "data['p'].x = torch.randn(100, 128)",
            "data['a'].x = torch.randn(200, 128)",
            "-    data['p', 'a'].edge_index = get_edge_index(100, 200, 500)",
            "+    data['p', 'a'].edge_index = get_random_edge_index(100, 200, 500)",
            "data['p'].edge_attr = torch.randn(500, 32)",
            "-    data['a', 'p'].edge_index = get_edge_index(200, 100, 400)",
            "+    data['a', 'p'].edge_index = get_random_edge_index(200, 100, 400)",
            "data['a', 'p'].edge_attr = torch.randn(400, 32)",
            "",
            "loader = DataLoader([data, data, data, data], batch_size=2, shuffle=False,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=get_edge_index), value='get_random_edge_index')",
            "Update(target_node=ASTNode(type=identifier, text=get_edge_index), value='get_random_edge_index')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 2424,
        "neg_line": [
            "-data['p', 'a'].edge_index = get_edge_index(100, 200, 500)",
            "-data['a', 'p'].edge_index = get_edge_index(200, 100, 400)"
        ],
        "pos_line": [
            "+data['p', 'a'].edge_index = get_random_edge_index(100, 200, 500)",
            "+data['a', 'p'].edge_index = get_random_edge_index(200, 100, 400)"
        ],
        "core_change": "-data['p', 'a'].edge_index = get_edge_index(100, 200, 500) +data['p', 'a'].edge_index = get_random_edge_index(100, 200, 500) -data['a', 'p'].edge_index = get_edge_index(200, 100, 400) +data['a', 'p'].edge_index = get_random_edge_index(200, 100, 400)",
        "core_API": "randn"
    },
    {
        "commit_hash": "00d48891ef440eaf6f5bc599e052f2b221a03a08",
        "index": "11ff1fcf..2f363906 100644",
        "commit_message": "Fix Head params to accept classifier_activation\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "def RegNet(",
            "in_channels = out_channels",
            "",
            "if include_top:",
            "-        x = Head(num_classes=classes)(x)",
            "imagenet_utils.validate_activation(classifier_activation, weights)",
            "+        x = Head(",
            "+            num_classes=classes,",
            "+            classifier_activation=classifier_activation,",
            "+            name=model_name,",
            "+        )(x)",
            "",
            "else:",
            "if pooling == \"avg\":"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('ERROR', None), position=7, insert_id=2571287)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=call), position=8)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=include_top), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=:, text=:), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=call), position=2)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=x), position=3)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type==, text==), position=4)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2571288)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2571289)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2571290)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2571291)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=2571292)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'classifier_activation'), position=0, insert_id=2571293)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2571294)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'classifier_activation'), position=2, insert_id=2571295)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'name'), position=0, insert_id=2571296)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2571297)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'model_name'), position=2, insert_id=2571298)",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 2428,
        "neg_line": [
            "-x = Head(num_classes=classes)(x)"
        ],
        "pos_line": [
            "+x = Head(",
            "+num_classes=classes,",
            "+classifier_activation=classifier_activation,",
            "+name=model_name,",
            "+)(x)"
        ],
        "core_change": "-x = Head(num_classes=classes)(x) +x = Head( +num_classes=classes, +classifier_activation=classifier_activation, +name=model_name, +)(x)",
        "core_API": "validate_activation"
    },
    {
        "commit_hash": "02b176c4ce14340d26d42825523f406959c6c202",
        "index": "363d337e2..37172d14f 100644",
        "commit_message": "Fix torch version comparisons (#18460)\n\nComparisons like\nversion.parse(torch.__version__) > version.parse(\"1.6\")\nare True for torch==1.6.0+cu101 or torch==1.6.0+cpu\n\nversion.parse(version.parse(torch.__version__).base_version) are preferred (and available in pytorch_utils.py\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def is_torch_tf32_available():",
            "return False",
            "if int(torch.version.cuda.split(\".\")[0]) < 11:",
            "return False",
            "-    if version.parse(torch.__version__) < version.parse(\"1.7\"):",
            "+    if version.parse(version.parse(torch.__version__).base_version) < version.parse(\"1.7\"):",
            "return False",
            "",
            "return True"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=1193642)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1193643)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'base_version'), position=2, insert_id=1193644)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1193645)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1193646)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'version'), position=0, insert_id=1193647)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1193648)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'parse'), position=2, insert_id=1193649)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1193650)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1193651)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 2430,
        "neg_line": [
            "-if version.parse(torch.__version__) < version.parse(\"1.7\"):"
        ],
        "pos_line": [
            "+if version.parse(version.parse(torch.__version__).base_version) < version.parse(\"1.7\"):"
        ],
        "core_change": "-if version.parse(torch.__version__) < version.parse(\"1.7\"): +if version.parse(version.parse(torch.__version__).base_version) < version.parse(\"1.7\"):",
        "core_API": "split"
    },
    {
        "commit_hash": "bf1d2bf01d76c4c38bfdabfcbb91fa64fd082046",
        "index": "e5ffad1..97a3374 100644",
        "commit_message": "Refactoring (#528)\n\n* Reorganize decoders, add deprecation for utils, add dataset\n* Fix imports\n",
        "file": "segmentation_models.pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SegmentationModel(torch.nn.Module):",
            "if self.training:",
            "self.eval()",
            "",
            "-        with torch.no_grad():",
            "-            x = self.forward(x)",
            "+        x = self.forward(x)",
            "",
            "return x"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=3)",
            "Delete(target_node=ASTNode(type=with, text=with))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=no_grad))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=with_item))",
            "Delete(target_node=ASTNode(type=with_clause))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=with_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 15,
        "number": 2431,
        "neg_line": [
            "-with torch.no_grad():",
            "-x = self.forward(x)"
        ],
        "pos_line": [
            "+x = self.forward(x)"
        ],
        "core_change": "-with torch.no_grad(): -x = self.forward(x) +x = self.forward(x)",
        "core_API": "eval"
    },
    {
        "commit_hash": "b96f9fdc2d5d0d375809ad9c16608830b01fc59a",
        "index": "700b65b5..ac78e011 100644",
        "commit_message": "fix sample_weight for BinAcc\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def binary_accuracy(y_true, y_pred, threshold=0.5):",
            "prediction values are 1 or 0.",
            "",
            "Returns:",
            "-    Binary accuracy values. shape = `[batch_size, d0, .. dN-1]`",
            "+    Binary accuracy values. shape = `[batch_size, d0, .. dN]`",
            "\"\"\"",
            "y_pred = tf.convert_to_tensor(y_pred)",
            "threshold = tf.cast(threshold, y_pred.dtype)",
            "y_pred = tf.cast(y_pred > threshold, y_pred.dtype)",
            "-  return backend.mean(tf.equal(y_true, y_pred), axis=-1)",
            "+  return tf.cast(tf.equal(y_true, y_pred), tf.int8)",
            "",
            "",
            "@keras_export('keras.metrics.categorical_accuracy')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=`[batch_size, d0, .. dN-1]`), value='`[batch_size, d0, .. dN]`')",
            "Update(target_node=ASTNode(type=identifier, text=backend), value='tf')",
            "Update(target_node=ASTNode(type=identifier, text=mean), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2068431)",
            "Update(target_node=ASTNode(type=identifier, text=axis), value='tf')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=axis), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2068432)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int8'), position=2, insert_id=2068433)",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 2433,
        "neg_line": [
            "-Binary accuracy values. shape = `[batch_size, d0, .. dN-1]`",
            "-return backend.mean(tf.equal(y_true, y_pred), axis=-1)"
        ],
        "pos_line": [
            "+Binary accuracy values. shape = `[batch_size, d0, .. dN]`",
            "+return tf.cast(tf.equal(y_true, y_pred), tf.int8)"
        ],
        "core_change": "-Binary accuracy values. shape = `[batch_size, d0, .. dN-1]` +Binary accuracy values. shape = `[batch_size, d0, .. dN]` -return backend.mean(tf.equal(y_true, y_pred), axis=-1) +return tf.cast(tf.equal(y_true, y_pred), tf.int8)",
        "core_API": "convert_to_tensor"
    },
    {
        "commit_hash": "e1301a1769ed07641f4b1c13d253eaf7a638e734",
        "index": "cfd7d09ee1..33d055ef74 100644",
        "commit_message": "Data type Fix Up (#2305)\n\nCo-authored-by: @simonetgordon   <simonegordon12@icloud.com>\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def iinfo(type: Union[DType, str, tf.Tensor, tf.Variable]) -> np.iinfo:",
            "",
            "def result_type(",
            "*arrays_and_dtypes: Union[tf.Tensor, tf.Variable, tf.DType],",
            "-) -> tf.DType:",
            "+) -> ivy.Dtype:",
            "if len(arrays_and_dtypes) <= 1:",
            "return tf.experimental.numpy.result_type(arrays_and_dtypes)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='ivy')",
            "Update(target_node=ASTNode(type=identifier, text=DType), value='Dtype')"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 2,
        "number": 2435,
        "neg_line": [
            "-) -> tf.DType:"
        ],
        "pos_line": [
            "+) -> ivy.Dtype:"
        ],
        "core_change": "-) -> tf.DType: +) -> ivy.Dtype:",
        "core_API": "result_type"
    },
    {
        "commit_hash": "1e15940f538bc5eb4ca5cd463f57a0b6323d6250",
        "index": "b64db90..0658843 100644",
        "commit_message": "fix big bug with continuous time gaussian diffusion noise level at inference time, thanks to @BIGJUN777\n\n",
        "file": "imagen-pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GaussianDiffusionContinuousTimes(nn.Module):",
            "self.num_timesteps = timesteps",
            "",
            "def get_times(self, batch_size, noise_level, *, device):",
            "-        return torch.full((batch_size,), noise_level, device = device, dtype = torch.long)",
            "+        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)",
            "",
            "def sample_random_times(self, batch_size, max_thres = 0.999, *, device):",
            "return torch.zeros((batch_size,), device = device).float().uniform_(0, max_thres)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=long), value='float32')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2436,
        "neg_line": [
            "-return torch.full((batch_size,), noise_level, device = device, dtype = torch.long)"
        ],
        "pos_line": [
            "+return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)"
        ],
        "core_change": "-return torch.full((batch_size,), noise_level, device = device, dtype = torch.long) +return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)",
        "core_API": "full"
    },
    {
        "commit_hash": "a5f9e6f900e27a879e7355587ddf10944948084c",
        "index": "8572fd7dac..d271be41b0 100644",
        "commit_message": "Removed blind upcasting and fixed data generation for norms based on typehints (#6587)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def var(",
            "tf.experimental.numpy.var(x, axis=axis, out=out, keepdims=keepdims),",
            "size / (size - correction),",
            "),",
            "-            dtype,",
            "+            x.dtype,",
            "copy=False,",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'x'), position=7, insert_id=1978502)",
            "Insert(target_node=ASTNode(type=ERROR), node=('.', '.'), position=8, insert_id=1978503)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 2437,
        "neg_line": [
            "-dtype,"
        ],
        "pos_line": [
            "+x.dtype,"
        ],
        "core_change": "-dtype, +x.dtype,",
        "core_API": "var"
    },
    {
        "commit_hash": "d0fab84e4dfdddc64cb4275ff98cf67f84ff86a0",
        "index": "aee50b320..a284af18e 100644",
        "commit_message": "[RLlib] DDPG PyTorch version. (#7953)\n\nThe DDPG/TD3 algorithms currently do not have a PyTorch implementation. This PR adds PyTorch support for DDPG/TD3 to RLlib.\nThis PR:\n- Depends on the re-factor PR for DDPG (Functional Algorithm API).\n- Adds learning regression tests for the PyTorch version of DDPG and a DDPG (torch)\n- Updates the documentation to reflect that DDPG and TD3 now support PyTorch.\n\n* Learning Pendulum-v0 on torch version (same config as tf). Wall time a little slower (~20% than tf).\n* Fix GPU target model problem.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_variable(value,",
            "tf_name, initializer=value, dtype=dtype, trainable=trainable)",
            "elif framework == \"torch\" and torch_tensor is True:",
            "torch, _ = try_import_torch()",
            "-        var_ = torch.from_numpy(value).to(device)",
            "+        var_ = torch.from_numpy(value)",
            "+        if device:",
            "+            var_ = var_.to(device)",
            "var_.requires_grad = trainable",
            "return var_",
            "# torch or None: Return python primitive."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=1124777)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1124778)",
            "Insert(target_node=IN(type=if_statement), node=('identifier', 'device'), position=1, insert_id=1124779)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1124780)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1124781)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1124782)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1124783)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'var_'), position=0, insert_id=1124784)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1124785)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1124786)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1124787)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'var_'), position=0, insert_id=1124788)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=to), position=2)",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 2438,
        "neg_line": [
            "-var_ = torch.from_numpy(value).to(device)"
        ],
        "pos_line": [
            "+var_ = torch.from_numpy(value)",
            "+if device:",
            "+var_ = var_.to(device)"
        ],
        "core_change": "-var_ = torch.from_numpy(value).to(device) +var_ = torch.from_numpy(value) +if device: +var_ = var_.to(device)",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "e850d36cab88360177f85d98d73c5efb4d9925ba",
        "index": "= adj._indices()",
        "commit_message": "fixed graclus and following transforms\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PolarAdj(object):",
            "theta += (theta < 0).type_as(theta)",
            "polar = torch.stack([rho, theta], dim=1)",
            "",
            "-        # Modify data and return.",
            "-        data.adj = SparseTensor(index, polar, torch.Size([n, n, 2]))",
            "-        return data",
            "+        return SparseTensor(index, polar, torch.Size([n, n, 2]))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=3, insert_id=1088404)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=1088405)",
            "Move(target_node=IN(type=return_statement), node=ASTNode(type=call), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=data))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=adj))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=data))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 13,
        "number": 2440,
        "neg_line": [
            "-# Modify data and return.",
            "-data.adj = SparseTensor(index, polar, torch.Size([n, n, 2]))",
            "-return data"
        ],
        "pos_line": [
            "+return SparseTensor(index, polar, torch.Size([n, n, 2]))"
        ],
        "core_change": "-# Modify data and return. -data.adj = SparseTensor(index, polar, torch.Size([n, n, 2])) -return data +return SparseTensor(index, polar, torch.Size([n, n, 2]))",
        "core_API": "stack"
    },
    {
        "commit_hash": "eb68afca0208a040d4e91eceae86f5f22ca24b04",
        "index": "9831efbd..432f81ea 100644",
        "commit_message": "fix a type mismatch in NAT quantization run\n\nSummary:\nFix a type mismatch which was found after patching NAT on top of quantization.\nNing suggested this fix. Need to further understand: why this only appears after patching quantization diff?\n\nReviewed By: kahne, jhcross\n\nDifferential Revision: D18147726\n\nfbshipit-source-id: a51becc9ad58a637a0180074eaa2b46990ab9f84\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def fill_tensors(x, mask, y, padding_idx: int):",
            "x = expand_2d_or_3d_tensor(x, y.size(1), padding_idx)",
            "x[mask] = y",
            "elif x.size(1) > y.size(1):",
            "-        x[mask] = torch.tensor(padding_idx)",
            "+        x[mask] = torch.tensor(padding_idx).type_as(x)",
            "if x.dim() == 2:",
            "x[mask, :y.size(1)] = y",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=217599)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=217600)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=217601)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type_as'), position=2, insert_id=217602)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=217603)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'x'), position=1, insert_id=217604)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=217605)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2443,
        "neg_line": [
            "-x[mask] = torch.tensor(padding_idx)"
        ],
        "pos_line": [
            "+x[mask] = torch.tensor(padding_idx).type_as(x)"
        ],
        "core_change": "-x[mask] = torch.tensor(padding_idx) +x[mask] = torch.tensor(padding_idx).type_as(x)",
        "core_API": "size"
    },
    {
        "commit_hash": "c67d1a0259cbb3aef31952b4f37d4fee0e36f134",
        "index": "9746fb008..ddb655656 100644",
        "commit_message": "Tf model outputs (#6247)\n\n* TF outputs and test on BERT\n\n* Albert to DistilBert\n\n* All remaining TF models except T5\n\n* Documentation\n\n* One file forgotten\n\n* TF outputs and test on BERT\n\n* Albert to DistilBert\n\n* All remaining TF models except T5\n\n* Documentation\n\n* One file forgotten\n\n* Add new models and fix issues\n\n* Quality improvements\n\n* Add T5\n\n* A bit of cleanup\n\n* Fix for slow tests\n\n* Style\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class XLNetModel(XLNetPreTrainedModel):",
            "# Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)",
            "output = output.permute(1, 0, 2).contiguous()",
            "",
            "-        # TODO Teven: fix this test to only use use_cache.",
            "if not use_cache:",
            "new_mems = None"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2447,
        "neg_line": [
            "-# TODO Teven: fix this test to only use use_cache."
        ],
        "pos_line": [],
        "core_change": "-# TODO Teven: fix this test to only use use_cache.",
        "core_API": "permute"
    },
    {
        "commit_hash": "85da4149f4100ca0aab1257311a277dcd2e11e9e",
        "index": "ce63231..c9a974d 100644",
        "commit_message": "[MRG]Fix win. Resolve Issue #300 (#304)\n\n* Change Process.Pool to subprocess to be compatible with google colab\n\n* Update test\n\n* Resolve conflict\n\n* Resolve one more conflict\n\n* Catch timeout exception from TimeError only\n\n* Fix merge error\n\n* Use spawn instead of fork when not in google colab, catch queue.Empty exception in searcher when timeout\n\n* Add unit test for get_device and get_system\n\n* Add missing function\n\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "from keras.datasets import mnist",
            "from autokeras import ImageClassifier",
            "-import tensorflow",
            "",
            "if __name__ == '__main__':",
            "-    print(tensorflow.__version__)",
            "(x_train, y_train), (x_test, y_test) = mnist.load_data()",
            "x_train = x_train.reshape(x_train.shape+(1,))",
            "x_test = x_test.reshape(x_test.shape+(1,))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('block', ''), position=3, insert_id=2396166)",
            "Delete(target_node=ASTNode(type=import, text=import))",
            "Delete(target_node=ASTNode(type=identifier, text=tensorflow))",
            "Delete(target_node=ASTNode(type=dotted_name))",
            "Delete(target_node=ASTNode(type=import_statement))",
            "Delete(target_node=ASTNode(type=identifier, text=print))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=tensorflow))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=__version__))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 2448,
        "neg_line": [
            "-import tensorflow",
            "-print(tensorflow.__version__)"
        ],
        "pos_line": [],
        "core_change": "-import tensorflow -print(tensorflow.__version__)",
        "core_API": "load_data"
    },
    {
        "commit_hash": "04e997fe0da22b8190da38e0d3224d9ccdf57218",
        "index": "fe3f40c86a..791cb487ef 100644",
        "commit_message": "Fix TF2 / rllib test (#5846)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class MyKerasRNN(RecurrentTFModelV2):",
            "shape=(None, obs_space.shape[0]), name=\"inputs\")",
            "state_in_h = tf.keras.layers.Input(shape=(cell_size, ), name=\"h\")",
            "state_in_c = tf.keras.layers.Input(shape=(cell_size, ), name=\"c\")",
            "-        seq_in = tf.keras.layers.Input(shape=(), name=\"seq_in\")",
            "+        seq_in = tf.keras.layers.Input(shape=(), name=\"seq_in\", dtype=tf.int32)",
            "",
            "# Preprocess observation with a hidden layer and send to LSTM cell",
            "dense1 = tf.keras.layers.Dense("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2620355)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2620356)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2620357)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2620358)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2620359)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2620360)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2620361)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int32'), position=2, insert_id=2620362)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2451,
        "neg_line": [
            "-seq_in = tf.keras.layers.Input(shape=(), name=\"seq_in\")"
        ],
        "pos_line": [
            "+seq_in = tf.keras.layers.Input(shape=(), name=\"seq_in\", dtype=tf.int32)"
        ],
        "core_change": "-seq_in = tf.keras.layers.Input(shape=(), name=\"seq_in\") +seq_in = tf.keras.layers.Input(shape=(), name=\"seq_in\", dtype=tf.int32)",
        "core_API": "Input"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "dc6472c1..a49f18ef 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestElmoLstmCell(AllenNlpTestCase):",
            "input_tensor[1, 4:, :] = 0.0",
            "input_tensor[2, 2:, :] = 0.0",
            "input_tensor[3, 1:, :] = 0.0",
            "-        mask = torch.ones([4, 5])",
            "-        mask[1, 4:] = 0.0",
            "-        mask[2, 2:] = 0.0",
            "-        mask[3, 1:] = 0.0",
            "+        mask = torch.ones([4, 5]).bool()",
            "+        mask[1, 4:] = False",
            "+        mask[2, 2:] = False",
            "+        mask[3, 1:] = False",
            "",
            "lstm = ElmoLstm(",
            "num_layers=2,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=19954)",
            "Insert(target_node=ASTNode(type=assignment), node=('false', 'False'), position=2, insert_id=19955)",
            "Insert(target_node=ASTNode(type=assignment), node=('false', 'False'), position=2, insert_id=19956)",
            "Insert(target_node=ASTNode(type=assignment), node=('false', 'False'), position=2, insert_id=19957)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=19958)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=19959)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=19960)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=19961)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=19962)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=19963)",
            "Delete(target_node=ASTNode(type=float, text=0.0))",
            "Delete(target_node=ASTNode(type=float, text=0.0))",
            "Delete(target_node=ASTNode(type=float, text=0.0))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 14,
        "number": 2453,
        "neg_line": [
            "-mask = torch.ones([4, 5])",
            "-mask[1, 4:] = 0.0",
            "-mask[2, 2:] = 0.0",
            "-mask[3, 1:] = 0.0"
        ],
        "pos_line": [
            "+mask = torch.ones([4, 5]).bool()",
            "+mask[1, 4:] = False",
            "+mask[2, 2:] = False",
            "+mask[3, 1:] = False"
        ],
        "core_change": "-mask = torch.ones([4, 5]) -mask[1, 4:] = 0.0 -mask[2, 2:] = 0.0 -mask[3, 1:] = 0.0 +mask = torch.ones([4, 5]).bool() +mask[1, 4:] = False +mask[2, 2:] = False +mask[3, 1:] = False",
        "core_API": "ones"
    },
    {
        "commit_hash": "165756cc19a19db0e75c128ccc97eff3579af1c3",
        "index": "a26e9cda..52b38b35 100644",
        "commit_message": "[Retiarii] Weight-sharing trainers (#3137)\n\n* First commit for WS in Retiarii\n\n* Refactor ENAS trainer\n\n* Fix DARTS trainer\n\n* Fix ENAS trainer\n\n* Fix issues in DARTS and Proxyless\n\n* Fix ProxylessNAS and Random trainer\n\n* Refactor mask\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ENASLayer(nn.Module):",
            "nn.init.kaiming_normal_(self.final_conv_w)",
            "",
            "def forward(self, pprev, prev):",
            "-        pprev_, prev_ = self.preproc0(pprev), self.preproc1(prev)",
            "+        pprev_, prev_ = self.preproc0(pprev), self.preproc1(prev)",
            "",
            "prev_nodes_out = [pprev_, prev_]",
            "nodes_used_mask = torch.zeros(self.num_nodes + 2, dtype=torch.bool, device=prev.device)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2455,
        "neg_line": [
            "-pprev_, prev_ = self.preproc0(pprev), self.preproc1(prev)"
        ],
        "pos_line": [
            "+pprev_, prev_ = self.preproc0(pprev), self.preproc1(prev)"
        ],
        "core_change": "-pprev_, prev_ = self.preproc0(pprev), self.preproc1(prev) +pprev_, prev_ = self.preproc0(pprev), self.preproc1(prev)",
        "core_API": "kaiming_normal_"
    },
    {
        "commit_hash": "f4a36ec97067b160948e650524a4c872f3b21fd5",
        "index": "a7baf3ea30..58520e499e 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def vector_norm(",
            "elif ord == 0:",
            "tn_normalized_vector = tf.reduce_sum(tf.cast(x != 0, x.dtype), axis, keepdims)",
            "else:",
            "-        tn_normalized_vector = tf.reduce_sum(tf.abs(x) ** ord, axis, keepdims) ** (1.0 / ord)",
            "+        tn_normalized_vector = tf.reduce_sum(tf.abs(x) ** ord, axis, keepdims) ** (",
            "+            1.0 / ord",
            "+        )",
            "return tn_normalized_vector"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2456,
        "neg_line": [
            "-tn_normalized_vector = tf.reduce_sum(tf.abs(x) ** ord, axis, keepdims) ** (1.0 / ord)"
        ],
        "pos_line": [
            "+tn_normalized_vector = tf.reduce_sum(tf.abs(x) ** ord, axis, keepdims) ** (",
            "+1.0 / ord",
            "+)"
        ],
        "core_change": "-tn_normalized_vector = tf.reduce_sum(tf.abs(x) ** ord, axis, keepdims) ** (1.0 / ord) +tn_normalized_vector = tf.reduce_sum(tf.abs(x) ** ord, axis, keepdims) ** ( +1.0 / ord +)",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "bc597e9a1c5d618e55a79bd04c05aed9f8767250",
        "index": "85317cf..067b09e 100644",
        "commit_message": "Add black and flake8 (#532)\n\n* Add black and flake8\n* Fix test losses\n* Fix pre-commit \n* Update README\n",
        "file": "segmentation_models.pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class SegmentationHead(nn.Sequential):",
            "",
            "",
            "class ClassificationHead(nn.Sequential):",
            "-",
            "def __init__(self, in_channels, classes, pooling=\"avg\", dropout=0.2, activation=None):",
            "if pooling not in (\"max\", \"avg\"):",
            "raise ValueError(\"Pooling should be one of ('max', 'avg'), got {}.\".format(pooling))",
            "-        pool = nn.AdaptiveAvgPool2d(1) if pooling == 'avg' else nn.AdaptiveMaxPool2d(1)",
            "+        pool = nn.AdaptiveAvgPool2d(1) if pooling == \"avg\" else nn.AdaptiveMaxPool2d(1)",
            "flatten = nn.Flatten()",
            "dropout = nn.Dropout(p=dropout, inplace=True) if dropout else nn.Identity()",
            "linear = nn.Linear(in_channels, classes, bias=True)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='avg'), value='\"avg\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2458,
        "neg_line": [
            "-",
            "-pool = nn.AdaptiveAvgPool2d(1) if pooling == 'avg' else nn.AdaptiveMaxPool2d(1)"
        ],
        "pos_line": [
            "+pool = nn.AdaptiveAvgPool2d(1) if pooling == \"avg\" else nn.AdaptiveMaxPool2d(1)"
        ],
        "core_change": "- -pool = nn.AdaptiveAvgPool2d(1) if pooling == 'avg' else nn.AdaptiveMaxPool2d(1) +pool = nn.AdaptiveAvgPool2d(1) if pooling == \"avg\" else nn.AdaptiveMaxPool2d(1)",
        "core_API": "AdaptiveAvgPool2d"
    },
    {
        "commit_hash": "5593b6f772c75ae016580e2579c2af9f86b19a39",
        "index": "e7da752d1..1d4510f4c 100644",
        "commit_message": "Merge pull request #7872 from PyTorchLightning/refactor/logger-poc-changes\n\nRandom fixes for logger connector PoC\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class ModelPruning(Callback):",
            "def _wrap_pruning_fn(pruning_fn: Callable, **kwargs: Any) -> Callable:",
            "return partial(pruning_fn, **kwargs)",
            "",
            "-    def make_pruning_permanent(self, pl_module: LightningModule) -> None:",
            "+    def make_pruning_permanent(self, module: nn.Module) -> None:",
            "\"\"\"",
            "Removes pruning buffers from any pruned modules",
            "",
            "Adapted from https://github.com/pytorch/pytorch/blob/1.7.1/torch/nn/utils/prune.py#L1176-L1180",
            "\"\"\"",
            "-        for _, module in pl_module.named_modules():",
            "+        for _, module in module.named_modules():",
            "for k in list(module._forward_pre_hooks):",
            "hook = module._forward_pre_hooks[k]",
            "if isinstance(hook, pytorch_prune.BasePruningMethod):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=pl_module), value='module')",
            "Update(target_node=ASTNode(type=identifier, text=pl_module), value='module')",
            "Insert(target_node=ASTNode(type=type), node=('attribute', None), position=0, insert_id=1412143)",
            "Update(target_node=ASTNode(type=identifier, text=LightningModule), value='nn')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=LightningModule), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1412144)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Module'), position=2, insert_id=1412145)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 2461,
        "neg_line": [
            "-def make_pruning_permanent(self, pl_module: LightningModule) -> None:",
            "-for _, module in pl_module.named_modules():"
        ],
        "pos_line": [
            "+def make_pruning_permanent(self, module: nn.Module) -> None:",
            "+for _, module in module.named_modules():"
        ],
        "core_change": "-def make_pruning_permanent(self, pl_module: LightningModule) -> None: +def make_pruning_permanent(self, module: nn.Module) -> None: -for _, module in pl_module.named_modules(): +for _, module in module.named_modules():",
        "core_API": "named_modules"
    },
    {
        "commit_hash": "6b423b675ddfc43385e3ec8f73965308e4f89191",
        "index": "dc8e3a2..fb95280 100644",
        "commit_message": "fix mulacc when both strides are 0\n\n",
        "file": "tinygrad.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "torch_fxn_for_op : Dict[Op, Callable] = {**base_fxn_for_op, **{",
            "MovementOps.PAD: lambda x, padding: torch.nn.functional.pad(x, [item for sublist in padding[::-1] for item in sublist]),",
            "MovementOps.STRIDED: lambda x, arg: x.contiguous().as_strided([y[0] for y in arg], [y[1] for y in arg]),",
            "ProcessingOps.CONV: lambda x,w,C: C.px == C.px_ and C.py == C.py_ and torch.conv2d(x, w, stride=(C.sy, C.sx), groups=C.groups, dilation=(C.dy, C.dx), padding=(C.py, C.px)),",
            "-  FusedOps.MULACC: einsum_mulacc(torch.einsum, lambda x: x.stride())",
            "+  FusedOps.MULACC: einsum_mulacc(torch.einsum, lambda x: x.stride(), lambda x,s: x.expand(s))",
            "}}",
            "",
            "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else (\"mps\" if getenv(\"MPS\", 0) else \"cpu\"))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1159327)",
            "Insert(target_node=ASTNode(type=argument_list), node=('lambda', None), position=5, insert_id=1159328)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=6, insert_id=1159329)",
            "Insert(target_node=IN(type=lambda), node=('lambda', 'lambda'), position=0, insert_id=1159330)",
            "Insert(target_node=IN(type=lambda), node=('lambda_parameters', None), position=1, insert_id=1159331)",
            "Insert(target_node=IN(type=lambda), node=(':', ':'), position=2, insert_id=1159332)",
            "Insert(target_node=IN(type=lambda), node=('call', None), position=3, insert_id=1159333)",
            "Insert(target_node=IN(type=lambda_parameters), node=('identifier', 'x'), position=0, insert_id=1159334)",
            "Insert(target_node=IN(type=lambda_parameters), node=(',', ','), position=1, insert_id=1159335)",
            "Insert(target_node=IN(type=lambda_parameters), node=('identifier', 's'), position=2, insert_id=1159336)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1159337)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1159338)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x'), position=0, insert_id=1159339)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1159340)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'expand'), position=2, insert_id=1159341)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1159342)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 's'), position=1, insert_id=1159343)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 2465,
        "neg_line": [
            "-FusedOps.MULACC: einsum_mulacc(torch.einsum, lambda x: x.stride())"
        ],
        "pos_line": [
            "+FusedOps.MULACC: einsum_mulacc(torch.einsum, lambda x: x.stride(), lambda x,s: x.expand(s))"
        ],
        "core_change": "-FusedOps.MULACC: einsum_mulacc(torch.einsum, lambda x: x.stride()) +FusedOps.MULACC: einsum_mulacc(torch.einsum, lambda x: x.stride(), lambda x,s: x.expand(s))",
        "core_API": "pad"
    },
    {
        "commit_hash": "cd4c9c1c3f4979dc8e08f8eaf3e563f97ec8c678",
        "index": "2f53d4b3..d7d7062b 100644",
        "commit_message": "fixed standardize preprocessor\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Standardize(Preprocessor):",
            "else:",
            "axes = tuple(range(1, util.rank(tensor)))",
            "",
            "-        mean, variance = tf.nn.moments(x=tensor, axes=axes)",
            "-        return (tensor - mean) / tf.maximum(x=variance, y=util.epsilon)",
            "+        mean, variance = tf.nn.moments(x=tensor, axes=axes, keep_dims=True)",
            "+        return (tensor - mean) / tf.maximum(x=tf.sqrt(variance), y=util.epsilon)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2237489)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2237490)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'keep_dims'), position=0, insert_id=2237491)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2237492)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=2237493)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('call', None), position=2, insert_id=2237494)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2237495)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2237496)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2237497)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2237498)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sqrt'), position=2, insert_id=2237499)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2237500)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=variance), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2237501)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 2466,
        "neg_line": [
            "-mean, variance = tf.nn.moments(x=tensor, axes=axes)",
            "-return (tensor - mean) / tf.maximum(x=variance, y=util.epsilon)"
        ],
        "pos_line": [
            "+mean, variance = tf.nn.moments(x=tensor, axes=axes, keep_dims=True)",
            "+return (tensor - mean) / tf.maximum(x=tf.sqrt(variance), y=util.epsilon)"
        ],
        "core_change": "-mean, variance = tf.nn.moments(x=tensor, axes=axes) -return (tensor - mean) / tf.maximum(x=variance, y=util.epsilon) +mean, variance = tf.nn.moments(x=tensor, axes=axes, keep_dims=True) +return (tensor - mean) / tf.maximum(x=tf.sqrt(variance), y=util.epsilon)",
        "core_API": "rank"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "f3ddd243..49678830 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Auc(Metric):",
            "",
            "if mask is None:",
            "batch_size = gold_labels.shape[0]",
            "-            mask = torch.ones(batch_size, device=gold_labels.device)",
            "-        mask = mask.to(dtype=torch.bool)",
            "+            mask = torch.ones(batch_size, device=gold_labels.device).bool()",
            "",
            "self._all_predictions = self._all_predictions.to(predictions.device)",
            "self._all_gold_labels = self._all_gold_labels.to(gold_labels.device)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=20176)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=20177)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=to), value='bool')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=to), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=mask))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=mask))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=bool))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 2468,
        "neg_line": [
            "-mask = torch.ones(batch_size, device=gold_labels.device)",
            "-mask = mask.to(dtype=torch.bool)"
        ],
        "pos_line": [
            "+mask = torch.ones(batch_size, device=gold_labels.device).bool()"
        ],
        "core_change": "-mask = torch.ones(batch_size, device=gold_labels.device) -mask = mask.to(dtype=torch.bool) +mask = torch.ones(batch_size, device=gold_labels.device).bool()",
        "core_API": "ones"
    },
    {
        "commit_hash": "96ae969993c6f476314c2bfc639a67f83fe97239",
        "index": "15c6fdd..4f2b885 100644",
        "commit_message": "Fix test.py and remove bbox iou compute for numpy arrays\n\n",
        "file": "PyTorch-YOLOv3.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ListDataset(Dataset):",
            "if np.random.random() < 0.5:",
            "img, labels = horisontal_flip(img, labels)",
            "",
            "-        boxes = torch.zeros((len(labels), 6))",
            "-        boxes[:, 1:] = labels",
            "+        # Add dummy label if there are none",
            "+        num_labels = 1 if labels is None else len(labels)",
            "+        boxes = torch.zeros((num_labels, 6))",
            "+        if labels is not None:",
            "+            boxes[:, 1:] = labels",
            "",
            "return img_path, img, boxes"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=906643)",
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=5, insert_id=906644)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=906645)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=906646)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=906647)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=906648)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=906649)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'num_labels'), position=0, insert_id=906650)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=906651)",
            "Insert(target_node=IN(type=assignment), node=('conditional_expression', None), position=2, insert_id=906652)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'labels'), position=0, insert_id=906653)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=906654)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=906655)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=906656)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=conditional_expression), node=('integer', '1'), position=0, insert_id=906657)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=906658)",
            "Insert(target_node=IN(type=conditional_expression), node=('comparison_operator', None), position=2, insert_id=906659)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=906660)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=call), position=4)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'labels'), position=0, insert_id=906661)",
            "Insert(target_node=IN(type=comparison_operator), node=('is', 'is'), position=1, insert_id=906662)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=2, insert_id=906663)",
            "Insert(target_node=ASTNode(type=tuple), node=('identifier', 'num_labels'), position=1, insert_id=906664)"
        ],
        "plus_line": 5,
        "minus_line": 2,
        "AST_diff_line": 24,
        "number": 2471,
        "neg_line": [
            "-boxes = torch.zeros((len(labels), 6))",
            "-boxes[:, 1:] = labels"
        ],
        "pos_line": [
            "+# Add dummy label if there are none",
            "+num_labels = 1 if labels is None else len(labels)",
            "+boxes = torch.zeros((num_labels, 6))",
            "+if labels is not None:",
            "+boxes[:, 1:] = labels"
        ],
        "core_change": "-boxes = torch.zeros((len(labels), 6)) -boxes[:, 1:] = labels +# Add dummy label if there are none +num_labels = 1 if labels is None else len(labels) +boxes = torch.zeros((num_labels, 6)) +if labels is not None: +boxes[:, 1:] = labels",
        "core_API": "random"
    },
    {
        "commit_hash": "3de066d408d811db29161d90c1c81c43d08f7efc",
        "index": "1fe587b4d..1ed975b6b 100644",
        "commit_message": "refactor: python api (#551)\n\n* refactor: remove unnecessary utility functions\n\n* fix: inherit RedirectedPrints from contextlib.redirect_stdout\n\n* refactor: CosineSimilarityClassifier uses Serializable.__init__()\n\n* style: add typing for download.py\n\n* refactor: deep_download accepts config as dict\n\n* fix: fix errors found with code inspection\n\n* refactor: move find_config function away to core.common.file\n\n* feat: allow to download config's file requirements with build_model_from_config function\n\n* refactor: rename build_model_from_config to build_model\n\n* feat: add a config tree object\n\n* refactor: rename Sturct's _to_dict method to _asdict\nto conform with namedtuple's naming\n\n* fix: correct root directory for configs to search\n\n* feat: allow to import configs Struct and build_model fn from deeppavlov\n\n* docs: update python usage examples in model's docs\n\n* fix: do not raise in deeppavlov.__init__ because of absent requirements\n\n* fix: resolve merge conflict of recursive imports\n\n* feat: add pretty representation of Struct objects in iPython\n\n* feat: add download parameter to train_evaluate_model_from_config\n\n* feat: import train_evaluate_model_from_config in __init__ of deeppavlov as train_model\n\n* feat: add `train_model` sugar function\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class KnowledgeBaseEntityNormalizer(Component):",
            "Example:",
            ".. code:: python",
            "",
            "-            >>> from models.seq2seq_go_bot.kb import KnowledgeBase",
            "+            >>> from deeppavlov.models.seq2seq_go_bot.kb import KnowledgeBase",
            ">>> kb = KnowledgeBase(save_path=\"kb.json\", load_path=\"kb.json\", tokenizer=lambda strings: [s.split() for s in strings])",
            ">>> kb.fit(['person1'], [['name', 'hair', 'eyes']], [[{'name': 'Sasha', 'hair': 'long   dark', 'eyes': 'light blue '}]])",
            ">>> kb(['person1'])",
            "[[('sasha_name', ['Sasha']), ('sasha_hair', ['long', 'dark']), ('sasha_eyes', ['light','blue'])]]",
            "",
            "-            >>> from models.seq2seq_go_bot.kb import KnowledgeBaseEntityNormalizer",
            "+            >>> from deeppavlov.models.seq2seq_go_bot.kb import KnowledgeBaseEntityNormalizer",
            ">>> normalizer = KnowledgeBaseEntityNormalizer(denormalize=False, remove=False)",
            ">>> normalizer([[\"some\", \"guy\", \"with\", \"long\", \"dark\", \"hair\", \"said\", \"hi\"]], kb(['person1']))",
            "[['some', 'guy', 'with', 'sasha_hair', 'hair', 'said', 'hi']]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=module), position=14)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=2, insert_id=2653870)",
            "Insert(target_node=IN(type=ERROR), node=('>>', '>>'), position=0, insert_id=2653871)",
            "Insert(target_node=IN(type=ERROR), node=('>', '>'), position=1, insert_id=2653872)",
            "Insert(target_node=ASTNode(type=dotted_name), node=('identifier', 'deeppavlov'), position=0, insert_id=2653873)",
            "Insert(target_node=ASTNode(type=dotted_name), node=('.', '.'), position=1, insert_id=2653874)",
            "Insert(target_node=ASTNode(type=dotted_name), node=('identifier', 'deeppavlov'), position=0, insert_id=2653875)",
            "Insert(target_node=ASTNode(type=dotted_name), node=('.', '.'), position=1, insert_id=2653876)",
            "Delete(target_node=ASTNode(type=>>, text=>>))",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 2474,
        "neg_line": [
            "->>> from models.seq2seq_go_bot.kb import KnowledgeBase",
            "->>> from models.seq2seq_go_bot.kb import KnowledgeBaseEntityNormalizer"
        ],
        "pos_line": [
            "+>>> from deeppavlov.models.seq2seq_go_bot.kb import KnowledgeBase",
            "+>>> from deeppavlov.models.seq2seq_go_bot.kb import KnowledgeBaseEntityNormalizer"
        ],
        "core_change": "->>> from models.seq2seq_go_bot.kb import KnowledgeBase +>>> from deeppavlov.models.seq2seq_go_bot.kb import KnowledgeBase ->>> from models.seq2seq_go_bot.kb import KnowledgeBaseEntityNormalizer +>>> from deeppavlov.models.seq2seq_go_bot.kb import KnowledgeBaseEntityNormalizer",
        "core_API": "split"
    },
    {
        "commit_hash": "15c72be444fab0d1ba6879d097399b12f6a2a8b0",
        "index": "4676aa5..a750ce6 100644",
        "commit_message": "Fix coordinate system conventions in renderer\n\nSummary:\n## Updates\n\n- Defined the world and camera coordinates according to this figure. The world coordinates are defined as having +Y up, +X left and +Z in.\n\n{F230888499}\n\n- Removed all flipping from blending functions.\n- Updated the rasterizer to return images with +Y up and +X left.\n- Updated all the mesh rasterizer tests\n    - The expected values are now defined in terms of the default +Y up, +X left\n    - Added tests where the triangles in the meshes are non symmetrical so that it is clear which direction +X and +Y are\n\n## Questions:\n- Should we have **scene settings** instead of raster settings?\n    - To be more correct we should be [z clipping in the rasterizer based on the far/near clipping planes](https://github.com/ShichenLiu/SoftRas/blob/master/soft_renderer/cuda/soft_rasterize_cuda_kernel.cu#L400) - these values are also required in the blending functions so should we make these scene level parameters and have a scene settings tuple which is available to the rasterizer and shader?\n\nReviewed By: gkioxari\n\nDifferential Revision: D20208604\n\nfbshipit-source-id: 55787301b1bffa0afa9618f0a0886cc681da51f3\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def softmax_rgb_blend(",
            "# Sum: weights * textures + background color",
            "weighted_colors = (weights[..., None] * colors).sum(dim=-2)",
            "weighted_background = (delta / denom) * background",
            "-    pix_colors[..., :3] = weighted_colors + weighted_background",
            "-    pix_colors[..., 3] = 1.0 - alpha",
            "+    pixel_colors[..., :3] = weighted_colors + weighted_background",
            "+    pixel_colors[..., 3] = 1.0 - alpha",
            "",
            "-    return torch.flip(pix_colors, [1])",
            "+    return pixel_colors"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'return'), position=4, insert_id=929070)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'pixel_colors'), position=5, insert_id=929071)",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=identifier, text=alpha), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=pix_colors), value='pixel_colors')",
            "Update(target_node=ASTNode(type=identifier, text=pix_colors), value='pixel_colors')",
            "Delete(target_node=ASTNode(type=identifier, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=flip))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=pix_colors))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 2475,
        "neg_line": [
            "-pix_colors[..., :3] = weighted_colors + weighted_background",
            "-pix_colors[..., 3] = 1.0 - alpha",
            "-return torch.flip(pix_colors, [1])"
        ],
        "pos_line": [
            "+pixel_colors[..., :3] = weighted_colors + weighted_background",
            "+pixel_colors[..., 3] = 1.0 - alpha",
            "+return pixel_colors"
        ],
        "core_change": "-pix_colors[..., :3] = weighted_colors + weighted_background -pix_colors[..., 3] = 1.0 - alpha +pixel_colors[..., :3] = weighted_colors + weighted_background +pixel_colors[..., 3] = 1.0 - alpha -return torch.flip(pix_colors, [1]) +return pixel_colors",
        "core_API": "flip"
    },
    {
        "commit_hash": "40c3ab6a68b56507c584b80522e7c86884255f13",
        "index": "d4f476a0..edba17d4 100644",
        "commit_message": "AccumGrad supports sparse update (fix #435)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class AccumGradOptimizer(ProxyOptimizer):",
            "grads_and_vars = FilterNoneGrad().process(grads_and_vars)",
            "vs = []",
            "for g, v in grads_and_vars:",
            "-            assert isinstance(g, tf.Tensor) and isinstance(v, tf.Variable), \\",
            "-                \"AccumGradOptimizer only works for dense update! \" \\",
            "-                \"Types of v and g are {} and {}\".format(type(v), type(g))",
            "+            assert isinstance(g, (tf.Tensor, tf.IndexedSlices)) and isinstance(v, tf.Variable), \\",
            "+                \"AccumGradOptimizer does not work for the gradient of {}! \" \\",
            "+                \"Types of v and g are {} and {}\".format(v.op.name, type(v), type(g))",
            "vs.append(v)",
            "",
            "with tf.control_dependencies(None):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=2274947)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2274948)",
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=3, insert_id=2274949)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=2274950)",
            "Update(target_node=ASTNode(type=string, text=\"AccumGradOptimizer only works for dense update! \"), value='\"AccumGradOptimizer does not work for the gradient of {}! \"')",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2274951)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2274952)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'name'), position=2, insert_id=2274953)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2274954)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2274955)",
            "Insert(target_node=IN(type=tuple), node=('attribute', None), position=3, insert_id=2274956)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=), text=)), position=4)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'v'), position=0, insert_id=2274957)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2274958)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'op'), position=2, insert_id=2274959)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2274960)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2274961)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'IndexedSlices'), position=2, insert_id=2274962)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 19,
        "number": 2477,
        "neg_line": [
            "-assert isinstance(g, tf.Tensor) and isinstance(v, tf.Variable), \\",
            "-\"AccumGradOptimizer only works for dense update! \" \\",
            "-\"Types of v and g are {} and {}\".format(type(v), type(g))"
        ],
        "pos_line": [
            "+assert isinstance(g, (tf.Tensor, tf.IndexedSlices)) and isinstance(v, tf.Variable), \\",
            "+\"AccumGradOptimizer does not work for the gradient of {}! \" \\",
            "+\"Types of v and g are {} and {}\".format(v.op.name, type(v), type(g))"
        ],
        "core_change": "-assert isinstance(g, tf.Tensor) and isinstance(v, tf.Variable), \\ -\"AccumGradOptimizer only works for dense update! \" \\ -\"Types of v and g are {} and {}\".format(type(v), type(g)) +assert isinstance(g, (tf.Tensor, tf.IndexedSlices)) and isinstance(v, tf.Variable), \\ +\"AccumGradOptimizer does not work for the gradient of {}! \" \\ +\"Types of v and g are {} and {}\".format(v.op.name, type(v), type(g))",
        "core_API": "append"
    },
    {
        "commit_hash": "c2b5b57dfd81e0668d900fab339b334eb6e45579",
        "index": "8a1485fb..f8102ecb 100644",
        "commit_message": "Bugfix and enhancements\n\n",
        "file": "stanza.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class RNNTokenizer(nn.Module):",
            "",
            "pred1 = self.dense_clf2(inp2)",
            "",
            "-        pred = torch.cat([pred0[:,:,:1], pred0[:,:,2].unsqueeze(2) + pred1, pred0[:,:,3].unsqueeze(2)], 2)",
            "+        pred = torch.cat([pred0[:,:,:2], pred0[:,:,2].unsqueeze(2) + pred1, pred0[:,:,3].unsqueeze(2)], 2)",
            "",
            "return pred, []"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=1), value='2')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2480,
        "neg_line": [
            "-pred = torch.cat([pred0[:,:,:1], pred0[:,:,2].unsqueeze(2) + pred1, pred0[:,:,3].unsqueeze(2)], 2)"
        ],
        "pos_line": [
            "+pred = torch.cat([pred0[:,:,:2], pred0[:,:,2].unsqueeze(2) + pred1, pred0[:,:,3].unsqueeze(2)], 2)"
        ],
        "core_change": "-pred = torch.cat([pred0[:,:,:1], pred0[:,:,2].unsqueeze(2) + pred1, pred0[:,:,3].unsqueeze(2)], 2) +pred = torch.cat([pred0[:,:,:2], pred0[:,:,2].unsqueeze(2) + pred1, pred0[:,:,3].unsqueeze(2)], 2)",
        "core_API": "dense_clf2"
    },
    {
        "commit_hash": "aa756cec4359aff3df1d9abb68dc6e6e92920e0c",
        "index": "b60b84d..d772fa8 100644",
        "commit_message": "minimal bert pipeline parallel test (#1216)\n\n* minimal bert pipeline parallel test\n\n* fix global and cleanup\n\n* use get_forward_backward_func\n\n* cleanup and fix some tests\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def forward_backward_func_template(",
            "assert isinstance(model, list)",
            "assert len(model) == (1 if virtual_pipeline_model_parallel_size is None else virtual_pipeline_model_parallel_size)",
            "_param_groups = _get_params_for_weight_decay_optimization(model)",
            "-    torch.optim.Adam(_param_groups)",
            "+    torch.optim.Adam(_param_groups, lr=1e-4)",
            "",
            "tensor_shape = [batch_size // parallel_state.get_data_parallel_world_size(), hidden_size]",
            "batch = (torch.randn(tensor_shape).cuda(),)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=50940)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=50941)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'lr'), position=0, insert_id=50942)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=50943)",
            "Insert(target_node=IN(type=keyword_argument), node=('float', '1e-4'), position=2, insert_id=50944)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2482,
        "neg_line": [
            "-torch.optim.Adam(_param_groups)"
        ],
        "pos_line": [
            "+torch.optim.Adam(_param_groups, lr=1e-4)"
        ],
        "core_change": "-torch.optim.Adam(_param_groups) +torch.optim.Adam(_param_groups, lr=1e-4)",
        "core_API": "Adam"
    },
    {
        "commit_hash": "0c166dcfb8b9e9d79d013b7967a59dfb1ac4b804",
        "index": "50f11135..039ea0ff 100644",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Laplacian(nn.Module):",
            "torch.Size([2, 4, 5, 5])",
            "\"\"\"",
            "",
            "-    def __init__(self,",
            "-                 kernel_size: int, border_type: str = 'reflect',",
            "-                 normalized: bool = True) -> None:",
            "+    def __init__(self, kernel_size: int, border_type: str = 'reflect', normalized: bool = True) -> None:",
            "super(Laplacian, self).__init__()",
            "self.kernel_size: int = kernel_size",
            "self.border_type: str = border_type"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 2483,
        "neg_line": [
            "-def __init__(self,",
            "-kernel_size: int, border_type: str = 'reflect',",
            "-normalized: bool = True) -> None:"
        ],
        "pos_line": [
            "+def __init__(self, kernel_size: int, border_type: str = 'reflect', normalized: bool = True) -> None:"
        ],
        "core_change": "-def __init__(self, -kernel_size: int, border_type: str = 'reflect', -normalized: bool = True) -> None: +def __init__(self, kernel_size: int, border_type: str = 'reflect', normalized: bool = True) -> None:",
        "core_API": "Size"
    },
    {
        "commit_hash": "0f0c6288c35a456707057b0aab7754fcb5ac4ffd",
        "index": "4864447d..09ce5cd9 100644",
        "commit_message": "[retiarii] fix test (#3197)\n\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "from nni.retiarii.trainer import PyTorchImageClassificationTrainer, PyTorchMulti",
            "from nni.retiarii.utils import import_",
            "",
            "def _load_mnist(n_models: int = 1):",
            "-    with open('converted_mnist_pytorch.json') as f:",
            "+    path = Path(__file__).parent / 'converted_mnist_pytorch.json'",
            "+    with open(path) as f:",
            "mnist_model = Model._load(json.load(f))",
            "if n_models == 1:",
            "return mnist_model"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=665404)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=665405)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'path'), position=0, insert_id=665406)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=665407)",
            "Insert(target_node=IN(type=assignment), node=('binary_operator', None), position=2, insert_id=665408)",
            "Insert(target_node=IN(type=binary_operator), node=('attribute', None), position=0, insert_id=665409)",
            "Insert(target_node=IN(type=binary_operator), node=('/', '/'), position=1, insert_id=665410)",
            "Insert(target_node=IN(type=binary_operator), node=('string', \"'converted_mnist_pytorch.json'\"), position=2, insert_id=665411)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=665412)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=665413)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'parent'), position=2, insert_id=665414)",
            "Insert(target_node=IN(type=call), node=('identifier', 'Path'), position=0, insert_id=665415)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=665416)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=665417)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', '__file__'), position=1, insert_id=665418)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=665419)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'path'), position=1, insert_id=665420)",
            "Delete(target_node=ASTNode(type=string, text='converted_mnist_pytorch.json'))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 2485,
        "neg_line": [
            "-with open('converted_mnist_pytorch.json') as f:"
        ],
        "pos_line": [
            "+path = Path(__file__).parent / 'converted_mnist_pytorch.json'",
            "+with open(path) as f:"
        ],
        "core_change": "-with open('converted_mnist_pytorch.json') as f: +path = Path(__file__).parent / 'converted_mnist_pytorch.json' +with open(path) as f:",
        "core_API": "_load"
    },
    {
        "commit_hash": "bad83008377bf01a34ac2e08c74e7da89eaf4e07",
        "index": "2d0adcddf..315206a19 100755",
        "commit_message": "Error (also in original) model, scaling only q matrix not qk.T dot product (qk.T/sqrt(dim_per_head)) (#21627)\n\n* Error in model, scaling only q matrix not qK.T dot product (qk.T/sqrt(dim_per_head))\n\nAs per Vaswani et al, 2017 p.4\nIs torch.matmul(q, k.transpose(2, 3)) / math.sqrt(dim_per_head) not q / math.sqrt(dim_per_head)\nhttps://arxiv.org/pdf/1912.05372.pdf\n\nError was in original FlauBERT repo and effectively scales queries but not values\ncf. https://github.com/getalp/Flaubert/pull/45/commits/6d176880ca3a1a8dfa2b76c97030bb51c5e917b8\n\n* Update modeling_flaubert.py\n\nUpdate to https://github.com/huggingface/transformers/pull/21627\nmake fixup\nmake repo_consistency\n\n* Update modeling_xlm.py\n\n* Update modeling_flaubert.py\n\n* Update modeling_xlm.py\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MultiHeadAttention(nn.Module):",
            "k, v = cache[self.layer_id]",
            "cache[self.layer_id] = (k, v)",
            "",
            "-        q = q / math.sqrt(dim_per_head)  # (bs, n_heads, qlen, dim_per_head)",
            "-        scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, qlen, klen)",
            "+        scores = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(dim_per_head)  # (bs, n_heads, qlen, klen)",
            "mask = (mask == 0).view(mask_reshape).expand_as(scores)  # (bs, n_heads, qlen, klen)",
            "scores.masked_fill_(mask, torch.finfo(scores.dtype).min)  # (bs, n_heads, qlen, klen)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('binary_operator', None), position=2, insert_id=1176353)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('/', '/'), position=1, insert_id=1176354)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=q))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=q))",
            "Delete(target_node=ASTNode(type=/, text=/))",
            "Delete(target_node=ASTNode(type=binary_operator))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 2486,
        "neg_line": [
            "-q = q / math.sqrt(dim_per_head)  # (bs, n_heads, qlen, dim_per_head)",
            "-scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, qlen, klen)"
        ],
        "pos_line": [
            "+scores = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(dim_per_head)  # (bs, n_heads, qlen, klen)"
        ],
        "core_change": "-q = q / math.sqrt(dim_per_head)  # (bs, n_heads, qlen, dim_per_head) -scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, qlen, klen) +scores = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(dim_per_head)  # (bs, n_heads, qlen, klen)",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "4ad77e2b2da57a167ce71391392caf36ff4e7e8e",
        "index": "d88b48c4..eec1fd1c 100644",
        "commit_message": "fixing rtd\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SeparableConv2dLayer(Layer):# Untested",
            "strides=strides, padding=padding, data_format=data_format,",
            "dilation_rate=dilation_rate, depth_multiplier=depth_multiplier, activation=act,",
            "use_bias=use_bias, depthwise_initializer=depthwise_initializer, pointwise_initializer=pointwise_initializer,",
            "-                 bias_initializer=tf.zeros_initializer(), depthwise_regularizer=None,",
            "+                 bias_initializer=bias_initializer, depthwise_regularizer=depthwise_regularizer,",
            "pointwise_regularizer=pointwise_regularizer, bias_regularizer=bias_regularizer, activity_regularizer=activity_regularizer,)",
            "#trainable=True, name=None, reuse=None)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('assignment', None), position=2, insert_id=2268298)",
            "Insert(target_node=IN(type=assignment), node=('pattern_list', None), position=0, insert_id=2268299)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type==, text==), position=1)",
            "Insert(target_node=IN(type=assignment), node=('expression_list', None), position=2, insert_id=2268300)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='bias_initializer')",
            "Move(target_node=IN(type=pattern_list), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=pattern_list), node=ASTNode(type=,, text=,), position=1)",
            "Move(target_node=IN(type=pattern_list), node=ASTNode(type=identifier, text=depthwise_regularizer), position=2)",
            "Insert(target_node=IN(type=expression_list), node=('identifier', 'depthwise_regularizer'), position=0, insert_id=2268301)",
            "Move(target_node=IN(type=expression_list), node=ASTNode(type=,, text=,), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=zeros_initializer))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=expression_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 2487,
        "neg_line": [
            "-bias_initializer=tf.zeros_initializer(), depthwise_regularizer=None,"
        ],
        "pos_line": [
            "+bias_initializer=bias_initializer, depthwise_regularizer=depthwise_regularizer,"
        ],
        "core_change": "-bias_initializer=tf.zeros_initializer(), depthwise_regularizer=None, +bias_initializer=bias_initializer, depthwise_regularizer=depthwise_regularizer,",
        "core_API": "zeros_initializer"
    },
    {
        "commit_hash": "df6110954405744639627f4ffb5dd27b4523239b",
        "index": "24227ed2..f3dc0c51 100644",
        "commit_message": "`MultiAggregation` and `aggregation_resolver` (#4749)\n\n* Add MulAggregation and MultiAggregation\n\n* Fix import issue\n\n* Support torch_geometric.nn.aggr package, note: jit errors to fix\n\n* Add tests for MulAggregation, MultiAggregation, aggregation_resolver and message_passing interface\n\n* Formatting\n\n* Fix __repr for gen aggrs\n\n* Move resolver\n\n* Fix test for MulAggregation\n\n* Add test for new mp interface\n\n* Add test for MultiAggregation\n\n* Minor fix\n\n* Add warming for MulAggregation with 'ptr'\n\n* Resolve aggr to Aggregation module, remove aggrs logic\n\n* changelog\n\n* Fix mul aggregation\n\n* update\n\n* update\n\n* update\n\n* update\n\n* reset\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_gen_aggregation(Aggregation, learn):",
            "ptr = torch.tensor([0, 2, 5, 6])",
            "",
            "aggr = Aggregation(learn=learn)",
            "-    assert str(aggr) == f'{Aggregation.__name__}()'",
            "+    assert str(aggr) == f'{Aggregation.__name__}(learn={learn})'",
            "",
            "out = aggr(x, index)",
            "assert out.size() == (3, x.size(1))"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=f'{Aggregation.__name__}()'), value=\"f'{Aggregation.__name__}(learn={learn})'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2488,
        "neg_line": [
            "-assert str(aggr) == f'{Aggregation.__name__}()'"
        ],
        "pos_line": [
            "+assert str(aggr) == f'{Aggregation.__name__}(learn={learn})'"
        ],
        "core_change": "-assert str(aggr) == f'{Aggregation.__name__}()' +assert str(aggr) == f'{Aggregation.__name__}(learn={learn})'",
        "core_API": "tensor"
    },
    {
        "commit_hash": "d70e5ec69de7d2a8d5bd81e1524b87c668d868d0",
        "index": "109c26b4..ee301345 100644",
        "commit_message": "[Feat] Add tpu-testing in circleci (#787)\n\n* add needed files for tpu-testing in circleci\n\n* adapt script for pytorch nghtlies\n\n* fix shape test\n\n* fix color tests\n\n* add pytorch version to the dockerfile\n\n* fix xla precision and disable torch-xla[1.6,1.7]\n\n* adjust rgb2luv precision\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestBgrToGrayscale(BaseTester):",
            "], device=device, dtype=dtype)",
            "",
            "# Output data generated with OpenCV 4.1.1: cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)",
            "-        expected = torch.tensor([",
            "+        expected = torch.tensor([[",
            "[0.4485849, 0.8233618, 0.6262833, 0.6218331, 0.6341921],",
            "[0.3200093, 0.4340172, 0.7107211, 0.5454938, 0.2801398],",
            "[0.6149265, 0.7018101, 0.3503231, 0.4891168, 0.5292346],",
            "[0.5096100, 0.4336508, 0.6704276, 0.4525143, 0.2134447],",
            "[0.7878902, 0.6494595, 0.5211386, 0.6623823, 0.6660464],",
            "-        ], device=device, dtype=dtype)",
            "+        ]], device=device, dtype=dtype)",
            "",
            "img_gray = kornia.bgr_to_grayscale(data)",
            "assert_allclose(img_gray, expected)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('[', '['), position=0, insert_id=433564)",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=list), position=1)",
            "Insert(target_node=ASTNode(type=list), node=(']', ']'), position=2, insert_id=433565)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 2489,
        "neg_line": [
            "-expected = torch.tensor([",
            "-], device=device, dtype=dtype)"
        ],
        "pos_line": [
            "+expected = torch.tensor([[",
            "+]], device=device, dtype=dtype)"
        ],
        "core_change": "-expected = torch.tensor([ +expected = torch.tensor([[ -], device=device, dtype=dtype) +]], device=device, dtype=dtype)",
        "core_API": "cvtColor"
    },
    {
        "commit_hash": "994be163e1ce916b9594e5227fc6551e015b152f",
        "index": "3b745018..273c62a5 100644",
        "commit_message": "Use packaging.version for version comparisons (#2310)\n\n* Use packaging.version for version comparisons\n\nThe distutils package is deprecated and relies on PEP 386 version\ncomparisons, which have been superseded by PEP 440 which is implemented\nthrough the packaging module.\n\nWith more recent distutils versions, provided through setuptools\nvendoring, we are seeing the following exception during version\ncomparisons:\n\n> TypeError: '<' not supported between instances of 'str' and 'int'\n\nThis is fixed by this migration.\n\n[1] https://docs.python.org/3/library/distutils.html\n[2] https://peps.python.org/pep-0386/\n[3] https://peps.python.org/pep-0440/\n\n* Improve espeak version detection robustness\n\nOn many modern systems espeak is just a symlink to espeak-ng. In that\ncase looking for the 3rd word in the version output will break the\nversion comparison, when it finds `text-to-speech:`, instead of a proper\nversion.\n\nThis will not break during runtime, where espeak-ng would be\nprioritized, but the phonemizer and tokenizer tests force the backend\nto `espeak`, which exhibits this breakage.\n\nThis improves the version detection by simply looking for the version\nafter the \"text-to-speech:\" token.\n\n* Replace distuils.copy_tree with shutil.copytree\n\nThe distutils module is deprecated and slated for removal in Python\n3.12. Its usage should be replaced, in this case by a compatible method\nfrom shutil.\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class InvConvNear(nn.Module):",
            "self.no_jacobian = no_jacobian",
            "self.weight_inv = None",
            "",
            "-        if LooseVersion(torch.__version__) < LooseVersion(\"1.9\"):",
            "+        if Version(torch.__version__) < Version(\"1.9\"):",
            "w_init = torch.qr(torch.FloatTensor(self.num_splits, self.num_splits).normal_())[0]",
            "else:",
            "w_init = torch.linalg.qr(torch.FloatTensor(self.num_splits, self.num_splits).normal_(), \"complete\")[0]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=LooseVersion), value='Version')",
            "Update(target_node=ASTNode(type=identifier, text=LooseVersion), value='Version')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 2490,
        "neg_line": [
            "-if LooseVersion(torch.__version__) < LooseVersion(\"1.9\"):"
        ],
        "pos_line": [
            "+if Version(torch.__version__) < Version(\"1.9\"):"
        ],
        "core_change": "-if LooseVersion(torch.__version__) < LooseVersion(\"1.9\"): +if Version(torch.__version__) < Version(\"1.9\"):",
        "core_API": "qr"
    },
    {
        "commit_hash": "ea61b6d4a647862b270eb9d97ab940257cd8b3ec",
        "index": "2a2ff9b0..2660ad43 100644",
        "commit_message": "add from homogeneous zero grad test and fix it\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def convert_points_from_homogeneous(",
            "# set the results of division by zeror/near-zero to 1.0",
            "# follow the convention of opencv:",
            "# https://github.com/opencv/opencv/pull/14411/files",
            "+    mask_valid_points = torch.abs(z_vec) > eps",
            "scale: torch.Tensor = torch.where(",
            "-        torch.abs(z_vec) > eps,",
            "-        torch.tensor(1.) / z_vec,",
            "+        mask_valid_points,",
            "+        torch.tensor(1.) / z_vec.masked_fill(~mask_valid_points, eps),",
            "torch.ones_like(z_vec))",
            "",
            "return scale * points[..., :-1]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=typed_default_parameter), node=('identifier', 'mask_valid_points'), position=0, insert_id=450110)",
            "Insert(target_node=ASTNode(type=typed_default_parameter), node=('ERROR', None), position=1, insert_id=450111)",
            "Insert(target_node=IN(type=ERROR), node=('=', '='), position=0, insert_id=450112)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=comparison_operator), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=scale), position=2)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'mask_valid_points'), position=1, insert_id=450113)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=450114)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=450115)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=450116)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=450117)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=z_vec), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=450118)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'masked_fill'), position=2, insert_id=450119)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=450120)",
            "Insert(target_node=IN(type=argument_list), node=('unary_operator', None), position=1, insert_id=450121)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=450122)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'eps'), position=3, insert_id=450123)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=450124)",
            "Insert(target_node=IN(type=unary_operator), node=('~', '~'), position=0, insert_id=450125)",
            "Insert(target_node=IN(type=unary_operator), node=('identifier', 'mask_valid_points'), position=1, insert_id=450126)",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 21,
        "number": 2491,
        "neg_line": [
            "-torch.abs(z_vec) > eps,",
            "-torch.tensor(1.) / z_vec,"
        ],
        "pos_line": [
            "+mask_valid_points = torch.abs(z_vec) > eps",
            "+mask_valid_points,",
            "+torch.tensor(1.) / z_vec.masked_fill(~mask_valid_points, eps),"
        ],
        "core_change": "+mask_valid_points = torch.abs(z_vec) > eps -torch.abs(z_vec) > eps, -torch.tensor(1.) / z_vec, +mask_valid_points, +torch.tensor(1.) / z_vec.masked_fill(~mask_valid_points, eps),",
        "core_API": "abs"
    },
    {
        "commit_hash": "4e9b453854c9ff92fc4a7ad990b7995279313fda",
        "index": "22a18bf33..c883ff504 100644",
        "commit_message": "[Fix] Move init dist connection into the setup function (#6506)\n\n* Move connection setup into the setup function. Call setup hook after we set up the accelerator\n\n* Added CHANGELOG.md\n\n* fix setup order in callback test\n\n* fix input arguments in test\n\n* Mock distributed function, remove protection to turn into training type hook\n\n* Remove import\n\n* Add missing mock, ensure custom plugin does not create children process\n\n* Skip test on windows\n\n* Update deepspeed to init connection in setup\n\n* Do not initialize distributed module\n\n* Move DeepSpeed tests to special tests since dist communication is being set up\n\n* Special the test to see if this fixes CI\n\n* Delete accelerator connector test to see if its causing build to fail\n\n* Delete deepspeed test\n\n* Revert \"Delete accelerator connector test to see if its causing build to fail\"\n\nThis reverts commit edde60b8\n\n* Revert \"Delete deepspeed test\"\n\nThis reverts commit 9d317429\n\n* Reverse hook\n\n* Reverse setup hooks to debug again\n\n* Add todo so i know where i left off\n\n* For single device move in pre_dispatch after setup function\n\n* Add additional model to device hook if any additional parameters have been set\n\n* See if we can enable deepspeed tests\n\n* Revert \"See if we can enable deepspeed tests\"\n\nThis reverts commit b5450def\n\n* See if this hook approach works\n\n* Introduce new granular hooks\n\n* Remove import, fix tpu spawn by moving the function to setup\n\n* Added missing special test\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TPUSpawnPlugin(DDPSpawnPlugin):",
            "self.tpu_local_core_rank = 0",
            "self.start_method = None",
            "",
            "-    def connect(self, model: torch.nn.Module) -> torch.nn.Module:",
            "+    def setup(self, model: torch.nn.Module) -> torch.nn.Module:",
            "self.create_mp_queue()",
            "-        self._model = model",
            "-        return self._model",
            "+        return self.model",
            "",
            "def create_mp_queue(self):",
            "self.start_method = 'fork'"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=connect), value='setup')",
            "Update(target_node=ASTNode(type=identifier, text=_model), value='model')",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=_model))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=model))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 10,
        "number": 2492,
        "neg_line": [
            "-def connect(self, model: torch.nn.Module) -> torch.nn.Module:",
            "-self._model = model",
            "-return self._model"
        ],
        "pos_line": [
            "+def setup(self, model: torch.nn.Module) -> torch.nn.Module:",
            "+return self.model"
        ],
        "core_change": "-def connect(self, model: torch.nn.Module) -> torch.nn.Module: +def setup(self, model: torch.nn.Module) -> torch.nn.Module: -self._model = model -return self._model +return self.model",
        "core_API": "create_mp_queue"
    },
    {
        "commit_hash": "2e7e4280aa6f380a4e3afad6524295a17901c56c",
        "index": "db58113d9..f57e3e74e 100755",
        "commit_message": "Traced models serialization and torchscripting fix (#17206)\n\n* Fix torch.jit.script and pickling issues\n\n* Fix get_attr issues\n\n* Fix import in function\n\n* Fix GPT-J and T5 tracing for torch=1.11\n\n* Gate graph surgery on torch version\n\n* Modeling minor changes to enable TorchScripting\n\n* Model serialization / deserialization test\n\n* Remove _assert_is_none users\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GPTJForSequenceClassification(GPTJPreTrainedModel):",
            "\"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"",
            ")",
            "",
            "-        pooled_logits = logits[torch.arange(batch_size, device=self.device), sequence_lengths]",
            "+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]",
            "",
            "loss = None",
            "if labels is not None:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=self), value='logits')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2499,
        "neg_line": [
            "-pooled_logits = logits[torch.arange(batch_size, device=self.device), sequence_lengths]"
        ],
        "pos_line": [
            "+pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]"
        ],
        "core_change": "-pooled_logits = logits[torch.arange(batch_size, device=self.device), sequence_lengths] +pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]",
        "core_API": "arange"
    },
    {
        "commit_hash": "a79a9e1241b9257bbce1d933fee47f3a8ff90797",
        "index": "6dc03af4e..0a1e68878 100644",
        "commit_message": "Fix TFAlbertForSequenceClassification classifier dropout probability. It was set to config.hidden_dropout_prob, but should be config.classifier_dropout_prob. (#3928)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class TFAlbertForSequenceClassification(TFAlbertPreTrainedModel):",
            "self.num_labels = config.num_labels",
            "",
            "self.albert = TFAlbertMainLayer(config, name=\"albert\")",
            "-        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)",
            "+        self.dropout = tf.keras.layers.Dropout(config.classifier_dropout_prob)",
            "self.classifier = tf.keras.layers.Dense(",
            "config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"classifier\"",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=hidden_dropout_prob), value='classifier_dropout_prob')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2500,
        "neg_line": [
            "-self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)"
        ],
        "pos_line": [
            "+self.dropout = tf.keras.layers.Dropout(config.classifier_dropout_prob)"
        ],
        "core_change": "-self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob) +self.dropout = tf.keras.layers.Dropout(config.classifier_dropout_prob)",
        "core_API": "Dropout"
    },
    {
        "commit_hash": "dfe166e3d08f9ca9851d2e9cd4336addc801c01a",
        "index": "68dcf05d..f450047e 100644",
        "commit_message": "Fix pruner issues (#2265)\n\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AGP_Pruner(Pruner):",
            "if epoch > 0:",
            "self.now_epoch = epoch",
            "for wrapper in self.get_modules_wrapper():",
            "-                wrapper.if_calculated.copy_(torch.tensor(0)) # pylint: disable=not-callable",
            "+                wrapper.if_calculated = False",
            "",
            "class SlimPruner(Pruner):",
            "\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=666641)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=666642)",
            "Insert(target_node=IN(type=assignment), node=('false', 'False'), position=2, insert_id=666643)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=copy_))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 2502,
        "neg_line": [
            "-wrapper.if_calculated.copy_(torch.tensor(0)) # pylint: disable=not-callable"
        ],
        "pos_line": [
            "+wrapper.if_calculated = False"
        ],
        "core_change": "-wrapper.if_calculated.copy_(torch.tensor(0)) # pylint: disable=not-callable +wrapper.if_calculated = False",
        "core_API": "get_modules_wrapper"
    },
    {
        "commit_hash": "0ae96ff8a7e2d371242452d81bee85da8df202f5",
        "index": "3ee9bdd84..6d3e4f97b 100644",
        "commit_message": "BIG Reorganize examples  (#4213)\n\n* Created using Colaboratory\n\n* [examples] reorganize files\n\n* remove run_tpu_glue.py as superseded by TPU support in Trainer\n\n* Bugfix: int, not tuple\n\n* move files around\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer:",
            "total_train_batch_size = (",
            "self.args.train_batch_size",
            "* self.args.gradient_accumulation_steps",
            "-                * (torch.distributed.get_world_size() if self.args.local_rank != -1 else 1),",
            "+                * (torch.distributed.get_world_size() if self.args.local_rank != -1 else 1)",
            ")",
            "logger.info(\"***** Running training *****\")",
            "logger.info(\"  Num examples = %d\", num_examples)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('parenthesized_expression', None), position=2, insert_id=1239268)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=(, text=(), position=0)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=binary_operator), position=1)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=tuple))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 2508,
        "neg_line": [
            "-* (torch.distributed.get_world_size() if self.args.local_rank != -1 else 1),"
        ],
        "pos_line": [
            "+* (torch.distributed.get_world_size() if self.args.local_rank != -1 else 1)"
        ],
        "core_change": "-* (torch.distributed.get_world_size() if self.args.local_rank != -1 else 1), +* (torch.distributed.get_world_size() if self.args.local_rank != -1 else 1)",
        "core_API": "get_world_size"
    },
    {
        "commit_hash": "da3aceb876e0f92b65405a20198836c08ee80d10",
        "index": "dc644e88..f01f075a 100644",
        "commit_message": "Add fp16 support (#963)\n\n* Update imgwarp.py\n\n* update fix\n\n* move _torch_inverse_cast under utils\n\n* fix some tests in pyramid\n\n* fix issues\n\n* implement other autocasting functions\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_perspective_transform(src, dst):",
            "], dim=1)",
            "",
            "# solve the system Ax = b",
            "-    X, LU = torch.solve(b, A)",
            "+    X, LU = _torch_solve_cast(b, A)",
            "",
            "# create variable to return",
            "batch_size = src.shape[0]"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='_torch_solve_cast')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=solve))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2511,
        "neg_line": [
            "-X, LU = torch.solve(b, A)"
        ],
        "pos_line": [
            "+X, LU = _torch_solve_cast(b, A)"
        ],
        "core_change": "-X, LU = torch.solve(b, A) +X, LU = _torch_solve_cast(b, A)",
        "core_API": "solve"
    },
    {
        "commit_hash": "3facd518628a7225a7d97131748996ea8e9c6e70",
        "index": "8b10de17..f61f3183 100644",
        "commit_message": "some small fix\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class CheckGradient(MapGradient):",
            "super(CheckGradient, self).__init__(self._mapper)",
            "",
            "def _mapper(self, grad, var):",
            "-        # this is very slow...",
            "+        # this is very slow.... see #3649",
            "#op = tf.Assert(tf.reduce_all(tf.is_finite(var)), [var], summarize=100)",
            "-        grad = tf.check_numerics(grad, 'CheckGradient')",
            "+        grad = tf.check_numerics(grad, 'CheckGradient-' + var.op.name)",
            "return grad",
            "",
            "class ScaleGradient(MapGradient):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=3, insert_id=2310337)",
            "Update(target_node=ASTNode(type=string, text='CheckGradient'), value=\"'CheckGradient-'\")",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=string, text='CheckGradient'), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=2310338)",
            "Insert(target_node=IN(type=binary_operator), node=('attribute', None), position=2, insert_id=2310339)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2310340)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2310341)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'name'), position=2, insert_id=2310342)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'var'), position=0, insert_id=2310343)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2310344)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'op'), position=2, insert_id=2310345)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 2514,
        "neg_line": [
            "-# this is very slow...",
            "-grad = tf.check_numerics(grad, 'CheckGradient')"
        ],
        "pos_line": [
            "+# this is very slow.... see #3649",
            "+grad = tf.check_numerics(grad, 'CheckGradient-' + var.op.name)"
        ],
        "core_change": "-# this is very slow... +# this is very slow.... see #3649 -grad = tf.check_numerics(grad, 'CheckGradient') +grad = tf.check_numerics(grad, 'CheckGradient-' + var.op.name)",
        "core_API": "Assert"
    },
    {
        "commit_hash": "0acc9da0116914b8bf86d9649c2682ef4f3c2e67",
        "index": "0ef45fd21..26280ceeb 100644",
        "commit_message": "fix: remove gpu requirements from ner and squad (#471)\n\n* fix: remove gpu requirements from ner and squad\n\n* fix: usage of seq_lengths in gru and lstm layers\n\n* fix: remove empty sequences from squad model\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def cudnn_compatible_lstm(units, n_hidden, n_layers=1, trainable_initial_states=",
            "",
            "# Extract last states if they are provided",
            "if seq_lengths is not None:",
            "-                indices = tf.stack([tf.range(tf.shape(h)[0]), seq_lengths], axis=1)",
            "+                indices = tf.stack([tf.range(tf.shape(h)[0]), seq_lengths-1], axis=1)",
            "h_last = tf.gather_nd(h, indices)",
            "",
            "return h, (h_last, c_last)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('binary_operator', None), position=3, insert_id=1923008)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=seq_lengths), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=1923009)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=2, insert_id=1923010)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 2519,
        "neg_line": [
            "-indices = tf.stack([tf.range(tf.shape(h)[0]), seq_lengths], axis=1)"
        ],
        "pos_line": [
            "+indices = tf.stack([tf.range(tf.shape(h)[0]), seq_lengths-1], axis=1)"
        ],
        "core_change": "-indices = tf.stack([tf.range(tf.shape(h)[0]), seq_lengths], axis=1) +indices = tf.stack([tf.range(tf.shape(h)[0]), seq_lengths-1], axis=1)",
        "core_API": "stack"
    },
    {
        "commit_hash": "4728dd0c077d42c900396f5d1b946c412aa929b7",
        "index": "aa80aee7..cecb676c 100644",
        "commit_message": "Fix typo\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class GeniePathLazy(torch.nn.Module):",
            "h = torch.zeros(1, x.shape[0], lstm_hidden).to(self.device)",
            "c = torch.zeros(1, x.shape[0], lstm_hidden).to(self.device)",
            "h_tmps = []",
            "-        for i, l in enumerate(self.breaths):",
            "-            h_tmps.append(self.breaths[i](x, edge_index))",
            "+        for i, l in enumerate(self.breadths):",
            "+            h_tmps.append(self.breadths[i](x, edge_index))",
            "x = x[None, :]",
            "for i, l in enumerate(self.depths):",
            "in_cat = torch.cat((h_tmps[i][None, :], x), -1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=breaths), value='breadths')",
            "Update(target_node=ASTNode(type=identifier, text=breaths), value='breadths')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 2520,
        "neg_line": [
            "-for i, l in enumerate(self.breaths):",
            "-h_tmps.append(self.breaths[i](x, edge_index))"
        ],
        "pos_line": [
            "+for i, l in enumerate(self.breadths):",
            "+h_tmps.append(self.breadths[i](x, edge_index))"
        ],
        "core_change": "-for i, l in enumerate(self.breaths): -h_tmps.append(self.breaths[i](x, edge_index)) +for i, l in enumerate(self.breadths): +h_tmps.append(self.breadths[i](x, edge_index))",
        "core_API": "zeros"
    },
    {
        "commit_hash": "7732d0fe7a759c9844215920e9f1c5540eafb1a6",
        "index": "3f42d74ca..56083bfd2 100755",
        "commit_message": "Upgrade black to version ~=22.0 (#15565)\n\n* Upgrade black to version ~=22.0\n\n* Check copies\n\n* Fix code\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LSHSelfAttention(nn.Module, EfficientAttentionMixin):",
            "\"\"\"",
            "length normalization",
            "\"\"\"",
            "-        variance = torch.mean(x ** 2, -1, keepdim=True)",
            "+        variance = torch.mean(x**2, -1, keepdim=True)",
            "norm_x = x * torch.rsqrt(variance + epsilon)",
            "return norm_x"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2521,
        "neg_line": [
            "-variance = torch.mean(x ** 2, -1, keepdim=True)"
        ],
        "pos_line": [
            "+variance = torch.mean(x**2, -1, keepdim=True)"
        ],
        "core_change": "-variance = torch.mean(x ** 2, -1, keepdim=True) +variance = torch.mean(x**2, -1, keepdim=True)",
        "core_API": "mean"
    },
    {
        "commit_hash": "337d3e7cc9f73404365e20ef83fe631ad141511f",
        "index": "1b7effe5..64a17316 100644",
        "commit_message": "fixed agents horizon estimation\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Estimator(CircularBuffer):",
            "x=tf.zeros_like(tensor=discounts, dtype=util.tf_dtype(dtype='float')),",
            "y=discounts",
            ")",
            "-            reward = reward + discounts * horizon_estimate",
            "+            reward = reward + discounts * tf.stop_gradient(input=horizon_estimate)",
            "# TODO: stop gradients?",
            "",
            "return reward"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=2229368)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2229369)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2229370)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2229371)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2229372)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'stop_gradient'), position=2, insert_id=2229373)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2229374)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=2229375)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2229376)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'input'), position=0, insert_id=2229377)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2229378)",
            "Move(target_node=IN(type=keyword_argument), node=ASTNode(type=identifier, text=horizon_estimate), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 2523,
        "neg_line": [
            "-reward = reward + discounts * horizon_estimate"
        ],
        "pos_line": [
            "+reward = reward + discounts * tf.stop_gradient(input=horizon_estimate)"
        ],
        "core_change": "-reward = reward + discounts * horizon_estimate +reward = reward + discounts * tf.stop_gradient(input=horizon_estimate)",
        "core_API": "zeros_like"
    },
    {
        "commit_hash": "fb22bc5ae3bed199e59586e8c6056788841feb50",
        "index": "cd69ecdc3b..e974fd9fbf 100644",
        "commit_message": "[AIR] Fix bug where `TensorflowPredictor.predict` creates extra axis (#25199)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class TensorflowPredictor(Predictor):",
            "if feature_columns:",
            "data = data[feature_columns]",
            "data = data.values",
            "-        else:",
            "-            data = data[:, feature_columns]",
            "",
            "tensor = tf.convert_to_tensor(data, dtype=dtype)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=else))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=data))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=data))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=slice))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=feature_columns))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 15,
        "number": 2525,
        "neg_line": [
            "-else:",
            "-data = data[:, feature_columns]"
        ],
        "pos_line": [],
        "core_change": "-else: -data = data[:, feature_columns]",
        "core_API": "convert_to_tensor"
    },
    {
        "commit_hash": "7afe79c71357cec91b4c12a08608e8c74aed7eaa",
        "index": "55ad8f0968..a9115cd75c 100644",
        "commit_message": "`set` submodule pytorch tests fix (#2408)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def unique_values(",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "ret = tf.unique(tf.reshape(x, [-1]))[0]",
            "-    return ret",
            "+    return tf.sort(ret)"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('call', None), position=1, insert_id=2004130)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2004131)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2004132)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2004133)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2004134)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sort'), position=2, insert_id=2004135)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2004136)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=ret), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2004137)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 2526,
        "neg_line": [
            "-return ret"
        ],
        "pos_line": [
            "+return tf.sort(ret)"
        ],
        "core_change": "-return ret +return tf.sort(ret)",
        "core_API": "unique"
    },
    {
        "commit_hash": "58f4620699ac8e8eecbc0746dfcc8a497f01d6ba",
        "index": "7cee0e2e..c301548e 100644",
        "commit_message": "ReLU6 rework (#687)\n\n* leaky_relu deprecated and leaky_relu6 and PReLU6Layer added\n\n* cleaning\n\n* doc corrections\n\n* YAPF fix\n\n* leaky_twice_relu6 function added\n\n* recenter initializer\n\n* YAPF correction\n\n* Update test_activations.py\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class ZeroPad2d(Layer):",
            "if not isinstance(padding, (int, tuple)):",
            "raise AssertionError(\"Padding should be of type `int` or `tuple`\")",
            "",
            "-        self.outputs = tf.keras.layers.ZeroPadding2D(padding=padding, name=name)(self.inputs)",
            "+        self.outputs = tf.keras.layers.ZeroPadding2D(padding=padding, name=name)(self.inputs)  # TODO: Stop using Keras",
            "+",
            "self._add_layers(self.outputs)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2527,
        "neg_line": [
            "-self.outputs = tf.keras.layers.ZeroPadding2D(padding=padding, name=name)(self.inputs)"
        ],
        "pos_line": [
            "+self.outputs = tf.keras.layers.ZeroPadding2D(padding=padding, name=name)(self.inputs)  # TODO: Stop using Keras",
            "+"
        ],
        "core_change": "-self.outputs = tf.keras.layers.ZeroPadding2D(padding=padding, name=name)(self.inputs) +self.outputs = tf.keras.layers.ZeroPadding2D(padding=padding, name=name)(self.inputs)  # TODO: Stop using Keras +",
        "core_API": "ZeroPadding2D"
    },
    {
        "commit_hash": "2d5a7f5e7dc686cfc8172101a81505bf421468af",
        "index": "47fe8f622..59cb8c3e6 100644",
        "commit_message": "Fixes #3276 (#4116)\n\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Result(Dict):",
            "else:",
            "tbptt_reduce_fx = meta[k]['tbptt_reduce_fx']",
            "",
            "+            if isinstance(value, list):",
            "+                value = torch.tensor(value)",
            "+",
            "if isinstance(value, dict):",
            "# TODO: recursive reduce:",
            "_recursive_fx_apply(value, tbptt_reduce_fx)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=2, insert_id=560644)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=560645)",
            "Insert(target_node=IN(type=if_statement), node=('call', None), position=1, insert_id=560646)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=560647)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=560648)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=560649)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=560650)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=560651)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=560652)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'value'), position=1, insert_id=560653)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=560654)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'list'), position=3, insert_id=560655)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=560656)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=560657)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'value'), position=0, insert_id=560658)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=560659)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=560660)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=560661)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=560662)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=560663)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=560664)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=560665)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=560666)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'value'), position=1, insert_id=560667)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=560668)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 25,
        "number": 2530,
        "neg_line": [],
        "pos_line": [
            "+if isinstance(value, list):",
            "+value = torch.tensor(value)",
            "+"
        ],
        "core_change": "+if isinstance(value, list): +value = torch.tensor(value) +",
        "core_API": "tensor"
    },
    {
        "commit_hash": "d0b43268a0b496192331b1e12529d1afb92dd1a7",
        "index": "628e7f97..0e3b66f5 100644",
        "commit_message": "[Feat] Enabled doctest for CI (#641)\n\n* Added doctest for CI and validated docs\n\n* Fixed doctests\n\n* Fixed flake8\n\n* test doctest ci\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def motion_blur(",
            ">>> torch.allclose(out_1[0], out_1[1])",
            "True",
            ">>> # perform element-wise motion blur accross the batch",
            "-        >>> out_1 = motion_blur(input, 5, torch.tensor([90., 180,]), torch.tensor([1, -1]))",
            "+        >>> out_1 = motion_blur(input, 5, torch.tensor([90., 180,]), torch.tensor([1., -1.]))",
            ">>> torch.allclose(out_1[0], out_1[1])",
            "False",
            "\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('float', '1.'), position=1, insert_id=433655)",
            "Insert(target_node=ASTNode(type=list), node=('unary_operator', None), position=4, insert_id=433656)",
            "Insert(target_node=IN(type=unary_operator), node=('-', '-'), position=0, insert_id=433657)",
            "Insert(target_node=IN(type=unary_operator), node=('float', '1.'), position=1, insert_id=433658)",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 2532,
        "neg_line": [
            "->>> out_1 = motion_blur(input, 5, torch.tensor([90., 180,]), torch.tensor([1, -1]))"
        ],
        "pos_line": [
            "+>>> out_1 = motion_blur(input, 5, torch.tensor([90., 180,]), torch.tensor([1., -1.]))"
        ],
        "core_change": "->>> out_1 = motion_blur(input, 5, torch.tensor([90., 180,]), torch.tensor([1, -1])) +>>> out_1 = motion_blur(input, 5, torch.tensor([90., 180,]), torch.tensor([1., -1.]))",
        "core_API": "allclose"
    },
    {
        "commit_hash": "fc64559c4583db4e38ce50a976c8d935b124cf67",
        "index": "dc20cf74b..033e100d7 100644",
        "commit_message": "Fix TF CTRL model naming (#6134)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class TFEncoderLayer(tf.keras.layers.Layer):",
            "super().__init__(**kwargs)",
            "",
            "self.multi_head_attention = TFMultiHeadAttention(d_model_size, num_heads, name=\"multi_head_attention\")",
            "-        self.ffn = point_wise_feed_forward_network(d_model_size, dff, name=\"ffn\")",
            "+        self.ffn = TFPointWiseFeedForwardLayer(d_model_size, dff, name=\"ffn\")",
            "",
            "self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name=\"layernorm1\")",
            "self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name=\"layernorm2\")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=point_wise_feed_forward_network), value='TFPointWiseFeedForwardLayer')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2533,
        "neg_line": [
            "-self.ffn = point_wise_feed_forward_network(d_model_size, dff, name=\"ffn\")"
        ],
        "pos_line": [
            "+self.ffn = TFPointWiseFeedForwardLayer(d_model_size, dff, name=\"ffn\")"
        ],
        "core_change": "-self.ffn = point_wise_feed_forward_network(d_model_size, dff, name=\"ffn\") +self.ffn = TFPointWiseFeedForwardLayer(d_model_size, dff, name=\"ffn\")",
        "core_API": "LayerNormalization"
    },
    {
        "commit_hash": "e94abf66783f5e24ab37adc754ac902e827a4b4d",
        "index": "de41826d..029cd443 100755",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "if nrpool != 0:  # pool + passthrough if nrpool == 0",
            "x4 = Conv2D('poolproj', x4, nrpool, 1)",
            "outs.append(x4)",
            "-                return tf.concat(3, outs, name='concat')",
            "+                return tf.concat_v2(outs, 3, name='concat')",
            "",
            "with argscope(Conv2D, nl=BNReLU, use_bias=False):",
            "l = Conv2D('conv0', image, 64, 7, stride=2)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=concat), value='concat_v2')",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'outs'), position=1, insert_id=2308272)",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '3'), position=4, insert_id=2308273)",
            "Delete(target_node=ASTNode(type=integer, text=3))",
            "Delete(target_node=ASTNode(type=identifier, text=outs))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2536,
        "neg_line": [
            "-return tf.concat(3, outs, name='concat')"
        ],
        "pos_line": [
            "+return tf.concat_v2(outs, 3, name='concat')"
        ],
        "core_change": "-return tf.concat(3, outs, name='concat') +return tf.concat_v2(outs, 3, name='concat')",
        "core_API": "append"
    },
    {
        "commit_hash": "885f67941461df341ad154f5418dfc32d8a8146f",
        "index": "460dd0d..3c6ecb5 100644",
        "commit_message": "fix bugs and succeed on batch training\n\n",
        "file": "faster-rcnn.pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class _RPN(nn.Module):",
            "rpn_label_tmp = torch.index_select(rpn_label[i], 0, rpn_keep)",
            "rpn_label_v = Variable(rpn_label_tmp.long())",
            "",
            "-                fg_cnt = torch.sum(rpn_label_v.data.ne(0))",
            "+                fg_cnt += torch.sum(rpn_label_v.data.ne(0))",
            "",
            "self.rpn_loss_cls += F.cross_entropy(rpn_cls_score_single, rpn_label_v)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('augmented_assignment', None), position=0, insert_id=229413)",
            "Move(target_node=IN(type=augmented_assignment), node=ASTNode(type=identifier, text=fg_cnt), position=0)",
            "Insert(target_node=IN(type=augmented_assignment), node=('+=', '+='), position=1, insert_id=229414)",
            "Move(target_node=IN(type=augmented_assignment), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 2540,
        "neg_line": [
            "-fg_cnt = torch.sum(rpn_label_v.data.ne(0))"
        ],
        "pos_line": [
            "+fg_cnt += torch.sum(rpn_label_v.data.ne(0))"
        ],
        "core_change": "-fg_cnt = torch.sum(rpn_label_v.data.ne(0)) +fg_cnt += torch.sum(rpn_label_v.data.ne(0))",
        "core_API": "index_select"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "609a1647..1327d5f0 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestStackedBidirectionalLstm:",
            ")",
            "encoder = Seq2VecEncoder.from_params(params)",
            "input_tensor = torch.rand(4, 5, 3)",
            "-        mask = torch.ones(4, 5)",
            "+        mask = torch.ones(4, 5).bool()",
            "output = encoder(input_tensor, mask)",
            "assert output.detach().numpy().shape == (4, 18)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=19946)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=19947)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=19948)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=19949)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=19950)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=19951)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 2541,
        "neg_line": [
            "-mask = torch.ones(4, 5)"
        ],
        "pos_line": [
            "+mask = torch.ones(4, 5).bool()"
        ],
        "core_change": "-mask = torch.ones(4, 5) +mask = torch.ones(4, 5).bool()",
        "core_API": "from_params"
    },
    {
        "commit_hash": "15e89ef0f6f22f823c19592a401b9e4ee477258c",
        "index": "88c94e54..a6ee577c 100644",
        "commit_message": "fix for unet hijack breaking the train tab\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "th = TorchHijackForUnet()",
            "",
            "# Below are monkey patches to enable upcasting a float16 UNet for float32 sampling",
            "def apply_model(orig_func, self, x_noisy, t, cond, **kwargs):",
            "-    for y in cond.keys():",
            "-        cond[y] = [x.to(devices.dtype_unet) if isinstance(x, torch.Tensor) else x for x in cond[y]]",
            "+",
            "+    if isinstance(cond, dict):",
            "+        for y in cond.keys():",
            "+            cond[y] = [x.to(devices.dtype_unet) if isinstance(x, torch.Tensor) else x for x in cond[y]]",
            "+",
            "with devices.autocast():",
            "return orig_func(self, x_noisy.to(devices.dtype_unet), t.to(devices.dtype_unet), cond, **kwargs).float()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('if_statement', None), position=0, insert_id=1132847)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1132848)",
            "Insert(target_node=IN(type=if_statement), node=('call', None), position=1, insert_id=1132849)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1132850)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=block), position=3)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=1132851)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1132852)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1132853)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'cond'), position=1, insert_id=1132854)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1132855)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'dict'), position=3, insert_id=1132856)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1132857)"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 2542,
        "neg_line": [
            "-for y in cond.keys():",
            "-cond[y] = [x.to(devices.dtype_unet) if isinstance(x, torch.Tensor) else x for x in cond[y]]"
        ],
        "pos_line": [
            "+",
            "+if isinstance(cond, dict):",
            "+for y in cond.keys():",
            "+cond[y] = [x.to(devices.dtype_unet) if isinstance(x, torch.Tensor) else x for x in cond[y]]",
            "+"
        ],
        "core_change": "-for y in cond.keys(): -cond[y] = [x.to(devices.dtype_unet) if isinstance(x, torch.Tensor) else x for x in cond[y]] + +if isinstance(cond, dict): +for y in cond.keys(): +cond[y] = [x.to(devices.dtype_unet) if isinstance(x, torch.Tensor) else x for x in cond[y]] +",
        "core_API": "keys"
    },
    {
        "commit_hash": "56db5630e7ec32de85fde7be2ba48d71c51ec3b9",
        "index": "d41bd0e..edbe972 100644",
        "commit_message": "fix assert bug\n\n",
        "file": "pytorch-CycleGAN-and-pix2pix.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "def define_D(input_nc, ndf, which_model_netD,",
            "n_layers_D=3, use_sigmoid=False, gpu_ids=[]):",
            "netD = None",
            "use_gpu = len(gpu_ids) > 0",
            "-    assert(torch.cuda.is_available() == use_gpu)",
            "+    if use_gpu:",
            "+        assert(torch.cuda.is_available())",
            "+",
            "if which_model_netD == 'basic':",
            "netD = define_D(input_nc, ndf, 'n_layers', use_sigmoid=use_sigmoid, gpu_ids=gpu_ids)",
            "elif which_model_netD == 'n_layers':"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=1863720)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1863721)",
            "Insert(target_node=IN(type=if_statement), node=('identifier', 'use_gpu'), position=1, insert_id=1863722)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1863723)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1863724)",
            "Move(target_node=IN(type=block), node=ASTNode(type=assert_statement), position=0)",
            "Move(target_node=ASTNode(type=parenthesized_expression), node=ASTNode(type=call), position=1)",
            "Delete(target_node=ASTNode(type===, text===))",
            "Delete(target_node=ASTNode(type=identifier, text=use_gpu))",
            "Delete(target_node=ASTNode(type=comparison_operator))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 2543,
        "neg_line": [
            "-assert(torch.cuda.is_available() == use_gpu)"
        ],
        "pos_line": [
            "+if use_gpu:",
            "+assert(torch.cuda.is_available())",
            "+"
        ],
        "core_change": "-assert(torch.cuda.is_available() == use_gpu) +if use_gpu: +assert(torch.cuda.is_available()) +",
        "core_API": "is_available"
    },
    {
        "commit_hash": "ab0614e445934a6a3329fd27672bb09e800aee38",
        "index": "eeab878675..22d7fa90c2 100644",
        "commit_message": "small fix to torch linspace.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def linspace(start, stop, num, axis=None, dev_str=None):",
            "res = [linspace_method(start, stp, num, device=str_to_dev(dev_str)) for stp in stop]",
            "else:",
            "return linspace_method(start, stop, num, device=str_to_dev(dev_str))",
            "-    res = _torch.cat(res, -1).reshape(start_shape + [num])",
            "+    res = _torch.cat(res, -1).reshape(sos_shape + [num])",
            "if axis is not None:",
            "res = _torch.transpose(res, axis, -1)",
            "return res.to(str_to_dev(dev_str))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=start_shape), value='sos_shape')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2552,
        "neg_line": [
            "-res = _torch.cat(res, -1).reshape(start_shape + [num])"
        ],
        "pos_line": [
            "+res = _torch.cat(res, -1).reshape(sos_shape + [num])"
        ],
        "core_change": "-res = _torch.cat(res, -1).reshape(start_shape + [num]) +res = _torch.cat(res, -1).reshape(sos_shape + [num])",
        "core_API": "cat"
    },
    {
        "commit_hash": "0b54d9fb2e42c2f40db3449ca34586952b8abe94",
        "index": "4e8f32c1..ebed9c90 100644",
        "commit_message": "fix formatting (#3350)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \n\nX-link: https://github.com/fairinternal/fairseq-py/pull/3350\n\nReviewed By: shruti-bh\n\nDifferential Revision: D36009526\n\nPulled By: dianaml0\n\nfbshipit-source-id: 9cdc3d53086b8d40a780bcb64cfe28108091ab98\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_add_bias_parity():",
            "k = torch.rand((seq_len, bsz, embedding))",
            "v = torch.rand((seq_len, bsz, embedding))",
            "",
            "-    k_orig, v_orig, kp_mask_orig, a_mask_orig = old_bias_code(k, v, key_padding_mask, attn_mask, bsz)",
            "-    k_new, v_new, kp_mask_new, a_mask_new = mha._add_bias(k, v, key_padding_mask, attn_mask, bsz)",
            "+    k_orig, v_orig, kp_mask_orig, a_mask_orig = old_bias_code(",
            "+        k, v, key_padding_mask, attn_mask, bsz",
            "+    )",
            "+    k_new, v_new, kp_mask_new, a_mask_new = mha._add_bias(",
            "+        k, v, key_padding_mask, attn_mask, bsz",
            "+    )",
            "",
            "assert torch.equal(k_orig, k_new)",
            "assert torch.equal(v_orig, v_new)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 6,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 2555,
        "neg_line": [
            "-k_orig, v_orig, kp_mask_orig, a_mask_orig = old_bias_code(k, v, key_padding_mask, attn_mask, bsz)",
            "-k_new, v_new, kp_mask_new, a_mask_new = mha._add_bias(k, v, key_padding_mask, attn_mask, bsz)"
        ],
        "pos_line": [
            "+k_orig, v_orig, kp_mask_orig, a_mask_orig = old_bias_code(",
            "+k, v, key_padding_mask, attn_mask, bsz",
            "+)",
            "+k_new, v_new, kp_mask_new, a_mask_new = mha._add_bias(",
            "+k, v, key_padding_mask, attn_mask, bsz",
            "+)"
        ],
        "core_change": "-k_orig, v_orig, kp_mask_orig, a_mask_orig = old_bias_code(k, v, key_padding_mask, attn_mask, bsz) -k_new, v_new, kp_mask_new, a_mask_new = mha._add_bias(k, v, key_padding_mask, attn_mask, bsz) +k_orig, v_orig, kp_mask_orig, a_mask_orig = old_bias_code( +k, v, key_padding_mask, attn_mask, bsz +) +k_new, v_new, kp_mask_new, a_mask_new = mha._add_bias( +k, v, key_padding_mask, attn_mask, bsz +)",
        "core_API": "rand"
    },
    {
        "commit_hash": "a5538af3558cf544dffd92b1b8bab3a5793f0ba0",
        "index": "f2b591cba..1739133ed 100644",
        "commit_message": "fix dtype/device property not getting updated in submodules (#2657)\n\n* recursive dtype device apply\n\n* simplify\n\n* simple test\n\n* submodule test\n\n* rename\n\n* explicit\n\n* type hints\n\n* test for dp backend\n\n* fix test skip\n\n* rename\n\n* add ddp_spawn test\n\n* fix None index in test\n\n* try fix ddp_spawn test\n\n* changelog\n\n* move _dtype and _device to mixin\n\n* additional doctest\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LightningModule(ABC, DeviceDtypeModuleMixin, GradInformation, ModelIO, Mod",
            "#: True if using amp",
            "self.use_amp = False",
            "",
            "-        #: Current dtype",
            "-        self._dtype = torch.float",
            "-",
            "-        #: device reference",
            "-        self._device = torch.device('cpu')",
            "-",
            "# optionally can be set by user",
            "self._example_input_array = None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=_dtype))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=_device))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text='cpu'))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 0,
        "minus_line": 4,
        "AST_diff_line": 25,
        "number": 2558,
        "neg_line": [
            "-#: Current dtype",
            "-self._dtype = torch.float",
            "-",
            "-#: device reference",
            "-self._device = torch.device('cpu')",
            "-"
        ],
        "pos_line": [],
        "core_change": "-#: Current dtype -self._dtype = torch.float - -#: device reference -self._device = torch.device('cpu') -",
        "core_API": "device"
    },
    {
        "commit_hash": "7d300ea5f450fd2c4dd572004d5be5d3eafd6670",
        "index": "03315451..8387a671 100755",
        "commit_message": "fix cifar example\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def get_data(train_or_test):",
            "imgaug.CenterPaste((40, 40)),",
            "imgaug.RandomCrop((32, 32)),",
            "imgaug.Flip(horiz=True),",
            "-            #imgaug.Brightness(20),",
            "-            #imgaug.Contrast((0.6,1.4)),",
            "imgaug.MapImage(lambda x: x - pp_mean),",
            "]",
            "else:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 2561,
        "neg_line": [
            "-#imgaug.Brightness(20),",
            "-#imgaug.Contrast((0.6,1.4)),"
        ],
        "pos_line": [],
        "core_change": "-#imgaug.Brightness(20), -#imgaug.Contrast((0.6,1.4)),",
        "core_API": "CenterPaste"
    },
    {
        "commit_hash": "795cbf9ec222433828aff4efe98391be6aec0cd2",
        "index": "ddf758db..1373858c 100644",
        "commit_message": "fixed nonzero deprecation warnings\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RENet(torch.nn.Module):",
            "_, perm = logits.sort(dim=1, descending=True)",
            "mask = (y.view(-1, 1) == perm)",
            "",
            "-        mrr = (1 / (mask.nonzero()[:, -1] + 1).to(torch.float)).mean().item()",
            "+        nnz = mask.nonzero(as_tuple=False)",
            "+        mrr = (1 / (nnz[:, -1] + 1).to(torch.float)).mean().item()",
            "hits1 = mask[:, :1].sum().item() / y.size(0)",
            "hits3 = mask[:, :3].sum().item() / y.size(0)",
            "hits10 = mask[:, :10].sum().item() / y.size(0)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1013331)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1013332)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'nnz'), position=0, insert_id=1013333)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1013334)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1013335)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1013336)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1013337)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=1013338)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1013339)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'as_tuple'), position=0, insert_id=1013340)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1013341)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=1013342)",
            "Insert(target_node=ASTNode(type=subscript), node=('identifier', 'nnz'), position=0, insert_id=1013343)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 2562,
        "neg_line": [
            "-mrr = (1 / (mask.nonzero()[:, -1] + 1).to(torch.float)).mean().item()"
        ],
        "pos_line": [
            "+nnz = mask.nonzero(as_tuple=False)",
            "+mrr = (1 / (nnz[:, -1] + 1).to(torch.float)).mean().item()"
        ],
        "core_change": "-mrr = (1 / (mask.nonzero()[:, -1] + 1).to(torch.float)).mean().item() +nnz = mask.nonzero(as_tuple=False) +mrr = (1 / (nnz[:, -1] + 1).to(torch.float)).mean().item()",
        "core_API": "sort"
    },
    {
        "commit_hash": "6643ade2f7ebd3c91ac0228d69867a48d4a83e40",
        "index": "fa007529..1fb463e8 100644",
        "commit_message": "More cuda fixes (#379)\n\nDefault use_cuda=torch.Tensor.is_cuda in irange and friends\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GaussianChainTests(TestCase):",
            "(self.N, reparameterized, n_repa_nodes, self.N))",
            "if self.N < 0:",
            "def array_to_string(y):",
            "-                    return str(map(lambda x: \"%.3f\" % x.data.numpy()[0], y))",
            "+                    return str(map(lambda x: \"%.3f\" % x.data.cpu().numpy()[0], y))",
            "",
            "print(\"lambdas: \" + array_to_string(self.lambdas))",
            "print(\"target_mus: \" + array_to_string(self.target_mus[1:]))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=760589)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=760590)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=760591)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=760592)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cpu'), position=2, insert_id=760593)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=760594)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=760595)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 2563,
        "neg_line": [
            "-return str(map(lambda x: \"%.3f\" % x.data.numpy()[0], y))"
        ],
        "pos_line": [
            "+return str(map(lambda x: \"%.3f\" % x.data.cpu().numpy()[0], y))"
        ],
        "core_change": "-return str(map(lambda x: \"%.3f\" % x.data.numpy()[0], y)) +return str(map(lambda x: \"%.3f\" % x.data.cpu().numpy()[0], y))",
        "core_API": "numpy"
    },
    {
        "commit_hash": "1d8d6f6072659e905d91a2b297d53e927853457d",
        "index": "a46b86a..1afdfd7 100644",
        "commit_message": "Fix two default args in DenseNet blocks... fix #1427\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class DenseBlock(nn.ModuleDict):",
            "",
            "",
            "class DenseTransition(nn.Sequential):",
            "-    def __init__(self, num_input_features, num_output_features, norm_layer=nn.BatchNorm2d, aa_layer=None):",
            "+    def __init__(self, num_input_features, num_output_features, norm_layer=BatchNormAct2d, aa_layer=None):",
            "super(DenseTransition, self).__init__()",
            "self.add_module('norm', norm_layer(num_input_features))",
            "self.add_module('conv', nn.Conv2d("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=nn), value='BatchNormAct2d')",
            "Move(target_node=ASTNode(type=default_parameter), node=ASTNode(type=identifier, text=nn), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=BatchNorm2d))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2564,
        "neg_line": [
            "-def __init__(self, num_input_features, num_output_features, norm_layer=nn.BatchNorm2d, aa_layer=None):"
        ],
        "pos_line": [
            "+def __init__(self, num_input_features, num_output_features, norm_layer=BatchNormAct2d, aa_layer=None):"
        ],
        "core_change": "-def __init__(self, num_input_features, num_output_features, norm_layer=nn.BatchNorm2d, aa_layer=None): +def __init__(self, num_input_features, num_output_features, norm_layer=BatchNormAct2d, aa_layer=None):",
        "core_API": "add_module"
    },
    {
        "commit_hash": "8b89705072e11da058e98f180b062b28d0939de5",
        "index": "8434675f..24245178 100644",
        "commit_message": "fix tests\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PANConv(MessagePassing):",
            "",
            "tmp = SparseTensor.eye(adj_t.size(0), adj_t.size(1), has_value=True,",
            "dtype=dtype, device=adj_t.device())",
            "-        tmp = tmp.mul_nnz(self.weight[0])",
            "+        tmp = tmp.mul_nnz(self.weight[0], layout='coo')",
            "",
            "outs = [tmp]",
            "for i in range(1, self.filter_size + 1):",
            "tmp = tmp @ adj_t",
            "-            tmp = tmp.mul_nnz(self.weight[i])",
            "+            tmp = tmp.mul_nnz(self.weight[i], layout='coo')",
            "outs += [tmp]",
            "",
            "row = torch.cat([out.storage.row() for out in outs], dim=0)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1006881)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tmp'), position=0, insert_id=1006882)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1006883)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'mul_nnz'), position=2, insert_id=1006884)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1006885)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1006886)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1006887)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1006888)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'layout'), position=0, insert_id=1006889)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1006890)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'coo'\"), position=2, insert_id=1006891)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'layout'), position=0, insert_id=1006892)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1006893)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'coo'\"), position=2, insert_id=1006894)",
            "Delete(target_node=ASTNode(type=identifier, text=tmp))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=mul_nnz))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 2565,
        "neg_line": [
            "-tmp = tmp.mul_nnz(self.weight[0])",
            "-tmp = tmp.mul_nnz(self.weight[i])"
        ],
        "pos_line": [
            "+tmp = tmp.mul_nnz(self.weight[0], layout='coo')",
            "+tmp = tmp.mul_nnz(self.weight[i], layout='coo')"
        ],
        "core_change": "-tmp = tmp.mul_nnz(self.weight[0]) +tmp = tmp.mul_nnz(self.weight[0], layout='coo') -tmp = tmp.mul_nnz(self.weight[i]) +tmp = tmp.mul_nnz(self.weight[i], layout='coo')",
        "core_API": "eye"
    },
    {
        "commit_hash": "e91518ee00f2cab0fc10c4741c775d2fb5a4a1cf",
        "index": "184c7dc7..b7b03091 100644",
        "commit_message": "Update tutorials (torch versions, ES version, replace Finder with Pipeline) (#814)\n\n* remove manual torch install on colab\n\n* update elasticsearch version everywhere to 7.9.2\n\n* fix FAQPipeline\n\n* update tutorials with new pipelines\n\n* Add latest docstring and tutorial changes\n\n* revert faqpipeline change. fix field names in tutorial 4\n\n* Add latest docstring and tutorial changes\n\nCo-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>\n",
        "file": "haystack.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "},",
            "\"outputs\": [],",
            "\"source\": [",
            "-    \"# Evaluate combination of Reader and Retriever through Finder\\n\",",
            "\"# Evaluate combination of Reader and Retriever through Finder\\n\",",
            "\"finder_eval_results = finder.eval(top_k_retriever=1, top_k_reader=10, label_index=label_index, doc_index=doc_index)\\n\",",
            "\"finder.print_eval_results(finder_eval_results)\""
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=string, text=\"# Evaluate combination of Reader and Retriever through Finder\\n\"))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 2566,
        "neg_line": [
            "-\"# Evaluate combination of Reader and Retriever through Finder\\n\","
        ],
        "pos_line": [],
        "core_change": "-\"# Evaluate combination of Reader and Retriever through Finder\\n\",",
        "core_API": "eval"
    },
    {
        "commit_hash": "2b4bfff116076124111e110b2f2d21709c9b3c67",
        "index": "ebe69fd0..26aafc25 100644",
        "commit_message": "dtype fix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "import torch",
            "",
            "",
            "def one_hot(src, num_classes=None, dtype=None):",
            "+    src = src.to(torch.long)",
            "src = src.unsqueeze(-1) if src.dim() == 1 else src",
            "assert src.dim() == 2"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=1067757)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1067758)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1067759)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'src'), position=0, insert_id=1067760)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1067761)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1067762)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1067763)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1067764)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'src'), position=0, insert_id=1067765)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1067766)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1067767)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1067768)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1067769)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1067770)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1067771)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1067772)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'long'), position=2, insert_id=1067773)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 18,
        "number": 2567,
        "neg_line": [],
        "pos_line": [
            "+src = src.to(torch.long)"
        ],
        "core_change": "+src = src.to(torch.long)",
        "core_API": "to"
    },
    {
        "commit_hash": "883fe188bee424b507674af5043127b63b0dd983",
        "index": "4f66f6cf..403e2e73 100644",
        "commit_message": "fix(ci): onnxmlir, spacy test cases (#1819)\n\n* fix(ci): onnxmlir and tensorflow test cases\n\n* fix: pycaret numba deps\n",
        "file": "BentoML.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class NativeModel(tf.Module):",
            "self.dense = lambda inputs: tf.matmul(inputs, self.weights)",
            "",
            "@tf.function(",
            "-        input_signature=[tf.TensorSpec(shape=None, dtype=tf.float64, name=\"inputs\")]",
            "+        input_signature=[tf.TensorSpec(shape=[1, 5], dtype=tf.float64, name=\"inputs\")]",
            ")",
            "def __call__(self, inputs):",
            "return self.dense(inputs)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('list', None), position=2, insert_id=1906183)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=1906184)",
            "Insert(target_node=IN(type=list), node=('integer', '1'), position=1, insert_id=1906185)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=1906186)",
            "Insert(target_node=IN(type=list), node=('integer', '5'), position=3, insert_id=1906187)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=1906188)",
            "Delete(target_node=ASTNode(type=none, text=None))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 2568,
        "neg_line": [
            "-input_signature=[tf.TensorSpec(shape=None, dtype=tf.float64, name=\"inputs\")]"
        ],
        "pos_line": [
            "+input_signature=[tf.TensorSpec(shape=[1, 5], dtype=tf.float64, name=\"inputs\")]"
        ],
        "core_change": "-input_signature=[tf.TensorSpec(shape=None, dtype=tf.float64, name=\"inputs\")] +input_signature=[tf.TensorSpec(shape=[1, 5], dtype=tf.float64, name=\"inputs\")]",
        "core_API": "matmul"
    },
    {
        "commit_hash": "3313037fd5ad03fe9fb311c4c39bd5cb905e3f29",
        "index": "e874729..c9e7a48 100644",
        "commit_message": "Bugfix: support both Keras and TF-Keras optimizers in hvd.load_model (#652)\n\n\n",
        "file": "horovod.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "def load_model(keras, wrap_optimizer, filepath, custom_optimizers, custom_object",
            "horovod_objects = {",
            "subclass.__name__.lower(): wrap_optimizer(subclass)",
            "for subclass in keras.optimizers.Optimizer.__subclasses__()",
            "-        if subclass.__module__ == 'keras.optimizers'",
            "+        if subclass.__module__ == keras.optimizers.Optimizer.__module__",
            "}",
            "",
            "if custom_optimizers is not None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('attribute', None), position=2, insert_id=2486478)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2486479)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2486480)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__module__'), position=2, insert_id=2486481)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2486482)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2486483)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Optimizer'), position=2, insert_id=2486484)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'keras'), position=0, insert_id=2486485)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2486486)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'optimizers'), position=2, insert_id=2486487)",
            "Delete(target_node=ASTNode(type=string, text='keras.optimizers'))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 2570,
        "neg_line": [
            "-if subclass.__module__ == 'keras.optimizers'"
        ],
        "pos_line": [
            "+if subclass.__module__ == keras.optimizers.Optimizer.__module__"
        ],
        "core_change": "-if subclass.__module__ == 'keras.optimizers' +if subclass.__module__ == keras.optimizers.Optimizer.__module__",
        "core_API": "lower"
    },
    {
        "commit_hash": "bd1af11901a2b3689ff58c369f9d8011b3819e69",
        "index": "1c880b6a..a397cc02 100644",
        "commit_message": "fix collect_env hirachy and try formatting code\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "import flair",
            "",
            "",
            "def main():",
            "-    print(\"## Versions:\")",
            "-    print(f\"### Flair\\n{flair.__version__}\")",
            "-    print(f\"### Pytorch\\n{torch.__version__}\")",
            "-    print(f\"### Transformers\\n{transformers.__version__}\")",
            "-    print(f\"## GPU\\n{torch.cuda.is_available()}\")",
            "+    print(\"#### Versions:\")",
            "+    print(f\"#### Flair\\n{flair.__version__}\")",
            "+    print(f\"#### Pytorch\\n{torch.__version__}\")",
            "+    print(f\"#### Transformers\\n{transformers.__version__}\")",
            "+    print(f\"#### GPU\\n{torch.cuda.is_available()}\")",
            "",
            "",
            "if __name__ == \"__main__\":"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=block), position=3)",
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=231842)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=231843)",
            "Insert(target_node=IN(type=call), node=('identifier', 'print'), position=0, insert_id=231844)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=231845)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=231846)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"#### Versions:\"'), position=1, insert_id=231847)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=231848)",
            "Update(target_node=ASTNode(type=string, text=f\"### Flair\\n{flair.__version__}\"), value='f\"#### Flair\\\\n{flair.__version__}\"')",
            "Update(target_node=ASTNode(type=string, text=f\"### Pytorch\\n{torch.__version__}\"), value='f\"#### Pytorch\\\\n{torch.__version__}\"')",
            "Update(target_node=ASTNode(type=string, text=f\"### Transformers\\n{transformers.__version__}\"), value='f\"#### Transformers\\\\n{transformers.__version__}\"')",
            "Update(target_node=ASTNode(type=string, text=\"## Versions:\"), value='f\"#### GPU\\\\n{torch.cuda.is_available()}\"')",
            "Delete(target_node=ASTNode(type=identifier, text=print))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=f\"## GPU\\n{torch.cuda.is_available()}\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 5,
        "minus_line": 5,
        "AST_diff_line": 19,
        "number": 2571,
        "neg_line": [
            "-print(\"## Versions:\")",
            "-print(f\"### Flair\\n{flair.__version__}\")",
            "-print(f\"### Pytorch\\n{torch.__version__}\")",
            "-print(f\"### Transformers\\n{transformers.__version__}\")",
            "-print(f\"## GPU\\n{torch.cuda.is_available()}\")"
        ],
        "pos_line": [
            "+print(\"#### Versions:\")",
            "+print(f\"#### Flair\\n{flair.__version__}\")",
            "+print(f\"#### Pytorch\\n{torch.__version__}\")",
            "+print(f\"#### Transformers\\n{transformers.__version__}\")",
            "+print(f\"#### GPU\\n{torch.cuda.is_available()}\")"
        ],
        "core_change": "-print(\"## Versions:\") -print(f\"### Flair\\n{flair.__version__}\") -print(f\"### Pytorch\\n{torch.__version__}\") -print(f\"### Transformers\\n{transformers.__version__}\") -print(f\"## GPU\\n{torch.cuda.is_available()}\") +print(\"#### Versions:\") +print(f\"#### Flair\\n{flair.__version__}\") +print(f\"#### Pytorch\\n{torch.__version__}\") +print(f\"#### Transformers\\n{transformers.__version__}\") +print(f\"#### GPU\\n{torch.cuda.is_available()}\")",
        "core_API": "is_available"
    },
    {
        "commit_hash": "8e154d2b392871544cd057465933f6ec50f87367",
        "index": "7b35e103..c727f92d 100755",
        "commit_message": "version 0.5.5, probably final commit before 0.6, few fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class NaturalGradient(Optimizer):",
            "return estimated_delta",
            "",
            "# Natural gradient step only works if constant > 0",
            "-        skip_step = constant > tf.constant(value=0.0, dtype=util.tf_dtype(dtype='float'))",
            "+        epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype='float'))",
            "+        skip_step = constant < (epsilon * learning_rate)",
            "return self.cond(pred=skip_step, true_fn=no_step, false_fn=apply_step)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=2224738)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2224739)",
            "Update(target_node=ASTNode(type=identifier, text=skip_step), value='epsilon')",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'skip_step'), position=0, insert_id=2224740)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2224741)",
            "Insert(target_node=IN(type=assignment), node=('comparison_operator', None), position=2, insert_id=2224742)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'constant'), position=0, insert_id=2224743)",
            "Insert(target_node=IN(type=comparison_operator), node=('<', '<'), position=1, insert_id=2224744)",
            "Insert(target_node=IN(type=comparison_operator), node=('parenthesized_expression', None), position=2, insert_id=2224745)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=2224746)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('binary_operator', None), position=1, insert_id=2224747)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=2224748)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('attribute', None), position=2, insert_id=2224749)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'epsilon'), position=0, insert_id=2224750)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=2224751)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'learning_rate'), position=2, insert_id=2224752)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'util'), position=0, insert_id=2224753)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2224754)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'epsilon'), position=2, insert_id=2224755)",
            "Delete(target_node=ASTNode(type=identifier, text=constant))",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=float, text=0.0))",
            "Delete(target_node=ASTNode(type=comparison_operator))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 2572,
        "neg_line": [
            "-skip_step = constant > tf.constant(value=0.0, dtype=util.tf_dtype(dtype='float'))"
        ],
        "pos_line": [
            "+epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype='float'))",
            "+skip_step = constant < (epsilon * learning_rate)"
        ],
        "core_change": "-skip_step = constant > tf.constant(value=0.0, dtype=util.tf_dtype(dtype='float')) +epsilon = tf.constant(value=util.epsilon, dtype=util.tf_dtype(dtype='float')) +skip_step = constant < (epsilon * learning_rate)",
        "core_API": "constant"
    },
    {
        "commit_hash": "731ea53c803ce00fbfd7531b4f1c6bf263a1f6b4",
        "index": "c37a3a6..f9f217a 100644",
        "commit_message": "Llff & blender convention fix\n\nSummary: Images were coming out in the wrong format.\n\nReviewed By: shapovalov\n\nDifferential Revision: D37291278\n\nfbshipit-source-id: c10871c37dd186982e7abf2071ac66ed583df2e6\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BlenderDatasetMapProvider(SingleSceneDatasetMapProviderBase):",
            ")",
            "H, W, focal = hwf",
            "H, W = int(H), int(W)",
            "-        images = torch.from_numpy(images)",
            "+        images = torch.from_numpy(images).permute(0, 3, 1, 2)[:, :3]",
            "",
            "# pyre-ignore[16]",
            "self.poses = _interpret_blender_cameras(poses, H, W, focal)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('subscript', None), position=2, insert_id=912221)",
            "Insert(target_node=IN(type=subscript), node=('call', None), position=0, insert_id=912222)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=912223)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=2, insert_id=912224)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=912225)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=4, insert_id=912226)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=5, insert_id=912227)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=912228)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=912229)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=912230)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=912231)",
            "Insert(target_node=IN(type=slice), node=('integer', '3'), position=1, insert_id=912232)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=912233)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'permute'), position=2, insert_id=912234)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=912235)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '0'), position=1, insert_id=912236)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=912237)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '3'), position=3, insert_id=912238)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=912239)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=5, insert_id=912240)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=6, insert_id=912241)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '2'), position=7, insert_id=912242)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=8, insert_id=912243)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 2573,
        "neg_line": [
            "-images = torch.from_numpy(images)"
        ],
        "pos_line": [
            "+images = torch.from_numpy(images).permute(0, 3, 1, 2)[:, :3]"
        ],
        "core_change": "-images = torch.from_numpy(images) +images = torch.from_numpy(images).permute(0, 3, 1, 2)[:, :3]",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "7e3fec26927c3d0655d5c083937ca86e5be0d6d1",
        "index": "4c5a7e48..4141ec43 100644",
        "commit_message": "support DeConv1dLayer, fix bugs and refine docs\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class BinaryConv2d(Layer):",
            "name=self.name",
            ")",
            "if self.b_init:",
            "-            outputs = tf.nn.bias_add(outputs, self.b, name='bias_add')",
            "+            outputs = tf.nn.bias_add(outputs, self.b, data_format=self.data_format, name='bias_add')",
            "if self.act:",
            "outputs = self.act(outputs)",
            "return outputs"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2258186)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=2258187)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'data_format'), position=0, insert_id=2258188)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2258189)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2258190)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2258191)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2258192)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'data_format'), position=2, insert_id=2258193)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2575,
        "neg_line": [
            "-outputs = tf.nn.bias_add(outputs, self.b, name='bias_add')"
        ],
        "pos_line": [
            "+outputs = tf.nn.bias_add(outputs, self.b, data_format=self.data_format, name='bias_add')"
        ],
        "core_change": "-outputs = tf.nn.bias_add(outputs, self.b, name='bias_add') +outputs = tf.nn.bias_add(outputs, self.b, data_format=self.data_format, name='bias_add')",
        "core_API": "bias_add"
    },
    {
        "commit_hash": "1a79d595f7eda9dc9dc8428f4461680ed2222ab6",
        "index": "e499a933..62ad43ca 100644",
        "commit_message": "TensorFlow Version 1 Compatibility Fix (#1538)\n\n* tf v1 compat fix\n\n* fix lint error for unused module\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class HorovodTrainer(SingleCostTrainer):",
            "# broadcast_op should be the last setup_graph: it needs to be created",
            "# \"right before\" the graph is finalized,",
            "# because it needs to capture all the variables (which may be created by callbacks).",
            "-        self._num_global_variables = len(tf.global_variables())",
            "+        self._num_global_variables = len(tfv1 .global_variables())",
            "self._broadcast_op = self.hvd.broadcast_global_variables(0)",
            "",
            "# it's important that our NewSessionCreator does not finalize the graph"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tfv1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2577,
        "neg_line": [
            "-self._num_global_variables = len(tf.global_variables())"
        ],
        "pos_line": [
            "+self._num_global_variables = len(tfv1 .global_variables())"
        ],
        "core_change": "-self._num_global_variables = len(tf.global_variables()) +self._num_global_variables = len(tfv1 .global_variables())",
        "core_API": "global_variables"
    },
    {
        "commit_hash": "342074a6bc29b03974fda3073af5e46bfa27cb0f",
        "index": "8bca17e..9e84f67 100644",
        "commit_message": "fix Image format issues (#310)\n\n* organize functions, move the image processing code outside make_np.\n\n* made add_image flexible\n\n* change video API\n\n* fixes #286. fix image_with_boxes\n\n* fixed #285\n\n",
        "file": "tensorboardX.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "writer.add_embedding(all_features, metadata=all_labels, label_img=all_images.uns",
            "",
            "# VIDEO",
            "vid_images = dataset.train_data[:16 * 48]",
            "-vid = vid_images.view(16, 1, 48, 28, 28)  # BxCxTxHxW",
            "+vid = vid_images.view(16, 48, 1, 28, 28)  # BxTxCxHxW",
            "",
            "writer.add_video('video', vid_tensor=vid)",
            "writer.add_video('video_1_fps', vid_tensor=vid, fps=1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=integer, text=1), node=ASTNode(type=argument_list), position=5)",
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=argument_list), position=6)"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 2,
        "number": 2578,
        "neg_line": [
            "-vid = vid_images.view(16, 1, 48, 28, 28)  # BxCxTxHxW"
        ],
        "pos_line": [
            "+vid = vid_images.view(16, 48, 1, 28, 28)  # BxTxCxHxW"
        ],
        "core_change": "-vid = vid_images.view(16, 1, 48, 28, 28)  # BxCxTxHxW +vid = vid_images.view(16, 48, 1, 28, 28)  # BxTxCxHxW",
        "core_API": "add_embedding"
    },
    {
        "commit_hash": "fab3b518ef5429c1eb885fece126631f9fd786e3",
        "index": "71a7acd5d..8fbdce883 100644",
        "commit_message": "fix deprecated tf method (#14671)\n\ntf.matrix_band_part -> tf.linalg.band_part\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFXLNetMainLayer(tf.keras.layers.Layer):",
            "",
            "\"\"\"",
            "attn_mask = tf.ones([qlen, qlen])",
            "-        mask_u = tf.matrix_band_part(attn_mask, 0, -1)",
            "-        mask_dia = tf.matrix_band_part(attn_mask, 0, 0)",
            "+        mask_u = tf.linalg.band_part(attn_mask, 0, -1)",
            "+        mask_dia = tf.linalg.band_part(attn_mask, 0, 0)",
            "attn_mask_pad = tf.zeros([qlen, mlen])",
            "ret = tf.concat([attn_mask_pad, mask_u - mask_dia], 1)",
            "if self.same_length:",
            "-            mask_l = tf.matrix_band_part(attn_mask, -1, 0)",
            "+            mask_l = tf.linalg.band_part(attn_mask, -1, 0)",
            "ret = tf.concat([ret[:, :qlen] + mask_l - mask_dia, ret[:, qlen:]], 1)",
            "return ret"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2368276)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2368277)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2368278)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'band_part'), position=2, insert_id=2368279)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2368280)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'band_part'), position=2, insert_id=2368281)",
            "Update(target_node=ASTNode(type=identifier, text=matrix_band_part), value='linalg')",
            "Update(target_node=ASTNode(type=identifier, text=matrix_band_part), value='linalg')",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2368282)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2368283)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'band_part'), position=2, insert_id=2368284)",
            "Update(target_node=ASTNode(type=identifier, text=matrix_band_part), value='linalg')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 15,
        "number": 2579,
        "neg_line": [
            "-mask_u = tf.matrix_band_part(attn_mask, 0, -1)",
            "-mask_dia = tf.matrix_band_part(attn_mask, 0, 0)",
            "-mask_l = tf.matrix_band_part(attn_mask, -1, 0)"
        ],
        "pos_line": [
            "+mask_u = tf.linalg.band_part(attn_mask, 0, -1)",
            "+mask_dia = tf.linalg.band_part(attn_mask, 0, 0)",
            "+mask_l = tf.linalg.band_part(attn_mask, -1, 0)"
        ],
        "core_change": "-mask_u = tf.matrix_band_part(attn_mask, 0, -1) -mask_dia = tf.matrix_band_part(attn_mask, 0, 0) +mask_u = tf.linalg.band_part(attn_mask, 0, -1) +mask_dia = tf.linalg.band_part(attn_mask, 0, 0) -mask_l = tf.matrix_band_part(attn_mask, -1, 0) +mask_l = tf.linalg.band_part(attn_mask, -1, 0)",
        "core_API": "ones"
    },
    {
        "commit_hash": "f332fc2db760e3f3d49ad09c246fe09869ac4f2f",
        "index": "551a832..1f698fb 100644",
        "commit_message": "Fix some test failures, torchscript issues\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class PyramidVisionTransformerV2(nn.Module):",
            "cur += depths[i]",
            "",
            "# classification head",
            "-        self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity()",
            "+        self.num_features = embed_dims[-1]",
            "+        self.head = nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity()",
            "",
            "self.apply(self._init_weights)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=1476980)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1476981)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=1476982)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1476983)",
            "Insert(target_node=IN(type=assignment), node=('subscript', None), position=2, insert_id=1476984)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1476985)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1476986)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'num_features'), position=2, insert_id=1476987)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'embed_dims'), position=0, insert_id=1476988)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1476989)",
            "Insert(target_node=IN(type=subscript), node=('unary_operator', '-1'), position=2, insert_id=1476990)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1476991)",
            "Insert(target_node=ASTNode(type=subscript), node=('unary_operator', '-1'), position=2, insert_id=1476992)",
            "Delete(target_node=ASTNode(type=integer, text=3))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 2581,
        "neg_line": [
            "-self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity()"
        ],
        "pos_line": [
            "+self.num_features = embed_dims[-1]",
            "+self.head = nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity()"
        ],
        "core_change": "-self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity() +self.num_features = embed_dims[-1] +self.head = nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity()",
        "core_API": "Linear"
    },
    {
        "commit_hash": "9559fb5302f437bce83b6d9092602b407f155193",
        "index": "f8d2b6c6..9f4e1c4b 100644",
        "commit_message": "Fix issue calling TextVectorization on non-tensor input\n\ntf.keras.layers.TextVectorization(vocabulary=[\"foo\"])([\"foo\"]) would fail with an error.\nThis was due to checking rank of input during build. We should instead check rank during\ncall, after converting to tensor.\n\nPiperOrigin-RevId: 406160462\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class StringLookupVocabularyTest(keras_parameterized.TestCase,",
            "fn()",
            "",
            "if __name__ == \"__main__\":",
            "+  # StringLookup is only exported as a TF2 API.",
            "+  tf.compat.v1.enable_v2_behavior()",
            "tf.test.main()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=2076309)",
            "Insert(target_node=ASTNode(type=attribute), node=('ERROR', None), position=1, insert_id=2076310)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2076311)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2076312)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=tf), position=0)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2076313)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2076314)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'enable_v2_behavior'), position=2, insert_id=2076315)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2076316)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2076317)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2076318)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2076319)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'v1'), position=2, insert_id=2076320)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2076321)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2076322)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'compat'), position=2, insert_id=2076323)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 16,
        "number": 2583,
        "neg_line": [],
        "pos_line": [
            "+# StringLookup is only exported as a TF2 API.",
            "+tf.compat.v1.enable_v2_behavior()"
        ],
        "core_change": "+# StringLookup is only exported as a TF2 API. +tf.compat.v1.enable_v2_behavior()",
        "core_API": "enable_v2_behavior"
    },
    {
        "commit_hash": "8a4fcd025a2f1d6e1f66446b337b57ed6e8dd130",
        "index": "92e35da..06acd66 100644",
        "commit_message": "Fix comment.\n\nPiperOrigin-RevId: 265850926\n\n",
        "file": "hub.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def train_and_export(export_path,",
            "metric.result().numpy()))",
            "",
            "# We have to call either predict or fit to make it possible to export with",
            "-  # tf.keras.models.save_model.",
            "+  # tf.saved_model.save.",
            "model.predict(next(iter(dataset))[\"image\"])",
            "# Export the model as SavedModel 2.0.",
            "tf.saved_model.save(model, export_path)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2584,
        "neg_line": [
            "-# tf.keras.models.save_model."
        ],
        "pos_line": [
            "+# tf.saved_model.save."
        ],
        "core_change": "-# tf.keras.models.save_model. +# tf.saved_model.save.",
        "core_API": "result"
    },
    {
        "commit_hash": "76086583a7f7c481c96c0335c41454d4a8a5272e",
        "index": "24955459..5909f3b6 100644",
        "commit_message": "Fix compressor op_types (#1670)\n\n* fix compressor op_types\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class Quantizer(Compressor):",
            "Base quantizer for pytorch quantizer",
            "\"\"\"",
            "",
            "-    def __call__(self, model):",
            "-        self.compress(model)",
            "-        return model",
            "-",
            "def quantize_weight(self, weight, config, op, op_type, op_name):",
            "\"\"\"user should know where dequantize goes and implement it in quantize method",
            "we now do not provide dequantize method"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n\n    def __call__(self, model):\n        self.compress(model)\n        return model\n\ndef quantize_weight(self, weight, config, op, op_type, op_name):\n\"\"\"), value='\"\"\"\\n\\ndef quantize_weight(self, weight, config, op, op_type, op_name):\\n\"\"\"')"
        ],
        "plus_line": 0,
        "minus_line": 3,
        "AST_diff_line": 1,
        "number": 2585,
        "neg_line": [
            "-def __call__(self, model):",
            "-self.compress(model)",
            "-return model",
            "-"
        ],
        "pos_line": [],
        "core_change": "-def __call__(self, model): -self.compress(model) -return model -",
        "core_API": "compress"
    },
    {
        "commit_hash": "979ca24e39b52d52d0b07d8e17bff2f881a8884e",
        "index": "1b31fa0ee..32923ce44 100755",
        "commit_message": "[Fix doc example] Wrong checkpoint name (#15079)\n\n* fix doc example - MarianForCausalLM example\n\n* try to keep copies\n\n* fix copies\n\n* fix more similar doc examples\n\n* fix more\n\n* fix style\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class PegasusForCausalLM(PegasusPreTrainedModel):",
            "```python",
            ">>> from transformers import PegasusTokenizer, PegasusForCausalLM",
            "",
            "-        >>> tokenizer = PegasusTokenizer.from_pretrained(\"facebook/bart-large\")",
            "-        >>> model = PegasusForCausalLM.from_pretrained(\"facebook/bart-large\", add_cross_attention=False)",
            "+        >>> tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")",
            "+        >>> model = PegasusForCausalLM.from_pretrained(\"google/pegasus-large\", add_cross_attention=False)",
            ">>> assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"",
            ">>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")",
            ">>> outputs = model(**inputs)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=module), position=9)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=2, insert_id=2684329)",
            "Insert(target_node=IN(type=ERROR), node=('>>', '>>'), position=0, insert_id=2684330)",
            "Insert(target_node=IN(type=ERROR), node=('>', '>'), position=1, insert_id=2684331)",
            "Update(target_node=ASTNode(type=string, text=\"facebook/bart-large\"), value='\"google/pegasus-large\"')",
            "Update(target_node=ASTNode(type=string, text=\"facebook/bart-large\"), value='\"google/pegasus-large\"')",
            "Delete(target_node=ASTNode(type=>>, text=>>))",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 2587,
        "neg_line": [
            "->>> tokenizer = PegasusTokenizer.from_pretrained(\"facebook/bart-large\")",
            "->>> model = PegasusForCausalLM.from_pretrained(\"facebook/bart-large\", add_cross_attention=False)"
        ],
        "pos_line": [
            "+>>> tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")",
            "+>>> model = PegasusForCausalLM.from_pretrained(\"google/pegasus-large\", add_cross_attention=False)"
        ],
        "core_change": "->>> tokenizer = PegasusTokenizer.from_pretrained(\"facebook/bart-large\") ->>> model = PegasusForCausalLM.from_pretrained(\"facebook/bart-large\", add_cross_attention=False) +>>> tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\") +>>> model = PegasusForCausalLM.from_pretrained(\"google/pegasus-large\", add_cross_attention=False)",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "1251072f46f895dd79f29c0470af737fdce3738b",
        "index": "2d98da264..05ddc89aa 100644",
        "commit_message": "Fix SEW-D implementation differences (#14191)\n\n* Fix SEW-D\n\n* Update tests\n\n* isort\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "SEWD_INPUTS_DOCSTRING = r\"\"\"",
            "\"The bare SEW-D Model transformer outputting raw hidden-states without any specific head on top.\",",
            "SEWD_START_DOCSTRING,",
            ")",
            "-# Copied from transformers.models.sew.modeling_sew.SEWModel with SEW->SEWD",
            "+# Copied from transformers.models.sew.modeling_sew.SEWModel with SEW->SEWD, layer_norm_eps->feature_layer_norm_eps",
            "class SEWDModel(SEWDPreTrainedModel):",
            "def __init__(self, config: SEWDConfig):",
            "super().__init__(config)",
            "self.config = config",
            "self.feature_extractor = SEWDFeatureExtractor(config)",
            "-        self.layer_norm = nn.LayerNorm(config.conv_dim[-1], eps=config.layer_norm_eps)",
            "+        self.layer_norm = nn.LayerNorm(config.conv_dim[-1], eps=config.feature_layer_norm_eps)",
            "",
            "self.project_features = config.conv_dim[-1] != config.hidden_size",
            "if self.project_features:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=layer_norm_eps), value='feature_layer_norm_eps')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2589,
        "neg_line": [
            "-# Copied from transformers.models.sew.modeling_sew.SEWModel with SEW->SEWD",
            "-self.layer_norm = nn.LayerNorm(config.conv_dim[-1], eps=config.layer_norm_eps)"
        ],
        "pos_line": [
            "+# Copied from transformers.models.sew.modeling_sew.SEWModel with SEW->SEWD, layer_norm_eps->feature_layer_norm_eps",
            "+self.layer_norm = nn.LayerNorm(config.conv_dim[-1], eps=config.feature_layer_norm_eps)"
        ],
        "core_change": "-# Copied from transformers.models.sew.modeling_sew.SEWModel with SEW->SEWD +# Copied from transformers.models.sew.modeling_sew.SEWModel with SEW->SEWD, layer_norm_eps->feature_layer_norm_eps -self.layer_norm = nn.LayerNorm(config.conv_dim[-1], eps=config.layer_norm_eps) +self.layer_norm = nn.LayerNorm(config.conv_dim[-1], eps=config.feature_layer_norm_eps)",
        "core_API": "LayerNorm"
    },
    {
        "commit_hash": "6bed86fb78097d610287708485ec4f65c27a4ae9",
        "index": "2227878..e7536d1 100644",
        "commit_message": "Fix tests\n\n",
        "file": "seq2seq.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ExtendedMultiRNNCellTest(tf.test.TestCase):",
            "",
            "with tf.variable_scope(\"root\", initializer=tf.constant_initializer(0.5)):",
            "test_cell = rnn_cell.ExtendedMultiRNNCell(",
            "-          [tf.contrib.rnn.GRUCell(2)] * 2,",
            "+          [tf.contrib.rnn.GRUCell(2) for _ in range(2)],",
            "residual_connections=True, **kwargs)",
            "res_test = test_cell(inputs, state, scope=\"test\")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('list_comprehension', None), position=1, insert_id=2160271)",
            "Move(target_node=IN(type=list_comprehension), node=ASTNode(type=[, text=[), position=0)",
            "Move(target_node=IN(type=list_comprehension), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=list_comprehension), node=('for_in_clause', None), position=2, insert_id=2160272)",
            "Insert(target_node=IN(type=list_comprehension), node=(']', ']'), position=3, insert_id=2160273)",
            "Insert(target_node=IN(type=for_in_clause), node=('for', 'for'), position=0, insert_id=2160274)",
            "Insert(target_node=IN(type=for_in_clause), node=('identifier', '_'), position=1, insert_id=2160275)",
            "Insert(target_node=IN(type=for_in_clause), node=('in', 'in'), position=2, insert_id=2160276)",
            "Insert(target_node=IN(type=for_in_clause), node=('call', None), position=3, insert_id=2160277)",
            "Insert(target_node=IN(type=call), node=('identifier', 'range'), position=0, insert_id=2160278)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2160279)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2160280)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=integer, text=2), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2160281)",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=*, text=*))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 2590,
        "neg_line": [
            "-[tf.contrib.rnn.GRUCell(2)] * 2,"
        ],
        "pos_line": [
            "+[tf.contrib.rnn.GRUCell(2) for _ in range(2)],"
        ],
        "core_change": "-[tf.contrib.rnn.GRUCell(2)] * 2, +[tf.contrib.rnn.GRUCell(2) for _ in range(2)],",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "a24fdf2d1b36e699d8f4e3efd33b7b78d6a02e7e",
        "index": "db730c62..4a441220 100644",
        "commit_message": "Fixing and error related to Floor Division (#4221)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes https://github.com/pytorch/fairseq/issues/4058\nWhile using the library the following warnings are shown which sometimes hinder the workflow. The warnings are\n\n`<USER_PATH>/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n  beams_buf = indices_buf // vocab_size`\n\n`<USER_PATH>/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n  unfin_idx = bbsz_idx // beam_size`\n\nThe methodology was simple, instead of using the `//`, it was replaced by `torch.div(arg1, arg2, rounding_mode='trunc')` and the variable alues do not change for both before and after, just the warning is resolved.\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \nYes, I did! Thanks!\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/4221\n\nReviewed By: arbabu123\n\nDifferential Revision: D34538147\n\nPulled By: alexeib\n\nfbshipit-source-id: 143897a249129a163b6a30ba9b5cf5595ef42330\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SequenceGenerator(nn.Module):",
            "cum_unfin.append(prev)",
            "cum_fin_tensor = torch.tensor(cum_unfin, dtype=torch.int).to(bbsz_idx)",
            "",
            "-        unfin_idx = bbsz_idx // beam_size",
            "+        unfin_idx = torch.div(bbsz_idx, beam_size, rounding_mode='trunc')",
            "sent = unfin_idx + torch.index_select(cum_fin_tensor, 0, unfin_idx)",
            "",
            "# Create a set of \"{sent}{unfin_idx}\", where"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=204076)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=204077)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=204078)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=204079)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=204080)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'div'), position=2, insert_id=204081)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=204082)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=bbsz_idx), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=204083)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=beam_size), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=204084)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=5, insert_id=204085)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=6, insert_id=204086)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'rounding_mode'), position=0, insert_id=204087)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=204088)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'trunc'\"), position=2, insert_id=204089)",
            "Delete(target_node=ASTNode(type=//, text=//))",
            "Delete(target_node=ASTNode(type=binary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 2592,
        "neg_line": [
            "-unfin_idx = bbsz_idx // beam_size"
        ],
        "pos_line": [
            "+unfin_idx = torch.div(bbsz_idx, beam_size, rounding_mode='trunc')"
        ],
        "core_change": "-unfin_idx = bbsz_idx // beam_size +unfin_idx = torch.div(bbsz_idx, beam_size, rounding_mode='trunc')",
        "core_API": "append"
    },
    {
        "commit_hash": "374df7d4fd08125c4f5c905ca35752655481ac4b",
        "index": "3af6e99039..b9716773f1 100644",
        "commit_message": "Creation functions backend test fixes (#2373)\n\nFixes from_dlpack() and logspace() from failing unit tests\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "eye.unsupported_dtypes = (\"uint16\",)",
            "def from_dlpack(",
            "x: Union[tf.Tensor, tf.Variable], *, out: Union[tf.Tensor, tf.Variable] = None",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    return tf.experimental.dlpack.from_dlpack(x)",
            "+    dlcapsule = tf.experimental.dlpack.to_dlpack(x)",
            "+    return tf.experimental.dlpack.from_dlpack(dlcapsule)",
            "",
            "",
            "def full("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=2004266)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2004267)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'dlcapsule'), position=0, insert_id=2004268)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2004269)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2004270)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2004271)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2004272)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2004273)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'dlcapsule'), position=1, insert_id=2004274)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2004275)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2004276)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2004277)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to_dlpack'), position=2, insert_id=2004278)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2004279)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2004280)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dlpack'), position=2, insert_id=2004281)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2004282)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2004283)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'experimental'), position=2, insert_id=2004284)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 2593,
        "neg_line": [
            "-return tf.experimental.dlpack.from_dlpack(x)"
        ],
        "pos_line": [
            "+dlcapsule = tf.experimental.dlpack.to_dlpack(x)",
            "+return tf.experimental.dlpack.from_dlpack(dlcapsule)"
        ],
        "core_change": "-return tf.experimental.dlpack.from_dlpack(x) +dlcapsule = tf.experimental.dlpack.to_dlpack(x) +return tf.experimental.dlpack.from_dlpack(dlcapsule)",
        "core_API": "from_dlpack"
    },
    {
        "commit_hash": "455c6390938a5c737fa63e78396cedae41e4e87e",
        "index": "35c3601ce..cda4c1fc2 100644",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "from .modeling_utils import Conv1D, PreTrainedModel, SequenceSummary, prune_conv",
            "logger = logging.getLogger(__name__)",
            "",
            "GPT2_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"gpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin\",",
            "-    \"gpt2-medium\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-pytorch_model.bin\",",
            "-    \"gpt2-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-pytorch_model.bin\",",
            "-    \"gpt2-xl\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-xl-pytorch_model.bin\",",
            "-    \"distilgpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-pytorch_model.bin\",",
            "+    \"gpt2\": \"https://cdn.huggingface.co/gpt2-pytorch_model.bin\",",
            "+    \"gpt2-medium\": \"https://cdn.huggingface.co/gpt2-medium-pytorch_model.bin\",",
            "+    \"gpt2-large\": \"https://cdn.huggingface.co/gpt2-large-pytorch_model.bin\",",
            "+    \"gpt2-xl\": \"https://cdn.huggingface.co/gpt2-xl-pytorch_model.bin\",",
            "+    \"distilgpt2\": \"https://cdn.huggingface.co/distilgpt2-pytorch_model.bin\",",
            "}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=,, text=,), node=ASTNode(type=dictionary), position=9)",
            "Insert(target_node=ASTNode(type=dictionary), node=(',', ','), position=2, insert_id=2689769)",
            "Insert(target_node=ASTNode(type=pair), node=(':', ':'), position=1, insert_id=2689770)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin\"), value='\"https://cdn.huggingface.co/gpt2-pytorch_model.bin\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-pytorch_model.bin\"), value='\"https://cdn.huggingface.co/gpt2-medium-pytorch_model.bin\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-pytorch_model.bin\"), value='\"https://cdn.huggingface.co/gpt2-large-pytorch_model.bin\"')",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-xl-pytorch_model.bin\"), value='\"https://cdn.huggingface.co/gpt2-xl-pytorch_model.bin\"')",
            "Move(target_node=ASTNode(type=pair), node=ASTNode(type=:, text=:), position=1)",
            "Update(target_node=ASTNode(type=string, text=\"https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-pytorch_model.bin\"), value='\"https://cdn.huggingface.co/distilgpt2-pytorch_model.bin\"')",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 5,
        "minus_line": 5,
        "AST_diff_line": 11,
        "number": 2596,
        "neg_line": [
            "-\"gpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin\",",
            "-\"gpt2-medium\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-pytorch_model.bin\",",
            "-\"gpt2-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-pytorch_model.bin\",",
            "-\"gpt2-xl\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-xl-pytorch_model.bin\",",
            "-\"distilgpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-pytorch_model.bin\","
        ],
        "pos_line": [
            "+\"gpt2\": \"https://cdn.huggingface.co/gpt2-pytorch_model.bin\",",
            "+\"gpt2-medium\": \"https://cdn.huggingface.co/gpt2-medium-pytorch_model.bin\",",
            "+\"gpt2-large\": \"https://cdn.huggingface.co/gpt2-large-pytorch_model.bin\",",
            "+\"gpt2-xl\": \"https://cdn.huggingface.co/gpt2-xl-pytorch_model.bin\",",
            "+\"distilgpt2\": \"https://cdn.huggingface.co/distilgpt2-pytorch_model.bin\","
        ],
        "core_change": "-\"gpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin\", -\"gpt2-medium\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-pytorch_model.bin\", -\"gpt2-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-pytorch_model.bin\", -\"gpt2-xl\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-xl-pytorch_model.bin\", -\"distilgpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-pytorch_model.bin\", +\"gpt2\": \"https://cdn.huggingface.co/gpt2-pytorch_model.bin\", +\"gpt2-medium\": \"https://cdn.huggingface.co/gpt2-medium-pytorch_model.bin\", +\"gpt2-large\": \"https://cdn.huggingface.co/gpt2-large-pytorch_model.bin\", +\"gpt2-xl\": \"https://cdn.huggingface.co/gpt2-xl-pytorch_model.bin\", +\"distilgpt2\": \"https://cdn.huggingface.co/distilgpt2-pytorch_model.bin\",",
        "core_API": "getLogger"
    },
    {
        "commit_hash": "875ab806bb71e7f69660dca7a106065ec6371869",
        "index": "d845e072..60f6e2dc 100644",
        "commit_message": "[Feat] improve testing framework (#560)\n\n* Parametrize device and dtype fixtures using CLI options\n\n* improve tests parametrise n MakeFile\n\n* add coverage tot tests and update ci config file\n\n* test pep8 using pytest-flak8\n\n* add pytest-mypy, update failing tests and remove verify script\n\n* update build-docs command in MakeFile\n\n* fix performance test issue with non cuda device\n\n* add pydocstyle, adapt normalisation module to pydocsyle\n\n* fix augmentation  docs rendering\n\nCo-authored-by: Aiden Nibali <dismaldenizen@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestRandomRotation:",
            "torch.manual_seed(0)  # for random reproductibility",
            "",
            "@torch.jit.script",
            "-        def op_script(data: torch.Tensor) -> torch.Tensor:",
            "-",
            "+        def op_script(data: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:",
            "return kornia.random_rotation(data, degrees=45.0)",
            "",
            "input = torch.tensor([[1., 0., 0., 2.],"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=type), node=('subscript', None), position=0, insert_id=441448)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Union'), position=0, insert_id=441449)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=441450)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=attribute), position=2)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=441451)",
            "Insert(target_node=IN(type=subscript), node=('subscript', None), position=4, insert_id=441452)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=5, insert_id=441453)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Tuple'), position=0, insert_id=441454)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=441455)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=2, insert_id=441456)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=441457)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=4, insert_id=441458)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=5, insert_id=441459)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=441460)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=441461)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=441462)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=441463)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=441464)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=441465)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 2598,
        "neg_line": [
            "-def op_script(data: torch.Tensor) -> torch.Tensor:",
            "-"
        ],
        "pos_line": [
            "+def op_script(data: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:"
        ],
        "core_change": "-def op_script(data: torch.Tensor) -> torch.Tensor: - +def op_script(data: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:",
        "core_API": "manual_seed"
    },
    {
        "commit_hash": "8ff2b6df8678a019d259b1e21fd09fd1ac9bd580",
        "index": "= 0",
        "commit_message": "Fix style\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def perform_analysis_torch(preds, indices, noise_eps=0.1, delta=1e-5, moments=8,",
            "data_ind_eps_list = (data_ind_log_mgf - math.log(delta)) / l_list",
            "",
            "return min(eps_list_nm), min(data_ind_eps_list)",
            "-",
            "-",
            "-",
            "\\ No newline at end of file"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 2602,
        "neg_line": [
            "-",
            "-",
            "-"
        ],
        "pos_line": [],
        "core_change": "- - -",
        "core_API": "log"
    },
    {
        "commit_hash": "9e510e5a1ab35bf9aadf5e3e2198f643f89ce510",
        "index": "d48e01fb..02f81c97 100755",
        "commit_message": "bug fixes for VPG/TRPO/PPO\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class NaturalGradient(Optimizer):",
            "",
            "applied = self.apply_step(variables=variables, diffs=estimated_diffs)",
            "",
            "-            with tf.control_dependencies(control_inputs=applied):",
            "+            with tf.control_dependencies(control_inputs=(applied,)):",
            "return [tf.identity(input=estimated_diff) for estimated_diff in estimated_diffs]",
            "",
            "def false_fn():"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('tuple', None), position=2, insert_id=2242709)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=2242710)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=identifier, text=applied), position=1)",
            "Insert(target_node=IN(type=tuple), node=(',', ','), position=2, insert_id=2242711)",
            "Insert(target_node=IN(type=tuple), node=(')', ')'), position=3, insert_id=2242712)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2603,
        "neg_line": [
            "-with tf.control_dependencies(control_inputs=applied):"
        ],
        "pos_line": [
            "+with tf.control_dependencies(control_inputs=(applied,)):"
        ],
        "core_change": "-with tf.control_dependencies(control_inputs=applied): +with tf.control_dependencies(control_inputs=(applied,)):",
        "core_API": "apply_step"
    },
    {
        "commit_hash": "6422ee88a9a3bdfd93ec98d2def1e9f2b02eacab",
        "index": "d4536e0c..eacf7b5f 100644",
        "commit_message": "Replace Distribution+TorchDistribution with a thin Distribution mixin (#769)\n\n* Sketch Distribution class as mixin\n\n* Remove TorchDistribution class\n\n* Simplify TransformedDistribution\n\n* Update torch wrappers for most distributions\n\n* Fix docs\n\n* Use dist.Reshape() to set extra_event_dims\n\n* Fix bugs in Reshape distribution\n\n* Fix rejector tests\n\n* Update _Subsample distribution\n\n* Use .reshape() method for extra_event_dims and sample_shape\n\n* Refactor Distribution -> TorchDistribution class hierarchy\n\n* Update docs\n\n* Fix json error in air.ipynb\n\n* Fix bugs in air.ipynb and abstract_infer.py\n\n* Fix distributions docs\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Rejector(Distribution):",
            "self._propose_batch_log_pdf_cache = x, self.propose.log_prob(x)",
            "return self._propose_batch_log_pdf_cache[1]",
            "",
            "-    def sample(self, sample_shape=torch.Size()):",
            "+    def rsample(self, sample_shape=torch.Size()):",
            "# Implements parallel batched accept-reject sampling.",
            "x = self.propose(sample_shape) if sample_shape else self.propose()",
            "log_prob_accept = self.log_prob_accept(x)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=sample), value='rsample')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2605,
        "neg_line": [
            "-def sample(self, sample_shape=torch.Size()):"
        ],
        "pos_line": [
            "+def rsample(self, sample_shape=torch.Size()):"
        ],
        "core_change": "-def sample(self, sample_shape=torch.Size()): +def rsample(self, sample_shape=torch.Size()):",
        "core_API": "log_prob"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "06676b7e..693dbf0e 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_conditional_whiten(Xnew, X, kernel, f_loc, f_scale_tril, loc, cov):",
            "loc0, cov0 = conditional(Xnew, X, kernel, f_loc, f_scale_tril, full_cov=True,",
            "whiten=False)",
            "Kff = kernel(X) + torch.eye(3) * 1e-6",
            "-    Lff = Kff.cholesky()",
            "+    Lff = torch.linalg.cholesky(Kff)",
            "whiten_f_loc = Lff.inverse().matmul(f_loc)",
            "whiten_f_scale_tril = Lff.inverse().matmul(f_scale_tril)",
            "loc1, cov1 = conditional(Xnew, X, kernel, whiten_f_loc, whiten_f_scale_tril,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=677056)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=677057)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=Kff), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=677058)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=677059)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 2606,
        "neg_line": [
            "-Lff = Kff.cholesky()"
        ],
        "pos_line": [
            "+Lff = torch.linalg.cholesky(Kff)"
        ],
        "core_change": "-Lff = Kff.cholesky() +Lff = torch.linalg.cholesky(Kff)",
        "core_API": "eye"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "d0899428d..fcffdfc5e 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario ako <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class MsTerms(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('ms_terms', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format(",
            "-                    path_to_manual_file, _FILENAME, self.manual_download_instructions",
            "-                )",
            "+                f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('ms_terms', data_dir=...)` that includes a file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions})\"",
            ")",
            "return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"path\": path_to_manual_file})]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'ms_terms\\', data_dir=...)` that includes a file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions})\"'), position=1, insert_id=1781616)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('ms_terms', data_dir=...)` that includes a file name {}. Manual download instructions: {})\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=path_to_manual_file))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=_FILENAME))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 2612,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('ms_terms', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format(",
            "-path_to_manual_file, _FILENAME, self.manual_download_instructions",
            "-)"
        ],
        "pos_line": [
            "+f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('ms_terms', data_dir=...)` that includes a file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions})\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('ms_terms', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format( -path_to_manual_file, _FILENAME, self.manual_download_instructions -) +f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('ms_terms', data_dir=...)` that includes a file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions})\"",
        "core_API": "exists"
    },
    {
        "commit_hash": "3a65e5507ce690d7e149ee2978639f598b9c3465",
        "index": "65654aaa8f..04843cef5d 100644",
        "commit_message": "Reformatting Eigvalsh (#6930)\n\n* changes for issue #6539\n\n* fixing docstring issues for eigvalsh\n\n* removed docstring blank line\n\n* lint test fixes\n\n* lint fixes\n\n* more lint fixes\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "eigh.support_native_out = True",
            "",
            "@with_unsupported_dtypes({\"1.11.0 and below\": (\"float16\", \"bfloat16\")}, backend_version)",
            "def eigvalsh(",
            "-    x: torch.Tensor, /, *, UPLO: Optional[str] = \"L\", out: Optional[torch.Tensor] = None",
            "+    x: torch.Tensor,",
            "+    /,",
            "+    *,",
            "+    UPLO: Optional[str] = \"L\",",
            "+    out: Optional[torch.Tensor] = None",
            ") -> torch.Tensor:",
            "return torch.linalg.eigvalsh(x, UPLO=UPLO, out=out)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2614,
        "neg_line": [
            "-x: torch.Tensor, /, *, UPLO: Optional[str] = \"L\", out: Optional[torch.Tensor] = None"
        ],
        "pos_line": [
            "+x: torch.Tensor,",
            "+/,",
            "+*,",
            "+UPLO: Optional[str] = \"L\",",
            "+out: Optional[torch.Tensor] = None"
        ],
        "core_change": "-x: torch.Tensor, /, *, UPLO: Optional[str] = \"L\", out: Optional[torch.Tensor] = None +x: torch.Tensor, +/, +*, +UPLO: Optional[str] = \"L\", +out: Optional[torch.Tensor] = None",
        "core_API": "eigvalsh"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "ef767d7e..5163ccd7 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class HardNet8(nn.Module):",
            "# use torch.hub to load pretrained model",
            "if pretrained:",
            "storage_fcn: Callable = lambda storage, loc: storage",
            "-            pretrained_dict = torch.hub.load_state_dict_from_url(",
            "-                urls['hardnet8v2'], map_location=storage_fcn",
            "-            )",
            "+            pretrained_dict = torch.hub.load_state_dict_from_url(urls['hardnet8v2'], map_location=storage_fcn)",
            "self.load_state_dict(pretrained_dict, strict=True)",
            "self.eval()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 2619,
        "neg_line": [
            "-pretrained_dict = torch.hub.load_state_dict_from_url(",
            "-urls['hardnet8v2'], map_location=storage_fcn",
            "-)"
        ],
        "pos_line": [
            "+pretrained_dict = torch.hub.load_state_dict_from_url(urls['hardnet8v2'], map_location=storage_fcn)"
        ],
        "core_change": "-pretrained_dict = torch.hub.load_state_dict_from_url( -urls['hardnet8v2'], map_location=storage_fcn -) +pretrained_dict = torch.hub.load_state_dict_from_url(urls['hardnet8v2'], map_location=storage_fcn)",
        "core_API": "load_state_dict_from_url"
    },
    {
        "commit_hash": "ffa518016df5470b3bd89a19a65b4b0fa13b8da1",
        "index": "b8cbd2b9..db88bb74 100644",
        "commit_message": "fix filter size\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestFilter2D:",
            "assert_allclose(actual, expected)",
            "",
            "def test_even_sized_filter(self, device):",
            "-        kernel = torch.ones(1, 4, 4).to(device)",
            "+        kernel = torch.ones(1, 2, 2).to(device)",
            "input = torch.tensor([[[",
            "[0., 0., 0., 0., 0.],",
            "[0., 0., 0., 0., 0.],"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=4), value='2')",
            "Update(target_node=ASTNode(type=integer, text=4), value='2')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 2620,
        "neg_line": [
            "-kernel = torch.ones(1, 4, 4).to(device)"
        ],
        "pos_line": [
            "+kernel = torch.ones(1, 2, 2).to(device)"
        ],
        "core_change": "-kernel = torch.ones(1, 4, 4).to(device) +kernel = torch.ones(1, 2, 2).to(device)",
        "core_API": "ones"
    },
    {
        "commit_hash": "52c17c905c45d3d6494bbd31c959c1c0ad42780c",
        "index": "10aaa063..54c191ee 100644",
        "commit_message": "Refactor unnecessary `else` / `elif` when `if` block has a `return` statement (#1072)\n\nCo-authored-by: deepsource-autofix[bot] <62050782+deepsource-autofix[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def rotation_matrix_to_quaternion(",
            "qz = 0.25 * sq",
            "if order == QuaternionCoeffOrder.XYZW:",
            "return torch.cat((qx, qy, qz, qw), dim=-1)",
            "-        else:",
            "-            return torch.cat((qw, qx, qy, qz), dim=-1)",
            "+        return torch.cat((qw, qx, qy, qz), dim=-1)",
            "",
            "where_2 = torch.where(m11 > m22, cond_2(), cond_3())",
            "where_1 = torch.where((m00 > m11) & (m00 > m22), cond_1(), where_2)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=qz), position=3)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type==, text==), position=4)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=binary_operator), position=5)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=if, text=if), position=6)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=ERROR), position=7)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=call), position=8)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=comparison_operator), position=0)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=call), position=3)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'return'), position=4, insert_id=423547)",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=else, text=else))",
            "Delete(target_node=ASTNode(type=conditional_expression))",
            "Delete(target_node=ASTNode(type=default_parameter))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 15,
        "number": 2626,
        "neg_line": [
            "-else:",
            "-return torch.cat((qw, qx, qy, qz), dim=-1)"
        ],
        "pos_line": [
            "+return torch.cat((qw, qx, qy, qz), dim=-1)"
        ],
        "core_change": "-else: -return torch.cat((qw, qx, qy, qz), dim=-1) +return torch.cat((qw, qx, qy, qz), dim=-1)",
        "core_API": "cat"
    },
    {
        "commit_hash": "a48886cb75b647131664ff2f92db205921bf6c5d",
        "index": "a155f10..a06e2c1 100644",
        "commit_message": "Fix Accuracy metric (#186)\n\n\n",
        "file": "segmentation_models.pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def accuracy(pr, gt, threshold=0.5, ignore_channels=None):",
            "pr = _threshold(pr, threshold=threshold)",
            "pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)",
            "",
            "-    tp = torch.sum(gt == pr)",
            "+    tp = torch.sum(gt == pr, dtype=pr.dtype)",
            "score = tp / gt.view(-1).shape[0]",
            "return score"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1131175)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1131176)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1131177)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1131178)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1131179)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'pr'), position=0, insert_id=1131180)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1131181)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1131182)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2627,
        "neg_line": [
            "-tp = torch.sum(gt == pr)"
        ],
        "pos_line": [
            "+tp = torch.sum(gt == pr, dtype=pr.dtype)"
        ],
        "core_change": "-tp = torch.sum(gt == pr) +tp = torch.sum(gt == pr, dtype=pr.dtype)",
        "core_API": "sum"
    },
    {
        "commit_hash": "3bbda81121ab16f9f788181ab60f741c596b6cea",
        "index": "1464baee..e9fcfa20 100644",
        "commit_message": "Function contains unused argument (#1240)\n\n* Function contains unused argument\n\n* fix linter\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def unsharp_mask(",
            ">>> output.shape",
            "torch.Size([2, 4, 5, 5])",
            "\"\"\"",
            "-    data_blur: torch.Tensor = gaussian_blur2d(input, kernel_size, sigma)",
            "+    data_blur: torch.Tensor = gaussian_blur2d(input, kernel_size, sigma, border_type)",
            "data_sharpened: torch.Tensor = input + (input - data_blur)",
            "return data_sharpened"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=419450)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'border_type'), position=7, insert_id=419451)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 2628,
        "neg_line": [
            "-data_blur: torch.Tensor = gaussian_blur2d(input, kernel_size, sigma)"
        ],
        "pos_line": [
            "+data_blur: torch.Tensor = gaussian_blur2d(input, kernel_size, sigma, border_type)"
        ],
        "core_change": "-data_blur: torch.Tensor = gaussian_blur2d(input, kernel_size, sigma) +data_blur: torch.Tensor = gaussian_blur2d(input, kernel_size, sigma, border_type)",
        "core_API": "Size"
    },
    {
        "commit_hash": "7a9a08c5d3ca4699fb439f691c40e1320b37507a",
        "index": "3b20c5335..e8fc243f4 100644",
        "commit_message": "Drop torch 1.6 testing (#10390)\n\n* Drop torch 1.6 support\n\n* Drop 1.6 support\n\n* Update CHANGELOG\n\n* Fixes\n\n* Split change\n\n* Undo change\n\n* 1.7 -> 1.7.1\n\nhttps://github.com/pytorch/pytorch/issues/47354\n\n* Force trigger nightly\n\n* Update .github/workflows/events-nightly.yml\n\nCo-authored-by: Aki Nitta <nitta@akihironitta.com>\n\n* Revert 1.7.1 change - try wildcard\n\n* Update adjust versions and test it\n\n* Undo test changes\n\n* Revert \"Undo test changes\"\n\nThis reverts commit 3a6acadd115e86f02d83a788f1978372ab6764f3.\n\n* Update CHANGELOG.md\n\nCo-authored-by: Aki Nitta <nitta@akihironitta.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def pl_worker_init_function(worker_id: int, rank: Optional[int] = None) -> None:",
            "np.random.seed(ss.generate_state(4))",
            "# Spawn distinct SeedSequences for the PyTorch PRNG and the stdlib random module",
            "torch_ss, stdlib_ss = ss.spawn(2)",
            "-    # PyTorch 1.7 and above takes a 64-bit seed",
            "-    dtype = np.uint64 if _TORCH_GREATER_EQUAL_1_7 else np.uint32",
            "-    torch.manual_seed(torch_ss.generate_state(1, dtype=dtype)[0])",
            "+    torch.manual_seed(torch_ss.generate_state(1, dtype=np.uint64)[0])",
            "# use 128 bits expressed as an integer",
            "stdlib_seed = (stdlib_ss.generate_state(2, dtype=np.uint64).astype(object) * [1 << 64, 1]).sum()",
            "random.seed(stdlib_seed)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=_TORCH_GREATER_EQUAL_1_7))",
            "Delete(target_node=ASTNode(type=else, text=else))",
            "Delete(target_node=ASTNode(type=identifier, text=np))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=uint32))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=conditional_expression))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 14,
        "number": 2629,
        "neg_line": [
            "-# PyTorch 1.7 and above takes a 64-bit seed",
            "-dtype = np.uint64 if _TORCH_GREATER_EQUAL_1_7 else np.uint32",
            "-torch.manual_seed(torch_ss.generate_state(1, dtype=dtype)[0])"
        ],
        "pos_line": [
            "+torch.manual_seed(torch_ss.generate_state(1, dtype=np.uint64)[0])"
        ],
        "core_change": "-# PyTorch 1.7 and above takes a 64-bit seed -dtype = np.uint64 if _TORCH_GREATER_EQUAL_1_7 else np.uint32 -torch.manual_seed(torch_ss.generate_state(1, dtype=dtype)[0]) +torch.manual_seed(torch_ss.generate_state(1, dtype=np.uint64)[0])",
        "core_API": "seed"
    },
    {
        "commit_hash": "7fb1eb6ff31fafc373e757f4ada3d6780df4ce67",
        "index": "3f61bbb6..ac5050c2 100644",
        "commit_message": "Unknown label support in `to_homogeneous` (#5540)\n\n* Update hetero_data.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update hetero_data.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update hetero_data.py\n\n* Update torch_geometric/data/hetero_data.py\n\nCo-authored-by: Padarn Wilson <padarn.wilson@grabtaxi.com>\n\n* Update hetero_data.py\n\n* Update hetero_data.py\n\n* test\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update hetero_data.py\n\n* Update test_hetero_data.py\n\n* Update test_hetero_data.py\n\n* Update test_hetero_data.py\n\n* Update test_hetero_data.py\n\n* Update test_hetero_data.py\n\n* Update test_hetero_data.py\n\n* update\n\n* update\n\n* update\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Padarn Wilson <padarn.wilson@grabtaxi.com>\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_to_homogeneous_and_vice_versa():",
            "del out._edge_type_names",
            "del out._node_type_names",
            "out = out.to_heterogeneous(node_type, edge_type)",
            "-    assert len(out) == 4",
            "+    assert len(out) == 5",
            "assert torch.allclose(data['paper'].x, out['0'].x)",
            "assert torch.allclose(data['author'].x, out['1'].x)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=4), value='5')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2631,
        "neg_line": [
            "-assert len(out) == 4"
        ],
        "pos_line": [
            "+assert len(out) == 5"
        ],
        "core_change": "-assert len(out) == 4 +assert len(out) == 5",
        "core_API": "to_heterogeneous"
    },
    {
        "commit_hash": "3b2654203db6954f47a250e3703285af369ea33a",
        "index": "e25cb467..cd023a92 100644",
        "commit_message": "fixing size mismatch\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Tacotron(nn.Module):",
            "self.encoder = Encoder(embedding_dim)",
            "self.decoder = Decoder(256, mel_dim, r)",
            "self.postnet = PostCBHG(mel_dim)",
            "-        self.last_linear = nn.Linear(256, linear_dim)",
            "+        self.last_linear = nn.Linear(self.postnet.cbhg.gru_features * 2, linear_dim)",
            "",
            "def forward(self, characters, mel_specs=None, mask=None):",
            "B = characters.size(0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=1559097)",
            "Insert(target_node=IN(type=binary_operator), node=('attribute', None), position=0, insert_id=1559098)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=1559099)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '2'), position=2, insert_id=1559100)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1559101)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1559102)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'gru_features'), position=2, insert_id=1559103)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1559104)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1559105)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cbhg'), position=2, insert_id=1559106)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1559107)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1559108)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'postnet'), position=2, insert_id=1559109)",
            "Delete(target_node=ASTNode(type=integer, text=256))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 2633,
        "neg_line": [
            "-self.last_linear = nn.Linear(256, linear_dim)"
        ],
        "pos_line": [
            "+self.last_linear = nn.Linear(self.postnet.cbhg.gru_features * 2, linear_dim)"
        ],
        "core_change": "-self.last_linear = nn.Linear(256, linear_dim) +self.last_linear = nn.Linear(self.postnet.cbhg.gru_features * 2, linear_dim)",
        "core_API": "Linear"
    },
    {
        "commit_hash": "c7fd1d9fb18862318b133c9474d37c5085850070",
        "index": "4ef0e6b2..c4d4ddf0 100755",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def resample(img, flow):",
            "img_flat = tf.reshape(tf.transpose(img, [0, 2, 3, 1]), [-1, c])",
            "",
            "dx, dy = tf.unstack(flow, axis=1)",
            "-    xf, yf = tf.meshgrid(tf.to_float(tf.range(w)), tf.to_float(tf.range(h)))",
            "+    xf, yf = tf.meshgrid(tf.cast(tf.range(w), tf.float32), tf.cast(tf.range(h), tf.float32))",
            "xf = xf + dx",
            "yf = yf + dy"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=to_float), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2278914)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2278915)",
            "Update(target_node=ASTNode(type=identifier, text=to_float), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2278916)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2278917)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2278918)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2278919)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2278920)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2278921)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2278922)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2278923)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 2635,
        "neg_line": [
            "-xf, yf = tf.meshgrid(tf.to_float(tf.range(w)), tf.to_float(tf.range(h)))"
        ],
        "pos_line": [
            "+xf, yf = tf.meshgrid(tf.cast(tf.range(w), tf.float32), tf.cast(tf.range(h), tf.float32))"
        ],
        "core_change": "-xf, yf = tf.meshgrid(tf.to_float(tf.range(w)), tf.to_float(tf.range(h))) +xf, yf = tf.meshgrid(tf.cast(tf.range(w), tf.float32), tf.cast(tf.range(h), tf.float32))",
        "core_API": "reshape"
    },
    {
        "commit_hash": "9e7e64edcc014bf6997397800cb6807ed28cf331",
        "index": "f7be2b3670..06e345c140 100644",
        "commit_message": "fix core test for `shape`.\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def shape(",
            "as_array: bool = False,",
            ") -> Union[tf.Tensor, ivy.Shape, ivy.Array]:",
            "if as_array:",
            "-        return ivy.array(tf.shape(x))",
            "+        return ivy.array(tf.shape(x), dtype=ivy.default_int_dtype())",
            "else:",
            "return ivy.Shape(x.shape)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1996944)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1996945)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=1996946)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1996947)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1996948)",
            "Insert(target_node=IN(type=keyword_argument), node=('call', None), position=2, insert_id=1996949)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1996950)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1996951)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'ivy'), position=0, insert_id=1996952)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1996953)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'default_int_dtype'), position=2, insert_id=1996954)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1996955)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=1)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 2636,
        "neg_line": [
            "-return ivy.array(tf.shape(x))"
        ],
        "pos_line": [
            "+return ivy.array(tf.shape(x), dtype=ivy.default_int_dtype())"
        ],
        "core_change": "-return ivy.array(tf.shape(x)) +return ivy.array(tf.shape(x), dtype=ivy.default_int_dtype())",
        "core_API": "array"
    },
    {
        "commit_hash": "a4d5b59f132126c06c1a6b1f266ee44c70440cce",
        "index": "adbadbea..009db156 100644",
        "commit_message": "Refactor Pipelines / Community pipelines and add better explanations. (#257)\n\n* [Examples readme]\n\n* Improve\n\n* more\n\n* save\n\n* save\n\n* save more\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\nCo-authored-by: Nathan Lambert <nathan@huggingface.co>\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* up\n\n* make deterministic\n\n* up\n\n* better\n\n* up\n\n* add generator to img2img pipe\n\n* save\n\n* make pipelines deterministic\n\n* Update src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py\n\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\n\n* apply all changes\n\n* more correctnios\n\n* finish\n\n* improve table\n\n* more fixes\n\n* up\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\n\n* Update src/diffusers/pipelines/README.md\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* add better links\n\n* fix more\n\n* finish\n\nCo-authored-by: Nathan Lambert <nathan@huggingface.co>\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DiagonalGaussianDistribution(object):",
            "if self.deterministic:",
            "self.var = self.std = torch.zeros_like(self.mean).to(device=self.parameters.device)",
            "",
            "-    def sample(self):",
            "-        x = self.mean + self.std * torch.randn(self.mean.shape).to(device=self.parameters.device)",
            "+    def sample(self, generator=None):",
            "+        x = self.mean + self.std * torch.randn(self.mean.shape, generator=generator, device=self.parameters.device)",
            "return x",
            "",
            "def kl(self, other=None):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=2, insert_id=106289)",
            "Insert(target_node=ASTNode(type=parameters), node=('default_parameter', None), position=3, insert_id=106290)",
            "Insert(target_node=IN(type=default_parameter), node=('identifier', 'generator'), position=0, insert_id=106291)",
            "Insert(target_node=IN(type=default_parameter), node=('=', '='), position=1, insert_id=106292)",
            "Insert(target_node=IN(type=default_parameter), node=('none', 'None'), position=2, insert_id=106293)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=106294)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=106295)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=106296)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'generator'), position=0, insert_id=106297)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=106298)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'generator'), position=2, insert_id=106299)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=to))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 2637,
        "neg_line": [
            "-def sample(self):",
            "-x = self.mean + self.std * torch.randn(self.mean.shape).to(device=self.parameters.device)"
        ],
        "pos_line": [
            "+def sample(self, generator=None):",
            "+x = self.mean + self.std * torch.randn(self.mean.shape, generator=generator, device=self.parameters.device)"
        ],
        "core_change": "-def sample(self): -x = self.mean + self.std * torch.randn(self.mean.shape).to(device=self.parameters.device) +def sample(self, generator=None): +x = self.mean + self.std * torch.randn(self.mean.shape, generator=generator, device=self.parameters.device)",
        "core_API": "zeros_like"
    },
    {
        "commit_hash": "306db252a7c4d2ff3f66b30aae3b37a72737ee3b",
        "index": "3b8d887aa..b7d09e41d 100644",
        "commit_message": "fixes and clean-up\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Conv2dSubsampling(torch.nn.Module):",
            "if self.output is not None:",
            "sequence = self.output(sequence)",
            "",
            "-        if mask is not None:",
            "-            return sequence, self.create_new_mask(mask)",
            "-",
            "-        return sequence, None",
            "+        return sequence, self.create_new_mask(mask)",
            "",
            "def create_new_conformer_mask(self, mask: torch.Tensor) -> torch.Tensor:",
            "\"\"\"Create new conformer mask for output sequences."
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=return_statement), position=3)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=mask))",
            "Delete(target_node=ASTNode(type=is not, text=is))",
            "Delete(target_node=ASTNode(type=is not, text=not))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if_statement))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=sequence))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=expression_list))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 2638,
        "neg_line": [
            "-if mask is not None:",
            "-return sequence, self.create_new_mask(mask)",
            "-",
            "-return sequence, None"
        ],
        "pos_line": [
            "+return sequence, self.create_new_mask(mask)"
        ],
        "core_change": "-if mask is not None: -return sequence, self.create_new_mask(mask) - -return sequence, None +return sequence, self.create_new_mask(mask)",
        "core_API": "output"
    },
    {
        "commit_hash": "0a3a987eaeefbbe1421574f82ebe24e71e9b7883",
        "index": "ee495ac64..dbfac4c3e 100755",
        "commit_message": "Really fix import\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "# extract",
            "logging.info('backend = ' + args.backend)",
            "if args.backend == \"pytorch\":",
            "-        fromespnet.lmpytorch.tts_pytorch import decode",
            "+        from espnet.lmpytorch.tts_pytorch import decode",
            "decode(args)",
            "else:",
            "raise NotImplementedError(\"Only pytorch is supported.\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('block', None), position=3, insert_id=178860)",
            "Insert(target_node=IN(type=block), node=('import_from_statement', None), position=0, insert_id=178861)",
            "Insert(target_node=IN(type=import_from_statement), node=('from', 'from'), position=0, insert_id=178862)",
            "Insert(target_node=IN(type=import_from_statement), node=('dotted_name', None), position=1, insert_id=178863)",
            "Move(target_node=IN(type=import_from_statement), node=ASTNode(type=import, text=import), position=2)",
            "Move(target_node=IN(type=import_from_statement), node=ASTNode(type=dotted_name), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=fromespnet), value='espnet')",
            "Move(target_node=IN(type=dotted_name), node=ASTNode(type=identifier, text=fromespnet), position=0)",
            "Move(target_node=IN(type=dotted_name), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=dotted_name), node=ASTNode(type=identifier, text=lmpytorch), position=2)",
            "Move(target_node=IN(type=dotted_name), node=ASTNode(type=., text=.), position=3)",
            "Move(target_node=IN(type=dotted_name), node=ASTNode(type=identifier, text=tts_pytorch), position=4)",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=import_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2641,
        "neg_line": [
            "-fromespnet.lmpytorch.tts_pytorch import decode"
        ],
        "pos_line": [
            "+from espnet.lmpytorch.tts_pytorch import decode"
        ],
        "core_change": "-fromespnet.lmpytorch.tts_pytorch import decode +from espnet.lmpytorch.tts_pytorch import decode",
        "core_API": "info"
    },
    {
        "commit_hash": "5fa0b17c3d10cdb6411a173a7dce42b0de56a8f2",
        "index": "aa17f4ba6..9fe18fdf5 100644",
        "commit_message": "[Past CI]  Leave Past CI failures in the past   (#20861)\n\n* torch.jit._state\n\n* Fix past CI\n\n* Fix for perceiver\n\n* Fix REALM\n\n* Fix for Bloom\n\n* Fix for SwinMode\n\n* Fix for TrajectoryTransformerModel\n\n* Fix for test_wav2vec2_with_lm\n\n* make style\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "class Wav2Vec2ModelIntegrationTest(unittest.TestCase):",
            "",
            "@require_pyctcdecode",
            "@require_torchaudio",
            "+    @unittest.skipIf(",
            "+        is_torch_less_than_1_9,",
            "+        reason=\"`torchaudio.functional.resample` needs torchaudio >= 0.9 which requires torch >= 0.9\",",
            "+    )",
            "def test_wav2vec2_with_lm_pool(self):",
            "ds = load_dataset(\"common_voice\", \"es\", split=\"test\", streaming=True)",
            "sample = next(iter(ds))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=decorated_definition), node=('decorator', None), position=2, insert_id=1754533)",
            "Insert(target_node=IN(type=decorator), node=('@', '@'), position=0, insert_id=1754534)",
            "Insert(target_node=IN(type=decorator), node=('call', None), position=1, insert_id=1754535)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1754536)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1754537)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'unittest'), position=0, insert_id=1754538)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1754539)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'skipIf'), position=2, insert_id=1754540)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1754541)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'is_torch_less_than_1_9'), position=1, insert_id=1754542)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1754543)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1754544)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=1754545)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=5, insert_id=1754546)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'reason'), position=0, insert_id=1754547)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1754548)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', '\"`torchaudio.functional.resample` needs torchaudio >= 0.9 which requires torch >= 0.9\"'), position=2, insert_id=1754549)"
        ],
        "plus_line": 4,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 2644,
        "neg_line": [],
        "pos_line": [
            "+@unittest.skipIf(",
            "+is_torch_less_than_1_9,",
            "+reason=\"`torchaudio.functional.resample` needs torchaudio >= 0.9 which requires torch >= 0.9\",",
            "+)"
        ],
        "core_change": "+@unittest.skipIf( +is_torch_less_than_1_9, +reason=\"`torchaudio.functional.resample` needs torchaudio >= 0.9 which requires torch >= 0.9\", +)",
        "core_API": "skipIf"
    },
    {
        "commit_hash": "e67b1378fa573a50cb2b683b85edcf74b26704c3",
        "index": "253aaaa..7e2075a 100644",
        "commit_message": "Fix network for TF 0.10\n\n",
        "file": "tensorflow-wavenet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class WaveNet(object):",
            "shifted = tf.slice(encoded, [0, 0, 1, 0], [-1, -1, tf.shape(encoded)[2] - 1, -1])",
            "shifted = tf.pad(shifted, [[0, 0], [0, 0], [0, 1], [0, 0]])",
            "",
            "-            loss = tf.nn.softmax_cross_entropy_with_logits(raw_output, tf.reshape(shifted, [-1, self.channels]))",
            "+            prediction = tf.reshape(raw_output, [-1, self.channels])",
            "+            loss = tf.nn.softmax_cross_entropy_with_logits(prediction, tf.reshape(shifted, [-1, self.channels]))",
            "reduced_loss =  tf.reduce_mean(loss)",
            "",
            "tf.scalar_summary('loss', reduced_loss)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=2208869)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2208870)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'prediction'), position=0, insert_id=2208871)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2208872)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2208873)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2208874)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2208875)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2208876)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2208877)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reshape'), position=2, insert_id=2208878)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2208879)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'raw_output'), position=1, insert_id=2208880)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2208881)",
            "Insert(target_node=IN(type=argument_list), node=('list', None), position=3, insert_id=2208882)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2208883)",
            "Update(target_node=ASTNode(type=identifier, text=raw_output), value='prediction')",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=2208884)",
            "Insert(target_node=IN(type=list), node=('unary_operator', '-1'), position=1, insert_id=2208885)",
            "Insert(target_node=IN(type=list), node=(',', ','), position=2, insert_id=2208886)",
            "Insert(target_node=IN(type=list), node=('attribute', None), position=3, insert_id=2208887)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=4, insert_id=2208888)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2208889)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2208890)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'channels'), position=2, insert_id=2208891)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 2645,
        "neg_line": [
            "-loss = tf.nn.softmax_cross_entropy_with_logits(raw_output, tf.reshape(shifted, [-1, self.channels]))"
        ],
        "pos_line": [
            "+prediction = tf.reshape(raw_output, [-1, self.channels])",
            "+loss = tf.nn.softmax_cross_entropy_with_logits(prediction, tf.reshape(shifted, [-1, self.channels]))"
        ],
        "core_change": "-loss = tf.nn.softmax_cross_entropy_with_logits(raw_output, tf.reshape(shifted, [-1, self.channels])) +prediction = tf.reshape(raw_output, [-1, self.channels]) +loss = tf.nn.softmax_cross_entropy_with_logits(prediction, tf.reshape(shifted, [-1, self.channels]))",
        "core_API": "slice"
    },
    {
        "commit_hash": "fbe42589b828be246359a97b602fc4bd9badfcd1",
        "index": "5b6f0f08..b9c63240 100644",
        "commit_message": "Test github actions for CI (#2531)\n\n* Test github actions for CI\n\n* Update ci.yml\n\n* Update ci.yml\n\n* Update ci.yml\n\n* remove .travis.yml; add ninja\n\n* modify readme; add coveralls\n\n* fix multiple dep in finish\n\n* change to coveralls token\n\n* attempt to fix coverall agg\n\n* add coverage badge\n\n* update python, pytorch versions\n\n* Add license for lint\n\n* update wheel, setuptools\n\n* run on ubuntu 20.04\n\n* add python headers\n\n* debug issues with build\n\n* use python3-dev\n\n* debug by adding numpy\n\n* install cibuildwheel\n\n* comment out dependency no lap\n\n* Install gcc-8 and g++8; reenable lap dependency\n\n* Install g++ during docs stage\n\n* Install gcc before using pip\n\n* update torch dep for funsor\n\n* Attempt to fix test_minipyro.py\n\nCo-authored-by: Neeraj Pradhan <neerajprad@fb.com>\nCo-authored-by: Fritz Obermeyer <fritz.obermeyer@gmail.com>\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def elbo_test_case(backend, jit, expected_elbo, data, steps=None):",
            "if backend == \"pyro\":",
            "# TODO: this is a difference between the two implementations",
            "elbo = elbo.loss",
            "-        assert elbo(constrained_model, guide_constrained_model, data) == approx(expected_elbo, rel=0.1)",
            "+        with torch.no_grad():",
            "+            actual = elbo(constrained_model, guide_constrained_model, data)",
            "+        assert actual == approx(expected_elbo, rel=0.1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('with_statement', None), position=3, insert_id=681909)",
            "Insert(target_node=IN(type=with_statement), node=('with', 'with'), position=0, insert_id=681910)",
            "Insert(target_node=IN(type=with_statement), node=('with_clause', None), position=1, insert_id=681911)",
            "Insert(target_node=IN(type=with_statement), node=(':', ':'), position=2, insert_id=681912)",
            "Insert(target_node=IN(type=with_statement), node=('block', None), position=3, insert_id=681913)",
            "Insert(target_node=IN(type=with_clause), node=('with_item', None), position=0, insert_id=681914)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=681915)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('identifier', 'actual'), position=0, insert_id=681916)",
            "Insert(target_node=IN(type=with_item), node=('call', None), position=0, insert_id=681917)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=681918)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=681919)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=681920)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'actual'), position=0, insert_id=681921)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=681922)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=681923)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=681924)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'no_grad'), position=2, insert_id=681925)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=681926)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=681927)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 2646,
        "neg_line": [
            "-assert elbo(constrained_model, guide_constrained_model, data) == approx(expected_elbo, rel=0.1)"
        ],
        "pos_line": [
            "+with torch.no_grad():",
            "+actual = elbo(constrained_model, guide_constrained_model, data)",
            "+assert actual == approx(expected_elbo, rel=0.1)"
        ],
        "core_change": "-assert elbo(constrained_model, guide_constrained_model, data) == approx(expected_elbo, rel=0.1) +with torch.no_grad(): +actual = elbo(constrained_model, guide_constrained_model, data) +assert actual == approx(expected_elbo, rel=0.1)",
        "core_API": "no_grad"
    },
    {
        "commit_hash": "4f0f9cb348eef16198309a679f3af5fd2132ac73",
        "index": "41528ef..6a7face 100644",
        "commit_message": "Fix #954 by bringing traceable _assert into timm to allow compat w/ PyTorch < 1.8\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PatchEmbed(nn.Module):",
            "",
            "def forward(self, x):",
            "B, C, H, W = x.shape",
            "-        torch._assert(H == self.img_size[0], f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\")",
            "-        torch._assert(W == self.img_size[1], f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\")",
            "+        _assert(H == self.img_size[0], f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\")",
            "+        _assert(W == self.img_size[1], f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\")",
            "x = self.proj(x)",
            "if self.flatten:",
            "x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=_assert), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=_assert), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 2647,
        "neg_line": [
            "-torch._assert(H == self.img_size[0], f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\")",
            "-torch._assert(W == self.img_size[1], f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\")"
        ],
        "pos_line": [
            "+_assert(H == self.img_size[0], f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\")",
            "+_assert(W == self.img_size[1], f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\")"
        ],
        "core_change": "-torch._assert(H == self.img_size[0], f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\") -torch._assert(W == self.img_size[1], f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\") +_assert(H == self.img_size[0], f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\") +_assert(W == self.img_size[1], f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\")",
        "core_API": "_assert"
    },
    {
        "commit_hash": "c5b6ceea4a0a151ab78b9c065584bc416d1ee275",
        "index": "97d7db10..0cbc0808 100644",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class Layer_Pooling_Test(CustomTestCase):",
            "cls.network = tl.layers.DenseLayer(cls.network, 800, tf.nn.relu, name='relu2')",
            "cls.network = tl.layers.DropoutLayer(cls.network, keep=0.5, name='drop3')",
            "",
            "-        cls.network = tl.layers.DenseLayer(cls.network, n_units=10, act=tf.identity, name='output')",
            "+        cls.network = tl.layers.DenseLayer(cls.network, n_units=10, name='output')",
            "",
            "# define cost function and metric.",
            "cls.y = cls.network.outputs"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=act))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=identity))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2648,
        "neg_line": [
            "-cls.network = tl.layers.DenseLayer(cls.network, n_units=10, act=tf.identity, name='output')"
        ],
        "pos_line": [
            "+cls.network = tl.layers.DenseLayer(cls.network, n_units=10, name='output')"
        ],
        "core_change": "-cls.network = tl.layers.DenseLayer(cls.network, n_units=10, act=tf.identity, name='output') +cls.network = tl.layers.DenseLayer(cls.network, n_units=10, name='output')",
        "core_API": "DenseLayer"
    },
    {
        "commit_hash": "e61132c54b9a475865060fc2af5ca83ad091988e",
        "index": "d26757a9b..03ff6c62e 100644",
        "commit_message": "fixed style issues in syft/core/torch_/__init__.py\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "autograd",
        "change": [
            "class TorchHook(object):",
            "is_old = re.match('old*', attr) is not None",
            "",
            "# Where the overloading happens",
            "-            if ((is_desc or (is_func and not is_service_func))",
            "-                and not is_base and not is_old):",
            "+            if ((is_desc or (is_func and not is_service_func)) and not is_base and not is_old):",
            "passer = self.pass_method_args(lit)",
            "new_attr = self.overload_method(passer)",
            "-                setattr(torch.autograd.variable.Variable,",
            "-                    'old_{}'.format(attr), lit)",
            "+                setattr(torch.autograd.variable.Variable,",
            "+                        'old_{}'.format(attr), lit)",
            "setattr(torch.autograd.variable.Variable, attr, new_attr)",
            "",
            "self.hook_var_send_()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 2649,
        "neg_line": [
            "-if ((is_desc or (is_func and not is_service_func))",
            "-and not is_base and not is_old):",
            "-setattr(torch.autograd.variable.Variable,",
            "-'old_{}'.format(attr), lit)"
        ],
        "pos_line": [
            "+if ((is_desc or (is_func and not is_service_func)) and not is_base and not is_old):",
            "+setattr(torch.autograd.variable.Variable,",
            "+'old_{}'.format(attr), lit)"
        ],
        "core_change": "-if ((is_desc or (is_func and not is_service_func)) -and not is_base and not is_old): +if ((is_desc or (is_func and not is_service_func)) and not is_base and not is_old): -setattr(torch.autograd.variable.Variable, -'old_{}'.format(attr), lit) +setattr(torch.autograd.variable.Variable, +'old_{}'.format(attr), lit)",
        "core_API": "match"
    },
    {
        "commit_hash": "9ef1ee84552a82dd42dffc5c4dd5521e2cc6fa09",
        "index": "6409005..69844d9 100644",
        "commit_message": "coarse rasterization bug fix\n\nSummary:\nFix a bug which resulted in a rendering artifacts if the image size was not a multiple of 16.\nFix: Revert coarse rasterization to original implementation and only update fine rasterization to reverse the ordering of Y and X axis. This is much simpler than the previous approach!\n\nAdditional changes:\n- updated mesh rendering end-end tests to check outputs from both naive and coarse to fine rasterization.\n- added pointcloud rendering end-end tests\n\nReviewed By: gkioxari\n\nDifferential Revision: D21102725\n\nfbshipit-source-id: 2e7e1b013dd6dd12b3a00b79eb8167deddb2e89a\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestRasterizeMeshes(TestCaseMixin, unittest.TestCase):",
            "bin_size,",
            "max_faces_per_bin,",
            ")",
            "-        # Flip x and y axis of output before comparing to expected",
            "+",
            "bin_faces_same = (bin_faces.squeeze() == bin_faces_expected).all()",
            "self.assertTrue(bin_faces_same.item() == 1)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2650,
        "neg_line": [
            "-# Flip x and y axis of output before comparing to expected"
        ],
        "pos_line": [
            "+"
        ],
        "core_change": "-# Flip x and y axis of output before comparing to expected +",
        "core_API": "squeeze"
    },
    {
        "commit_hash": "a4c2391e5953ad9e097c1ee27aa0e91b17077109",
        "index": "31185c80f0..9f4de94c3b 100644",
        "commit_message": "Small Formatting fix\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def broadcast_to(",
            "shape: Union[ivy.NativeShape, Sequence[int]],",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "if tf.rank(x) > len(shape):",
            "-        return tf.broadcast_to(tf.reshape(x,-1), shape)",
            "+        return tf.broadcast_to(tf.reshape(x, -1), shape)",
            "return tf.broadcast_to(x, shape)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2651,
        "neg_line": [
            "-return tf.broadcast_to(tf.reshape(x,-1), shape)"
        ],
        "pos_line": [
            "+return tf.broadcast_to(tf.reshape(x, -1), shape)"
        ],
        "core_change": "-return tf.broadcast_to(tf.reshape(x,-1), shape) +return tf.broadcast_to(tf.reshape(x, -1), shape)",
        "core_API": "rank"
    },
    {
        "commit_hash": "6ba2231d7227825dfb76e9df161824d04234e69f",
        "index": "ef790f28..244987f8 100644",
        "commit_message": "Reproducibility 3/3 (#1924)\n\n* make tests deterministic\n\n* run slow tests\n\n* prepare for testing\n\n* finish\n\n* refactor\n\n* add print statements\n\n* finish more\n\n* correct some test failures\n\n* more fixes\n\n* set up to correct tests\n\n* more corrections\n\n* up\n\n* fix more\n\n* more prints\n\n* add\n\n* up\n\n* up\n\n* up\n\n* uP\n\n* uP\n\n* more fixes\n\n* uP\n\n* up\n\n* up\n\n* up\n\n* up\n\n* fix more\n\n* up\n\n* up\n\n* clean tests\n\n* up\n\n* up\n\n* up\n\n* more fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* make\n\n* correct\n\n* finish\n\n* finish\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LDMTextToImagePipelineNightlyTests(unittest.TestCase):",
            "torch.cuda.empty_cache()",
            "",
            "def get_inputs(self, device, dtype=torch.float32, seed=0):",
            "-        generator = torch.Generator(device=device).manual_seed(seed)",
            "+        generator = torch.manual_seed(seed)",
            "latents = np.random.RandomState(seed).standard_normal((1, 4, 32, 32))",
            "latents = torch.from_numpy(latents).to(device=device, dtype=dtype)",
            "inputs = {"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=Generator))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 2652,
        "neg_line": [
            "-generator = torch.Generator(device=device).manual_seed(seed)"
        ],
        "pos_line": [
            "+generator = torch.manual_seed(seed)"
        ],
        "core_change": "-generator = torch.Generator(device=device).manual_seed(seed) +generator = torch.manual_seed(seed)",
        "core_API": "empty_cache"
    },
    {
        "commit_hash": "2e2dd861f309f64bafbb4e3d0126e953f2ed4d18",
        "index": "17215cd3..8bf0a21c 100755",
        "commit_message": "Dist testing backend fixes, etc. (#708)\n\n\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def create_deepspeed_args():",
            "# We assume up to one full node executing unit tests",
            "assert torch.distributed.get_world_size() <= torch.cuda.device_count()",
            "args.local_rank = torch.distributed.get_rank()",
            "-    else:",
            "-        args.local_rank = 0",
            "return args"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=else))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=args))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=local_rank))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 2653,
        "neg_line": [
            "-else:",
            "-args.local_rank = 0"
        ],
        "pos_line": [],
        "core_change": "-else: -args.local_rank = 0",
        "core_API": "get_world_size"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "72c2d603..033e677d 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Entropy(Metric):",
            "def __call__(",
            "self,  # type: ignore",
            "logits: torch.Tensor,",
            "-        mask: Optional[torch.Tensor] = None,",
            "+        mask: Optional[torch.BoolTensor] = None,",
            "):",
            "\"\"\"",
            "# Parameters",
            "",
            "logits : `torch.Tensor`, required.",
            "A tensor of unnormalized log probabilities of shape (batch_size, ..., num_classes).",
            "-        mask : `torch.Tensor`, optional (default = None).",
            "+        mask : `torch.BoolTensor`, optional (default = None).",
            "A masking tensor of shape (batch_size, ...).",
            "\"\"\"",
            "logits, mask = self.detach_tensors(logits, mask)",
            "",
            "if mask is None:",
            "-            mask = torch.ones(logits.size()[:-1], device=logits.device)",
            "+            mask = torch.ones(logits.size()[:-1], device=logits.device).bool()",
            "",
            "log_probs = torch.nn.functional.log_softmax(logits, dim=-1)",
            "probabilities = torch.exp(log_probs) * mask.unsqueeze(-1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\n# Parameters\n\nlogits : `torch.Tensor`, required.\nA tensor of unnormalized log probabilities of shape (batch_size, ..., num_classes).\n        mask : `torch.Tensor`, optional (default = None).\nA masking tensor of shape (batch_size, ...).\n\"\"\"), value='\"\"\"\\n# Parameters\\n\\nlogits : `torch.Tensor`, required.\\nA tensor of unnormalized log probabilities of shape (batch_size, ..., num_classes).\\n        mask : `torch.BoolTensor`, optional (default = None).\\nA masking tensor of shape (batch_size, ...).\\n\"\"\"')",
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=20196)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=20197)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=20198)",
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='BoolTensor')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=20199)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=20200)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=20201)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=20202)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 10,
        "number": 2654,
        "neg_line": [
            "-mask: Optional[torch.Tensor] = None,",
            "-mask : `torch.Tensor`, optional (default = None).",
            "-mask = torch.ones(logits.size()[:-1], device=logits.device)"
        ],
        "pos_line": [
            "+mask: Optional[torch.BoolTensor] = None,",
            "+mask : `torch.BoolTensor`, optional (default = None).",
            "+mask = torch.ones(logits.size()[:-1], device=logits.device).bool()"
        ],
        "core_change": "-mask: Optional[torch.Tensor] = None, +mask: Optional[torch.BoolTensor] = None, -mask : `torch.Tensor`, optional (default = None). +mask : `torch.BoolTensor`, optional (default = None). -mask = torch.ones(logits.size()[:-1], device=logits.device) +mask = torch.ones(logits.size()[:-1], device=logits.device).bool()",
        "core_API": "detach_tensors"
    },
    {
        "commit_hash": "98cc35b6a8e53e15829ce64fd8da835db0c61da9",
        "index": "2159ebf1..a24e4afe 100644",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class DeepSpeedTransformerInference(nn.Module):",
            "merge_count,",
            "mlp_extra_grouping)",
            "",
            "-        device = torch.cuda.current_device()  #if config.bigscience_bloom else 'cpu'",
            "+        device = get_accelerator().current_device_name(",
            "+        )  # if config.bigscience_bloom else 'cpu'",
            "self.norm_w = nn.Parameter(torch.empty(self.config.hidden_size,",
            "dtype=data_type,",
            "device=device),"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=1816236)",
            "Update(target_node=ASTNode(type=identifier, text=current_device), value='current_device_name')",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='get_accelerator')",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1816237)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1816238)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1816239)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 2660,
        "neg_line": [
            "-device = torch.cuda.current_device()  #if config.bigscience_bloom else 'cpu'"
        ],
        "pos_line": [
            "+device = get_accelerator().current_device_name(",
            "+)  # if config.bigscience_bloom else 'cpu'"
        ],
        "core_change": "-device = torch.cuda.current_device()  #if config.bigscience_bloom else 'cpu' +device = get_accelerator().current_device_name( +)  # if config.bigscience_bloom else 'cpu'",
        "core_API": "current_device"
    },
    {
        "commit_hash": "039742126fd01805e0afc2303a2b40c8b9a828cb",
        "index": "10a8a91ad..7540fc459 100644",
        "commit_message": "clean up, fixes and refactor\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_transducer_task_io(",
            "encoder_out_lens = list(map(int, encoder_out_lens))",
            "",
            "t_len = torch.IntTensor(encoder_out_lens).to(device)",
            "-    u_len = torch.IntTensor([y.size(0) for y in ys]).to(device)",
            "+    u_len = torch.IntTensor([y.size(0) for y in labels_unpad]).to(device)",
            "",
            "-    return target, t_len, u_len",
            "+    return decoder_in, target, t_len, u_len"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'target'), position=6, insert_id=131093)",
            "Insert(target_node=ASTNode(type=ERROR), node=(',', ','), position=7, insert_id=131094)",
            "Update(target_node=ASTNode(type=identifier, text=target), value='decoder_in')",
            "Update(target_node=ASTNode(type=identifier, text=ys), value='labels_unpad')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 2662,
        "neg_line": [
            "-u_len = torch.IntTensor([y.size(0) for y in ys]).to(device)",
            "-return target, t_len, u_len"
        ],
        "pos_line": [
            "+u_len = torch.IntTensor([y.size(0) for y in labels_unpad]).to(device)",
            "+return decoder_in, target, t_len, u_len"
        ],
        "core_change": "-u_len = torch.IntTensor([y.size(0) for y in ys]).to(device) +u_len = torch.IntTensor([y.size(0) for y in labels_unpad]).to(device) -return target, t_len, u_len +return decoder_in, target, t_len, u_len",
        "core_API": "IntTensor"
    },
    {
        "commit_hash": "7cc0379dadfdd0660612133b9c052e3f94635acb",
        "index": "fc92ebb..48efd27 100644",
        "commit_message": "Test clip for Gelu\n\nReviewed By: jfix71, mleshen\n\nDifferential Revision: D27570444\n\nfbshipit-source-id: b797f49987babdbf0e718e2ad6142f06bad5711e\n\n",
        "file": "pytext.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def lower_modules_to_accelerator(",
            "backend = \"NNPI\"",
            "backend_qualifier = \"\"",
            "",
            "-        if throughput_optimize:",
            "+        if throughput_optimize and gelu_clip:",
            "+            backend_qualifier = \":throughput_optimized_gelu_clip\"",
            "+        elif throughput_optimize:",
            "backend_qualifier = \":throughput_optimized\"",
            "",
            "modules_to_lower = accelerator.get_modules(model, backend + backend_qualifier)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type==, text==), node=ASTNode(type=ERROR), position=1)",
            "Insert(target_node=ASTNode(type=ERROR), node=('boolean_operator', None), position=0, insert_id=1469054)",
            "Insert(target_node=ASTNode(type=ERROR), node=(':', ':'), position=1, insert_id=1469055)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'backend_qualifier'), position=2, insert_id=1469056)",
            "Insert(target_node=ASTNode(type=ERROR), node=('=', '='), position=3, insert_id=1469057)",
            "Insert(target_node=ASTNode(type=ERROR), node=('string', '\":throughput_optimized_gelu_clip\"'), position=4, insert_id=1469058)",
            "Insert(target_node=ASTNode(type=ERROR), node=('elif', 'elif'), position=5, insert_id=1469059)",
            "Insert(target_node=ASTNode(type=ERROR), node=('=', '='), position=10, insert_id=1469060)",
            "Insert(target_node=IN(type=boolean_operator), node=('identifier', 'throughput_optimize'), position=0, insert_id=1469061)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1469062)",
            "Insert(target_node=IN(type=boolean_operator), node=('identifier', 'gelu_clip'), position=2, insert_id=1469063)",
            "Delete(target_node=ASTNode(type==, text==))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 2663,
        "neg_line": [
            "-if throughput_optimize:"
        ],
        "pos_line": [
            "+if throughput_optimize and gelu_clip:",
            "+backend_qualifier = \":throughput_optimized_gelu_clip\"",
            "+elif throughput_optimize:"
        ],
        "core_change": "-if throughput_optimize: +if throughput_optimize and gelu_clip: +backend_qualifier = \":throughput_optimized_gelu_clip\" +elif throughput_optimize:",
        "core_API": "get_modules"
    },
    {
        "commit_hash": "b41cffaa93e8205bd8bd309f82c33c07c420eefd",
        "index": "a3c8953..8110fcc 100644",
        "commit_message": "Fix a few issues loading pretrained vit/bit npz weights w/ num_classes=0 __init__ arg. Missed a few other small classifier handling detail on Mlp, GhostNet, Levit. Should fix #713\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def _load_weights(model: nn.Module, checkpoint_path: str, prefix: str = 'resnet/",
            "model.stem.conv.weight.copy_(stem_conv_w)",
            "model.norm.weight.copy_(t2p(weights[f'{prefix}group_norm/gamma']))",
            "model.norm.bias.copy_(t2p(weights[f'{prefix}group_norm/beta']))",
            "-    if model.head.fc.weight.shape[0] == weights[f'{prefix}head/conv2d/kernel'].shape[-1]:",
            "+    if isinstance(model.head.fc, nn.Conv2d) and \\",
            "+            model.head.fc.weight.shape[0] == weights[f'{prefix}head/conv2d/kernel'].shape[-1]:",
            "model.head.fc.weight.copy_(t2p(weights[f'{prefix}head/conv2d/kernel']))",
            "model.head.fc.bias.copy_(t2p(weights[f'{prefix}head/conv2d/bias']))",
            "for i, (sname, stage) in enumerate(model.stages.named_children()):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('boolean_operator', None), position=0, insert_id=1477817)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=1477818)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1477819)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=2)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=1477820)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1477821)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1477822)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1477823)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1477824)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=1477825)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1477826)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1477827)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1477828)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'fc'), position=2, insert_id=1477829)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn'), position=0, insert_id=1477830)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1477831)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Conv2d'), position=2, insert_id=1477832)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'model'), position=0, insert_id=1477833)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1477834)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'head'), position=2, insert_id=1477835)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 2664,
        "neg_line": [
            "-if model.head.fc.weight.shape[0] == weights[f'{prefix}head/conv2d/kernel'].shape[-1]:"
        ],
        "pos_line": [
            "+if isinstance(model.head.fc, nn.Conv2d) and \\",
            "+model.head.fc.weight.shape[0] == weights[f'{prefix}head/conv2d/kernel'].shape[-1]:"
        ],
        "core_change": "-if model.head.fc.weight.shape[0] == weights[f'{prefix}head/conv2d/kernel'].shape[-1]: +if isinstance(model.head.fc, nn.Conv2d) and \\ +model.head.fc.weight.shape[0] == weights[f'{prefix}head/conv2d/kernel'].shape[-1]:",
        "core_API": "copy_"
    },
    {
        "commit_hash": "a9fc87029ce450ea851a5fa3f810962684632d67",
        "index": "0e43cd4a..c2a02e02 100644",
        "commit_message": "fixed trpo\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TRPOModel(PGModel):",
            "prob_ratio = tf.exp(current_log_prob - prev_log_prob)",
            "surrogate_loss = -tf.reduce_mean(prob_ratio * self.advantage)",
            "variables = tf.trainable_variables()",
            "-            for v in variables:",
            "-                print(v.name)",
            "+",
            "batch_float = tf.cast(self.batch_size, tf.float32)",
            "",
            "mean_kl_divergence = self.dist.kl_divergence(self.prev_dist, self.policy.get_policy_variables())\\"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=for, text=for))",
            "Delete(target_node=ASTNode(type=identifier, text=v))",
            "Delete(target_node=ASTNode(type=in, text=in))",
            "Delete(target_node=ASTNode(type=identifier, text=variables))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=print))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=v))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=name))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=for_statement))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 2666,
        "neg_line": [
            "-for v in variables:",
            "-print(v.name)"
        ],
        "pos_line": [
            "+"
        ],
        "core_change": "-for v in variables: -print(v.name) +",
        "core_API": "exp"
    },
    {
        "commit_hash": "b713e68eef480b2bb2eb017e27b77421dd055d38",
        "index": "6e78397a..ac559bc9 100644",
        "commit_message": "cuda fixes\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def k_hop_subgraph(node_idx, num_hops, edge_index, relabel_nodes=False,",
            "",
            "if relabel_nodes:",
            "node_idx = row.new_full((num_nodes, ), -1)",
            "-        node_idx[subset] = torch.arange(subset.size(0))",
            "+        node_idx[subset] = torch.arange(subset.size(0), device=row.device)",
            "edge_index = node_idx[edge_index]",
            "",
            "return subset, edge_index, edge_mask"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1034955)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1034956)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1034957)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1034958)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1034959)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'row'), position=0, insert_id=1034960)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1034961)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1034962)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2670,
        "neg_line": [
            "-node_idx[subset] = torch.arange(subset.size(0))"
        ],
        "pos_line": [
            "+node_idx[subset] = torch.arange(subset.size(0), device=row.device)"
        ],
        "core_change": "-node_idx[subset] = torch.arange(subset.size(0)) +node_idx[subset] = torch.arange(subset.size(0), device=row.device)",
        "core_API": "new_full"
    },
    {
        "commit_hash": "f4dd151037e4a534499f4e90ce918103f17436dd",
        "index": "d76d15d..26350b5 100644",
        "commit_message": "fix internal index.Tensor test on wrong device\n\nSummary: After landing https://github.com/pytorch/pytorch/pull/69607, that made it an error to use indexing with `cpu_tensor[cuda_indices]`. There was one outstanding test in fbcode that incorrectly used indexing in that way, which is fixed here\n\nReviewed By: bottler, osalpekar\n\nDifferential Revision: D37128838\n\nfbshipit-source-id: 611b6f717b5b5d89fa61fd9ebeb513ad7e65a656\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestTexturesVertex(TestCaseMixin, unittest.TestCase):",
            ")",
            "",
            "# define TexturesVertex",
            "-        verts_texture = torch.rand(verts.shape)",
            "+        verts_texture = torch.rand(verts.shape, device=device)",
            "textures = TexturesVertex(verts_features=verts_texture)",
            "",
            "# compute packed faces"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=911812)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=911813)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=911814)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=911815)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=911816)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2673,
        "neg_line": [
            "-verts_texture = torch.rand(verts.shape)"
        ],
        "pos_line": [
            "+verts_texture = torch.rand(verts.shape, device=device)"
        ],
        "core_change": "-verts_texture = torch.rand(verts.shape) +verts_texture = torch.rand(verts.shape, device=device)",
        "core_API": "rand"
    },
    {
        "commit_hash": "c96f7a2614ae336ac8b4c1657af444846719cd83",
        "index": "e705b1e0..d49f2725 100644",
        "commit_message": "TorchSTFT to device fix\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TorchSTFT():",
            "center=True,",
            "pad_mode=\"reflect\",  # compatible with audio.py",
            "normalized=False,",
            "-                       onesided=True)",
            "+                       onesided=True,",
            "+                       return_complex=False)",
            "M = o[:, :, :, 0]",
            "P = o[:, :, :, 1]",
            "return torch.sqrt(torch.clamp(M ** 2 + P ** 2, min=1e-8))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1554796)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1554797)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'onesided'), position=0, insert_id=1554798)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1554799)",
            "Insert(target_node=IN(type=assignment), node=('expression_list', None), position=2, insert_id=1554800)",
            "Update(target_node=ASTNode(type=identifier, text=onesided), value='return_complex')",
            "Insert(target_node=IN(type=expression_list), node=('true', 'True'), position=0, insert_id=1554801)",
            "Insert(target_node=IN(type=expression_list), node=(',', ','), position=1, insert_id=1554802)",
            "Insert(target_node=ASTNode(type=ERROR), node=('false', 'False'), position=0, insert_id=1554803)",
            "Delete(target_node=ASTNode(type=true, text=True))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 2674,
        "neg_line": [
            "-onesided=True)"
        ],
        "pos_line": [
            "+onesided=True,",
            "+return_complex=False)"
        ],
        "core_change": "-onesided=True) +onesided=True, +return_complex=False)",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "8a4cb4d99c9fea60527dea6d9f27891c958f0fa8",
        "index": "b35de2e..2ec3244 100644",
        "commit_message": "Fix for test failure\n\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class CategoricalToNumerical(preprocessor.Preprocessor):",
            "\"column_names\": config[\"column_names\"],",
            "}",
            "obj = cls(**init_config)",
            "-        obj.layer = preprocessors.deserialize(config[\"layer\"])",
            "+        obj.layer = keras_layers.MultiCategoryEncoding(config[\"encoding\"])",
            "for encoding_layer, vocab in zip(",
            "obj.layer.encoding_layers, config[\"encoding_vocab\"]",
            "):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=preprocessors), value='keras_layers')",
            "Update(target_node=ASTNode(type=identifier, text=deserialize), value='MultiCategoryEncoding')",
            "Update(target_node=ASTNode(type=string, text=\"layer\"), value='\"encoding\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 2678,
        "neg_line": [
            "-obj.layer = preprocessors.deserialize(config[\"layer\"])"
        ],
        "pos_line": [
            "+obj.layer = keras_layers.MultiCategoryEncoding(config[\"encoding\"])"
        ],
        "core_change": "-obj.layer = preprocessors.deserialize(config[\"layer\"]) +obj.layer = keras_layers.MultiCategoryEncoding(config[\"encoding\"])",
        "core_API": "deserialize"
    },
    {
        "commit_hash": "d215aa258e77f43b6c36950bc9a484d5d849eb4f",
        "index": "e4377b71..7948c203 100644",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class IoUBalancedNegSampler(RandomSampler):",
            "return sampled_inds",
            "",
            "def _sample_neg(self, assign_result, num_expected, **kwargs):",
            "-        neg_inds = torch.nonzero(assign_result.gt_inds == 0)",
            "+        neg_inds = torch.nonzero(assign_result.gt_inds == 0, as_tuple=False)",
            "if neg_inds.numel() != 0:",
            "neg_inds = neg_inds.squeeze(1)",
            "if len(neg_inds) <= num_expected:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=638734)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=638735)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'as_tuple'), position=0, insert_id=638736)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=638737)",
            "Insert(target_node=IN(type=keyword_argument), node=('false', 'False'), position=2, insert_id=638738)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2679,
        "neg_line": [
            "-neg_inds = torch.nonzero(assign_result.gt_inds == 0)"
        ],
        "pos_line": [
            "+neg_inds = torch.nonzero(assign_result.gt_inds == 0, as_tuple=False)"
        ],
        "core_change": "-neg_inds = torch.nonzero(assign_result.gt_inds == 0) +neg_inds = torch.nonzero(assign_result.gt_inds == 0, as_tuple=False)",
        "core_API": "nonzero"
    },
    {
        "commit_hash": "7cd779dffd53e16f8fc66f677e508a700b2fd9da",
        "index": "0440c3808..d7ed049b4 100644",
        "commit_message": "added notebook for working forward pass\nremove .decode from all operations\nremove conversion to FixedPrecisionTensor in PhiTensor\nadd CrossEntropy class\nupdate all layer to be class inherited from nn.Module\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "from ..autodp.phi_tensor import PhiTensor",
            "",
            "def leaky_relu(input: PhiTensor, negative_slope: float = 0.01) -> PhiTensor:",
            "",
            "-    data = nn.functional.leaky_relu(Tensor(input.child.decode()), negative_slope)",
            "+    data = nn.functional.leaky_relu(Tensor(input.child), negative_slope)",
            "data_as_numpy = data.detach().numpy()",
            "",
            "return PhiTensor("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=decode))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 2680,
        "neg_line": [
            "-data = nn.functional.leaky_relu(Tensor(input.child.decode()), negative_slope)"
        ],
        "pos_line": [
            "+data = nn.functional.leaky_relu(Tensor(input.child), negative_slope)"
        ],
        "core_change": "-data = nn.functional.leaky_relu(Tensor(input.child.decode()), negative_slope) +data = nn.functional.leaky_relu(Tensor(input.child), negative_slope)",
        "core_API": "leaky_relu"
    },
    {
        "commit_hash": "fdc907675050cb9e3aa887bf89d6c92519178349",
        "index": "6994ef0f..ea788b19 100755",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(mnist_example.Model):",
            "",
            "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)",
            "cost = tf.reduce_mean(cost, name='cross_entropy_loss')",
            "-        wd_cost = tf.mul(1e-5, regularize_cost('fc.*/W', tf.nn.l2_loss),",
            "-                         name='regularize_loss')",
            "+        wd_cost = tf.multiply(1e-5, regularize_cost('fc.*/W', tf.nn.l2_loss),",
            "+                              name='regularize_loss')",
            "",
            "self.cost = tf.add_n([wd_cost, cost], name='cost')",
            "add_moving_summary(cost, wd_cost, self.cost)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=mul), value='multiply')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 2684,
        "neg_line": [
            "-wd_cost = tf.mul(1e-5, regularize_cost('fc.*/W', tf.nn.l2_loss),",
            "-name='regularize_loss')"
        ],
        "pos_line": [
            "+wd_cost = tf.multiply(1e-5, regularize_cost('fc.*/W', tf.nn.l2_loss),",
            "+name='regularize_loss')"
        ],
        "core_change": "-wd_cost = tf.mul(1e-5, regularize_cost('fc.*/W', tf.nn.l2_loss), -name='regularize_loss') +wd_cost = tf.multiply(1e-5, regularize_cost('fc.*/W', tf.nn.l2_loss), +name='regularize_loss')",
        "core_API": "sparse_softmax_cross_entropy_with_logits"
    },
    {
        "commit_hash": "0fe17f375a4f0fdd9aea260d0645ccfd4896e958",
        "index": "869014bee..66163ad49 100755",
        "commit_message": "FX tracing improvement (#14321)\n\n* Change the way tracing happens, enabling dynamic axes out of the box\n\n* Update the tests and modeling xlnet\n\n* Add the non recoding of leaf modules to avoid recording more values for the methods to record than what will be seen at tracing time (which would otherwise desynchronize the recorded values and the values that need to be given to the proxies during tracing, causing errors).\n\n* Comments and making tracing work for gpt-j and xlnet\n\n* Refactore things related to num_choices (and batch_size, sequence_length)\n\n* Update fx to work on PyTorch 1.10\n\n* Postpone autowrap_function feature usage for later\n\n* Add copyrights\n\n* Remove unnecessary file\n\n* Fix issue with add_new_model_like\n\n* Apply suggestions\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GPTJForSequenceClassification(GPTJPreTrainedModel):",
            "f\"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"",
            ")",
            "",
            "-        pooled_logits = logits[range(batch_size), sequence_lengths]",
            "+        pooled_logits = logits[torch.arange(batch_size, device=self.device), sequence_lengths]",
            "",
            "loss = None",
            "if labels is not None:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1207208)",
            "Update(target_node=ASTNode(type=identifier, text=range), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=range), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1207209)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'arange'), position=2, insert_id=1207210)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1207211)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1207212)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1207213)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1207214)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1207215)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1207216)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1207217)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1207218)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 2685,
        "neg_line": [
            "-pooled_logits = logits[range(batch_size), sequence_lengths]"
        ],
        "pos_line": [
            "+pooled_logits = logits[torch.arange(batch_size, device=self.device), sequence_lengths]"
        ],
        "core_change": "-pooled_logits = logits[range(batch_size), sequence_lengths] +pooled_logits = logits[torch.arange(batch_size, device=self.device), sequence_lengths]",
        "core_API": "arange"
    },
    {
        "commit_hash": "46865ea4554b7805857ed7741b42abfb6ea44b83",
        "index": "e4754e57..5ba15154 100644",
        "commit_message": "Fix types in documentation (#6698)\n\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SchNet(torch.nn.Module):",
            "batch: OptTensor = None) -> Tensor:",
            "r\"\"\"",
            "Args:",
            "-            z (LongTensor): Atomic number of each atom with shape",
            "+            z (torch.Tensor): Atomic number of each atom with shape",
            ":obj:`[num_atoms]`.",
            "-            pos (Tensor): Coordinates of each atom with shape",
            "+            pos (torch.Tensor): Coordinates of each atom with shape",
            ":obj:`[num_atoms, 3]`.",
            "-            batch (LongTensor, optional): Batch indices assigning each atom to",
            "-                a separate molecule with shape :obj:`[num_atoms]`.",
            "+            batch (torch.Tensor, optional): Batch indices assigning each atom",
            "+                to a separate molecule with shape :obj:`[num_atoms]`.",
            "(default: :obj:`None`)",
            "\"\"\"",
            "batch = torch.zeros_like(z) if batch is None else batch"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=r\"\"\"\nArgs:\n            z (LongTensor): Atomic number of each atom with shape\n:obj:`[num_atoms]`.\n            pos (Tensor): Coordinates of each atom with shape\n:obj:`[num_atoms, 3]`.\n            batch (LongTensor, optional): Batch indices assigning each atom to\n                a separate molecule with shape :obj:`[num_atoms]`.\n(default: :obj:`None`)\n\"\"\"), value='r\"\"\"\\nArgs:\\n            z (torch.Tensor): Atomic number of each atom with shape\\n:obj:`[num_atoms]`.\\n            pos (torch.Tensor): Coordinates of each atom with shape\\n:obj:`[num_atoms, 3]`.\\n            batch (torch.Tensor, optional): Batch indices assigning each atom\\n                to a separate molecule with shape :obj:`[num_atoms]`.\\n(default: :obj:`None`)\\n\"\"\"')"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 1,
        "number": 2687,
        "neg_line": [
            "-z (LongTensor): Atomic number of each atom with shape",
            "-pos (Tensor): Coordinates of each atom with shape",
            "-batch (LongTensor, optional): Batch indices assigning each atom to",
            "-a separate molecule with shape :obj:`[num_atoms]`."
        ],
        "pos_line": [
            "+z (torch.Tensor): Atomic number of each atom with shape",
            "+pos (torch.Tensor): Coordinates of each atom with shape",
            "+batch (torch.Tensor, optional): Batch indices assigning each atom",
            "+to a separate molecule with shape :obj:`[num_atoms]`."
        ],
        "core_change": "-z (LongTensor): Atomic number of each atom with shape +z (torch.Tensor): Atomic number of each atom with shape -pos (Tensor): Coordinates of each atom with shape +pos (torch.Tensor): Coordinates of each atom with shape -batch (LongTensor, optional): Batch indices assigning each atom to -a separate molecule with shape :obj:`[num_atoms]`. +batch (torch.Tensor, optional): Batch indices assigning each atom +to a separate molecule with shape :obj:`[num_atoms]`.",
        "core_API": "zeros_like"
    },
    {
        "commit_hash": "f8798c1c13bece96a0fd92d9b7e7bd69576fa34d",
        "index": "4bb102b1..0841fa41 100644",
        "commit_message": "enable `disallow_incomplete_defs` on mypy (#2094)\n\n* enable `disallow_incomplete_defs` on mypy\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix `blur_pool2d` doc\n\n* finish v1: works on torch 1.13.1\n\n- Remove JIT support for Boxes3D\n\n* rip off the np typing\n\n* replace `Size` with `Tuple[int, ...]` on augs\n\n* add `Dtype` to kornia.filters.kernels\n\n* minor fix after rebase\n\n* Remove old torch from typing CI\n\n---------\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RandomElasticTransform(AugmentationBase2D):",
            "padding_mode=padding_mode,",
            ")",
            "",
            "-    def generate_parameters(self, shape: torch.Size) -> Dict[str, Tensor]:",
            "+    def generate_parameters(self, shape: Tuple[int, ...]) -> Dict[str, Tensor]:",
            "B, _, H, W = shape",
            "if self.same_on_batch:",
            "noise = torch.rand(1, 2, H, W, device=self.device, dtype=self.dtype).expand(B, 2, H, W)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('type', None), position=0, insert_id=387786)",
            "Insert(target_node=IN(type=type), node=('subscript', None), position=0, insert_id=387787)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Tuple'), position=0, insert_id=387788)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=387789)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'int'), position=2, insert_id=387790)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=387791)",
            "Insert(target_node=IN(type=subscript), node=('ellipsis', '...'), position=4, insert_id=387792)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=5, insert_id=387793)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Size))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=type))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 2692,
        "neg_line": [
            "-def generate_parameters(self, shape: torch.Size) -> Dict[str, Tensor]:"
        ],
        "pos_line": [
            "+def generate_parameters(self, shape: Tuple[int, ...]) -> Dict[str, Tensor]:"
        ],
        "core_change": "-def generate_parameters(self, shape: torch.Size) -> Dict[str, Tensor]: +def generate_parameters(self, shape: Tuple[int, ...]) -> Dict[str, Tensor]:",
        "core_API": "rand"
    },
    {
        "commit_hash": "2ac731a999c3ad0a5286766785e33501d128c531",
        "index": "3bcf7017..523104f5 100644",
        "commit_message": "fix pulint issues\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DropconnectDenseLayer(Layer):",
            "self.n_units = n_units",
            "logging.info(\"DropconnectDenseLayer %s: %d %s\" % (self.name, self.n_units, act.__name__))",
            "",
            "-        with tf.variable_scope(name) as vs:",
            "+        with tf.variable_scope(name):",
            "W = tf.get_variable(name='W', shape=(n_in, n_units), initializer=W_init, dtype=D_TYPE, **W_init_args)",
            "b = tf.get_variable(name='b', shape=(n_units), initializer=b_init, dtype=D_TYPE, **b_init_args)",
            "-            self.outputs = act(tf.matmul(self.inputs, W) + b)  #, name=name)    # 1.2",
            "+            self.outputs = act(tf.matmul(self.inputs, W) + b)",
            "",
            "set_keep[name] = tf.placeholder(tf.float32)",
            "W_dropcon = tf.nn.dropout(W, set_keep[name])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=with_item), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=as, text=as))",
            "Delete(target_node=ASTNode(type=identifier, text=vs))",
            "Delete(target_node=ASTNode(type=as_pattern_target))",
            "Delete(target_node=ASTNode(type=as_pattern))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 5,
        "number": 2693,
        "neg_line": [
            "-with tf.variable_scope(name) as vs:",
            "-self.outputs = act(tf.matmul(self.inputs, W) + b)  #, name=name)    # 1.2"
        ],
        "pos_line": [
            "+with tf.variable_scope(name):",
            "+self.outputs = act(tf.matmul(self.inputs, W) + b)"
        ],
        "core_change": "-with tf.variable_scope(name) as vs: +with tf.variable_scope(name): -self.outputs = act(tf.matmul(self.inputs, W) + b)  #, name=name)    # 1.2 +self.outputs = act(tf.matmul(self.inputs, W) + b)",
        "core_API": "info"
    },
    {
        "commit_hash": "4768d55734471e476d526a4e6d15cfcf1a3e8fe9",
        "index": "616a5d23..ea489635 100644",
        "commit_message": "[layers] fix prelu layer typo\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class PReluLayer(Layer):",
            "with tf.variable_scope(name) as vs:",
            "alphas = tf.get_variable(name='alphas', shape=w_shape, initializer=a_init, **a_init_args )",
            "try:  ## TF 1.0",
            "-                self.outputs = tf.nn.relu(self.inputs) + tf.mulitply(alphas, (self.inputs - tf.abs(self.inputs))) * 0.5",
            "+                self.outputs = tf.nn.relu(self.inputs) + tf.multiply(alphas, (self.inputs - tf.abs(self.inputs))) * 0.5",
            "except: ## TF 0.12",
            "self.outputs = tf.nn.relu(self.inputs) + tf.mul(alphas, (self.inputs - tf.abs(self.inputs))) * 0.5",
            "",
            "+",
            "self.all_layers = list(layer.all_layers)",
            "self.all_params = list(layer.all_params)",
            "self.all_drop = dict(layer.all_drop)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2696,
        "neg_line": [
            "-self.outputs = tf.nn.relu(self.inputs) + tf.mulitply(alphas, (self.inputs - tf.abs(self.inputs))) * 0.5"
        ],
        "pos_line": [
            "+self.outputs = tf.nn.relu(self.inputs) + tf.multiply(alphas, (self.inputs - tf.abs(self.inputs))) * 0.5",
            "+"
        ],
        "core_change": "-self.outputs = tf.nn.relu(self.inputs) + tf.mulitply(alphas, (self.inputs - tf.abs(self.inputs))) * 0.5 +self.outputs = tf.nn.relu(self.inputs) + tf.multiply(alphas, (self.inputs - tf.abs(self.inputs))) * 0.5 +",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "0888dbfadee6eb1169a185a8ede3882ac42d1a72",
        "index": "d92ede6..5b8ea87 100644",
        "commit_message": "add faster sampling for global attention (#43)\n\n* add faster sampling for global attention\n\n* allow user to toggle slow sampling, in case fast sampling contains a bug\n\n* make fast sampling compatible with local attention by expanding and reducing queries before and after the local attention op\n\n* remove pasts variable, in favor of using contexts object\n\n* fix a bunch of broken things\n\n* remove use of reshapes in attention blocks\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def clip_by_global_norm(grads, clip_norm):",
            "",
            "def get_optimizer(loss, params, summary, variable_dtype, inp_var_grads=None):",
            "\"\"\"Creates and returns an optimizer training op.\"\"\"",
            "+    mesh = loss.mesh  # get mesh info from loss",
            "+    graph = mesh.graph  # get graph info from mesh",
            "+    global_step = tf.train.get_or_create_global_step() # get global step",
            "",
            "learning_rate = tf.constant(value=params[\"lr\"], shape=[], dtype=variable_dtype.slice_dtype) # grab lr param",
            "clip_value = mtf.constant(mesh, params[\"gradient_clipping\"], dtype=variable_dtype.slice_dtype)",
            "",
            "-",
            "-    global_step = tf.train.get_or_create_global_step() # get global step",
            "-    mesh = loss.mesh  # get mesh info from loss",
            "-    graph = mesh.graph  # get graph info from mesh",
            "-",
            "if inp_var_grads is None:",
            "var_grads = mtf.gradients([loss], [v.outputs[0] for v in graph.trainable_variables])",
            "else:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=3)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=3)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=7)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 3,
        "number": 2697,
        "neg_line": [
            "-",
            "-global_step = tf.train.get_or_create_global_step() # get global step",
            "-mesh = loss.mesh  # get mesh info from loss",
            "-graph = mesh.graph  # get graph info from mesh",
            "-"
        ],
        "pos_line": [
            "+mesh = loss.mesh  # get mesh info from loss",
            "+graph = mesh.graph  # get graph info from mesh",
            "+global_step = tf.train.get_or_create_global_step() # get global step"
        ],
        "core_change": "+mesh = loss.mesh  # get mesh info from loss +graph = mesh.graph  # get graph info from mesh +global_step = tf.train.get_or_create_global_step() # get global step - -global_step = tf.train.get_or_create_global_step() # get global step -mesh = loss.mesh  # get mesh info from loss -graph = mesh.graph  # get graph info from mesh -",
        "core_API": "get_or_create_global_step"
    },
    {
        "commit_hash": "fb6e98d2cfc62a81d4829d26fb59c45755bf169a",
        "index": "6936cd4..7c7967f 100644",
        "commit_message": "fixed indendentation\n\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def model_fn(features, labels, mode, params):",
            "# TODO: this is mtf code - figure out what this does",
            "fully_replicated_logits = mtf.anonymize(logits)",
            "",
            "+    # Getting total number of trainable vars",
            "print('\\n')",
            "total_parameters = 0",
            "for variable in graph.trainable_variables:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 2698,
        "neg_line": [],
        "pos_line": [
            "+# Getting total number of trainable vars"
        ],
        "core_change": "+# Getting total number of trainable vars",
        "core_API": "anonymize"
    },
    {
        "commit_hash": "2f003f9f11e4b8efad33d4078ddae8533fd19f51",
        "index": "c4daa032..a416d22f 100644",
        "commit_message": "Fix the epsilon sign in Adam.\n\nPiperOrigin-RevId: 513895046\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class NonFusedAdam(optimizer_v2.OptimizerV2):",
            "vhat = self.get_slot(var, \"vhat\")",
            "vhat.assign(tf.maximum(vhat, v))",
            "v = vhat",
            "-        var.assign_sub((m * alpha) / (tf.sqrt(v) - coefficients[\"epsilon\"]))",
            "+        var.assign_sub((m * alpha) / (tf.sqrt(v) + coefficients[\"epsilon\"]))",
            "",
            "@tf.function(jit_compile=True)",
            "def _resource_apply_sparse(self, grad, var, indices, apply_state=None):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('+', '+'), position=1, insert_id=2041383)",
            "Delete(target_node=ASTNode(type=-, text=-))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 2700,
        "neg_line": [
            "-var.assign_sub((m * alpha) / (tf.sqrt(v) - coefficients[\"epsilon\"]))"
        ],
        "pos_line": [
            "+var.assign_sub((m * alpha) / (tf.sqrt(v) + coefficients[\"epsilon\"]))"
        ],
        "core_change": "-var.assign_sub((m * alpha) / (tf.sqrt(v) - coefficients[\"epsilon\"])) +var.assign_sub((m * alpha) / (tf.sqrt(v) + coefficients[\"epsilon\"]))",
        "core_API": "get_slot"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "d8b76839..dbfe9a66 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpanField(Field[torch.Tensor]):",
            "@overrides",
            "def as_tensor(self,",
            "padding_lengths: Dict[str, int],",
            "-                  cuda_device: int = -1,",
            "-                  for_training: bool = True) -> torch.Tensor:",
            "+                  cuda_device: int = -1) -> torch.Tensor:",
            "# pylint: disable=unused-argument",
            "-        tensor = Variable(torch.LongTensor([self.span_start, self.span_end]), volatile=not for_training)",
            "+        tensor = torch.LongTensor([self.span_start, self.span_end])",
            "return tensor if cuda_device == -1 else tensor.cuda(cuda_device)",
            "",
            "@overrides"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=for_training))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=bool))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=typed_default_parameter))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=volatile))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=not, text=not))",
            "Delete(target_node=ASTNode(type=identifier, text=for_training))",
            "Delete(target_node=ASTNode(type=not_operator))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 2701,
        "neg_line": [
            "-cuda_device: int = -1,",
            "-for_training: bool = True) -> torch.Tensor:",
            "-tensor = Variable(torch.LongTensor([self.span_start, self.span_end]), volatile=not for_training)"
        ],
        "pos_line": [
            "+cuda_device: int = -1) -> torch.Tensor:",
            "+tensor = torch.LongTensor([self.span_start, self.span_end])"
        ],
        "core_change": "-cuda_device: int = -1, -for_training: bool = True) -> torch.Tensor: +cuda_device: int = -1) -> torch.Tensor: -tensor = Variable(torch.LongTensor([self.span_start, self.span_end]), volatile=not for_training) +tensor = torch.LongTensor([self.span_start, self.span_end])",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "69bf9a3489c3e4878b500627317aa0dc0fddd594",
        "index": "fa3a58e4..4e6b5697 100644",
        "commit_message": "Add overdispersed models to contrib.epidemiology (#2498)\n\n* Fix ExtendedBetaBinomial gradient issue\n\n* Use approximate log_beta in CompartmentalModel\n\n* Use overdispersed distribution in contrib.epidemiology\n\n* Revive change that had been lost in merge conflict\n\n* Fix another merge conflict error\n\n* Revise bounds logic on examples/.../sir.py\n\n* Relax default bounds for generating data\n\n* Revert unnecessary changes\n\n* Move precision warning\n\n* Add overdispersion references\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CompartmentalModel(ABC):",
            "to a tensor whose first dimension corresponds to sample batching.",
            ":rtype: dict",
            "\"\"\"",
            "+        _require_double_precision()",
            "if not self.samples:",
            "raise RuntimeError(\"Missing samples, try running .fit() first\")",
            "+",
            "samples = self.samples",
            "num_samples = len(next(iter(samples.values())))",
            "particle_plate = pyro.plate(\"particles\", num_samples,"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\nif not self.samples:\nraise RuntimeError(\"Missing samples, try running .fit() first\")\nsamples = self.samples\nnum_samples = len(next(iter(samples.values())))\nparticle_plate = pyro.plate(\"particles\"), value='\"\"\"\\n        _require_double_precision()\\nif not self.samples:\\nraise RuntimeError(\"Missing samples, try running .fit() first\")\\n\\nsamples = self.samples\\nnum_samples = len(next(iter(samples.values())))\\nparticle_plate = pyro.plate(\"particles\"')"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 1,
        "number": 2703,
        "neg_line": [],
        "pos_line": [
            "+_require_double_precision()",
            "+"
        ],
        "core_change": "+_require_double_precision() +",
        "core_API": "values"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "54746111ff..cc8ed3d59e 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class FullyConnectedNetwork(Model):",
            "name=label)",
            "i += 1",
            "",
            "-            output = tf.layers.dense(",
            "+            output = tf1.layers.dense(",
            "last_layer,",
            "num_outputs,",
            "kernel_initializer=normc_initializer(0.01),"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2704,
        "neg_line": [
            "-output = tf.layers.dense("
        ],
        "pos_line": [
            "+output = tf1.layers.dense("
        ],
        "core_change": "-output = tf.layers.dense( +output = tf1.layers.dense(",
        "core_API": "dense"
    },
    {
        "commit_hash": "3a134c7224c7d66155097973e968a1a2f3c30186",
        "index": "ad627f533..cb0b73576 100644",
        "commit_message": "[RaySGD] Rename PyTorch API endpoints to start with Torch (#7425)\n\n* Start renaming pytorch to torch\n\n* Rename PyTorchTrainer to TorchTrainer\n\n* Rename PyTorch runners to Torch runners\n\n* Finish renaming API\n\n* Rename to torch in tests\n\n* Finish renaming docs + tests\n\n* Run format + fix DeprecationWarning\n\n* fix\n\n* move tests up\n\n* rename\n\nCo-authored-by: Richard Liaw <rliaw@berkeley.edu>\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def train_example(num_replicas=1, use_gpu=False, test_mode=False):",
            "\"test_mode\": test_mode,",
            "\"classification_model_path\": os.path.join(",
            "os.path.dirname(ray.__file__),",
            "-            \"util/sgd/pytorch/examples/mnist_cnn.pt\")",
            "+            \"util/sgd/torch/examples/mnist_cnn.pt\")",
            "}",
            "-    trainer = PyTorchTrainer(",
            "+    trainer = TorchTrainer(",
            "model_creator,",
            "data_creator,",
            "optimizer_creator,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=PyTorchTrainer), value='TorchTrainer')",
            "Update(target_node=ASTNode(type=string, text=\"util/sgd/pytorch/examples/mnist_cnn.pt\"), value='\"util/sgd/torch/examples/mnist_cnn.pt\"')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 2707,
        "neg_line": [
            "-\"util/sgd/pytorch/examples/mnist_cnn.pt\")",
            "-trainer = PyTorchTrainer("
        ],
        "pos_line": [
            "+\"util/sgd/torch/examples/mnist_cnn.pt\")",
            "+trainer = TorchTrainer("
        ],
        "core_change": "-\"util/sgd/pytorch/examples/mnist_cnn.pt\") +\"util/sgd/torch/examples/mnist_cnn.pt\") -trainer = PyTorchTrainer( +trainer = TorchTrainer(",
        "core_API": "join"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "400c16c40d..5db64d7da7 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TestFrameWorkAgnosticComponents(unittest.TestCase):",
            "# Test recognizing default package path.",
            "scope = None",
            "if sess:",
            "-                scope = tf.variable_scope(\"exploration_object\")",
            "+                scope = tf1.variable_scope(\"exploration_object\")",
            "scope.__enter__()",
            "component = from_config(",
            "Exploration, {"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2708,
        "neg_line": [
            "-scope = tf.variable_scope(\"exploration_object\")"
        ],
        "pos_line": [
            "+scope = tf1.variable_scope(\"exploration_object\")"
        ],
        "core_change": "-scope = tf.variable_scope(\"exploration_object\") +scope = tf1.variable_scope(\"exploration_object\")",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "fd16e5abef94e274572d40912f12baeffece8696",
        "index": "e4cfe3c3..12de55e5 100644",
        "commit_message": "fix: check dtype when loading models (#872)\n\n* fix: check dtype when loading models\n\n* fix: black\n",
        "file": "clip-as-service.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def load_openai_model(",
            "",
            "# model from OpenAI state dict is in manually cast fp16 mode, must be converted for AMP/fp32/bf16 use",
            "model = model.to(device)",
            "-        if dtype == torch.float32 or dtype.startswith('amp'):",
            "+        if dtype == torch.float32 or (",
            "+            isinstance(dtype, str) and dtype.startswith('amp')",
            "+        ):",
            "model.float()",
            "elif dtype == torch.bfloat16:",
            "convert_weights_to_lp(model, dtype=torch.bfloat16)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=boolean_operator), node=('parenthesized_expression', None), position=2, insert_id=59502)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=59503)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('boolean_operator', None), position=1, insert_id=59504)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=59505)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=59506)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=59507)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=59508)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=59509)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=59510)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'dtype'), position=1, insert_id=59511)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=59512)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'str'), position=3, insert_id=59513)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=59514)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 2711,
        "neg_line": [
            "-if dtype == torch.float32 or dtype.startswith('amp'):"
        ],
        "pos_line": [
            "+if dtype == torch.float32 or (",
            "+isinstance(dtype, str) and dtype.startswith('amp')",
            "+):"
        ],
        "core_change": "-if dtype == torch.float32 or dtype.startswith('amp'): +if dtype == torch.float32 or ( +isinstance(dtype, str) and dtype.startswith('amp') +):",
        "core_API": "to"
    },
    {
        "commit_hash": "c5b6ceea4a0a151ab78b9c065584bc416d1ee275",
        "index": "67d004f1..a98d7409 100644",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "with tf.device('/cpu:0'):",
            "net = tl.layers.FlattenLayer(net, name='flatten')",
            "net = tl.layers.TernaryDenseLayer(net, 384, act=tf.nn.relu, name='d1relu')",
            "net = tl.layers.TernaryDenseLayer(net, 192, act=tf.nn.relu, name='d2relu')",
            "-            net = tl.layers.DenseLayer(net, 10, act=tf.identity, name='output')",
            "+            net = tl.layers.DenseLayer(net, 10, act=None, name='output')",
            "y = net.outputs",
            "",
            "ce = tl.cost.cross_entropy(y, y_, name='cost')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('none', 'None'), position=2, insert_id=2634202)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=identity))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2713,
        "neg_line": [
            "-net = tl.layers.DenseLayer(net, 10, act=tf.identity, name='output')"
        ],
        "pos_line": [
            "+net = tl.layers.DenseLayer(net, 10, act=None, name='output')"
        ],
        "core_change": "-net = tl.layers.DenseLayer(net, 10, act=tf.identity, name='output') +net = tl.layers.DenseLayer(net, 10, act=None, name='output')",
        "core_API": "device"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "5cd06bb7..04f875bd 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestBidirectonalEndpointSpanExtractor:",
            "# size: (batch_size=1, sequence_length=2, emb_dim=2)",
            "sequence_tensor = torch.FloatTensor([[[0.0, 0.0], [0.0, 0.0]]])",
            "# size: (batch_size=1, sequence_length=2)",
            "-        sequence_mask = torch.LongTensor([[0, 0]])",
            "+        sequence_mask = torch.BoolTensor([[False, False]])",
            "# size: (batch_size=1, spans_count=1, 2)",
            "span_indices = torch.LongTensor([[[-1, -1]]])",
            "# size: (batch_size=1, spans_count=1)",
            "-        span_indices_mask = torch.LongTensor([[0]])",
            "+        span_indices_mask = torch.BoolTensor([[False]])",
            "extractor = BidirectionalEndpointSpanExtractor(",
            "input_dim=2, forward_combination=\"x,y\", backward_combination=\"x,y\"",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=LongTensor), value='BoolTensor')",
            "Update(target_node=ASTNode(type=identifier, text=LongTensor), value='BoolTensor')",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=1, insert_id=19905)",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=4, insert_id=19906)",
            "Insert(target_node=ASTNode(type=list), node=('false', 'False'), position=1, insert_id=19907)",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=integer, text=0))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 2715,
        "neg_line": [
            "-sequence_mask = torch.LongTensor([[0, 0]])",
            "-span_indices_mask = torch.LongTensor([[0]])"
        ],
        "pos_line": [
            "+sequence_mask = torch.BoolTensor([[False, False]])",
            "+span_indices_mask = torch.BoolTensor([[False]])"
        ],
        "core_change": "-sequence_mask = torch.LongTensor([[0, 0]]) +sequence_mask = torch.BoolTensor([[False, False]]) -span_indices_mask = torch.LongTensor([[0]]) +span_indices_mask = torch.BoolTensor([[False]])",
        "core_API": "FloatTensor"
    },
    {
        "commit_hash": "ebd0bc17929b36b851fc825bbfd5fbd41f6d8111",
        "index": "6fe082cf..09dbe687 100644",
        "commit_message": "Update to PyTorch 1.11.0 (#3045)\n\n* Run black\n\n* Fix ProvenanceTensor\n\n* Replace torch.triangular_solve -> torch.linalg.solve_triangular\n\n* More fixes to torch.linalg.solve_triangular\n\n* Bump torch version\n\n* Bump Python version 3.6 -> 3.7\n\n* Fix some tests\n\n* Decrease tolerance on stable tests\n\n* Remove obsolete xfail\n\n* Resolve #3032\n\n* Fix solve_triangular in ops.gaussian, revert test weakening\n\n* Fix catching of singular matrices in hmc\n\n* xfail funsor tests\n\n* Work around pandas 1.3 bug\n\n* Allow mypy to install missing stubs\n\n* Clarify triangular_solve test\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def _triu_inverse(x):",
            "return x.reciprocal()",
            "else:",
            "identity = torch.eye(x.size(-1), dtype=x.dtype, device=x.device)",
            "-        return torch.triangular_solve(identity, x, upper=True)[0]",
            "+        return torch.linalg.solve_triangular(x, identity, upper=True)",
            "",
            "",
            "class BlockMassMatrix:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=return_statement), node=ASTNode(type=call), position=1)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=672076)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=672077)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'solve_triangular'), position=2, insert_id=672078)",
            "Update(target_node=ASTNode(type=identifier, text=identity), value='x')",
            "Update(target_node=ASTNode(type=identifier, text=x), value='identity')",
            "Update(target_node=ASTNode(type=identifier, text=triangular_solve), value='linalg')",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 2716,
        "neg_line": [
            "-return torch.triangular_solve(identity, x, upper=True)[0]"
        ],
        "pos_line": [
            "+return torch.linalg.solve_triangular(x, identity, upper=True)"
        ],
        "core_change": "-return torch.triangular_solve(identity, x, upper=True)[0] +return torch.linalg.solve_triangular(x, identity, upper=True)",
        "core_API": "reciprocal"
    },
    {
        "commit_hash": "c402309470740e17973eeee22a7b90123f85b310",
        "index": "2e0d03827..c97582c8e 100644",
        "commit_message": "fix CI error\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class E2E(ASRInterface, torch.nn.Module):",
            "",
            "if self.use_aux_ctc:",
            "if \"custom\" in self.etype:",
            "-                hs_mask = torch.IntTensor(",
            "-                    [h.size(1) for h in hs_mask],",
            "-                ).to(hs_mask.device)",
            "+                hs_mask = torch.IntTensor([h.size(1) for h in hs_mask]).to(",
            "+                    hs_mask.device",
            "+                )",
            "",
            "loss_ctc = self.aux_ctc_weight * self.aux_ctc(hs_pad, hs_mask, ys_pad)",
            "else:"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 1,
        "number": 2718,
        "neg_line": [
            "-hs_mask = torch.IntTensor(",
            "-[h.size(1) for h in hs_mask],",
            "-).to(hs_mask.device)"
        ],
        "pos_line": [
            "+hs_mask = torch.IntTensor([h.size(1) for h in hs_mask]).to(",
            "+hs_mask.device",
            "+)"
        ],
        "core_change": "-hs_mask = torch.IntTensor( -[h.size(1) for h in hs_mask], -).to(hs_mask.device) +hs_mask = torch.IntTensor([h.size(1) for h in hs_mask]).to( +hs_mask.device +)",
        "core_API": "IntTensor"
    },
    {
        "commit_hash": "7e986cfba8e8e09fbd24ffc1cbfef2914681e02c",
        "index": "254340f..f2bfaa1 100644",
        "commit_message": "Avoid torch.square\n\nSummary: Fix axis_angle conversions where I used torch.square which doesn't work with pytorch 1.4\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D24451546\n\nfbshipit-source-id: ba26f7dad5fa991f0a8f7d3d09ee7151163aecf4\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def quaternion_to_axis_angle(quaternions):",
            "# for x small, sin(x/2) is about x/2 - (x/2)^3/6",
            "# so sin(x/2)/x is about 1/2 - (x*x)/48",
            "sin_half_angles_over_angles[small_angles] = (",
            "-        0.5 - torch.square(angles[small_angles]) / 48",
            "+        0.5 - (angles[small_angles] * angles[small_angles]) / 48",
            ")",
            "return quaternions[..., 1:] / sin_half_angles_over_angles"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('parenthesized_expression', None), position=0, insert_id=921977)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=921978)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('binary_operator', None), position=1, insert_id=921979)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=binary_operator), node=('subscript', None), position=0, insert_id=921980)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=921981)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=subscript), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='angles')",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=torch), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=921982)",
            "Update(target_node=ASTNode(type=identifier, text=square), value='small_angles')",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=square), position=2)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=921983)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 2719,
        "neg_line": [
            "-0.5 - torch.square(angles[small_angles]) / 48"
        ],
        "pos_line": [
            "+0.5 - (angles[small_angles] * angles[small_angles]) / 48"
        ],
        "core_change": "-0.5 - torch.square(angles[small_angles]) / 48 +0.5 - (angles[small_angles] * angles[small_angles]) / 48",
        "core_API": "square"
    },
    {
        "commit_hash": "fdc907675050cb9e3aa887bf89d6c92519178349",
        "index": "6e970f8f..06a4b538 100644",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def PReLU(x, init=0.001, name='output'):",
            "init = tf.constant_initializer(init)",
            "alpha = tf.get_variable('alpha', [], initializer=init)",
            "x = ((1 + alpha) * x + (1 - alpha) * tf.abs(x))",
            "-    return tf.mul(x, 0.5, name=name)",
            "+    return tf.multiply(x, 0.5, name=name)",
            "",
            "",
            "@layer_register(use_scope=False, log_shape=False)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=mul), value='multiply')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2720,
        "neg_line": [
            "-return tf.mul(x, 0.5, name=name)"
        ],
        "pos_line": [
            "+return tf.multiply(x, 0.5, name=name)"
        ],
        "core_change": "-return tf.mul(x, 0.5, name=name) +return tf.multiply(x, 0.5, name=name)",
        "core_API": "constant_initializer"
    },
    {
        "commit_hash": "d2cec0d4159a2faefd503ce75f299f2324631a73",
        "index": "56817eb..2734908 100644",
        "commit_message": "fix sd --no-half\n\n",
        "file": "lama-cleaner.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SD(InpaintModel):",
            "))",
            "",
            "use_gpu = device == torch.device('cuda') and torch.cuda.is_available()",
            "-        torch_dtype = torch.float16 if use_gpu else torch.float32",
            "+        torch_dtype = torch.float16 if use_gpu and fp16 else torch.float32",
            "self.model = StableDiffusionInpaintPipeline.from_pretrained(",
            "self.model_id_or_path,",
            "revision=\"fp16\" if use_gpu and fp16 else \"main\","
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=conditional_expression), node=('boolean_operator', None), position=2, insert_id=483703)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=identifier, text=use_gpu), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=483704)",
            "Insert(target_node=IN(type=boolean_operator), node=('identifier', 'fp16'), position=2, insert_id=483705)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 2721,
        "neg_line": [
            "-torch_dtype = torch.float16 if use_gpu else torch.float32"
        ],
        "pos_line": [
            "+torch_dtype = torch.float16 if use_gpu and fp16 else torch.float32"
        ],
        "core_change": "-torch_dtype = torch.float16 if use_gpu else torch.float32 +torch_dtype = torch.float16 if use_gpu and fp16 else torch.float32",
        "core_API": "device"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "614bd54ca..970d52c76 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario ako <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Newsroom(datasets.GeneratorBasedBuilder):",
            "data_dir = os.path.abspath(os.path.expanduser(dl_manager.manual_dir))",
            "if not os.path.exists(data_dir):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('newsroom', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {}\".format(",
            "-                    data_dir, self.manual_download_instructions",
            "-                )",
            "+                f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('newsroom', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'newsroom\\', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {self.manual_download_instructions}\"'), position=1, insert_id=1781634)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('newsroom', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {}\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=data_dir))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 2723,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('newsroom', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {}\".format(",
            "-data_dir, self.manual_download_instructions",
            "-)"
        ],
        "pos_line": [
            "+f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('newsroom', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {self.manual_download_instructions}\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('newsroom', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {}\".format( -data_dir, self.manual_download_instructions -) +f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('newsroom', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {self.manual_download_instructions}\"",
        "core_API": "abspath"
    },
    {
        "commit_hash": "4c93a0ac8e309cb186165eba098fb8a379f9e349",
        "index": "f7771aa..358d130 100644",
        "commit_message": "fix the black background issue on certain GPUs.\n\n",
        "file": "CodeFormer.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def set_realesrgan():",
            "tile=args.bg_tile,",
            "tile_pad=40,",
            "pre_pad=0,",
            "-        half=torch.cuda.is_available(), # need to set False in CPU/MPS mode",
            "+        half=use_half",
            ")",
            "",
            "if not gpu_is_available():  # CPU"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='use_half')",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=torch), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=is_available))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=expression_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 2724,
        "neg_line": [
            "-half=torch.cuda.is_available(), # need to set False in CPU/MPS mode"
        ],
        "pos_line": [
            "+half=use_half"
        ],
        "core_change": "-half=torch.cuda.is_available(), # need to set False in CPU/MPS mode +half=use_half",
        "core_API": "is_available"
    },
    {
        "commit_hash": "9b6385488617ee99188b60c0525cbe4ec7e8f2a0",
        "index": "1c840123..9a616a20 100644",
        "commit_message": "Improve reproduceability 2/3 (#1906)\n\n* [Repro] Correct reproducability\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* need better image\n\n* allow conversion from no state dict checkpoints\n\n* up\n\n* up\n\n* up\n\n* up\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* next try\n\n* up\n\n* up\n\n* better name\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\n* correct more\n\n* up\n\n* replace all torch randn\n\n* fix\n\n* correct\n\n* correct\n\n* finish\n\n* fix more\n\n* up\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ValueGuidedRLPipeline(DiffusionPipeline):",
            "shape = (batch_size, planning_horizon, self.state_dim + self.action_dim)",
            "",
            "# generate initial noise and apply our conditions (to make the trajectories start at current state)",
            "-        x1 = torch.randn(shape, device=self.unet.device)",
            "+        x1 = randn_tensor(shape, device=self.unet.device)",
            "x = self.reset_x0(x1, conditions, self.action_dim)",
            "x = self.to_torch(x)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='randn_tensor')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=randn))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2726,
        "neg_line": [
            "-x1 = torch.randn(shape, device=self.unet.device)"
        ],
        "pos_line": [
            "+x1 = randn_tensor(shape, device=self.unet.device)"
        ],
        "core_change": "-x1 = torch.randn(shape, device=self.unet.device) +x1 = randn_tensor(shape, device=self.unet.device)",
        "core_API": "randn"
    },
    {
        "commit_hash": "dd2ae0b76a1a84c4dcc3a4c5a93ea2ccbb4c5c43",
        "index": "15a3bad50..cd9d01688 100644",
        "commit_message": "Small fix at apply_to_network\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class DomainClient(Client):",
            "",
            "return response",
            "",
            "-    def apply_to_network(self, target: str, reason: str):",
            "+    def apply_to_network(self,",
            "+            target: str,",
            "+            reason: str,",
            "+            route_index: int = 0):",
            "self.association.create(",
            "target=target,",
            "-            sender=self.conn.base_url.replace(\"/api/v1\", \"\"),",
            "+            sender=self.routes[route_index].connection.base_url.replace(\"/api/v1\", \"\"),",
            "reason=reason,",
            "node_name=self.name,",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=6, insert_id=1446641)",
            "Insert(target_node=ASTNode(type=parameters), node=('typed_default_parameter', None), position=7, insert_id=1446642)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('identifier', 'route_index'), position=0, insert_id=1446643)",
            "Insert(target_node=IN(type=typed_default_parameter), node=(':', ':'), position=1, insert_id=1446644)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('type', None), position=2, insert_id=1446645)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('=', '='), position=3, insert_id=1446646)",
            "Insert(target_node=IN(type=typed_default_parameter), node=('integer', '0'), position=4, insert_id=1446647)",
            "Insert(target_node=IN(type=type), node=('identifier', 'int'), position=0, insert_id=1446648)",
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=1446649)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1446650)",
            "Insert(target_node=IN(type=attribute), node=('subscript', None), position=0, insert_id=1446651)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'connection'), position=2, insert_id=1446652)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1446653)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'route_index'), position=2, insert_id=1446654)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1446655)",
            "Update(target_node=ASTNode(type=identifier, text=conn), value='routes')"
        ],
        "plus_line": 5,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 2727,
        "neg_line": [
            "-def apply_to_network(self, target: str, reason: str):",
            "-sender=self.conn.base_url.replace(\"/api/v1\", \"\"),"
        ],
        "pos_line": [
            "+def apply_to_network(self,",
            "+target: str,",
            "+reason: str,",
            "+route_index: int = 0):",
            "+sender=self.routes[route_index].connection.base_url.replace(\"/api/v1\", \"\"),"
        ],
        "core_change": "-def apply_to_network(self, target: str, reason: str): +def apply_to_network(self, +target: str, +reason: str, +route_index: int = 0): -sender=self.conn.base_url.replace(\"/api/v1\", \"\"), +sender=self.routes[route_index].connection.base_url.replace(\"/api/v1\", \"\"),",
        "core_API": "create"
    },
    {
        "commit_hash": "b70891e252cd223c920c37ba7873f7ab6a338589",
        "index": "ebfdb929..4e175e10 100644",
        "commit_message": "Fix OOM while loading best model state (#3506)\n\nFixes #3499\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Checkpointer(Registrable):",
            "if self._serialization_dir:",
            "logger.info(\"loading best weights\")",
            "best_model_state_path = os.path.join(self._serialization_dir, \"best.th\")",
            "-            return torch.load(best_model_state_path)",
            "+            return torch.load(best_model_state_path, map_location=nn_util.device_mapping(-1))",
            "else:",
            "logger.info(",
            "\"cannot load best weights without `serialization_dir`, \""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=23085)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=23086)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=23087)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'map_location'), position=0, insert_id=23088)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=23089)",
            "Insert(target_node=IN(type=keyword_argument), node=('call', None), position=2, insert_id=23090)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=23091)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=23092)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nn_util'), position=0, insert_id=23093)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=23094)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device_mapping'), position=2, insert_id=23095)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=23096)",
            "Insert(target_node=IN(type=argument_list), node=('unary_operator', '-1'), position=1, insert_id=23097)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 2728,
        "neg_line": [
            "-return torch.load(best_model_state_path)"
        ],
        "pos_line": [
            "+return torch.load(best_model_state_path, map_location=nn_util.device_mapping(-1))"
        ],
        "core_change": "-return torch.load(best_model_state_path) +return torch.load(best_model_state_path, map_location=nn_util.device_mapping(-1))",
        "core_API": "info"
    },
    {
        "commit_hash": "4d3bf767c4dbe846fa585b72401479501f6e347d",
        "index": "e029ecb505..df4b0daf61 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "def test_exp2(",
            "",
            "# copysign",
            "@handle_test(",
            "-    fn_tree='functional.experimental.copysign',",
            "+    fn_tree=\"functional.experimental.copysign\",",
            "dtype_x1_x2=helpers.dtype_and_values(",
            "available_dtypes=helpers.get_dtypes(\"float\"),",
            "num_arrays=2,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='functional.experimental.copysign'), value='\"functional.experimental.copysign\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2729,
        "neg_line": [
            "-fn_tree='functional.experimental.copysign',"
        ],
        "pos_line": [
            "+fn_tree=\"functional.experimental.copysign\","
        ],
        "core_change": "-fn_tree='functional.experimental.copysign', +fn_tree=\"functional.experimental.copysign\",",
        "core_API": "dtype_and_values"
    },
    {
        "commit_hash": "62e3d1dbdc88af9ed8afb37f7e8ecc011f5486a3",
        "index": "f5db2fb9..11dac622 100644",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PretrainedTransformerEmbedder(TokenEmbedder):",
            "return self.output_dim",
            "",
            "def forward(",
            "-        self, token_ids: torch.LongTensor, attention_mask: torch.LongTensor",
            "+        self, token_ids: torch.LongTensor, mask: torch.LongTensor",
            ") -> torch.Tensor:  # type: ignore",
            "",
            "-        return self.transformer_model(input_ids=token_ids, attention_mask=attention_mask)[0]",
            "+        return self.transformer_model(input_ids=token_ids, attention_mask=mask)[0]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=attention_mask), value='mask')",
            "Update(target_node=ASTNode(type=identifier, text=attention_mask), value='mask')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 2730,
        "neg_line": [
            "-self, token_ids: torch.LongTensor, attention_mask: torch.LongTensor",
            "-return self.transformer_model(input_ids=token_ids, attention_mask=attention_mask)[0]"
        ],
        "pos_line": [
            "+self, token_ids: torch.LongTensor, mask: torch.LongTensor",
            "+return self.transformer_model(input_ids=token_ids, attention_mask=mask)[0]"
        ],
        "core_change": "-self, token_ids: torch.LongTensor, attention_mask: torch.LongTensor +self, token_ids: torch.LongTensor, mask: torch.LongTensor -return self.transformer_model(input_ids=token_ids, attention_mask=attention_mask)[0] +return self.transformer_model(input_ids=token_ids, attention_mask=mask)[0]",
        "core_API": "transformer_model"
    },
    {
        "commit_hash": "a2f4f439e0396810032d00cea8938abd3950053a",
        "index": "eb090a41..2238ec6c 100644",
        "commit_message": "misc fix\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def class_balanced_sigmoid_cross_entropy(logits, label, name='cross_entropy_loss",
            ":param label: size: the ground truth in {0,1}, of the same shape as logits.",
            ":returns: a scalar. class-balanced cross entropy loss",
            "\"\"\"",
            "-    z = batch_flatten(logits)",
            "-    y = tf.cast(batch_flatten(label), tf.float32)",
            "+    y = tf.cast(label, tf.float32)",
            "",
            "count_neg = tf.reduce_sum(1. - y)",
            "count_pos = tf.reduce_sum(y)",
            "beta = count_neg / (count_neg + count_pos)",
            "",
            "pos_weight = beta / (1 - beta)",
            "-    cost = tf.nn.weighted_cross_entropy_with_logits(z, y, pos_weight)",
            "+    cost = tf.nn.weighted_cross_entropy_with_logits(logits, y, pos_weight)",
            "cost = tf.reduce_mean(cost * (1 - beta), name=name)",
            "",
            "#logstable = tf.log(1 + tf.exp(-tf.abs(z)))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=z), value='y')",
            "Update(target_node=ASTNode(type=identifier, text=z), value='logits')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=label), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=batch_flatten))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=logits))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=y))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=batch_flatten))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 18,
        "number": 2733,
        "neg_line": [
            "-z = batch_flatten(logits)",
            "-y = tf.cast(batch_flatten(label), tf.float32)",
            "-cost = tf.nn.weighted_cross_entropy_with_logits(z, y, pos_weight)"
        ],
        "pos_line": [
            "+y = tf.cast(label, tf.float32)",
            "+cost = tf.nn.weighted_cross_entropy_with_logits(logits, y, pos_weight)"
        ],
        "core_change": "-z = batch_flatten(logits) -y = tf.cast(batch_flatten(label), tf.float32) +y = tf.cast(label, tf.float32) -cost = tf.nn.weighted_cross_entropy_with_logits(z, y, pos_weight) +cost = tf.nn.weighted_cross_entropy_with_logits(logits, y, pos_weight)",
        "core_API": "cast"
    },
    {
        "commit_hash": "9000ad931722064fb2efe38649389a5154538ce4",
        "index": "b9b99d01..635bc9e8 100755",
        "commit_message": "various improvements and fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SubsamplingStep(MetaOptimizer):",
            "num_samples = tf.maximum(x=tf.cast(x=num_samples, dtype=util.tf_dtype('int')), y=one)",
            "indices = tf.random.uniform(shape=(num_samples,), maxval=batch_size, dtype=tf.int32)",
            "",
            "-        function = (lambda x: x if util.rank(x=x) == 0 else tf.gather(params=x, indices=indices))",
            "+        function = (lambda x: tf.gather(params=x, indices=indices))",
            "subsampled_arguments = util.fmap(function=function, xs=arguments)",
            "",
            "return self.optimizer.step(variables=variables, arguments=subsampled_arguments, **kwargs)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=lambda), node=ASTNode(type=call), position=3)",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=util))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=rank))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=x))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type===, text===))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=else, text=else))",
            "Delete(target_node=ASTNode(type=conditional_expression))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 2734,
        "neg_line": [
            "-function = (lambda x: x if util.rank(x=x) == 0 else tf.gather(params=x, indices=indices))"
        ],
        "pos_line": [
            "+function = (lambda x: tf.gather(params=x, indices=indices))"
        ],
        "core_change": "-function = (lambda x: x if util.rank(x=x) == 0 else tf.gather(params=x, indices=indices)) +function = (lambda x: tf.gather(params=x, indices=indices))",
        "core_API": "maximum"
    },
    {
        "commit_hash": "24bf6ad607b491fc28c83b930e7cec927599d73f",
        "index": "bb21ce429..9eec9348e 100644",
        "commit_message": "[raysgd] Improve raysgd examples (#7818)\n\n* better_example\n\n* test\n\n* improve some usability things\n\n* submit\n\n* fix\n\n* flake\n\n* Update python/ray/util/sgd/torch/training_operator.py\n\n* trythis\n\n* fix\n\n* fix\n\n* smoke\n\n* fail\n\n* fix\n\n* fix\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class GANOperator(TrainingOperator):",
            "",
            "# Compute a discriminator update for real images",
            "discriminator.zero_grad()",
            "+        # self.device is set automatically",
            "real_cpu = batch[0].to(self.device)",
            "batch_size = real_cpu.size(0)",
            "label = torch.full((batch_size, ), real_label, device=self.device)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 2735,
        "neg_line": [],
        "pos_line": [
            "+# self.device is set automatically"
        ],
        "core_change": "+# self.device is set automatically",
        "core_API": "zero_grad"
    },
    {
        "commit_hash": "8cb9bd403e0b8206eb69780b97b35cf7aa84bd4e",
        "index": "10e7c23..922779c 100644",
        "commit_message": "fix colorspace bug & support multi-gpu and multi-processing (#312)\n\n* fix colorspace bug of ffmpeg stream, add multi-gpu and multi-processing suport for inference_realesrgan_video.py\n\n* fix code format\n\nCo-authored-by: yanzewu <yanzewu@tencent.com>\n",
        "file": "Real-ESRGAN.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RealESRGANer():",
            "self.half = half",
            "",
            "# initialize model",
            "-        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
            "+        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else device",
            "# if the model_path starts with https, it will first download models to the folder: realesrgan/weights",
            "if model_path.startswith('https://'):",
            "model_path = load_file_from_url("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('conditional_expression', None), position=2, insert_id=1128758)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=1128759)",
            "Insert(target_node=IN(type=conditional_expression), node=('comparison_operator', None), position=2, insert_id=1128760)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=1128761)",
            "Insert(target_node=IN(type=conditional_expression), node=('identifier', 'device'), position=4, insert_id=1128762)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'device'), position=0, insert_id=1128763)",
            "Insert(target_node=IN(type=comparison_operator), node=('is', 'is'), position=1, insert_id=1128764)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=2, insert_id=1128765)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 2738,
        "neg_line": [
            "-self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
        ],
        "pos_line": [
            "+self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else device"
        ],
        "core_change": "-self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') +self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else device",
        "core_API": "device"
    },
    {
        "commit_hash": "c253f3a041d0e5091508fea205b25d8ab3946804",
        "index": "7c3789ac..3794422a 100644",
        "commit_message": "GH-1660: fix mean pooling in PooledFlairEmbeddings\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PooledFlairEmbeddings(TokenEmbeddings):",
            "",
            "# set aggregation operation",
            "if self.pooling == \"mean\":",
            "-                                aggregated_embedding = torch.mean(self.word_embeddings[token.text], local_embedding)",
            "+                                aggregated_embedding = torch.add(self.word_embeddings[token.text], local_embedding)",
            "elif self.pooling == \"fade\":",
            "aggregated_embedding = torch.add(self.word_embeddings[token.text], local_embedding)",
            "aggregated_embedding /= 2"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=mean), value='add')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2741,
        "neg_line": [
            "-aggregated_embedding = torch.mean(self.word_embeddings[token.text], local_embedding)"
        ],
        "pos_line": [
            "+aggregated_embedding = torch.add(self.word_embeddings[token.text], local_embedding)"
        ],
        "core_change": "-aggregated_embedding = torch.mean(self.word_embeddings[token.text], local_embedding) +aggregated_embedding = torch.add(self.word_embeddings[token.text], local_embedding)",
        "core_API": "mean"
    },
    {
        "commit_hash": "6cc8eb6fda5827a82a1ae52ec11c355a013af2fc",
        "index": "557ce1e..47d4728 100644",
        "commit_message": "Fix cafferesnet101 (fc -> last_linear)\n\n",
        "file": "pretrained-models.pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class ResNet(nn.Module):",
            "",
            "x = self.avgpool(x)",
            "x = x.view(x.size(0), -1)",
            "-    x = self.fc(x)",
            "+    x = self.last_linear(x)",
            "",
            "return x"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=fc), value='last_linear')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2743,
        "neg_line": [
            "-x = self.fc(x)"
        ],
        "pos_line": [
            "+x = self.last_linear(x)"
        ],
        "core_change": "-x = self.fc(x) +x = self.last_linear(x)",
        "core_API": "avgpool"
    },
    {
        "commit_hash": "ed8236fa188c81426fba004ea198b05f0c1c4e38",
        "index": "23d688e..c8be695 100755",
        "commit_message": "Fix for unscale usage in fp16_utils.FP16_Optimizer\n\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class FP16_Optimizer(object):",
            "# print([param.grad.data for param in self.all_fp32_from_fp32_params])",
            "# quit()",
            "self.overflow = self.loss_scaler.update_scale()",
            "+        # torch.cuda.nvtx.range_pop()",
            "",
            "",
            "def inspect_master_grad_data(self):"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 2745,
        "neg_line": [],
        "pos_line": [
            "+# torch.cuda.nvtx.range_pop()"
        ],
        "core_change": "+# torch.cuda.nvtx.range_pop()",
        "core_API": "update_scale"
    },
    {
        "commit_hash": "44af3fafb527302282f6b6507b952de7435f0979",
        "index": "b41016de4..d1649a8f5 100644",
        "commit_message": "Fix NonMatchingChecksumError in mbpp dataset (#4788)\n\n* Increase patch version of mbpp dataset\n\n* Update metadata JSON\n\n* Rename dummy data paths\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "_LICENSE = \"CC-BY-4.0\"",
            "class MBPP(datasets.GeneratorBasedBuilder):",
            "\"\"\"MBPP: Mostly Basic Python Problems Dataset\"\"\"",
            "",
            "-    VERSION = datasets.Version(\"1.0.0\")",
            "+    VERSION = datasets.Version(\"1.0.1\")",
            "",
            "BUILDER_CONFIGS = [",
            "datasets.BuilderConfig(",
            "name=f\"{split}\",",
            "-            version=datasets.Version(\"1.0.0\"),",
            "+            version=datasets.Version(\"1.0.1\"),",
            "description=_DESCRIPTION,",
            ")",
            "for split in _SPLITS"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1777190)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'datasets'), position=0, insert_id=1777191)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1777192)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Version'), position=2, insert_id=1777193)",
            "Update(target_node=ASTNode(type=string, text=\"1.0.0\"), value='\"1.0.1\"')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=string, text=\"1.0.0\"), value='\"1.0.1\"')",
            "Delete(target_node=ASTNode(type=identifier, text=datasets))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Version))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 2746,
        "neg_line": [
            "-VERSION = datasets.Version(\"1.0.0\")",
            "-version=datasets.Version(\"1.0.0\"),"
        ],
        "pos_line": [
            "+VERSION = datasets.Version(\"1.0.1\")",
            "+version=datasets.Version(\"1.0.1\"),"
        ],
        "core_change": "-VERSION = datasets.Version(\"1.0.0\") +VERSION = datasets.Version(\"1.0.1\") -version=datasets.Version(\"1.0.0\"), +version=datasets.Version(\"1.0.1\"),",
        "core_API": "Version"
    },
    {
        "commit_hash": "723a002e60695eaa64271a43e482e037fc1f54dc",
        "index": "5c08ace..c954be6 100644",
        "commit_message": "go through proposal_layer, minor fix\n\n",
        "file": "faster-rcnn.pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class _ProposalLayer(nn.Module):",
            "# blob = np.hstack((batch_inds, proposals.astype(np.float32, copy=False)))",
            "# top[0].reshape(*(blob.shape))",
            "# top[0].data[...] = blob",
            "-        batch_inds = torch.FloatTensor(proposals.size(0), 1).zero_()",
            "-        output = torch.cat((batch_inds, proposals), 1)",
            "+",
            "+        self.batch_inds.resize_(proposals.size(0), 1).zero_()",
            "+        output = torch.cat((self.batch_inds, proposals), 1)",
            "",
            "# [Optional] output scores blob",
            "# if len(top) > 1:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=231494)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=231495)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'resize_'), position=2, insert_id=231496)",
            "Insert(target_node=ASTNode(type=tuple), node=('attribute', None), position=1, insert_id=231497)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='self')",
            "Update(target_node=ASTNode(type=identifier, text=FloatTensor), value='batch_inds')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=231498)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=231499)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=batch_inds), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=batch_inds))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 14,
        "number": 2747,
        "neg_line": [
            "-batch_inds = torch.FloatTensor(proposals.size(0), 1).zero_()",
            "-output = torch.cat((batch_inds, proposals), 1)"
        ],
        "pos_line": [
            "+",
            "+self.batch_inds.resize_(proposals.size(0), 1).zero_()",
            "+output = torch.cat((self.batch_inds, proposals), 1)"
        ],
        "core_change": "-batch_inds = torch.FloatTensor(proposals.size(0), 1).zero_() -output = torch.cat((batch_inds, proposals), 1) + +self.batch_inds.resize_(proposals.size(0), 1).zero_() +output = torch.cat((self.batch_inds, proposals), 1)",
        "core_API": "hstack"
    },
    {
        "commit_hash": "47d5c244706ab01859a3931f66066407a9d5efbb",
        "index": "fb7b8e1e..0f7f8d67 100755",
        "commit_message": "Summaries completely changed, distributed mode incompletely changed, various fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Evolutionary(Optimizer):",
            "applied = self.apply_step(variables=variables, diffs=perturbation_diffs)",
            "",
            "with tf.control_dependencies(control_inputs=(applied,)):",
            "-            return [tf.identity(input=diff) for diff in diffs]",
            "+            return [diff + 0.0 for diff in diffs]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list_comprehension), node=('binary_operator', None), position=1, insert_id=2242665)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='diff')",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=tf), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=2242666)",
            "Insert(target_node=IN(type=binary_operator), node=('float', '0.0'), position=2, insert_id=2242667)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=identity))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=input))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=diff))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 2750,
        "neg_line": [
            "-return [tf.identity(input=diff) for diff in diffs]"
        ],
        "pos_line": [
            "+return [diff + 0.0 for diff in diffs]"
        ],
        "core_change": "-return [tf.identity(input=diff) for diff in diffs] +return [diff + 0.0 for diff in diffs]",
        "core_API": "apply_step"
    },
    {
        "commit_hash": "bf4d36ace84efa7f53136023dc8e452bbeb80a07",
        "index": "6ef06a4..8ff2cbd 100644",
        "commit_message": "fix @tf.function bug when iteration with tf.tensor\n\n",
        "file": "TensorFlowTTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFTacotron2(tf.keras.Model):",
            "alignment_size=max_length_encoder",
            ")",
            "",
            "-        for i in tf.range(max_decoder_steps):",
            "+        for i in range(max_decoder_steps):",
            "decoder_inputs = TFTacotronDecoderInput(",
            "time_first_mels_outputs[i],",
            "encoder_hidden_states,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=range), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 2753,
        "neg_line": [
            "-for i in tf.range(max_decoder_steps):"
        ],
        "pos_line": [
            "+for i in range(max_decoder_steps):"
        ],
        "core_change": "-for i in tf.range(max_decoder_steps): +for i in range(max_decoder_steps):",
        "core_API": "range"
    },
    {
        "commit_hash": "7152ed2bae51da937d614173980c82dcd51cc79f",
        "index": "f39fc48d2..7a3c5635f 100644",
        "commit_message": "Result of new doc style with fixes (#17015)\n\n* Result of new doc style with fixes\n\n* Add last two files\n\n* Bump hf-doc-builder\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class RagRetriever:",
            "",
            ">>> dataset = (",
            "...     ...",
            "-    >>> )  # dataset must be a datasets.Datasets object with columns \"title\", \"text\" and \"embeddings\", and it must have a faiss index",
            "+    ... )  # dataset must be a datasets.Datasets object with columns \"title\", \"text\" and \"embeddings\", and it must have a faiss index",
            ">>> retriever = RagRetriever.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", indexed_dataset=dataset)",
            "",
            ">>> # To load your own indexed dataset built with the datasets library that was saved on disk. More info in examples/rag/use_own_knowledge_dataset.py"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=parenthesized_expression), position=1)",
            "Insert(target_node=ASTNode(type=ERROR), node=('ellipsis', '...'), position=0, insert_id=1804737)",
            "Delete(target_node=ASTNode(type=>>, text=>>))",
            "Delete(target_node=ASTNode(type=>, text=>))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 2754,
        "neg_line": [
            "->>> )  # dataset must be a datasets.Datasets object with columns \"title\", \"text\" and \"embeddings\", and it must have a faiss index"
        ],
        "pos_line": [
            "+... )  # dataset must be a datasets.Datasets object with columns \"title\", \"text\" and \"embeddings\", and it must have a faiss index"
        ],
        "core_change": "->>> )  # dataset must be a datasets.Datasets object with columns \"title\", \"text\" and \"embeddings\", and it must have a faiss index +... )  # dataset must be a datasets.Datasets object with columns \"title\", \"text\" and \"embeddings\", and it must have a faiss index",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "dac779a5473a40a8231b9dcf95587bcb15400c1a",
        "index": "72ae804..29aefff 100644",
        "commit_message": "Fix pylint errors\n\n",
        "file": "seq2seq.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DecodeText(InferenceTask):",
            "if \"attention_scores\" in self._predictions:",
            "fetches[\"attention_scores\"] = self._predictions[\"attention_scores\"]",
            "",
            "-    return SessionRunArgs(fetches)",
            "+    return tf.train.SessionRunArgs(fetches)",
            "",
            "def after_run(self, _run_context, run_values):",
            "fetches_batch = run_values.results"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2159301)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2159302)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2159303)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=SessionRunArgs), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2159304)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2159305)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'train'), position=2, insert_id=2159306)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 2761,
        "neg_line": [
            "-return SessionRunArgs(fetches)"
        ],
        "pos_line": [
            "+return tf.train.SessionRunArgs(fetches)"
        ],
        "core_change": "-return SessionRunArgs(fetches) +return tf.train.SessionRunArgs(fetches)",
        "core_API": "SessionRunArgs"
    },
    {
        "commit_hash": "25361dcc2e2f62d45a0544bf9eaacc8b739e7029",
        "index": "c7441ae1..760ce291 100644",
        "commit_message": "Minor fixes in tagger/parser\n\n",
        "file": "stanza.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Parser(nn.Module):",
            "dist_kld = dist_kld[:, 1:].masked_select(goldmask)",
            "loss -= dist_kld.sum()",
            "",
            "-            loss /= word.size(0)",
            "+            loss /= wordchars.size(0) # number of words",
            "else:",
            "loss = 0",
            "preds.append(F.log_softmax(unlabeled_scores, 2).detach().cpu().numpy())"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=word), value='wordchars')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2765,
        "neg_line": [
            "-loss /= word.size(0)"
        ],
        "pos_line": [
            "+loss /= wordchars.size(0) # number of words"
        ],
        "core_change": "-loss /= word.size(0) +loss /= wordchars.size(0) # number of words",
        "core_API": "sum"
    },
    {
        "commit_hash": "c4c532d25e012dbe6ab1ac14bca75e53e0acc621",
        "index": "1edc96f2..6c5dd469 100644",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Highway(torch.nn.Module):",
            "layer.bias[input_dim:].data.fill_(1)",
            "",
            "@overrides",
            "-    def forward(self, inputs: torch.Tensor) -> torch.Tensor:  # pylint: disable=arguments-differ",
            "+    def forward(self, inputs: torch.Tensor) -> torch.Tensor:",
            "current_input = inputs",
            "for layer in self._layers:",
            "projected_input = layer(current_input)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2766,
        "neg_line": [
            "-def forward(self, inputs: torch.Tensor) -> torch.Tensor:  # pylint: disable=arguments-differ"
        ],
        "pos_line": [
            "+def forward(self, inputs: torch.Tensor) -> torch.Tensor:"
        ],
        "core_change": "-def forward(self, inputs: torch.Tensor) -> torch.Tensor:  # pylint: disable=arguments-differ +def forward(self, inputs: torch.Tensor) -> torch.Tensor:",
        "core_API": "fill_"
    },
    {
        "commit_hash": "2097311899dff8247fcfb8d5092bde3078dfd32a",
        "index": "bd75dad1..77e9cfdf 100755",
        "commit_message": "bug fix in hed\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "for idx, b in enumerate([b1, b2, b3, b4, b5, final_map]):",
            "output = tf.nn.sigmoid(b, name='output{}'.format(idx+1))",
            "xentropy = class_balanced_sigmoid_cross_entropy(",
            "-                tf.squeeze(b, [3]), edgemap,",
            "+                b, edgemap,",
            "name='xentropy{}'.format(idx+1))",
            "costs.append(xentropy)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=b), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=squeeze))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=3))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 2769,
        "neg_line": [
            "-tf.squeeze(b, [3]), edgemap,"
        ],
        "pos_line": [
            "+b, edgemap,"
        ],
        "core_change": "-tf.squeeze(b, [3]), edgemap, +b, edgemap,",
        "core_API": "sigmoid"
    },
    {
        "commit_hash": "2fe1a3ad30cb5fa7226eee375f1008f1b647b1d3",
        "index": "dc7a713b..eadf0659 100644",
        "commit_message": "fix mypy error\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def focal_loss(",
            "device=input.device, dtype=input.dtype)",
            "",
            "# compute the actual focal loss",
            "-    weight = torch.pow(1. - input_soft, gamma)",
            "+    weight = torch.pow(-input_soft + 1., gamma)",
            "",
            "focal = -alpha * weight * torch.log(input_soft)",
            "loss_tmp = torch.sum(target_one_hot * focal, dim=1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('unary_operator', None), position=0, insert_id=453864)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('+', '+'), position=1, insert_id=453865)",
            "Move(target_node=IN(type=unary_operator), node=ASTNode(type=-, text=-), position=0)",
            "Move(target_node=IN(type=unary_operator), node=ASTNode(type=identifier, text=input_soft), position=1)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 2771,
        "neg_line": [
            "-weight = torch.pow(1. - input_soft, gamma)"
        ],
        "pos_line": [
            "+weight = torch.pow(-input_soft + 1., gamma)"
        ],
        "core_change": "-weight = torch.pow(1. - input_soft, gamma) +weight = torch.pow(-input_soft + 1., gamma)",
        "core_API": "pow"
    },
    {
        "commit_hash": "fda703a55374b3caaf4e886016f7de5810fa3571",
        "index": "18f41e8cf..493326157 100644",
        "commit_message": "Fix integration slow tests (#10670)\n\n* PoC\n\n* Fix slow tests for the PT1.8 Embedding problem\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SqueezeBertModelIntegrationTest(unittest.TestCase):",
            "def test_inference_classification_head(self):",
            "model = SqueezeBertForSequenceClassification.from_pretrained(\"squeezebert/squeezebert-mnli\")",
            "",
            "-        input_ids = torch.tensor([[0, 29414, 232, 328, 740, 1140, 12695, 69, 13, 1588, 2]])",
            "+        input_ids = torch.tensor([[1, 29414, 232, 328, 740, 1140, 12695, 69, 13, 1588, 2]])",
            "output = model(input_ids)[0]",
            "expected_shape = torch.Size((1, 3))",
            "self.assertEqual(output.shape, expected_shape)",
            "-        expected_tensor = torch.tensor([[0.5075, 0.0682, -0.5881]])",
            "+        expected_tensor = torch.tensor([[0.6401, -0.0349, -0.6041]])",
            "self.assertTrue(torch.allclose(output, expected_tensor, atol=1e-4))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1219212)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1219213)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1219214)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=1219215)",
            "Update(target_node=ASTNode(type=integer, text=0), value='1')",
            "Update(target_node=ASTNode(type=float, text=0.5075), value='0.6401')",
            "Insert(target_node=ASTNode(type=list), node=('unary_operator', None), position=3, insert_id=1219216)",
            "Insert(target_node=IN(type=unary_operator), node=('-', '-'), position=0, insert_id=1219217)",
            "Insert(target_node=IN(type=unary_operator), node=('float', '0.0349'), position=1, insert_id=1219218)",
            "Update(target_node=ASTNode(type=float, text=0.5881), value='0.6041')",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=float, text=0.0682))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 2772,
        "neg_line": [
            "-input_ids = torch.tensor([[0, 29414, 232, 328, 740, 1140, 12695, 69, 13, 1588, 2]])",
            "-expected_tensor = torch.tensor([[0.5075, 0.0682, -0.5881]])"
        ],
        "pos_line": [
            "+input_ids = torch.tensor([[1, 29414, 232, 328, 740, 1140, 12695, 69, 13, 1588, 2]])",
            "+expected_tensor = torch.tensor([[0.6401, -0.0349, -0.6041]])"
        ],
        "core_change": "-input_ids = torch.tensor([[0, 29414, 232, 328, 740, 1140, 12695, 69, 13, 1588, 2]]) +input_ids = torch.tensor([[1, 29414, 232, 328, 740, 1140, 12695, 69, 13, 1588, 2]]) -expected_tensor = torch.tensor([[0.5075, 0.0682, -0.5881]]) +expected_tensor = torch.tensor([[0.6401, -0.0349, -0.6041]])",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "bc7480f19408721d85949a12d6efe9e663638461",
        "index": "8cfe47a..96bbb8b 100644",
        "commit_message": "fixed bugs in torch1.6\n\n",
        "file": "insightface.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SyntheticDataset(Dataset):",
            "img = torch.from_numpy(img).squeeze(0).float()",
            "img = ((img / 255) - 0.5) / 0.5",
            "self.img = img",
            "-        self.label = torch.ones([1], dtype=torch.long)",
            "+        self.label = 1",
            "",
            "def __getitem__(self, index):",
            "return self.img, self.label"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=integer, text=1), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=ones))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=long))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 2774,
        "neg_line": [
            "-self.label = torch.ones([1], dtype=torch.long)"
        ],
        "pos_line": [
            "+self.label = 1"
        ],
        "core_change": "-self.label = torch.ones([1], dtype=torch.long) +self.label = 1",
        "core_API": "from_numpy"
    },
    {
        "commit_hash": "c7fd1d9fb18862318b133c9474d37c5085850070",
        "index": "315eb751..675c5cb0 100755",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(ModelDesc):",
            "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
            "cost = tf.reduce_mean(cost, name='cross_entropy_loss')",
            "",
            "-        wrong = tf.to_float(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), name='incorrect_vector')",
            "+        wrong = tf.cast(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), tf.float32, name='incorrect_vector')",
            "summary.add_moving_summary(tf.reduce_mean(wrong, name='train_error'))",
            "",
            "wd_cost = tf.multiply(1e-5, regularize_cost('fc.*/W', tf.nn.l2_loss),"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=to_float), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2278952)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2278953)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2278954)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2278955)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2278956)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 2775,
        "neg_line": [
            "-wrong = tf.to_float(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), name='incorrect_vector')"
        ],
        "pos_line": [
            "+wrong = tf.cast(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), tf.float32, name='incorrect_vector')"
        ],
        "core_change": "-wrong = tf.to_float(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), name='incorrect_vector') +wrong = tf.cast(tf.logical_not(tf.nn.in_top_k(logits, label, 1)), tf.float32, name='incorrect_vector')",
        "core_API": "sparse_softmax_cross_entropy_with_logits"
    },
    {
        "commit_hash": "76fe1f40c8a2cabc05ed7bdb6a80116586ea65a5",
        "index": "e768592f..89609d76 100644",
        "commit_message": "padding_mode augmentations (#640)\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\nCo-authored-by: shijianjian <sj8716643@126.com>\n\nFixed #635 \n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def random_affine_generator(",
            "sx=sx,",
            "sy=sy,",
            "resample=torch.tensor(Resample.get(resample).value),",
            "+                padding_mode=torch.tensor(SamplePadding.get(padding_mode).value),",
            "align_corners=torch.tensor(align_corners))"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=('default_parameter', None), position=7, insert_id=438025)",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=438026)",
            "Insert(target_node=IN(type=default_parameter), node=('identifier', 'padding_mode'), position=0, insert_id=438027)",
            "Insert(target_node=IN(type=default_parameter), node=('=', '='), position=1, insert_id=438028)",
            "Insert(target_node=IN(type=default_parameter), node=('call', None), position=2, insert_id=438029)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=438030)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=438031)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=438032)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=438033)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=438034)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=438035)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=438036)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=438037)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=438038)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=438039)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'value'), position=2, insert_id=438040)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=438041)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=438042)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'SamplePadding'), position=0, insert_id=438043)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=438044)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get'), position=2, insert_id=438045)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=438046)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'padding_mode'), position=1, insert_id=438047)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=438048)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 24,
        "number": 2779,
        "neg_line": [],
        "pos_line": [
            "+padding_mode=torch.tensor(SamplePadding.get(padding_mode).value),"
        ],
        "core_change": "+padding_mode=torch.tensor(SamplePadding.get(padding_mode).value),",
        "core_API": "tensor"
    },
    {
        "commit_hash": "ebaacba38be267e8f7e2a98e7810bb584deffc29",
        "index": "5dd169031..c46ba13b0 100644",
        "commit_message": "fixing typo in docstring\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BertForNextSentencePrediction(PreTrainedBertModel):",
            "# Already been converted into WordPiece token ids",
            "input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])",
            "input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])",
            "-    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])",
            "+    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])",
            "",
            "config = BertConfig(vocab_size=32000, hidden_size=512,",
            "num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=2), value='1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2780,
        "neg_line": [
            "-token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])"
        ],
        "pos_line": [
            "+token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])"
        ],
        "core_change": "-token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]]) +token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "b9bb417324c0d9013c505dc39c016ab9ca0e23c8",
        "index": "f79491c67..032dbdc1e 100755",
        "commit_message": "Fix a typo relative_postion_if_large -> relative_position_if_large (#17366)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class WavLMAttention(nn.Module):",
            "relative_positions_if_large = torch.log(relative_positions.float() / max_exact)",
            "relative_positions_if_large = relative_positions_if_large / math.log(self.max_distance / max_exact)",
            "relative_positions_if_large = relative_positions_if_large * (num_buckets - max_exact)",
            "-        relative_postion_if_large = (max_exact + relative_positions_if_large).to(torch.long)",
            "-        relative_postion_if_large = torch.min(",
            "-            relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1)",
            "+        relative_position_if_large = (max_exact + relative_positions_if_large).to(torch.long)",
            "+        relative_position_if_large = torch.min(",
            "+            relative_position_if_large, torch.full_like(relative_position_if_large, num_buckets - 1)",
            ")",
            "",
            "-        relative_buckets += torch.where(is_small, relative_positions, relative_postion_if_large)",
            "+        relative_buckets += torch.where(is_small, relative_positions, relative_position_if_large)",
            "return relative_buckets"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=relative_postion_if_large), value='relative_position_if_large')",
            "Update(target_node=ASTNode(type=identifier, text=relative_postion_if_large), value='relative_position_if_large')",
            "Update(target_node=ASTNode(type=identifier, text=relative_postion_if_large), value='relative_position_if_large')",
            "Update(target_node=ASTNode(type=identifier, text=relative_postion_if_large), value='relative_position_if_large')",
            "Update(target_node=ASTNode(type=identifier, text=relative_postion_if_large), value='relative_position_if_large')"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 5,
        "number": 2781,
        "neg_line": [
            "-relative_postion_if_large = (max_exact + relative_positions_if_large).to(torch.long)",
            "-relative_postion_if_large = torch.min(",
            "-relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1)",
            "-relative_buckets += torch.where(is_small, relative_positions, relative_postion_if_large)"
        ],
        "pos_line": [
            "+relative_position_if_large = (max_exact + relative_positions_if_large).to(torch.long)",
            "+relative_position_if_large = torch.min(",
            "+relative_position_if_large, torch.full_like(relative_position_if_large, num_buckets - 1)",
            "+relative_buckets += torch.where(is_small, relative_positions, relative_position_if_large)"
        ],
        "core_change": "-relative_postion_if_large = (max_exact + relative_positions_if_large).to(torch.long) -relative_postion_if_large = torch.min( -relative_postion_if_large, torch.full_like(relative_postion_if_large, num_buckets - 1) +relative_position_if_large = (max_exact + relative_positions_if_large).to(torch.long) +relative_position_if_large = torch.min( +relative_position_if_large, torch.full_like(relative_position_if_large, num_buckets - 1) -relative_buckets += torch.where(is_small, relative_positions, relative_postion_if_large) +relative_buckets += torch.where(is_small, relative_positions, relative_position_if_large)",
        "core_API": "log"
    },
    {
        "commit_hash": "305280def38dedfc447bef4ce1ed140f78fcadbf",
        "index": "524487f..1d73b52 100644",
        "commit_message": "Fix broken TF DistributedOptimizer with Keras 2.11+ (#3822)\n\nSigned-off-by: Nicolas Castet <ncastet@nvidia.com>\n",
        "file": "horovod.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "def train_fn(compute_config: TfDataServiceConfig, reuse_dataset: bool = False, r",
            "",
            "# Horovod: adjust learning rate based on number of GPUs.",
            "scaled_lr = 0.001 * hvd.size()",
            "-    opt = tf.optimizers.Adam(scaled_lr)",
            "+    opt = optimizers.Adam(scaled_lr)",
            "",
            "# Horovod: add Horovod DistributedOptimizer.",
            "opt = hvd.DistributedOptimizer("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=optimizers), value='Adam')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='optimizers')",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Adam))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2782,
        "neg_line": [
            "-opt = tf.optimizers.Adam(scaled_lr)"
        ],
        "pos_line": [
            "+opt = optimizers.Adam(scaled_lr)"
        ],
        "core_change": "-opt = tf.optimizers.Adam(scaled_lr) +opt = optimizers.Adam(scaled_lr)",
        "core_API": "size"
    },
    {
        "commit_hash": "6c16951d1f4e5407698c478fc53dd08b8e149f4f",
        "index": "4ed0360..9d1422f 100644",
        "commit_message": "Pytorch v0.4 compatibility and minor fixes\n\n",
        "file": "pytorch-cnn-visualizations.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CNNLayerVisualization():",
            "self.conv_output = x[0, self.selected_filter]",
            "# Loss function is the mean of the output of the selected layer/filter",
            "# We try to minimize the mean of the output of that specific filter",
            "-            loss = torch.mean(self.conv_output)",
            "-            print('Iteration:', str(i), 'Loss:', \"{0:.2f}\".format(loss.data.numpy()[0]))",
            "+            loss = -torch.mean(self.conv_output)",
            "+            print('Iteration:', str(i), 'Loss:', \"{0:.2f}\".format(loss.data.numpy()))",
            "# Backward",
            "loss.backward()",
            "# Update image"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('unary_operator', None), position=2, insert_id=887676)",
            "Insert(target_node=IN(type=unary_operator), node=('-', '-'), position=0, insert_id=887677)",
            "Move(target_node=IN(type=unary_operator), node=ASTNode(type=call), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=call), position=1)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 2783,
        "neg_line": [
            "-loss = torch.mean(self.conv_output)",
            "-print('Iteration:', str(i), 'Loss:', \"{0:.2f}\".format(loss.data.numpy()[0]))"
        ],
        "pos_line": [
            "+loss = -torch.mean(self.conv_output)",
            "+print('Iteration:', str(i), 'Loss:', \"{0:.2f}\".format(loss.data.numpy()))"
        ],
        "core_change": "-loss = torch.mean(self.conv_output) -print('Iteration:', str(i), 'Loss:', \"{0:.2f}\".format(loss.data.numpy()[0])) +loss = -torch.mean(self.conv_output) +print('Iteration:', str(i), 'Loss:', \"{0:.2f}\".format(loss.data.numpy()))",
        "core_API": "mean"
    },
    {
        "commit_hash": "93b54368f53c7f19931688575ad2c3fb597ccde8",
        "index": "ac88a704b..df85add18 100644",
        "commit_message": "[`BiT`] Small patch fix (#20657)\n\n* patch fix for `fp16`\n\n* use `np` instead\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BitEncoder(nn.Module):",
            "dilation = 1",
            "",
            "layer_dropouts = [",
            "-            x.tolist() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths)).split(config.depths)",
            "+            x.tolist()",
            "+            for x in torch.Tensor(np.linspace(0, config.drop_path_rate, sum(config.depths))).split(config.depths)",
            "]",
            "",
            "for stage_idx, (current_depth, current_hidden_size, layer_dropout) in enumerate("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1181515)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1181516)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1181517)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1181518)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=1181519)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1181520)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=1181521)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1181522)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='np')"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 2784,
        "neg_line": [
            "-x.tolist() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths)).split(config.depths)"
        ],
        "pos_line": [
            "+x.tolist()",
            "+for x in torch.Tensor(np.linspace(0, config.drop_path_rate, sum(config.depths))).split(config.depths)"
        ],
        "core_change": "-x.tolist() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths)).split(config.depths) +x.tolist() +for x in torch.Tensor(np.linspace(0, config.drop_path_rate, sum(config.depths))).split(config.depths)",
        "core_API": "tolist"
    },
    {
        "commit_hash": "d3cb28886ac68beba9a6646b422a4d727b056c0c",
        "index": "ebb3c5034..79a1e0292 100755",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MultiHeadAttention(nn.Module):",
            "q = q / math.sqrt(dim_per_head)  # (bs, n_heads, qlen, dim_per_head)",
            "scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, qlen, klen)",
            "mask = (mask == 0).view(mask_reshape).expand_as(scores)  # (bs, n_heads, qlen, klen)",
            "-        scores.masked_fill_(mask, -float(\"inf\"))  # (bs, n_heads, qlen, klen)",
            "+        scores.masked_fill_(mask, torch.finfo(scores.dtype).min)  # (bs, n_heads, qlen, klen)",
            "",
            "weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)  # (bs, n_heads, qlen, klen)",
            "weights = nn.functional.dropout(weights, p=self.dropout, training=self.training)  # (bs, n_heads, qlen, klen)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=1197381)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=1197382)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1197383)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'min'), position=2, insert_id=1197384)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1197385)",
            "Update(target_node=ASTNode(type=identifier, text=float), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=float), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1197386)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'finfo'), position=2, insert_id=1197387)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1197388)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'scores'), position=0, insert_id=1197389)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1197390)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1197391)",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=string, text=\"inf\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=unary_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 2785,
        "neg_line": [
            "-scores.masked_fill_(mask, -float(\"inf\"))  # (bs, n_heads, qlen, klen)"
        ],
        "pos_line": [
            "+scores.masked_fill_(mask, torch.finfo(scores.dtype).min)  # (bs, n_heads, qlen, klen)"
        ],
        "core_change": "-scores.masked_fill_(mask, -float(\"inf\"))  # (bs, n_heads, qlen, klen) +scores.masked_fill_(mask, torch.finfo(scores.dtype).min)  # (bs, n_heads, qlen, klen)",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "6fa628a1cb68800faaedc98ce6f448bf0c3bfb9f",
        "index": "7316543..9e1eb98 100644",
        "commit_message": "Fix some tensorflow_hub errors triggered when tests are run with python3 -bb.\n\nPiperOrigin-RevId: 295136697\n\n",
        "file": "hub.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def absolute_path(path):",
            "This implementation avoids calling os.path.abspath(path) if 'path' already",
            "represents an absolute Tensorflow filesystem location (e.g. <fs type>://).",
            "\"\"\"",
            "-  return path if \"://\" in str(path) else os.path.abspath(path)",
            "+  return path if b\"://\" in tf.compat.as_bytes(path) else os.path.abspath(path)",
            "",
            "",
            "def fc2_implements_resources():"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1949140)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1949141)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1949142)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'as_bytes'), position=2, insert_id=1949143)",
            "Update(target_node=ASTNode(type=identifier, text=str), value='tf')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=str), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1949144)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'compat'), position=2, insert_id=1949145)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2786,
        "neg_line": [
            "-return path if \"://\" in str(path) else os.path.abspath(path)"
        ],
        "pos_line": [
            "+return path if b\"://\" in tf.compat.as_bytes(path) else os.path.abspath(path)"
        ],
        "core_change": "-return path if \"://\" in str(path) else os.path.abspath(path) +return path if b\"://\" in tf.compat.as_bytes(path) else os.path.abspath(path)",
        "core_API": "abspath"
    },
    {
        "commit_hash": "d15a432bad5da120595136dd2734589d14be0b70",
        "index": "2b089c78..67feaccf 100644",
        "commit_message": "fix mean_iou documentation issue\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def mean_iou(",
            "# iterate over classes",
            "for class_id in range(num_classes):",
            "tp: torch.Tensor = conf_mat[..., None, class_id, class_id]",
            "-        total = torch.sum(conf_mat[..., class_id, :], dim=-1, keepdim=True) + \\",
            "+        total: torch.Tensor = \\",
            "+            torch.sum(conf_mat[..., class_id, :], dim=-1, keepdim=True) + \\",
            "torch.sum(conf_mat[..., :, class_id], dim=-1, keepdim=True)",
            "iou_val: torch.Tensor = tp / (total.float() - tp + 1e-6)",
            "ious[..., class_id:class_id + 1] += iou_val"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=(':', ':'), position=1, insert_id=473599)",
            "Insert(target_node=ASTNode(type=assignment), node=('type', None), position=2, insert_id=473600)",
            "Insert(target_node=IN(type=type), node=('attribute', None), position=0, insert_id=473601)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=473602)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=473603)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=473604)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 2787,
        "neg_line": [
            "-total = torch.sum(conf_mat[..., class_id, :], dim=-1, keepdim=True) + \\"
        ],
        "pos_line": [
            "+total: torch.Tensor = \\",
            "+torch.sum(conf_mat[..., class_id, :], dim=-1, keepdim=True) + \\"
        ],
        "core_change": "-total = torch.sum(conf_mat[..., class_id, :], dim=-1, keepdim=True) + \\ +total: torch.Tensor = \\ +torch.sum(conf_mat[..., class_id, :], dim=-1, keepdim=True) + \\",
        "core_API": "sum"
    },
    {
        "commit_hash": "1c37746892a5fd680e88264346197bb313c8dd08",
        "index": "8121f4f5a..67e1da741 100644",
        "commit_message": "fixing run_generation\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "if requires_preprocessing:",
            "prepare_input = PREPROCESSING_FUNCTIONS.get(args.model_type)",
            "prompt_text, model_kwargs = prepare_input(args, model, tokenizer, prompt_text)",
            "-    encoded_prompt = torch.tensor(tokenizer.encode(prompt_text, add_special_tokens=False)).unsqueeze(0)",
            "+    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors='pt')",
            "",
            "output_sequences = model.generate(",
            "-        intput_ids=encoded_prompt,",
            "-        length=args.length,",
            "+        input_ids=encoded_prompt,",
            "+        max_length=args.length,",
            "temperature=args.temperature,",
            "top_k=args.k,",
            "top_p=args.p,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=intput_ids), value='input_ids')",
            "Update(target_node=ASTNode(type=identifier, text=length), value='max_length')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1243537)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1243538)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'return_tensors'), position=0, insert_id=1243539)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1243540)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'pt'\"), position=2, insert_id=1243541)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=unsqueeze))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 24,
        "number": 2791,
        "neg_line": [
            "-encoded_prompt = torch.tensor(tokenizer.encode(prompt_text, add_special_tokens=False)).unsqueeze(0)",
            "-intput_ids=encoded_prompt,",
            "-length=args.length,"
        ],
        "pos_line": [
            "+encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors='pt')",
            "+input_ids=encoded_prompt,",
            "+max_length=args.length,"
        ],
        "core_change": "-encoded_prompt = torch.tensor(tokenizer.encode(prompt_text, add_special_tokens=False)).unsqueeze(0) +encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors='pt') -intput_ids=encoded_prompt, -length=args.length, +input_ids=encoded_prompt, +max_length=args.length,",
        "core_API": "get"
    },
    {
        "commit_hash": "763a42e9912d367cd3c5a506b62d48a74cd0d0e5",
        "index": "1e209c7ad1..f510210cb8 100644",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "import tensorflow as tf",
            "from typing import Union",
            "",
            "",
            "-def l2_normalize(x: Union[tf.Tensor, tf.Variable],",
            "-                 axis: int = None,",
            "-                 out=None",
            "-                 ) -> tf.Tensor:",
            "+def l2_normalize(",
            "+    x: Union[tf.Tensor, tf.Variable], axis: int = None, out=None",
            "+) -> tf.Tensor:",
            "",
            "denorm = tf.norm(x, axis=axis, keepdims=True)",
            "denorm = tf.math.maximum(denorm, 1e-12)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 2793,
        "neg_line": [
            "-def l2_normalize(x: Union[tf.Tensor, tf.Variable],",
            "-axis: int = None,",
            "-out=None",
            "-) -> tf.Tensor:"
        ],
        "pos_line": [
            "+def l2_normalize(",
            "+x: Union[tf.Tensor, tf.Variable], axis: int = None, out=None",
            "+) -> tf.Tensor:"
        ],
        "core_change": "-def l2_normalize(x: Union[tf.Tensor, tf.Variable], -axis: int = None, -out=None -) -> tf.Tensor: +def l2_normalize( +x: Union[tf.Tensor, tf.Variable], axis: int = None, out=None +) -> tf.Tensor:",
        "core_API": "norm"
    },
    {
        "commit_hash": "d08bc09495a289519a774e881f399cf864178f9b",
        "index": "b3b215b4..b73d90c5 100644",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def rgba_to_rgb(image: torch.Tensor) -> torch.Tensor:",
            "g_new: torch.Tensor = a_one * g + a * g",
            "b_new: torch.Tensor = a_one * b + a * b",
            "",
            "-    return torch.cat([r, g, b], dim=-3)",
            "+    return torch.cat([r_new, g_new, b_new], dim=-3)",
            "",
            "",
            "def rgba_to_bgr(image: torch.Tensor) -> torch.Tensor:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=identifier, text=g), node=ASTNode(type=list), position=4)",
            "Update(target_node=ASTNode(type=identifier, text=r), value='r_new')",
            "Update(target_node=ASTNode(type=identifier, text=g), value='g_new')",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=4, insert_id=419221)",
            "Update(target_node=ASTNode(type=identifier, text=b), value='b_new')",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 2794,
        "neg_line": [
            "-return torch.cat([r, g, b], dim=-3)"
        ],
        "pos_line": [
            "+return torch.cat([r_new, g_new, b_new], dim=-3)"
        ],
        "core_change": "-return torch.cat([r, g, b], dim=-3) +return torch.cat([r_new, g_new, b_new], dim=-3)",
        "core_API": "cat"
    },
    {
        "commit_hash": "91b845775a9d1955cd93e9e65c841e070777e254",
        "index": "67b8ce10cc..6f35652e2f 100644",
        "commit_message": "`random_uniform` fix (#2289)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def random_uniform(",
            "device: str,",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "+    shape = _check_bounds_and_get_shape(low, high, shape)",
            "low = tf.cast(low, dtype)",
            "high = tf.cast(high, dtype)",
            "with tf.device(device):",
            "-        return tf.random.uniform(shape if shape else (), low, high, dtype=dtype)",
            "+        return tf.random.uniform(shape, low, high, dtype=dtype)",
            "",
            "",
            "def random_normal("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('block', None), position=6, insert_id=2005431)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2005432)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2005433)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'shape'), position=0, insert_id=2005434)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2005435)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=2005436)",
            "Insert(target_node=IN(type=call), node=('identifier', '_check_bounds_and_get_shape'), position=0, insert_id=2005437)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2005438)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=shape), position=1)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2005439)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'low'), position=1, insert_id=2005440)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2005441)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'high'), position=3, insert_id=2005442)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=2005443)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'shape'), position=5, insert_id=2005444)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=6, insert_id=2005445)",
            "Delete(target_node=ASTNode(type=block, text=))",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=shape))",
            "Delete(target_node=ASTNode(type=else, text=else))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=tuple))",
            "Delete(target_node=ASTNode(type=conditional_expression))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 24,
        "number": 2795,
        "neg_line": [
            "-return tf.random.uniform(shape if shape else (), low, high, dtype=dtype)"
        ],
        "pos_line": [
            "+shape = _check_bounds_and_get_shape(low, high, shape)",
            "+return tf.random.uniform(shape, low, high, dtype=dtype)"
        ],
        "core_change": "+shape = _check_bounds_and_get_shape(low, high, shape) -return tf.random.uniform(shape if shape else (), low, high, dtype=dtype) +return tf.random.uniform(shape, low, high, dtype=dtype)",
        "core_API": "cast"
    },
    {
        "commit_hash": "5ecce832170355f068c8e6e5b655513807e2f338",
        "index": "396eaa1..c3d0148 100644",
        "commit_message": "PyTorch 1.4 compat\n\nSummary: Restore compatibility with PyTorch 1.4 and 1.5, and a few lint fixes.\n\nReviewed By: patricklabatut\n\nDifferential Revision: D30048115\n\nfbshipit-source-id: ee05efa7c625f6079fb06a3cc23be93e48df9433\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def matrix_to_quaternion(matrix: torch.Tensor) -> torch.Tensor:",
            "dim=-2,",
            ")",
            "",
            "-    # clipping is not important here; if q_abs is small, the candidate won't be picked",
            "-    quat_candidates = quat_by_rijk / (2.0 * q_abs[..., None].clip(0.1))",
            "+    # We floor here at 0.1 but the exact level is not important; if q_abs is small,",
            "+    # the candidate won't be picked.",
            "+    # pyre-ignore [16]: `torch.Tensor` has no attribute `new_tensor`.",
            "+    quat_candidates = quat_by_rijk / (2.0 * q_abs[..., None].max(q_abs.new_tensor(0.1)))",
            "",
            "# if not for numerical problems, quat_candidates[i] should be same (up to a sign),",
            "# forall i; we pick the best-conditioned one (with the largest denominator)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=918325)",
            "Update(target_node=ASTNode(type=identifier, text=clip), value='max')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=918326)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=918327)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=918328)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=918329)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'q_abs'), position=0, insert_id=918330)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=918331)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'new_tensor'), position=2, insert_id=918332)"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 2796,
        "neg_line": [
            "-# clipping is not important here; if q_abs is small, the candidate won't be picked",
            "-quat_candidates = quat_by_rijk / (2.0 * q_abs[..., None].clip(0.1))"
        ],
        "pos_line": [
            "+# We floor here at 0.1 but the exact level is not important; if q_abs is small,",
            "+# the candidate won't be picked.",
            "+# pyre-ignore [16]: `torch.Tensor` has no attribute `new_tensor`.",
            "+quat_candidates = quat_by_rijk / (2.0 * q_abs[..., None].max(q_abs.new_tensor(0.1)))"
        ],
        "core_change": "-# clipping is not important here; if q_abs is small, the candidate won't be picked -quat_candidates = quat_by_rijk / (2.0 * q_abs[..., None].clip(0.1)) +# We floor here at 0.1 but the exact level is not important; if q_abs is small, +# the candidate won't be picked. +# pyre-ignore [16]: `torch.Tensor` has no attribute `new_tensor`. +quat_candidates = quat_by_rijk / (2.0 * q_abs[..., None].max(q_abs.new_tensor(0.1)))",
        "core_API": "new_tensor"
    },
    {
        "commit_hash": "100e094cc9af523d3f71783bceda82b22b091af2",
        "index": "8ade4107..9fb7e8ea 100644",
        "commit_message": "Fix autoencoder test (#886)\n\nFix autoencoder test.\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AutoencoderKLTests(ModelTesterMixin, unittest.TestCase):",
            "model.config.in_channels,",
            "model.config.sample_size,",
            "model.config.sample_size,",
            "-            generator=generator,",
            "+            generator=torch.manual_seed(0),",
            ")",
            "image = image.to(torch_device)",
            "with torch.no_grad():"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_list), node=('call', None), position=0, insert_id=102335)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=102336)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=102337)",
            "Update(target_node=ASTNode(type=identifier, text=generator), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=generator), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=102338)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'manual_seed'), position=2, insert_id=102339)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=102340)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '0'), position=1, insert_id=102341)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=102342)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 2797,
        "neg_line": [
            "-generator=generator,"
        ],
        "pos_line": [
            "+generator=torch.manual_seed(0),"
        ],
        "core_change": "-generator=generator, +generator=torch.manual_seed(0),",
        "core_API": "manual_seed"
    },
    {
        "commit_hash": "682bfd2a8f3276e1c079e60eeebb32f96d9bcf13",
        "index": "3b19fcf27..79061ee7a 100644",
        "commit_message": "FL MNIST example with autograd tracing (#3567)\n\n* FL MNIST example with autograd tracing\n\n* docstring coverage\n\n* Address review comments\n\n* Formatting fix\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class PlanTranslatorTorchscript(AbstractPlanTranslator):",
            "translation_plan = self.plan.copy()",
            "translation_plan.forward = None",
            "",
            "-        args_shape = translation_plan.get_args_shape()",
            "-        args = PlaceHolder.create_placeholders(args_shape)",
            "+        args = translation_plan.create_dummy_args()",
            "",
            "-        # To avoid storing Plan state tensors in torchscript, they will be send as parameters",
            "+        # jit.trace clones input args and can change their type, so we have to skip types check",
            "+        # TODO see if type check can be made less strict,",
            "+        #  e.g. tensor/custom tensor/nn.Parameter could be considered same type",
            "+        translation_plan.validate_input_types = False",
            "+",
            "+        # To avoid storing Plan state tensors in torchscript, they will be sent as parameters",
            "# we trace wrapper func, which accepts state parameters as last arg",
            "# and sets them into the Plan before executing the Plan",
            "def wrap_stateful_plan(*args):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=args_shape), value='args')",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=assignment), node=('=', '='), position=1, insert_id=1451532)",
            "Insert(target_node=ASTNode(type=assignment), node=('false', 'False'), position=2, insert_id=1451533)",
            "Update(target_node=ASTNode(type=identifier, text=PlaceHolder), value='translation_plan')",
            "Update(target_node=ASTNode(type=identifier, text=create_placeholders), value='validate_input_types')",
            "Update(target_node=ASTNode(type=identifier, text=get_args_shape), value='create_dummy_args')",
            "Delete(target_node=ASTNode(type=identifier, text=args))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=args_shape))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 6,
        "minus_line": 3,
        "AST_diff_line": 14,
        "number": 2798,
        "neg_line": [
            "-args_shape = translation_plan.get_args_shape()",
            "-args = PlaceHolder.create_placeholders(args_shape)",
            "-# To avoid storing Plan state tensors in torchscript, they will be send as parameters"
        ],
        "pos_line": [
            "+args = translation_plan.create_dummy_args()",
            "+# jit.trace clones input args and can change their type, so we have to skip types check",
            "+# TODO see if type check can be made less strict,",
            "+#  e.g. tensor/custom tensor/nn.Parameter could be considered same type",
            "+translation_plan.validate_input_types = False",
            "+",
            "+# To avoid storing Plan state tensors in torchscript, they will be sent as parameters"
        ],
        "core_change": "-args_shape = translation_plan.get_args_shape() -args = PlaceHolder.create_placeholders(args_shape) +args = translation_plan.create_dummy_args() -# To avoid storing Plan state tensors in torchscript, they will be send as parameters +# jit.trace clones input args and can change their type, so we have to skip types check +# TODO see if type check can be made less strict, +#  e.g. tensor/custom tensor/nn.Parameter could be considered same type +translation_plan.validate_input_types = False + +# To avoid storing Plan state tensors in torchscript, they will be sent as parameters",
        "core_API": "copy"
    },
    {
        "commit_hash": "82ad120f955eed7f2d4da472919561f8eb0ebfec",
        "index": "cacc089..fed94ec 100644",
        "commit_message": "fix error if stop_at_token is None\n\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def sample_autoregressive(partial_sequences,",
            "partial_sequences_eos_count = 0",
            "else:",
            "initial_states = context_first_part.new_states",
            "-    partial_sequences_eos_count = mtf.reduce_sum(",
            "-        mtf.to_int32(mtf.equal(partial_sequences, stop_at_token)),",
            "-        reduced_dim=length_dim)",
            "+    if stop_at_token is not None:",
            "+        partial_sequences_eos_count = mtf.reduce_sum(",
            "+            mtf.to_int32(mtf.equal(partial_sequences, stop_at_token)),",
            "+            reduced_dim=length_dim)",
            "",
            "def cond_fn(position, ids, *unused_states):",
            "\"\"\"Should we run another loop iteration.\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=typed_default_parameter), node=('ERROR', None), position=1, insert_id=1939310)",
            "Insert(target_node=ASTNode(type=typed_default_parameter), node=(':', ':'), position=2, insert_id=1939311)",
            "Insert(target_node=ASTNode(type=typed_default_parameter), node=('type', None), position=3, insert_id=1939312)",
            "Move(target_node=ASTNode(type=typed_default_parameter), node=ASTNode(type==, text==), position=4)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type==, text==), position=0)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=integer, text=0), position=1)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=else), position=2)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=:, text=:), position=3)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=type), position=4)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type==, text==), position=5)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=attribute), position=6)",
            "Insert(target_node=IN(type=ERROR), node=('if', 'if'), position=7, insert_id=1939313)",
            "Insert(target_node=IN(type=ERROR), node=('comparison_operator', None), position=8, insert_id=1939314)",
            "Move(target_node=IN(type=type), node=ASTNode(type=identifier, text=partial_sequences_eos_count), position=0)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'stop_at_token'), position=0, insert_id=1939315)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=1939316)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=1939317)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=1939318)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=mtf), position=0)",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 4,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 2799,
        "neg_line": [
            "-partial_sequences_eos_count = mtf.reduce_sum(",
            "-mtf.to_int32(mtf.equal(partial_sequences, stop_at_token)),",
            "-reduced_dim=length_dim)"
        ],
        "pos_line": [
            "+if stop_at_token is not None:",
            "+partial_sequences_eos_count = mtf.reduce_sum(",
            "+mtf.to_int32(mtf.equal(partial_sequences, stop_at_token)),",
            "+reduced_dim=length_dim)"
        ],
        "core_change": "-partial_sequences_eos_count = mtf.reduce_sum( -mtf.to_int32(mtf.equal(partial_sequences, stop_at_token)), -reduced_dim=length_dim) +if stop_at_token is not None: +partial_sequences_eos_count = mtf.reduce_sum( +mtf.to_int32(mtf.equal(partial_sequences, stop_at_token)), +reduced_dim=length_dim)",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "02ee4e194376a9273a88b967725fdaa0d7774cb0",
        "index": "daf43c45..39e34e79 100644",
        "commit_message": "Uint8 fix (#2105)\n\n* fix (uint8 index warning): use bool for inside flags\n\n* fix (unint8 index): convert to bool in the final to avoid 1.1 constriant\n\n* fix (unmap): convert inds in unmap to be bool type\n\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def unmap(data, count, inds, fill=0):",
            "size count) \"\"\"",
            "if data.dim() == 1:",
            "ret = data.new_full((count, ), fill)",
            "-        ret[inds] = data",
            "+        ret[inds.type(torch.bool)] = data",
            "else:",
            "new_size = (count, ) + data.size()[1:]",
            "ret = data.new_full(new_size, fill)",
            "-        ret[inds, :] = data",
            "+        ret[inds.type(torch.bool), :] = data",
            "return ret"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=subscript), node=('call', None), position=2, insert_id=640957)",
            "Insert(target_node=ASTNode(type=subscript), node=('call', None), position=2, insert_id=640958)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=640959)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=640960)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=640961)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=640962)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=inds), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=640963)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type'), position=2, insert_id=640964)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=640965)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=640966)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=640967)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=inds), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=640968)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type'), position=2, insert_id=640969)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=640970)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=640971)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=640972)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=640973)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=640974)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=640975)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=640976)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=640977)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=640978)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 24,
        "number": 2802,
        "neg_line": [
            "-ret[inds] = data",
            "-ret[inds, :] = data"
        ],
        "pos_line": [
            "+ret[inds.type(torch.bool)] = data",
            "+ret[inds.type(torch.bool), :] = data"
        ],
        "core_change": "-ret[inds] = data +ret[inds.type(torch.bool)] = data -ret[inds, :] = data +ret[inds.type(torch.bool), :] = data",
        "core_API": "dim"
    },
    {
        "commit_hash": "30d7bcc186b0bff9ef1bfc5c512ba5e28b96b841",
        "index": "a5c65b9..dff08f8 100755",
        "commit_message": " Fix (#251).\n\n",
        "file": "TensorFlowTTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFEmbedding(tf.keras.layers.Embedding):",
            "super().__init__(*args, **kwargs)",
            "",
            "def call(self, inputs):",
            "-        inputs = tf.cast(tf.expand_dims(inputs, -1), tf.int32)",
            "-        outputs = tf.gather_nd(self.embeddings, inputs)",
            "+        inputs = tf.cast(inputs, tf.int32)",
            "+        outputs = tf.gather(self.embeddings, inputs)",
            "return outputs"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=inputs), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=,, text=,), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=gather_nd), value='gather')",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=expand_dims))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 13,
        "number": 2803,
        "neg_line": [
            "-inputs = tf.cast(tf.expand_dims(inputs, -1), tf.int32)",
            "-outputs = tf.gather_nd(self.embeddings, inputs)"
        ],
        "pos_line": [
            "+inputs = tf.cast(inputs, tf.int32)",
            "+outputs = tf.gather(self.embeddings, inputs)"
        ],
        "core_change": "-inputs = tf.cast(tf.expand_dims(inputs, -1), tf.int32) -outputs = tf.gather_nd(self.embeddings, inputs) +inputs = tf.cast(inputs, tf.int32) +outputs = tf.gather(self.embeddings, inputs)",
        "core_API": "cast"
    },
    {
        "commit_hash": "b1347c956af4752560b53b891d352c48c6050305",
        "index": "57b3cdbb1..a279cb3a4 100644",
        "commit_message": "[Metrics] AUROC error on multilabel + improved testing (#3350)\n\n* error on multilabel\n\n* fix tests\n\n* fix pep8\n\n* changelog\n\n* update doc test\n\n* fix doctest\n\n* fix doctest\n\n* update from suggestion\n\n* Apply suggestions from code review\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* Update test_classification.py\n\n* Update test_classification.py\n\n* retrigger test\n\n* 'pep8\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AUROC(TensorMetric):",
            "Example:",
            "",
            ">>> pred = torch.tensor([0, 1, 2, 3])",
            "-        >>> target = torch.tensor([0, 1, 2, 2])",
            "+        >>> target = torch.tensor([0, 1, 1, 0])",
            ">>> metric = AUROC()",
            ">>> metric(pred, target)",
            "-        tensor(0.3333)",
            "+        tensor(0.5000)",
            "",
            "\"\"\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=module), position=5)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=2, insert_id=565983)",
            "Insert(target_node=IN(type=ERROR), node=('>>', '>>'), position=0, insert_id=565984)",
            "Insert(target_node=IN(type=ERROR), node=('>', '>'), position=1, insert_id=565985)",
            "Update(target_node=ASTNode(type=float, text=0.3333), value='0.5000')",
            "Update(target_node=ASTNode(type=integer, text=2), value='1')",
            "Update(target_node=ASTNode(type=integer, text=2), value='0')",
            "Delete(target_node=ASTNode(type=>>, text=>>))",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 2804,
        "neg_line": [
            "->>> target = torch.tensor([0, 1, 2, 2])",
            "-tensor(0.3333)"
        ],
        "pos_line": [
            "+>>> target = torch.tensor([0, 1, 1, 0])",
            "+tensor(0.5000)"
        ],
        "core_change": "->>> target = torch.tensor([0, 1, 2, 2]) +>>> target = torch.tensor([0, 1, 1, 0]) -tensor(0.3333) +tensor(0.5000)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "9d2cee8b48b503e2f71d73e1fb580c3de34c92ce",
        "index": "8dc0ab214..2a8f05d7a 100644",
        "commit_message": "CLIPFeatureExtractor should resize images with kept aspect ratio (#11994)\n\n* Resize with kept aspect ratio\n\n* Fixed failed test\n\n* Overload center_crop and resize methods instead\n\n* resize should handle non-PIL images\n\n* update slow test\n\n* Tensor => tensor\n\nCo-authored-by: patil-suraj <surajp815@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CLIPModelIntegrationTest(unittest.TestCase):",
            "torch.Size((inputs.input_ids.shape[0], inputs.pixel_values.shape[0])),",
            ")",
            "",
            "-        expected_logits = torch.tensor([[24.5056, 18.8076]], device=torch_device)",
            "+        expected_logits = torch.tensor([[24.5701, 19.3049]], device=torch_device)",
            "",
            "self.assertTrue(torch.allclose(outputs.logits_per_image, expected_logits, atol=1e-3))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=24.5056), value='24.5701')",
            "Update(target_node=ASTNode(type=float, text=18.8076), value='19.3049')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 2805,
        "neg_line": [
            "-expected_logits = torch.tensor([[24.5056, 18.8076]], device=torch_device)"
        ],
        "pos_line": [
            "+expected_logits = torch.tensor([[24.5701, 19.3049]], device=torch_device)"
        ],
        "core_change": "-expected_logits = torch.tensor([[24.5056, 18.8076]], device=torch_device) +expected_logits = torch.tensor([[24.5701, 19.3049]], device=torch_device)",
        "core_API": "Size"
    },
    {
        "commit_hash": "c25cb509753fa023efb7110dcce171e8e5aea713",
        "index": "bd57e070..be9aab43 100755",
        "commit_message": "Update notes, small stylistic fixes.\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TRPOModel(PolicyGradientModel):",
            "# Improve update step through simple backtracking line search",
            "# N.b. some implementations skip the line search",
            "previous_theta = self.flat_variable_helper.get()",
            "-        improved, theta = line_search(self.compute_surrogate_loss, previous_theta, update_step, negative_gradient_direction / (lagrange_multiplier + util.epsilon), self.ls_max_backtracks, self.ls_accept_ratio)",
            "+        improved, theta = line_search(self.compute_surrogate_loss, previous_theta, update_step,",
            "+                                      negative_gradient_direction / (lagrange_multiplier + util.epsilon),",
            "+                                      self.ls_max_backtracks, self.ls_accept_ratio)",
            "",
            "# Use line search results, otherwise take full step",
            "# N.B. some implementations don't use the line search"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2806,
        "neg_line": [
            "-improved, theta = line_search(self.compute_surrogate_loss, previous_theta, update_step, negative_gradient_direction / (lagrange_multiplier + util.epsilon), self.ls_max_backtracks, self.ls_accept_ratio)"
        ],
        "pos_line": [
            "+improved, theta = line_search(self.compute_surrogate_loss, previous_theta, update_step,",
            "+negative_gradient_direction / (lagrange_multiplier + util.epsilon),",
            "+self.ls_max_backtracks, self.ls_accept_ratio)"
        ],
        "core_change": "-improved, theta = line_search(self.compute_surrogate_loss, previous_theta, update_step, negative_gradient_direction / (lagrange_multiplier + util.epsilon), self.ls_max_backtracks, self.ls_accept_ratio) +improved, theta = line_search(self.compute_surrogate_loss, previous_theta, update_step, +negative_gradient_direction / (lagrange_multiplier + util.epsilon), +self.ls_max_backtracks, self.ls_accept_ratio)",
        "core_API": "get"
    },
    {
        "commit_hash": "47d663f0fa97919cce60a64ffa977d9aed6cff45",
        "index": "85d16c56..0f585864 100644",
        "commit_message": "Add docstrings of core modules and methods (#3120)\n\n* Add docstrings of core modules and methods\n\n* Update docs and fix comments\n\n* Complete docstrings\n\n* Resolve comments\n\n* reformat docstrings\n\n* resolve comments\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AssignResult(util_mixins.NiceRepr):",
            "return self",
            "",
            "def add_gt_(self, gt_labels):",
            "+        \"\"\"Add ground truth as assigned results",
            "+",
            "+        Args:",
            "+            gt_labels (torch.Tensor): Labels of gt boxes",
            "+        \"\"\"",
            "self_inds = torch.arange(",
            "1, len(gt_labels) + 1, dtype=torch.long, device=gt_labels.device)",
            "self.gt_inds = torch.cat([self_inds, self.gt_inds])"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=636595)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=636596)",
            "Insert(target_node=IN(type=expression_statement), node=('string', '\"\"\"Add ground truth as assigned results\\n\\n        Args:\\n            gt_labels (torch.Tensor): Labels of gt boxes\\n        \"\"\"'), position=0, insert_id=636597)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 4,
        "minus_line": 0,
        "AST_diff_line": 4,
        "number": 2810,
        "neg_line": [],
        "pos_line": [
            "+\"\"\"Add ground truth as assigned results",
            "+",
            "+Args:",
            "+gt_labels (torch.Tensor): Labels of gt boxes",
            "+\"\"\""
        ],
        "core_change": "+\"\"\"Add ground truth as assigned results + +Args: +gt_labels (torch.Tensor): Labels of gt boxes +\"\"\"",
        "core_API": "arange"
    },
    {
        "commit_hash": "6b423ba8bde19c831b7e8c7eb1d6d67f886d71ad",
        "index": "ac130364..2f1babd2 100644",
        "commit_message": "Heterogeneous Graph Support + GraphGym (#3068)\n\n* added HGT DBLP example\n\n* typo\n\n* Merge PyG master (#52)\n\n* Adding the Facebok Page-Page dataset\n\n* type hints\n\n* documentation CI\n\n* py 3.8\n\n* fix links\n\n* fix links\n\n* fail on warning\n\n* fail on warning\n\n* fix doc\n\nCo-authored-by: benedekrozemberczki <benedek.rozemberczki@gmail.com>\n\n* revert\n\n* Fix Documentation Rendering (#51)\n\n* fix doc rendering\n\n* fix linting\n\n* retrigger checks\n\n* remove pytorch 1.7.0 legacy code (#50)\n\n* Fix `copy.deepcopy` within lazy `nn.dense.Linear` (#44)\n\n* fix deepcopy within lazy Linear\n\n* fix merge\n\n* assert exception\n\n* example to doc\n\n* resolve conflict\n\n* resolve conflict\n\n* Add Figure and Equation to `to_hetero` docstring (#60)\n\n* add tex\n\n* add svg + docstring\n\n* typo\n\n* added equation\n\n* Message Passing Hooks (#53)\n\n* add hooks\n\n* docstring\n\n* add docstring\n\n* allow modification of inputs/output\n\n* add test for modifying output\n\n* add additional asserts for modifying output test\n\n* Rename `HeteroData.get_edges` and `HeteroData.get_nodes` (#58)\n\n* rename to_edges and to_nodes\n\n* typo\n\n* `HeteroConv` (#64)\n\n* clean heteroconv\n\n* init\n\n* init\n\n* clean up\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix documentation\n\n* bipartite function\n\n* fix test CI\n\n* remove pillow version\n\n* clean up for merge\n\n* Merge PyG master (#69)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\n\n* re-merge\n\n* add lazy column to GNN cheatsheet (#70)\n\n* `to_hetero_with_bases(model)` (#63)\n\n* update\n\n* fix linting\n\n* basisconv\n\n* add ValueError\n\n* to_hetero_with_bases impl done\n\n* add test\n\n* add comments\n\n* add comments\n\n* docstring\n\n* typo\n\n* update figure\n\n* svg\n\n* typo\n\n* add test\n\n* update\n\n* add rgcn equality test\n\n* typos\n\n* update\n\n* typos\n\n* update figures\n\n* generate new svgs\n\n* fix assignment\n\n* rename\n\n* delete sorted edge types\n\n* rename\n\n* add legend\n\n* fix typo\n\n* Test: Check equal outputs of `to_hetero` and `RGCNConv` (#59)\n\n* check equal output\n\n* add sparsetensor test\n\n* check equal output\n\n* add sparsetensor test\n\n* rename\n\n* linting\n\n* add missing import\n\n* `HeteroData` support for `T.NormalizeFeatures` (#56)\n\n* normalize features\n\n* allow normalization of any feature\n\n* in-place div\n\n* normalize features\n\n* allow normalization of any feature\n\n* in-place div\n\n* fix test\n\n* no need to re-assign\n\n* `HeteroData` support for `T.AddSelfLoops` (#54)\n\n* hetero support for AddSelfLoops\n\n* check for edge_index attribute\n\n* f-string\n\n* retrigger checks\n\n* revert bipartite changes\n\n* hetero support for AddSelfLoops\n\n* check for edge_index attribute\n\n* f-string\n\n* retrigger checks\n\n* revert bipartite changes\n\n* merge master\n\n* merge master\n\n* `HeteroData` support for `T.ToSparseTensor` (#55)\n\n* hetero support for ToSparseTensor\n\n* add test\n\n* customize the attribute of SparseTensor.value\n\n* rework sort_edge_index\n\n* hetero support for ToSparseTensor\n\n* add test\n\n* customize the attribute of SparseTensor.value\n\n* rework sort_edge_index\n\n* linting\n\n* `HeteroData` support for `T.ToUndirected` (#57)\n\n* to_undirected\n\n* revert bipartite changes\n\n* coalesce + undirected enhancement\n\n* merge master\n\n* revert bipartite changes\n\n* coalesce + undirected enhancement\n\n* merge master\n\n* clean up\n\n* new default relation type\n\n* fix tests\n\n* resolve merge conflicts\n\n* resolve merge conflicts 2\n\n* resolve merge conflicts 3\n\n* Merge PyG master (#74)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\n* clean up doc for to_dense_batch\n\n* clean up\n\n* add legend to cheatsheet\n\n* Improve terminology (#2837)\n\nI think the previous version of the document uses the term 'symmetric' incorrectly. A symmetric matrix is a square matrix that is is equal to its transpose (https://en.wikipedia.org/wiki/Symmetric_matrix). However, the text is only talking about the shape of the matrix, not its content. Hence, 'square (matrix)' would be the correct term to use.\n\n* Add batch_size input to to_dense_batch (#2838)\n\n* Add batch_size input to to_dense_batch\n\n* to_dense_batch fix typo in batch_size param use\n\n* add typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Added return_attention_weights to TransformerConv. (#2807)\n\n* added return_weights functionality to tranformer\n\n* added return attn weights tests\n\n* flake8\n\n* added typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* MD17 (#2843)\n\n* Added MD17 dataset\n\n* Updated Documentation\n\n* Added link to sGDML website in doc\n\n* fixed typos in doc and made train variable description clearer\n\n* clean up\n\n* fix linting\n\n* fix doc warning\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* update doc\n\n* remove forward doc\n\n* add static graph support info to cheatsheet\n\n* fix num_nodes in case edge_index is empty\n\n* fix math formula\n\n* faster GDC import\n\n* lazy import\n\n* lazy import for datasets\n\n* lazy import for nn\n\n* Sequential jittable + traceable\n\n* typo\n\n* typo\n\n* update doc\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Schfer <moritz.schaefer@protonmail.com>\n\n* re-add\n\n* GraphGym cleaned version (#82)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Update .gitignore\n\n* Integrate GraphGym into PyG (#85)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Integrate graphgym into pyg, keep user API in project root\n\n* fix merge conflict\n\n* fix lint errors\n\n* Make optional dependencies\n\n* merge LICENSE from GraphGym\n\n* add import\n\n* clean up LICENSE\n\n* fix import\n\n* resolve merge conflicts\n\n* resolve merge conflicts 2\n\n* Merge PyG master (#87)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\n* clean up doc for to_dense_batch\n\n* clean up\n\n* add legend to cheatsheet\n\n* Improve terminology (#2837)\n\nI think the previous version of the document uses the term 'symmetric' incorrectly. A symmetric matrix is a square matrix that is is equal to its transpose (https://en.wikipedia.org/wiki/Symmetric_matrix). However, the text is only talking about the shape of the matrix, not its content. Hence, 'square (matrix)' would be the correct term to use.\n\n* Add batch_size input to to_dense_batch (#2838)\n\n* Add batch_size input to to_dense_batch\n\n* to_dense_batch fix typo in batch_size param use\n\n* add typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Added return_attention_weights to TransformerConv. (#2807)\n\n* added return_weights functionality to tranformer\n\n* added return attn weights tests\n\n* flake8\n\n* added typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* MD17 (#2843)\n\n* Added MD17 dataset\n\n* Updated Documentation\n\n* Added link to sGDML website in doc\n\n* fixed typos in doc and made train variable description clearer\n\n* clean up\n\n* fix linting\n\n* fix doc warning\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* update doc\n\n* remove forward doc\n\n* add static graph support info to cheatsheet\n\n* fix num_nodes in case edge_index is empty\n\n* fix math formula\n\n* faster GDC import\n\n* lazy import\n\n* lazy import for datasets\n\n* lazy import for nn\n\n* Sequential jittable + traceable\n\n* typo\n\n* typo\n\n* update doc\n\n* Simple models (#2869)\n\n* Inclusion of new backbone models\n\n* Eliminating head from asap.py\n\n* small correction\n\n* Create test_gcn.py\n\n* Update __init__.py\n\n* Update test_gcn.py\n\n* Left only the convolutional simple models\n\n* Tests included\n\n* update\n\n* clean up\n\n* clean up v2\n\n* fix activation\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* Example for MemPooling. (#2729)\n\n* example for mem pooling\n\n* backprop on kl loss is done at the end of an epoch. Keys in memory layers are trained only on kl loss.\n\n* added learning rate decay. Using PROTIENS_full\n\n* flake8\n\n* reduced lr. increased weight decay\n\n* changed download location\n\n* added comments\n\n* clean up\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typos\n\n* fix removeisolatednodes transform in case 'data.num_nodes' is present\n\n* fix XConv with dilation > 1\n\n* fix XConv with dilation > 1\n\n* rgcn link prediction  (#2734)\n\n* implemented LinkPrediction dataset for loading FB15k237\n\n* implemented evaluation for relational link prediction\n\n* implemented R-GCNConf link prediction example\n\n* fixed bug: wrong initial objects in negative_sampling\n\n* changed file downloader urllib.request.urlretrieve  to pytorch.data.download_url; renamed LinkPrediction class to RelationalLinkPredictionDataset\n\n* update dataset\n\n* update example script\n\n* rename\n\nCo-authored-by: Moritz <moritzblum>\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix gnnexplainer draw kwargs\n\n* remove python-louvain dependency\n\n* allow customization of output in MP jit mode\n\n* fix test for py3.6\n\n* changed normalisation to same norm from instance norm to be robust to small var (#2917)\n\n* add CITATION.cff\n\n* format\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* add basetransform ABC (#2924)\n\n* clean up BaseTransform\n\n* clean up GATConv and add comments\n\n* add max_num_neighbors as an additional argument\n\n* fix jit GATConv on PyTorch 1.8.0\n\n* fix doc\n\n* fix gnn explainer with existing self-loops\n\n* Rgcn link pred fix (#2946)\n\n* added regularization, removed typo in test\n\n* clean up\n\nCo-authored-by: Moritz <moritzblum>\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Correct gini coefficient mathcal formula (#2932)\n\n* typo\n\n* typo\n\n* Update from_networkx (#2923)\n\n* Update from_networkx\n\n* Update test\n\n* Update convert.py\n\n* Minor corrections\n\n* Update test_convert.py\n\n* Corrections\n\n* Update test_convert.py\n\n* Case where there are no edges\n\n* Correcting how edge_attr are concatenated\n\n* clean up + new test\n\n* remove unused code\n\n* add union type\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix deterministic ordering in from_networkx\n\n* recursive-include *.jinja files\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Schfer <moritz.schaefer@protonmail.com>\nCo-authored-by: PabloAMC <pmorenocf@alumnos.unex.es>\nCo-authored-by: Moritz Blum <31183934+moritzblum@users.noreply.github.com>\nCo-authored-by: fbragman <fbragman@users.noreply.github.com>\nCo-authored-by: Christopher Lee <2824685+CCInc@users.noreply.github.com>\nCo-authored-by: Tim Daubenschtz <tim@daubenschuetz.de>\n\n* resolve merge conflicts 3\n\n* resolve merge conflicts 4\n\n* Implementation of the `HGTLoader` + `ogbn-mag` example (#73)\n\n* first try\n\n* update\n\n* HGT Loader\n\n* typo\n\n* first try\n\n* update\n\n* HGT Loader\n\n* typo\n\n* bugfixes\n\n* lazy GATConv\n\n* bugfix\n\n* bugfix\n\n* full working pipeline\n\n* update\n\n* rename\n\n* docstring\n\n* typos\n\n* update\n\n* typo\n\n* typo\n\n* typo\n\n* added comments\n\n* add test\n\n* add tests\n\n* fix example\n\n* rename\n\n* linting\n\n* Random split functionalities (#72)\n\n* link split\n\n* create split\n\n* example tests\n\n* link split tests\n\n* fix linting\n\n* update docstring\n\n* undirected option, refactor and docs\n\n* add num nodes as argument to neg sampling\n\n* clean up + remove single object\n\n* update example\n\n* typo\n\n* fix compose\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* add basetransform\n\n* typo\n\n* typo\n\n* fix test\n\n* Improve `torch_geometric.data` Documentation (#98)\n\n* update data doc\n\n* typo\n\n* typo\n\n* note\n\n* typo\n\n* add docstring\n\n* only show inherited members for data and hetero_data\n\n* documentation update for batch and dataset\n\n* update doc\n\n* update\n\n* fix\n\n* record_stream\n\n* update\n\n* typo\n\n* add/fix data functionality\n\n* linting\n\n* typo\n\n* `_parent` memory leak fix (#103)\n\n* memory leak fix\n\n* Clean up\n\n* clean up\n\n* bugfix tests\n\n* typos\n\n* fix test\n\n* fix test\n\n* rename reverse\n\n* (Heterogeneous) `NeighborLoader` (#92)\n\n* initial commit\n\n* typo\n\n* neighbor loader functionality + tests\n\n* docstring\n\n* fix docstring\n\n* skip tests\n\n* fix share_memory_\n\n* typo\n\n* typo\n\n* update example\n\n* typo\n\n* share_strategy\n\n* fix cuda calls\n\n* better print\n\n* fix size\n\n* fix print\n\n* final commit\n\n* fix\n\n* some todos\n\n* preprocessed features\n\n* fix to_undirected\n\n* more documentation\n\n* update doc\n\n* fix doc\n\n* fix doc\n\n* Add benchmark code and the example with existing graph classification examples (#93)\n\n* add benchmarking utilities\n\n* update graph classification benchmark\n\n* improve code style\n\n* add pytorch-memlab for benchmark code\n\n* skip some tests when cuda is not available\n\n* add type hint when appropriate\n\n* add seed_everything to improve code\n\n* code refactoring\n\n* code refactoring\n\n* code refactoring\n\n* code improvement\n\n* remove unnecessary dataloader import\n\n* change benchmark interface with decorator\n\n* documentation improvement\n\n* linting\n\n* linting part 2\n\n* linting part 3\n\n* seed_everything\n\n* create utils file\n\n* update\n\n* use utils functions\n\n* fix test\n\n* update the profiler to the latest torch (1.8.1+)\n\n* refactor profiler and add more documentation\n\n* refactor profiler and add more documentation\n\n* resolve lint errors\n\n* resolve lint errors\n\n* update\n\n* clean up test and profile\n\n* fix linting\n\n* add to doc\n\n* fix doc\n\n* typo\n\n* update benchmark\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* Move `HGTLoader` to `torch_geometric.loader` + clean up (#104)\n\n* move files\n\n* use utils functions\n\n* fix example\n\n* update\n\n* fix tests\n\n* fix seed\n\n* fix linear test\n\n* rename\n\n* Support GraphGym custom modules outside PyG package (#102)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Integrate graphgym into pyg, keep user API in project root\n\n* fix merge conflict\n\n* fix lint errors\n\n* Make optional dependencies\n\n* merge LICENSE from GraphGym\n\n* Enable adding GraphGym customized modules outside PyG package\n\n* lint\n\n* Rename `AddTrainValTestMask` to `RandomNodeSplit` (#108)\n\n* initial commit\n\n* rename example\n\n* remove AddTrainValTestMask\n\n* fix linting\n\n* create optimizer config and scheduler config separately (#113)\n\n* create optimizer config and scheduler config separately\n\n* fix format\n\n* import explicitly\n\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\n\n* Heterogeneous Graph Tutorial (#83)\n\n* add HG tutorial roadmap\n\n* started working on hg tutorial\n\n* hg_tutorial, some text and .tex figure\n\n* added svg\n\n* hg tutorial content\n\n* fix CI\n\n* text and structure\n\n* finished first draft\n\n* fixed one code example\n\n* fixing conventions\n\n* fixing links\n\n* update svg\n\n* some smaller improvements of tutorial\n\n* improvements on tutorial\n\n* hg-tutorial: fixed compiling issue, added detailed content\n\n* added absolute links\n\n* fixed warnings\n\n* streamlined dataset section\n\n* update svg\n\n* update tutorial\n\n* update 2\n\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\n\n* typo\n\n* Move data loaders to `torch_geometric.loader` (#110)\n\n* move graphsaint\n\n* deprecations\n\n* move clusterloader\n\n* deprecations\n\n* type hints\n\n* move shadow\n\n* typo\n\n* typo\n\n* move datalistloader\n\n* dense data loader\n\n* random node sampler\n\n* fix doc\n\n* Lazy GNN operators (#89)\n\n* lazy cheb conv\n\n* lazy GraphConv\n\n* lazy GATv2Conv\n\n* lazy TAGConv\n\n* lazy FAConv\n\n* lazy FeaStConv\n\n* lazy NNConv\n\n* typo\n\n* fix tests\n\n* lazy SuperGATConv\n\n* lazy SuperGATConv fix\n\n* lazy SplineConv\n\n* fix lazy check\n\n* lazy GravNetConv\n\n* arma conv lazy\n\n* dense linear in gmmconv\n\n* typo\n\n* add test\n\n* lazy GMMConv\n\n* doc\n\n* rename (#116)\n\n* Revisit `MetaPath2Vec` (#114)\n\n* revisit metapath2vec\n\n* update\n\n* typo\n\n* update\n\n* fix doc\n\n* update\n\n* check for attributes rather than key\n\n* Clean up `torch_geometric.profile` further (#111)\n\n* remove print_layer_stats\n\n* typos\n\n* update\n\n* readme highlights and quick tour (#99)\n\n* readme highlights and quick tour\n\n* arch\n\n* arch image\n\n* arch overview\n\n* list categories\n\n* categorization\n\n* category description\n\n* Update README.md\n\nfrom Matthias\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* improved highlights\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* minor\n\n* update readme\n\n* update\n\n* update\n\n* update\n\n* update\n\n* fix url\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* move ops\n\n* toc\n\n* typo\n\n* typo\n\n* add svgs\n\n* update figure\n\n* fix links\n\n* fix size\n\n* fix size\n\n* typo\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* fix broken links\n\n* fix links\n\n* Heterogeneous Graph Sampler Tutorial (#117)\n\n* initial commit\n\n* address comments\n\n* remove todo\n\n* typo\n\n* Conversion between heterogenous and homogeneous graph objects (#115)\n\n* temp checkpoint (wip, will remove)\n\n* (wip) typed graph conversion\n\n* (wip) typed graph conversion\n\n* (wip) typed graph conversion\n\n* update\n\n* typo\n\n* delete examples\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix test\n\n* update doc\n\n* deprecate NeighborSampler (#119)\n\n* Move `torch_geometric.data.DataLoader` to `torch_geometric.loader.DataLoader` (#120)\n\n* move dataloader\n\n* rename\n\n* typos\n\n* typos\n\n* fix __cat_dim__\n\n* updategp\n\n* Deprecate `train_test_split_edges` + Modifications to `RandomLinkSplit` (#121)\n\n* deprecate train_test_split_edges\n\n* to device transform\n\n* fix example\n\n* add split_labels argument\n\n* fix autoencoder example\n\n* typos\n\n* add docstring\n\n* ARGVA\n\n* seal\n\n* adress comments\n\n* Create example to load `*.csv` and transfer to `HeteroData` (#76)\n\n* create example to load csv file and transfer to heter-data\n\n* add ipython notebook version load csv with documentation\n\n* address comment\n\n* first version of csv loading doc\n\n* first version of csv loading doc\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestions csv tutorial\n\n* example script load csv + extract fix\n\n* fixed edge index stacking dimension in example and jupyter nb\n\n* linting\n\n* linting2\n\n* rename\n\n* update\n\n* update\n\n* update\n\n* typo\n\n* typo\n\n* update\n\n* rename\n\n* update tutorial\n\n* typo\n\n* address comments\n\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* fix\n\n* typo\n\n* update\n\n* fix\n\n* fix\n\nCo-authored-by: benedekrozemberczki <benedek.rozemberczki@gmail.com>\nCo-authored-by: Rex Ying <rexying@stanford.edu>\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Schfer <moritz.schaefer@protonmail.com>\nCo-authored-by: Jiaxuan <youjiaxuan@gmail.com>\nCo-authored-by: PabloAMC <pmorenocf@alumnos.unex.es>\nCo-authored-by: Moritz Blum <31183934+moritzblum@users.noreply.github.com>\nCo-authored-by: fbragman <fbragman@users.noreply.github.com>\nCo-authored-by: Christopher Lee <2824685+CCInc@users.noreply.github.com>\nCo-authored-by: Tim Daubenschtz <tim@daubenschuetz.de>\nCo-authored-by: Yue Zhao <yzhao062@gmail.com>\nCo-authored-by: Dong Wang <dongw89@gmail.com>\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ShaDowKHopSampler(torch.utils.data.DataLoader):",
            "batch.edge_index = torch.stack([row, col], dim=0)",
            "",
            "for k, v in self.data:",
            "-            if k in ['edge_index', 'adj_t']:",
            "+            if k in ['edge_index', 'adj_t', 'num_nodes']:",
            "continue",
            "if k == 'y' and v.size(0) == self.data.num_nodes:",
            "batch[k] = v[n_id][root_n_id]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=4, insert_id=1001492)",
            "Insert(target_node=ASTNode(type=list), node=('string', \"'num_nodes'\"), position=5, insert_id=1001493)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 2812,
        "neg_line": [
            "-if k in ['edge_index', 'adj_t']:"
        ],
        "pos_line": [
            "+if k in ['edge_index', 'adj_t', 'num_nodes']:"
        ],
        "core_change": "-if k in ['edge_index', 'adj_t']: +if k in ['edge_index', 'adj_t', 'num_nodes']:",
        "core_API": "stack"
    },
    {
        "commit_hash": "24bece4d27f15195d24aafcc8db37e412ec0d44d",
        "index": "838ec528..9a8f6531 100644",
        "commit_message": "small fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Model(object):",
            "self.deterministic_mode = config.get('deterministic_mode', False)",
            "self.episode_length = tf.placeholder(tf.int32, (None,), name='episode_length')",
            "",
            "-        self.alpha = config.get('alpha', 0.001)",
            "+        self.learning_rate = config.get('learning_rate', 0.001)",
            "",
            "optimizer = config.get('optimizer')",
            "if not optimizer:",
            "-            self.optimizer = tf.train.AdamOptimizer(self.alpha)",
            "+            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)",
            "else:",
            "args = config.get('optimizer_args', [])",
            "kwargs = config.get('optimizer_kwargs', {})",
            "optimizer_cls = get_function(optimizer)",
            "-            self.optimizer = optimizer_cls(self.alpha, *args, **kwargs)",
            "+            self.optimizer = optimizer_cls(self.learning_rate, *args, **kwargs)",
            "",
            "exploration = config.get('exploration')",
            "if not exploration:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=alpha), value='learning_rate')",
            "Update(target_node=ASTNode(type=string, text='alpha'), value=\"'learning_rate'\")",
            "Insert(target_node=ASTNode(type=assignment), node=('attribute', None), position=0, insert_id=2245848)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2245849)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2245850)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'optimizer'), position=2, insert_id=2245851)",
            "Update(target_node=ASTNode(type=identifier, text=alpha), value='learning_rate')",
            "Update(target_node=ASTNode(type=identifier, text=alpha), value='learning_rate')",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=optimizer))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 13,
        "number": 2819,
        "neg_line": [
            "-self.alpha = config.get('alpha', 0.001)",
            "-self.optimizer = tf.train.AdamOptimizer(self.alpha)",
            "-self.optimizer = optimizer_cls(self.alpha, *args, **kwargs)"
        ],
        "pos_line": [
            "+self.learning_rate = config.get('learning_rate', 0.001)",
            "+self.optimizer = tf.train.AdamOptimizer(self.learning_rate)",
            "+self.optimizer = optimizer_cls(self.learning_rate, *args, **kwargs)"
        ],
        "core_change": "-self.alpha = config.get('alpha', 0.001) +self.learning_rate = config.get('learning_rate', 0.001) -self.optimizer = tf.train.AdamOptimizer(self.alpha) +self.optimizer = tf.train.AdamOptimizer(self.learning_rate) -self.optimizer = optimizer_cls(self.alpha, *args, **kwargs) +self.optimizer = optimizer_cls(self.learning_rate, *args, **kwargs)",
        "core_API": "get"
    },
    {
        "commit_hash": "a9ed3f40f87e769e686dffb4571ec57b5aa6aa6a",
        "index": "020d70c..70e0be8 100644",
        "commit_message": "fix tf-nightly broken (#1741)\n\nCo-authored-by: Haifeng Jin <haifeng-jin@users.noreply.github.com>\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class BertEncoder(layers.Layer):",
            "super().__init__(**kwargs)",
            "embedding_width = 768",
            "dropout_rate = 0.1",
            "-        initializer = keras.initializers.TruncatedNormal(stddev=0.02)",
            "+        initializer = keras.initializers.TruncatedNormal(stddev=0.02, seed=42)",
            "",
            "self._embedding_layer = OnDeviceEmbedding(",
            "vocab_size=30522,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2465030)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2465031)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'seed'), position=0, insert_id=2465032)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2465033)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '42'), position=2, insert_id=2465034)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2820,
        "neg_line": [
            "-initializer = keras.initializers.TruncatedNormal(stddev=0.02)"
        ],
        "pos_line": [
            "+initializer = keras.initializers.TruncatedNormal(stddev=0.02, seed=42)"
        ],
        "core_change": "-initializer = keras.initializers.TruncatedNormal(stddev=0.02) +initializer = keras.initializers.TruncatedNormal(stddev=0.02, seed=42)",
        "core_API": "TruncatedNormal"
    },
    {
        "commit_hash": "8880f696b6b8368a76296126476ea020fc7c814c",
        "index": "793d2ed..4aed09d 100644",
        "commit_message": "Refactoring, cleanup, improved test coverage.\n* Add eca_nfnet_l2 weights, 84.7 @ 384x384\n* All 'non-std' (ie transformer / mlp) models have classifier / default_cfg test added\n* Fix #694 reset_classifer / num_features / forward_features / num_classes=0 consistency for transformer / mlp models\n* Add direct loading of npz to vision transformer (pure transformer so far, hybrid to come)\n* Rename vit_deit* to deit_*\n* Remove some deprecated vit hybrid model defs\n* Clean up classifier flatten for conv classifiers and unusual cases (mobilenetv3/ghostnet)\n* Remove explicit model fns for levit conv, just pass in arg\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Twins(nn.Module):",
            "",
            "def reset_classifier(self, num_classes, global_pool=''):",
            "self.num_classes = num_classes",
            "-        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()",
            "+        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()",
            "",
            "def _init_weights(self, m):",
            "if isinstance(m, nn.Linear):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=embed_dim), value='num_features')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2822,
        "neg_line": [
            "-self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()"
        ],
        "pos_line": [
            "+self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()"
        ],
        "core_change": "-self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity() +self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()",
        "core_API": "Linear"
    },
    {
        "commit_hash": "4ad5a78dce48317d206419a9cd19dda6293c294b",
        "index": "7f295da59..9c319add4 100644",
        "commit_message": "to_torchscript method for LightningModule (#3258)\n\n* script\n\n* docs\n\n* simple test\n\n* move test\n\n* fix doctest\n\n* no grad context\n\n* extend tests\n\n\ntest\n\n\ntest\n\n* datamodule test\n\n* clean up test\n\n* docs\n\n* name\n\n* fix import\n\n* update changelog\n\n* fix import\n\n* skip pytorch 1.3 in test\n\n* update codeblock\n\n* skip bugged 1.4\n\n* typehints\n\n* doctest not working on all pytorch versions\n\n* rename TestGAN to prevent pytest interference\n\n* add note about pytorch version\n\n* fix torchscript version inconsistency in tests\n\n* reset training state + tests\n\n* update docstring\n\n* Apply suggestions from code review\n\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\n\n* update docstring, dict return\n\n* add docs to index\n\n* add link\n\n* doc eval mode\n\n* forward\n\n* optional save to file path\n\n* optional\n\n* test torchscript device\n\n* test save load with file path\n\n* pep\n\n* str\n\n* Commit typing suggestion\n\nCo-authored-by: ananthsub <ananth.subramaniam@gmail.com>\n\n* skip test if cuda not available\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\nCo-authored-by: ananthsub <ananth.subramaniam@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ParityModuleMNIST(LightningModule):",
            "self.c_d1_bn = nn.BatchNorm1d(128)",
            "self.c_d1_drop = nn.Dropout(0.3)",
            "self.c_d2 = nn.Linear(in_features=128, out_features=10)",
            "+        self.example_input_array = torch.rand(2, 1, 28, 28)",
            "",
            "def forward(self, x):",
            "x = x.view(x.size(0), -1)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=569496)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=569497)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=569498)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=569499)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=569500)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=569501)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=569502)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'example_input_array'), position=2, insert_id=569503)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=569504)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=569505)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=569506)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=569507)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'rand'), position=2, insert_id=569508)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=569509)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '2'), position=1, insert_id=569510)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=569511)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=3, insert_id=569512)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=569513)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '28'), position=5, insert_id=569514)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=6, insert_id=569515)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '28'), position=7, insert_id=569516)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=8, insert_id=569517)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 22,
        "number": 2824,
        "neg_line": [],
        "pos_line": [
            "+self.example_input_array = torch.rand(2, 1, 28, 28)"
        ],
        "core_change": "+self.example_input_array = torch.rand(2, 1, 28, 28)",
        "core_API": "BatchNorm1d"
    },
    {
        "commit_hash": "fe290280be18a689a53bdf56419a76987ed6aeed",
        "index": "bbba5a421..49a79f942 100644",
        "commit_message": "Metric aggregation testing (#3517)\n\n* aggregation testing\n\n* add more tests\n\n* mse\n\n* more tests\n\n* fix tests\n\n* fix doctest\n\n* fix codefactor\n\n* fix import error\n\n* fix doctest\n\n* revert docfix\n\n* test for model integration\n\n* fix integration test\n\n* added test cases\n\n* fix rmsle\n\n* aggregation testing\n\n* add more tests\n\n* mse\n\n* more tests\n\n* fix tests\n\n* fix doctest\n\n* fix codefactor\n\n* fix import error\n\n* fix doctest\n\n* revert docfix\n\n* test for model integration\n\n* fix integration test\n\n* fix psnr\n\n* add warning/valueerror to embedding similarity\n\n* fixed f scores\n\n* disable some test\n\n* fix tests\n\n* fixing codefactor\n\n* fix pep8\n\n* changelog\n\n* fix doctest\n\n* cleaning test\n\n* fix pickle error\n\n* pickle fix\n\n* fix pickle error\n\n* Apply suggestions from code review\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* code cleanup + changes based on suggestions\n\n* update based on suggestion\n\n* update based on suggestions\n\n* Apply suggestions from code review\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\nCo-authored-by: Nicki Skafte <nugginea@gmail.com>\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_rmsle(pred, target, expected):",
            "])",
            "def test_psnr_with_skimage(pred, target):",
            "score = psnr(pred=torch.tensor(pred),",
            "-                 target=torch.tensor(target))",
            "+                 target=torch.tensor(target), data_range=3)",
            "sk_score = ski_psnr(np.array(pred), np.array(target), data_range=3)",
            "assert torch.allclose(score, torch.tensor(sk_score, dtype=torch.float), atol=1e-3)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=564611)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=564612)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'data_range'), position=0, insert_id=564613)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=564614)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '3'), position=2, insert_id=564615)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2825,
        "neg_line": [
            "-target=torch.tensor(target))"
        ],
        "pos_line": [
            "+target=torch.tensor(target), data_range=3)"
        ],
        "core_change": "-target=torch.tensor(target)) +target=torch.tensor(target), data_range=3)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "ddebbdc53544c64a1957cf0ec04a34578e9b57ba",
        "index": "15b8f6b0..fb6af0ef 100644",
        "commit_message": "Make most metrics work on GPU (#3851)\n\n* Make most metrics work on GPU\n\n* Make metric tests work on both GPU and CPU\n\n* Add a test for the test utility\n\n* mypy\n\n* Update allennlp/common/testing/test_case.py\n\nCo-Authored-By: Mark Neumann <markn@allenai.org>\n\n* Fix a PR comment\n\n* flake8\n\nCo-authored-by: Mark Neumann <markn@allenai.org>\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Metric(Registrable):",
            "raise NotImplementedError",
            "",
            "@staticmethod",
            "-    def unwrap_to_tensors(*tensors: torch.Tensor):",
            "+    def detach_tensors(*tensors: torch.Tensor) -> Iterable[torch.Tensor]:",
            "\"\"\"",
            "If you actually passed gradient-tracking Tensors to a Metric, there will be",
            "a huge memory leak, because it will prevent garbage collection for the computation",
            "-        graph. This method ensures that you're using tensors directly and that they are on",
            "-        the CPU.",
            "+        graph. This method ensures the tensors are detached.",
            "\"\"\"",
            "-        return (x.detach().cpu() if isinstance(x, torch.Tensor) else x for x in tensors)",
            "+        # Check if it's actually a tensor in case something else was passed.",
            "+        return (x.detach() if isinstance(x, torch.Tensor) else x for x in tensors)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"\nIf you actually passed gradient-tracking Tensors to a Metric, there will be\na huge memory leak, because it will prevent garbage collection for the computation\n        graph. This method ensures that you're using tensors directly and that they are on\n        the CPU.\n\"\"\"), value='\"\"\"\\nIf you actually passed gradient-tracking Tensors to a Metric, there will be\\na huge memory leak, because it will prevent garbage collection for the computation\\n        graph. This method ensures the tensors are detached.\\n\"\"\"')",
            "Update(target_node=ASTNode(type=identifier, text=unwrap_to_tensors), value='detach_tensors')",
            "Insert(target_node=ASTNode(type=function_definition), node=('->', '->'), position=3, insert_id=20653)",
            "Insert(target_node=ASTNode(type=function_definition), node=('type', None), position=4, insert_id=20654)",
            "Insert(target_node=IN(type=type), node=('subscript', None), position=0, insert_id=20655)",
            "Move(target_node=ASTNode(type=conditional_expression), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'Iterable'), position=0, insert_id=20656)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=20657)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=2, insert_id=20658)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=20659)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=20660)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=20661)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=20662)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cpu))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 20,
        "number": 2827,
        "neg_line": [
            "-def unwrap_to_tensors(*tensors: torch.Tensor):",
            "-graph. This method ensures that you're using tensors directly and that they are on",
            "-the CPU.",
            "-return (x.detach().cpu() if isinstance(x, torch.Tensor) else x for x in tensors)"
        ],
        "pos_line": [
            "+def detach_tensors(*tensors: torch.Tensor) -> Iterable[torch.Tensor]:",
            "+graph. This method ensures the tensors are detached.",
            "+# Check if it's actually a tensor in case something else was passed.",
            "+return (x.detach() if isinstance(x, torch.Tensor) else x for x in tensors)"
        ],
        "core_change": "-def unwrap_to_tensors(*tensors: torch.Tensor): +def detach_tensors(*tensors: torch.Tensor) -> Iterable[torch.Tensor]: -graph. This method ensures that you're using tensors directly and that they are on -the CPU. +graph. This method ensures the tensors are detached. -return (x.detach().cpu() if isinstance(x, torch.Tensor) else x for x in tensors) +# Check if it's actually a tensor in case something else was passed. +return (x.detach() if isinstance(x, torch.Tensor) else x for x in tensors)",
        "core_API": "detach"
    },
    {
        "commit_hash": "2fe7d030ddd22d4780647ea151f26b0e4621ec78",
        "index": "be6d031..0a4ace1 100644",
        "commit_message": "Fix #20: should not apply softmax into nn.cross_entropy\n\n",
        "file": "generative-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "D_gan = torch.nn.Sequential(",
            "",
            "D_aux = torch.nn.Sequential(",
            "torch.nn.Linear(h_dim, y_dim),",
            "-    torch.nn.Softmax()",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=3)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Softmax))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 2828,
        "neg_line": [
            "-torch.nn.Softmax()"
        ],
        "pos_line": [],
        "core_change": "-torch.nn.Softmax()",
        "core_API": "Sequential"
    },
    {
        "commit_hash": "9ebaea545ffddf2e9079994f2ea657a7fa5f358c",
        "index": "5e3ee091..3ea8829b 100644",
        "commit_message": "Optimize Stable Diffusion (#371)\n\n* initial commit\n\n* make UNet stream capturable\n\n* try to fix noise_pred value\n\n* remove cuda graph and keep NB\n\n* non blocking unet with PNDMScheduler\n\n* make timesteps np arrays for pndm scheduler\nbecause lists don't get formatted to tensors in `self.set_format`\n\n* make max async in pndm\n\n* use channel last format in unet\n\n* avoid moving timesteps device in each unet call\n\n* avoid memcpy op in `get_timestep_embedding`\n\n* add `channels_last` kwarg to `DiffusionPipeline.from_pretrained`\n\n* update TODO\n\n* replace `channels_last` kwarg with `memory_format` for more generality\n\n* revert the channels_last changes to leave it for another PR\n\n* remove non_blocking when moving input ids to device\n\n* remove blocking from all .to() operations at beginning of pipeline\n\n* fix merging\n\n* fix merging\n\n* model can run in other precisions without autocast\n\n* attn refactoring\n\n* Revert \"attn refactoring\"\n\nThis reverts commit 0c70c0e189cd2c4d8768274c9fcf5b940ee310fb.\n\n* remove restriction to run conv_norm in fp32\n\n* use `baddbmm` instead of `matmul`for better in attention for better perf\n\n* removing all reshapes to test perf\n\n* Revert \"removing all reshapes to test perf\"\n\nThis reverts commit 006ccb8a8c6bc7eb7e512392e692a29d9b1553cd.\n\n* add shapes comments\n\n* hardcore whats needed for jitting\n\n* Revert \"hardcore whats needed for jitting\"\n\nThis reverts commit 2fa9c698eae2890ac5f8e367ca80532ecf94df9a.\n\n* Revert \"remove restriction to run conv_norm in fp32\"\n\nThis reverts commit cec592890c32da3d1b78d38b49e4307aedf459b9.\n\n* revert using baddmm in attention's forward\n\n* cleanup comment\n\n* remove restriction to run conv_norm in fp32. no quality loss was noticed\n\nThis reverts commit cc9bc1339c998ebe9e7d733f910c6d72d9792213.\n\n* add more optimizations techniques to docs\n\n* Revert \"add shapes comments\"\n\nThis reverts commit 31c58eadb8892f95478cdf05229adf678678c5f4.\n\n* apply suggestions\n\n* make quality\n\n* apply suggestions\n\n* styling\n\n* `scheduler.timesteps` are now arrays so we dont need .to()\n\n* remove useless .type()\n\n* use mean instead of max in `test_stable_diffusion_inpaint_pipeline_k_lms`\n\n* move scheduler timestamps to correct device if tensors\n\n* add device to `set_timesteps` in LMSD scheduler\n\n* `self.scheduler.set_timesteps` now uses device arg for schedulers that accept it\n\n* quick fix\n\n* styling\n\n* remove kwargs from schedulers `set_timesteps`\n\n* revert to using max in K-LMS inpaint pipeline test\n\n* Revert \"`self.scheduler.set_timesteps` now uses device arg for schedulers that accept it\"\n\nThis reverts commit 00d5a51e5c20d8d445c8664407ef29608106d899.\n\n* move timesteps to correct device before loop in SD pipeline\n\n* apply previous fix to other SD pipelines\n\n* UNet now accepts tensor timesteps even on wrong device, to avoid errors\n- it shouldnt affect performance if timesteps are alrdy on correct device\n- it does slow down performance if they're on the wrong device\n\n* fix pipeline when timesteps are arrays with strides\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class UNet2DConditionModel(ModelMixin, ConfigMixin):",
            "# 6. post-process",
            "# make sure hidden states is in float32",
            "# when running in half-precision",
            "-        sample = self.conv_norm_out(sample.float()).type(sample.dtype)",
            "+        sample = self.conv_norm_out(sample)",
            "sample = self.conv_act(sample)",
            "sample = self.conv_out(sample)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=sample), position=1)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=type))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=sample))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=dtype))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 2832,
        "neg_line": [
            "-sample = self.conv_norm_out(sample.float()).type(sample.dtype)"
        ],
        "pos_line": [
            "+sample = self.conv_norm_out(sample)"
        ],
        "core_change": "-sample = self.conv_norm_out(sample.float()).type(sample.dtype) +sample = self.conv_norm_out(sample)",
        "core_API": "conv_norm_out"
    },
    {
        "commit_hash": "d01dc9e22d5e8625ae6ac49e2e689eebf472b5f8",
        "index": "c0687b4880..e9c1652121 100644",
        "commit_message": "[rllib] format with yapf (#2427)\n\n* initial yapf\n\n* manual fix yapf bugs\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Reshaper(object):",
            "def split_tensor(self, tensor, axis=-1):",
            "# FIXME (ev) This won't work for mixed action distributions like",
            "# one agent Gaussian one agent discrete",
            "-        slice_rescale = int(tensor.shape.as_list()[axis] /",
            "-                            int(np.sum(self.get_slice_lengths())))",
            "-        return tf.split(tensor, slice_rescale*self.get_slice_lengths(),",
            "-                        axis=axis)",
            "+        slice_rescale = int(tensor.shape.as_list()[axis] / int(",
            "+            np.sum(self.get_slice_lengths())))",
            "+        return tf.split(",
            "+            tensor, slice_rescale * self.get_slice_lengths(), axis=axis)",
            "",
            "def split_number(self, number):",
            "slice_rescale = int(number / int(np.sum(self.get_slice_lengths())))",
            "-        return slice_rescale*self.get_slice_lengths()",
            "+        return slice_rescale * self.get_slice_lengths()"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 5,
        "minus_line": 5,
        "AST_diff_line": 17,
        "number": 2833,
        "neg_line": [
            "-slice_rescale = int(tensor.shape.as_list()[axis] /",
            "-int(np.sum(self.get_slice_lengths())))",
            "-return tf.split(tensor, slice_rescale*self.get_slice_lengths(),",
            "-axis=axis)",
            "-return slice_rescale*self.get_slice_lengths()"
        ],
        "pos_line": [
            "+slice_rescale = int(tensor.shape.as_list()[axis] / int(",
            "+np.sum(self.get_slice_lengths())))",
            "+return tf.split(",
            "+tensor, slice_rescale * self.get_slice_lengths(), axis=axis)",
            "+return slice_rescale * self.get_slice_lengths()"
        ],
        "core_change": "-slice_rescale = int(tensor.shape.as_list()[axis] / -int(np.sum(self.get_slice_lengths()))) -return tf.split(tensor, slice_rescale*self.get_slice_lengths(), -axis=axis) +slice_rescale = int(tensor.shape.as_list()[axis] / int( +np.sum(self.get_slice_lengths()))) +return tf.split( +tensor, slice_rescale * self.get_slice_lengths(), axis=axis) -return slice_rescale*self.get_slice_lengths() +return slice_rescale * self.get_slice_lengths()",
        "core_API": "as_list"
    },
    {
        "commit_hash": "e850d36cab88360177f85d98d73c5efb4d9925ba",
        "index": "= adj._indices()",
        "commit_message": "fixed graclus and following transforms\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TargetIndegreeAdj(object):",
            "degree /= degree.max()  # Normalize.",
            "degree = degree[col]  # Target nodes.",
            "",
            "-        # Modify data and return.",
            "-        data.adj = SparseTensor(index, degree, torch.Size([n, n]))",
            "-        return data",
            "+        return SparseTensor(index, degree, torch.Size([n, n]))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=3, insert_id=1088420)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=1088421)",
            "Move(target_node=IN(type=return_statement), node=ASTNode(type=call), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=data))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=adj))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=data))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 13,
        "number": 2836,
        "neg_line": [
            "-# Modify data and return.",
            "-data.adj = SparseTensor(index, degree, torch.Size([n, n]))",
            "-return data"
        ],
        "pos_line": [
            "+return SparseTensor(index, degree, torch.Size([n, n]))"
        ],
        "core_change": "-# Modify data and return. -data.adj = SparseTensor(index, degree, torch.Size([n, n])) -return data +return SparseTensor(index, degree, torch.Size([n, n]))",
        "core_API": "max"
    },
    {
        "commit_hash": "f68b4f7839834dd18c4753fd92f0e8280eb7b6a1",
        "index": "c00bd49c2f..e3889a81e6 100644",
        "commit_message": "remove excess examples & fix docstrings\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def max_pool2d(",
            "pad_h = ivy.handle_padding(x_shape[0], strides[0], kernel[0], padding)",
            "pad_w = ivy.handle_padding(x_shape[1], strides[1], kernel[1], padding)",
            "x = torch.nn.functional.pad(",
            "-        x, [pad_w // 2,",
            "-            pad_w - pad_w // 2,",
            "-            pad_h // 2,",
            "-            pad_h - pad_h // 2],",
            "-        value=float(\"-inf\")",
            "+        x,",
            "+        [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2],",
            "+        value=float(\"-inf\"),",
            ")",
            "if padding != \"VALID\" and padding != \"SAME\":",
            "raise ivy.exceptions.IvyException("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=316513)"
        ],
        "plus_line": 3,
        "minus_line": 5,
        "AST_diff_line": 1,
        "number": 2838,
        "neg_line": [
            "-x, [pad_w // 2,",
            "-pad_w - pad_w // 2,",
            "-pad_h // 2,",
            "-pad_h - pad_h // 2],",
            "-value=float(\"-inf\")"
        ],
        "pos_line": [
            "+x,",
            "+[pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2],",
            "+value=float(\"-inf\"),"
        ],
        "core_change": "-x, [pad_w // 2, -pad_w - pad_w // 2, -pad_h // 2, -pad_h - pad_h // 2], -value=float(\"-inf\") +x, +[pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2], +value=float(\"-inf\"),",
        "core_API": "handle_padding"
    },
    {
        "commit_hash": "83a7b1a748ba1ea83d16d76be0948eb3a737617d",
        "index": "fea3415ea..5c0db489f 100644",
        "commit_message": "FIX CI tests and rollback syft endpoints authentication due to major changes\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def connect(",
            "metadata, _user_key = conn.login(credentials=credentials)  # type: ignore",
            "_user_key = SigningKey(_user_key.encode(), encoder=HexEncoder)",
            "else:",
            "-        metadata = conn.auth_using_key(user_key=user_key)  # type: ignore",
            "+        # metadata = conn.auth_using_key(user_key=user_key)  # type: ignore",
            "_user_key = user_key",
            "",
            "# Check node client type based on metadata response"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=_user_key), position=5)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=ERROR), position=6)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=:, text=:), position=7)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=type), position=8)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type==, text==), position=9)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=user_key), position=10)",
            "Update(target_node=ASTNode(type=identifier, text=metadata), value='_user_key')",
            "Delete(target_node=ASTNode(type=identifier, text=conn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=auth_using_key))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=user_key))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=typed_default_parameter))",
            "Delete(target_node=ASTNode(type=identifier, text=_user_key))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=user_key))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 2839,
        "neg_line": [
            "-metadata = conn.auth_using_key(user_key=user_key)  # type: ignore"
        ],
        "pos_line": [
            "+# metadata = conn.auth_using_key(user_key=user_key)  # type: ignore"
        ],
        "core_change": "-metadata = conn.auth_using_key(user_key=user_key)  # type: ignore +# metadata = conn.auth_using_key(user_key=user_key)  # type: ignore",
        "core_API": "login"
    },
    {
        "commit_hash": "03efab5db9931514534ef858f7ac7367f7b29d20",
        "index": "ec31070e..31e7aa90 100644",
        "commit_message": "Support constraints.sphere in AutoDelta (#2749)\n\n* Support constraints.sphere in AutoDelta\n\n* Fix test\n\n* Add docs\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ProjectedNormal(TorchDistribution):",
            "Note this is the mean in the sense of a centroid in the submanifold",
            "that minimizes expected squared geodesic distance.",
            "\"\"\"",
            "-        return safe_project(self.concentration)",
            "+        return safe_normalize(self.concentration)",
            "",
            "@property",
            "def mode(self):",
            "-        return safe_project(self.concentration)",
            "+        return safe_normalize(self.concentration)",
            "",
            "def rsample(self, sample_shape=torch.Size()):",
            "shape = self._extended_shape(sample_shape)",
            "x = self.concentration.new_empty(shape).normal_()",
            "x = x + self.concentration",
            "-        x = safe_project(x)",
            "+        x = safe_normalize(x)",
            "return x",
            "",
            "def log_prob(self, value):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=4, insert_id=686903)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=686904)",
            "Insert(target_node=IN(type=return_statement), node=('call', None), position=1, insert_id=686905)",
            "Insert(target_node=IN(type=call), node=('identifier', 'safe_normalize'), position=0, insert_id=686906)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=686907)",
            "Move(target_node=ASTNode(type=block), node=ASTNode(type=return_statement), position=0)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=686908)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=686909)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=686910)",
            "Update(target_node=ASTNode(type=identifier, text=safe_project), value='safe_normalize')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=686911)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=686912)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'concentration'), position=2, insert_id=686913)",
            "Update(target_node=ASTNode(type=identifier, text=safe_project), value='safe_normalize')",
            "Delete(target_node=ASTNode(type=return, text=return))",
            "Delete(target_node=ASTNode(type=identifier, text=safe_project))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=concentration))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=return_statement))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 25,
        "number": 2840,
        "neg_line": [
            "-return safe_project(self.concentration)",
            "-return safe_project(self.concentration)",
            "-x = safe_project(x)"
        ],
        "pos_line": [
            "+return safe_normalize(self.concentration)",
            "+return safe_normalize(self.concentration)",
            "+x = safe_normalize(x)"
        ],
        "core_change": "-return safe_project(self.concentration) +return safe_normalize(self.concentration) -return safe_project(self.concentration) +return safe_normalize(self.concentration) -x = safe_project(x) +x = safe_normalize(x)",
        "core_API": "Size"
    },
    {
        "commit_hash": "bc6764a5c7c58463d2a879f54ca0cc390ca69070",
        "index": "347bef21..afc083aa 100644",
        "commit_message": "bug fix at server\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Synthesizer(object):",
            "sample_rate=self.ap.sample_rate,",
            ").cuda()",
            "",
            "-        check = torch.load(model_file)",
            "-        self.wavernn.load_state_dict(check['model'], map_location=\"cpu\")",
            "+        check = torch.load(model_file, map_location=\"cpu\")",
            "+        self.wavernn.load_state_dict(check['model'])",
            "if use_cuda:",
            "self.wavernn.cuda()",
            "self.wavernn.eval()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1557912)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=keyword_argument), position=3)",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 2841,
        "neg_line": [
            "-check = torch.load(model_file)",
            "-self.wavernn.load_state_dict(check['model'], map_location=\"cpu\")"
        ],
        "pos_line": [
            "+check = torch.load(model_file, map_location=\"cpu\")",
            "+self.wavernn.load_state_dict(check['model'])"
        ],
        "core_change": "-check = torch.load(model_file) -self.wavernn.load_state_dict(check['model'], map_location=\"cpu\") +check = torch.load(model_file, map_location=\"cpu\") +self.wavernn.load_state_dict(check['model'])",
        "core_API": "load"
    },
    {
        "commit_hash": "b72bbfc9680d4410e0ce7a48a233f834593d9e4a",
        "index": "26d129f5..d2224294 100644",
        "commit_message": "fix constraint bug in beam search, clean up tests (#5328)\n\n* fix constraint bug in beam search, clean up tests\n\n* fix linting error\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BeamSearch(Registrable):",
            "",
            "for i, constraint in enumerate(self.constraints):",
            "constraint_states[i] = constraint.update_state(",
            "-                    constraint_states[i], restricted_predicted_classes",
            "+                    constraint_states[i], restricted_predicted_classes, last_backpointer=backpointer",
            ")",
            "",
            "# Warn about \"-inf\" log probabilities if not using any constraints (negligible"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1228)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1229)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'last_backpointer'), position=0, insert_id=1230)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1231)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'backpointer'), position=2, insert_id=1232)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2843,
        "neg_line": [
            "-constraint_states[i], restricted_predicted_classes"
        ],
        "pos_line": [
            "+constraint_states[i], restricted_predicted_classes, last_backpointer=backpointer"
        ],
        "core_change": "-constraint_states[i], restricted_predicted_classes +constraint_states[i], restricted_predicted_classes, last_backpointer=backpointer",
        "core_API": "update_state"
    },
    {
        "commit_hash": "8595376da52c360bfbc3b3785c9863382350854d",
        "index": "e094d501..8c0b3572 100644",
        "commit_message": "Remove Re-definition found for builtin input function - Update tests (#2255)\n\n* change input to sample in test/feature folder\n\n* change input to sample in test/filters folder\n\n* change input to sample in test/morphology folder\n\n* change input to sample in test/geometry/subpix/test_spatial_softargmax.py\n\n* change input to sample in test/geometry/transfrom in test_imgwarp3d.py and test_pyramid.py\n\n* change input to input_org in test/geometry/transfrom in test_imgwarp.py\n\n* change input to inpt in test/grad_estimator/test_ste.py\n\n* change input to inpt and sample in test/enhance/ folder\n\n* change input to input_tensor in test/augmentation/test_augmentation_3d.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n---------\n\nCo-authored-by: oleksadobush <oleksandra.stasiuk@ucu.edu.ua>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestEqualization(BaseTester):",
            "inputs = torch.rand(bs, channels, height, width, device=device, dtype=dtype)",
            "inputs = tensor_to_gradcheck_var(inputs)",
            "",
            "-        def grad_rot(input, a, b, c):",
            "-            rot = rotate(input, torch.tensor(30.0, dtype=input.dtype, device=device))",
            "+        def grad_rot(inpt, a, b, c):",
            "+            rot = rotate(inpt, torch.tensor(30.0, dtype=inpt.dtype, device=device))",
            "return enhance.equalize_clahe(rot, a, b, c)",
            "",
            "assert gradcheck(grad_rot, (inputs, 40.0, (2, 2), True), nondet_tol=1e-4, raise_exception=True, fast_mode=True)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=input), value='inpt')",
            "Update(target_node=ASTNode(type=identifier, text=input), value='inpt')",
            "Update(target_node=ASTNode(type=identifier, text=input), value='inpt')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 2844,
        "neg_line": [
            "-def grad_rot(input, a, b, c):",
            "-rot = rotate(input, torch.tensor(30.0, dtype=input.dtype, device=device))"
        ],
        "pos_line": [
            "+def grad_rot(inpt, a, b, c):",
            "+rot = rotate(inpt, torch.tensor(30.0, dtype=inpt.dtype, device=device))"
        ],
        "core_change": "-def grad_rot(input, a, b, c): -rot = rotate(input, torch.tensor(30.0, dtype=input.dtype, device=device)) +def grad_rot(inpt, a, b, c): +rot = rotate(inpt, torch.tensor(30.0, dtype=inpt.dtype, device=device))",
        "core_API": "rand"
    },
    {
        "commit_hash": "7b09325a85d76435ced550bd5f7b757e8028e944",
        "index": "64a958b105..d89a56aaf9 100644",
        "commit_message": "Type promotion fixes (#2516)\n\n* casting fixes\n\n* lint fixes\n\n* changes\n\n* more changes\n\n* lint fixes\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def floor_divide(",
            "if (not np.all(x2)) or (np.any(x2) == -0):  # check for division by zero",
            "ret = np.floor_divide(x1, x2)",
            "else:",
            "-        ret = tf.math.floordiv(x1, x2)",
            "+        ret = tf.experimental.numpy.floor_divide(x1, x2)",
            "",
            "if (any(isinf(x1)) and any(isfinite(x2))) or (any(isfinite(x1)) and any(isinf(x2))):",
            "return ivy.full_like(ret, floor(divide(x1, x2)), dtype=ret.dtype)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2002828)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2002829)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'floor_divide'), position=2, insert_id=2002830)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=floordiv), value='numpy')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=floordiv), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=math), value='experimental')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2845,
        "neg_line": [
            "-ret = tf.math.floordiv(x1, x2)"
        ],
        "pos_line": [
            "+ret = tf.experimental.numpy.floor_divide(x1, x2)"
        ],
        "core_change": "-ret = tf.math.floordiv(x1, x2) +ret = tf.experimental.numpy.floor_divide(x1, x2)",
        "core_API": "all"
    },
    {
        "commit_hash": "753add230ab7c522ec164d15cbe314eacf1b6e9d",
        "index": "63f3b11f7..2fef2ad1b 100644",
        "commit_message": "Coding style fixup (#3434)\n\n* Coding style fixup\n\n* Storage to store\n\n* Update lr.py\n\n* Add run websocket client\n\n* Base worker true/false test\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_plan_module_tracing():",
            "y = torch.rand([1])",
            "return x + y",
            "",
            "-    p = plan_test(torch.tensor([3]))",
            "+    plan_test(torch.tensor([3]))",
            "assert len(plan_test.role.actions) == 2"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=p))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 2848,
        "neg_line": [
            "-p = plan_test(torch.tensor([3]))"
        ],
        "pos_line": [
            "+plan_test(torch.tensor([3]))"
        ],
        "core_change": "-p = plan_test(torch.tensor([3])) +plan_test(torch.tensor([3]))",
        "core_API": "rand"
    },
    {
        "commit_hash": "03aaac35021dead4fb7ad354fe9c986d16869f03",
        "index": "4d5877c43..0f3d5ab68 100644",
        "commit_message": "Fix TVLT (torch device issue) (#21710)\n\n* fix tvlt ci\n\n* fix tvlt ci\n\n* fix tvlt ci\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TvltModelIntegrationTest(unittest.TestCase):",
            "outputs = model(**inputs)",
            "",
            "# verify the logits",
            "-        expected_last_hidden_state_slice = torch.tensor([[-0.0186, -0.0691], [0.0242, -0.0398]])",
            "+        expected_last_hidden_state_slice = torch.tensor([[-0.0186, -0.0691], [0.0242, -0.0398]], device=torch_device)",
            "self.assertTrue(",
            "torch.allclose(outputs.last_hidden_state[:, :2, :2], expected_last_hidden_state_slice, atol=1e-4)",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1176288)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1176289)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1176290)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1176291)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'torch_device'), position=2, insert_id=1176292)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2850,
        "neg_line": [
            "-expected_last_hidden_state_slice = torch.tensor([[-0.0186, -0.0691], [0.0242, -0.0398]])"
        ],
        "pos_line": [
            "+expected_last_hidden_state_slice = torch.tensor([[-0.0186, -0.0691], [0.0242, -0.0398]], device=torch_device)"
        ],
        "core_change": "-expected_last_hidden_state_slice = torch.tensor([[-0.0186, -0.0691], [0.0242, -0.0398]]) +expected_last_hidden_state_slice = torch.tensor([[-0.0186, -0.0691], [0.0242, -0.0398]], device=torch_device)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "876c2d8918c6132ed25db64fc279dbc8a4e988de",
        "index": "5d6e5f2f..26801d19 100644",
        "commit_message": "fix doc example\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RandomVerticalFlip(RandomFlip):",
            "wont be concatenated",
            "",
            "Examples:",
            "-        >>> input = torch.tensor([[[",
            "-            [0., 0., 0.],",
            "-            [0., 0., 0.],",
            "-            [0., 1., 1.]]]])",
            "+        >>> input = torch.tensor([[[[0., 0., 0.],",
            "+                                    [0., 0., 0.],",
            "+                                    [0., 1., 1.]]]])",
            ">>> seq = nn.Sequential(kornia.augmentation.RandomVerticalFlip(p=1.0, return_transform=True))",
            ">>> seq(input)",
            "(tensor([[0., 1., 1.],"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 2852,
        "neg_line": [
            "->>> input = torch.tensor([[[",
            "-[0., 0., 0.],",
            "-[0., 0., 0.],",
            "-[0., 1., 1.]]]])"
        ],
        "pos_line": [
            "+>>> input = torch.tensor([[[[0., 0., 0.],",
            "+[0., 0., 0.],",
            "+[0., 1., 1.]]]])"
        ],
        "core_change": "->>> input = torch.tensor([[[ -[0., 0., 0.], -[0., 0., 0.], -[0., 1., 1.]]]]) +>>> input = torch.tensor([[[[0., 0., 0.], +[0., 0., 0.], +[0., 1., 1.]]]])",
        "core_API": "tensor"
    },
    {
        "commit_hash": "cb0bf0bd0b8d4ab41855dc687392a7a80ccd8af7",
        "index": "1aebb27a..8e2f5d92 100644",
        "commit_message": "fix(DDIM scheduler): use correct dtype for noise (#742)\n\nOtherwise, it crashes when eta > 0 with float16.\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DDIMScheduler(SchedulerMixin, ConfigMixin):",
            "prev_sample = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction",
            "",
            "if eta > 0:",
            "+            # randn_like does not support generator https://github.com/pytorch/pytorch/issues/27072",
            "device = model_output.device if torch.is_tensor(model_output) else \"cpu\"",
            "-            noise = torch.randn(model_output.shape, generator=generator).to(device)",
            "+            noise = torch.randn(model_output.shape, dtype=model_output.dtype, generator=generator).to(device)",
            "variance = self._get_variance(timestep, prev_timestep) ** (0.5) * eta * noise",
            "",
            "prev_sample = prev_sample + variance"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=103049)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=103050)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=103051)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=103052)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=103053)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'model_output'), position=0, insert_id=103054)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=103055)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=103056)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2859,
        "neg_line": [
            "-noise = torch.randn(model_output.shape, generator=generator).to(device)"
        ],
        "pos_line": [
            "+# randn_like does not support generator https://github.com/pytorch/pytorch/issues/27072",
            "+noise = torch.randn(model_output.shape, dtype=model_output.dtype, generator=generator).to(device)"
        ],
        "core_change": "+# randn_like does not support generator https://github.com/pytorch/pytorch/issues/27072 -noise = torch.randn(model_output.shape, generator=generator).to(device) +noise = torch.randn(model_output.shape, dtype=model_output.dtype, generator=generator).to(device)",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "c3293a9bad06dd13b5f6edbf4d35d2a959b60185",
        "index": "62edf09..942c6a5 100644",
        "commit_message": "Changes to named arguments. (Fix for tf 1.0.0)\n\nTo avoid confusion, tf.nn.softmax_cross_entropy_with_logits only accepts named arguments in Tensorflow 1.0.0.\n",
        "file": "gcn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def masked_accuracy(preds, labels, mask):",
            "mask = tf.cast(mask, dtype=tf.float32)",
            "mask /= tf.reduce_mean(mask)",
            "accuracy_all *= mask",
            "-    return tf.reduce_mean(accuracy_all)",
            "\\ No newline at end of file",
            "+    return tf.reduce_mean(accuracy_all)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=return_statement), node=ASTNode(type=call), position=1)",
            "Insert(target_node=ASTNode(type=augmented_assignment), node=('ERROR', None), position=2, insert_id=1931938)",
            "Insert(target_node=ASTNode(type=augmented_assignment), node=('identifier', 'file'), position=3, insert_id=1931939)",
            "Move(target_node=IN(type=ERROR), node=ASTNode(type=identifier, text=mask), position=0)",
            "Insert(target_node=IN(type=ERROR), node=('ERROR', ' '), position=1, insert_id=1931940)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'No'), position=2, insert_id=1931941)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'newline'), position=3, insert_id=1931942)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'at'), position=4, insert_id=1931943)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'end'), position=5, insert_id=1931944)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'of'), position=6, insert_id=1931945)",
            "Delete(target_node=ASTNode(type=ERROR, text= ))",
            "Delete(target_node=ASTNode(type=identifier, text=No))",
            "Delete(target_node=ASTNode(type=identifier, text=newline))",
            "Delete(target_node=ASTNode(type=identifier, text=at))",
            "Delete(target_node=ASTNode(type=identifier, text=end))",
            "Delete(target_node=ASTNode(type=identifier, text=of))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=file))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 2861,
        "neg_line": [
            "-return tf.reduce_mean(accuracy_all)"
        ],
        "pos_line": [
            "+return tf.reduce_mean(accuracy_all)"
        ],
        "core_change": "-return tf.reduce_mean(accuracy_all) +return tf.reduce_mean(accuracy_all)",
        "core_API": "cast"
    },
    {
        "commit_hash": "55835619fbeb2bf3e12f93a21dd8babfce85b2e7",
        "index": "41d70593..6658a576 100644",
        "commit_message": "fixed forward pass\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SplineGCN(Module):",
            "self.reset_parameters()",
            "",
            "def reset_parameters(self):",
            "-        stdv = 1. / math.sqrt(self.in_features * self.k_max)",
            "+        stdv = 1. / math.sqrt(self.in_features * self.K)",
            "",
            "self.weight.data.uniform_(-stdv, stdv)",
            "if self.bias is not None:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=k_max), value='K')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2864,
        "neg_line": [
            "-stdv = 1. / math.sqrt(self.in_features * self.k_max)"
        ],
        "pos_line": [
            "+stdv = 1. / math.sqrt(self.in_features * self.K)"
        ],
        "core_change": "-stdv = 1. / math.sqrt(self.in_features * self.k_max) +stdv = 1. / math.sqrt(self.in_features * self.K)",
        "core_API": "reset_parameters"
    },
    {
        "commit_hash": "a5327c6a9a2c47a9e0c0f5707e54d2cdf39c12ca",
        "index": "8e29fd734..457c477fc 100644",
        "commit_message": "Fixed issue #21053 (#21065)\n\nCo-authored-by: susnato <susnato@tensorflow123456@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFGPT2MainLayer(tf.keras.layers.Layer):",
            "# indices on GPU, returning zeros instead. This is a dangerous silent behavior.",
            "tf.debugging.assert_less(",
            "input_ids,",
            "-                tf.cast(self.vocab_size, dtype=input_ids.dtype),",
            "+                tf.cast(self.config.vocab_size, dtype=input_ids.dtype),",
            "message=(",
            "\"input_ids must be smaller than the embedding layer's input dimension (got\"",
            "f\" {tf.math.reduce_max(input_ids)} >= {self.vocab_size})\""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2357533)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2357534)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=self), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=2, insert_id=2357535)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2865,
        "neg_line": [
            "-tf.cast(self.vocab_size, dtype=input_ids.dtype),"
        ],
        "pos_line": [
            "+tf.cast(self.config.vocab_size, dtype=input_ids.dtype),"
        ],
        "core_change": "-tf.cast(self.vocab_size, dtype=input_ids.dtype), +tf.cast(self.config.vocab_size, dtype=input_ids.dtype),",
        "core_API": "assert_less"
    },
    {
        "commit_hash": "1cc1bfadaef6db2d1ee5d5170be8cd7595f5c117",
        "index": "36dcad5b0..95c9d8a7e 100644",
        "commit_message": "fix for ci tests\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CTC(torch.nn.Module):",
            "self.ctc_lo = torch.nn.Linear(eprojs, odim)",
            "",
            "# In case of Pytorch >= 1.2.0, CTC will be always builtin",
            "-        torch_ver = int(torch.__version__.replace('.', ''))",
            "+        torch_ver = int(torch.__version__.replace('.', '').replace('post2', ''))",
            "self.ctc_type = ctc_type if torch_ver < 120 else 'builtin'",
            "",
            "if self.ctc_type == 'builtin':"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=158284)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=158285)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=158286)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'replace'), position=2, insert_id=158287)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=158288)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'post2'\"), position=1, insert_id=158289)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=158290)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"''\"), position=3, insert_id=158291)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=158292)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 2870,
        "neg_line": [
            "-torch_ver = int(torch.__version__.replace('.', ''))"
        ],
        "pos_line": [
            "+torch_ver = int(torch.__version__.replace('.', '').replace('post2', ''))"
        ],
        "core_change": "-torch_ver = int(torch.__version__.replace('.', '')) +torch_ver = int(torch.__version__.replace('.', '').replace('post2', ''))",
        "core_API": "Linear"
    },
    {
        "commit_hash": "3dda5ac9fcf390d8b83eb855249360711707c11c",
        "index": "e326a2f2..16128004 100644",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestDotProductSimilarityFunction(AllenNlpTestCase):",
            "a_vectors = numpy.random.rand(5, 4, 3, 6, 7)",
            "b_vectors = numpy.random.rand(5, 4, 3, 6, 7)",
            "desired_result = numpy.sum(a_vectors * b_vectors, axis=-1)",
            "-        result = dot_product(torch.from_numpy(a_vectors),",
            "-                             torch.from_numpy(b_vectors)).data.numpy()",
            "+        result = dot_product(torch.from_numpy(a_vectors), torch.from_numpy(b_vectors)).data.numpy()",
            "assert result.shape == (5, 4, 3, 6)",
            "# We're cutting this down here with a random partial index, so that if this test fails the",
            "# output isn't so huge and slow.",
            "assert_almost_equal(result[2, 3, 1], desired_result[2, 3, 1])",
            "",
            "def test_can_construct_from_params(self):",
            "-        assert DotProductSimilarity.from_params(Params({})).__class__.__name__ == 'DotProductSimilarity'",
            "+        assert (",
            "+            DotProductSimilarity.from_params(Params({})).__class__.__name__",
            "+            == \"DotProductSimilarity\"",
            "+        )"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assert_statement), node=('parenthesized_expression', None), position=1, insert_id=23837)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=23838)",
            "Move(target_node=IN(type=parenthesized_expression), node=ASTNode(type=comparison_operator), position=1)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=23839)",
            "Update(target_node=ASTNode(type=string, text='DotProductSimilarity'), value='\"DotProductSimilarity\"')"
        ],
        "plus_line": 5,
        "minus_line": 3,
        "AST_diff_line": 5,
        "number": 2872,
        "neg_line": [
            "-result = dot_product(torch.from_numpy(a_vectors),",
            "-torch.from_numpy(b_vectors)).data.numpy()",
            "-assert DotProductSimilarity.from_params(Params({})).__class__.__name__ == 'DotProductSimilarity'"
        ],
        "pos_line": [
            "+result = dot_product(torch.from_numpy(a_vectors), torch.from_numpy(b_vectors)).data.numpy()",
            "+assert (",
            "+DotProductSimilarity.from_params(Params({})).__class__.__name__",
            "+== \"DotProductSimilarity\"",
            "+)"
        ],
        "core_change": "-result = dot_product(torch.from_numpy(a_vectors), -torch.from_numpy(b_vectors)).data.numpy() +result = dot_product(torch.from_numpy(a_vectors), torch.from_numpy(b_vectors)).data.numpy() -assert DotProductSimilarity.from_params(Params({})).__class__.__name__ == 'DotProductSimilarity' +assert ( +DotProductSimilarity.from_params(Params({})).__class__.__name__ +== \"DotProductSimilarity\" +)",
        "core_API": "rand"
    },
    {
        "commit_hash": "66d4b129903058365f6cf1d4fb4ac73814ca3bfc",
        "index": "8e091241..5f1644a2 100644",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def guide(data, k):",
            "",
            "def local_guide(latent, k):",
            "# The local guide simply guesses category assignments.",
            "-    latent.ps.param_(Variable(torch.ones(k) / k, requires_grad=True))",
            "+    latent.ps.param_(torch.tensor(torch.ones(k) / k, requires_grad=True))",
            "latent.id.sample_(dist.Categorical(softmax(latent.ps)))",
            "",
            "",
            "def main(args):",
            "optim = Adam({\"lr\": 0.1})",
            "inference = SVI(model, guide, optim, loss=\"ELBO\")",
            "-    data = Variable(torch.Tensor([0, 1, 2, 20, 30, 40]))",
            "+    data = torch.tensor([0, 1, 2, 20, 30, 40])",
            "k = 2",
            "",
            "print('Step\\tLoss')"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='tensor')",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=750254)",
            "Update(target_node=ASTNode(type=identifier, text=Variable), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=Variable), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=750255)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tensor'), position=2, insert_id=750256)",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 2874,
        "neg_line": [
            "-latent.ps.param_(Variable(torch.ones(k) / k, requires_grad=True))",
            "-data = Variable(torch.Tensor([0, 1, 2, 20, 30, 40]))"
        ],
        "pos_line": [
            "+latent.ps.param_(torch.tensor(torch.ones(k) / k, requires_grad=True))",
            "+data = torch.tensor([0, 1, 2, 20, 30, 40])"
        ],
        "core_change": "-latent.ps.param_(Variable(torch.ones(k) / k, requires_grad=True)) +latent.ps.param_(torch.tensor(torch.ones(k) / k, requires_grad=True)) -data = Variable(torch.Tensor([0, 1, 2, 20, 30, 40])) +data = torch.tensor([0, 1, 2, 20, 30, 40])",
        "core_API": "param_"
    },
    {
        "commit_hash": "eb66e0d01ecded59160ab8d5c0e8e302eb1836a1",
        "index": "2c5f4d61..9df9c966 100644",
        "commit_message": "add pat change (#3414)\n\n* add pat change\n\n* fix grid roi head\n\n* fix comments\n\n* clean\n\n* revert change\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def patch_norm_fp32(module):",
            "\"\"\"",
            "if isinstance(module, (nn.modules.batchnorm._BatchNorm, nn.GroupNorm)):",
            "module.float()",
            "-        if isinstance(module, nn.GroupNorm) or torch.__version__ < '1.3':",
            "+        if isinstance(module, nn.GroupNorm) or torch.__version__ == 'parrots':",
            "module.forward = patch_forward_method(module.forward, torch.half,",
            "torch.float)",
            "for child in module.children():"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('==', '=='), position=1, insert_id=1426458)",
            "Update(target_node=ASTNode(type=string, text='1.3'), value=\"'parrots'\")",
            "Delete(target_node=ASTNode(type=<, text=<))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 2877,
        "neg_line": [
            "-if isinstance(module, nn.GroupNorm) or torch.__version__ < '1.3':"
        ],
        "pos_line": [
            "+if isinstance(module, nn.GroupNorm) or torch.__version__ == 'parrots':"
        ],
        "core_change": "-if isinstance(module, nn.GroupNorm) or torch.__version__ < '1.3': +if isinstance(module, nn.GroupNorm) or torch.__version__ == 'parrots':",
        "core_API": "float"
    },
    {
        "commit_hash": "afe5d42d8d1d80af911ed980c2936bfe887078f6",
        "index": "09fd338d3..900b425b3 100755",
        "commit_message": "Black preview (#17217)\n\n* Black preview\n\n* Fixup too!\n\n* Fix check copies\n\n* Use the same version as the CI\n\n* Bump black\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ModelTesterMixin:",
            "self.assertLessEqual(max_diff, tol, f\"{name}: Difference between torch and tf is {max_diff} (>= {tol}).\")",
            "else:",
            "raise ValueError(",
            "-                f\"`tf_outputs` should be an instance of `tf.Tensor`, a `tuple`, or an instance of `tf.Tensor`. Got {type(tf_outputs)} instead.\"",
            "+                \"`tf_outputs` should be an instance of `tf.Tensor`, a `tuple`, or an instance of `tf.Tensor`. Got\"",
            "+                f\" {type(tf_outputs)} instead.\"",
            ")",
            "",
            "def prepare_tf_inputs_from_pt_inputs(self, pt_inputs_dict):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('concatenated_string', None), position=1, insert_id=2364300)",
            "Update(target_node=ASTNode(type=string, text=f\"`tf_outputs` should be an instance of `tf.Tensor`, a `tuple`, or an instance of `tf.Tensor`. Got {type(tf_outputs)} instead.\"), value='\"`tf_outputs` should be an instance of `tf.Tensor`, a `tuple`, or an instance of `tf.Tensor`. Got\"')",
            "Move(target_node=IN(type=concatenated_string), node=ASTNode(type=string, text=f\"`tf_outputs` should be an instance of `tf.Tensor`, a `tuple`, or an instance of `tf.Tensor`. Got {type(tf_outputs)} instead.\"), position=0)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', 'f\" {type(tf_outputs)} instead.\"'), position=1, insert_id=2364301)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 2878,
        "neg_line": [
            "-f\"`tf_outputs` should be an instance of `tf.Tensor`, a `tuple`, or an instance of `tf.Tensor`. Got {type(tf_outputs)} instead.\""
        ],
        "pos_line": [
            "+\"`tf_outputs` should be an instance of `tf.Tensor`, a `tuple`, or an instance of `tf.Tensor`. Got\"",
            "+f\" {type(tf_outputs)} instead.\""
        ],
        "core_change": "-f\"`tf_outputs` should be an instance of `tf.Tensor`, a `tuple`, or an instance of `tf.Tensor`. Got {type(tf_outputs)} instead.\" +\"`tf_outputs` should be an instance of `tf.Tensor`, a `tuple`, or an instance of `tf.Tensor`. Got\" +f\" {type(tf_outputs)} instead.\"",
        "core_API": "assertLessEqual"
    },
    {
        "commit_hash": "556f0051521ec539026977e91c0e4b42445340ea",
        "index": "81762c10..df65fb31 100755",
        "commit_message": "Fix random token-generation issue + MP-checkpoint loading/saving (#2132)\n\nCo-authored-by: Jeff Rasley <jerasley@microsoft.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DeepSpeedSelfAttentionFunction(Function):",
            "return tensor_list",
            "",
            "def backup_attention(mixed_x_layer, layer_past, alibi, input_mask, norm_factor):",
            "+            alibi = alibi.to(torch.cuda.current_device())",
            "head_dim = hidden_size_per_partition // num_attention_heads_per_partition",
            "new_tensor_shape = mixed_x_layer.size()[:-1] + (",
            "num_attention_heads_per_partition,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=77352)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=77353)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=77354)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'alibi'), position=0, insert_id=77355)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=77356)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=77357)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=77358)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=77359)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'alibi'), position=0, insert_id=77360)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=77361)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=77362)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=77363)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=77364)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=77365)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=77366)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=77367)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=77368)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=77369)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'current_device'), position=2, insert_id=77370)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=77371)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=77372)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=77373)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=77374)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=77375)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 25,
        "number": 2880,
        "neg_line": [],
        "pos_line": [
            "+alibi = alibi.to(torch.cuda.current_device())"
        ],
        "core_change": "+alibi = alibi.to(torch.cuda.current_device())",
        "core_API": "to"
    },
    {
        "commit_hash": "5383188c7e5ab8089333ebff995cd83c75c6f116",
        "index": "f328a440..3a6d5139 100644",
        "commit_message": "StableDiffusionDepth2ImgPipeline (#1531)\n\n* begin depth pipeline\n\n* add depth estimation model\n\n* fix prepare_depth_mask\n\n* add a comment about autocast\n\n* copied from, quality, cleanup\n\n* begin tests\n\n* handle tensors\n\n* norm image tensor\n\n* fix batch size\n\n* fix tests\n\n* fix enable_sequential_cpu_offload\n\n* fix save load\n\n* fix test_save_load_float16\n\n* fix test_save_load_optional_components\n\n* fix test_float16_inference\n\n* fix test_cpu_offload_forward_pass\n\n* fix test_dict_tuple_outputs_equivalent\n\n* up\n\n* fix fast tests\n\n* fix test_stable_diffusion_img2img_multiple_init_images\n\n* fix few more fast tests\n\n* don't use device map for DPT\n\n* fix test_stable_diffusion_pipeline_with_sequential_cpu_offloading\n\n* accept external depth maps\n\n* prepare_depth_mask -> prepare_depth_map\n\n* fix file name\n\n* fix file name\n\n* quality\n\n* check transformers version\n\n* fix test names\n\n* use skipif\n\n* fix import\n\n* add docs\n\n* skip tests on mps\n\n* correct version\n\n* uP\n\n* Update docs/source/api/pipelines/stable_diffusion_2.mdx\n\n* fix fix-copies\n\n* fix fix-copies\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: anton- <anton@huggingface.co>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PipelineFastTests(unittest.TestCase):",
            "",
            "# Validate that the text encoder safetensor exists and are of the correct format",
            "text_encoder_path = os.path.join(tmpdirname, \"text_encoder\", \"model.safetensors\")",
            "-            if transformers.__version__ >= \"4.25.1\":",
            "-                assert os.path.exists(text_encoder_path), f\"Could not find {text_encoder_path}\"",
            "-                _ = safetensors.torch.load_file(text_encoder_path)",
            "+            assert os.path.exists(text_encoder_path), f\"Could not find {text_encoder_path}\"",
            "+            _ = safetensors.torch.load_file(text_encoder_path)",
            "",
            "pipeline = StableDiffusionPipeline.from_pretrained(tmpdirname)",
            "assert pipeline.unet is not None"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=assert_statement), position=2)",
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=3)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=transformers))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=__version__))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=>=, text=>=))",
            "Delete(target_node=ASTNode(type=string, text=\"4.25.1\"))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 13,
        "number": 2881,
        "neg_line": [
            "-if transformers.__version__ >= \"4.25.1\":",
            "-assert os.path.exists(text_encoder_path), f\"Could not find {text_encoder_path}\"",
            "-_ = safetensors.torch.load_file(text_encoder_path)"
        ],
        "pos_line": [
            "+assert os.path.exists(text_encoder_path), f\"Could not find {text_encoder_path}\"",
            "+_ = safetensors.torch.load_file(text_encoder_path)"
        ],
        "core_change": "-if transformers.__version__ >= \"4.25.1\": -assert os.path.exists(text_encoder_path), f\"Could not find {text_encoder_path}\" -_ = safetensors.torch.load_file(text_encoder_path) +assert os.path.exists(text_encoder_path), f\"Could not find {text_encoder_path}\" +_ = safetensors.torch.load_file(text_encoder_path)",
        "core_API": "join"
    },
    {
        "commit_hash": "c17169dc110ed0699eefab04ca17537eb68ce713",
        "index": "37caa68d6..b22e784f3 100644",
        "commit_message": "[RLlib] Fix all example scripts to run on GPUs. (#11105)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchBinaryAutoregressiveDistribution(TorchDistributionWrapper):",
            "",
            "def _a1_distribution(self):",
            "BATCH = self.inputs.shape[0]",
            "-        a1_logits, _ = self.model.action_module(self.inputs,",
            "-                                                torch.zeros((BATCH, 1)))",
            "+        zeros = torch.zeros((BATCH, 1)).to(self.inputs.device)",
            "+        a1_logits, _ = self.model.action_module(self.inputs, zeros)",
            "a1_dist = TorchCategorical(a1_logits)",
            "return a1_dist"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1121542)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1121543)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'zeros'), position=0, insert_id=1121544)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1121545)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1121546)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1121547)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1121548)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1121549)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1121550)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1121551)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1121552)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1121553)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'zeros'), position=3, insert_id=1121554)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=1121555)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1121556)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1121557)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1121558)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1121559)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1121560)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'inputs'), position=2, insert_id=1121561)",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 22,
        "number": 2882,
        "neg_line": [
            "-a1_logits, _ = self.model.action_module(self.inputs,",
            "-torch.zeros((BATCH, 1)))"
        ],
        "pos_line": [
            "+zeros = torch.zeros((BATCH, 1)).to(self.inputs.device)",
            "+a1_logits, _ = self.model.action_module(self.inputs, zeros)"
        ],
        "core_change": "-a1_logits, _ = self.model.action_module(self.inputs, -torch.zeros((BATCH, 1))) +zeros = torch.zeros((BATCH, 1)).to(self.inputs.device) +a1_logits, _ = self.model.action_module(self.inputs, zeros)",
        "core_API": "action_module"
    },
    {
        "commit_hash": "9c7052dcc1188d950d24b7f4d2cc31d3c563ab92",
        "index": "7c3d5d70..5b5caa03 100644",
        "commit_message": "Small fixes for compatibility with pytorch master (#442)\n\n* Small fixes for campatibility with pytorch master\n\n* Support older pytorch 0.2 torch.nn.functional.softmax\n\n* Work around bugs in torch.cat()\n\n* Replace torch_cat(-) with torch.stack(-).squeeze()\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Fixture(object):",
            "",
            "def _convert_logits_to_ps(self, dist_params):",
            "if 'logits' in dist_params:",
            "-            logits = torch.Tensor(dist_params.pop('logits'))",
            "+            logits = Variable(torch.Tensor(dist_params.pop('logits')))",
            "is_multidimensional = self.get_test_distribution_name() != 'Bernoulli'",
            "ps, _ = get_probs_and_logits(logits=logits, is_multidimensional=is_multidimensional)",
            "dist_params['ps'] = list(ps.data.cpu().numpy())"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('identifier', 'Variable'), position=0, insert_id=759091)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=759092)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=759093)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=759094)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2883,
        "neg_line": [
            "-logits = torch.Tensor(dist_params.pop('logits'))"
        ],
        "pos_line": [
            "+logits = Variable(torch.Tensor(dist_params.pop('logits')))"
        ],
        "core_change": "-logits = torch.Tensor(dist_params.pop('logits')) +logits = Variable(torch.Tensor(dist_params.pop('logits')))",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "9d7abd0d8165e6f693d20133814591d9b3fac4eb",
        "index": "61bcb52c..c8986e08 100644",
        "commit_message": "geddataset fix\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GEDDataset(InMemoryDataset):",
            "xs += [assoc[x]]",
            "ys += [assoc[y]]",
            "gs += [g]",
            "-            x, y, g = torch.tensor(xs), torch.tensor(ys), torch.tensor(gs)",
            "+            x, y = torch.tensor(xs), torch.tensor(ys)",
            "+            g = torch.tensor(gs, dtype=torch.float)",
            "mat[x, y], mat[y, x] = g, g",
            "",
            "path = osp.join(self.processed_dir, '{}_ged.pt'.format(self.name))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=5, insert_id=1028903)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1028904)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'g'), position=0, insert_id=1028905)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1028906)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1028907)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1028908)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1028909)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1028910)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1028911)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1028912)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1028913)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float'), position=2, insert_id=1028914)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=g))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 2884,
        "neg_line": [
            "-x, y, g = torch.tensor(xs), torch.tensor(ys), torch.tensor(gs)"
        ],
        "pos_line": [
            "+x, y = torch.tensor(xs), torch.tensor(ys)",
            "+g = torch.tensor(gs, dtype=torch.float)"
        ],
        "core_change": "-x, y, g = torch.tensor(xs), torch.tensor(ys), torch.tensor(gs) +x, y = torch.tensor(xs), torch.tensor(ys) +g = torch.tensor(gs, dtype=torch.float)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "c8dc40a75cc4ff1f43c6ff9178d91c08155d7973",
        "index": "3db3d153..c9f18b51 100644",
        "commit_message": "Add automatic reparametrization strategies (#2884)\n\n* Sketch automatic reparametrization strategies\n\n* Clean up\n\n* Revise interface; get first tests to pass\n\n* Add an end-to-end smoke test\n\n* Add more tests, fix handling of ROHC\n\n* Simplify\n\n* Address review comments\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TransformReparam(Reparam):",
            "is_observed = msg[\"is_observed\"]",
            "",
            "fn, event_dim = self._unwrap(fn)",
            "-        assert isinstance(fn, dist.TransformedDistribution)",
            "+        assert isinstance(fn, torch.distributions.TransformedDistribution)",
            "",
            "# Differentiably invert transform.",
            "value_base = value"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=675347)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=675348)",
            "Update(target_node=ASTNode(type=identifier, text=dist), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=dist), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'distributions'), position=2, insert_id=675349)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 2886,
        "neg_line": [
            "-assert isinstance(fn, dist.TransformedDistribution)"
        ],
        "pos_line": [
            "+assert isinstance(fn, torch.distributions.TransformedDistribution)"
        ],
        "core_change": "-assert isinstance(fn, dist.TransformedDistribution) +assert isinstance(fn, torch.distributions.TransformedDistribution)",
        "core_API": "_unwrap"
    },
    {
        "commit_hash": "e24d9433b15969ee0bd75ccbb7f3f6b88eca4a41",
        "index": "c038ca6..42a8468 100644",
        "commit_message": "Add support for HuggingFace's TensorFlow models (#127)\n\n* added support for for HuggingFace's TensorFlow models\n\n* added notebook for HuggingFace's tensorflow bert model\n\n* change nebullvm name in logs\n\n* Add optimized model details + warning if static shape is used for HF models (#1)\n\n* add optimized model type info\n\n* fix tvm issue\n\n* edit dockerfile and add image auto building\n\n* add docker installation on azure pipeline\n\n* fix bug in neural compressor output shape\n\n* add support for openvino with python 3.10\n\n* add build docker image to azure pipelines\n\n* revert docker build from az pipelines and edit format of the optimization results\n\n* revert docker build from az pipelines\n\n* added tabulate to setup.py and general fixes\n",
        "file": "nebullvm.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def extract_info_from_torch_data(",
            "input_types = ifnone(",
            "input_types,",
            "[",
            "-            \"int\" if isinstance(x.cpu(), torch.LongTensor) else \"float\"",
            "+            \"int64\"",
            "+            if isinstance(x.cpu(), torch.LongTensor)",
            "+            else \"int32\"",
            "+            if isinstance(x.cpu(), torch.IntTensor)",
            "+            else \"float32\"",
            "for x in input_row",
            "],",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"int\"), value='\"int64\"')",
            "Insert(target_node=ASTNode(type=conditional_expression), node=('conditional_expression', None), position=4, insert_id=653118)",
            "Update(target_node=ASTNode(type=string, text=\"float\"), value='\"int32\"')",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=string, text=\"float\"), position=0)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=653119)",
            "Insert(target_node=IN(type=conditional_expression), node=('call', None), position=2, insert_id=653120)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=653121)",
            "Insert(target_node=IN(type=conditional_expression), node=('string', '\"float32\"'), position=4, insert_id=653122)",
            "Insert(target_node=IN(type=call), node=('identifier', 'isinstance'), position=0, insert_id=653123)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=653124)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=653125)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=653126)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=653127)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=653128)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=653129)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=653130)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=653131)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=653132)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=653133)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'IntTensor'), position=2, insert_id=653134)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'x'), position=0, insert_id=653135)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=653136)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cpu'), position=2, insert_id=653137)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=653138)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=653139)"
        ],
        "plus_line": 5,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 2888,
        "neg_line": [
            "-\"int\" if isinstance(x.cpu(), torch.LongTensor) else \"float\""
        ],
        "pos_line": [
            "+\"int64\"",
            "+if isinstance(x.cpu(), torch.LongTensor)",
            "+else \"int32\"",
            "+if isinstance(x.cpu(), torch.IntTensor)",
            "+else \"float32\""
        ],
        "core_change": "-\"int\" if isinstance(x.cpu(), torch.LongTensor) else \"float\" +\"int64\" +if isinstance(x.cpu(), torch.LongTensor) +else \"int32\" +if isinstance(x.cpu(), torch.IntTensor) +else \"float32\"",
        "core_API": "cpu"
    },
    {
        "commit_hash": "66d4b129903058365f6cf1d4fb4ac73814ca3bfc",
        "index": "b082451e..d13358b3 100644",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "\"def guide(data):\\n\",",
            "\"    with pyro.iarange('data'):\\n\",",
            "\"        p = softmax(pyro.param('unconstrained_p',\\n\",",
            "-    \"                               Variable(torch.zeros(len(data), K), requires_grad=True)))\\n\",",
            "+    \"                               torch.zeros(len(data), K, requires_grad=True)))\\n\",",
            "\"        pyro.sample('z', Categorical(p))\"",
            "]",
            "},"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"                               Variable(torch.zeros(len(data), K), requires_grad=True)))\\n\"), value='\"                               torch.zeros(len(data), K, requires_grad=True)))\\\\n\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2889,
        "neg_line": [
            "-\"                               Variable(torch.zeros(len(data), K), requires_grad=True)))\\n\","
        ],
        "pos_line": [
            "+\"                               torch.zeros(len(data), K, requires_grad=True)))\\n\","
        ],
        "core_change": "-\"                               Variable(torch.zeros(len(data), K), requires_grad=True)))\\n\", +\"                               torch.zeros(len(data), K, requires_grad=True)))\\n\",",
        "core_API": "iarange"
    },
    {
        "commit_hash": "0b9fdc69dfcc60c8851338b334d0f2e713aa9e9f",
        "index": "a2c4f57..ee698a6 100644",
        "commit_message": "fix error if num_microbatches = 1\n\n",
        "file": "gpt-neo.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def block(params, scope, past, append_dim, train=False):",
            "def model(features, labels, params, mesh, past=None):",
            "\"\"\"A GPT style model implemented in mesh tensorlfow.\"\"\"",
            "results = {}",
            "+    sequence_dim = mtf.Dimension('sequence', params[\"n_ctx\"])",
            "if params[\"num_microbatches\"] > 1:",
            "x = features[\"inputs\"]",
            "labels = features[\"labels\"]",
            "batch_dim = x.shape[0]",
            "-",
            "-",
            "else:",
            "+      batch_dim = mtf.Dimension('batch', params[\"train_batch_size\"])",
            "x = mtf.import_tf_tensor(mesh, features, mtf.Shape([batch_dim, sequence_dim]))",
            "# In this case, labels are simply input shifted one token to the right",
            "# this op is done in the input_fn",
            "# define mtf dims",
            "-      batch_dim = mtf.Dimension('batch', params[\"train_batch_size\"])",
            "labels = mtf.import_tf_tensor(mesh, labels, mtf.Shape([batch_dim, sequence_dim]))",
            "",
            "-    sequence_dim = mtf.Dimension('sequence', params[\"n_ctx\"])",
            "",
            "# we need this because gathering when both the args have the same dimension in them it breaks stuff.",
            "# this dim is specifically for the weights"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=7)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=module), position=4)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=else), position=0)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=:, text=:), position=1)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=type), position=2)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=x), position=0)",
            "Move(target_node=ASTNode(type=type), node=ASTNode(type=identifier, text=batch_dim), position=0)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 2891,
        "neg_line": [
            "-",
            "-",
            "-batch_dim = mtf.Dimension('batch', params[\"train_batch_size\"])",
            "-sequence_dim = mtf.Dimension('sequence', params[\"n_ctx\"])"
        ],
        "pos_line": [
            "+sequence_dim = mtf.Dimension('sequence', params[\"n_ctx\"])",
            "+batch_dim = mtf.Dimension('batch', params[\"train_batch_size\"])"
        ],
        "core_change": "+sequence_dim = mtf.Dimension('sequence', params[\"n_ctx\"]) - - +batch_dim = mtf.Dimension('batch', params[\"train_batch_size\"]) -batch_dim = mtf.Dimension('batch', params[\"train_batch_size\"]) -sequence_dim = mtf.Dimension('sequence', params[\"n_ctx\"])",
        "core_API": "Dimension"
    },
    {
        "commit_hash": "52decab371259bba640342e157f9836496557eed",
        "index": "f7fe21e04..2fd4256f6 100644",
        "commit_message": "fix test (#7947)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GPT2ModelTest(ModelTesterMixin, unittest.TestCase):",
            "",
            "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)",
            "",
            "-        torch.manual_seed(0)",
            "outputs = model.generate(",
            "input_ids=inputs[\"input_ids\"].to(torch_device),",
            "attention_mask=inputs[\"attention_mask\"].to(torch_device),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_seed))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 2892,
        "neg_line": [
            "-torch.manual_seed(0)"
        ],
        "pos_line": [],
        "core_change": "-torch.manual_seed(0)",
        "core_API": "manual_seed"
    },
    {
        "commit_hash": "87a12b41c0ea23525ae552bb3140f6944d70f077",
        "index": "5e28a5357..23e8c08c7 100644",
        "commit_message": "Fix conflict\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Conv2dSubsampling(torch.nn.Module):",
            "torch.nn.ReLU()",
            ")",
            "self.out = torch.nn.Sequential(",
            "-            torch.nn.Linear(odim * (idim // 4), odim),",
            "+            torch.nn.Linear(odim * (((idim - 1) // 2 - 1) // 2), odim),",
            "PositionalEncoding(odim, dropout_rate)",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('parenthesized_expression', None), position=0, insert_id=1341300)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('//', '//'), position=1, insert_id=1341301)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('integer', '2'), position=2, insert_id=1341302)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1341303)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('binary_operator', None), position=1, insert_id=1341304)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1341305)",
            "Insert(target_node=IN(type=binary_operator), node=('binary_operator', None), position=0, insert_id=1341306)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=1341307)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=2, insert_id=1341308)",
            "Insert(target_node=IN(type=binary_operator), node=('parenthesized_expression', None), position=0, insert_id=1341309)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=//, text=//), position=1)",
            "Update(target_node=ASTNode(type=integer, text=4), value='2')",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=integer, text=4), position=2)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('(', '('), position=0, insert_id=1341310)",
            "Insert(target_node=IN(type=parenthesized_expression), node=('binary_operator', None), position=1, insert_id=1341311)",
            "Insert(target_node=IN(type=parenthesized_expression), node=(')', ')'), position=2, insert_id=1341312)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=idim), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('-', '-'), position=1, insert_id=1341313)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '1'), position=2, insert_id=1341314)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 2893,
        "neg_line": [
            "-torch.nn.Linear(odim * (idim // 4), odim),"
        ],
        "pos_line": [
            "+torch.nn.Linear(odim * (((idim - 1) // 2 - 1) // 2), odim),"
        ],
        "core_change": "-torch.nn.Linear(odim * (idim // 4), odim), +torch.nn.Linear(odim * (((idim - 1) // 2 - 1) // 2), odim),",
        "core_API": "ReLU"
    },
    {
        "commit_hash": "4ac18961c3c85b54f0fb5c4f2ea82fc0a4471402",
        "index": "85232a83..da9ab00d 100644",
        "commit_message": "GH-2534: fix mypy for models\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class LanguageModel(nn.Module):",
            "",
            "text = prefix + \"\".join(characters)",
            "",
            "-            log_prob = log_prob.item()",
            "-            log_prob /= len(characters)",
            "+            log_prob_float = log_prob.item()",
            "+            log_prob_float /= len(characters)",
            "",
            "if not self.is_forward_lm:",
            "text = text[::-1]",
            "",
            "-            return text, log_prob",
            "+            return text, log_prob_float",
            "",
            "def calculate_perplexity(self, text: str) -> float:"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=log_prob), value='log_prob_float')",
            "Update(target_node=ASTNode(type=identifier, text=log_prob), value='log_prob_float')",
            "Update(target_node=ASTNode(type=identifier, text=log_prob), value='log_prob_float')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 3,
        "number": 2898,
        "neg_line": [
            "-log_prob = log_prob.item()",
            "-log_prob /= len(characters)",
            "-return text, log_prob"
        ],
        "pos_line": [
            "+log_prob_float = log_prob.item()",
            "+log_prob_float /= len(characters)",
            "+return text, log_prob_float"
        ],
        "core_change": "-log_prob = log_prob.item() -log_prob /= len(characters) +log_prob_float = log_prob.item() +log_prob_float /= len(characters) -return text, log_prob +return text, log_prob_float",
        "core_API": "item"
    },
    {
        "commit_hash": "2c364c0df885deead04d6c7cc66ab2643a784e82",
        "index": "38de84a9..e9fdc761 100644",
        "commit_message": "test fixes\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "optim",
        "change": [
            "class GlowTTSTrainTest(unittest.TestCase):",
            "assert (param - param_ref).sum() == 0, param",
            "count += 1",
            "",
            "-        optimizer = optim.Adam(model.parameters(), lr=c.lr)",
            "+        optimizer = optim.Adam(model.parameters(), lr=0.001)",
            "for _ in range(5):",
            "+            optimizer.zero_grad()",
            "z, logdet, y_mean, y_log_scale, alignments, o_dur_log, o_total_dur = model.forward(",
            "input_dummy, input_lengths, mel_spec, mel_lengths, None)",
            "-            optimizer.zero_grad()",
            "loss_dict = criterion(z, y_mean, y_log_scale, logdet, mel_lengths,",
            "o_dur_log, o_total_dur, input_lengths)",
            "loss = loss_dict['loss']"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=for_statement), node=('block', None), position=5, insert_id=1621985)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('float', '0.001'), position=2, insert_id=1621986)",
            "Delete(target_node=ASTNode(type=identifier, text=c))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=lr))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 2899,
        "neg_line": [
            "-optimizer = optim.Adam(model.parameters(), lr=c.lr)",
            "-optimizer.zero_grad()"
        ],
        "pos_line": [
            "+optimizer = optim.Adam(model.parameters(), lr=0.001)",
            "+optimizer.zero_grad()"
        ],
        "core_change": "-optimizer = optim.Adam(model.parameters(), lr=c.lr) +optimizer = optim.Adam(model.parameters(), lr=0.001) +optimizer.zero_grad() -optimizer.zero_grad()",
        "core_API": "Adam"
    },
    {
        "commit_hash": "d0358bb38e4a40d8faaa155900ef7859c9b867b5",
        "index": "03601f69..545357eb 100644",
        "commit_message": "fix inconsistency w/ recent pytorch cuda device logic\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/892\n\nDifferential Revision: D18109685\n\nPulled By: jma127\n\nfbshipit-source-id: f96e1080a5577b8ee0748dfdd956bf72bed47474\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class Trainer(object):",
            "print(msg, file=sys.stderr)",
            "if torch.cuda.is_available() and hasattr(torch.cuda, \"memory_summary\"):",
            "for device_idx in range(torch.cuda.device_count()):",
            "-                            print(torch.cuda.memory_summary(device=torch.cuda.device(device_idx)),",
            "+                            print(torch.cuda.memory_summary(device=device_idx),",
            "file=sys.stderr)",
            "sys.stderr.flush()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=keyword_argument), node=ASTNode(type=identifier, text=device_idx), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 2902,
        "neg_line": [
            "-print(torch.cuda.memory_summary(device=torch.cuda.device(device_idx)),"
        ],
        "pos_line": [
            "+print(torch.cuda.memory_summary(device=device_idx),"
        ],
        "core_change": "-print(torch.cuda.memory_summary(device=torch.cuda.device(device_idx)), +print(torch.cuda.memory_summary(device=device_idx),",
        "core_API": "is_available"
    },
    {
        "commit_hash": "c7fd1d9fb18862318b133c9474d37c5085850070",
        "index": "c49b008a..39eab7e4 100644",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def rpn_losses(anchor_labels, anchor_boxes, label_logits, box_logits):",
            "# But the total RPN loss will be fine.  TODO make the summary op smarter",
            "placeholder = 0.",
            "label_loss = tf.nn.sigmoid_cross_entropy_with_logits(",
            "-        labels=tf.to_float(valid_anchor_labels), logits=valid_label_logits)",
            "+        labels=tf.cast(valid_anchor_labels, tf.float32), logits=valid_label_logits)",
            "label_loss = tf.reduce_sum(label_loss) * (1. / cfg.RPN.BATCH_PER_IM)",
            "label_loss = tf.where(tf.equal(nr_valid, 0), placeholder, label_loss, name='label_loss')"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=to_float), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2278893)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2278894)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2278895)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2278896)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2278897)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 2904,
        "neg_line": [
            "-labels=tf.to_float(valid_anchor_labels), logits=valid_label_logits)"
        ],
        "pos_line": [
            "+labels=tf.cast(valid_anchor_labels, tf.float32), logits=valid_label_logits)"
        ],
        "core_change": "-labels=tf.to_float(valid_anchor_labels), logits=valid_label_logits) +labels=tf.cast(valid_anchor_labels, tf.float32), logits=valid_label_logits)",
        "core_API": "sigmoid_cross_entropy_with_logits"
    },
    {
        "commit_hash": "e76256affe85aa50c790d46db3bf15f3eff68304",
        "index": "031165b7..c6d3d8bc 100644",
        "commit_message": "Major renaming of agent/model arguments, various memory-related fixes and improvements\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Iterative(Solver):",
            "",
            "# Initialization step",
            "args = self.initialize(x_init, *args)",
            "+        # args = util.map_tensors(fn=tf.stop_gradient, tensors=args)",
            "",
            "# Iteration loop with termination condition",
            "if self.unroll_loop:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 2907,
        "neg_line": [],
        "pos_line": [
            "+# args = util.map_tensors(fn=tf.stop_gradient, tensors=args)"
        ],
        "core_change": "+# args = util.map_tensors(fn=tf.stop_gradient, tensors=args)",
        "core_API": "initialize"
    },
    {
        "commit_hash": "3ceee79920df762961cd58a6b600c50fb0639722",
        "index": "c81b77b34..c629b38f4 100644",
        "commit_message": "Fix args\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class DynamicConvolution(nn.Module):",
            "#                 Linear",
            "self.linear1 = nn.Linear(n_feat, n_feat * 2)",
            "self.linear2 = nn.Linear(n_feat, n_feat)",
            "-        self.linear_weight = nn.Linear(n_feat, self.wshare * 1 * self.kernel_size)",
            "+        self.linear_weight = nn.Linear(n_feat, self.wshare * 1 * kernel_size)",
            "nn.init.xavier_uniform(self.linear_weight.weight)",
            "self.act = nn.GLU()"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=identifier, text=kernel_size), position=2)",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 2908,
        "neg_line": [
            "-self.linear_weight = nn.Linear(n_feat, self.wshare * 1 * self.kernel_size)"
        ],
        "pos_line": [
            "+self.linear_weight = nn.Linear(n_feat, self.wshare * 1 * kernel_size)"
        ],
        "core_change": "-self.linear_weight = nn.Linear(n_feat, self.wshare * 1 * self.kernel_size) +self.linear_weight = nn.Linear(n_feat, self.wshare * 1 * kernel_size)",
        "core_API": "Linear"
    },
    {
        "commit_hash": "6ba2231d7227825dfb76e9df161824d04234e69f",
        "index": "b8595378..82fd037c 100644",
        "commit_message": "Reproducibility 3/3 (#1924)\n\n* make tests deterministic\n\n* run slow tests\n\n* prepare for testing\n\n* finish\n\n* refactor\n\n* add print statements\n\n* finish more\n\n* correct some test failures\n\n* more fixes\n\n* set up to correct tests\n\n* more corrections\n\n* up\n\n* fix more\n\n* more prints\n\n* add\n\n* up\n\n* up\n\n* up\n\n* uP\n\n* uP\n\n* more fixes\n\n* uP\n\n* up\n\n* up\n\n* up\n\n* up\n\n* fix more\n\n* up\n\n* up\n\n* clean tests\n\n* up\n\n* up\n\n* up\n\n* more fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* make\n\n* correct\n\n* finish\n\n* finish\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class VersatileDiffusionDualGuidedPipelineIntegrationTests(unittest.TestCase):",
            "image_slice = image[0, 253:256, 253:256, -1]",
            "",
            "assert image.shape == (1, 512, 512, 3)",
            "-        expected_slice = np.array([0.014, 0.0112, 0.0136, 0.0145, 0.0107, 0.0113, 0.0272, 0.0215, 0.0216])",
            "+        expected_slice = np.array([0.0787, 0.0849, 0.0826, 0.0812, 0.0807, 0.0795, 0.0818, 0.0798, 0.0779])",
            "+",
            "assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=0.014), value='0.0787')",
            "Update(target_node=ASTNode(type=float, text=0.0112), value='0.0849')",
            "Update(target_node=ASTNode(type=float, text=0.0136), value='0.0826')",
            "Update(target_node=ASTNode(type=float, text=0.0145), value='0.0812')",
            "Update(target_node=ASTNode(type=float, text=0.0107), value='0.0807')",
            "Update(target_node=ASTNode(type=float, text=0.0113), value='0.0795')",
            "Update(target_node=ASTNode(type=float, text=0.0272), value='0.0818')",
            "Update(target_node=ASTNode(type=float, text=0.0215), value='0.0798')",
            "Update(target_node=ASTNode(type=float, text=0.0216), value='0.0779')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 2912,
        "neg_line": [
            "-expected_slice = np.array([0.014, 0.0112, 0.0136, 0.0145, 0.0107, 0.0113, 0.0272, 0.0215, 0.0216])"
        ],
        "pos_line": [
            "+expected_slice = np.array([0.0787, 0.0849, 0.0826, 0.0812, 0.0807, 0.0795, 0.0818, 0.0798, 0.0779])",
            "+"
        ],
        "core_change": "-expected_slice = np.array([0.014, 0.0112, 0.0136, 0.0145, 0.0107, 0.0113, 0.0272, 0.0215, 0.0216]) +expected_slice = np.array([0.0787, 0.0849, 0.0826, 0.0812, 0.0807, 0.0795, 0.0818, 0.0798, 0.0779]) +",
        "core_API": "array"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "4ffe37825..cfc9fbdf4 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario ako <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Hippocorpus(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(data_dir):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('hippocorpus', data_dir=...)` that includes files unzipped from the hippocorpus zip. Manual download instructions: {}\".format(",
            "-                    data_dir, self.manual_download_instructions",
            "-                )",
            "+                \"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('hippocorpus', data_dir=...)` that includes files unzipped from the hippocorpus zip. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', '\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'hippocorpus\\', data_dir=...)` that includes files unzipped from the hippocorpus zip. Manual download instructions: {self.manual_download_instructions}\"'), position=1, insert_id=1781598)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('hippocorpus', data_dir=...)` that includes files unzipped from the hippocorpus zip. Manual download instructions: {}\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=data_dir))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 2917,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('hippocorpus', data_dir=...)` that includes files unzipped from the hippocorpus zip. Manual download instructions: {}\".format(",
            "-data_dir, self.manual_download_instructions",
            "-)"
        ],
        "pos_line": [
            "+\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('hippocorpus', data_dir=...)` that includes files unzipped from the hippocorpus zip. Manual download instructions: {self.manual_download_instructions}\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('hippocorpus', data_dir=...)` that includes files unzipped from the hippocorpus zip. Manual download instructions: {}\".format( -data_dir, self.manual_download_instructions -) +\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('hippocorpus', data_dir=...)` that includes files unzipped from the hippocorpus zip. Manual download instructions: {self.manual_download_instructions}\"",
        "core_API": "exists"
    },
    {
        "commit_hash": "b325e54b5a0310a2c2d89ca6ebc5ba2033881975",
        "index": "2afd77f1..eac45960 100644",
        "commit_message": "Fix a tiny bug when rendering gp tutorial (#1112)\n\n* fix links\n\n* fix vae image render\n\n* missing inducing output at gp tutorial\n\n* forget make scrub\n\n* Change some links from .ipynb to .html\n\n* Replace .html with .ipynb in links\n\n* fix tutorials\n\n* fix rendering gp tutor\n\n* fix rendering gp\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "\"metadata\": {},",
            "\"outputs\": [],",
            "\"source\": [",
            "-    \"kernel = gp.kernels.RBF(input_dim=1, variance=torch.tensor(5.), lengthscale=torch.tensor(10.))\\n\",",
            "+    \"kernel = gp.kernels.RBF(input_dim=1, variance=torch.tensor(5.),\\n\",",
            "+    \"                        lengthscale=torch.tensor(10.))\\n\",",
            "\"gpr = gp.models.GPRegression(X, y, kernel, noise=torch.tensor(1.))\"",
            "]",
            "},"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"kernel = gp.kernels.RBF(input_dim=1, variance=torch.tensor(5.), lengthscale=torch.tensor(10.))\\n\"), value='\"kernel = gp.kernels.RBF(input_dim=1, variance=torch.tensor(5.),\\\\n\"')",
            "Insert(target_node=ASTNode(type=subscript), node=('string', '\"                        lengthscale=torch.tensor(10.))\\\\n\"'), position=5, insert_id=744177)",
            "Insert(target_node=ASTNode(type=subscript), node=(',', ','), position=6, insert_id=744178)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 2918,
        "neg_line": [
            "-\"kernel = gp.kernels.RBF(input_dim=1, variance=torch.tensor(5.), lengthscale=torch.tensor(10.))\\n\","
        ],
        "pos_line": [
            "+\"kernel = gp.kernels.RBF(input_dim=1, variance=torch.tensor(5.),\\n\",",
            "+\"                        lengthscale=torch.tensor(10.))\\n\","
        ],
        "core_change": "-\"kernel = gp.kernels.RBF(input_dim=1, variance=torch.tensor(5.), lengthscale=torch.tensor(10.))\\n\", +\"kernel = gp.kernels.RBF(input_dim=1, variance=torch.tensor(5.),\\n\", +\"                        lengthscale=torch.tensor(10.))\\n\",",
        "core_API": "RBF"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "cd9cc250..cb696f50 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Embedding(TokenEmbedder):",
            "if weight is None:",
            "weight = torch.FloatTensor(num_embeddings, embedding_dim)",
            "self.weight = torch.nn.Parameter(weight, requires_grad=trainable)",
            "-            torch.nn.init.xavier_uniform(self.weight.data)",
            "+            torch.nn.init.xavier_uniform_(self.weight)",
            "else:",
            "if weight.size() != (num_embeddings, embedding_dim):",
            "raise ConfigurationError(\"A weight matrix was passed with contradictory embedding shapes.\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=xavier_uniform), value='xavier_uniform_')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=data))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2920,
        "neg_line": [
            "-torch.nn.init.xavier_uniform(self.weight.data)"
        ],
        "pos_line": [
            "+torch.nn.init.xavier_uniform_(self.weight)"
        ],
        "core_change": "-torch.nn.init.xavier_uniform(self.weight.data) +torch.nn.init.xavier_uniform_(self.weight)",
        "core_API": "FloatTensor"
    },
    {
        "commit_hash": "dfc00b919a1f8e91bb97e18a121cc34aa251b6c0",
        "index": "27da552..5d81c40 100644",
        "commit_message": "bugfix & optional tensorflow implementation (#96)\n\n* bugfix & optional tensorflow implementation\n\n* removed torch max version\n\n* bugfix compressor\n\n* bugfix metric\n\nCo-authored-by: Valerio Sofi <v.sofi@nebuly.ai>\n",
        "file": "nebullvm.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class HalfPrecisionTransformation(BaseTransformation):",
            "if _input.dtype == torch.float32",
            "else _input",
            ")",
            "-        elif isinstance(_input, tf.Tensor):",
            "+        elif isinstance(_input, tf.Tensor) and _input is not None:",
            "return (",
            "self._transform_tf(_input)",
            "if _input.dtype == tf.float32"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=2, insert_id=2123891)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=2123892)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=2, insert_id=2123893)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', '_input'), position=0, insert_id=2123894)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=2123895)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=2123896)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=2123897)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2921,
        "neg_line": [
            "-elif isinstance(_input, tf.Tensor):"
        ],
        "pos_line": [
            "+elif isinstance(_input, tf.Tensor) and _input is not None:"
        ],
        "core_change": "-elif isinstance(_input, tf.Tensor): +elif isinstance(_input, tf.Tensor) and _input is not None:",
        "core_API": "_transform_tf"
    },
    {
        "commit_hash": "1666c42f0bd65236d0df725829cfe93fad1ce55e",
        "index": "abd5199ed..405d74603 100644",
        "commit_message": "[`bnb`] Let's make the daily CI green   (#21597)\n\n* fix bnb slow test\n\n* make fixup\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MixedInt8Test(BaseMixedInt8Test):",
            "super().setUp()",
            "",
            "# Models and tokenizer",
            "-        self.model_fp16 = AutoModelForCausalLM.from_pretrained(self.model_name, torch_dtype=\"auto\", device_map=\"auto\")",
            "+        self.model_fp16 = AutoModelForCausalLM.from_pretrained(",
            "+            self.model_name, torch_dtype=torch.float16, device_map=\"auto\"",
            "+        )",
            "self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map=\"auto\")",
            "",
            "def tearDown(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('attribute', None), position=2, insert_id=1176361)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1176362)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1176363)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float16'), position=2, insert_id=1176364)",
            "Delete(target_node=ASTNode(type=string, text=\"auto\"))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2923,
        "neg_line": [
            "-self.model_fp16 = AutoModelForCausalLM.from_pretrained(self.model_name, torch_dtype=\"auto\", device_map=\"auto\")"
        ],
        "pos_line": [
            "+self.model_fp16 = AutoModelForCausalLM.from_pretrained(",
            "+self.model_name, torch_dtype=torch.float16, device_map=\"auto\"",
            "+)"
        ],
        "core_change": "-self.model_fp16 = AutoModelForCausalLM.from_pretrained(self.model_name, torch_dtype=\"auto\", device_map=\"auto\") +self.model_fp16 = AutoModelForCausalLM.from_pretrained( +self.model_name, torch_dtype=torch.float16, device_map=\"auto\" +)",
        "core_API": "from_pretrained"
    },
    {
        "commit_hash": "c1a7e32416a04062cb3058ea37b6b30624a2cd76",
        "index": "517f73d6..eb17859e 100644",
        "commit_message": "fixed input tensor\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "class _EdgewiseSplineGcn_gpu(Function):",
            "features_grad_in = features_grad_out.new(e,M_in)",
            "weight_grad_in = features_grad_out.new(K, M_in, M_out)",
            "n = features_grad_in.numel()*self.k",
            "-        with torch.cuda.device_of(input):",
            "+        with torch.cuda.device_of(features_grad_out):",
            "f = load_kernel('bspline_basis_backward_kernel', _bspline_basis_backward_kernel, Dtype=Dtype(input),",
            "num_edges=e,num_threads=n, M_in=M_in, M_out=M_out, k_max=self.k, K=K)",
            "f(block=(CUDA_NUM_THREADS, 1, 1),"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=input), value='features_grad_out')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2925,
        "neg_line": [
            "-with torch.cuda.device_of(input):"
        ],
        "pos_line": [
            "+with torch.cuda.device_of(features_grad_out):"
        ],
        "core_change": "-with torch.cuda.device_of(input): +with torch.cuda.device_of(features_grad_out):",
        "core_API": "new"
    },
    {
        "commit_hash": "ced7c9601affad9fb24f7dce3311a424bbd6cdb8",
        "index": "f09172be..8b855a5e 100644",
        "commit_message": "fix upcast in slice attention (#1591)\n\n* fix upcast in slice attention\n\n* fix dtype\n\n* add test\n\n* fix test\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CrossAttention(nn.Module):",
            "key_slice = key_slice.float()",
            "",
            "attn_slice = torch.baddbmm(",
            "-                torch.empty(slice_size, query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),",
            "-                query[start_idx:end_idx],",
            "-                key[start_idx:end_idx].transpose(-1, -2),",
            "+                torch.empty(slice_size, query.shape[1], key.shape[1], dtype=query_slice.dtype, device=query.device),",
            "+                query_slice,",
            "+                key_slice.transpose(-1, -2),",
            "beta=0,",
            "alpha=self.scale,",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=query), value='query_slice')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=query), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=key), value='key_slice')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=key), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=query), value='query_slice')",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=start_idx))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=end_idx))",
            "Delete(target_node=ASTNode(type=slice))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=start_idx))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=end_idx))",
            "Delete(target_node=ASTNode(type=slice))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 19,
        "number": 2926,
        "neg_line": [
            "-torch.empty(slice_size, query.shape[1], key.shape[1], dtype=query.dtype, device=query.device),",
            "-query[start_idx:end_idx],",
            "-key[start_idx:end_idx].transpose(-1, -2),"
        ],
        "pos_line": [
            "+torch.empty(slice_size, query.shape[1], key.shape[1], dtype=query_slice.dtype, device=query.device),",
            "+query_slice,",
            "+key_slice.transpose(-1, -2),"
        ],
        "core_change": "-torch.empty(slice_size, query.shape[1], key.shape[1], dtype=query.dtype, device=query.device), -query[start_idx:end_idx], -key[start_idx:end_idx].transpose(-1, -2), +torch.empty(slice_size, query.shape[1], key.shape[1], dtype=query_slice.dtype, device=query.device), +query_slice, +key_slice.transpose(-1, -2),",
        "core_API": "float"
    },
    {
        "commit_hash": "2cdfbd18a750cabaff1499fad7473dd91f3c7fa7",
        "index": "9f4e1c4b..93d73486 100644",
        "commit_message": "Previously, many unit test files started with `enable_v2_behavior`, which would have caused them to run in V2 mode when executing with a V1 test flag. The correct behavior would in fact be to skip such tests when executing with a V1 test flag.\n\nThis fix significantly reduces the total V1 + V2 test load by eliminating redundancy.\n\nPiperOrigin-RevId: 424734850\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class StringLookupVocabularyTest(keras_parameterized.TestCase,",
            "fn()",
            "",
            "if __name__ == \"__main__\":",
            "-  # StringLookup is only exported as a TF2 API.",
            "-  tf.compat.v1.enable_v2_behavior()",
            "tf.test.main()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=compat))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=v1))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=enable_v2_behavior))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 2928,
        "neg_line": [
            "-# StringLookup is only exported as a TF2 API.",
            "-tf.compat.v1.enable_v2_behavior()"
        ],
        "pos_line": [],
        "core_change": "-# StringLookup is only exported as a TF2 API. -tf.compat.v1.enable_v2_behavior()",
        "core_API": "enable_v2_behavior"
    },
    {
        "commit_hash": "28dc9993ed6b1f434bf033d6773b0bf0b12088da",
        "index": "7276478b..27828013 100644",
        "commit_message": "PermutationFlow (#1492)\n\n* Separated NN tests from flow tests\n\n* PermutationFlow\n\n* Tests for PermutationFLow\n\n* Bug fix\n\n* Renamed PermutationFlow to PermuteTransform\n\n* Added PermuteTransform to docs\n\n* Added device to permutation vectors\n\n* PEP8\n\n* Removed 'flow', link to IAF in docs, fixed other bug in docs\n\n* Removed more 'flow's\n\n* Added lazy_property to inv_permutation of PermuteTransform\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TorchDistribution(torch.distributions.Distribution, TorchDistributionMixin",
            "assert d.shape(sample_shape) == sample_shape + d.batch_shape + d.event_shape",
            "",
            "Distributions provide a vectorized",
            "-    :meth`~torch.distributions.distribution.Distribution.log_prob` method that",
            "+    :meth:`~torch.distributions.distribution.Distribution.log_prob` method that",
            "evaluates the log probability density of each event in a batch",
            "independently, returning a tensor of shape",
            "``sample_shape + d.batch_shape``::"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=(':', ':'), position=7, insert_id=732639)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2934,
        "neg_line": [
            "-:meth`~torch.distributions.distribution.Distribution.log_prob` method that"
        ],
        "pos_line": [
            "+:meth:`~torch.distributions.distribution.Distribution.log_prob` method that"
        ],
        "core_change": "-:meth`~torch.distributions.distribution.Distribution.log_prob` method that +:meth:`~torch.distributions.distribution.Distribution.log_prob` method that",
        "core_API": "shape"
    },
    {
        "commit_hash": "f3128c87881a5442388358ca40d2ed868b04b358",
        "index": "9d40331e..2ee69d62 100644",
        "commit_message": "Actually fix the grad ckpt test (#734)\n\n* use_deterministic_algorithms  for grad ckpt test\n\n* remove eval\n\n* Apply suggestions from code review\n\n* Update tests/test_models_unet.py\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class UNet2DConditionModelTests(ModelTesterMixin, unittest.TestCase):",
            "for name in grad_checkpointed:",
            "self.assertTrue(torch.allclose(grad_checkpointed[name], grad_not_checkpointed[name], atol=5e-5))",
            "",
            "+        # disable deterministic behavior for gradient checkpointing",
            "+        del os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]",
            "+        torch.use_deterministic_algorithms(False)",
            "+",
            "",
            "#    TODO(Patrick) - Re-add this test after having cleaned up LDM",
            "#    def test_output_pretrained_spatial_transformer(self):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('delete_statement', None), position=3, insert_id=103143)",
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=103144)",
            "Insert(target_node=IN(type=delete_statement), node=('del', 'del'), position=0, insert_id=103145)",
            "Insert(target_node=IN(type=delete_statement), node=('subscript', None), position=1, insert_id=103146)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=103147)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=0, insert_id=103148)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=103149)",
            "Insert(target_node=IN(type=subscript), node=('string', '\"CUBLAS_WORKSPACE_CONFIG\"'), position=2, insert_id=103150)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=103151)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=103152)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=103153)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'os'), position=0, insert_id=103154)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=103155)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'environ'), position=2, insert_id=103156)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=103157)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=103158)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'use_deterministic_algorithms'), position=2, insert_id=103159)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=103160)",
            "Insert(target_node=IN(type=argument_list), node=('false', 'False'), position=1, insert_id=103161)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=103162)"
        ],
        "plus_line": 3,
        "minus_line": 0,
        "AST_diff_line": 20,
        "number": 2935,
        "neg_line": [],
        "pos_line": [
            "+# disable deterministic behavior for gradient checkpointing",
            "+del os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]",
            "+torch.use_deterministic_algorithms(False)",
            "+"
        ],
        "core_change": "+# disable deterministic behavior for gradient checkpointing +del os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] +torch.use_deterministic_algorithms(False) +",
        "core_API": "assertTrue"
    },
    {
        "commit_hash": "e6817ea2cacff41f2b11b5c148a8f220288fcf82",
        "index": "dd60f5a4..ac6734fd 100644",
        "commit_message": "Style fixes for enabling PEP8 501 (#10916)\n\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "def test_sequential_as_downstream_of_masking_layer():",
            "np.random.random((10, 3, 5)), epochs=1, batch_size=6)",
            "",
            "mask_outputs = [model.layers[1].compute_mask(model.layers[1].input)]",
            "-    mask_outputs += [model.layers[2].compute_mask(model.layers[2].input, mask_outputs[-1])]",
            "+    mask_outputs += [model.layers[2].compute_mask(model.layers[2].input,",
            "+                                                  mask_outputs[-1])]",
            "func = K.function([model.input], mask_outputs)",
            "mask_outputs_val = func([model_input])",
            "assert np.array_equal(mask_outputs_val[0], np.any(model_input, axis=-1))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2937,
        "neg_line": [
            "-mask_outputs += [model.layers[2].compute_mask(model.layers[2].input, mask_outputs[-1])]"
        ],
        "pos_line": [
            "+mask_outputs += [model.layers[2].compute_mask(model.layers[2].input,",
            "+mask_outputs[-1])]"
        ],
        "core_change": "-mask_outputs += [model.layers[2].compute_mask(model.layers[2].input, mask_outputs[-1])] +mask_outputs += [model.layers[2].compute_mask(model.layers[2].input, +mask_outputs[-1])]",
        "core_API": "random"
    },
    {
        "commit_hash": "c402309470740e17973eeee22a7b90123f85b310",
        "index": "5a0853631..70404f32f 100644",
        "commit_message": "fix CI error\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_fastspeech2(",
            "with torch.no_grad():",
            "model.eval()",
            "",
            "-        inputs = dict(",
            "-            text=torch.randint(0, 10, (2,)),",
            "-        )",
            "+        inputs = dict(text=torch.randint(0, 10, (2,)))",
            "if use_gst:",
            "inputs.update(speech=torch.randn(5, 5))",
            "if spk_embed_dim is not None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 1,
        "number": 2938,
        "neg_line": [
            "-inputs = dict(",
            "-text=torch.randint(0, 10, (2,)),",
            "-)"
        ],
        "pos_line": [
            "+inputs = dict(text=torch.randint(0, 10, (2,)))"
        ],
        "core_change": "-inputs = dict( -text=torch.randint(0, 10, (2,)), -) +inputs = dict(text=torch.randint(0, 10, (2,)))",
        "core_API": "no_grad"
    },
    {
        "commit_hash": "0eca74fa5f7bf82f3b93e3e38dd1d84cfedc5630",
        "index": "b17725b..e734044 100644",
        "commit_message": "lint fixes\n\nSummary:\nRan the linter.\nTODO: need to update the linter as per D21353065.\n\nReviewed By: bottler\n\nDifferential Revision: D21362270\n\nfbshipit-source-id: ad0e781de0a29f565ad25c43bc94a19b1828c020\n\n",
        "file": "pytorch3d.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Meshes(object):",
            "return",
            "",
            "if self.isempty():",
            "-            self._edges_packed = -torch.ones(",
            "-                (0, 2), dtype=torch.int64, device=self.device",
            "+            self._edges_packed = torch.full(",
            "+                (0, 2), fill_value=-1, dtype=torch.int64, device=self.device",
            ")",
            "self._edges_packed_to_mesh_idx = torch.zeros(",
            "(0,), dtype=torch.int64, device=self.device"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=ones), value='full')",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=925703)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=925704)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'fill_value'), position=0, insert_id=925705)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=925706)",
            "Insert(target_node=IN(type=keyword_argument), node=('unary_operator', '-1'), position=2, insert_id=925707)",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=unary_operator))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 2943,
        "neg_line": [
            "-self._edges_packed = -torch.ones(",
            "-(0, 2), dtype=torch.int64, device=self.device"
        ],
        "pos_line": [
            "+self._edges_packed = torch.full(",
            "+(0, 2), fill_value=-1, dtype=torch.int64, device=self.device"
        ],
        "core_change": "-self._edges_packed = -torch.ones( -(0, 2), dtype=torch.int64, device=self.device +self._edges_packed = torch.full( +(0, 2), fill_value=-1, dtype=torch.int64, device=self.device",
        "core_API": "isempty"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "0f6e4a3f0..1259a3899 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario ako <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Wikihow(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('wikihow', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format(",
            "-                    path_to_manual_file, self.config.filename, self.manual_download_instructions",
            "-                )",
            "+                f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('wikihow', data_dir=...)` that includes a file name {self.config.filename}. Manual download instructions: {self.manual_download_instructions})\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'wikihow\\', data_dir=...)` that includes a file name {self.config.filename}. Manual download instructions: {self.manual_download_instructions})\"'), position=1, insert_id=1781664)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('wikihow', data_dir=...)` that includes a file name {}. Manual download instructions: {})\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=path_to_manual_file))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=config))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=filename))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 24,
        "number": 2946,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('wikihow', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format(",
            "-path_to_manual_file, self.config.filename, self.manual_download_instructions",
            "-)"
        ],
        "pos_line": [
            "+f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('wikihow', data_dir=...)` that includes a file name {self.config.filename}. Manual download instructions: {self.manual_download_instructions})\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('wikihow', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format( -path_to_manual_file, self.config.filename, self.manual_download_instructions -) +f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('wikihow', data_dir=...)` that includes a file name {self.config.filename}. Manual download instructions: {self.manual_download_instructions})\"",
        "core_API": "exists"
    },
    {
        "commit_hash": "75f23d5304cef26f844f14e0bbdd7d011d8980bf",
        "index": "54bb0e17..84726794 100644",
        "commit_message": "delaunay fixes in cases num_nodes < 3, resizing of images in PascalVOC and WILLOW\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class InMemoryDataset(Dataset):",
            "for key in keys:",
            "item = data_list[0][key]",
            "if torch.is_tensor(item):",
            "-                data[key] = torch.cat(",
            "-                    data[key], dim=data.__cat_dim__(key, data_list[0][key]))",
            "+                data[key] = torch.cat(data[key],",
            "+                                      dim=data.__cat_dim__(key, item))",
            "elif isinstance(item, int) or isinstance(item, float):",
            "data[key] = torch.tensor(data[key])"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=data_list), value='item')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=data_list), position=3)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=key))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 2948,
        "neg_line": [
            "-data[key] = torch.cat(",
            "-data[key], dim=data.__cat_dim__(key, data_list[0][key]))"
        ],
        "pos_line": [
            "+data[key] = torch.cat(data[key],",
            "+dim=data.__cat_dim__(key, item))"
        ],
        "core_change": "-data[key] = torch.cat( -data[key], dim=data.__cat_dim__(key, data_list[0][key])) +data[key] = torch.cat(data[key], +dim=data.__cat_dim__(key, item))",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "06af499000527002ce9eacb01e6bcb2b1d4dd235",
        "index": "f1afb9cc4c..9dc8bc4502 100644",
        "commit_message": "fix small formatting error in functional/backends/tensorflow/experimental/elementwise\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def frexp(",
            "x: Union[tf.Tensor, tf.Variable],",
            "/,",
            "*,",
            "-    out: Optional[Union[Tuple[tf.Tensor, tf.Tensor], Tuple[tf.Variable, tf.Variable]]] = None,",
            "+    out: Optional[",
            "+        Union[Tuple[tf.Tensor, tf.Tensor], Tuple[tf.Variable, tf.Variable]]",
            "+    ] = None,",
            ") -> Union[Tuple[tf.Tensor, tf.Tensor], Tuple[tf.Variable, tf.Variable]]:",
            "e = tf.math.floor(tf.math.log(tf.math.abs(x)) / tf.cast(tf.math.log(2.), x.dtype))",
            "e = tf.cast(e, x.dtype)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 2949,
        "neg_line": [
            "-out: Optional[Union[Tuple[tf.Tensor, tf.Tensor], Tuple[tf.Variable, tf.Variable]]] = None,"
        ],
        "pos_line": [
            "+out: Optional[",
            "+Union[Tuple[tf.Tensor, tf.Tensor], Tuple[tf.Variable, tf.Variable]]",
            "+] = None,"
        ],
        "core_change": "-out: Optional[Union[Tuple[tf.Tensor, tf.Tensor], Tuple[tf.Variable, tf.Variable]]] = None, +out: Optional[ +Union[Tuple[tf.Tensor, tf.Tensor], Tuple[tf.Variable, tf.Variable]] +] = None,",
        "core_API": "floor"
    },
    {
        "commit_hash": "d667351eac57da2b07a50c07482652103a7839ee",
        "index": "4f5d95a..9fdbe13 100644",
        "commit_message": "Tweak accuracy topk safety. Fix #807\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def accuracy(output, target, topk=(1,)):",
            "_, pred = output.topk(maxk, 1, True, True)",
            "pred = pred.t()",
            "correct = pred.eq(target.reshape(1, -1).expand_as(pred))",
            "-    return [",
            "-        correct[:k].reshape(-1).float().sum(0) * 100. / batch_size",
            "-        if k <= maxk else torch.tensor(100.) for k in topk",
            "-    ]",
            "+    return [correct[:min(k, maxk)].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=list_comprehension), node=ASTNode(type=binary_operator), position=1)",
            "Insert(target_node=ASTNode(type=slice), node=('call', None), position=1, insert_id=895708)",
            "Insert(target_node=IN(type=call), node=('identifier', 'min'), position=0, insert_id=895709)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=895710)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=895711)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=k), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=895712)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'maxk'), position=3, insert_id=895713)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=895714)",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=k))",
            "Delete(target_node=ASTNode(type=<=, text=<=))",
            "Delete(target_node=ASTNode(type=identifier, text=maxk))",
            "Delete(target_node=ASTNode(type=comparison_operator))",
            "Delete(target_node=ASTNode(type=else, text=else))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=float, text=100.))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=conditional_expression))"
        ],
        "plus_line": 1,
        "minus_line": 4,
        "AST_diff_line": 25,
        "number": 2952,
        "neg_line": [
            "-return [",
            "-correct[:k].reshape(-1).float().sum(0) * 100. / batch_size",
            "-if k <= maxk else torch.tensor(100.) for k in topk",
            "-]"
        ],
        "pos_line": [
            "+return [correct[:min(k, maxk)].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]"
        ],
        "core_change": "-return [ -correct[:k].reshape(-1).float().sum(0) * 100. / batch_size -if k <= maxk else torch.tensor(100.) for k in topk -] +return [correct[:min(k, maxk)].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]",
        "core_API": "topk"
    },
    {
        "commit_hash": "146524464add4aedb6517421f16dccb6f9a27951",
        "index": "586eb97..dc9611d 100644",
        "commit_message": "Fix model export for DocClassificationModel with dense features (#394)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/394\n\nDocClassificationTask supports dense features, but exporting a DocClassification model with dense features to caffe2 doesn't work. Also, exporting any model that uses both text features (word/char) and dense features together doesn't work. This diff fixes export for dense features.\n\nThere are two things that needed fixing:\n\n- exporter puts model inputs in a list like this: `[features, feature lengths]`. However, dense features don't go through the representation layer - so they need to be hardcoded to be at the end of model inputs (ugh). The order of features needs to be `[features, feature lengths, dense features]`\n- dummy_model_input for all other fields has two entries. dummy_model_input for dense_features has one entry\n\nIn the long run, exporter should be tightly coupled with the model itself - we should enforce that model inputs have the same order while training and exporting. But that requires a lot of refactoring.\n\nThis is a blocking feature for [on-device M-suggestions](https://fb.workplace.com/groups/323789728341681/permalink/326686971385290/). A public test for that is scheduled next week.\nPersonalization models in M-suggestions need dense features.\n\nReviewed By: snisarg, gardenia22\n\nDifferential Revision: D14473020\n\nfbshipit-source-id: e7e9eec5d3f3ac8fb017bb88d24af4b9f860e316\n\n",
        "file": "pytext.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FloatVectorField(Field):",
            ")",
            "self.dim_error_check = dim_error_check  # dims in data should match config",
            "self.dummy_model_input = torch.tensor(",
            "-            [[1.0] * dim], dtype=torch.float, device=\"cpu\"",
            "+            [[1.0] * dim, [1.0] * dim], dtype=torch.float, device=\"cpu\"",
            ")",
            "",
            "def _parse_vector(self, s):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=2, insert_id=886383)",
            "Insert(target_node=ASTNode(type=list), node=('binary_operator', None), position=3, insert_id=886384)",
            "Insert(target_node=ASTNode(type=list), node=(']', ']'), position=4, insert_id=886385)",
            "Insert(target_node=IN(type=binary_operator), node=('list', None), position=0, insert_id=886386)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=886387)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'dim'), position=2, insert_id=886388)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=886389)",
            "Insert(target_node=IN(type=list), node=('float', '1.0'), position=1, insert_id=886390)",
            "Move(target_node=IN(type=list), node=ASTNode(type=], text=]), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 2956,
        "neg_line": [
            "-[[1.0] * dim], dtype=torch.float, device=\"cpu\""
        ],
        "pos_line": [
            "+[[1.0] * dim, [1.0] * dim], dtype=torch.float, device=\"cpu\""
        ],
        "core_change": "-[[1.0] * dim], dtype=torch.float, device=\"cpu\" +[[1.0] * dim, [1.0] * dim], dtype=torch.float, device=\"cpu\"",
        "core_API": "tensor"
    },
    {
        "commit_hash": "3dbd7b0b33016ba8c492c786ad1ddcd053fc707c",
        "index": "cd3d5214..0d411d3e 100644",
        "commit_message": "batch fix if items are only sparsely available\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Data(object):",
            "@property",
            "def num_edges(self):",
            "for key, item in self('edge_index', 'edge_attr'):",
            "-            return item.size(self.cat_dim(key))",
            "+            return item.size(self.cat_dim(key, item))",
            "return None",
            "",
            "@property"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1067648)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'item'), position=3, insert_id=1067649)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 2958,
        "neg_line": [
            "-return item.size(self.cat_dim(key))"
        ],
        "pos_line": [
            "+return item.size(self.cat_dim(key, item))"
        ],
        "core_change": "-return item.size(self.cat_dim(key)) +return item.size(self.cat_dim(key, item))",
        "core_API": "size"
    },
    {
        "commit_hash": "786be2a3f8b8160640614638f2fddc11572e2bc8",
        "index": "98b4aaaef..71ec38513 100755",
        "commit_message": "Should be fixed finally\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "np.random.seed(args.seed)",
            "",
            "if args.backend == \"pytorch\":",
            "-        from espnet.lmpytorch.tts_pytorch import train",
            "+        from espnet.tts.pytorch.tts_pytorch import train",
            "train(args)",
            "else:",
            "raise NotImplementedError(\"Only pytorch is supported.\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=lmpytorch), value='tts')",
            "Insert(target_node=ASTNode(type=dotted_name), node=('.', '.'), position=3, insert_id=178814)",
            "Insert(target_node=ASTNode(type=dotted_name), node=('identifier', 'pytorch'), position=4, insert_id=178815)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 2959,
        "neg_line": [
            "-from espnet.lmpytorch.tts_pytorch import train"
        ],
        "pos_line": [
            "+from espnet.tts.pytorch.tts_pytorch import train"
        ],
        "core_change": "-from espnet.lmpytorch.tts_pytorch import train +from espnet.tts.pytorch.tts_pytorch import train",
        "core_API": "seed"
    },
    {
        "commit_hash": "09458e2666ff9d9f65d70d8037846151bfd9c7b4",
        "index": "b09763c5..6cfb2c8c 100644",
        "commit_message": "Fixes preprocessing templating functions in model.\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Preprocessor(object):",
            "self.summaries = list()",
            "",
            "def custom_getter(getter, name, registered=False, **kwargs):",
            "-            print(name)",
            "variable = getter(name=name, registered=True, **kwargs)",
            "if not registered:",
            "self.variables[name] = variable",
            "return variable",
            "",
            "-        self.explore = tf.make_template(",
            "+        self.process = tf.make_template(",
            "name_=(scope + '/process'),",
            "func_=self.tf_process,",
            "custom_getter_=custom_getter"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('block', ''), position=4, insert_id=2238064)",
            "Update(target_node=ASTNode(type=identifier, text=explore), value='process')",
            "Delete(target_node=ASTNode(type=identifier, text=print))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=name))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=block))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 2961,
        "neg_line": [
            "-print(name)",
            "-self.explore = tf.make_template("
        ],
        "pos_line": [
            "+self.process = tf.make_template("
        ],
        "core_change": "-print(name) -self.explore = tf.make_template( +self.process = tf.make_template(",
        "core_API": "make_template"
    },
    {
        "commit_hash": "754e0d3b63a64a06815ff425c07df391aef774c5",
        "index": "aea6575d..e3f78176 100644",
        "commit_message": "bug fix\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LJSpeechDataset(Dataset):",
            "linear = torch.FloatTensor(linear)",
            "mel = torch.FloatTensor(mel)",
            "mel_lengths = torch.LongTensor(mel_lengths)",
            "-            stop_targets = torch.FloatTensor(stop_targets)",
            "+            stop_targets = torch.FloatTensor(stop_targets).squeeze()",
            "",
            "return text, text_lenghts, linear, mel, mel_lengths, stop_targets, item_idxs[0]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1275192)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1275193)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1275194)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'squeeze'), position=2, insert_id=1275195)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1275196)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1275197)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 2962,
        "neg_line": [
            "-stop_targets = torch.FloatTensor(stop_targets)"
        ],
        "pos_line": [
            "+stop_targets = torch.FloatTensor(stop_targets).squeeze()"
        ],
        "core_change": "-stop_targets = torch.FloatTensor(stop_targets) +stop_targets = torch.FloatTensor(stop_targets).squeeze()",
        "core_API": "FloatTensor"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "930fe77c..5765a2bb 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class AutoLaplaceApproximation(AutoContinuous):",
            "H = hessian(loss, self.loc)",
            "cov = H.inverse()",
            "loc = self.loc",
            "-        scale_tril = cov.cholesky()",
            "+        scale_tril = torch.linalg.cholesky(cov)",
            "",
            "gaussian_guide = AutoMultivariateNormal(self.model)",
            "gaussian_guide._setup_prototype(*args, **kwargs)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=676937)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=676938)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=cov), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=676939)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'linalg'), position=2, insert_id=676940)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 2967,
        "neg_line": [
            "-scale_tril = cov.cholesky()"
        ],
        "pos_line": [
            "+scale_tril = torch.linalg.cholesky(cov)"
        ],
        "core_change": "-scale_tril = cov.cholesky() +scale_tril = torch.linalg.cholesky(cov)",
        "core_API": "inverse"
    },
    {
        "commit_hash": "06cc307b15fafdab194bc394d23fa113200c7f97",
        "index": "68e2d6b..347004c 100644",
        "commit_message": "fix bugs in WGAN, WGAN-GP, DRAGAN\n\noutput of discriminator is changed into linear output (logit value) from\nsigmoid output\n\n",
        "file": "tensorflow-generative-model-collections.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class WGAN_GP(object):",
            "alpha = tf.random_uniform(shape=self.inputs.get_shape(), minval=0.,maxval=1.)",
            "differences = G - self.inputs # This is different from MAGAN",
            "interpolates = self.inputs + (alpha * differences)",
            "-        D_inter,_,_=self.discriminator(interpolates, is_training=True, reuse=True)",
            "+        _,D_inter,_=self.discriminator(interpolates, is_training=True, reuse=True)",
            "gradients = tf.gradients(D_inter, [interpolates])[0]",
            "slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))",
            "gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=identifier, text=D_inter), node=ASTNode(type=pattern_list), position=1)",
            "Insert(target_node=ASTNode(type=pattern_list), node=('identifier', '_'), position=0, insert_id=2206378)",
            "Delete(target_node=ASTNode(type=identifier, text=_))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 2969,
        "neg_line": [
            "-D_inter,_,_=self.discriminator(interpolates, is_training=True, reuse=True)"
        ],
        "pos_line": [
            "+_,D_inter,_=self.discriminator(interpolates, is_training=True, reuse=True)"
        ],
        "core_change": "-D_inter,_,_=self.discriminator(interpolates, is_training=True, reuse=True) +_,D_inter,_=self.discriminator(interpolates, is_training=True, reuse=True)",
        "core_API": "random_uniform"
    },
    {
        "commit_hash": "6b292113725fd4523585432d1b89bf2484832ab3",
        "index": "4e148a184..a358d300c 100644",
        "commit_message": "Fixed bug: replaced bce_loss_with_logits with bce_loss (#7096)\n\n* Fixed bug: replaced bce_loss_with_logits with bec_loss\n\n* Fixed bug: removed sigmoid activation from forward pass\n\n* switched names for scores and logits\n\nCo-authored-by: Alexey Misev <amisev@fb.com>\nCo-authored-by: Alexey Misev <alexey@MacBook-Pro-Natalia.local>\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TransferLearningModel(pl.LightningModule):",
            "# 1. Forward pass:",
            "x, y = batch",
            "y_logits = self.forward(x)",
            "+        y_scores = torch.sigmoid(y_logits)",
            "y_true = y.view((-1, 1)).type_as(x)",
            "",
            "# 2. Compute loss",
            "self.log(\"val_loss\", self.loss(y_logits, y_true), prog_bar=True)",
            "",
            "# 3. Compute accuracy:",
            "-        self.log(\"val_acc\", self.valid_acc(y_logits, y_true.int()), prog_bar=True)",
            "+        self.log(\"val_acc\", self.valid_acc(y_scores, y_true.int()), prog_bar=True)",
            "",
            "def configure_optimizers(self):",
            "parameters = list(self.parameters())"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=535705)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=535706)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'y_scores'), position=0, insert_id=535707)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=535708)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=535709)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=535710)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=535711)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=535712)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=535713)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sigmoid'), position=2, insert_id=535714)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=535715)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'y_logits'), position=1, insert_id=535716)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=535717)",
            "Update(target_node=ASTNode(type=identifier, text=y_logits), value='y_scores')"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 2971,
        "neg_line": [
            "-self.log(\"val_acc\", self.valid_acc(y_logits, y_true.int()), prog_bar=True)"
        ],
        "pos_line": [
            "+y_scores = torch.sigmoid(y_logits)",
            "+self.log(\"val_acc\", self.valid_acc(y_scores, y_true.int()), prog_bar=True)"
        ],
        "core_change": "+y_scores = torch.sigmoid(y_logits) -self.log(\"val_acc\", self.valid_acc(y_logits, y_true.int()), prog_bar=True) +self.log(\"val_acc\", self.valid_acc(y_scores, y_true.int()), prog_bar=True)",
        "core_API": "forward"
    },
    {
        "commit_hash": "cdf58a4ec6d00d5ac05de0293c9cc51a45b12218",
        "index": "9838fc6e..4c42087b 100644",
        "commit_message": "fix BDDMPipeline\n\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BDDMPipeline(DiffusionPipeline):",
            "num_prediction_steps = len(self.noise_scheduler)",
            "for t in tqdm.tqdm(reversed(range(num_prediction_steps)), total=num_prediction_steps):",
            "# 1. predict noise residual",
            "-            with torch.no_grad():",
            "-                t = (torch.tensor(timestep_values[t]) * torch.ones((1, 1))).to(torch_device)",
            "-                residual = self.diffwave(audio, mel_spectrogram, t)",
            "+            ts = (torch.tensor(timestep_values[t]) * torch.ones((1, 1))).to(torch_device)",
            "+            residual = self.diffwave((audio, mel_spectrogram, ts))",
            "",
            "# 2. predict previous mean of audio x_t-1",
            "pred_prev_audio = self.noise_scheduler.step(residual, audio, t)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=IN(type=root), node=ASTNode(type=block), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=t), value='ts')",
            "Insert(target_node=ASTNode(type=argument_list), node=('tuple', None), position=1, insert_id=114078)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=2, insert_id=114079)",
            "Insert(target_node=IN(type=tuple), node=('(', '('), position=0, insert_id=114080)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=identifier, text=audio), position=1)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=,, text=,), position=2)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=identifier, text=mel_spectrogram), position=3)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=,, text=,), position=4)",
            "Update(target_node=ASTNode(type=identifier, text=t), value='ts')",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=identifier, text=t), position=5)",
            "Move(target_node=IN(type=tuple), node=ASTNode(type=), text=)), position=6)",
            "Delete(target_node=ASTNode(type=with, text=with))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=no_grad))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=with_item))",
            "Delete(target_node=ASTNode(type=with_clause))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=with_statement))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 25,
        "number": 2972,
        "neg_line": [
            "-with torch.no_grad():",
            "-t = (torch.tensor(timestep_values[t]) * torch.ones((1, 1))).to(torch_device)",
            "-residual = self.diffwave(audio, mel_spectrogram, t)"
        ],
        "pos_line": [
            "+ts = (torch.tensor(timestep_values[t]) * torch.ones((1, 1))).to(torch_device)",
            "+residual = self.diffwave((audio, mel_spectrogram, ts))"
        ],
        "core_change": "-with torch.no_grad(): -t = (torch.tensor(timestep_values[t]) * torch.ones((1, 1))).to(torch_device) -residual = self.diffwave(audio, mel_spectrogram, t) +ts = (torch.tensor(timestep_values[t]) * torch.ones((1, 1))).to(torch_device) +residual = self.diffwave((audio, mel_spectrogram, ts))",
        "core_API": "tqdm"
    },
    {
        "commit_hash": "e80ebd00e382112df78d92e9e299227fc12a4263",
        "index": "6f8460cc7c..47e1a03272 100644",
        "commit_message": "fix `fn_name` to `fn_tree` in `test_frontend_function` (#2991)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def test_tensorflow_negative(",
            "native_array_flags=native_array,",
            "fw=fw,",
            "frontend=\"tensorflow\",",
            "-        fn_name=\"negative\",",
            "+        fn_tree=\"negative\",",
            "x=np.asarray(x, dtype=input_dtype),",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=fn_name), value='fn_tree')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2973,
        "neg_line": [
            "-fn_name=\"negative\","
        ],
        "pos_line": [
            "+fn_tree=\"negative\","
        ],
        "core_change": "-fn_name=\"negative\", +fn_tree=\"negative\",",
        "core_API": "asarray"
    },
    {
        "commit_hash": "a4474abd8365939f64db1799a298fa7b5afe3588",
        "index": "e895968c..6865c902 100644",
        "commit_message": "tacotron parse output bug fix\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Decoder(nn.Module):",
            "self.attention = inputs.data.new(B, T).zero_()",
            "self.attention_cum = inputs.data.new(B, T).zero_()",
            "",
            "-    def _parse_outputs(self, outputs, stop_tokens, attentions):",
            "+    def _parse_outputs(self, outputs, attentions, stop_tokens):",
            "# Back to batch first",
            "attentions = torch.stack(attentions).transpose(0, 1)",
            "outputs = torch.stack(outputs).transpose(0, 1).contiguous()",
            "-        stop_tokens = torch.stack(stop_tokens).transpose(0, 1)",
            "-        return outputs, stop_tokens, attentions",
            "+        stop_tokens = torch.stack(stop_tokens).transpose(0, 1).squeeze(-1)",
            "+        return outputs, attentions, stop_tokens",
            "",
            "def decode(self,",
            "inputs,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=stop_tokens), value='attentions')",
            "Update(target_node=ASTNode(type=identifier, text=attentions), value='stop_tokens')",
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=1273318)",
            "Update(target_node=ASTNode(type=identifier, text=stop_tokens), value='attentions')",
            "Update(target_node=ASTNode(type=identifier, text=attentions), value='stop_tokens')",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1273319)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1273320)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1273321)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'squeeze'), position=2, insert_id=1273322)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1273323)",
            "Insert(target_node=IN(type=argument_list), node=('unary_operator', '-1'), position=1, insert_id=1273324)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1273325)"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 13,
        "number": 2975,
        "neg_line": [
            "-def _parse_outputs(self, outputs, stop_tokens, attentions):",
            "-stop_tokens = torch.stack(stop_tokens).transpose(0, 1)",
            "-return outputs, stop_tokens, attentions"
        ],
        "pos_line": [
            "+def _parse_outputs(self, outputs, attentions, stop_tokens):",
            "+stop_tokens = torch.stack(stop_tokens).transpose(0, 1).squeeze(-1)",
            "+return outputs, attentions, stop_tokens"
        ],
        "core_change": "-def _parse_outputs(self, outputs, stop_tokens, attentions): +def _parse_outputs(self, outputs, attentions, stop_tokens): -stop_tokens = torch.stack(stop_tokens).transpose(0, 1) -return outputs, stop_tokens, attentions +stop_tokens = torch.stack(stop_tokens).transpose(0, 1).squeeze(-1) +return outputs, attentions, stop_tokens",
        "core_API": "new"
    },
    {
        "commit_hash": "59e4d78fa0d92d877dc2cd89883c4c274c0142af",
        "index": "7a9bbb8b..0005c75d 100644",
        "commit_message": "fixed inference\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GAE(torch.nn.Module):",
            "data.val_pos_edge_index = torch.stack([r, c], dim=0)",
            "r, c = row[n_v:n_v + n_t], col[n_v:n_v + n_t]",
            "data.test_pos_edge_index = torch.stack([r, c], dim=0)",
            "+",
            "r, c = row[n_v + n_t:], col[n_v + n_t:]",
            "-        data.train_pos_edge_index = torch.stack([r, c], dim=0)",
            "+        edge_index = torch.stack([r, c], dim=0)",
            "+        data.train_pos_edge_index = to_undirected(edge_index)",
            "",
            "# Negative edges.",
            "num_nodes = data.num_nodes"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=6, insert_id=1055917)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1055918)",
            "Insert(target_node=ASTNode(type=assignment), node=('identifier', 'edge_index'), position=0, insert_id=1055919)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1055920)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=1055921)",
            "Insert(target_node=IN(type=call), node=('identifier', 'to_undirected'), position=0, insert_id=1055922)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1055923)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1055924)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'edge_index'), position=1, insert_id=1055925)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1055926)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 2978,
        "neg_line": [
            "-data.train_pos_edge_index = torch.stack([r, c], dim=0)"
        ],
        "pos_line": [
            "+",
            "+edge_index = torch.stack([r, c], dim=0)",
            "+data.train_pos_edge_index = to_undirected(edge_index)"
        ],
        "core_change": "+ -data.train_pos_edge_index = torch.stack([r, c], dim=0) +edge_index = torch.stack([r, c], dim=0) +data.train_pos_edge_index = to_undirected(edge_index)",
        "core_API": "stack"
    },
    {
        "commit_hash": "a3784522a8af8f285dda4bb3c90a458e12d8fa37",
        "index": "88751312..71530ba4 100644",
        "commit_message": "fix initial image in ddim\n\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LatentDiffusion(DiffusionPipeline):",
            "num_trained_timesteps = self.noise_scheduler.timesteps",
            "inference_step_times = range(0, num_trained_timesteps, num_trained_timesteps // num_inference_steps)",
            "",
            "-        image = self.noise_scheduler.sample_noise(",
            "+        image = torch.randn(",
            "(batch_size, self.unet.in_channels, self.unet.image_size, self.unet.image_size),",
            "device=torch_device,",
            "generator=generator,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=self), value='torch')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=self), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=noise_scheduler), value='randn')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=noise_scheduler), position=2)",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=sample_noise))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 2980,
        "neg_line": [
            "-image = self.noise_scheduler.sample_noise("
        ],
        "pos_line": [
            "+image = torch.randn("
        ],
        "core_change": "-image = self.noise_scheduler.sample_noise( +image = torch.randn(",
        "core_API": "sample_noise"
    },
    {
        "commit_hash": "a346dd6c4ad1c5df8bbcc7af1afd13fc491f17da",
        "index": "f19fabb6..b44a341e 100644",
        "commit_message": "Sift test fix (#341)\n\n* fix filter2d docs\n\n* speed-up patch local feature tests\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestSIFTDescriptor:",
            "assert_allclose(out, expected, atol=1e-3, rtol=1e-3)",
            "",
            "def test_gradcheck(self):",
            "-        batch_size, channels, height, width = 1, 1, 41, 41",
            "+        batch_size, channels, height, width = 1, 1, 13, 13",
            "patches = torch.rand(batch_size, channels, height, width)",
            "patches = utils.tensor_to_gradcheck_var(patches)  # to var",
            "-        assert gradcheck(sift_describe, (patches, 41),",
            "+        assert gradcheck(sift_describe, (patches, 13),",
            "raise_exception=True)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=41), value='13')",
            "Update(target_node=ASTNode(type=integer, text=41), value='13')",
            "Update(target_node=ASTNode(type=integer, text=41), value='13')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 2982,
        "neg_line": [
            "-batch_size, channels, height, width = 1, 1, 41, 41",
            "-assert gradcheck(sift_describe, (patches, 41),"
        ],
        "pos_line": [
            "+batch_size, channels, height, width = 1, 1, 13, 13",
            "+assert gradcheck(sift_describe, (patches, 13),"
        ],
        "core_change": "-batch_size, channels, height, width = 1, 1, 41, 41 +batch_size, channels, height, width = 1, 1, 13, 13 -assert gradcheck(sift_describe, (patches, 41), +assert gradcheck(sift_describe, (patches, 13),",
        "core_API": "rand"
    },
    {
        "commit_hash": "f203d6806afdc18cb2f3595dd5d792631607e183",
        "index": "4263b3ce6b..604b836080 100644",
        "commit_message": "fixed trace, svd and rint in numpy frontend and trace in frontend (#10442)\n\nThe sum on the return of the torch backend is supposed to be calculated on the main diagonal, set the dim to always be  `-1` \n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def trace(",
            "if len(x) == 0:",
            "return ivy.array([])",
            "ret = torch.diagonal(x, offset=offset, dim1=axis1, dim2=axis2)",
            "-    ret = torch.sum(ret)",
            "+    ret = torch.sum(ret, dim=-1)",
            "return ret"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=267699)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=267700)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dim'), position=0, insert_id=267701)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=267702)",
            "Insert(target_node=IN(type=keyword_argument), node=('unary_operator', '-1'), position=2, insert_id=267703)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 2983,
        "neg_line": [
            "-ret = torch.sum(ret)"
        ],
        "pos_line": [
            "+ret = torch.sum(ret, dim=-1)"
        ],
        "core_change": "-ret = torch.sum(ret) +ret = torch.sum(ret, dim=-1)",
        "core_API": "array"
    },
    {
        "commit_hash": "0d4e97e765cc40245dfba663e223f89f0b588343",
        "index": "460c844..f3b0f49 100644",
        "commit_message": "fix rnn dropout\n\n",
        "file": "text-classification-cnn-rnn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TextRNN(object):",
            "with tf.name_scope(\"score\"):",
            "# dropoutrelu",
            "fc = tf.layers.dense(last, self.config.hidden_dim, name='fc1')",
            "-            fc = tf.contrib.layers.dropout(fc,",
            "-                self.config.dropout_keep_prob)",
            "+            fc = tf.contrib.layers.dropout(fc, self.keep_prob)",
            "fc = tf.nn.relu(fc)",
            "",
            "# "
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=self), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=config), value='keep_prob')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=config), position=3)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=dropout_keep_prob))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 2984,
        "neg_line": [
            "-fc = tf.contrib.layers.dropout(fc,",
            "-self.config.dropout_keep_prob)"
        ],
        "pos_line": [
            "+fc = tf.contrib.layers.dropout(fc, self.keep_prob)"
        ],
        "core_change": "-fc = tf.contrib.layers.dropout(fc, -self.config.dropout_keep_prob) +fc = tf.contrib.layers.dropout(fc, self.keep_prob)",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "c51dc4f92755c67a83f3fc8a0bd6b3e64df199e4",
        "index": "ef04a24b2..cd6e6318e 100644",
        "commit_message": "[torch] remove deprecated uint8 in favor of bool (#21384)\n\n* uint8 -> bool\n\n* fix copies\n\n* style\n\n* update test modeling commen when checking attention buffers\n\n* style\n\n* use logical not on random mask instead of subtraction with 1\n\n* remove torch uint8\n\n* quality\n\n* remove modified modeling utils\n\n* Update based on review\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n\n---------\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class XSoftmax(torch.autograd.Function):",
            "g, self, r_mask, g.op(\"Constant\", value_t=torch.tensor(torch.finfo(self.type().dtype()).min))",
            ")",
            "output = softmax(g, output, dim)",
            "-        return masked_fill(g, output, r_mask, g.op(\"Constant\", value_t=torch.tensor(0, dtype=torch.uint8)))",
            "+        return masked_fill(g, output, r_mask, g.op(\"Constant\", value_t=torch.tensor(0, dtype=torch.bool)))",
            "",
            "",
            "# Copied from transformers.models.deberta.modeling_deberta.DropoutContext"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=uint8), value='bool')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 2986,
        "neg_line": [
            "-return masked_fill(g, output, r_mask, g.op(\"Constant\", value_t=torch.tensor(0, dtype=torch.uint8)))"
        ],
        "pos_line": [
            "+return masked_fill(g, output, r_mask, g.op(\"Constant\", value_t=torch.tensor(0, dtype=torch.bool)))"
        ],
        "core_change": "-return masked_fill(g, output, r_mask, g.op(\"Constant\", value_t=torch.tensor(0, dtype=torch.uint8))) +return masked_fill(g, output, r_mask, g.op(\"Constant\", value_t=torch.tensor(0, dtype=torch.bool)))",
        "core_API": "op"
    },
    {
        "commit_hash": "4087a241eb299d9bdf4687dc615f15c8d60b1bc9",
        "index": "6d40cb310a..ad85705eec 100644",
        "commit_message": "Fix submodule linear algerbra (#2013)\n\nLinear algebra submodule fix up\n\nCo-authored-by: Darshan-H-E <darshanjhe@gmail.com>\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def bitwise_invert(",
            "bitwise_invert.support_native_out = True",
            "",
            "",
            "-def isfinite(",
            "-    x: torch.Tensor, *, out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:",
            "+def isfinite(x: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "return torch.isfinite(x)",
            "",
            "",
            "-def isinf(",
            "-    x: torch.Tensor, *, out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:",
            "+def isinf(x: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "return torch.isinf(x)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 0,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 2987,
        "neg_line": [
            "-def isfinite(",
            "-x: torch.Tensor, *, out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:",
            "-def isinf(",
            "-x: torch.Tensor, *, out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:"
        ],
        "pos_line": [
            "+def isfinite(x: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "+def isinf(x: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:"
        ],
        "core_change": "-def isfinite( -x: torch.Tensor, *, out: Optional[torch.Tensor] = None -) -> torch.Tensor: +def isfinite(x: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor: -def isinf( -x: torch.Tensor, *, out: Optional[torch.Tensor] = None -) -> torch.Tensor: +def isinf(x: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
        "core_API": "isfinite"
    },
    {
        "commit_hash": "fec3112cbacf19cd211109b1bc0b33a502595aca",
        "index": "d11aa6bf..5b5118da 100644",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_gaussian_hmm_distribution(diag, sample_shape, batch_shape, num_steps, h",
            "actual_std = actual_cov.diagonal(dim1=-2, dim2=-1).sqrt()",
            "actual_corr = actual_cov / (actual_std.unsqueeze(-1) * actual_std.unsqueeze(-2))",
            "",
            "-            expected_cov = g.precision.cholesky().cholesky_inverse()",
            "+            expected_cov = torch.linalg.cholesky(g.precision).cholesky_inverse()",
            "expected_mean = expected_cov.matmul(g.info_vec.unsqueeze(-1)).squeeze(-1)",
            "expected_std = expected_cov.diagonal(dim1=-2, dim2=-1).sqrt()",
            "expected_corr = expected_cov / (expected_std.unsqueeze(-1) * expected_std.unsqueeze(-2))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=677077)",
            "Update(target_node=ASTNode(type=identifier, text=precision), value='linalg')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'g'), position=0, insert_id=677078)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=677079)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'precision'), position=2, insert_id=677080)",
            "Update(target_node=ASTNode(type=identifier, text=g), value='torch')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 2990,
        "neg_line": [
            "-expected_cov = g.precision.cholesky().cholesky_inverse()"
        ],
        "pos_line": [
            "+expected_cov = torch.linalg.cholesky(g.precision).cholesky_inverse()"
        ],
        "core_change": "-expected_cov = g.precision.cholesky().cholesky_inverse() +expected_cov = torch.linalg.cholesky(g.precision).cholesky_inverse()",
        "core_API": "diagonal"
    },
    {
        "commit_hash": "75c7405bf2de3a126c91428138ba7bfe9aed840a",
        "index": "efd09b60..5e010ad7 100644",
        "commit_message": "Implement more sophisticated import / export APIs (#2237)\n\n* feat(internal): implement more sophisticated export\n\n* feat(internal): add typename method to StoreItem\n\n* feat(internal): try to unnecessarily flush bento metadata less\n\n(also models)\n\n* fix(internal): clear up errors when creating bentos/models with 'from_fs'\n\n* Fix docstring formatting\n\nCo-authored-by: Chaoyu <paranoyang@gmail.com>\n",
        "file": "BentoML.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "def test_models(tmpdir: \"Path\"):",
            "export_path_2 = os.path.join(tmpdir, \"testmodel1\")",
            "bentoml.models.export_model(testmodel1tag, export_path_2, _model_store=store)",
            "bentoml.models.delete(testmodel1tag, _model_store=store)",
            "-    bentoml.models.import_model(export_path_2, _model_store=store)",
            "+    bentoml.models.import_model(export_path_2 + \".bentomodel\", _model_store=store)",
            "",
            "assert bentoml.models.get(\"testmodel\", _model_store=store).tag == testmodel2tag"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=1, insert_id=2650502)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=identifier, text=export_path_2), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=2650503)",
            "Insert(target_node=IN(type=binary_operator), node=('string', '\".bentomodel\"'), position=2, insert_id=2650504)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 2992,
        "neg_line": [
            "-bentoml.models.import_model(export_path_2, _model_store=store)"
        ],
        "pos_line": [
            "+bentoml.models.import_model(export_path_2 + \".bentomodel\", _model_store=store)"
        ],
        "core_change": "-bentoml.models.import_model(export_path_2, _model_store=store) +bentoml.models.import_model(export_path_2 + \".bentomodel\", _model_store=store)",
        "core_API": "join"
    },
    {
        "commit_hash": "44bfc28ea6cd3f97c384fc5b6bd76d022cfc7543",
        "index": "e75c20ca..40eab65c 100644",
        "commit_message": "Fix EmpiricalMarginal with multiple GPUs (#1762)\n\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EmpiricalMarginal(Empirical):",
            "in ``[0, num_chains - 1]``, and there must be equal number",
            "of samples per chain.",
            "\"\"\"",
            "-        weight_type = value.new_empty(1).float().type() if value.dtype in (torch.int32, torch.int64) \\",
            "-            else value.type()",
            "# Apply default weight of 1.0.",
            "if log_weight is None:",
            "-            log_weight = torch.tensor(0.0).type(weight_type)",
            "-        if isinstance(log_weight, numbers.Number):",
            "-            log_weight = torch.tensor(log_weight).type(weight_type)",
            "-        if self._validate_args and log_weight.dim() > 0:",
            "+            log_weight = 0.0",
            "+        if self._validate_args and not isinstance(log_weight, numbers.Number) and log_weight.dim() > 0:",
            "raise ValueError(\"``weight.dim() > 0``, but weight should be a scalar.\")",
            "",
            "# Append to the buffer list"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=escape_sequence, text=\\\n))"
        ],
        "plus_line": 2,
        "minus_line": 6,
        "AST_diff_line": 1,
        "number": 2995,
        "neg_line": [
            "-weight_type = value.new_empty(1).float().type() if value.dtype in (torch.int32, torch.int64) \\",
            "-else value.type()",
            "-log_weight = torch.tensor(0.0).type(weight_type)",
            "-if isinstance(log_weight, numbers.Number):",
            "-log_weight = torch.tensor(log_weight).type(weight_type)",
            "-if self._validate_args and log_weight.dim() > 0:"
        ],
        "pos_line": [
            "+log_weight = 0.0",
            "+if self._validate_args and not isinstance(log_weight, numbers.Number) and log_weight.dim() > 0:"
        ],
        "core_change": "-weight_type = value.new_empty(1).float().type() if value.dtype in (torch.int32, torch.int64) \\ -else value.type() -log_weight = torch.tensor(0.0).type(weight_type) -if isinstance(log_weight, numbers.Number): -log_weight = torch.tensor(log_weight).type(weight_type) -if self._validate_args and log_weight.dim() > 0: +log_weight = 0.0 +if self._validate_args and not isinstance(log_weight, numbers.Number) and log_weight.dim() > 0:",
        "core_API": "new_empty"
    },
    {
        "commit_hash": "d2a6f7958a5126c6234e3f47eb80578deb2fc5a4",
        "index": "f40707a014..bee27a8891 100644",
        "commit_message": "[tune] Fix for keras threading (#5517)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def set_keras_threads(threads):",
            "# We set threads here to avoid contention, as Keras",
            "# is heavily parallelized across multiple cores.",
            "K.set_session(",
            "-        K.tf.Session(",
            "-            config=K.tf.ConfigProto(",
            "+        tf.Session(",
            "+            config=tf.ConfigProto(",
            "intra_op_parallelism_threads=threads,",
            "inter_op_parallelism_threads=threads)))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=K))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=K))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 2999,
        "neg_line": [
            "-K.tf.Session(",
            "-config=K.tf.ConfigProto("
        ],
        "pos_line": [
            "+tf.Session(",
            "+config=tf.ConfigProto("
        ],
        "core_change": "-K.tf.Session( -config=K.tf.ConfigProto( +tf.Session( +config=tf.ConfigProto(",
        "core_API": "set_session"
    },
    {
        "commit_hash": "5cf72f4934f3104ac2378c8b9b3638afea38ba1e",
        "index": "0e7eb47f..1588d7c8 100644",
        "commit_message": "fix the rest\n\n",
        "file": "keras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DeterministicRandomTestToolTest(tf.test.TestCase):",
            "a_prime = tf.random.uniform(shape=(3, 1))",
            "a_prime = a_prime * 3",
            "error_string = \"An exception should have been raised before this\"",
            "-            error_raised = \"An exception should have been raised before this\"",
            "try:",
            "-                c = tf.random.uniform(shape=(3, 1))",
            "+                tf.random.uniform(shape=(3, 1))",
            "raise RuntimeError(error_string)",
            "",
            "except ValueError as err:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Delete(target_node=ASTNode(type=identifier, text=error_raised))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text=\"An exception should have been raised before this\"))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=expression_statement))",
            "Delete(target_node=ASTNode(type=identifier, text=c))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 3000,
        "neg_line": [
            "-error_raised = \"An exception should have been raised before this\"",
            "-c = tf.random.uniform(shape=(3, 1))"
        ],
        "pos_line": [
            "+tf.random.uniform(shape=(3, 1))"
        ],
        "core_change": "-error_raised = \"An exception should have been raised before this\" -c = tf.random.uniform(shape=(3, 1)) +tf.random.uniform(shape=(3, 1))",
        "core_API": "uniform"
    },
    {
        "commit_hash": "04e0a63d39dc57dfbc88ecf57b095acf1406768b",
        "index": "6aa56f9e..f39c5c52 100644",
        "commit_message": "fix: typo miss codespelling documentation and commented code\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _for_loop(*, dim, steps_num, current_state, drift_fn, volatility_fn,",
            "dt, sqrt_dt, time_indices, keep_mask, random_type, seed,",
            "normal_draws, input_gradients, stratonovich_order,",
            "aux_normal_draws):",
            "-  \"\"\"Smaple paths using custom for_loop.\"\"\"",
            "+  \"\"\"Sample paths using custom for_loop.\"\"\"",
            "num_time_points = time_indices.shape.as_list()[-1]",
            "if num_time_points == 1:",
            "iter_nums = steps_num"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Smaple paths using custom for_loop.\"\"\"), value='\"\"\"Sample paths using custom for_loop.\"\"\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3001,
        "neg_line": [
            "-\"\"\"Smaple paths using custom for_loop.\"\"\""
        ],
        "pos_line": [
            "+\"\"\"Sample paths using custom for_loop.\"\"\""
        ],
        "core_change": "-\"\"\"Smaple paths using custom for_loop.\"\"\" +\"\"\"Sample paths using custom for_loop.\"\"\"",
        "core_API": "as_list"
    },
    {
        "commit_hash": "ca6e72e3b7cd0b119b0eb36c37d4a45da06b81d1",
        "index": "7c0a7f67..d17225a7 100644",
        "commit_message": "Fix clamping in LSTMCellWithProjection (#862)\n\n* More logging options in trainer\n\n* Don't clamp in place\n\n* cleanup\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LstmCellWithProjection(torch.nn.Module):",
            "timestep_output = self.state_projection(pre_projection_timestep_output)",
            "if self.state_projection_clip_value:",
            "# pylint: disable=invalid-unary-operand-type",
            "-                timestep_output.data.clamp_(-self.state_projection_clip_value,",
            "-                                            self.state_projection_clip_value)",
            "+                timestep_output = torch.clamp(timestep_output,",
            "+                                              -self.state_projection_clip_value,",
            "+                                              self.state_projection_clip_value)",
            "",
            "# Only do dropout if the dropout prob is > 0.0 and we are in training mode.",
            "if dropout_mask is not None:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=38748)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'timestep_output'), position=0, insert_id=38749)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=38750)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=timestep_output), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=data), value='clamp')",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'timestep_output'), position=1, insert_id=38751)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=38752)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=clamp_))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 3006,
        "neg_line": [
            "-timestep_output.data.clamp_(-self.state_projection_clip_value,",
            "-self.state_projection_clip_value)"
        ],
        "pos_line": [
            "+timestep_output = torch.clamp(timestep_output,",
            "+-self.state_projection_clip_value,",
            "+self.state_projection_clip_value)"
        ],
        "core_change": "-timestep_output.data.clamp_(-self.state_projection_clip_value, -self.state_projection_clip_value) +timestep_output = torch.clamp(timestep_output, +-self.state_projection_clip_value, +self.state_projection_clip_value)",
        "core_API": "state_projection"
    },
    {
        "commit_hash": "9a0f1e20ca1e783cb14c1ab1cc2f54b0b5b201e8",
        "index": "12aca33c0..a7c24be2e 100644",
        "commit_message": "fix tf context import\n\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tensorflow",
        "change": [
            "def temp_seed(seed: int, set_pytorch=False, set_tensorflow=False):",
            "if not tf.executing_eagerly():",
            "raise ValueError(\"Setting random seed for TensorFlow is only available in eager mode\")",
            "",
            "-        tf_context = tfpy.context.context()  # eager mode context",
            "+        tf_context = tfpycontext.context()  # eager mode context",
            "tf_seed = tf_context._seed",
            "tf_rng_initialized = hasattr(tf_context, \"_rng\")",
            "if tf_rng_initialized:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tfpy), value='tfpycontext')",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=tfpy), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=context))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 3007,
        "neg_line": [
            "-tf_context = tfpy.context.context()  # eager mode context"
        ],
        "pos_line": [
            "+tf_context = tfpycontext.context()  # eager mode context"
        ],
        "core_change": "-tf_context = tfpy.context.context()  # eager mode context +tf_context = tfpycontext.context()  # eager mode context",
        "core_API": "executing_eagerly"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "6f376065..ad70fc29 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CopyNetTest(ModelTestCase):",
            "]",
            ")",
            "",
            "-        generation_scores_mask = generation_scores.new_full(generation_scores.size(), 1.0)",
            "+        generation_scores_mask = generation_scores.new_full(",
            "+            generation_scores.size(), True, dtype=torch.bool",
            "+        )",
            "ll_actual, selective_weights_actual = self.model._get_ll_contrib(",
            "generation_scores,",
            "generation_scores_mask,"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('true', 'True'), position=3, insert_id=19657)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=19658)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=19659)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=19660)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=19661)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=19662)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=19663)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=19664)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'bool'), position=2, insert_id=19665)",
            "Delete(target_node=ASTNode(type=float, text=1.0))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 3008,
        "neg_line": [
            "-generation_scores_mask = generation_scores.new_full(generation_scores.size(), 1.0)"
        ],
        "pos_line": [
            "+generation_scores_mask = generation_scores.new_full(",
            "+generation_scores.size(), True, dtype=torch.bool",
            "+)"
        ],
        "core_change": "-generation_scores_mask = generation_scores.new_full(generation_scores.size(), 1.0) +generation_scores_mask = generation_scores.new_full( +generation_scores.size(), True, dtype=torch.bool +)",
        "core_API": "new_full"
    },
    {
        "commit_hash": "cf9f9e71270b6802c0d5e346fc37913b0f73ef4b",
        "index": "2b9eed3d35..893d27a97c 100644",
        "commit_message": "fix test_torch_is_tensor\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_torch_is_tensor(",
            "helpers.test_frontend_function(",
            "input_dtypes=input_dtype,",
            "as_variable_flags=as_variable,",
            "-        with_out=with_out,",
            "+        with_out=False,",
            "num_positional_args=num_positional_args,",
            "native_array_flags=native_array,",
            "fw=fw,",
            "frontend=\"torch\",",
            "fn_tree=\"is_tensor\",",
            "-        input=np.asarray(x, dtype=input_dtype),",
            "-        out=None,",
            "+        obj=np.asarray(x, dtype=input_dtype),",
            ")"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=default_parameter), node=('false', 'False'), position=2, insert_id=324244)",
            "Update(target_node=ASTNode(type=identifier, text=input), value='obj')",
            "Delete(target_node=ASTNode(type=identifier, text=with_out))",
            "Delete(target_node=ASTNode(type=identifier, text=out))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=none, text=None))",
            "Delete(target_node=ASTNode(type=default_parameter))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 8,
        "number": 3010,
        "neg_line": [
            "-with_out=with_out,",
            "-input=np.asarray(x, dtype=input_dtype),",
            "-out=None,"
        ],
        "pos_line": [
            "+with_out=False,",
            "+obj=np.asarray(x, dtype=input_dtype),"
        ],
        "core_change": "-with_out=with_out, +with_out=False, -input=np.asarray(x, dtype=input_dtype), -out=None, +obj=np.asarray(x, dtype=input_dtype),",
        "core_API": "test_frontend_function"
    },
    {
        "commit_hash": "a0dcb45dc30586f728403a04170d7511f87c940d",
        "index": "5c09e24648..fc54907781 100644",
        "commit_message": "[rllib] Fix APEX priorities returning zero all the time (#5980)\n\n* fix\n\n* move example tests to end\n\n* level err\n\n* guard against none\n\n* no trace test\n\n* ignore thumbs\n\n* np\n\n* fix multi node\n\n* fix\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _convert_to_tf(x):",
            "return x",
            "",
            "if x is not None:",
            "-        x = tf.nest.map_structure(tf.convert_to_tensor, x)",
            "+        x = tf.nest.map_structure(",
            "+            lambda f: tf.convert_to_tensor(f) if f is not None else None, x)",
            "return x"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('lambda', None), position=1, insert_id=2147967)",
            "Insert(target_node=IN(type=lambda), node=('lambda', 'lambda'), position=0, insert_id=2147968)",
            "Insert(target_node=IN(type=lambda), node=('lambda_parameters', None), position=1, insert_id=2147969)",
            "Insert(target_node=IN(type=lambda), node=(':', ':'), position=2, insert_id=2147970)",
            "Insert(target_node=IN(type=lambda), node=('conditional_expression', None), position=3, insert_id=2147971)",
            "Insert(target_node=IN(type=lambda_parameters), node=('identifier', 'f'), position=0, insert_id=2147972)",
            "Insert(target_node=IN(type=conditional_expression), node=('call', None), position=0, insert_id=2147973)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=2147974)",
            "Insert(target_node=IN(type=conditional_expression), node=('comparison_operator', None), position=2, insert_id=2147975)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=2147976)",
            "Insert(target_node=IN(type=conditional_expression), node=('none', 'None'), position=4, insert_id=2147977)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2147978)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'f'), position=0, insert_id=2147979)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=2147980)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=2147981)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=2147982)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2147983)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'f'), position=1, insert_id=2147984)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2147985)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 3012,
        "neg_line": [
            "-x = tf.nest.map_structure(tf.convert_to_tensor, x)"
        ],
        "pos_line": [
            "+x = tf.nest.map_structure(",
            "+lambda f: tf.convert_to_tensor(f) if f is not None else None, x)"
        ],
        "core_change": "-x = tf.nest.map_structure(tf.convert_to_tensor, x) +x = tf.nest.map_structure( +lambda f: tf.convert_to_tensor(f) if f is not None else None, x)",
        "core_API": "map_structure"
    },
    {
        "commit_hash": "ba81114d05287ec00479f38d4e302f5ebe6a0f0b",
        "index": "c93d5a0..585e20f 100644",
        "commit_message": "Tests (#793)\n\n* improve tests\n\n* mock\n\n* remove extra test files\n\n* change the directory structure of the tests\n\n* style fixes\n\n* improve tests and bug fix\n\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class StructuredDataInput(Input):",
            "",
            "def transform(self, x):",
            "if isinstance(x, pd.DataFrame):",
            "-            # convert x,y,validation_data to tf.Dataset",
            "+            # convert x, y, validation_data to tf.Dataset",
            "x = tf.data.Dataset.from_tensor_slices(",
            "x.values.astype(np.unicode))",
            "if isinstance(x, np.ndarray):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3015,
        "neg_line": [
            "-# convert x,y,validation_data to tf.Dataset"
        ],
        "pos_line": [
            "+# convert x, y, validation_data to tf.Dataset"
        ],
        "core_change": "-# convert x,y,validation_data to tf.Dataset +# convert x, y, validation_data to tf.Dataset",
        "core_API": "from_tensor_slices"
    },
    {
        "commit_hash": "b6b4c96b7e3f5d5ca80b2365ecf21f9a0141553a",
        "index": "9f11f285e..a8776503c 100644",
        "commit_message": "Assure blank is not scored by CTC prefix scorer\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CTCPrefixScoreTH(object):",
            "r_prev, s_prev, f_min_prev, f_max_prev = state",
            "",
            "# select input dimensions for scoring",
            "-        if self.scoring_num > 0 and prep_scores is not None:",
            "-            scoring_ids = torch.topk(prep_scores, self.scoring_num, 1)[1]",
            "+        if self.scoring_num > 0 and pre_scores is not None:",
            "+            pre_scores[:, self.blank] = self.logzero  # ignore blank from pre-selection",
            "+            scoring_ids = torch.topk(pre_scores, self.scoring_num, 1)[1]",
            "scoring_idmap = torch.full((self.n_bb, self.odim), -1, dtype=torch.long, device=self.device)",
            "snum = scoring_ids.size(1)",
            "scoring_idmap[self.bb_idx, scoring_ids] = torch.arange(snum, device=self.device)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=block), node=('expression_statement', None), position=0, insert_id=168130)",
            "Update(target_node=ASTNode(type=identifier, text=prep_scores), value='pre_scores')",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=168131)",
            "Insert(target_node=IN(type=assignment), node=('subscript', None), position=0, insert_id=168132)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=168133)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=2, insert_id=168134)",
            "Insert(target_node=IN(type=subscript), node=('identifier', 'pre_scores'), position=0, insert_id=168135)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=168136)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=2, insert_id=168137)",
            "Insert(target_node=IN(type=subscript), node=(',', ','), position=3, insert_id=168138)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=4, insert_id=168139)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=5, insert_id=168140)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=168141)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=168142)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'logzero'), position=2, insert_id=168143)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=168144)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=168145)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=168146)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'blank'), position=2, insert_id=168147)",
            "Update(target_node=ASTNode(type=identifier, text=prep_scores), value='pre_scores')"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 3017,
        "neg_line": [
            "-if self.scoring_num > 0 and prep_scores is not None:",
            "-scoring_ids = torch.topk(prep_scores, self.scoring_num, 1)[1]"
        ],
        "pos_line": [
            "+if self.scoring_num > 0 and pre_scores is not None:",
            "+pre_scores[:, self.blank] = self.logzero  # ignore blank from pre-selection",
            "+scoring_ids = torch.topk(pre_scores, self.scoring_num, 1)[1]"
        ],
        "core_change": "-if self.scoring_num > 0 and prep_scores is not None: -scoring_ids = torch.topk(prep_scores, self.scoring_num, 1)[1] +if self.scoring_num > 0 and pre_scores is not None: +pre_scores[:, self.blank] = self.logzero  # ignore blank from pre-selection +scoring_ids = torch.topk(pre_scores, self.scoring_num, 1)[1]",
        "core_API": "topk"
    },
    {
        "commit_hash": "95b6d985ffb231d9a2047c039c95660913063b9d",
        "index": "9b0c1240..7d1553ae 100644",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FBetaMeasureTest(AllenNlpTestCase):",
            "def test_fbeta_handles_batch_size_of_one(self, device: str):",
            "predictions = torch.tensor([[0.2862, 0.3479, 0.1627, 0.2033]], device=device)",
            "targets = torch.tensor([1], device=device)",
            "-        mask = torch.tensor([1], device=device)",
            "+        mask = torch.BoolTensor([True], device=device)",
            "",
            "fbeta = FBetaMeasure()",
            "fbeta(predictions, targets, mask)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tensor), value='BoolTensor')",
            "Insert(target_node=ASTNode(type=list), node=('true', 'True'), position=1, insert_id=20086)",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 3019,
        "neg_line": [
            "-mask = torch.tensor([1], device=device)"
        ],
        "pos_line": [
            "+mask = torch.BoolTensor([True], device=device)"
        ],
        "core_change": "-mask = torch.tensor([1], device=device) +mask = torch.BoolTensor([True], device=device)",
        "core_API": "tensor"
    },
    {
        "commit_hash": "3aec7d3c80d34aa7d7d44131e6b6e8891fd1f700",
        "index": "dfdcf0419..e1348192b 100644",
        "commit_message": "fixed mask\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class MultiHeadedAttention(nn.Module):",
            "",
            "scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)  # (batch, head, time1, time2)",
            "if mask is not None:",
            "-            mask.unsqueeze_(1).eq_(0)  # (batch, 1, time1, time2)",
            "+            mask = mask.unsqueeze(1).eq(0)  # (batch, 1, time1, time2)",
            "scores = scores.masked_fill(mask, MIN_VALUE)",
            "self.attn = torch.softmax(scores, dim=-1).masked_fill(mask, 0.0)  # (batch, head, time1, time2)",
            "else:",
            "-            self.attn = torch.softmax(scores, dim=-1)",
            "+            self.attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)",
            "",
            "p_attn = self.dropout(self.attn)",
            "x = torch.matmul(p_attn, v)  # (batch, head, time1, d_k)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=173507)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'mask'), position=0, insert_id=173508)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=173509)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=eq_), value='eq')",
            "Update(target_node=ASTNode(type=identifier, text=unsqueeze_), value='unsqueeze')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 3021,
        "neg_line": [
            "-mask.unsqueeze_(1).eq_(0)  # (batch, 1, time1, time2)",
            "-self.attn = torch.softmax(scores, dim=-1)"
        ],
        "pos_line": [
            "+mask = mask.unsqueeze(1).eq(0)  # (batch, 1, time1, time2)",
            "+self.attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)"
        ],
        "core_change": "-mask.unsqueeze_(1).eq_(0)  # (batch, 1, time1, time2) +mask = mask.unsqueeze(1).eq(0)  # (batch, 1, time1, time2) -self.attn = torch.softmax(scores, dim=-1) +self.attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)",
        "core_API": "matmul"
    },
    {
        "commit_hash": "da09e7e5c10613e25b90bbe49af12b1d6d120b42",
        "index": "33d23d1f..4fac2f60 100644",
        "commit_message": "Support CPU mode for inference (#2385)\n\n* add CPU only mode which can be activated during install\n\n* fixed flake8 errors for too long lines, still have to deal with \"import not at top of file\"\n\n* reversing changes in MinIoURandomCrop, that not relevant to the CPU_ONLY pull request\n\n* moving the CPU_ONLY checks into deeper parts of the code\n\n* completing previous commit\n\n* using isort for imports sorting\n\n* yapf fix\n\n* followed @xvjiarui suggestions for the pull request\n\n* use mmdet.CPU_ONLY and replace the \"--cpu\" flag in setup.py with automatic check of cuda\n\n* make setup code cleaner\n\n* back to original implementation of MinIoURandomCrop\n\n* build all extensions with CUDA, if available\n\n* fixed DC\n\n* update doc\n\n* fixed masked_conv2d_ext\n\n* set warning once, update comment\n\nCo-authored-by: Yossi Biton <yossi.biton@alibaba-inc.com>\nCo-authored-by: Yossi Biton <yossibit10@gmail.com>\n",
        "file": "mmdetection.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def parse_args():",
            "def main():",
            "args = parse_args()",
            "",
            "-    model = init_detector(",
            "-        args.config, args.checkpoint, device=torch.device('cuda', args.device))",
            "+    device = torch.device(args.device)",
            "+",
            "+    model = init_detector(args.config, args.checkpoint, device=device)",
            "",
            "camera = cv2.VideoCapture(args.camera_id)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=640042)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=640043)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'device'), position=0, insert_id=640044)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=640045)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=640046)",
            "Delete(target_node=ASTNode(type=string, text='cuda'))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 8,
        "number": 3022,
        "neg_line": [
            "-model = init_detector(",
            "-args.config, args.checkpoint, device=torch.device('cuda', args.device))"
        ],
        "pos_line": [
            "+device = torch.device(args.device)",
            "+",
            "+model = init_detector(args.config, args.checkpoint, device=device)"
        ],
        "core_change": "-model = init_detector( -args.config, args.checkpoint, device=torch.device('cuda', args.device)) +device = torch.device(args.device) + +model = init_detector(args.config, args.checkpoint, device=device)",
        "core_API": "device"
    },
    {
        "commit_hash": "baadbdf8d43a5171d65f24a793640b9f9523f260",
        "index": "715c95b13e..e58c739d23 100644",
        "commit_message": "[rllib] Execute PPO using training workflow (#8206)\n\n* wip\n\n* add kl\n\n* kl\n\n* works now\n\n* doc update\n\n* reorg\n\n* add ddppo\n\n* add stats\n\n* fix fetch\n\n* comment\n\n* fix learner stat regression\n\n* test fixes\n\n* fix test\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Trainer(Trainable):",
            "logger.info(\"Executing eagerly, with eager_tracing={}\".format(",
            "\"True\" if config.get(\"eager_tracing\") else \"False\"))",
            "",
            "-        if tf and not tf.executing_eagerly():",
            "+        if tf and not tf.executing_eagerly() and not config.get(\"use_pytorch\"):",
            "logger.info(\"Tip: set 'eager': true or the --eager flag to enable \"",
            "\"TensorFlow eager execution\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=boolean_operator), node=ASTNode(type=boolean_operator), position=0)",
            "Insert(target_node=ASTNode(type=boolean_operator), node=('and', 'and'), position=1, insert_id=2146451)",
            "Insert(target_node=ASTNode(type=boolean_operator), node=('not_operator', None), position=2, insert_id=2146452)",
            "Insert(target_node=IN(type=not_operator), node=('not', 'not'), position=0, insert_id=2146453)",
            "Insert(target_node=IN(type=not_operator), node=('call', None), position=1, insert_id=2146454)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2146455)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2146456)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=0, insert_id=2146457)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2146458)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'get'), position=2, insert_id=2146459)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2146460)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"use_pytorch\"'), position=1, insert_id=2146461)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2146462)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 3026,
        "neg_line": [
            "-if tf and not tf.executing_eagerly():"
        ],
        "pos_line": [
            "+if tf and not tf.executing_eagerly() and not config.get(\"use_pytorch\"):"
        ],
        "core_change": "-if tf and not tf.executing_eagerly(): +if tf and not tf.executing_eagerly() and not config.get(\"use_pytorch\"):",
        "core_API": "info"
    },
    {
        "commit_hash": "7cef03dddd6fba26fff6748ed1cfdd18208c193e",
        "index": "a1b0ff6..9b9c3d9 100644",
        "commit_message": "Fix torch `long` to `float` tensor on HUB macOS (#8067)\n\n\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ComputeLoss:",
            "gi, gj = gij.T  # grid indices",
            "",
            "# Append",
            "-            indices.append((b, a, gj.clamp_(0, gain[3] - 1), gi.clamp_(0, gain[2] - 1)))  # image, anchor, grid indices",
            "+            indices.append((b, a, gj.clamp_(0, shape[2] - 1), gi.clamp_(0, shape[3] - 1)))  # image, anchor, grid",
            "tbox.append(torch.cat((gxy - gij, gwh), 1))  # box",
            "anch.append(anchors[a])  # anchors",
            "tcls.append(c)  # class"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=gain), value='shape')",
            "Update(target_node=ASTNode(type=integer, text=3), value='2')",
            "Update(target_node=ASTNode(type=identifier, text=gain), value='shape')",
            "Update(target_node=ASTNode(type=integer, text=2), value='3')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 3028,
        "neg_line": [
            "-indices.append((b, a, gj.clamp_(0, gain[3] - 1), gi.clamp_(0, gain[2] - 1)))  # image, anchor, grid indices"
        ],
        "pos_line": [
            "+indices.append((b, a, gj.clamp_(0, shape[2] - 1), gi.clamp_(0, shape[3] - 1)))  # image, anchor, grid"
        ],
        "core_change": "-indices.append((b, a, gj.clamp_(0, gain[3] - 1), gi.clamp_(0, gain[2] - 1)))  # image, anchor, grid indices +indices.append((b, a, gj.clamp_(0, shape[2] - 1), gi.clamp_(0, shape[3] - 1)))  # image, anchor, grid",
        "core_API": "append"
    },
    {
        "commit_hash": "097049b81bdd91e981ddbae68a5243e00d0f7c6e",
        "index": "63a1ddfc3..334a8e4d9 100644",
        "commit_message": "Distributed Trainer: 2 little fixes (#7461)\n\n* reset model.config\n\n* Update src/transformers/trainer.py\n\n* use lower case tensor\n\n* Just tensor change\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def distributed_broadcast_scalars(",
            ") -> \"torch.Tensor\":",
            "if is_torch_available():",
            "try:",
            "-            tensorized_scalar = torch.Tensor(scalars).cuda()",
            "+            tensorized_scalar = torch.tensor(scalars).cuda()",
            "output_tensors = [tensorized_scalar.clone() for _ in range(torch.distributed.get_world_size())]",
            "torch.distributed.all_gather(output_tensors, tensorized_scalar)",
            "concat = torch.cat(output_tensors, dim=0)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Tensor), value='tensor')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3031,
        "neg_line": [
            "-tensorized_scalar = torch.Tensor(scalars).cuda()"
        ],
        "pos_line": [
            "+tensorized_scalar = torch.tensor(scalars).cuda()"
        ],
        "core_change": "-tensorized_scalar = torch.Tensor(scalars).cuda() +tensorized_scalar = torch.tensor(scalars).cuda()",
        "core_API": "Tensor"
    },
    {
        "commit_hash": "941e6970ad90b0b8d1b374934ec514f0b75ec639",
        "index": "c096455e2..f47405b00 100644",
        "commit_message": "- get activation from string name instead of callable class\n- fix typing in model.py, base.py, activations.py\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "def initialize_model(input_shape) -> nn.Model:",
            "model.add(",
            "nn.Convolution(nb_filter=32, filter_size=3, padding=2, input_shape=input_shape)",
            ")",
            "-    model.add(nn.BatchNorm(activation=nn.leaky_ReLU()))",
            "+    model.add(nn.BatchNorm(activation=\"leaky_relu\"))",
            "model.add(nn.MaxPool(pool_size=2, stride=2))",
            "",
            "# Layer 2",
            "# model.add(nn.Convolution(nb_filter=64, filter_size=3, padding=2))",
            "-    # model.add(nn.BatchNorm(activation=nn.leaky_ReLU()))",
            "+    # model.add(nn.BatchNorm(activation=\"leaky_relu\"))",
            "# model.add(nn.MaxPool(pool_size=2, stride=2))",
            "",
            "# Layer 3"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('string', '\"leaky_relu\"'), position=2, insert_id=1445023)",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=leaky_ReLU))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 3033,
        "neg_line": [
            "-model.add(nn.BatchNorm(activation=nn.leaky_ReLU()))",
            "-# model.add(nn.BatchNorm(activation=nn.leaky_ReLU()))"
        ],
        "pos_line": [
            "+model.add(nn.BatchNorm(activation=\"leaky_relu\"))",
            "+# model.add(nn.BatchNorm(activation=\"leaky_relu\"))"
        ],
        "core_change": "-model.add(nn.BatchNorm(activation=nn.leaky_ReLU())) +model.add(nn.BatchNorm(activation=\"leaky_relu\")) -# model.add(nn.BatchNorm(activation=nn.leaky_ReLU())) +# model.add(nn.BatchNorm(activation=\"leaky_relu\"))",
        "core_API": "add"
    },
    {
        "commit_hash": "64236e92e99fe64467dda8e05d3e8dd63ec7d274",
        "index": "34954258..fd071961 100644",
        "commit_message": "fix all tests\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ClusterData(torch.utils.data.Dataset):",
            "",
            "N, E = self.data.num_nodes, self.data.num_edges",
            "data = copy.copy(self.data)",
            "-        if hasattr(data, '__num_nodes__'):",
            "-            del data.__num_nodes__",
            "+        del data.num_nodes",
            "+        del data.num_edges",
            "adj, data.adj = data.adj, None",
            "",
            "adj = adj.narrow(0, start, length).narrow(1, start, length)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('delete_statement', None), position=3, insert_id=1005623)",
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=delete_statement), position=4)",
            "Insert(target_node=IN(type=delete_statement), node=('del', 'del'), position=0, insert_id=1005624)",
            "Insert(target_node=IN(type=delete_statement), node=('attribute', None), position=1, insert_id=1005625)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=data), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1005626)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'num_nodes'), position=2, insert_id=1005627)",
            "Update(target_node=ASTNode(type=identifier, text=__num_nodes__), value='num_edges')",
            "Delete(target_node=ASTNode(type=if, text=if))",
            "Delete(target_node=ASTNode(type=identifier, text=hasattr))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=string, text='__num_nodes__'))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 3034,
        "neg_line": [
            "-if hasattr(data, '__num_nodes__'):",
            "-del data.__num_nodes__"
        ],
        "pos_line": [
            "+del data.num_nodes",
            "+del data.num_edges"
        ],
        "core_change": "-if hasattr(data, '__num_nodes__'): -del data.__num_nodes__ +del data.num_nodes +del data.num_edges",
        "core_API": "copy"
    },
    {
        "commit_hash": "48b3b14cf0a8f47579801eb58df3737f4548fce0",
        "index": "c682f8e..8c4ca8a 100644",
        "commit_message": "fix of the rnn_seq2seq in order to be compatible with tensorflow 0.7\n\n",
        "file": "skflow.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def rnn_seq2seq(encoder_inputs, decoder_inputs, encoder_cell, decoder_cell=None,",
            "List of tensors for outputs and states for trianing and sampling sub-graphs.",
            "\"\"\"",
            "with tf.variable_scope(scope or \"rnn_seq2seq\"):",
            "-        _, enc_states = tf.nn.rnn(encoder_cell, encoder_inputs, dtype=dtype)",
            "-        return rnn_decoder(decoder_inputs, enc_states[-1], decoder_cell or encoder_cell)",
            "-",
            "+        _, last_enc_state = tf.nn.rnn(encoder_cell, encoder_inputs, dtype=dtype)",
            "+        return rnn_decoder(decoder_inputs, last_enc_state, decoder_cell or encoder_cell)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=enc_states), value='last_enc_state')",
            "Update(target_node=ASTNode(type=identifier, text=enc_states), value='last_enc_state')",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=enc_states), position=3)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 7,
        "number": 3035,
        "neg_line": [
            "-_, enc_states = tf.nn.rnn(encoder_cell, encoder_inputs, dtype=dtype)",
            "-return rnn_decoder(decoder_inputs, enc_states[-1], decoder_cell or encoder_cell)",
            "-"
        ],
        "pos_line": [
            "+_, last_enc_state = tf.nn.rnn(encoder_cell, encoder_inputs, dtype=dtype)",
            "+return rnn_decoder(decoder_inputs, last_enc_state, decoder_cell or encoder_cell)"
        ],
        "core_change": "-_, enc_states = tf.nn.rnn(encoder_cell, encoder_inputs, dtype=dtype) -return rnn_decoder(decoder_inputs, enc_states[-1], decoder_cell or encoder_cell) - +_, last_enc_state = tf.nn.rnn(encoder_cell, encoder_inputs, dtype=dtype) +return rnn_decoder(decoder_inputs, last_enc_state, decoder_cell or encoder_cell)",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "f8e90d6fb962716d0a79f71013f2485ac0982146",
        "index": "7c33981b6..33c14d921 100755",
        "commit_message": "Fix typing error in Trainer class (prediction_step) (#11138)\n\n* fix: docstrings in prediction_step\n\n* ci: Satisfy line length requirements\n\n* ci: character length requirements\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer:",
            "gathering predictions.",
            "",
            "Return:",
            "-            Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss, logits and",
            "-            labels (each being optional).",
            "+            Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss,",
            "+            logits and labels (each being optional).",
            "\"\"\"",
            "has_labels = all(inputs.get(k) is not None for k in self.label_names)",
            "inputs = self._prepare_inputs(inputs)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=subscript), node=('attribute', None), position=2, insert_id=1217848)",
            "Update(target_node=ASTNode(type=identifier, text=float), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=float), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1217849)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Tensor'), position=2, insert_id=1217850)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 5,
        "number": 3037,
        "neg_line": [
            "-Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss, logits and",
            "-labels (each being optional)."
        ],
        "pos_line": [
            "+Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss,",
            "+logits and labels (each being optional)."
        ],
        "core_change": "-Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss, logits and -labels (each being optional). +Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss, +logits and labels (each being optional).",
        "core_API": "get"
    },
    {
        "commit_hash": "04a17f8550c686e339dfd77ccfdbda9ee168b112",
        "index": "9757a0add..8766aef43 100644",
        "commit_message": "Doc fixes in preparation for the docstyle PR (#8061)\n\n* Fixes in preparation for doc styling\n\n* More fixes\n\n* Better syntax\n\n* Fixes\n\n* Style\n\n* More fixes\n\n* More fixes\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFLongformerEmbeddings(tf.keras.layers.Layer):",
            "def create_position_ids_from_inputs_embeds(self, inputs_embeds):",
            "\"\"\"We are provided embeddings directly. We cannot infer which are padded so just generate",
            "sequential position ids.",
            "-        :param tf.Tensor inputs_embeds:",
            "-        :return tf.Tensor:",
            "+",
            "+        Args:",
            "+            inputs_embeds: tf.Tensor",
            "+",
            "+        Returns: tf.Tensor",
            "\"\"\"",
            "seq_length = shape_list(inputs_embeds)[1]",
            "position_ids = tf.range(self.padding_idx + 1, seq_length + self.padding_idx + 1, dtype=tf.int32)[tf.newaxis, :]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"We are provided embeddings directly. We cannot infer which are padded so just generate\nsequential position ids.\n        :param tf.Tensor inputs_embeds:\n        :return tf.Tensor:\n\"\"\"), value='\"\"\"We are provided embeddings directly. We cannot infer which are padded so just generate\\nsequential position ids.\\n\\n        Args:\\n            inputs_embeds: tf.Tensor\\n\\n        Returns: tf.Tensor\\n\"\"\"')"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 3040,
        "neg_line": [
            "-:param tf.Tensor inputs_embeds:",
            "-:return tf.Tensor:"
        ],
        "pos_line": [
            "+",
            "+Args:",
            "+inputs_embeds: tf.Tensor",
            "+",
            "+Returns: tf.Tensor"
        ],
        "core_change": "-:param tf.Tensor inputs_embeds: -:return tf.Tensor: + +Args: +inputs_embeds: tf.Tensor + +Returns: tf.Tensor",
        "core_API": "range"
    },
    {
        "commit_hash": "6ba2231d7227825dfb76e9df161824d04234e69f",
        "index": "b5188e90..08ca2e0b 100644",
        "commit_message": "Reproducibility 3/3 (#1924)\n\n* make tests deterministic\n\n* run slow tests\n\n* prepare for testing\n\n* finish\n\n* refactor\n\n* add print statements\n\n* finish more\n\n* correct some test failures\n\n* more fixes\n\n* set up to correct tests\n\n* more corrections\n\n* up\n\n* fix more\n\n* more prints\n\n* add\n\n* up\n\n* up\n\n* up\n\n* uP\n\n* uP\n\n* more fixes\n\n* uP\n\n* up\n\n* up\n\n* up\n\n* up\n\n* fix more\n\n* up\n\n* up\n\n* clean tests\n\n* up\n\n* up\n\n* up\n\n* more fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* make\n\n* correct\n\n* finish\n\n* finish\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PipelineIntegrationTests(unittest.TestCase):",
            "pipe = pipe.to(device)",
            "pipe.set_progress_bar_config(disable=None)",
            "",
            "-        generator = torch.Generator(device=device).manual_seed(0)",
            "+        generator = torch.manual_seed(0)",
            "output = pipe(generator=generator, num_inference_steps=100, audio_length_in_s=4.096)",
            "audio = output.audios",
            "",
            "audio_slice = audio[0, -3:, -3:]",
            "",
            "assert audio.shape == (1, 2, pipe.unet.sample_size)",
            "-        expected_slice = np.array([-0.1693, -0.1698, -0.1447, -0.3044, -0.3203, -0.2937])",
            "+        expected_slice = np.array([-0.0367, -0.0488, -0.0771, -0.0525, -0.0444, -0.0341])",
            "+",
            "assert np.abs(audio_slice.flatten() - expected_slice).max() < 1e-2"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=torch), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=float, text=0.1693), value='0.0367')",
            "Update(target_node=ASTNode(type=float, text=0.1698), value='0.0488')",
            "Update(target_node=ASTNode(type=float, text=0.1447), value='0.0771')",
            "Update(target_node=ASTNode(type=float, text=0.3044), value='0.0525')",
            "Update(target_node=ASTNode(type=float, text=0.3203), value='0.0444')",
            "Update(target_node=ASTNode(type=float, text=0.2937), value='0.0341')",
            "Delete(target_node=ASTNode(type=identifier, text=Generator))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 3043,
        "neg_line": [
            "-generator = torch.Generator(device=device).manual_seed(0)",
            "-expected_slice = np.array([-0.1693, -0.1698, -0.1447, -0.3044, -0.3203, -0.2937])"
        ],
        "pos_line": [
            "+generator = torch.manual_seed(0)",
            "+expected_slice = np.array([-0.0367, -0.0488, -0.0771, -0.0525, -0.0444, -0.0341])",
            "+"
        ],
        "core_change": "-generator = torch.Generator(device=device).manual_seed(0) +generator = torch.manual_seed(0) -expected_slice = np.array([-0.1693, -0.1698, -0.1447, -0.3044, -0.3203, -0.2937]) +expected_slice = np.array([-0.0367, -0.0488, -0.0771, -0.0525, -0.0444, -0.0341]) +",
        "core_API": "to"
    },
    {
        "commit_hash": "d03223d4d64b89e76b48b00602aba5aa2f817f1e",
        "index": "887567e59..1738e766c 100644",
        "commit_message": "Use packaging to handle versions (#2777)\n\n* Get Python version from platform module\n\n* Set PY_VERSION as version class\n\n* Set PYARROW_VERSION as version class\n\n* Set TORCH_VERSION as version class\n\n* Set TF_VERSION as version class\n\n* Set JAX_VERSION as version class\n\n* Set BEAM_VERSION as version class\n\n* Set RARFILE_VERSION as version class\n\n* Use version class to validate PyArrow version at import\n\n* Use version class in SCRIPTS_VERSION at import\n\n* Use config.PYARROW_VERSION for parquet submodules\n\n* Fix style\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class LocalDatasetTest(parameterized.TestCase):",
            "",
            "def get_packaged_dataset_names():",
            "packaged_datasets = [{\"testcase_name\": x, \"dataset_name\": x} for x in _PACKAGED_DATASETS_MODULES.keys()]",
            "-    if version.parse(pa.__version__) < version.parse(\"3.0.0\"):  # parquet is not supported for pyarrow<3.0.0",
            "+    if datasets.config.PYARROW_VERSION.major < 3:  # parquet is not supported for pyarrow<3.0.0",
            "packaged_datasets = [pd for pd in packaged_datasets if pd[\"dataset_name\"] != \"parquet\"]",
            "return packaged_datasets"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('attribute', None), position=0, insert_id=1783801)",
            "Insert(target_node=ASTNode(type=comparison_operator), node=('integer', '3'), position=3, insert_id=1783802)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1783803)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=__version__), value='major')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=__version__), position=2)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=pa), value='PYARROW_VERSION')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=pa), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=version), value='datasets')",
            "Update(target_node=ASTNode(type=identifier, text=parse), value='config')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=identifier, text=version))",
            "Delete(target_node=ASTNode(type=identifier, text=parse))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=string, text=\"3.0.0\"))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 3046,
        "neg_line": [
            "-if version.parse(pa.__version__) < version.parse(\"3.0.0\"):  # parquet is not supported for pyarrow<3.0.0"
        ],
        "pos_line": [
            "+if datasets.config.PYARROW_VERSION.major < 3:  # parquet is not supported for pyarrow<3.0.0"
        ],
        "core_change": "-if version.parse(pa.__version__) < version.parse(\"3.0.0\"):  # parquet is not supported for pyarrow<3.0.0 +if datasets.config.PYARROW_VERSION.major < 3:  # parquet is not supported for pyarrow<3.0.0",
        "core_API": "keys"
    },
    {
        "commit_hash": "bf675ec07680eefca7fc6a55b647dac79a2292a2",
        "index": "= (- cum_num_nodes[batch]) + (batch * max_num_nodes)",
        "commit_message": "topk pooling example, bugfixes\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TopKPooling(torch.nn.Module):",
            "",
            "weight = F.normalize(self.weight, p=2, dim=-1)",
            "score = (x * weight).sum(dim=-1)",
            "-        perm = self.topk(score, self.k, batch)",
            "-",
            "-        x = x[perm] * self.tanh(score[perm])",
            "+        perm = self.topk(score, self.ratio, batch)",
            "+        x = x[perm] * torch.tanh(score[perm]).view(-1, 1)",
            "batch = batch[perm]",
            "edge_index, edge_attr = self.filter_adj(",
            "-            edge_index, edge_attr, perm, num_nodes=x.size(0))",
            "+            edge_index, edge_attr, perm, num_nodes=score.size(0))",
            "",
            "-        return x, edge_index, edge_attr, batch",
            "+        return x, edge_index, edge_attr, batch, perm",
            "",
            "def __repr__(self):",
            "return '{}({})'.format(self.__class__.__name__, self.ratio)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_list), node=(',', ','), position=7, insert_id=1067657)",
            "Insert(target_node=ASTNode(type=expression_list), node=('identifier', 'perm'), position=8, insert_id=1067658)",
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=1067659)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1067660)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1067661)",
            "Update(target_node=ASTNode(type=identifier, text=k), value='ratio')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1067662)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'view'), position=2, insert_id=1067663)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1067664)",
            "Insert(target_node=IN(type=argument_list), node=('unary_operator', '-1'), position=1, insert_id=1067665)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1067666)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '1'), position=3, insert_id=1067667)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1067668)",
            "Update(target_node=ASTNode(type=identifier, text=self), value='torch')",
            "Update(target_node=ASTNode(type=identifier, text=x), value='score')"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 16,
        "number": 3048,
        "neg_line": [
            "-perm = self.topk(score, self.k, batch)",
            "-",
            "-x = x[perm] * self.tanh(score[perm])",
            "-edge_index, edge_attr, perm, num_nodes=x.size(0))",
            "-return x, edge_index, edge_attr, batch"
        ],
        "pos_line": [
            "+perm = self.topk(score, self.ratio, batch)",
            "+x = x[perm] * torch.tanh(score[perm]).view(-1, 1)",
            "+edge_index, edge_attr, perm, num_nodes=score.size(0))",
            "+return x, edge_index, edge_attr, batch, perm"
        ],
        "core_change": "-perm = self.topk(score, self.k, batch) - -x = x[perm] * self.tanh(score[perm]) +perm = self.topk(score, self.ratio, batch) +x = x[perm] * torch.tanh(score[perm]).view(-1, 1) -edge_index, edge_attr, perm, num_nodes=x.size(0)) +edge_index, edge_attr, perm, num_nodes=score.size(0)) -return x, edge_index, edge_attr, batch +return x, edge_index, edge_attr, batch, perm",
        "core_API": "normalize"
    },
    {
        "commit_hash": "ee7592779238a1f1cae38c112d6d8168ef1500da",
        "index": "c06e467..6a3a57a 100644",
        "commit_message": "fixed typo\n",
        "file": "pointnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def get_learning_rate(batch):",
            "DECAY_STEP,          # Decay step.",
            "DECAY_RATE,          # Decay rate.",
            "staircase=True)",
            "-    learing_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!!",
            "+    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!!",
            "return learning_rate",
            "",
            "def get_bn_decay(batch):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=learing_rate), value='learning_rate')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3049,
        "neg_line": [
            "-learing_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!!"
        ],
        "pos_line": [
            "+learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!!"
        ],
        "core_change": "-learing_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!! +learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!!",
        "core_API": "maximum"
    },
    {
        "commit_hash": "e15c3886ba8303868d5f86a43b0e5c4837eb4df2",
        "index": "bd5d1b4..8452aa9 100644",
        "commit_message": "Defaul lambda r=7. Define '26t' stage 4/5 256x256 variants for all of bot/halo/lambda nets for experiment. Add resnet50t for exp. Fix a few comments.\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class HaloAttn(nn.Module):",
            "",
            "kv = self.kv(x)",
            "# FIXME I 'think' this unfold does what I want it to, but I should investigate",
            "-        k = F.unfold(kv, kernel_size=self.win_size, stride=self.block_size, padding=self.halo_size)",
            "-        k = k.reshape(",
            "+        kv = F.unfold(kv, kernel_size=self.win_size, stride=self.block_size, padding=self.halo_size)",
            "+        kv = kv.reshape(",
            "B * self.num_heads, self.dim_head + (self.dim_v // self.num_heads), -1, num_blocks).transpose(1, 3)",
            "-        k, v = torch.split(k, [self.dim_head, self.dim_v // self.num_heads], dim=-1)",
            "+        k, v = torch.split(kv, [self.dim_head, self.dim_v // self.num_heads], dim=-1)",
            "",
            "attn_logits = (q @ k.transpose(-1, -2)) * self.scale  # FIXME should usual attn scale be applied?",
            "attn_logits = attn_logits + self.pos_embed(q)  # B * num_heads, block_size ** 2, win_size ** 2"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=k), value='kv')",
            "Update(target_node=ASTNode(type=identifier, text=k), value='kv')",
            "Update(target_node=ASTNode(type=identifier, text=k), value='kv')",
            "Update(target_node=ASTNode(type=identifier, text=k), value='kv')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 4,
        "number": 3050,
        "neg_line": [
            "-k = F.unfold(kv, kernel_size=self.win_size, stride=self.block_size, padding=self.halo_size)",
            "-k = k.reshape(",
            "-k, v = torch.split(k, [self.dim_head, self.dim_v // self.num_heads], dim=-1)"
        ],
        "pos_line": [
            "+kv = F.unfold(kv, kernel_size=self.win_size, stride=self.block_size, padding=self.halo_size)",
            "+kv = kv.reshape(",
            "+k, v = torch.split(kv, [self.dim_head, self.dim_v // self.num_heads], dim=-1)"
        ],
        "core_change": "-k = F.unfold(kv, kernel_size=self.win_size, stride=self.block_size, padding=self.halo_size) -k = k.reshape( +kv = F.unfold(kv, kernel_size=self.win_size, stride=self.block_size, padding=self.halo_size) +kv = kv.reshape( -k, v = torch.split(k, [self.dim_head, self.dim_v // self.num_heads], dim=-1) +k, v = torch.split(kv, [self.dim_head, self.dim_v // self.num_heads], dim=-1)",
        "core_API": "kv"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "c733890ec0..da908966c7 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class VTraceSurrogateLoss:",
            "tf.float32))",
            "",
            "self.is_ratio = tf.clip_by_value(",
            "-            tf.exp(prev_actions_logp - old_policy_actions_logp), 0.0, 2.0)",
            "+            tf.math.exp(prev_actions_logp - old_policy_actions_logp), 0.0, 2.0)",
            "logp_ratio = self.is_ratio * tf.exp(actions_logp - prev_actions_logp)",
            "",
            "advantages = self.vtrace_returns.pg_advantages"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2145646)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2145647)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tf), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2145648)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 3055,
        "neg_line": [
            "-tf.exp(prev_actions_logp - old_policy_actions_logp), 0.0, 2.0)"
        ],
        "pos_line": [
            "+tf.math.exp(prev_actions_logp - old_policy_actions_logp), 0.0, 2.0)"
        ],
        "core_change": "-tf.exp(prev_actions_logp - old_policy_actions_logp), 0.0, 2.0) +tf.math.exp(prev_actions_logp - old_policy_actions_logp), 0.0, 2.0)",
        "core_API": "clip_by_value"
    },
    {
        "commit_hash": "94d20f71a3b9be4ce6764240fa34767114ad616b",
        "index": "a09d2c6f..896fbfd9 100644",
        "commit_message": "Remove tl.layers.initialize_global_variables(sess) (#931)\n\n* update sampling layers\n\n* upadte zoom\n\n* fix bug zoom\n\n* typo\n\n* fix bug affine_transform_cv2 x and y\n\n* fix bug crop when crop size equal to image size\n\n* fix file docs typo\n\n* fix bug instance norm\n\n* fix docs\n\n* update examples , init variables\n\n* changelog\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "def main(_):",
            "optimizer = tf.train.GradientDescentOptimizer(lr)",
            "train_op = optimizer.apply_gradients(zip(grads, tvars))",
            "",
            "-    # sess.run(tf.global_variables_initializer())",
            "-    tl.layers.initialize_global_variables(sess)",
            "+    sess.run(tf.global_variables_initializer())",
            "",
            "net.print_params()",
            "net.print_layers()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2633328)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=sess), position=0)",
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=layers), value='run')",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=(, text=(), position=0)",
            "Insert(target_node=IN(type=argument_list), node=('call', None), position=1, insert_id=2633329)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2633330)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2633331)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2633332)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2633333)",
            "Update(target_node=ASTNode(type=identifier, text=initialize_global_variables), value='global_variables_initializer')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=initialize_global_variables), position=2)",
            "Insert(target_node=ASTNode(type=argument_list), node=('(', '('), position=0, insert_id=2633334)",
            "Delete(target_node=ASTNode(type=identifier, text=tl))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 18,
        "number": 3059,
        "neg_line": [
            "-# sess.run(tf.global_variables_initializer())",
            "-tl.layers.initialize_global_variables(sess)"
        ],
        "pos_line": [
            "+sess.run(tf.global_variables_initializer())"
        ],
        "core_change": "-# sess.run(tf.global_variables_initializer()) -tl.layers.initialize_global_variables(sess) +sess.run(tf.global_variables_initializer())",
        "core_API": "GradientDescentOptimizer"
    },
    {
        "commit_hash": "cd51772bbe4eec0addd4e149b2f47f999020e7a0",
        "index": "575d2be5..8329e837 100644",
        "commit_message": "remove numpy import (#1116)\n\n* remove numpy import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* undo and add setup develop in gh actions\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove create_checkerboard\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestTopHat:",
            "None, None, :, :",
            "]",
            "assert_allclose(",
            "-            top_hat(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected,",
            "-            atol=1e-3, rtol=1e-3",
            "+            top_hat(tensor, torch.ones_like(structural_element), structuring_element=structural_element),",
            "+            expected,",
            "+            atol=1e-3,",
            "+            rtol=1e-3,",
            ")",
            "",
            "def test_exception(self, device, dtype):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=8, insert_id=423231)"
        ],
        "plus_line": 4,
        "minus_line": 2,
        "AST_diff_line": 1,
        "number": 3063,
        "neg_line": [
            "-top_hat(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected,",
            "-atol=1e-3, rtol=1e-3"
        ],
        "pos_line": [
            "+top_hat(tensor, torch.ones_like(structural_element), structuring_element=structural_element),",
            "+expected,",
            "+atol=1e-3,",
            "+rtol=1e-3,"
        ],
        "core_change": "-top_hat(tensor, torch.ones_like(structural_element), structuring_element=structural_element), expected, -atol=1e-3, rtol=1e-3 +top_hat(tensor, torch.ones_like(structural_element), structuring_element=structural_element), +expected, +atol=1e-3, +rtol=1e-3,",
        "core_API": "ones_like"
    },
    {
        "commit_hash": "6013d3f1f6ed341727206ed3585182de1cfa8023",
        "index": "3236b8e7..1d46d3b0 100644",
        "commit_message": "fixed problem with a3c\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DistributedRunner(object):",
            "logdir=\"/tmp/train_logs\",",
            "global_step=worker_agent.model.global_step,",
            "init_op=init_op,",
            "+                                             local_init_op=local_init_op,",
            "init_fn=init_fn,",
            "ready_op=tf.report_uninitialized_variables(variables_to_save),",
            "saver=worker_agent.model.saver)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=2246351)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=2246352)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'local_init_op'), position=0, insert_id=2246353)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2246354)",
            "Insert(target_node=IN(type=assignment), node=('expression_list', None), position=2, insert_id=2246355)",
            "Insert(target_node=IN(type=expression_list), node=('identifier', 'local_init_op'), position=0, insert_id=2246356)",
            "Insert(target_node=IN(type=expression_list), node=(',', ','), position=1, insert_id=2246357)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 7,
        "number": 3065,
        "neg_line": [],
        "pos_line": [
            "+local_init_op=local_init_op,"
        ],
        "core_change": "+local_init_op=local_init_op,",
        "core_API": "report_uninitialized_variables"
    },
    {
        "commit_hash": "e0853644bcb25d486a61c157f8b6f2ee24364017",
        "index": "ae7270f0..95cb7bd0 100644",
        "commit_message": "Fix default scope name for segment cumsum.\n\nPiperOrigin-RevId: 279963528\n\n",
        "file": "tf-quant-finance.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def segment_cumsum(x, segment_ids, exclusive=False, dtype=None, name=None):",
            "`n-sum(min(order, length(segment_j)), j)` where the sum is over segments.",
            "If `exclusive` is False, then the size is `n`.",
            "\"\"\"",
            "-  with tf.compat.v1.name_scope(name, default_name='segment_diff', values=[x]):",
            "+  with tf.compat.v1.name_scope(name, default_name='segment_cumsum', values=[x]):",
            "x = tf.convert_to_tensor(x, dtype=dtype)",
            "raw_cumsum = tf.math.cumsum(x, exclusive=exclusive)",
            "if segment_ids is None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='segment_diff'), value=\"'segment_cumsum'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3066,
        "neg_line": [
            "-with tf.compat.v1.name_scope(name, default_name='segment_diff', values=[x]):"
        ],
        "pos_line": [
            "+with tf.compat.v1.name_scope(name, default_name='segment_cumsum', values=[x]):"
        ],
        "core_change": "-with tf.compat.v1.name_scope(name, default_name='segment_diff', values=[x]): +with tf.compat.v1.name_scope(name, default_name='segment_cumsum', values=[x]):",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "aa6064d2..dcba7319 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FullAttention(Module):",
            "QK.masked_fill_(~(q_mask[:, :, None, None] * kv_mask[:, None, :, None]), float('-inf'))",
            "",
            "# Compute the attention and the weighted average",
            "-        softmax_temp = 1. / queries.size(3)**.5  # sqrt(D)",
            "+        softmax_temp = 1.0 / queries.size(3) ** 0.5  # sqrt(D)",
            "A = torch.softmax(softmax_temp * QK, dim=2)",
            "if self.use_dropout:",
            "A = self.dropout(A)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=1.), value='1.0')",
            "Update(target_node=ASTNode(type=float, text=.5), value='0.5')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 3067,
        "neg_line": [
            "-softmax_temp = 1. / queries.size(3)**.5  # sqrt(D)"
        ],
        "pos_line": [
            "+softmax_temp = 1.0 / queries.size(3) ** 0.5  # sqrt(D)"
        ],
        "core_change": "-softmax_temp = 1. / queries.size(3)**.5  # sqrt(D) +softmax_temp = 1.0 / queries.size(3) ** 0.5  # sqrt(D)",
        "core_API": "masked_fill_"
    },
    {
        "commit_hash": "de4159a318f9a4f2f474733306c00ac326e643a7",
        "index": "cc269bcdd..ff20ff1d1 100644",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFT5EncoderModel(TFT5PreTrainedModel):",
            "",
            "@property",
            "def dummy_inputs(self):",
            "-        return {\"input_ids\": tf.constant(DUMMY_INPUTS)}",
            "+        return {\"input_ids\": tf.constant(DUMMY_INPUTS, dtype=tf.int32)}",
            "",
            "def get_encoder(self):",
            "return self.encoder"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2358181)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2358182)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2358183)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2358184)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2358185)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2358186)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2358187)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int32'), position=2, insert_id=2358188)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 3074,
        "neg_line": [
            "-return {\"input_ids\": tf.constant(DUMMY_INPUTS)}"
        ],
        "pos_line": [
            "+return {\"input_ids\": tf.constant(DUMMY_INPUTS, dtype=tf.int32)}"
        ],
        "core_change": "-return {\"input_ids\": tf.constant(DUMMY_INPUTS)} +return {\"input_ids\": tf.constant(DUMMY_INPUTS, dtype=tf.int32)}",
        "core_API": "constant"
    },
    {
        "commit_hash": "a43b1e94eeb552ad1d0353591695ee08448bff71",
        "index": "fbbd5b7..676c42b 100644",
        "commit_message": "Stability improvements (#99)\n\n* bugfix & torch fx implementation\n\n* update torch tensorrt installation\n\n* fix sparseml colab\n\n* fix sparseml training on gpu\n\n* restored lost fixes\n\n* fixed torch_tensorrt install\n\n* fix quantization\n\n* improved tests & bugfix\n\n* prevent deepsparse installation for arm cpu\n\n* bugfix tvm & improved tests\n\n* added tests for tensorflow and onnx\n\n* fix test torchscript\n\n* fix tensorrt static quant\n\n* update notebooks\n\n* fix deepsparse bugs and implemented tests\n\n* add test for sparseml compressor\n\n* bug fixes on tensorflow backend and added tests\n\n* add version limit to tensorflow due to protobuffers 2.x not being supported from tf2onnx\n\n* update onnx version\n\n* remove numpy update\n\n* restored cpu tests\n\n* add python 3.10 in cpu tests\n\n* limit tensorflow gpu usage\n\n* fix python 3.10\n\n* improved tests\n\n* add warmup in model latency computation & add original model latency\n\n* fix pytorch tensorrt for transformers models\n\n* fixed bugs on onnx model handling\n\n* added onnx simplifier to fix tensorrt in onnx pipeline\n\n* fix deepsparse support to NO_COMPILER_INSTALLATION flag\n\n* fix model to onnx conversion problem and tensorrt issue with static quantization\n\n* add valerio citation code\n\n* added readme to notebooks folder\n\n* added tensorflow and onnx notebooks\n\n* style fix\n\n* fix tensor RT bug with static quantization when using new version of polygraphy and update pytorch resnet50 notebook\n\n* fix huggingface bug when passing tokenizer to optimize_model\n\n* updated notebooks readme and bugfix\n\n* minor fixes & added pruning with intel neural compressor\n\n* fixes and added test for intel pruning\n\n* fixes & added neural compressor quantization\n\n* fix test intel pruning compressor\n\n* added tests for neural compressor optimizer & bug fixes\n\n* removed transformers from requirements\n\n* changed openvino dynamic shape\n\n* computing latency using different data\n\n* bugfix openvino\n\n* make onnxsim optional\n\n* install onnx_sim only on intel machines\n\n* added bf16 and dynamic_quantization to neural_compressor\n\n* check output of compiled models\n\n* internal fixes\n\n* fix when no optimized model is found\n\nCo-authored-by: Valerio Sofi <v.sofi@nebuly.ai>\n",
        "file": "nebullvm.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TensorflowBackendInferenceLearner(TensorflowBaseInferenceLearner):",
            "super(TensorflowBackendInferenceLearner, self).__init__(**kwargs)",
            "self.model = tf_model",
            "",
            "-    @tf.function(jit_compile=True)",
            "def run(self, *input_tensors: tf.Tensor) -> Tuple[tf.Tensor, ...]:",
            "-        res = self.model.predict(*input_tensors)",
            "+        res = self.model.predict(input_tensors)",
            "if not isinstance(res, tuple):",
            "return (res,)",
            "return res"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=function_definition), position=3)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=input_tensors), position=1)",
            "Delete(target_node=ASTNode(type=@, text=@))",
            "Delete(target_node=ASTNode(type=identifier, text=tf))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=function))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=jit_compile))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=decorator))",
            "Delete(target_node=ASTNode(type=*, text=*))",
            "Delete(target_node=ASTNode(type=list_splat))",
            "Delete(target_node=ASTNode(type=decorated_definition))"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 19,
        "number": 3075,
        "neg_line": [
            "-@tf.function(jit_compile=True)",
            "-res = self.model.predict(*input_tensors)"
        ],
        "pos_line": [
            "+res = self.model.predict(input_tensors)"
        ],
        "core_change": "-@tf.function(jit_compile=True) -res = self.model.predict(*input_tensors) +res = self.model.predict(input_tensors)",
        "core_API": "function"
    },
    {
        "commit_hash": "9c634f064915a2424984d17294e2b5f2407540f1",
        "index": "9175fbf7..76cd89f5 100644",
        "commit_message": "test fixed points\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FixedPoints(object):",
            "choice = np.random.choice(data.num_nodes, self.num, replace=True)",
            "",
            "for key, item in data:",
            "-            if item.size(0) == num_nodes:",
            "+            if torch.is_tensor(item) and item.size(0) == num_nodes:",
            "data[key] = item[choice]",
            "",
            "return data"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=1054163)",
            "Insert(target_node=IN(type=boolean_operator), node=('call', None), position=0, insert_id=1054164)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1054165)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=2)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1054166)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1054167)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1054168)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1054169)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_tensor'), position=2, insert_id=1054170)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1054171)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'item'), position=1, insert_id=1054172)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1054173)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 3077,
        "neg_line": [
            "-if item.size(0) == num_nodes:"
        ],
        "pos_line": [
            "+if torch.is_tensor(item) and item.size(0) == num_nodes:"
        ],
        "core_change": "-if item.size(0) == num_nodes: +if torch.is_tensor(item) and item.size(0) == num_nodes:",
        "core_API": "choice"
    },
    {
        "commit_hash": "0353168f794effec77ac104e755b8d09d4ddc737",
        "index": "595c8785..ca773683 100644",
        "commit_message": "fixing stride\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DeepQNetwork(ValueFunction):",
            "",
            "with tf.name_scope(\"update\"):",
            "self.q_targets = tf.placeholder(tf.float32, [None], name='q_targets')",
            "-            self.actions = tf.placeholder(tf.int64, [None], name='actions')",
            "+            self.actions = tf.placeholder(tf.float32, [None, self.action_count], name='actions')",
            "",
            "# Q values for actions taken in batch",
            "-            actions_one_hot = tf.one_hot(self.actions, self.env_actions, 1.0, 0.0, name='action_one_hot')",
            "+            actions_one_hot = tf.one_hot(self.actions, self.action_count, 1.0, 0.0, name='action_one_hot')",
            "q_values_actions_taken = tf.reduce_sum(self.training_output * actions_one_hot, reduction_indices=1,",
            "name='q_acted')"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('attribute', None), position=0, insert_id=2248906)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2248907)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2248908)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'actions'), position=2, insert_id=2248909)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=attribute), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=int64), value='float32')",
            "Insert(target_node=ASTNode(type=list), node=(',', ','), position=2, insert_id=2248910)",
            "Insert(target_node=ASTNode(type=list), node=('attribute', None), position=3, insert_id=2248911)",
            "Update(target_node=ASTNode(type=identifier, text=env_actions), value='action_count')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2248912)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2248913)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'action_count'), position=2, insert_id=2248914)",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=actions))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 16,
        "number": 3078,
        "neg_line": [
            "-self.actions = tf.placeholder(tf.int64, [None], name='actions')",
            "-actions_one_hot = tf.one_hot(self.actions, self.env_actions, 1.0, 0.0, name='action_one_hot')"
        ],
        "pos_line": [
            "+self.actions = tf.placeholder(tf.float32, [None, self.action_count], name='actions')",
            "+actions_one_hot = tf.one_hot(self.actions, self.action_count, 1.0, 0.0, name='action_one_hot')"
        ],
        "core_change": "-self.actions = tf.placeholder(tf.int64, [None], name='actions') +self.actions = tf.placeholder(tf.float32, [None, self.action_count], name='actions') -actions_one_hot = tf.one_hot(self.actions, self.env_actions, 1.0, 0.0, name='action_one_hot') +actions_one_hot = tf.one_hot(self.actions, self.action_count, 1.0, 0.0, name='action_one_hot')",
        "core_API": "name_scope"
    },
    {
        "commit_hash": "b2e9aea908c3a316cdd9b7256211414710b10143",
        "index": "13e947dc5f..9ad060b919 100644",
        "commit_message": "[Ray dataset] detect dataframe dtype as object (#25811)\n\n* fix ci\n\n* not break master\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def convert_pandas_to_tf_tensor(",
            "# them. If the columns contain different types (for example, `float32`s",
            "# and `int32`s), then `tf.concat` raises an error.",
            "dtype: np.dtype = np.find_common_type(df.dtypes, [])",
            "+",
            "+            # if the columns are `ray.data.extensions.tensor_extension.TensorArray`,",
            "+            # the dtype will be `object`. In this case, we need to set the dtype to",
            "+            # none, and use the automatic type casting of `tf.convert_to_tensor`.",
            "+            if is_object_dtype(dtype):",
            "+                dtype = None",
            "+",
            "except TypeError:",
            "# `find_common_type` fails if a series has `TensorDtype`. In this case,",
            "# don't cast any of the series and continue."
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=dtype), position=3)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=:, text=:), position=4)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=type), position=5)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type==, text==), position=6)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=call), position=7)",
            "Insert(target_node=ASTNode(type=ERROR), node=('if', 'if'), position=8, insert_id=2136235)",
            "Insert(target_node=ASTNode(type=ERROR), node=('ERROR', None), position=9, insert_id=2136236)",
            "Insert(target_node=IN(type=ERROR), node=('call', None), position=0, insert_id=2136237)",
            "Insert(target_node=IN(type=ERROR), node=(':', ':'), position=1, insert_id=2136238)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'dtype'), position=2, insert_id=2136239)",
            "Insert(target_node=IN(type=ERROR), node=('=', '='), position=3, insert_id=2136240)",
            "Insert(target_node=IN(type=ERROR), node=('none', 'None'), position=4, insert_id=2136241)",
            "Insert(target_node=IN(type=ERROR), node=('except', 'except'), position=5, insert_id=2136242)",
            "Insert(target_node=IN(type=call), node=('identifier', 'is_object_dtype'), position=0, insert_id=2136243)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2136244)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2136245)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'dtype'), position=1, insert_id=2136246)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=2136247)",
            "Delete(target_node=ASTNode(type=typed_default_parameter))",
            "Delete(target_node=ASTNode(type=identifier, text=except))"
        ],
        "plus_line": 5,
        "minus_line": 0,
        "AST_diff_line": 20,
        "number": 3080,
        "neg_line": [],
        "pos_line": [
            "+",
            "+# if the columns are `ray.data.extensions.tensor_extension.TensorArray`,",
            "+# the dtype will be `object`. In this case, we need to set the dtype to",
            "+# none, and use the automatic type casting of `tf.convert_to_tensor`.",
            "+if is_object_dtype(dtype):",
            "+dtype = None",
            "+"
        ],
        "core_change": "+ +# if the columns are `ray.data.extensions.tensor_extension.TensorArray`, +# the dtype will be `object`. In this case, we need to set the dtype to +# none, and use the automatic type casting of `tf.convert_to_tensor`. +if is_object_dtype(dtype): +dtype = None +",
        "core_API": "find_common_type"
    },
    {
        "commit_hash": "7601a7b1a8eea865120f2f669c5e339f9e739fef",
        "index": "f25afcc74..663667ab1 100644",
        "commit_message": "Add The Pile Enron Emails subset (#3427)\n\n* Add The Pile Enron Emails subset\n\n* Update dataset card\n\n* Fix style\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class ThePile(datasets.GeneratorBasedBuilder):",
            "key += 1",
            "else:",
            "for subset in files:",
            "-                if subset in {\"europarl\", \"free_law\", \"nih_exporter\", \"pubmed\", \"ubuntu_irc\"}:",
            "+                if subset in {\"enron_emails\", \"europarl\", \"free_law\", \"nih_exporter\", \"pubmed\", \"ubuntu_irc\"}:",
            "import zstandard as zstd",
            "",
            "with zstd.open(open(files[subset], \"rb\"), \"rt\", encoding=\"utf-8\") as f:"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Move(target_node=ASTNode(type=string, text=\"europarl\"), node=ASTNode(type=set), position=2)",
            "Move(target_node=ASTNode(type=string, text=\"free_law\"), node=ASTNode(type=set), position=5)",
            "Move(target_node=ASTNode(type=string, text=\"nih_exporter\"), node=ASTNode(type=set), position=7)",
            "Move(target_node=ASTNode(type=string, text=\"pubmed\"), node=ASTNode(type=set), position=9)",
            "Insert(target_node=ASTNode(type=set), node=('string', '\"enron_emails\"'), position=1, insert_id=1781074)",
            "Insert(target_node=ASTNode(type=set), node=(',', ','), position=9, insert_id=1781075)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 3081,
        "neg_line": [
            "-if subset in {\"europarl\", \"free_law\", \"nih_exporter\", \"pubmed\", \"ubuntu_irc\"}:"
        ],
        "pos_line": [
            "+if subset in {\"enron_emails\", \"europarl\", \"free_law\", \"nih_exporter\", \"pubmed\", \"ubuntu_irc\"}:"
        ],
        "core_change": "-if subset in {\"europarl\", \"free_law\", \"nih_exporter\", \"pubmed\", \"ubuntu_irc\"}: +if subset in {\"enron_emails\", \"europarl\", \"free_law\", \"nih_exporter\", \"pubmed\", \"ubuntu_irc\"}:",
        "core_API": "open"
    },
    {
        "commit_hash": "ba9b6419049e09fc5c7298c75d6264364575dcf6",
        "index": "f5dc8a86d..c6ee02443 100644",
        "commit_message": "Sinc Convs - fixed scale tests and pytorch compability\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_mel_scale():",
            "f = 16000.0",
            "x = MelScale.convert(f)",
            "f_back = MelScale.invert(x)",
            "-    assert torch.abs(f_back - f) < 0.0001",
            "+    assert torch.abs(f_back - f) < 0.1",
            "MelScale.bank(128, 16000.0)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=0.0001), value='0.1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3084,
        "neg_line": [
            "-assert torch.abs(f_back - f) < 0.0001"
        ],
        "pos_line": [
            "+assert torch.abs(f_back - f) < 0.1"
        ],
        "core_change": "-assert torch.abs(f_back - f) < 0.0001 +assert torch.abs(f_back - f) < 0.1",
        "core_API": "convert"
    },
    {
        "commit_hash": "35f2da4c40ea7a493c59f83d1a39a96e9bf76104",
        "index": "45323a84..3a1af100 100644",
        "commit_message": "Format the entire codebase according to pre-commit packages. (#1575)\n\n* Format the entire codebase\n\n* Fix decoders import issue.\n",
        "file": "ludwig.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "import torch.nn as nn",
            "def glu(x: torch.Tensor, dim: int = -1):",
            "\"\"\"Generalized linear unit nonlinear activation.",
            "",
            "-    Expects 2*n_units-dimensional input.",
            "-    Half of it is used to determine the gating of the GLU activation",
            "-    and the other half is used as an input to GLU,",
            "+    Expects 2*n_units-dimensional input. Half of it is used to determine the gating of the GLU activation and the other",
            "+    half is used as an input to GLU,",
            "\"\"\"",
            "return nn.functional.glu(x, dim)",
            "",
            "",
            "def gelu(features: torch.Tensor, approximate: bool = False):",
            "if approximate:",
            "-        return 0.5 * features * (1.0 + nn.tanh(",
            "-            0.7978845608028654 * (features + 0.044715 * (features ** 3))",
            "-        ))",
            "+        return 0.5 * features * (1.0 + nn.tanh(0.7978845608028654 * (features + 0.044715 * (features ** 3))))",
            "else:",
            "return 0.5 * features * (1.0 + torch.erf(features / 1.4142135623730951))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Generalized linear unit nonlinear activation.\n\n    Expects 2*n_units-dimensional input.\n    Half of it is used to determine the gating of the GLU activation\n    and the other half is used as an input to GLU,\n\"\"\"), value='\"\"\"Generalized linear unit nonlinear activation.\\n\\n    Expects 2*n_units-dimensional input. Half of it is used to determine the gating of the GLU activation and the other\\n    half is used as an input to GLU,\\n\"\"\"')"
        ],
        "plus_line": 3,
        "minus_line": 6,
        "AST_diff_line": 1,
        "number": 3087,
        "neg_line": [
            "-Expects 2*n_units-dimensional input.",
            "-Half of it is used to determine the gating of the GLU activation",
            "-and the other half is used as an input to GLU,",
            "-return 0.5 * features * (1.0 + nn.tanh(",
            "-0.7978845608028654 * (features + 0.044715 * (features ** 3))",
            "-))"
        ],
        "pos_line": [
            "+Expects 2*n_units-dimensional input. Half of it is used to determine the gating of the GLU activation and the other",
            "+half is used as an input to GLU,",
            "+return 0.5 * features * (1.0 + nn.tanh(0.7978845608028654 * (features + 0.044715 * (features ** 3))))"
        ],
        "core_change": "-Expects 2*n_units-dimensional input. -Half of it is used to determine the gating of the GLU activation -and the other half is used as an input to GLU, +Expects 2*n_units-dimensional input. Half of it is used to determine the gating of the GLU activation and the other +half is used as an input to GLU, -return 0.5 * features * (1.0 + nn.tanh( -0.7978845608028654 * (features + 0.044715 * (features ** 3)) -)) +return 0.5 * features * (1.0 + nn.tanh(0.7978845608028654 * (features + 0.044715 * (features ** 3))))",
        "core_API": "glu"
    },
    {
        "commit_hash": "98d40fed3a4515077163adab9dfd8fb2fccf1267",
        "index": "ca4c5bcb6..96e3f5f4e 100644",
        "commit_message": "Cleanup the usage of `layer_norm_eps` in some models (#21336)\n\n* fix\n\n* fix\n\n* make style\n\n* For CLIP\n\n* For OwlViT\n\n* For XCLIP\n\n* For CLIPSeg\n\n* For GroupViT\n\n* fix docstrings\n\n* fix docstrings\n\n* For AltCLIP\n\n* For ChineseCLIP\n\n* For Blip\n\n* For GiT\n\n* make style\n\n* update\n\n* update\n\n* update\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class BlipVisionModel(BlipPreTrainedModel):",
            "",
            "self.embeddings = BlipVisionEmbeddings(config)",
            "self.encoder = BlipEncoder(config)",
            "-        self.post_layernorm = nn.LayerNorm(embed_dim)",
            "+        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)",
            "",
            "self.post_init()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1532681)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1532682)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'eps'), position=0, insert_id=1532683)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1532684)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1532685)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'config'), position=0, insert_id=1532686)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1532687)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'layer_norm_eps'), position=2, insert_id=1532688)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 3089,
        "neg_line": [
            "-self.post_layernorm = nn.LayerNorm(embed_dim)"
        ],
        "pos_line": [
            "+self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)"
        ],
        "core_change": "-self.post_layernorm = nn.LayerNorm(embed_dim) +self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)",
        "core_API": "LayerNorm"
    },
    {
        "commit_hash": "9ffda216ec65126f3fd834da072445ec146f1d77",
        "index": "15a0ed0ad..e72f316b3 100644",
        "commit_message": "Fix missed head transpose\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class XLNetRelativeAttention(nn.Module):",
            "",
            "# Mask heads if we want to",
            "if head_mask is not None:",
            "-            attn_prob = attn_prob * head_mask",
            "+            attn_prob = attn_prob * torch.einsum('ijbn->bnij', head_mask)",
            "",
            "# attention output",
            "attn_vec = torch.einsum('bnij,jbnd->ibnd', attn_prob, v_head_h)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=binary_operator), node=('call', None), position=2, insert_id=1246359)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1246360)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1246361)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1246362)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1246363)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'einsum'), position=2, insert_id=1246364)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1246365)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'ijbn->bnij'\"), position=1, insert_id=1246366)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=1246367)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=head_mask), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=1246368)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 3092,
        "neg_line": [
            "-attn_prob = attn_prob * head_mask"
        ],
        "pos_line": [
            "+attn_prob = attn_prob * torch.einsum('ijbn->bnij', head_mask)"
        ],
        "core_change": "-attn_prob = attn_prob * head_mask +attn_prob = attn_prob * torch.einsum('ijbn->bnij', head_mask)",
        "core_API": "einsum"
    },
    {
        "commit_hash": "9ebaea545ffddf2e9079994f2ea657a7fa5f358c",
        "index": "ec445e72..8fd8c2b8 100644",
        "commit_message": "Optimize Stable Diffusion (#371)\n\n* initial commit\n\n* make UNet stream capturable\n\n* try to fix noise_pred value\n\n* remove cuda graph and keep NB\n\n* non blocking unet with PNDMScheduler\n\n* make timesteps np arrays for pndm scheduler\nbecause lists don't get formatted to tensors in `self.set_format`\n\n* make max async in pndm\n\n* use channel last format in unet\n\n* avoid moving timesteps device in each unet call\n\n* avoid memcpy op in `get_timestep_embedding`\n\n* add `channels_last` kwarg to `DiffusionPipeline.from_pretrained`\n\n* update TODO\n\n* replace `channels_last` kwarg with `memory_format` for more generality\n\n* revert the channels_last changes to leave it for another PR\n\n* remove non_blocking when moving input ids to device\n\n* remove blocking from all .to() operations at beginning of pipeline\n\n* fix merging\n\n* fix merging\n\n* model can run in other precisions without autocast\n\n* attn refactoring\n\n* Revert \"attn refactoring\"\n\nThis reverts commit 0c70c0e189cd2c4d8768274c9fcf5b940ee310fb.\n\n* remove restriction to run conv_norm in fp32\n\n* use `baddbmm` instead of `matmul`for better in attention for better perf\n\n* removing all reshapes to test perf\n\n* Revert \"removing all reshapes to test perf\"\n\nThis reverts commit 006ccb8a8c6bc7eb7e512392e692a29d9b1553cd.\n\n* add shapes comments\n\n* hardcore whats needed for jitting\n\n* Revert \"hardcore whats needed for jitting\"\n\nThis reverts commit 2fa9c698eae2890ac5f8e367ca80532ecf94df9a.\n\n* Revert \"remove restriction to run conv_norm in fp32\"\n\nThis reverts commit cec592890c32da3d1b78d38b49e4307aedf459b9.\n\n* revert using baddmm in attention's forward\n\n* cleanup comment\n\n* remove restriction to run conv_norm in fp32. no quality loss was noticed\n\nThis reverts commit cc9bc1339c998ebe9e7d733f910c6d72d9792213.\n\n* add more optimizations techniques to docs\n\n* Revert \"add shapes comments\"\n\nThis reverts commit 31c58eadb8892f95478cdf05229adf678678c5f4.\n\n* apply suggestions\n\n* make quality\n\n* apply suggestions\n\n* styling\n\n* `scheduler.timesteps` are now arrays so we dont need .to()\n\n* remove useless .type()\n\n* use mean instead of max in `test_stable_diffusion_inpaint_pipeline_k_lms`\n\n* move scheduler timestamps to correct device if tensors\n\n* add device to `set_timesteps` in LMSD scheduler\n\n* `self.scheduler.set_timesteps` now uses device arg for schedulers that accept it\n\n* quick fix\n\n* styling\n\n* remove kwargs from schedulers `set_timesteps`\n\n* revert to using max in K-LMS inpaint pipeline test\n\n* Revert \"`self.scheduler.set_timesteps` now uses device arg for schedulers that accept it\"\n\nThis reverts commit 00d5a51e5c20d8d445c8664407ef29608106d899.\n\n* move timesteps to correct device before loop in SD pipeline\n\n* apply previous fix to other SD pipelines\n\n* UNet now accepts tensor timesteps even on wrong device, to avoid errors\n- it shouldnt affect performance if timesteps are alrdy on correct device\n- it does slow down performance if they're on the wrong device\n\n* fix pipeline when timesteps are arrays with strides\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LMSDiscreteScheduler(SchedulerMixin, ConfigMixin):",
            "sigmas = np.array(((1 - self.alphas_cumprod) / self.alphas_cumprod) ** 0.5)",
            "sigmas = np.interp(timesteps, np.arange(0, len(sigmas)), sigmas)",
            "sigmas = np.concatenate([sigmas, [0.0]]).astype(np.float32)",
            "-        self.sigmas = torch.from_numpy(sigmas)",
            "-        self.timesteps = torch.from_numpy(timesteps)",
            "+        self.sigmas = torch.from_numpy(sigmas).to(device=device)",
            "+        self.timesteps = torch.from_numpy(timesteps).to(device=device)",
            "",
            "self.derivatives = []"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=104264)",
            "Insert(target_node=ASTNode(type=assignment), node=('call', None), position=2, insert_id=104265)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=104266)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=104267)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=104268)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=104269)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=104270)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=104271)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=104272)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=104273)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=104274)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=104275)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=104276)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=104277)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=1, insert_id=104278)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=104279)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=104280)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=104281)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=104282)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=104283)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=104284)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=2, insert_id=104285)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 24,
        "number": 3094,
        "neg_line": [
            "-self.sigmas = torch.from_numpy(sigmas)",
            "-self.timesteps = torch.from_numpy(timesteps)"
        ],
        "pos_line": [
            "+self.sigmas = torch.from_numpy(sigmas).to(device=device)",
            "+self.timesteps = torch.from_numpy(timesteps).to(device=device)"
        ],
        "core_change": "-self.sigmas = torch.from_numpy(sigmas) -self.timesteps = torch.from_numpy(timesteps) +self.sigmas = torch.from_numpy(sigmas).to(device=device) +self.timesteps = torch.from_numpy(timesteps).to(device=device)",
        "core_API": "array"
    },
    {
        "commit_hash": "0ba7472da92a89af9aca84de5b01a228d18340a2",
        "index": "8b50dfd1d..bbb4338b8 100644",
        "commit_message": "[Testing] Fix LINT/sphinx errors. (#8874)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class TorchModelV2(ModelV2, nn.Module):",
            "model_config,",
            "name,",
            "framework=\"torch\")",
            "-        nn.Module.__init__(self)",
            "",
            "@override(ModelV2)",
            "def variables(self, as_dict=False):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('decorated_definition', None), position=3, insert_id=1506468)",
            "Insert(target_node=ASTNode(type=ERROR), node=(')', ')'), position=0, insert_id=1506469)",
            "Insert(target_node=IN(type=decorated_definition), node=('decorator', None), position=0, insert_id=1506470)",
            "Move(target_node=IN(type=decorated_definition), node=ASTNode(type=function_definition), position=1)",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=string, text=\"torch\"), position=2)",
            "Insert(target_node=IN(type=decorator), node=('@', '@'), position=0, insert_id=1506471)",
            "Move(target_node=IN(type=decorator), node=ASTNode(type=call), position=1)",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Module))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=__init__))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=@, text=@))",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 0,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 3095,
        "neg_line": [
            "-nn.Module.__init__(self)"
        ],
        "pos_line": [],
        "core_change": "-nn.Module.__init__(self)",
        "core_API": "__init__"
    },
    {
        "commit_hash": "41c99fbf385a8c875fb6181ce7301e4bc218535b",
        "index": "1466a8d..5e8bac3 100644",
        "commit_message": "Use preprocessing layers for categorical encoding (#1090)\n\n* removed sigmoid layer\n\n* added lookup\n\n* bug fix\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class ClassificationHead(head_module.Head):",
            "output_node = layers.Dropout(dropout_rate)(output_node)",
            "output_node = layers.Dense(self.output_shape[-1])(output_node)",
            "if self.loss == 'binary_crossentropy':",
            "-            output_node = keras_layers.Sigmoid(name=self.name)(output_node)",
            "+            output_node = layers.Activation(activations.sigmoid,",
            "+                                            name=self.name)(output_node)",
            "else:",
            "output_node = layers.Softmax(name=self.name)(output_node)",
            "return output_node"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=keras_layers), value='layers')",
            "Update(target_node=ASTNode(type=identifier, text=Sigmoid), value='Activation')",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=2560930)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2560931)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'activations'), position=0, insert_id=2560932)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2560933)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sigmoid'), position=2, insert_id=2560934)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 3100,
        "neg_line": [
            "-output_node = keras_layers.Sigmoid(name=self.name)(output_node)"
        ],
        "pos_line": [
            "+output_node = layers.Activation(activations.sigmoid,",
            "+name=self.name)(output_node)"
        ],
        "core_change": "-output_node = keras_layers.Sigmoid(name=self.name)(output_node) +output_node = layers.Activation(activations.sigmoid, +name=self.name)(output_node)",
        "core_API": "Dropout"
    },
    {
        "commit_hash": "d1ac376595143a50074dca824ed2d0343df8c1fe",
        "index": "ab1812b..4903d89 100644",
        "commit_message": "Fix issues with tests\n\n",
        "file": "nebullvm.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_onnxruntime_half(",
            "assert all(",
            "[",
            "torch.allclose(",
            "-                        res_tensor, res_orig_tensor.half(), rtol=1e-01",
            "+                        res_tensor.float(), res_orig_tensor, rtol=1e-01",
            ")",
            "for (res_tensor, res_orig_tensor) in zip(res, res_orig)",
            "]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'res_orig_tensor'), position=4, insert_id=651173)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=5, insert_id=651174)",
            "Update(target_node=ASTNode(type=identifier, text=res_orig_tensor), value='res_tensor')",
            "Update(target_node=ASTNode(type=identifier, text=half), value='float')",
            "Delete(target_node=ASTNode(type=identifier, text=res_tensor))",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 3101,
        "neg_line": [
            "-res_tensor, res_orig_tensor.half(), rtol=1e-01"
        ],
        "pos_line": [
            "+res_tensor.float(), res_orig_tensor, rtol=1e-01"
        ],
        "core_change": "-res_tensor, res_orig_tensor.half(), rtol=1e-01 +res_tensor.float(), res_orig_tensor, rtol=1e-01",
        "core_API": "allclose"
    },
    {
        "commit_hash": "d8fb7fdbe4e546db9c6236478819ef3e6f831914",
        "index": "1c7fa800..0556cfff 100644",
        "commit_message": "[Docs] Improve docs for geometry transform (#913)\n\n* fixes linter in clahe transpose\n\n* add tolerance in add_weighted jit test\n\n* update exclude in github matrix\n\n* fix docs in camera matrix shape for unproject_points\n\n* fix transpose in clahe\n\n* reorganize geometry transform docs\n\n* improve geometry documentation links\n\n* improve image transforms docs\n\n* fix mypy in affine3d\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def draw_rectangle(",
            "fill (bool, optional): is a flag used to fill the boxes with color if True. Default: False.",
            "width (int): The line width. Default: 1. (Not implemented yet).",
            "Returns:",
            "-            torch.Tensor: This operation modifies image inplace but also returns",
            "-            the drawn tensor for convenience with same shape the of the input BxCxHxW.",
            "+            torch.Tensor: This operation modifies image inplace but also returns the drawn tensor for",
            "+            convenience with same shape the of the input BxCxHxW.",
            "",
            "Example:",
            ">>> img = torch.rand(2, 3, 10, 12)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=the), value='convenience')",
            "Insert(target_node=ASTNode(type=ERROR), node=('with', 'with'), position=1, insert_id=429513)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'returns'), position=7, insert_id=429514)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'the'), position=8, insert_id=429515)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'drawn'), position=9, insert_id=429516)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'tensor'), position=10, insert_id=429517)",
            "Update(target_node=ASTNode(type=identifier, text=returns), value='for')",
            "Delete(target_node=ASTNode(type=identifier, text=drawn))",
            "Delete(target_node=ASTNode(type=identifier, text=tensor))",
            "Delete(target_node=ASTNode(type=for, text=for))",
            "Delete(target_node=ASTNode(type=identifier, text=convenience))",
            "Delete(target_node=ASTNode(type=identifier, text=with))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 3102,
        "neg_line": [
            "-torch.Tensor: This operation modifies image inplace but also returns",
            "-the drawn tensor for convenience with same shape the of the input BxCxHxW."
        ],
        "pos_line": [
            "+torch.Tensor: This operation modifies image inplace but also returns the drawn tensor for",
            "+convenience with same shape the of the input BxCxHxW."
        ],
        "core_change": "-torch.Tensor: This operation modifies image inplace but also returns -the drawn tensor for convenience with same shape the of the input BxCxHxW. +torch.Tensor: This operation modifies image inplace but also returns the drawn tensor for +convenience with same shape the of the input BxCxHxW.",
        "core_API": "rand"
    },
    {
        "commit_hash": "a7e255625d3bc08c43a070583e488ebc1609d07d",
        "index": "960563e5aa..e082092333 100644",
        "commit_message": "fixing some nightly tests\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def subtract(",
            "return torch.subtract(x1, x2, out=out)",
            "return torch.subtract(",
            "x1 if isinstance(x1, torch.Tensor) else torch.tensor(x1),",
            "-        x2 if isinstance(x2, torch.Tensor) else torch.tensor(x2)",
            "+        x2 if isinstance(x2, torch.Tensor) else torch.tensor(x2),",
            ")"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=3, insert_id=351548)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3103,
        "neg_line": [
            "-x2 if isinstance(x2, torch.Tensor) else torch.tensor(x2)"
        ],
        "pos_line": [
            "+x2 if isinstance(x2, torch.Tensor) else torch.tensor(x2),"
        ],
        "core_change": "-x2 if isinstance(x2, torch.Tensor) else torch.tensor(x2) +x2 if isinstance(x2, torch.Tensor) else torch.tensor(x2),",
        "core_API": "subtract"
    },
    {
        "commit_hash": "5e7d0daeafd3a4471a0465d65817bf7a804535ce",
        "index": "0996736ef..67e1ee66c 100644",
        "commit_message": "fixed lm model loading in cpu calculation\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def recog(args):",
            "if args.rnnlm:",
            "rnnlm = lm_train_th.ClassifierWithState(",
            "lm_train_th.RNNLM(len(train_args.char_list), 650))",
            "-        rnnlm.load_state_dict(torch.load(args.rnnlm))",
            "+        rnnlm.load_state_dict(torch.load(args.rnnlm, map_location=cpu_loader))",
            "else:",
            "rnnlm = None"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=186350)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=186351)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'map_location'), position=0, insert_id=186352)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=186353)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'cpu_loader'), position=2, insert_id=186354)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 3104,
        "neg_line": [
            "-rnnlm.load_state_dict(torch.load(args.rnnlm))"
        ],
        "pos_line": [
            "+rnnlm.load_state_dict(torch.load(args.rnnlm, map_location=cpu_loader))"
        ],
        "core_change": "-rnnlm.load_state_dict(torch.load(args.rnnlm)) +rnnlm.load_state_dict(torch.load(args.rnnlm, map_location=cpu_loader))",
        "core_API": "ClassifierWithState"
    },
    {
        "commit_hash": "a5d51c01e00f53657015face18314e4ebb41c444",
        "index": "6a0ed1a..264f4dd 100644",
        "commit_message": "Bug fix\n\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpatialBottleneckFunction(torch.autograd.Function):",
            "w1by3 = w[:,:1,:,:].clone()",
            "ctx.stream2.wait_stream(ctx.stream1) # wait for halo transfers to finish",
            "ctx.stream2.wait_stream(torch.cuda.current_stream()) # wait for backward_grad_out1_mask to finish before launching halo correction kernel",
            "-                    with torch.cuda.stream(ctx.stream1):",
            "+                    with torch.cuda.stream(ctx.stream2):",
            "btm_grad_out1_halo = fast_bottleneck.backward_grad_out1_halo_corr(ctx.explicit_nhwc, ctx.stride_1x1, t_list, w1by3, grads, btm_halo, btm_relu_halo, btm_grad_out1.clone())",
            "btm_grad_out1.copy_(btm_grad_out1_halo)",
            "if ctx.spatial_group_rank > 0:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=stream1), value='stream2')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3109,
        "neg_line": [
            "-with torch.cuda.stream(ctx.stream1):"
        ],
        "pos_line": [
            "+with torch.cuda.stream(ctx.stream2):"
        ],
        "core_change": "-with torch.cuda.stream(ctx.stream1): +with torch.cuda.stream(ctx.stream2):",
        "core_API": "wait_stream"
    },
    {
        "commit_hash": "1ebbacf11aa8eaaa0b715737778ac34598ce5a8d",
        "index": "26d8d3063..0bb9d141f 100644",
        "commit_message": "Fix (roughly) spdz and fix a test on hook\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_hook_args_and_cmd_signature_malleability():",
            "assert (r1 == syft.LoggingTensor().on(torch.tensor([2.0, 4]))).all()",
            "",
            "r2 = a + 1",
            "-    assert (r2 == syft.LoggingTensor().on(torch.tensor([2.0, 4]))).all()",
            "+    assert (r2 == syft.LoggingTensor().on(torch.tensor([2.0, 3]))).all()",
            "",
            "r3 = a + b",
            "assert (r3 == syft.LoggingTensor().on(torch.tensor([2.0, 4]))).all()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=4), value='3')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3112,
        "neg_line": [
            "-assert (r2 == syft.LoggingTensor().on(torch.tensor([2.0, 4]))).all()"
        ],
        "pos_line": [
            "+assert (r2 == syft.LoggingTensor().on(torch.tensor([2.0, 3]))).all()"
        ],
        "core_change": "-assert (r2 == syft.LoggingTensor().on(torch.tensor([2.0, 4]))).all() +assert (r2 == syft.LoggingTensor().on(torch.tensor([2.0, 3]))).all()",
        "core_API": "LoggingTensor"
    },
    {
        "commit_hash": "c74f40cbe86047da9723cf4d68f6baddec4a3f6f",
        "index": "83f320ae..27ce5c1f 100644",
        "commit_message": "Fix build test. (#1057)\n\n* 2.2.1 release\n\n* Fix build test.\n\n* apply yapf\n\n* ping yapf to 0.28.0\n\n* fix build\n\n* use yapf 0.29\n\n* fix yapf\n\n* include tests in make format.\n\n* ping autoflake and isort version.\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def conv_module(",
            "",
            "",
            "def dense_module(",
            "-        prev_layer, n_units, is_train, use_batchnorm=True, activation_fn=None,",
            "-        dense_init=tl.initializers.random_uniform(),",
            "-        batch_norm_init=tl.initializers.truncated_normal(mean=1.,",
            "-                                                         stddev=0.02), bias_init=tf.zeros_initializer(), name=None",
            "+    prev_layer, n_units, is_train, use_batchnorm=True, activation_fn=None, dense_init=tl.initializers.random_uniform(),",
            "+    batch_norm_init=tl.initializers.truncated_normal(mean=1., stddev=0.02), bias_init=tf.zeros_initializer(), name=None",
            "):",
            "",
            "if activation_fn not in [\"ReLU\", \"ReLU6\", \"Leaky_ReLU\", \"PReLU\", \"PReLU6\", \"PTReLU6\", \"CReLU\", \"ELU\", \"SELU\","
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 4,
        "AST_diff_line": 17,
        "number": 3114,
        "neg_line": [
            "-prev_layer, n_units, is_train, use_batchnorm=True, activation_fn=None,",
            "-dense_init=tl.initializers.random_uniform(),",
            "-batch_norm_init=tl.initializers.truncated_normal(mean=1.,",
            "-stddev=0.02), bias_init=tf.zeros_initializer(), name=None"
        ],
        "pos_line": [
            "+prev_layer, n_units, is_train, use_batchnorm=True, activation_fn=None, dense_init=tl.initializers.random_uniform(),",
            "+batch_norm_init=tl.initializers.truncated_normal(mean=1., stddev=0.02), bias_init=tf.zeros_initializer(), name=None"
        ],
        "core_change": "-prev_layer, n_units, is_train, use_batchnorm=True, activation_fn=None, -dense_init=tl.initializers.random_uniform(), -batch_norm_init=tl.initializers.truncated_normal(mean=1., -stddev=0.02), bias_init=tf.zeros_initializer(), name=None +prev_layer, n_units, is_train, use_batchnorm=True, activation_fn=None, dense_init=tl.initializers.random_uniform(), +batch_norm_init=tl.initializers.truncated_normal(mean=1., stddev=0.02), bias_init=tf.zeros_initializer(), name=None",
        "core_API": "random_uniform"
    },
    {
        "commit_hash": "b3ebc18bcb0b4034fdfbc90fc3d79c9786cb59c1",
        "index": "daba22316..8cdaba833 100644",
        "commit_message": "Hardware specific parts of Accelerator Refactoring (#5719)\n\n* add basic accelerator class.\nCo-Authored with @awaelchi\n\n* pep8\n\nCo-authored-by: @awaelchi\n\n* add cpu accelerator\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* add gpu accelerator\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* add tpu accelerator\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* add accelerator connector\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* add single device training\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* add single tpu\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* add tpu spawn\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* make on_colab_kaggle utility func\n\n* add basic accelerator class.\nCo-Authored with @awaelchi\n\n* pep8\n\nCo-authored-by: @awaelchi\n\n* add cpu accelerator\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* add gpu accelerator\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* add tpu accelerator\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* add accelerator connector\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* add single device training\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* add single tpu\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* add tpu spawn\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\n\n* make on_colab_kaggle utility func\n\n* fixes\n\n* move\n\n* yapf\n\n* .\n\n* .\n\n* .\n\n* flake8\n\n* sync accelerator connector changes from dev1.2\n\n* changelog\n\n* fix tpu handling\n\n* tpu\n\n* aval\n\n* yapf\n\n* Update pytorch_lightning/plugins/training_type/tpu_spawn.py\n\nCo-authored-by: chaton <thomas@grid.ai>\n\n* Update pytorch_lightning/accelerators/accelerator_connector.py\n\nCo-authored-by: chaton <thomas@grid.ai>\n\n* Update pytorch_lightning/plugins/training_type/tpu_spawn.py\n\nCo-authored-by: chaton <thomas@grid.ai>\n\n* Update tpu_spawn.py\n\n* Update pytorch_lightning/accelerators/accelerator_connector.py\n\nCo-authored-by: chaton <thomas@grid.ai>\n\n* indentation\n\nCo-authored-by: Adrian Wlchli <aedu.waelchli@gmail.com>\nCo-authored-by: Jirka Borovec <jirka.borovec@seznam.cz>\nCo-authored-by: chaton <thomas@grid.ai>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class NativeMixedPrecisionPlugin(MixedPrecisionPlugin):",
            "return closure_loss",
            "",
            "@contextmanager",
            "-    def train_step_context(self) -> Generator[torch.cuda.amp.autocast, None, None]:",
            "+    def train_step_context(self) -> Generator[autocast, None, None]:",
            "\"\"\"Enable autocast context\"\"\"",
            "yield torch.cuda.amp.autocast()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=subscript), node=ASTNode(type=identifier, text=autocast), position=2)",
            "Insert(target_node=ASTNode(type=subscript), node=(',', ','), position=3, insert_id=549220)",
            "Insert(target_node=ASTNode(type=subscript), node=('none', 'None'), position=4, insert_id=549221)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cuda))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=amp))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=none, text=None))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 3116,
        "neg_line": [
            "-def train_step_context(self) -> Generator[torch.cuda.amp.autocast, None, None]:"
        ],
        "pos_line": [
            "+def train_step_context(self) -> Generator[autocast, None, None]:"
        ],
        "core_change": "-def train_step_context(self) -> Generator[torch.cuda.amp.autocast, None, None]: +def train_step_context(self) -> Generator[autocast, None, None]:",
        "core_API": "autocast"
    },
    {
        "commit_hash": "c4c532d25e012dbe6ab1ac14bca75e53e0acc621",
        "index": "8db231fb..6f5f0d02 100644",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestBiMPMMatching(AllenNlpTestCase):",
            "test1_fw, test1_bw = torch.split(test1, d // 2, dim=-1)",
            "test2_fw, test2_bw = torch.split(test2, d // 2, dim=-1)",
            "",
            "-        ml_fw = BiMpmMatching.from_params(Params({\"is_forward\": True, \"num_perspectives\": l}))",
            "-        ml_bw = BiMpmMatching.from_params(Params({\"is_forward\": False, \"num_perspectives\": l}))",
            "+        ml_fw = BiMpmMatching.from_params(Params({\"is_forward\": True, \"num_perspectives\": n}))",
            "+        ml_bw = BiMpmMatching.from_params(Params({\"is_forward\": False, \"num_perspectives\": n}))",
            "",
            "vecs_p_fw, vecs_h_fw = ml_fw(test1_fw, mask1, test2_fw, mask2)",
            "vecs_p_bw, vecs_h_bw = ml_bw(test1_bw, mask1, test2_bw, mask2)",
            "vecs_p, vecs_h = torch.cat(vecs_p_fw + vecs_p_bw, dim=2), torch.cat(vecs_h_fw + vecs_h_bw, dim=2)",
            "",
            "-        assert vecs_p.size() == torch.Size([batch, len1, 10 + 10 * l])",
            "-        assert vecs_h.size() == torch.Size([batch, len2, 10 + 10 * l])",
            "+        assert vecs_p.size() == torch.Size([batch, len1, 10 + 10 * n])",
            "+        assert vecs_h.size() == torch.Size([batch, len2, 10 + 10 * n])",
            "assert ml_fw.get_output_dim() == ml_bw.get_output_dim() == vecs_p.size(2) // 2 == vecs_h.size(2) // 2"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=23855)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=23856)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'BiMpmMatching'), position=0, insert_id=23857)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=23858)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'from_params'), position=2, insert_id=23859)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=23860)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=23861)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Size'), position=2, insert_id=23862)",
            "Update(target_node=ASTNode(type=identifier, text=l), value='n')",
            "Update(target_node=ASTNode(type=identifier, text=l), value='n')",
            "Update(target_node=ASTNode(type=identifier, text=l), value='n')",
            "Update(target_node=ASTNode(type=identifier, text=l), value='n')",
            "Delete(target_node=ASTNode(type=identifier, text=BiMpmMatching))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=from_params))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=Size))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 22,
        "number": 3118,
        "neg_line": [
            "-ml_fw = BiMpmMatching.from_params(Params({\"is_forward\": True, \"num_perspectives\": l}))",
            "-ml_bw = BiMpmMatching.from_params(Params({\"is_forward\": False, \"num_perspectives\": l}))",
            "-assert vecs_p.size() == torch.Size([batch, len1, 10 + 10 * l])",
            "-assert vecs_h.size() == torch.Size([batch, len2, 10 + 10 * l])"
        ],
        "pos_line": [
            "+ml_fw = BiMpmMatching.from_params(Params({\"is_forward\": True, \"num_perspectives\": n}))",
            "+ml_bw = BiMpmMatching.from_params(Params({\"is_forward\": False, \"num_perspectives\": n}))",
            "+assert vecs_p.size() == torch.Size([batch, len1, 10 + 10 * n])",
            "+assert vecs_h.size() == torch.Size([batch, len2, 10 + 10 * n])"
        ],
        "core_change": "-ml_fw = BiMpmMatching.from_params(Params({\"is_forward\": True, \"num_perspectives\": l})) -ml_bw = BiMpmMatching.from_params(Params({\"is_forward\": False, \"num_perspectives\": l})) +ml_fw = BiMpmMatching.from_params(Params({\"is_forward\": True, \"num_perspectives\": n})) +ml_bw = BiMpmMatching.from_params(Params({\"is_forward\": False, \"num_perspectives\": n})) -assert vecs_p.size() == torch.Size([batch, len1, 10 + 10 * l]) -assert vecs_h.size() == torch.Size([batch, len2, 10 + 10 * l]) +assert vecs_p.size() == torch.Size([batch, len1, 10 + 10 * n]) +assert vecs_h.size() == torch.Size([batch, len2, 10 + 10 * n])",
        "core_API": "split"
    },
    {
        "commit_hash": "35fe3213ef5b1c566c4d73a9d90b0c2aace04f88",
        "index": "23759d1e..fa47795f 100644",
        "commit_message": "RANSAC improvements (#1435)\n\n- Added LU solver for RANSAC-homography, it speeds-up the minimal-solver part x10\n- Now find_fundamental weights is optional, as it was for homography\n- Fix test precision and made one xFail test to be compulsory\n- Fixed tests for fundamental matrix, as one have to have >= 8 points (also added to docstring)\n- updates deprecated torch.svd to torch.linalg.svd\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def solve_pnp_dlt(",
            "# Checking if world_points_norm (of any element of the batch) has rank = 3. This",
            "# function cannot be used if all world points (of any element of the batch) lie",
            "# on a line or if all world points (of any element of the batch) lie on a plane.",
            "-    _, s, _ = torch.svd(world_points_norm)",
            "+    s = torch.linalg.svdvals(world_points_norm)",
            "if torch.any(s[:, -1] < svd_eps):",
            "raise AssertionError(",
            "f\"The last singular value of one/more of the elements of the batch is smaller \""
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=398973)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=398974)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'svdvals'), position=2, insert_id=398975)",
            "Update(target_node=ASTNode(type=identifier, text=svd), value='linalg')",
            "Delete(target_node=ASTNode(type=identifier, text=_))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=_))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 3120,
        "neg_line": [
            "-_, s, _ = torch.svd(world_points_norm)"
        ],
        "pos_line": [
            "+s = torch.linalg.svdvals(world_points_norm)"
        ],
        "core_change": "-_, s, _ = torch.svd(world_points_norm) +s = torch.linalg.svdvals(world_points_norm)",
        "core_API": "svd"
    },
    {
        "commit_hash": "178854918b1f1abd2942214926ca1a883949838c",
        "index": "da963d34..be971c33 100644",
        "commit_message": "Improve docs (#331)\n\n* check layers docs\n\n* check cost docs\n\n* check prepro\n\n* fixed DeformableConv2d TODO\n\n* check iterate\n\n* check utils\n\n* check nlp\n\n* check rein\n\n* check files\n\n* check visulize\n\n* check activation\n\n* yapf\n\n* fix utils docs\n\n* add ops back\n\n* add supress stdout back for compatibility.\n\n* add an alias for ops.supress_output\n\n* fix indentation\n\n* fix all docs warnings.\n\n* fix luomai comment\n\n* nlp\n\n* fix nlp typos.\n\n",
        "file": "TensorLayer.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _RoiPoolingShape(op):",
            "pool_width = op.get_attr('pool_width')",
            "",
            "#TODO: check the width/hegiht order",
            "-    return [tf.TensorShape([n_rois, n_channels, pool_width, pool_height]),",
            "-            tf.TensorShape(None)]",
            "+    return [tf.TensorShape([n_rois, n_channels, pool_width, pool_height]), tf.TensorShape(None)]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 3121,
        "neg_line": [
            "-return [tf.TensorShape([n_rois, n_channels, pool_width, pool_height]),",
            "-tf.TensorShape(None)]"
        ],
        "pos_line": [
            "+return [tf.TensorShape([n_rois, n_channels, pool_width, pool_height]), tf.TensorShape(None)]"
        ],
        "core_change": "-return [tf.TensorShape([n_rois, n_channels, pool_width, pool_height]), -tf.TensorShape(None)] +return [tf.TensorShape([n_rois, n_channels, pool_width, pool_height]), tf.TensorShape(None)]",
        "core_API": "get_attr"
    },
    {
        "commit_hash": "761f77f0fe0a0bdb06042e6f2b6312e1165ba007",
        "index": "5b4ab3ebf2..16e03b2358 100644",
        "commit_message": "small format fixes\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "def choose_random_framework(excluded=None):",
            "while True:",
            "if len(excluded) == 5:",
            "raise Exception(",
            "-                \"Unable to select framework, all backends are either excluded or not installed.\"",
            "+                \"Unable to select framework, all backends are either excluded \"",
            "+                \"or not installed.\"",
            ")",
            "f = np.random.choice(",
            "[f_srt for f_srt in list(FW_DICT.keys()) if f_srt not in excluded]"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('concatenated_string', None), position=1, insert_id=1731929)",
            "Update(target_node=ASTNode(type=string, text=\"Unable to select framework, all backends are either excluded or not installed.\"), value='\"Unable to select framework, all backends are either excluded \"')",
            "Move(target_node=IN(type=concatenated_string), node=ASTNode(type=string, text=\"Unable to select framework, all backends are either excluded or not installed.\"), position=0)",
            "Insert(target_node=IN(type=concatenated_string), node=('string', '\"or not installed.\"'), position=1, insert_id=1731930)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 3126,
        "neg_line": [
            "-\"Unable to select framework, all backends are either excluded or not installed.\""
        ],
        "pos_line": [
            "+\"Unable to select framework, all backends are either excluded \"",
            "+\"or not installed.\""
        ],
        "core_change": "-\"Unable to select framework, all backends are either excluded or not installed.\" +\"Unable to select framework, all backends are either excluded \" +\"or not installed.\"",
        "core_API": "choice"
    },
    {
        "commit_hash": "fe65657de112531a2d5303491f245f9e7534ae8d",
        "index": "4205ef2eb..a0f9b3bd0 100644",
        "commit_message": "Fix FP16 inference in TextGenerationPipeline (#20913)\n\n* add torch_dtype attribute to Pipeline\n\n* Use torch_dtype to cast input tensor type in AutomaticSpeechRecognitionPipeline\n\n* Fix code quality\n\n* Add TextGenerationPipeline fp16 test\n\n* Fix code quality\n\n* Remove useless require in tests\n\nCo-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>\n\nCo-authored-by: Nicolas Patry <patry.nicolas@protonmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Pipeline(_ScikitCompat):",
            "self.device = torch.device(f\"cuda:{device}\")",
            "else:",
            "self.device = device",
            "+        self.torch_dtype = torch_dtype",
            "self.binary_output = binary_output",
            "",
            "# Special handling"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1180668)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1180669)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=1180670)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1180671)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'torch_dtype'), position=2, insert_id=1180672)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1180673)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1180674)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch_dtype'), position=2, insert_id=1180675)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 8,
        "number": 3128,
        "neg_line": [],
        "pos_line": [
            "+self.torch_dtype = torch_dtype"
        ],
        "core_change": "+self.torch_dtype = torch_dtype",
        "core_API": "device"
    },
    {
        "commit_hash": "6d839c8290b67dabfad9c1ffdc7e428c6030a499",
        "index": "556f8dfe..46c8d4de 100644",
        "commit_message": "Convert docstrings to active tense (#1275)\n\n* Convert docstrings to active tense\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def load_depth(file_name):",
            "",
            "",
            "def load_camera_data(file_name):",
            "-    \"\"\"Loads the camera data using the syntel SDK and converts to torch.Tensor.\"\"\"",
            "+    \"\"\"Load the camera data using the syntel SDK and converts to torch.Tensor.\"\"\"",
            "if not os.path.isfile(file_name):",
            "raise AssertionError(f\"Invalid file {file_name}\")",
            "import sintel_io"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Loads the camera data using the syntel SDK and converts to torch.Tensor.\"\"\"), value='\"\"\"Load the camera data using the syntel SDK and converts to torch.Tensor.\"\"\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3129,
        "neg_line": [
            "-\"\"\"Loads the camera data using the syntel SDK and converts to torch.Tensor.\"\"\""
        ],
        "pos_line": [
            "+\"\"\"Load the camera data using the syntel SDK and converts to torch.Tensor.\"\"\""
        ],
        "core_change": "-\"\"\"Loads the camera data using the syntel SDK and converts to torch.Tensor.\"\"\" +\"\"\"Load the camera data using the syntel SDK and converts to torch.Tensor.\"\"\"",
        "core_API": "isfile"
    },
    {
        "commit_hash": "1e4b436d5174bf4895f77538e1dca5e6b06af7b7",
        "index": "bc3afb9e..47bb57e8 100644",
        "commit_message": "bugfixes gcn\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GCNTest(TestCase):",
            "conv = GCN(1, 10)",
            "i = torch.LongTensor([[0, 0, 1, 1, 2, 2], [1, 2, 0, 2, 0, 1]])",
            "w = torch.FloatTensor([1, 1, 1, 1, 1, 1])",
            "-        adj = Variable(torch.sparse.FloatTensor(i, w, torch.Size([3, 3])))",
            "+        adj = torch.sparse.FloatTensor(i, w, torch.Size([3, 3]))",
            "features = Variable(torch.FloatTensor([[1], [2], [3]]))",
            "",
            "out = conv(adj, features)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 3130,
        "neg_line": [
            "-adj = Variable(torch.sparse.FloatTensor(i, w, torch.Size([3, 3])))"
        ],
        "pos_line": [
            "+adj = torch.sparse.FloatTensor(i, w, torch.Size([3, 3]))"
        ],
        "core_change": "-adj = Variable(torch.sparse.FloatTensor(i, w, torch.Size([3, 3]))) +adj = torch.sparse.FloatTensor(i, w, torch.Size([3, 3]))",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "c21764e2..c72c2d4e 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LabelField(Field[torch.Tensor]):",
            "@overrides",
            "def as_tensor(self,",
            "padding_lengths: Dict[str, int],",
            "-                  cuda_device: int = -1,",
            "-                  for_training: bool = True) -> torch.Tensor:",
            "+                  cuda_device: int = -1) -> torch.Tensor:",
            "# pylint: disable=unused-argument",
            "-        tensor = Variable(torch.LongTensor([self._label_id]), volatile=not for_training)",
            "+        tensor = torch.LongTensor([self._label_id])",
            "return tensor if cuda_device == -1 else tensor.cuda(cuda_device)",
            "",
            "@overrides"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=for_training))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=bool))",
            "Delete(target_node=ASTNode(type=type))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=typed_default_parameter))",
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=volatile))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=not, text=not))",
            "Delete(target_node=ASTNode(type=identifier, text=for_training))",
            "Delete(target_node=ASTNode(type=not_operator))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 21,
        "number": 3131,
        "neg_line": [
            "-cuda_device: int = -1,",
            "-for_training: bool = True) -> torch.Tensor:",
            "-tensor = Variable(torch.LongTensor([self._label_id]), volatile=not for_training)"
        ],
        "pos_line": [
            "+cuda_device: int = -1) -> torch.Tensor:",
            "+tensor = torch.LongTensor([self._label_id])"
        ],
        "core_change": "-cuda_device: int = -1, -for_training: bool = True) -> torch.Tensor: +cuda_device: int = -1) -> torch.Tensor: -tensor = Variable(torch.LongTensor([self._label_id]), volatile=not for_training) +tensor = torch.LongTensor([self._label_id])",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "9738eb1999bdaa02f9c2874c58896617aeb91823",
        "index": "707d7ee91..cd5e2314e 100644",
        "commit_message": "fix errors in egs2/wsj0_2mix_spatialized/enh1; fix bugs in _permutation_loss and beamformer_net; initialize egs2/chime4/enh1\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class BeamformerNet(AbsEnhancement):",
            "enhanced = [torch.stack([enh.real, enh.imag], dim=-1) for enh in enhanced]",
            "else:",
            "# single-speaker output",
            "-            enhanced = torch.stack([enhanced.real, enhanced.imag], dim=-1).float()",
            "+            enhanced = [torch.stack([enhanced.real, enhanced.imag], dim=-1)]",
            "return enhanced, flens, masks",
            "",
            "def forward_rawwav(self, input: torch.Tensor, ilens: torch.Tensor):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('list', None), position=4, insert_id=145788)",
            "Insert(target_node=IN(type=list), node=('[', '['), position=0, insert_id=145789)",
            "Move(target_node=IN(type=list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=list), node=(']', ']'), position=2, insert_id=145790)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=float))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 3138,
        "neg_line": [
            "-enhanced = torch.stack([enhanced.real, enhanced.imag], dim=-1).float()"
        ],
        "pos_line": [
            "+enhanced = [torch.stack([enhanced.real, enhanced.imag], dim=-1)]"
        ],
        "core_change": "-enhanced = torch.stack([enhanced.real, enhanced.imag], dim=-1).float() +enhanced = [torch.stack([enhanced.real, enhanced.imag], dim=-1)]",
        "core_API": "stack"
    },
    {
        "commit_hash": "607814feb9b44977b83812dc38ee952c8ae61f84",
        "index": "bdd95591..c17ef493 100644",
        "commit_message": "Fix bug in fp32 optimizer state loading (#289)\n\n\n",
        "file": "DeepSpeed.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SimpleOptimizer(torch.optim.Optimizer):",
            "return loss",
            "",
            "",
            "-def random_dataloader(model, total_samples, hidden_dim, device):",
            "+def random_dataloader(model, total_samples, hidden_dim, device, dtype=torch.half):",
            "batch_size = model.train_micro_batch_size_per_gpu()",
            "-    train_data = torch.randn(total_samples, hidden_dim, device=device, dtype=torch.half)",
            "+    train_data = torch.randn(total_samples, hidden_dim, device=device, dtype=dtype)",
            "train_label = torch.empty(total_samples,",
            "dtype=torch.long,",
            "device=device).random_(hidden_dim)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=8, insert_id=86219)",
            "Insert(target_node=ASTNode(type=parameters), node=('default_parameter', None), position=9, insert_id=86220)",
            "Insert(target_node=IN(type=default_parameter), node=('identifier', 'dtype'), position=0, insert_id=86221)",
            "Insert(target_node=IN(type=default_parameter), node=('=', '='), position=1, insert_id=86222)",
            "Move(target_node=IN(type=default_parameter), node=ASTNode(type=attribute), position=2)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'dtype'), position=2, insert_id=86223)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 3142,
        "neg_line": [
            "-def random_dataloader(model, total_samples, hidden_dim, device):",
            "-train_data = torch.randn(total_samples, hidden_dim, device=device, dtype=torch.half)"
        ],
        "pos_line": [
            "+def random_dataloader(model, total_samples, hidden_dim, device, dtype=torch.half):",
            "+train_data = torch.randn(total_samples, hidden_dim, device=device, dtype=dtype)"
        ],
        "core_change": "-def random_dataloader(model, total_samples, hidden_dim, device): +def random_dataloader(model, total_samples, hidden_dim, device, dtype=torch.half): -train_data = torch.randn(total_samples, hidden_dim, device=device, dtype=torch.half) +train_data = torch.randn(total_samples, hidden_dim, device=device, dtype=dtype)",
        "core_API": "train_micro_batch_size_per_gpu"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "f7aedf91..6010da28 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TextField(SequenceField[Dict[str, torch.Tensor]]):",
            "# than a LongTensor here, and it's not clear how to signal that.  Maybe we'll need to",
            "# add a class method to TokenIndexer to tell us the type?  But we can worry about that",
            "# when there's a compelling use case for it.",
            "-            tensor = Variable(torch.LongTensor(padded_array), volatile=not for_training)",
            "+            tensor = torch.LongTensor(padded_array)",
            "tensors[indexer_name] = tensor if cuda_device == -1 else tensor.cuda(cuda_device)",
            "return tensors"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=Variable))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=volatile))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=not, text=not))",
            "Delete(target_node=ASTNode(type=identifier, text=for_training))",
            "Delete(target_node=ASTNode(type=not_operator))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 11,
        "number": 3143,
        "neg_line": [
            "-tensor = Variable(torch.LongTensor(padded_array), volatile=not for_training)"
        ],
        "pos_line": [
            "+tensor = torch.LongTensor(padded_array)"
        ],
        "core_change": "-tensor = Variable(torch.LongTensor(padded_array), volatile=not for_training) +tensor = torch.LongTensor(padded_array)",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "8f63de41fccdf5a0ddbb51692ecfc5f032e07974",
        "index": "71983c4..fe41aa6 100644",
        "commit_message": "fix fake8\n\n",
        "file": "TensorFlowTTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class WeightNormalization(WeightNormalizationOriginal):",
            "self.built = True",
            "",
            "def call(self, inputs):",
            "-        \"\"\"Call `Layer`\"\"\"",
            "+        \"\"\"Call `Layer`.\"\"\"",
            "",
            "def _do_nothing():",
            "return tf.identity(self.g)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"\"\"Call `Layer`\"\"\"), value='\"\"\"Call `Layer`.\"\"\"')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3144,
        "neg_line": [
            "-\"\"\"Call `Layer`\"\"\""
        ],
        "pos_line": [
            "+\"\"\"Call `Layer`.\"\"\""
        ],
        "core_change": "-\"\"\"Call `Layer`\"\"\" +\"\"\"Call `Layer`.\"\"\"",
        "core_API": "identity"
    },
    {
        "commit_hash": "3e8d90fd80d377c01ce4c4b903986d4b7d10d78b",
        "index": "e93f7e6fe..68ca2af9b 100644",
        "commit_message": "handles for left_context=0 + fix right_context default values\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Conformer(torch.nn.Module):",
            "",
            "residual = x",
            "x = self.norm_self_att(x)",
            "-        key = torch.cat([self.cache[0], x], dim=1)",
            "+        if left_context > 0:",
            "+            key = torch.cat([self.cache[0], x], dim=1)",
            "+        else:",
            "+            key = x",
            "val = key",
            "",
            "if right_context > 0:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=121135)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=121136)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=121137)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=121138)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=121139)",
            "Insert(target_node=IN(type=if_statement), node=('else_clause', None), position=4, insert_id=121140)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'left_context'), position=0, insert_id=121141)",
            "Insert(target_node=IN(type=comparison_operator), node=('>', '>'), position=1, insert_id=121142)",
            "Insert(target_node=IN(type=comparison_operator), node=('integer', '0'), position=2, insert_id=121143)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Insert(target_node=IN(type=else_clause), node=('else', 'else'), position=0, insert_id=121144)",
            "Insert(target_node=IN(type=else_clause), node=(':', ':'), position=1, insert_id=121145)",
            "Insert(target_node=IN(type=else_clause), node=('block', None), position=2, insert_id=121146)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=121147)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=121148)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'key'), position=0, insert_id=121149)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=121150)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'x'), position=2, insert_id=121151)"
        ],
        "plus_line": 4,
        "minus_line": 1,
        "AST_diff_line": 18,
        "number": 3146,
        "neg_line": [
            "-key = torch.cat([self.cache[0], x], dim=1)"
        ],
        "pos_line": [
            "+if left_context > 0:",
            "+key = torch.cat([self.cache[0], x], dim=1)",
            "+else:",
            "+key = x"
        ],
        "core_change": "-key = torch.cat([self.cache[0], x], dim=1) +if left_context > 0: +key = torch.cat([self.cache[0], x], dim=1) +else: +key = x",
        "core_API": "norm_self_att"
    },
    {
        "commit_hash": "2708b4e882e4c635a62757c387925537c50f03ca",
        "index": "5117217230..63936fb79f 100644",
        "commit_message": "fix unravel_index of tensorflow backend as tf.constant does not accept the product of reverse(<list>)\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def unravel_index(",
            "for dim in reversed(shape):",
            "output.append(temp % dim)",
            "temp = temp // dim",
            "-    ret = tf.constant(reversed(output), dtype=tf.int32)",
            "+    output.reverse()",
            "+    ret = tf.constant(output, dtype=tf.int32)",
            "return tuple(ret)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1961637)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1961638)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1961639)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1961640)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'output'), position=0, insert_id=1961641)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1961642)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reverse'), position=2, insert_id=1961643)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1961644)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1961645)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=output), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=reversed))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 3149,
        "neg_line": [
            "-ret = tf.constant(reversed(output), dtype=tf.int32)"
        ],
        "pos_line": [
            "+output.reverse()",
            "+ret = tf.constant(output, dtype=tf.int32)"
        ],
        "core_change": "-ret = tf.constant(reversed(output), dtype=tf.int32) +output.reverse() +ret = tf.constant(output, dtype=tf.int32)",
        "core_API": "append"
    },
    {
        "commit_hash": "995c204337d16a6146a433cee360e5a5bfbc9a6f",
        "index": "638fef0f..4a17d718 100644",
        "commit_message": "Data2vec prelim (#2929)\n\nSummary:\nPreliminaries for data2vec release, include some minor improvements and bug fixes\n\nMost important change is that we now default to raising an exception when fields in config do not have a corresponding field in the model dataclass\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2929\n\nReviewed By: wnhsu\n\nDifferential Revision: D33649708\n\nPulled By: alexeib\n\nfbshipit-source-id: 629bdb4c361550740b451c570c2005bb956c6fcb\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TransformerEncoderLayerBase(nn.Module):",
            "def _get_fc_rank(self, remove_num: int) -> List[int]:",
            "f1_filter_param = []",
            "for i in range(self.fc1.out_features):",
            "-            f1_filter_param.append(torch.sum(torch.abs(self.fc1.weight[i])) + torch.sum(torch.abs(self.fc2.weight[:, i])) + torch.abs(self.fc1.bias[i]))",
            "-        return sorted(range(len(f1_filter_param)), key=lambda k: f1_filter_param[k], reverse=False)[0:remove_num]",
            "+            f1_filter_param.append(",
            "+                torch.sum(torch.abs(self.fc1.weight[i]))",
            "+                + torch.sum(torch.abs(self.fc2.weight[:, i]))",
            "+                + torch.abs(self.fc1.bias[i])",
            "+            )",
            "+        return sorted(",
            "+            range(len(f1_filter_param)), key=lambda k: f1_filter_param[k], reverse=False",
            "+        )[0:remove_num]",
            "",
            "def _prune_fc_layer(self, remove_index: List[int]):",
            "new_fc1_weight = []"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 8,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 3153,
        "neg_line": [
            "-f1_filter_param.append(torch.sum(torch.abs(self.fc1.weight[i])) + torch.sum(torch.abs(self.fc2.weight[:, i])) + torch.abs(self.fc1.bias[i]))",
            "-return sorted(range(len(f1_filter_param)), key=lambda k: f1_filter_param[k], reverse=False)[0:remove_num]"
        ],
        "pos_line": [
            "+f1_filter_param.append(",
            "+torch.sum(torch.abs(self.fc1.weight[i]))",
            "++ torch.sum(torch.abs(self.fc2.weight[:, i]))",
            "++ torch.abs(self.fc1.bias[i])",
            "+)",
            "+return sorted(",
            "+range(len(f1_filter_param)), key=lambda k: f1_filter_param[k], reverse=False",
            "+)[0:remove_num]"
        ],
        "core_change": "-f1_filter_param.append(torch.sum(torch.abs(self.fc1.weight[i])) + torch.sum(torch.abs(self.fc2.weight[:, i])) + torch.abs(self.fc1.bias[i])) -return sorted(range(len(f1_filter_param)), key=lambda k: f1_filter_param[k], reverse=False)[0:remove_num] +f1_filter_param.append( +torch.sum(torch.abs(self.fc1.weight[i])) ++ torch.sum(torch.abs(self.fc2.weight[:, i])) ++ torch.abs(self.fc1.bias[i]) +) +return sorted( +range(len(f1_filter_param)), key=lambda k: f1_filter_param[k], reverse=False +)[0:remove_num]",
        "core_API": "append"
    },
    {
        "commit_hash": "6a6e1c7602df5dc112c99792681422a529d9a13e",
        "index": "eb9114a727..85688a55cc 100644",
        "commit_message": "Fixing out arg (#1770)\n\n* Updated out arg in functional ivy activations\n\n* Updated out arg docstring in activations\n\n* Updated out arg in functional ivy creation\n\n* Updated out arg in functional ivy dtype\n\n* Updated out arg in functional ivy elementwise\n\n* Updated out arg in functional ivy general\n\n* Updated out arg in functional ivy gradients\n\n* Updated out arg in functional ivy layers\n\n* Updated out arg in functional ivy linalg\n\n* Updated out arg in functional ivy manipulation\n\n* Updated out arg in functional ivy random\n\n* Updated out arg in functional ivy searching\n\n* Updated out arg in functional ivy set\n\n* Updated out arg in functional ivy statistical\n\n* Updated out arg in functional ivy norms\n\n* Updated @handle-out-arg decorator in compositional functions\n\n* Updated out arg in functional ivy general\n\n* Updated out arg in ivy functional layers, searching and set\n\n* lint fixes\n\n* lint fixes in dtype and general\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def asarray(",
            "dtype = as_native_dtype((default_dtype(dtype, object_in)))",
            "",
            "if copy is True:",
            "-        return (",
            "-            torch.as_tensor(object_in, dtype=dtype)",
            "-            .clone()",
            "-            .detach()",
            "-            .to(device)",
            "-        )",
            "+        return torch.as_tensor(object_in, dtype=dtype).clone().detach().to(device)",
            "else:",
            "return torch.as_tensor(object_in, dtype=dtype).to(device)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=call), position=3)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=parenthesized_expression))"
        ],
        "plus_line": 1,
        "minus_line": 6,
        "AST_diff_line": 4,
        "number": 3155,
        "neg_line": [
            "-return (",
            "-torch.as_tensor(object_in, dtype=dtype)",
            "-.clone()",
            "-.detach()",
            "-.to(device)",
            "-)"
        ],
        "pos_line": [
            "+return torch.as_tensor(object_in, dtype=dtype).clone().detach().to(device)"
        ],
        "core_change": "-return ( -torch.as_tensor(object_in, dtype=dtype) -.clone() -.detach() -.to(device) -) +return torch.as_tensor(object_in, dtype=dtype).clone().detach().to(device)",
        "core_API": "as_tensor"
    },
    {
        "commit_hash": "3a55d553ddbbd76ba72467790f6ed634be306bc7",
        "index": "b05bdce..940d35b 100644",
        "commit_message": "Fix Fairseq FP16 Optimizer (#1460)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/1460\n\nAccounting for D23593137 in building FP16 PyText Optimizer\n\nReviewed By: mwu1993\n\nDifferential Revision: D23727851\n\nfbshipit-source-id: bc9445a3f32da8b18524fcfc2fd9eb99975d0362\n\n",
        "file": "pytext.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class FP16OptimizerFairseq(Fairseq_FP16OptimizerMixin, FP16Optimizer):",
            "",
            "# reset fp32_optimizer param groups to using master weights",
            "fp32_param_group = self.fp32_optimizer.param_groups[0]",
            "-        fp32_param_group[\"params\"] = [self.fp32_params]",
            "+        fp32_param_group[\"params\"] = [self.fp32_params[torch.cuda.current_device()]]",
            "self.fp32_optimizer.param_groups = []",
            "self.fp32_optimizer.add_param_group(fp32_param_group)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('subscript', None), position=1, insert_id=872877)",
            "Insert(target_node=ASTNode(type=list), node=(']', ']'), position=2, insert_id=872878)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=872879)",
            "Insert(target_node=IN(type=subscript), node=('call', None), position=2, insert_id=872880)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=], text=]), position=3)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=872881)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=872882)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=872883)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=872884)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'current_device'), position=2, insert_id=872885)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=872886)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=872887)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=872888)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=872889)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cuda'), position=2, insert_id=872890)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 3156,
        "neg_line": [
            "-fp32_param_group[\"params\"] = [self.fp32_params]"
        ],
        "pos_line": [
            "+fp32_param_group[\"params\"] = [self.fp32_params[torch.cuda.current_device()]]"
        ],
        "core_change": "-fp32_param_group[\"params\"] = [self.fp32_params] +fp32_param_group[\"params\"] = [self.fp32_params[torch.cuda.current_device()]]",
        "core_API": "current_device"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "f71ef9c8..f663c0c9 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def image_histogram2d(",
            "u = torch.abs(image.unsqueeze(0) - centers) / bandwidth",
            "",
            "if kernel == \"gaussian\":",
            "-        kernel_values = torch.exp(-0.5 * u ** 2)",
            "-    elif kernel in (\"triangular\", \"uniform\", \"epanechnikov\",):",
            "+        kernel_values = torch.exp(-0.5 * u**2)",
            "+    elif kernel in (\"triangular\", \"uniform\", \"epanechnikov\"):",
            "# compute the mask and cast to floating point",
            "mask = (u <= 1).to(u.dtype)",
            "if kernel == \"triangular\":",
            "-            kernel_values = (1. - u) * mask",
            "+            kernel_values = (1.0 - u) * mask",
            "elif kernel == \"uniform\":",
            "kernel_values = torch.ones_like(u) * mask",
            "else:  # kernel == \"epanechnikov\"",
            "-            kernel_values = (1. - u ** 2) * mask",
            "+            kernel_values = (1.0 - u**2) * mask",
            "else:",
            "raise ValueError(f\"Kernel must be 'triangular', 'gaussian', \" f\"'uniform' or 'epanechnikov'. Got {kernel}.\")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=1.), value='1.0')",
            "Update(target_node=ASTNode(type=float, text=1.), value='1.0')",
            "Delete(target_node=ASTNode(type=,, text=,))"
        ],
        "plus_line": 4,
        "minus_line": 4,
        "AST_diff_line": 3,
        "number": 3157,
        "neg_line": [
            "-kernel_values = torch.exp(-0.5 * u ** 2)",
            "-elif kernel in (\"triangular\", \"uniform\", \"epanechnikov\",):",
            "-kernel_values = (1. - u) * mask",
            "-kernel_values = (1. - u ** 2) * mask"
        ],
        "pos_line": [
            "+kernel_values = torch.exp(-0.5 * u**2)",
            "+elif kernel in (\"triangular\", \"uniform\", \"epanechnikov\"):",
            "+kernel_values = (1.0 - u) * mask",
            "+kernel_values = (1.0 - u**2) * mask"
        ],
        "core_change": "-kernel_values = torch.exp(-0.5 * u ** 2) -elif kernel in (\"triangular\", \"uniform\", \"epanechnikov\",): +kernel_values = torch.exp(-0.5 * u**2) +elif kernel in (\"triangular\", \"uniform\", \"epanechnikov\"): -kernel_values = (1. - u) * mask +kernel_values = (1.0 - u) * mask -kernel_values = (1. - u ** 2) * mask +kernel_values = (1.0 - u**2) * mask",
        "core_API": "abs"
    },
    {
        "commit_hash": "1a741f7e47d6ac52f7294c7ba34e0f42ddef28ae",
        "index": "5941854..0fe6292 100644",
        "commit_message": "fix device\n\n",
        "file": "deep-learning-for-image-processing.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def main():",
            "# download url: https://download.pytorch.org/models/mobilenet_v2-b0353104.pth",
            "model_weight_path = \"./mobilenet_v2.pth\"",
            "assert os.path.exists(model_weight_path), \"file {} dose not exist.\".format(model_weight_path)",
            "-    pre_weights = torch.load(model_weight_path, map_location=device)",
            "+    pre_weights = torch.load(model_weight_path, map_location='cpu')",
            "",
            "# delete classifier weights",
            "pre_dict = {k: v for k, v in pre_weights.items() if net.state_dict()[k].numel() == v.numel()}"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('string', \"'cpu'\"), position=2, insert_id=69027)",
            "Delete(target_node=ASTNode(type=identifier, text=device))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 3158,
        "neg_line": [
            "-pre_weights = torch.load(model_weight_path, map_location=device)"
        ],
        "pos_line": [
            "+pre_weights = torch.load(model_weight_path, map_location='cpu')"
        ],
        "core_change": "-pre_weights = torch.load(model_weight_path, map_location=device) +pre_weights = torch.load(model_weight_path, map_location='cpu')",
        "core_API": "exists"
    },
    {
        "commit_hash": "aaef09bc4e29adb9b0313e0550eded78887e0574",
        "index": "0e21529b..f2c3a36c 100644",
        "commit_message": "bugfix in ori\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LAFOrienter(nn.Module):",
            "self.patch_size,",
            "self.patch_size)",
            "angles_radians: torch.Tensor = self.angle_detector(patches).view(B, N)",
            "-        laf_out: torch.Tensor = set_laf_orientation(laf, rad2deg(angles_radians))",
            "+        prev_angle = get_laf_orientation(laf).view_as(angles_radians)",
            "+        laf_out: torch.Tensor = set_laf_orientation(laf, rad2deg(angles_radians) + prev_angle)",
            "return laf_out"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=429541)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=429542)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'prev_angle'), position=0, insert_id=429543)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=429544)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=429545)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=429546)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=429547)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=429548)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=429549)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'view_as'), position=2, insert_id=429550)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=429551)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'angles_radians'), position=1, insert_id=429552)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=429553)",
            "Insert(target_node=ASTNode(type=argument_list), node=('binary_operator', None), position=3, insert_id=429554)",
            "Insert(target_node=IN(type=call), node=('identifier', 'get_laf_orientation'), position=0, insert_id=429555)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=429556)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=429557)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'prev_angle'), position=2, insert_id=429558)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=429559)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'laf'), position=1, insert_id=429560)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=429561)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 3161,
        "neg_line": [
            "-laf_out: torch.Tensor = set_laf_orientation(laf, rad2deg(angles_radians))"
        ],
        "pos_line": [
            "+prev_angle = get_laf_orientation(laf).view_as(angles_radians)",
            "+laf_out: torch.Tensor = set_laf_orientation(laf, rad2deg(angles_radians) + prev_angle)"
        ],
        "core_change": "-laf_out: torch.Tensor = set_laf_orientation(laf, rad2deg(angles_radians)) +prev_angle = get_laf_orientation(laf).view_as(angles_radians) +laf_out: torch.Tensor = set_laf_orientation(laf, rad2deg(angles_radians) + prev_angle)",
        "core_API": "angle_detector"
    },
    {
        "commit_hash": "785a28cb85407bac9ae7738d9e17eaf1115a1d41",
        "index": "10aa5c2d..bb68ac69 100644",
        "commit_message": "fix black formatting\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RelationExtractor(flair.nn.DefaultClassifier[Sentence, Relation]):",
            "]",
            ")",
            "else:",
            "-            return torch.cat([span_1.tokens[0].get_embedding(embedding_names), span_2.tokens[0].get_embedding(embedding_names)])",
            "+            return torch.cat(",
            "+                [span_1.tokens[0].get_embedding(embedding_names), span_2.tokens[0].get_embedding(embedding_names)]",
            "+            )",
            "",
            "def _print_predictions(self, batch, gold_label_type):",
            "lines = []"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3165,
        "neg_line": [
            "-return torch.cat([span_1.tokens[0].get_embedding(embedding_names), span_2.tokens[0].get_embedding(embedding_names)])"
        ],
        "pos_line": [
            "+return torch.cat(",
            "+[span_1.tokens[0].get_embedding(embedding_names), span_2.tokens[0].get_embedding(embedding_names)]",
            "+)"
        ],
        "core_change": "-return torch.cat([span_1.tokens[0].get_embedding(embedding_names), span_2.tokens[0].get_embedding(embedding_names)]) +return torch.cat( +[span_1.tokens[0].get_embedding(embedding_names), span_2.tokens[0].get_embedding(embedding_names)] +)",
        "core_API": "cat"
    },
    {
        "commit_hash": "ed63b7ee9e68ba120893aaaaca0d4525a56ed484",
        "index": "f63a0fef..1683773d 100644",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ScalarMix(torch.nn.Module):",
            "",
            "normed_weights = torch.nn.functional.softmax(torch.cat([parameter for parameter",
            "in self.scalar_parameters]), dim=0)",
            "-        normed_weights = torch.split(normed_weights, split_size=1)",
            "+        normed_weights = torch.split(normed_weights, split_size_or_sections=1)",
            "",
            "if not self.do_layer_norm:",
            "pieces = []"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=split_size), value='split_size_or_sections')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3166,
        "neg_line": [
            "-normed_weights = torch.split(normed_weights, split_size=1)"
        ],
        "pos_line": [
            "+normed_weights = torch.split(normed_weights, split_size_or_sections=1)"
        ],
        "core_change": "-normed_weights = torch.split(normed_weights, split_size=1) +normed_weights = torch.split(normed_weights, split_size_or_sections=1)",
        "core_API": "softmax"
    },
    {
        "commit_hash": "127b7cc2f5e652dcbf7d444e7e69876a2d141d97",
        "index": "5c444f4c..96ebf0ff 100644",
        "commit_message": "fixes for regression tutorial (#511)\n\n* updated regression tutorial to be about regression, fixed the code to say prediction instead of latent, made tutorial consistent with example code\n\n* fixed linting\n\n* added extra sentence mentioning lifting for nn\n\n* fixing JPs comments\n\n* fixing JPs comments part 2\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def guide(data):",
            "priors = {'linear.weight': w_prior, 'linear.bias': b_prior}",
            "# overloading the parameters in the module with random samples from the prior",
            "lifted_module = pyro.random_module(\"module\", regression_model, priors)",
            "-    # sample a nn",
            "-    lifted_module()",
            "+    # sample a regressor",
            "+    return lifted_module()",
            "",
            "",
            "# instantiate optim and inference objects"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('return_statement', None), position=3, insert_id=758043)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=758044)",
            "Move(target_node=IN(type=return_statement), node=ASTNode(type=call), position=1)",
            "Delete(target_node=ASTNode(type=expression_statement))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 4,
        "number": 3167,
        "neg_line": [
            "-# sample a nn",
            "-lifted_module()"
        ],
        "pos_line": [
            "+# sample a regressor",
            "+return lifted_module()"
        ],
        "core_change": "-# sample a nn -lifted_module() +# sample a regressor +return lifted_module()",
        "core_API": "random_module"
    },
    {
        "commit_hash": "0c166dcfb8b9e9d79d013b7967a59dfb1ac4b804",
        "index": "c8681f75..7ac1cfd6 100644",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def generate_two_view_random_scene(",
            "P2 = scene['P'][1:2].to(device, dtype)",
            "",
            "# fundamental matrix",
            "-    F_mat = epi.fundamental_from_projections(",
            "-        P1[..., :3, :], P2[..., :3, :])",
            "+    F_mat = epi.fundamental_from_projections(P1[..., :3, :], P2[..., :3, :])",
            "",
            "F_mat = epi.normalize_transformation(F_mat)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 3170,
        "neg_line": [
            "-F_mat = epi.fundamental_from_projections(",
            "-P1[..., :3, :], P2[..., :3, :])"
        ],
        "pos_line": [
            "+F_mat = epi.fundamental_from_projections(P1[..., :3, :], P2[..., :3, :])"
        ],
        "core_change": "-F_mat = epi.fundamental_from_projections( -P1[..., :3, :], P2[..., :3, :]) +F_mat = epi.fundamental_from_projections(P1[..., :3, :], P2[..., :3, :])",
        "core_API": "fundamental_from_projections"
    },
    {
        "commit_hash": "bda4f25c05dcc666607c202b4b1134e249a9da12",
        "index": "41682eb1..b491ab83 100644",
        "commit_message": "replacing .repeat(...) with .expand(...) (#2059)\n\n* expand vs repeat\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: Nitai Fingerhut <nitai@Nitais-MacBook-Pro-3.local>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class RandomElasticTransform(GeometricAugmentationBase2D):",
            "def generate_parameters(self, shape: torch.Size) -> Dict[str, Tensor]:",
            "B, _, H, W = shape",
            "if self.same_on_batch:",
            "-            noise = torch.rand(1, 2, H, W, device=self.device, dtype=self.dtype).repeat(B, 1, 1, 1)",
            "+            noise = torch.rand(1, 2, H, W, device=self.device, dtype=self.dtype).expand(B, 2, H, W)",
            "else:",
            "noise = torch.rand(B, 2, H, W, device=self.device, dtype=self.dtype)",
            "return dict(noise=noise * 2 - 1)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=repeat), value='expand')",
            "Update(target_node=ASTNode(type=integer, text=1), value='2')",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'H'), position=5, insert_id=394773)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'W'), position=8, insert_id=394774)",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 3173,
        "neg_line": [
            "-noise = torch.rand(1, 2, H, W, device=self.device, dtype=self.dtype).repeat(B, 1, 1, 1)"
        ],
        "pos_line": [
            "+noise = torch.rand(1, 2, H, W, device=self.device, dtype=self.dtype).expand(B, 2, H, W)"
        ],
        "core_change": "-noise = torch.rand(1, 2, H, W, device=self.device, dtype=self.dtype).repeat(B, 1, 1, 1) +noise = torch.rand(1, 2, H, W, device=self.device, dtype=self.dtype).expand(B, 2, H, W)",
        "core_API": "rand"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "13efd60e..00872f55 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class SpatialSoftArgmax2d(nn.Module):",
            "See :func:`~kornia.geometry.subpix.spatial_soft_argmax2d` for details.",
            "\"\"\"",
            "",
            "-    def __init__(",
            "-        self, temperature: torch.Tensor = torch.tensor(1.0), normalized_coordinates: bool = True",
            "-    ) -> None:",
            "+    def __init__(self, temperature: torch.Tensor = torch.tensor(1.0), normalized_coordinates: bool = True) -> None:",
            "super().__init__()",
            "self.temperature: torch.Tensor = temperature",
            "self.normalized_coordinates: bool = normalized_coordinates"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 17,
        "number": 3174,
        "neg_line": [
            "-def __init__(",
            "-self, temperature: torch.Tensor = torch.tensor(1.0), normalized_coordinates: bool = True",
            "-) -> None:"
        ],
        "pos_line": [
            "+def __init__(self, temperature: torch.Tensor = torch.tensor(1.0), normalized_coordinates: bool = True) -> None:"
        ],
        "core_change": "-def __init__( -self, temperature: torch.Tensor = torch.tensor(1.0), normalized_coordinates: bool = True -) -> None: +def __init__(self, temperature: torch.Tensor = torch.tensor(1.0), normalized_coordinates: bool = True) -> None:",
        "core_API": "tensor"
    },
    {
        "commit_hash": "519d358fbdcf5b63faedc9a10260c06cfb4d9dee",
        "index": "2358daf..1af1824 100644",
        "commit_message": "timm upgrade 0.6.12 (#696)\n\n* config: timm version bump 0.4.12 -> 0.6.12\n\n* fix: timm based EfficientNetBaseEncoder after timm upgrade to 0.6.12\n\nEfficientNet act1 layer in timm is merged with bn1 layer after\nab49d275de8a9c344aea086fd86d04c4cabb6098 commit\n\n* fix: use \"public\" API for timm models availability check\n\n* fix: timm SkNetEncoder after timm upgrade to 0.6.12\n\n`zero_init_last_bn` was renamed to `zero_init_last` in\n372ad5fa0dbeb74dcec81db06e9ff69b3d5a2eb6 commit\n\n* fix: timm RegNet encoder after timm upgrade to 0.6.12\n\nInstead of plain dict config RegNet uses RegNetCfg dataclass.\nDataclasses module were added in Python 3.7, thats why min required\nPython version for package is also increased.\n\n* feat: bump Python version used in Github Actions to 3.7\n",
        "file": "segmentation_models.pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class EfficientNetBaseEncoder(EfficientNet, EncoderMixin):",
            "def get_stages(self):",
            "return [",
            "nn.Identity(),",
            "-            nn.Sequential(self.conv_stem, self.bn1, self.act1),",
            "+            nn.Sequential(self.conv_stem, self.bn1),",
            "self.blocks[: self._stage_idxs[0]],",
            "self.blocks[self._stage_idxs[0] : self._stage_idxs[1]],",
            "self.blocks[self._stage_idxs[1] : self._stage_idxs[2]],"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=act1))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 3175,
        "neg_line": [
            "-nn.Sequential(self.conv_stem, self.bn1, self.act1),"
        ],
        "pos_line": [
            "+nn.Sequential(self.conv_stem, self.bn1),"
        ],
        "core_change": "-nn.Sequential(self.conv_stem, self.bn1, self.act1), +nn.Sequential(self.conv_stem, self.bn1),",
        "core_API": "Identity"
    },
    {
        "commit_hash": "4929503258d80abbc4b5f40da034298fe3803906",
        "index": "e69c1fe3..f00079c6 100644",
        "commit_message": "fix bugs\n\nSigned-off-by: zhaohu xing <920232796@qq.com>\n\n",
        "file": "stable-diffusion-webui.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_optimal_device():",
            "if torch.cuda.is_available():",
            "return torch.device(get_cuda_device_string())",
            "",
            "-    # if has_mps():",
            "-    #     return torch.device(\"mps\")",
            "+    if has_mps():",
            "+        return torch.device(\"mps\")",
            "",
            "return cpu"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=3, insert_id=1134748)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1134749)",
            "Insert(target_node=IN(type=if_statement), node=('call', None), position=1, insert_id=1134750)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1134751)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1134752)",
            "Insert(target_node=IN(type=call), node=('identifier', 'has_mps'), position=0, insert_id=1134753)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1134754)",
            "Insert(target_node=IN(type=block), node=('return_statement', None), position=0, insert_id=1134755)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1134756)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1134757)",
            "Insert(target_node=IN(type=return_statement), node=('return', 'return'), position=0, insert_id=1134758)",
            "Insert(target_node=IN(type=return_statement), node=('call', None), position=1, insert_id=1134759)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1134760)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1134761)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1134762)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1134763)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1134764)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1134765)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"mps\"'), position=1, insert_id=1134766)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1134767)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 3176,
        "neg_line": [
            "-# if has_mps():",
            "-#     return torch.device(\"mps\")"
        ],
        "pos_line": [
            "+if has_mps():",
            "+return torch.device(\"mps\")"
        ],
        "core_change": "-# if has_mps(): -#     return torch.device(\"mps\") +if has_mps(): +return torch.device(\"mps\")",
        "core_API": "is_available"
    },
    {
        "commit_hash": "a62e68a3eb8c3397bc656acf86cbb75b2afecf2c",
        "index": "ff334e1a..aa6916b1 100644",
        "commit_message": "Remove prefix for first tower in replicated mode. Support inference now.\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def regularize_cost(regex, func, name='regularize_cost'):",
            "for p in params:",
            "para_name = p.name",
            "# in replicated mode, only regularize variables inside this tower",
            "-        if ctx.has_own_variables and (not para_name.startswith(ctx.name)):",
            "+        if ctx.has_own_variables and (not para_name.startswith(ctx.vs_name)):",
            "continue",
            "if re.search(regex, para_name):",
            "costs.append(func(p))",
            "_log_regularizer(para_name)",
            "if not costs:",
            "-        return 0",
            "+        return tf.constant(0, dtype=tf.float32, name='empty_regularize_cost')",
            "return tf.add_n(costs, name=name)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=return_statement), node=('call', None), position=1, insert_id=2301324)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2301325)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2301326)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2301327)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2301328)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'constant'), position=2, insert_id=2301329)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2301330)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=integer, text=0), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2301331)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=3, insert_id=2301332)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=4, insert_id=2301333)",
            "Insert(target_node=IN(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2301334)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=6, insert_id=2301335)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=2301336)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2301337)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=2301338)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'name'), position=0, insert_id=2301339)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2301340)",
            "Insert(target_node=IN(type=keyword_argument), node=('string', \"'empty_regularize_cost'\"), position=2, insert_id=2301341)",
            "Update(target_node=ASTNode(type=identifier, text=name), value='vs_name')",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2301342)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2301343)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2301344)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 23,
        "number": 3177,
        "neg_line": [
            "-if ctx.has_own_variables and (not para_name.startswith(ctx.name)):",
            "-return 0"
        ],
        "pos_line": [
            "+if ctx.has_own_variables and (not para_name.startswith(ctx.vs_name)):",
            "+return tf.constant(0, dtype=tf.float32, name='empty_regularize_cost')"
        ],
        "core_change": "-if ctx.has_own_variables and (not para_name.startswith(ctx.name)): +if ctx.has_own_variables and (not para_name.startswith(ctx.vs_name)): -return 0 +return tf.constant(0, dtype=tf.float32, name='empty_regularize_cost')",
        "core_API": "startswith"
    },
    {
        "commit_hash": "31d4076dc5cca4d84dc9b3daee4c3c2bff349076",
        "index": "44915e4..ee0bdad 100644",
        "commit_message": "Skip KerasLayer test with hparams for TF 2.0.0-alpha0 which has a bug\naffecting Python defaults for tensor arguments (meanwhile fixed).\n\nPiperOrigin-RevId: 241536440\n\n",
        "file": "hub.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class KerasLayerTest(tf.test.TestCase):",
            "self.assertEqual(result, new_result)",
            "",
            "def testGetConfigFromConfigWithHParams(self):",
            "+    if tf.__version__ == \"2.0.0-alpha0\":",
            "+      self.skipTest(\"b/127938157 broke use of default hparams\")",
            "export_dir = os.path.join(self.get_temp_dir(), \"with-hparams\")",
            "_save_model_with_hparams(export_dir)",
            "layer = hub.KerasLayer(export_dir, arguments=dict(a=10.))  # Leave b=0."
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=IN(type=root), node=('block', None), position=0, insert_id=1950918)",
            "Insert(target_node=IN(type=block), node=('if_statement', None), position=0, insert_id=1950919)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=1950920)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=1950921)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=1950922)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=1950923)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=1950924)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=1950925)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', '\"2.0.0-alpha0\"'), position=2, insert_id=1950926)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=1950927)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1950928)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1950929)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__version__'), position=2, insert_id=1950930)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1950931)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1950932)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1950933)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1950934)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1950935)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'skipTest'), position=2, insert_id=1950936)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1950937)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"b/127938157 broke use of default hparams\"'), position=1, insert_id=1950938)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1950939)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 23,
        "number": 3182,
        "neg_line": [],
        "pos_line": [
            "+if tf.__version__ == \"2.0.0-alpha0\":",
            "+self.skipTest(\"b/127938157 broke use of default hparams\")"
        ],
        "core_change": "+if tf.__version__ == \"2.0.0-alpha0\": +self.skipTest(\"b/127938157 broke use of default hparams\")",
        "core_API": "assertEqual"
    },
    {
        "commit_hash": "e27aa15ed7fd4f3d735bf7a8a29b97fdede710bd",
        "index": "844f087..8ab9135 100644",
        "commit_message": "Add usage of batch norm in conv test and fix usage of is_training collection\n\n",
        "file": "skflow.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class OpsTest(tf.test.TestCase):",
            "filter_shape = (5, 5)",
            "vals = np.random.randn(batch_size, input_shape[0], input_shape[1], 1)",
            "with self.test_session() as sess:",
            "+            tf.add_to_collection(\"IS_TRAINING\", True)",
            "tensor_in = tf.placeholder(tf.float32, [batch_size, input_shape[0],",
            "input_shape[1], 1])",
            "-            res = ops.conv2d(tensor_in, n_filters, filter_shape)",
            "+            res = ops.conv2d(",
            "+                tensor_in, n_filters, filter_shape, batch_norm=True)",
            "sess.run(tf.initialize_all_variables())",
            "conv = sess.run(res, feed_dict={tensor_in.name: vals})",
            "self.assertEqual(conv.shape, (batch_size, input_shape[0],"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=with_statement), node=('block', None), position=3, insert_id=2170557)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2170558)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2170559)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2170560)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2170561)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=6, insert_id=2170562)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=7, insert_id=2170563)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2170564)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2170565)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'add_to_collection'), position=2, insert_id=2170566)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2170567)",
            "Insert(target_node=IN(type=argument_list), node=('string', '\"IS_TRAINING\"'), position=1, insert_id=2170568)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2170569)",
            "Insert(target_node=IN(type=argument_list), node=('true', 'True'), position=3, insert_id=2170570)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2170571)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'batch_norm'), position=0, insert_id=2170572)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2170573)",
            "Insert(target_node=IN(type=keyword_argument), node=('true', 'True'), position=2, insert_id=2170574)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 3184,
        "neg_line": [
            "-res = ops.conv2d(tensor_in, n_filters, filter_shape)"
        ],
        "pos_line": [
            "+tf.add_to_collection(\"IS_TRAINING\", True)",
            "+res = ops.conv2d(",
            "+tensor_in, n_filters, filter_shape, batch_norm=True)"
        ],
        "core_change": "+tf.add_to_collection(\"IS_TRAINING\", True) -res = ops.conv2d(tensor_in, n_filters, filter_shape) +res = ops.conv2d( +tensor_in, n_filters, filter_shape, batch_norm=True)",
        "core_API": "randn"
    },
    {
        "commit_hash": "40835a9e47a472771d9d9a068b654386160a2a20",
        "index": "dea64c87..e40e5806 100644",
        "commit_message": "fixes gpu test for drawing (#1476)\n\n* fixes gpu test for drawing\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* flake8 fix\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestRandomEqualize3D:",
            "",
            "class TestRandomAffine3D:",
            "def test_batch_random_affine_3d(self, device, dtype):",
            "-        # TODO(jian): crashes with pytorch 1.10, cuda and fp64",
            "-        if torch_version_geq(1, 10) and \"cuda\" in str(device) and dtype == torch.float64:",
            "+        # TODO(jian): cuda and fp64",
            "+        if \"cuda\" in str(device) and dtype == torch.float64:",
            "pytest.skip(\"AssertionError: assert tensor(False, device='cuda:0')\")",
            "",
            "f = RandomAffine3D((0, 0, 0), p=1.0, return_transform=True)  # No rotation"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=torch_version_geq))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=1))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=integer, text=10))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=and, text=and))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 3185,
        "neg_line": [
            "-# TODO(jian): crashes with pytorch 1.10, cuda and fp64",
            "-if torch_version_geq(1, 10) and \"cuda\" in str(device) and dtype == torch.float64:"
        ],
        "pos_line": [
            "+# TODO(jian): cuda and fp64",
            "+if \"cuda\" in str(device) and dtype == torch.float64:"
        ],
        "core_change": "-# TODO(jian): crashes with pytorch 1.10, cuda and fp64 -if torch_version_geq(1, 10) and \"cuda\" in str(device) and dtype == torch.float64: +# TODO(jian): cuda and fp64 +if \"cuda\" in str(device) and dtype == torch.float64:",
        "core_API": "skip"
    },
    {
        "commit_hash": "8b240a06617455eae59e1116af6a1a016664e963",
        "index": "85eadc7a7..0cd7e6fa0 100644",
        "commit_message": "Add TFEncoderDecoderModel + Add cross-attention to some TF models (#13222)\n\n* Add cross attentions to TFGPT2Model\n\n* Add TFEncoderDecoderModel\n\n* Add TFBaseModelOutputWithPoolingAndCrossAttentions\n\n* Add cross attentions to TFBertModel\n\n* Fix past or past_key_values argument issue\n\n* Fix generation\n\n* Fix save and load\n\n* Add some checks and comments\n\n* Clean the code that deals with past keys/values\n\n* Add kwargs to processing_inputs\n\n* Add serving_output to TFEncoderDecoderModel\n\n* Some cleaning + fix use_cache value issue\n\n* Fix tests + add bert2bert/bert2gpt2 tests\n\n* Fix more tests\n\n* Ignore crossattention.bias when loading GPT2 weights into TFGPT2\n\n* Fix return_dict_in_generate in tf generation\n\n* Fix is_token_logit_eos_token bug in tf generation\n\n* Finalize the tests after fixing some bugs\n\n* Fix another is_token_logit_eos_token bug in tf generation\n\n* Add/Update docs\n\n* Add TFBertEncoderDecoderModelTest\n\n* Clean test script\n\n* Add TFEncoderDecoderModel to the library\n\n* Add cross attentions to TFRobertaModel\n\n* Add TFRobertaEncoderDecoderModelTest\n\n* make style\n\n* Change the way of position_ids computation\n\n* bug fix\n\n* Fix copies in tf_albert\n\n* Remove some copied from and apply some fix-copies\n\n* Remove some copied\n\n* Add cross attentions to some other TF models\n\n* Remove encoder_hidden_states from TFLayoutLMModel.call for now\n\n* Make style\n\n* Fix TFRemBertForCausalLM\n\n* Revert the change to longformer + Remove copies\n\n* Revert the change to albert and convbert + Remove copies\n\n* make quality\n\n* make style\n\n* Add TFRembertEncoderDecoderModelTest\n\n* make quality and fix-copies\n\n* test TFRobertaForCausalLM\n\n* Fixes for failed tests\n\n* Fixes for failed tests\n\n* fix more tests\n\n* Fixes for failed tests\n\n* Fix Auto mapping order\n\n* Fix TFRemBertEncoder return value\n\n* fix tf_rembert\n\n* Check copies are OK\n\n* Fix missing TFBaseModelOutputWithPastAndCrossAttentions is not defined\n\n* Add TFEncoderDecoderModelSaveLoadTests\n\n* fix tf weight loading\n\n* check the change of use_cache\n\n* Revert the change\n\n* Add missing test_for_causal_lm for TFRobertaModelTest\n\n* Try cleaning past\n\n* fix _reorder_cache\n\n* Revert some files to original versions\n\n* Keep as many copies as possible\n\n* Apply suggested changes - Use raise ValueError instead of assert\n\n* Move import to top\n\n* Fix wrong require_torch\n\n* Replace more assert by raise ValueError\n\n* Add test_pt_tf_model_equivalence (the test won't pass for now)\n\n* add test for loading/saving\n\n* finish\n\n* finish\n\n* Remove test_pt_tf_model_equivalence\n\n* Update tf modeling template\n\n* Remove pooling, added in the prev. commit, from MainLayer\n\n* Update tf modeling test template\n\n* Move inputs[\"use_cache\"] = False to modeling_tf_utils.py\n\n* Fix torch.Tensor in the comment\n\n* fix use_cache\n\n* Fix missing use_cache in ElectraConfig\n\n* Add a note to from_pretrained\n\n* Fix style\n\n* Change test_encoder_decoder_save_load_from_encoder_decoder_from_pt\n\n* Fix TFMLP (in TFGPT2) activation issue\n\n* Fix None past_key_values value in serving_output\n\n* Don't call get_encoderdecoder_model in TFEncoderDecoderModelTest.test_configuration_tie until we have a TF checkpoint on Hub\n\n* Apply review suggestions - style for cross_attns in serving_output\n\n* Apply review suggestions - change assert + docstrings\n\n* break the error message to respect the char limit\n\n* deprecate the argument past\n\n* fix docstring style\n\n* Update the encoder-decoder rst file\n\n* fix Unknown interpreted text role \"method\"\n\n* fix typo\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFConvBertEmbeddings(tf.keras.layers.Layer):",
            "token_type_ids = tf.fill(dims=input_shape, value=0)",
            "",
            "if position_ids is None:",
            "-            position_ids = tf.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)",
            "+            position_ids = tf.expand_dims(",
            "+                tf.range(start=past_key_values_length, limit=input_shape[1] + past_key_values_length), axis=0",
            "+            )",
            "",
            "position_embeds = tf.gather(params=self.position_embeddings, indices=position_ids)",
            "position_embeds = tf.tile(input=position_embeds, multiples=(input_shape[0], 1, 1))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('identifier', 'past_key_values_length'), position=2, insert_id=2368951)",
            "Insert(target_node=ASTNode(type=keyword_argument), node=('binary_operator', None), position=2, insert_id=2368952)",
            "Move(target_node=IN(type=binary_operator), node=ASTNode(type=subscript), position=0)",
            "Insert(target_node=IN(type=binary_operator), node=('+', '+'), position=1, insert_id=2368953)",
            "Insert(target_node=IN(type=binary_operator), node=('identifier', 'past_key_values_length'), position=2, insert_id=2368954)",
            "Insert(target_node=ASTNode(type=subscript), node=('integer', '1'), position=2, insert_id=2368955)",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=unary_operator, text=-1))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 3187,
        "neg_line": [
            "-position_ids = tf.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)"
        ],
        "pos_line": [
            "+position_ids = tf.expand_dims(",
            "+tf.range(start=past_key_values_length, limit=input_shape[1] + past_key_values_length), axis=0",
            "+)"
        ],
        "core_change": "-position_ids = tf.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0) +position_ids = tf.expand_dims( +tf.range(start=past_key_values_length, limit=input_shape[1] + past_key_values_length), axis=0 +)",
        "core_API": "fill"
    },
    {
        "commit_hash": "fb4236a19f9b85800655695ad5cd237557183c78",
        "index": "ef82c77..15cda4b 100644",
        "commit_message": "Fixed PyTorch MNIST dataset (#2707)\n\nSigned-off-by: Travis Addair <tgaddair@gmail.com>\n",
        "file": "horovod.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def load_data_mnist():",
            "torch.set_num_threads(4)",
            "",
            "kwargs = {'num_workers': 0, 'pin_memory': True} if args.cuda else {}",
            "+    data_dir = args.data_dir or './data'",
            "from filelock import FileLock",
            "with FileLock(os.path.expanduser(\"~/.datalock\")):",
            "train_dataset = \\",
            "-            datasets.MNIST('./data', train=True, download=True,",
            "+            datasets.MNIST(data_dir, train=True, download=True,",
            "transform=transforms.Compose([",
            "transforms.ToTensor(),",
            "transforms.Normalize((0.1307,), (0.3081,))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1794895)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1794896)",
            "Insert(target_node=ASTNode(type=ERROR), node=('identifier', 'data_dir'), position=4, insert_id=1794897)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'data_dir'), position=0, insert_id=1794898)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1794899)",
            "Insert(target_node=IN(type=assignment), node=('boolean_operator', None), position=2, insert_id=1794900)",
            "Insert(target_node=IN(type=boolean_operator), node=('attribute', None), position=0, insert_id=1794901)",
            "Insert(target_node=IN(type=boolean_operator), node=('or', 'or'), position=1, insert_id=1794902)",
            "Insert(target_node=IN(type=boolean_operator), node=('string', \"'./data'\"), position=2, insert_id=1794903)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'args'), position=0, insert_id=1794904)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1794905)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'data_dir'), position=2, insert_id=1794906)",
            "Delete(target_node=ASTNode(type=string, text='./data'))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 3188,
        "neg_line": [
            "-datasets.MNIST('./data', train=True, download=True,"
        ],
        "pos_line": [
            "+data_dir = args.data_dir or './data'",
            "+datasets.MNIST(data_dir, train=True, download=True,"
        ],
        "core_change": "+data_dir = args.data_dir or './data' -datasets.MNIST('./data', train=True, download=True, +datasets.MNIST(data_dir, train=True, download=True,",
        "core_API": "set_num_threads"
    },
    {
        "commit_hash": "66641e8e683757c05148373cd43a00f034df302e",
        "index": "ca1bf03b..aec11961 100644",
        "commit_message": "Clarify and fix for ResNeXt\n\n",
        "file": "d2l-en.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class ResNeXtBlock(nn.Module):",
            "super().__init__()",
            "bot_channels = int(round(num_channels * bot_mul))",
            "self.conv1 = nn.LazyConv2d(bot_channels, kernel_size=1, stride=1)",
            "-        self.conv2 = nn.LazyConv2d(bot_channels, kernel_size=3, stride=strides,",
            "-                                   padding=1, groups=bot_channels//groups)",
            "+        self.conv2 = nn.LazyConv2d(bot_channels, kernel_size=3,",
            "+                                   stride=strides, padding=1,",
            "+                                   groups=bot_channels//groups)",
            "self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1, stride=1)",
            "self.bn1 = nn.LazyBatchNorm2d()",
            "self.bn2 = nn.LazyBatchNorm2d()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 3189,
        "neg_line": [
            "-self.conv2 = nn.LazyConv2d(bot_channels, kernel_size=3, stride=strides,",
            "-padding=1, groups=bot_channels//groups)"
        ],
        "pos_line": [
            "+self.conv2 = nn.LazyConv2d(bot_channels, kernel_size=3,",
            "+stride=strides, padding=1,",
            "+groups=bot_channels//groups)"
        ],
        "core_change": "-self.conv2 = nn.LazyConv2d(bot_channels, kernel_size=3, stride=strides, -padding=1, groups=bot_channels//groups) +self.conv2 = nn.LazyConv2d(bot_channels, kernel_size=3, +stride=strides, padding=1, +groups=bot_channels//groups)",
        "core_API": "LazyConv2d"
    },
    {
        "commit_hash": "a6627b522a28afa8d2b17638e8880aca435aa0ce",
        "index": "6df3cfb..6163900 100644",
        "commit_message": "fix bug\n\n",
        "file": "tutorials.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class DQNPrioritizedReplay:",
            "def store_transition(self, s, a, r, s_):",
            "if self.prioritized:    # prioritized replay",
            "transition = np.hstack((s, [a, r], s_))",
            "-            self.memory.store(1, transition)    # have 1 priority for newly arrived transition",
            "+            self.memory.store(0.9, transition)    # have 1 priority for newly arrived transition",
            "else:       # random replay",
            "if not hasattr(self, 'memory_counter'):",
            "self.memory_counter = 0"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('float', '0.9'), position=1, insert_id=2386900)",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 3190,
        "neg_line": [
            "-self.memory.store(1, transition)    # have 1 priority for newly arrived transition"
        ],
        "pos_line": [
            "+self.memory.store(0.9, transition)    # have 1 priority for newly arrived transition"
        ],
        "core_change": "-self.memory.store(1, transition)    # have 1 priority for newly arrived transition +self.memory.store(0.9, transition)    # have 1 priority for newly arrived transition",
        "core_API": "hstack"
    },
    {
        "commit_hash": "ec6cd7633f8c1aaf41d21d919c63fc8c170cd5df",
        "index": "ae0c83fae..a1071408f 100644",
        "commit_message": "TF: Add missing cast to GPT-J (#18201)\n\n* Fix TF GPT-J tests\n\n* add try/finally block\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFGPTJAttention(tf.keras.layers.Layer):",
            "key = self._split_heads(key, True)",
            "value = self._split_heads(value, False)",
            "",
            "-        sincos = tf.gather(self.embed_positions, position_ids, axis=0)",
            "+        sincos = tf.cast(tf.gather(self.embed_positions, position_ids, axis=0), hidden_states.dtype)",
            "sincos = tf.split(sincos, 2, axis=-1)",
            "if self.rotary_dim is not None:",
            "k_rot = key[:, :, :, : self.rotary_dim]"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=2362854)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=2362855)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2362856)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2362857)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'cast'), position=2, insert_id=2362858)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2362859)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2362860)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=2362861)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2362862)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'hidden_states'), position=0, insert_id=2362863)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2362864)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=2362865)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 3192,
        "neg_line": [
            "-sincos = tf.gather(self.embed_positions, position_ids, axis=0)"
        ],
        "pos_line": [
            "+sincos = tf.cast(tf.gather(self.embed_positions, position_ids, axis=0), hidden_states.dtype)"
        ],
        "core_change": "-sincos = tf.gather(self.embed_positions, position_ids, axis=0) +sincos = tf.cast(tf.gather(self.embed_positions, position_ids, axis=0), hidden_states.dtype)",
        "core_API": "_split_heads"
    },
    {
        "commit_hash": "5ae087cf8ec080b121c9cdc9bafdc2b35b6e110e",
        "index": "1ab1a635b..7d8abbf98 100644",
        "commit_message": "Fix T5/mT5 tests (#18029)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFMT5ModelIntegrationTest(unittest.TestCase):",
            "labels = tokenizer(\"Hi I am\", return_tensors=\"tf\").input_ids",
            "",
            "loss = model(input_ids, labels=labels).loss",
            "-        mtf_score = -tf.math.reduce_sum(loss).numpy()",
            "+        mtf_score = -tf.math.reduce_mean(loss).numpy()",
            "",
            "-        EXPECTED_SCORE = -84.9127",
            "+        EXPECTED_SCORE = -21.210594",
            "self.assertTrue(abs(mtf_score - EXPECTED_SCORE) < 2e-4)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=float, text=84.9127), value='21.210594')",
            "Update(target_node=ASTNode(type=identifier, text=reduce_sum), value='reduce_mean')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 3193,
        "neg_line": [
            "-mtf_score = -tf.math.reduce_sum(loss).numpy()",
            "-EXPECTED_SCORE = -84.9127"
        ],
        "pos_line": [
            "+mtf_score = -tf.math.reduce_mean(loss).numpy()",
            "+EXPECTED_SCORE = -21.210594"
        ],
        "core_change": "-mtf_score = -tf.math.reduce_sum(loss).numpy() +mtf_score = -tf.math.reduce_mean(loss).numpy() -EXPECTED_SCORE = -84.9127 +EXPECTED_SCORE = -21.210594",
        "core_API": "reduce_sum"
    },
    {
        "commit_hash": "136174c9d92259f026328795aca75eaa7be8071f",
        "index": "87e3afd0..892403be 100755",
        "commit_message": "bug fix in DQN.py\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def get_config():",
            ")",
            "",
            "def update_target_param():",
            "-        vars = tf.trainable_variables()",
            "+        vars = tf.global_variables()",
            "ops = []",
            "G = tf.get_default_graph()",
            "for v in vars:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=trainable_variables), value='global_variables')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3196,
        "neg_line": [
            "-vars = tf.trainable_variables()"
        ],
        "pos_line": [
            "+vars = tf.global_variables()"
        ],
        "core_change": "-vars = tf.trainable_variables() +vars = tf.global_variables()",
        "core_API": "trainable_variables"
    },
    {
        "commit_hash": "6d4f83cae02129b7f49acf022561711cd937e8b8",
        "index": "7b1735f8..eb764c20 100644",
        "commit_message": "Drop JIT support for `core.check`, `Boxes`, and others (#2219)\n\n* Drop JIT support for `core.check` API\n\n- Consequently for this, we drop support of JIT on the following items: (in of dynamo)\n  - enhance\n    - AdjustSigmoid\n    - AdjustLog\n    - AddWeighted\n  - geometry\n    - UndistortPoints\n    - bbox and Boxes - follow up on #2218\n    - EuclideanDistance\n    - TransformPoints\n    - HomographyWarper\n    - WarpPerspective\n    - UpscaleDouble\n  - losses\n\n* Update typing with pyupgrade\n* drop all jit related from bbox and boxes\n\nfrom #2218\n* fix/skip failing dynamo tests\n* fix loss hd\n* fix typing\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestCropByBoxes3D:",
            "patches = kornia.geometry.transform.crop_by_boxes3d(inp, src_box, dst_box, align_corners=True)",
            "assert_close(patches, expected, rtol=1e-4, atol=1e-4)",
            "",
            "-    def test_jit(self, device, dtype):",
            "+    def test_dynamo(self, device, dtype, torch_optimizer):",
            "# Define script",
            "op = kornia.geometry.transform.crop_by_boxes3d",
            "-        op_script = torch.jit.script(op)",
            "+        op_script = torch_optimizer(op)",
            "# Define input",
            "inp = torch.randn((1, 1, 7, 7, 7), device=device, dtype=dtype)",
            "src_box = torch.tensor("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=test_jit), value='test_dynamo')",
            "Insert(target_node=ASTNode(type=parameters), node=(',', ','), position=6, insert_id=388606)",
            "Insert(target_node=ASTNode(type=parameters), node=('identifier', 'torch_optimizer'), position=7, insert_id=388607)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='torch_optimizer')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=jit))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=script))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 3199,
        "neg_line": [
            "-def test_jit(self, device, dtype):",
            "-op_script = torch.jit.script(op)"
        ],
        "pos_line": [
            "+def test_dynamo(self, device, dtype, torch_optimizer):",
            "+op_script = torch_optimizer(op)"
        ],
        "core_change": "-def test_jit(self, device, dtype): +def test_dynamo(self, device, dtype, torch_optimizer): -op_script = torch.jit.script(op) +op_script = torch_optimizer(op)",
        "core_API": "crop_by_boxes3d"
    },
    {
        "commit_hash": "3f77c26d74e1282955fefa8dfff2451e44f6d4a9",
        "index": "20480f083..e3c747c93 100644",
        "commit_message": "Fix Longformer and LED (#9942)\n\n* Fix Longformer and LED\n\n* Add a test for graph execution with inputs_embeds\n\n* Apply style\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFLongformerMainLayer(tf.keras.layers.Layer):",
            "inputs_embeds_padding = self.embeddings(input_ids_padding)",
            "return tf.concat([inputs_embeds, inputs_embeds_padding], axis=-2)",
            "",
            "-            inputs_embeds = tf.cond(padding_len > 0, pad_embeddings, lambda: inputs_embeds)",
            "+            inputs_embeds = tf.cond(tf.math.greater(padding_len, 0), pad_embeddings, lambda: inputs_embeds)",
            "",
            "attention_mask = tf.pad(attention_mask, paddings, constant_values=False)  # no attention on the padding tokens",
            "token_type_ids = tf.pad(token_type_ids, paddings, constant_values=0)  # pad with token_type_id = 0"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=2373305)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2373306)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2373307)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2373308)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2373309)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'greater'), position=2, insert_id=2373310)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2373311)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=padding_len), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2373312)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=integer, text=0), position=3)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2373313)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2373314)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2373315)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'math'), position=2, insert_id=2373316)",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=comparison_operator))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 3203,
        "neg_line": [
            "-inputs_embeds = tf.cond(padding_len > 0, pad_embeddings, lambda: inputs_embeds)"
        ],
        "pos_line": [
            "+inputs_embeds = tf.cond(tf.math.greater(padding_len, 0), pad_embeddings, lambda: inputs_embeds)"
        ],
        "core_change": "-inputs_embeds = tf.cond(padding_len > 0, pad_embeddings, lambda: inputs_embeds) +inputs_embeds = tf.cond(tf.math.greater(padding_len, 0), pad_embeddings, lambda: inputs_embeds)",
        "core_API": "embeddings"
    },
    {
        "commit_hash": "81350a12be769568a0f87c695ced10c12316e8aa",
        "index": "6266422..75ff4b2 100644",
        "commit_message": "bug fix for predict on multi modal data (#1452)\n\n\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Pipeline(pps_module.Preprocessor):",
            "transformed.append(data)",
            "if len(transformed) == 1:",
            "return transformed[0]",
            "-        return tuple(transformed)",
            "+        return tf.data.Dataset.zip(tuple(transformed))",
            "",
            "def save(self, filepath):",
            "io_utils.save_json(filepath, self.get_config())"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1886953)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1886954)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1886955)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1886956)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'zip'), position=2, insert_id=1886957)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1886958)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1886959)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=1886960)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1886961)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'Dataset'), position=2, insert_id=1886962)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1886963)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1886964)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'data'), position=2, insert_id=1886965)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 3205,
        "neg_line": [
            "-return tuple(transformed)"
        ],
        "pos_line": [
            "+return tf.data.Dataset.zip(tuple(transformed))"
        ],
        "core_change": "-return tuple(transformed) +return tf.data.Dataset.zip(tuple(transformed))",
        "core_API": "append"
    },
    {
        "commit_hash": "76bb45964df1e62d1411b0a9e9fc673e9a791c9a",
        "index": "e0eaba73d..95a5a946a 100644",
        "commit_message": "Add Image feature (#3163)\n\n* Initial commit\n\n* Add basic decoding\n\n* Replace features.Audio with Audio\n\n* Add Image to package reference\n\n* Use np.array\n\n* Update error msg\n\n* Add mode and channel decoding\n\n* Fix return value\n\n* Finish decoding\n\n* Make CI happy\n\n* Some more fixes\n\n* Minor doc fix\n\n* Remove animated option\n\n* Pin version\n\n* Remove unused imports in setup.py\n\n* Add vision requirements to setup.py\n\n* Add initial tests\n\n* Delete other formats\n\n* Make Image feature hashable\n\n* Add more tests\n\n* Support numpy array in alter data check in TypedSequence\n\n* Fix TypedSequence converion\n\n* Finish tests\n\n* Update Image - add ImageExtensionType and supporting functions\n\n* Update encoding functions\n\n* Add support in TypedSequence for ImageExtensionType\n\n* Add tests\n\n* Remove unused import\n\n* Fix doc and style\n\n* Fix doc indentation\n\n* Improve comment\n\n* Return single image instead of dict\n\n* Return PIL Image and not dict\n\n* Encode dict\n\n* Update tests\n\n* Style\n\n* np.ndarray encoding/decoding\n\n* Minor improvements\n\n* PIL Image support in cast_to_python_objects\n\n* Test cast\n\n* Doc fix\n\n* Extension type fixes\n\n* Style\n\n* Use types_mapper in Dataset.to_pandas\n\n* Add pandas extension array for image type\n\n* Update tests\n\n* image type inference\n\n* Remvoe cast_to_python test after Quentin's change\n\n* Improve tests\n\n* Add storage type\n\n* Improve tests\n\n* Test map that returns np.ndarray\n\n* Rename functions\n\n* Add streaming test\n\n* Use image struct in all situations\n\n* Update src/datasets/features/image.py - encode_example type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Update src/datasets/features/image.py -list_image_compression_formats type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Support str in encode_objects_to_image_dicts\n\n* Update src/datasets/features/image.py - objects_to_list_of_image_dicts type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Style\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class Covost2(datasets.GeneratorBasedBuilder):",
            "features=datasets.Features(",
            "client_id=datasets.Value(\"string\"),",
            "file=datasets.Value(\"string\"),",
            "-                audio=datasets.features.Audio(sampling_rate=16_000),",
            "+                audio=datasets.Audio(sampling_rate=16_000),",
            "sentence=datasets.Value(\"string\"),",
            "translation=datasets.Value(\"string\"),",
            "id=datasets.Value(\"string\"),"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=features))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 3209,
        "neg_line": [
            "-audio=datasets.features.Audio(sampling_rate=16_000),"
        ],
        "pos_line": [
            "+audio=datasets.Audio(sampling_rate=16_000),"
        ],
        "core_change": "-audio=datasets.features.Audio(sampling_rate=16_000), +audio=datasets.Audio(sampling_rate=16_000),",
        "core_API": "Features"
    },
    {
        "commit_hash": "23c65e3a7f5db24abc9a08cd5b42603fad1d34cb",
        "index": "495f6c9a9..ab72b9999 100644",
        "commit_message": "fix wrong variable in greedy decode\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DecoderRNNTAtt(torch.nn.Module):",
            "",
            "hyp = {'score': 0.0, 'yseq': [self.blank]}",
            "",
            "-        eys = torch.zeros((1, self.dunits))",
            "+        eys = torch.zeros((1, self.embed_dim))",
            "att_c, att_w = self.att[0](h.unsqueeze(0), [h.size(0)],",
            "self.dropout_dec[0](z_list[0]), None)",
            "ey = torch.cat((eys, att_c), dim=1)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=dunits), value='embed_dim')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3210,
        "neg_line": [
            "-eys = torch.zeros((1, self.dunits))"
        ],
        "pos_line": [
            "+eys = torch.zeros((1, self.embed_dim))"
        ],
        "core_change": "-eys = torch.zeros((1, self.dunits)) +eys = torch.zeros((1, self.embed_dim))",
        "core_API": "zeros"
    },
    {
        "commit_hash": "f38aaa5be05104ddfc3452249eeb521f139ded99",
        "index": "751b254e..a4dcda26 100644",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Iterative(Solver):",
            "next_step = self.next_step(*args)",
            "step = (lambda: self.step(*args))",
            "do_nothing = (lambda: args)",
            "-                args = tf.cond(pred=next_step, true_fn=step, false_fn=do_nothing)",
            "+                args = self.cond(pred=next_step, true_fn=step, false_fn=do_nothing)",
            "",
            "else:",
            "# TensorFlow while loop",
            "-            args = tf.while_loop(",
            "+            args = self.while_loop(",
            "cond=self.next_step, body=self.step, loop_vars=args,",
            "maximum_iterations=self.max_iterations",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='self')",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='self')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 3220,
        "neg_line": [
            "-args = tf.cond(pred=next_step, true_fn=step, false_fn=do_nothing)",
            "-args = tf.while_loop("
        ],
        "pos_line": [
            "+args = self.cond(pred=next_step, true_fn=step, false_fn=do_nothing)",
            "+args = self.while_loop("
        ],
        "core_change": "-args = tf.cond(pred=next_step, true_fn=step, false_fn=do_nothing) +args = self.cond(pred=next_step, true_fn=step, false_fn=do_nothing) -args = tf.while_loop( +args = self.while_loop(",
        "core_API": "next_step"
    },
    {
        "commit_hash": "afe1df385b5ef37c3f75a49ceda94fdc448bf490",
        "index": "484190a..e9be7e4 100644",
        "commit_message": "dist.destroy_process_group() bug fix\n\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train(hyp):",
            "if not opt.evolve:",
            "plot_results()  # save as results.png",
            "print('%g epochs completed in %.3f hours.\\n' % (epoch - start_epoch + 1, (time.time() - t0) / 3600))",
            "-    dist.destroy_process_group() if torch.cuda.device_count() > 1 else None",
            "+    dist.destroy_process_group() if device.type != 'cpu' and torch.cuda.device_count() > 1 else None",
            "torch.cuda.empty_cache()",
            "return results"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=conditional_expression), node=('boolean_operator', None), position=2, insert_id=1304254)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=0, insert_id=1304255)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1304256)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=comparison_operator), position=2)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=1304257)",
            "Insert(target_node=IN(type=comparison_operator), node=('!=', '!='), position=1, insert_id=1304258)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', \"'cpu'\"), position=2, insert_id=1304259)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=0, insert_id=1304260)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1304261)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'type'), position=2, insert_id=1304262)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 3223,
        "neg_line": [
            "-dist.destroy_process_group() if torch.cuda.device_count() > 1 else None"
        ],
        "pos_line": [
            "+dist.destroy_process_group() if device.type != 'cpu' and torch.cuda.device_count() > 1 else None"
        ],
        "core_change": "-dist.destroy_process_group() if torch.cuda.device_count() > 1 else None +dist.destroy_process_group() if device.type != 'cpu' and torch.cuda.device_count() > 1 else None",
        "core_API": "time"
    },
    {
        "commit_hash": "b7afac351b61a1f90e9b0611e267731058c8cda0",
        "index": "f529ce573..19fcd4219 100644",
        "commit_message": "Add onnx export (#2596)\n\n* export model to onnx\n\n* prepare data before exporting\n\n* support for dataloaders and tensors\n\n* added tests\n\n* use example_input_array\nadd to changelog\n\n* updated docstring\n\n* added onnx inference tests\n\n* temp commit\n\n* removed schema valid test\n\n* add onnxruntime to environment.yml\n\n* moved onnxruntime to environment.yml pip\n\n* add example in doc\n\n* add lines between code block\n\n* added PR to changelog\n\n* is file check\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* remove *\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\n* infer example outputs\n\n* added doctest for onnx\n\n* fix windows tests\n\n* moved eval within condition block\n\n* self.forward to self\n\n* added docs\n\n* fixed docs error\n\n* added to toctree\n\n* Update CHANGELOG.md\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EvalModelTemplate(",
            "self.test_step_end_called = False",
            "self.test_epoch_end_called = False",
            "",
            "-        # if you specify an example input, the summary will show input/output for each layer",
            "-        # TODO: to be fixed in #1773",
            "-        # self.example_input_array = torch.rand(5, 28 * 28)",
            "+        self.example_input_array = torch.rand(5, 28 * 28)",
            "",
            "# build model",
            "self.__build_model()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=ERROR), node=('attribute', None), position=6, insert_id=574651)",
            "Insert(target_node=ASTNode(type=ERROR), node=('=', '='), position=7, insert_id=574652)",
            "Insert(target_node=ASTNode(type=ERROR), node=('call', None), position=8, insert_id=574653)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=574654)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=574655)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'example_input_array'), position=2, insert_id=574656)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=574657)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=574658)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=574659)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=574660)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'rand'), position=2, insert_id=574661)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=574662)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '5'), position=1, insert_id=574663)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=574664)",
            "Insert(target_node=IN(type=argument_list), node=('binary_operator', None), position=3, insert_id=574665)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=574666)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '28'), position=0, insert_id=574667)",
            "Insert(target_node=IN(type=binary_operator), node=('*', '*'), position=1, insert_id=574668)",
            "Insert(target_node=IN(type=binary_operator), node=('integer', '28'), position=2, insert_id=574669)"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 19,
        "number": 3225,
        "neg_line": [
            "-# if you specify an example input, the summary will show input/output for each layer",
            "-# TODO: to be fixed in #1773",
            "-# self.example_input_array = torch.rand(5, 28 * 28)"
        ],
        "pos_line": [
            "+self.example_input_array = torch.rand(5, 28 * 28)"
        ],
        "core_change": "-# if you specify an example input, the summary will show input/output for each layer -# TODO: to be fixed in #1773 -# self.example_input_array = torch.rand(5, 28 * 28) +self.example_input_array = torch.rand(5, 28 * 28)",
        "core_API": "rand"
    },
    {
        "commit_hash": "af74038a4df2337404736e0d3b80a47d6f928ea8",
        "index": "615f718..f2df76a 100644",
        "commit_message": "Closing session to fix problem with memory leak in test cases\n",
        "file": "facenet.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def main(args):",
            "if args.lfw_dir:",
            "evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder,",
            "embeddings, label_batch, lfw_paths, actual_issame, args.lfw_batch_size, args.lfw_nrof_folds, log_dir, step, summary_writer)",
            "+    sess.close()",
            "return model_dir",
            "",
            "def find_threshold(var, percentile):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=3, insert_id=1927545)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=1927546)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=1927547)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1927548)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'sess'), position=0, insert_id=1927549)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1927550)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'close'), position=2, insert_id=1927551)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1927552)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=1927553)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 9,
        "number": 3226,
        "neg_line": [],
        "pos_line": [
            "+sess.close()"
        ],
        "core_change": "+sess.close()",
        "core_API": "close"
    },
    {
        "commit_hash": "43043ee4d5c672f7fbc22ac9559e8164731fd053",
        "index": "ffb56c5eb3..8ff85633fc 100644",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "layers",
        "change": [
            "class Model:",
            "def flatten(obs, framework):",
            "\"\"\"Flatten the given tensor.\"\"\"",
            "if framework == \"tf\":",
            "-        return tf.layers.flatten(obs)",
            "+        return tf1.layers.flatten(obs)",
            "elif framework == \"torch\":",
            "assert torch is not None",
            "return torch.flatten(obs, start_dim=1)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=tf), value='tf1')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3227,
        "neg_line": [
            "-return tf.layers.flatten(obs)"
        ],
        "pos_line": [
            "+return tf1.layers.flatten(obs)"
        ],
        "core_change": "-return tf.layers.flatten(obs) +return tf1.layers.flatten(obs)",
        "core_API": "flatten"
    },
    {
        "commit_hash": "031c8a880701ee83c66e667556ed3fddfd610631",
        "index": "390a658b..b53b130b 100644",
        "commit_message": "fix(ci): mro imports and misc (#1786)\n\n* fix: mro imports\n\n* fix: tests imports\n",
        "file": "BentoML.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def predict_df(inference_sess: ExecutionSession, df: pd.DataFrame):",
            "@pytest.fixture()",
            "def tensorflow_model(tmpdir):",
            "model = NativeModel()",
            "-    tf.saved_model.save(model, tmpdir)",
            "+    tf.saved_model.save(model, str(tmpdir))",
            "",
            "",
            "@pytest.fixture()"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=3, insert_id=1906191)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=4, insert_id=1906192)",
            "Insert(target_node=IN(type=call), node=('identifier', 'str'), position=0, insert_id=1906193)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1906194)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1906195)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=identifier, text=tmpdir), position=1)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 3230,
        "neg_line": [
            "-tf.saved_model.save(model, tmpdir)"
        ],
        "pos_line": [
            "+tf.saved_model.save(model, str(tmpdir))"
        ],
        "core_change": "-tf.saved_model.save(model, tmpdir) +tf.saved_model.save(model, str(tmpdir))",
        "core_API": "fixture"
    },
    {
        "commit_hash": "39b0faa3ec14648b4af24cfa5ae2b9294ed2e327",
        "index": "0e5663f60d..70f4274f3e 100644",
        "commit_message": "[RLlib]: bug fix, should be input_dict['is_training'] (#19805)\n\n\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class BatchNormModel(TFModelV2):",
            "# Add a batch norm layer",
            "last_layer = tf1.layers.batch_normalization(",
            "last_layer,",
            "-                    training=input_dict.is_training,",
            "+                    training=input_dict[\"is_training\"],",
            "name=\"bn_{}\".format(i))",
            "",
            "output = tf1.layers.dense("
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=keyword_argument), node=('subscript', None), position=2, insert_id=2139076)",
            "Move(target_node=IN(type=subscript), node=ASTNode(type=identifier, text=input_dict), position=0)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=2139077)",
            "Insert(target_node=IN(type=subscript), node=('string', '\"is_training\"'), position=2, insert_id=2139078)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=2139079)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=is_training))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 3231,
        "neg_line": [
            "-training=input_dict.is_training,"
        ],
        "pos_line": [
            "+training=input_dict[\"is_training\"],"
        ],
        "core_change": "-training=input_dict.is_training, +training=input_dict[\"is_training\"],",
        "core_API": "batch_normalization"
    },
    {
        "commit_hash": "2d2ed2cc180475833d4315dde09fc5e5a1ccc9b5",
        "index": "6ed8037f1..c12a8f4a8 100644",
        "commit_message": "[T5] Fix speed degradation bug t5 (#10496)\n\n* fix speed degradation bug t5\n\n* fix for all models\n\n* fix code quality\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class T5Block(nn.Module):",
            "",
            "# Apply Feed Forward layer",
            "hidden_states = self.layer[-1](hidden_states)",
            "-        if torch.isinf(hidden_states).any():",
            "+",
            "+        # clamp inf values to enable fp16 training",
            "+        if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():",
            "clamp_value = torch.finfo(hidden_states.dtype).max - 1000",
            "hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)",
            "+",
            "outputs = (hidden_states,)",
            "",
            "outputs = outputs + (present_key_value_state,) + attention_outputs"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=if_statement), node=('boolean_operator', None), position=1, insert_id=1221076)",
            "Insert(target_node=IN(type=boolean_operator), node=('comparison_operator', None), position=0, insert_id=1221077)",
            "Insert(target_node=IN(type=boolean_operator), node=('and', 'and'), position=1, insert_id=1221078)",
            "Move(target_node=IN(type=boolean_operator), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=0, insert_id=1221079)",
            "Insert(target_node=IN(type=comparison_operator), node=('==', '=='), position=1, insert_id=1221080)",
            "Insert(target_node=IN(type=comparison_operator), node=('attribute', None), position=2, insert_id=1221081)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'hidden_states'), position=0, insert_id=1221082)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1221083)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1221084)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1221085)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1221086)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float16'), position=2, insert_id=1221087)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 3236,
        "neg_line": [
            "-if torch.isinf(hidden_states).any():"
        ],
        "pos_line": [
            "+",
            "+# clamp inf values to enable fp16 training",
            "+if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():",
            "+"
        ],
        "core_change": "-if torch.isinf(hidden_states).any(): + +# clamp inf values to enable fp16 training +if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any(): +",
        "core_API": "isinf"
    },
    {
        "commit_hash": "a2b72dccb8200be48e1b9fc3bdc95f05446ed06f",
        "index": "6f312144..87c09ee3 100644",
        "commit_message": "fixing doctest in pinhole (#1743)\n\n* fixing doctest in pinhole\n\n* Update pinhole.py\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PinholeCamera:",
            ">>> _ = torch.manual_seed(0)",
            ">>> x = torch.rand(1, 2)",
            ">>> depth = torch.ones(1, 1)",
            "-            >>> I = torch.eye(4)[None]",
            "+            >>> K = torch.eye(4)[None]",
            ">>> E = torch.eye(4)[None]",
            ">>> h = torch.ones(1)",
            ">>> w = torch.ones(1)",
            ">>> pinhole = kornia.geometry.camera.PinholeCamera(K, E, h, w)",
            "-            >>> pinhole.unproject_points(x, depth)",
            "+            >>> pinhole.unproject(x, depth)",
            "tensor([[0.4963, 0.7682, 1.0000]])",
            "\"\"\"",
            "P = self.intrinsics @ self.extrinsics"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=module), position=16)",
            "Insert(target_node=ASTNode(type=module), node=('ERROR', None), position=1, insert_id=402856)",
            "Insert(target_node=IN(type=ERROR), node=('>>', '>>'), position=0, insert_id=402857)",
            "Insert(target_node=IN(type=ERROR), node=('>', '>'), position=1, insert_id=402858)",
            "Update(target_node=ASTNode(type=identifier, text=I), value='K')",
            "Update(target_node=ASTNode(type=identifier, text=unproject_points), value='unproject')",
            "Delete(target_node=ASTNode(type=>>, text=>>))",
            "Delete(target_node=ASTNode(type=>, text=>))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 9,
        "number": 3238,
        "neg_line": [
            "->>> I = torch.eye(4)[None]",
            "->>> pinhole.unproject_points(x, depth)"
        ],
        "pos_line": [
            "+>>> K = torch.eye(4)[None]",
            "+>>> pinhole.unproject(x, depth)"
        ],
        "core_change": "->>> I = torch.eye(4)[None] +>>> K = torch.eye(4)[None] ->>> pinhole.unproject_points(x, depth) +>>> pinhole.unproject(x, depth)",
        "core_API": "manual_seed"
    },
    {
        "commit_hash": "cca0a999032b60b851800145d7009fb941db427d",
        "index": "33665869c..dd3cc36e9 100644",
        "commit_message": "Fix module mismatch in hook for func like torch.argmax\n>\n>\nCo-authored-by: Alexis Thual <alexisthual@gmail.com>\n\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "functional",
        "change": [
            "class TorchHook:",
            "if type(native_func) in [types.FunctionType, types.BuiltinFunctionType]:",
            "# 3. Build the hooked function",
            "new_func = self.get_hooked_func(native_func)",
            "-                # 4. Move the native function",
            "-                setattr(torch_module, f\"native_{func}\", native_func)",
            "+                # 4. Move the native function to its original module",
            "+                # /!\\ Can be different from the torch_module!",
            "+                # Ex: in torch.py `torch.argmax = torch.functional.argmax`",
            "+                # ... So torch.argmax.__module__ is 'torch.functional' != 'torch'",
            "+                setattr(eval(native_func.__module__), f\"native_{func}\", native_func)",
            "# 5. Put instead the hooked one",
            "setattr(torch_module, func, new_func)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=1744594)",
            "Update(target_node=ASTNode(type=identifier, text=torch_module), value='eval')",
            "Move(target_node=IN(type=call), node=ASTNode(type=identifier, text=torch_module), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1744595)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1744596)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=1744597)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1744598)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'native_func'), position=0, insert_id=1744599)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1744600)",
            "Insert(target_node=IN(type=attribute), node=('identifier', '__module__'), position=2, insert_id=1744601)"
        ],
        "plus_line": 5,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 3239,
        "neg_line": [
            "-# 4. Move the native function",
            "-setattr(torch_module, f\"native_{func}\", native_func)"
        ],
        "pos_line": [
            "+# 4. Move the native function to its original module",
            "+# /!\\ Can be different from the torch_module!",
            "+# Ex: in torch.py `torch.argmax = torch.functional.argmax`",
            "+# ... So torch.argmax.__module__ is 'torch.functional' != 'torch'",
            "+setattr(eval(native_func.__module__), f\"native_{func}\", native_func)"
        ],
        "core_change": "-# 4. Move the native function -setattr(torch_module, f\"native_{func}\", native_func) +# 4. Move the native function to its original module +# /!\\ Can be different from the torch_module! +# Ex: in torch.py `torch.argmax = torch.functional.argmax` +# ... So torch.argmax.__module__ is 'torch.functional' != 'torch' +setattr(eval(native_func.__module__), f\"native_{func}\", native_func)",
        "core_API": "get_hooked_func"
    },
    {
        "commit_hash": "3910ad033074367f6abfe0001562db725a75cb73",
        "index": "670d36902..a9a96abaf 100644",
        "commit_message": "bugfix/3185 transpose (#3252)\n\n* change t() to transpose() as xla devices do not support .t() on 1-dim tensor\n\n* detach tensor before copying\n\n* Revert \"detach tensor before copying\"\n\nThis reverts commit 37cc7bbe\n\n* changed dims\n\n* added test_result_obj_on_tpu\n\n* detach before copying\n\n* detach before copying\n\n* detach before copying\n\n* replace torch.cat with sum\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class EvalResult(Result):",
            "",
            "def weighted_mean(result, weights):",
            "weights = weights.to(result.device)",
            "-    numerator = torch.dot(result.float(), weights.t().float())",
            "+    numerator = torch.dot(result.float(), weights.transpose(-1, 0).float())",
            "result = numerator / weights.sum().float()",
            "return result"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=t), value='transpose')",
            "Insert(target_node=ASTNode(type=argument_list), node=('unary_operator', '-1'), position=1, insert_id=569794)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=569795)",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '0'), position=3, insert_id=569796)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 3240,
        "neg_line": [
            "-numerator = torch.dot(result.float(), weights.t().float())"
        ],
        "pos_line": [
            "+numerator = torch.dot(result.float(), weights.transpose(-1, 0).float())"
        ],
        "core_change": "-numerator = torch.dot(result.float(), weights.t().float()) +numerator = torch.dot(result.float(), weights.transpose(-1, 0).float())",
        "core_API": "to"
    },
    {
        "commit_hash": "091568ec36900509b424ae3121a64b46fe5a836e",
        "index": "8905da4d..4c5f056b 100644",
        "commit_message": "fix DistributedTrainer (fix #505)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TowerContext(object):",
            "global _CurrentTowerContext",
            "assert _CurrentTowerContext is None, \"Cannot nest TowerContext!\"",
            "_CurrentTowerContext = self",
            "-        curr_vs = tf.get_variable_scope()",
            "-        assert curr_vs.name == '', \"Cannot nest TowerContext with an existing variable scope!\"",
            "+        if self.is_training:",
            "+            curr_vs = tf.get_variable_scope()",
            "+            assert curr_vs.name == '', \"In training, cannot nest TowerContext with an existing variable scope!\"",
            "",
            "self._ctxs = self._get_scopes()",
            "self._ctxs.append(self._collection_guard)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=4, insert_id=2292416)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=2292417)",
            "Insert(target_node=IN(type=if_statement), node=('attribute', None), position=1, insert_id=2292418)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=2292419)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=2292420)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=2292421)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2292422)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'is_training'), position=2, insert_id=2292423)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)",
            "Move(target_node=IN(type=block), node=ASTNode(type=assert_statement), position=1)",
            "Update(target_node=ASTNode(type=string, text=\"Cannot nest TowerContext with an existing variable scope!\"), value='\"In training, cannot nest TowerContext with an existing variable scope!\"')"
        ],
        "plus_line": 3,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 3242,
        "neg_line": [
            "-curr_vs = tf.get_variable_scope()",
            "-assert curr_vs.name == '', \"Cannot nest TowerContext with an existing variable scope!\""
        ],
        "pos_line": [
            "+if self.is_training:",
            "+curr_vs = tf.get_variable_scope()",
            "+assert curr_vs.name == '', \"In training, cannot nest TowerContext with an existing variable scope!\""
        ],
        "core_change": "-curr_vs = tf.get_variable_scope() -assert curr_vs.name == '', \"Cannot nest TowerContext with an existing variable scope!\" +if self.is_training: +curr_vs = tf.get_variable_scope() +assert curr_vs.name == '', \"In training, cannot nest TowerContext with an existing variable scope!\"",
        "core_API": "get_variable_scope"
    },
    {
        "commit_hash": "ae985fc4f34bba9b93d65e24ae8ad0ef008b6903",
        "index": "54e3ad5c..303624db 100644",
        "commit_message": "ugly fix of MODEL_KEY\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Trainer(object):",
            "\"\"\"",
            "assert isinstance(config, TrainConfig), type(config)",
            "self.config = config",
            "-        tf.add_to_collection(MODEL_KEY, config.model)",
            "+        self.model = config.model",
            "",
            "@abstractmethod",
            "def train(self):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=expression_statement), node=('assignment', None), position=0, insert_id=2318988)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=2318989)",
            "Move(target_node=IN(type=assignment), node=ASTNode(type=attribute), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=tf), value='self')",
            "Update(target_node=ASTNode(type=identifier, text=add_to_collection), value='model')",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=MODEL_KEY))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 3243,
        "neg_line": [
            "-tf.add_to_collection(MODEL_KEY, config.model)"
        ],
        "pos_line": [
            "+self.model = config.model"
        ],
        "core_change": "-tf.add_to_collection(MODEL_KEY, config.model) +self.model = config.model",
        "core_API": "add_to_collection"
    },
    {
        "commit_hash": "5754fa7a961b4b6dd7651436bd29dd5712bc134f",
        "index": "34a5707..0811af0 100644",
        "commit_message": "Fixes to Multihead Attention with LayerNorm and Dropout-Add (#860)\n\n\n",
        "file": "apex.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class SelfMultiheadAttn(nn.Module):",
            "self.register_parameter('lyr_norm_beta_weights', None)",
            "self.lyr_nrm_gamma_weights = None",
            "self.lyr_nrm_beta_weights  = None",
            "-                self.lyr_nrm = torch.nn.LayerNorm(embed_dim)",
            "+                self.lyr_nrm = FusedLayerNorm(embed_dim)",
            "self.reset_parameters()",
            "",
            "if self.include_norm_add:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=torch), value='FusedLayerNorm')",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=identifier, text=torch), position=0)",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=nn))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=LayerNorm))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 3244,
        "neg_line": [
            "-self.lyr_nrm = torch.nn.LayerNorm(embed_dim)"
        ],
        "pos_line": [
            "+self.lyr_nrm = FusedLayerNorm(embed_dim)"
        ],
        "core_change": "-self.lyr_nrm = torch.nn.LayerNorm(embed_dim) +self.lyr_nrm = FusedLayerNorm(embed_dim)",
        "core_API": "register_parameter"
    },
    {
        "commit_hash": "2eb8a6ff539ac0f4838ce1161b6d239e912ca007",
        "index": "65a0df5..a5609cd 100644",
        "commit_message": "Fix device in submit.py\n",
        "file": "Pytorch-UNet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def submit(net, gpu=False):",
            "if __name__ == '__main__':",
            "net = UNet(3, 1).cuda()",
            "net.load_state_dict(torch.load('MODEL.pth'))",
            "-    submit(net, True)",
            "+    submit(net)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=true, text=True))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 3247,
        "neg_line": [
            "-submit(net, True)"
        ],
        "pos_line": [
            "+submit(net)"
        ],
        "core_change": "-submit(net, True) +submit(net)",
        "core_API": "load_state_dict"
    },
    {
        "commit_hash": "2775c3f69bf9fb44b7beba68d5843a17a42743f9",
        "index": "6139974..d46fbce 100644",
        "commit_message": "fix #172\n\n",
        "file": "tflearn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Trainer(object):",
            "# 0.7 workaround, restore values",
            "for t in l_stags:",
            "tf.add_to_collection(\"summary_tags\", t)",
            "+        for t in l4_stags:",
            "+            tf.add_to_collection(tf.GraphKeys.GRAPH_CONFIG, t)",
            "for t in l1_dtags:",
            "tf.add_to_collection(tf.GraphKeys.DATA_PREP, t)",
            "for t in l2_dtags:"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('for_statement', None), position=3, insert_id=2353717)",
            "Insert(target_node=IN(type=for_statement), node=('for', 'for'), position=0, insert_id=2353718)",
            "Insert(target_node=IN(type=for_statement), node=('identifier', 't'), position=1, insert_id=2353719)",
            "Insert(target_node=IN(type=for_statement), node=('in', 'in'), position=2, insert_id=2353720)",
            "Insert(target_node=IN(type=for_statement), node=('identifier', 'l4_stags'), position=3, insert_id=2353721)",
            "Insert(target_node=IN(type=for_statement), node=(':', ':'), position=4, insert_id=2353722)",
            "Insert(target_node=IN(type=for_statement), node=('block', None), position=5, insert_id=2353723)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=2353724)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=2353725)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2353726)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2353727)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2353728)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2353729)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'add_to_collection'), position=2, insert_id=2353730)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2353731)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=2353732)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=2353733)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 't'), position=3, insert_id=2353734)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=2353735)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=2353736)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2353737)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'GRAPH_CONFIG'), position=2, insert_id=2353738)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2353739)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2353740)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'GraphKeys'), position=2, insert_id=2353741)"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 25,
        "number": 3248,
        "neg_line": [],
        "pos_line": [
            "+for t in l4_stags:",
            "+tf.add_to_collection(tf.GraphKeys.GRAPH_CONFIG, t)"
        ],
        "core_change": "+for t in l4_stags: +tf.add_to_collection(tf.GraphKeys.GRAPH_CONFIG, t)",
        "core_API": "add_to_collection"
    },
    {
        "commit_hash": "c163b638d30ec6924067a4735718d4f61da03d99",
        "index": "2c4546c7..15895ed0 100644",
        "commit_message": "Fixed memory error in `make_vocab` on big dataset. (#2606)\n\nUsed generator instead of reading all instances to memory before\npassing to Vocabulary class(as in train loop).\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "def make_vocab_from_params(params: Params, serialization_dir: str):",
            "logger.info(\"From dataset instances, %s will be considered for vocabulary creation.\",",
            "\", \".join(datasets_for_vocab_creation))",
            "",
            "-    instances = [instance for key, dataset in all_datasets.items()",
            "+    instances = (instance for key, dataset in all_datasets.items()",
            "for instance in dataset",
            "-                 if key in datasets_for_vocab_creation]",
            "+                 if key in datasets_for_vocab_creation)",
            "",
            "vocab = Vocabulary.from_params(vocab_params, instances)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('generator_expression', None), position=2, insert_id=1774294)",
            "Insert(target_node=IN(type=generator_expression), node=('(', '('), position=0, insert_id=1774295)",
            "Move(target_node=IN(type=generator_expression), node=ASTNode(type=identifier, text=instance), position=1)",
            "Move(target_node=IN(type=generator_expression), node=ASTNode(type=for_in_clause), position=2)",
            "Move(target_node=IN(type=generator_expression), node=ASTNode(type=for_in_clause), position=3)",
            "Move(target_node=IN(type=generator_expression), node=ASTNode(type=if_clause), position=4)",
            "Insert(target_node=IN(type=generator_expression), node=(')', ')'), position=5, insert_id=1774296)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=list_comprehension))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 10,
        "number": 3249,
        "neg_line": [
            "-instances = [instance for key, dataset in all_datasets.items()",
            "-if key in datasets_for_vocab_creation]"
        ],
        "pos_line": [
            "+instances = (instance for key, dataset in all_datasets.items()",
            "+if key in datasets_for_vocab_creation)"
        ],
        "core_change": "-instances = [instance for key, dataset in all_datasets.items() +instances = (instance for key, dataset in all_datasets.items() -if key in datasets_for_vocab_creation] +if key in datasets_for_vocab_creation)",
        "core_API": "info"
    },
    {
        "commit_hash": "63cf61930f7760c56671c0032ae913a6b2be7a27",
        "index": "c599645e..dce25da9 100644",
        "commit_message": "batch and test fixed\n\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ScaleSpaceDetector(nn.Module):",
            "max_coords_best = _scale_index_to_scale(max_coords_best, sigmas_oct)",
            "",
            "# Create local affine frames (LAFs)",
            "-            rotmat = angle_to_rotation_matrix(torch.zeros(B, N))",
            "+            rotmat = angle_to_rotation_matrix(torch.zeros(B, N).to(max_coords_best.device).to(max_coords_best.dtype))",
            "current_lafs = torch.cat([self.mr_size * max_coords_best[:, :, 0].view(B, N, 1, 1) * rotmat,",
            "max_coords_best[:, :, 1:3].view(B, N, 2, 1)], dim=3)",
            "# Normalize LAFs"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=458998)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=458999)",
            "Insert(target_node=IN(type=attribute), node=('call', None), position=0, insert_id=459000)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=459001)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=459002)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=459003)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=459004)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=459005)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=459006)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=459007)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'max_coords_best'), position=0, insert_id=459008)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=459009)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=459010)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=459011)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=459012)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=459013)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=1, insert_id=459014)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=459015)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'max_coords_best'), position=0, insert_id=459016)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=459017)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=459018)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 22,
        "number": 3250,
        "neg_line": [
            "-rotmat = angle_to_rotation_matrix(torch.zeros(B, N))"
        ],
        "pos_line": [
            "+rotmat = angle_to_rotation_matrix(torch.zeros(B, N).to(max_coords_best.device).to(max_coords_best.dtype))"
        ],
        "core_change": "-rotmat = angle_to_rotation_matrix(torch.zeros(B, N)) +rotmat = angle_to_rotation_matrix(torch.zeros(B, N).to(max_coords_best.device).to(max_coords_best.dtype))",
        "core_API": "zeros"
    },
    {
        "commit_hash": "38ba7b439bcdadc73db03bfd7504fae44f74ab93",
        "index": "2736e34d7..374a57c34 100644",
        "commit_message": "fixed BertForMultipleChoice model init and forward pass\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class BertForMultipleChoice(BertPreTrainedModel):",
            "self.num_choices = num_choices",
            "self.bert = BertModel(config)",
            "self.dropout = nn.Dropout(config.hidden_dropout_prob)",
            "-        self.classifier = nn.Linear(config.hidden_size, 1)",
            "+        self.classifier = nn.Linear(config.hidden_size, num_choices)",
            "self.apply(self.init_bert_weights)",
            "",
            "def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):",
            "flat_input_ids = input_ids.view(-1, input_ids.size(-1))",
            "-        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))",
            "-        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1))",
            "+        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None",
            "+        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None",
            "_, pooled_output = self.bert(flat_input_ids, flat_token_type_ids, flat_attention_mask, output_all_encoded_layers=False)",
            "pooled_output = self.dropout(pooled_output)",
            "logits = self.classifier(pooled_output)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=assignment), node=('conditional_expression', None), position=2, insert_id=1547402)",
            "Insert(target_node=ASTNode(type=assignment), node=('conditional_expression', None), position=2, insert_id=1547403)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=1547404)",
            "Insert(target_node=IN(type=conditional_expression), node=('comparison_operator', None), position=2, insert_id=1547405)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=1547406)",
            "Insert(target_node=IN(type=conditional_expression), node=('none', 'None'), position=4, insert_id=1547407)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=1547408)",
            "Insert(target_node=IN(type=conditional_expression), node=('comparison_operator', None), position=2, insert_id=1547409)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=1547410)",
            "Insert(target_node=IN(type=conditional_expression), node=('none', 'None'), position=4, insert_id=1547411)",
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'num_choices'), position=3, insert_id=1547412)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'token_type_ids'), position=0, insert_id=1547413)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=1547414)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=1547415)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=1547416)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'attention_mask'), position=0, insert_id=1547417)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'is'), position=1, insert_id=1547418)",
            "Insert(target_node=IN(type=comparison_operator), node=('is not', 'not'), position=2, insert_id=1547419)",
            "Insert(target_node=IN(type=comparison_operator), node=('none', 'None'), position=3, insert_id=1547420)",
            "Delete(target_node=ASTNode(type=integer, text=1))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 22,
        "number": 3251,
        "neg_line": [
            "-self.classifier = nn.Linear(config.hidden_size, 1)",
            "-flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))",
            "-flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1))"
        ],
        "pos_line": [
            "+self.classifier = nn.Linear(config.hidden_size, num_choices)",
            "+flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None",
            "+flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None"
        ],
        "core_change": "-self.classifier = nn.Linear(config.hidden_size, 1) +self.classifier = nn.Linear(config.hidden_size, num_choices) -flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) -flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) +flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None +flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None",
        "core_API": "Dropout"
    },
    {
        "commit_hash": "580dc8b0e2c6491d4d75b54c3b15b34b462e0c67",
        "index": "a78dd7a3..99bc416e 100644",
        "commit_message": "(mostly) remove from_params (#1191)\n\n* remove from_params\n\n* more progress on merge\n\n* still not working\n\n* get more tests to pass\n\n* more work\n\n* fixing tests\n\n* more\n\n* progress\n\n* yo\n\n* all tests passing\n\n* remove print statements\n\n* get things working\n\n* oops\n\n* more cleanup\n\n* pylint cleanup\n\n* break stuff out\n\n* add some from_params tests\n\n* add missing api doc\n\n* appease the CI overlords\n\n* remove weird empty file\n\n* address pr feedback\n\n* more PR feedback\n\n* remove from_params from ESIM model\n\n* mypy and pylint\n\n* sphinx\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LearningRateScheduler(Registrable):",
            "self.lr_scheduler.step_batch(batch_num_total)",
            "return",
            "",
            "+    # Requires custom from_params",
            "@classmethod",
            "-    def from_params(cls, optimizer: torch.optim.Optimizer, params: Params):",
            "+    def from_params(cls, optimizer: torch.optim.Optimizer, params: Params):  # type: ignore",
            "+        # pylint: disable=arguments-differ",
            "scheduler = params.pop_choice(\"type\", LearningRateScheduler.list_available())",
            "",
            "schedulers = LearningRateScheduler.by_name(scheduler)(optimizer, **params.as_dict())  # type: ignore"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3255,
        "neg_line": [
            "-def from_params(cls, optimizer: torch.optim.Optimizer, params: Params):"
        ],
        "pos_line": [
            "+# Requires custom from_params",
            "+def from_params(cls, optimizer: torch.optim.Optimizer, params: Params):  # type: ignore",
            "+# pylint: disable=arguments-differ"
        ],
        "core_change": "+# Requires custom from_params -def from_params(cls, optimizer: torch.optim.Optimizer, params: Params): +def from_params(cls, optimizer: torch.optim.Optimizer, params: Params):  # type: ignore +# pylint: disable=arguments-differ",
        "core_API": "step_batch"
    },
    {
        "commit_hash": "244e1b5ba331cb4c1ed96d88d0895c252567f7f3",
        "index": "bb1763517..a58ac0a42 100755",
        "commit_message": "Fix #7304 (#7305)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Trainer:",
            "elif is_torch_tpu_available():",
            "# tpu-comment: Get all predictions and labels from all worker shards of eval dataset",
            "if preds is not None:",
            "-                preds = nested_xla_mesh_reduce(\"eval_preds\", preds)",
            "+                preds = nested_xla_mesh_reduce(preds, \"eval_preds\")",
            "if label_ids is not None:",
            "-                label_ids = nested_xla_mesh_reduce(\"eval_label_ids\", label_ids, torch.cat)",
            "+                label_ids = nested_xla_mesh_reduce(label_ids, \"eval_label_ids\")",
            "if eval_losses is not None:",
            "eval_losses = xm.mesh_reduce(\"eval_losses\", torch.tensor(eval_losses), torch.cat).tolist()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('identifier', 'preds'), position=1, insert_id=1232590)",
            "Insert(target_node=ASTNode(type=argument_list), node=('string', '\"eval_preds\"'), position=4, insert_id=1232591)",
            "Insert(target_node=ASTNode(type=argument_list), node=('string', '\"eval_label_ids\"'), position=4, insert_id=1232592)",
            "Delete(target_node=ASTNode(type=string, text=\"eval_preds\"))",
            "Delete(target_node=ASTNode(type=identifier, text=preds))",
            "Delete(target_node=ASTNode(type=string, text=\"eval_label_ids\"))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=cat))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 11,
        "number": 3257,
        "neg_line": [
            "-preds = nested_xla_mesh_reduce(\"eval_preds\", preds)",
            "-label_ids = nested_xla_mesh_reduce(\"eval_label_ids\", label_ids, torch.cat)"
        ],
        "pos_line": [
            "+preds = nested_xla_mesh_reduce(preds, \"eval_preds\")",
            "+label_ids = nested_xla_mesh_reduce(label_ids, \"eval_label_ids\")"
        ],
        "core_change": "-preds = nested_xla_mesh_reduce(\"eval_preds\", preds) +preds = nested_xla_mesh_reduce(preds, \"eval_preds\") -label_ids = nested_xla_mesh_reduce(\"eval_label_ids\", label_ids, torch.cat) +label_ids = nested_xla_mesh_reduce(label_ids, \"eval_label_ids\")",
        "core_API": "mesh_reduce"
    },
    {
        "commit_hash": "ca9c993d6c3c9f59c44d28b22d8968709cd11693",
        "index": "4eff6e5..9efece2 100644",
        "commit_message": "Standardize warnings with `WARNING   ...` (#9467)\n\n* Standardize warnings\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def train(hyp, opt, device, callbacks):  # hyp is path/to/hyp.yaml or hyp dictio",
            "",
            "# DP mode",
            "if cuda and RANK == -1 and torch.cuda.device_count() > 1:",
            "-        LOGGER.warning('WARNING: DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\\n'",
            "+        LOGGER.warning('WARNING  DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\\n'",
            "'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.')",
            "model = torch.nn.DataParallel(model)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text='WARNING: DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\\n'), value=\"'WARNING  DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\\\\n'\")"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3259,
        "neg_line": [
            "-LOGGER.warning('WARNING: DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\\n'"
        ],
        "pos_line": [
            "+LOGGER.warning('WARNING  DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\\n'"
        ],
        "core_change": "-LOGGER.warning('WARNING: DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\\n' +LOGGER.warning('WARNING  DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\\n'",
        "core_API": "device_count"
    },
    {
        "commit_hash": "e5dcceb82ca2c1e0d23a57f4856940a36c810a5b",
        "index": "60522344d..5149ff39c 100644",
        "commit_message": "Fixes to TF collators (#21143)\n\n* Add num_workers for prepare_tf_dataset\n\n* Bugfix in the default collator and change default tensor type\n\n* Remove the \"num_workers\" arg and move it to a new PR\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def tf_default_data_collator(features: List[InputDataClass]) -> Dict[str, Any]:",
            "label_col_name = None",
            "if label_col_name is not None:",
            "if isinstance(first[label_col_name], tf.Tensor):",
            "-            dtype = tf.int64 if first[label_col_name].dtype.is_integer() else tf.float32",
            "+            dtype = tf.int64 if first[label_col_name].dtype.is_integer else tf.float32",
            "elif isinstance(first[label_col_name], np.ndarray) or isinstance(first[label_col_name], np.generic):",
            "dtype = tf.int64 if np.issubdtype(first[label_col_name].dtype, np.integer) else tf.float32",
            "elif isinstance(first[label_col_name], (tuple, list)):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=conditional_expression), node=ASTNode(type=attribute), position=2)",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 3260,
        "neg_line": [
            "-dtype = tf.int64 if first[label_col_name].dtype.is_integer() else tf.float32"
        ],
        "pos_line": [
            "+dtype = tf.int64 if first[label_col_name].dtype.is_integer else tf.float32"
        ],
        "core_change": "-dtype = tf.int64 if first[label_col_name].dtype.is_integer() else tf.float32 +dtype = tf.int64 if first[label_col_name].dtype.is_integer else tf.float32",
        "core_API": "is_integer"
    },
    {
        "commit_hash": "cb1a61568bc8fa0d4d76b0595a1cee3a0da8fe99",
        "index": "af75e28f..3d6ebf80 100644",
        "commit_message": "fix bart directory not found (#2341)\n\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def load_bart_od():",
            "-   \"counts\": a ``torch.FloatTensor`` of ridership counts, with shape",
            "``(num_hours, len(stations), len(stations))``.",
            "\"\"\"",
            "+    _mkdir_p(DATA)",
            "filename = os.path.join(DATA, \"bart_full.pkl.bz2\")",
            "# Work around apparent bug in torch.load(),torch.save().",
            "pkl_file = filename.rsplit(\".\", 1)[0]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=string, text=\"counts\"))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=identifier, text=a))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=string, text=``))",
            "Delete(target_node=ASTNode(type=concatenated_string))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=ERROR))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=FloatTensor))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=string, text=``))",
            "Delete(target_node=ASTNode(type=identifier, text=of))",
            "Delete(target_node=ASTNode(type=identifier, text=ridership))",
            "Delete(target_node=ASTNode(type=identifier, text=counts))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=with))",
            "Delete(target_node=ASTNode(type=identifier, text=shape))",
            "Delete(target_node=ASTNode(type=ERROR))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 19,
        "number": 3263,
        "neg_line": [
            "-\"counts\": a ``torch.FloatTensor`` of ridership counts, with shape"
        ],
        "pos_line": [
            "+_mkdir_p(DATA)"
        ],
        "core_change": "-\"counts\": a ``torch.FloatTensor`` of ridership counts, with shape +_mkdir_p(DATA)",
        "core_API": "join"
    },
    {
        "commit_hash": "0af8eb6aa9c1b40e6805fd3dd84b65d101cb55f8",
        "index": "8bbb3e4f..7b6f0d3e 100644",
        "commit_message": "[Feat] Jit warp perspective (#574)\n\n* Torchscriptable warp_perspective()\n\n* Simplify device and type casting\n\n* fix tests\n\n* Remove redundant parameter and update docs of warp_grid\n\n* Update docs\n\n* fix typing errors\n\n* Fixed homography warper tests\n\n* Added jit warp perspective tests\n\n* Added jit warp perspective tests\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def warp_perspective(src: torch.Tensor, M: torch.Tensor, dsize: Tuple[int, int],",
            "See a working example `here <https://kornia.readthedocs.io/en/latest/",
            "tutorials/warp_perspective.html>`_.",
            "\"\"\"",
            "-    if not torch.is_tensor(src):",
            "-        raise TypeError(\"Input src type is not a torch.Tensor. Got {}\"",
            "-                        .format(type(src)))",
            "-",
            "-    if not torch.is_tensor(M):",
            "-        raise TypeError(\"Input M type is not a torch.Tensor. Got {}\"",
            "-                        .format(type(M)))",
            "+    check_is_tensor(src)",
            "+    check_is_tensor(M)",
            "",
            "if not len(src.shape) == 4:",
            "raise ValueError(\"Input src must be a BxCxHxW tensor. Got {}\""
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 6,
        "AST_diff_line": 17,
        "number": 3264,
        "neg_line": [
            "-if not torch.is_tensor(src):",
            "-raise TypeError(\"Input src type is not a torch.Tensor. Got {}\"",
            "-.format(type(src)))",
            "-",
            "-if not torch.is_tensor(M):",
            "-raise TypeError(\"Input M type is not a torch.Tensor. Got {}\"",
            "-.format(type(M)))"
        ],
        "pos_line": [
            "+check_is_tensor(src)",
            "+check_is_tensor(M)"
        ],
        "core_change": "-if not torch.is_tensor(src): -raise TypeError(\"Input src type is not a torch.Tensor. Got {}\" -.format(type(src))) - -if not torch.is_tensor(M): -raise TypeError(\"Input M type is not a torch.Tensor. Got {}\" -.format(type(M))) +check_is_tensor(src) +check_is_tensor(M)",
        "core_API": "is_tensor"
    },
    {
        "commit_hash": "99abd7848969d44a5d45d3b1203de653e6c0292e",
        "index": "1f6726da79..38d0b1b1dc 100644",
        "commit_message": "more lint fix\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def test_tensorflow_logical_xor(",
            "dtype_and_x=helpers.dtype_and_values(",
            "available_dtypes=tuple(",
            "set(ivy_np.valid_float_dtypes).intersection(",
            "-            set(ivy_tf.valid_float_dtypes)",
            "+                set(ivy_tf.valid_float_dtypes)",
            ")",
            "),",
            "num_arrays=2,"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3266,
        "neg_line": [
            "-set(ivy_tf.valid_float_dtypes)"
        ],
        "pos_line": [
            "+set(ivy_tf.valid_float_dtypes)"
        ],
        "core_change": "-set(ivy_tf.valid_float_dtypes) +set(ivy_tf.valid_float_dtypes)",
        "core_API": "dtype_and_values"
    },
    {
        "commit_hash": "0ff57efdb239c09cd2532ba822e6447e178a5ec9",
        "index": "c27b5e8e..c04f0666 100644",
        "commit_message": "fixed affine 2d shearing matrix translations (#612)\n\n* fixed affine 2d shearing matrix translations\n\n* Updated test_affine.py\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_affine_matrix2d(translations: torch.Tensor, center: torch.Tensor, scale:",
            "sy_tan = torch.tan(sy)  # type: ignore",
            "zeros = torch.zeros_like(sx)  # type: ignore",
            "ones = torch.ones_like(sx)  # type: ignore",
            "-        shear_mat = torch.stack([ones, -sx_tan, sx_tan * x,  # type: ignore   # noqa: E241",
            "-                                 -sy_tan, ones + sx_tan * sy_tan, sy_tan * (-sx_tan * x + y)],  # noqa: E241",
            "+        shear_mat = torch.stack([ones, -sx_tan, sx_tan * y,  # type: ignore   # noqa: E241",
            "+                                 -sy_tan, ones + sx_tan * sy_tan, sy_tan * (sx_tan * y + x)],  # noqa: E241",
            "dim=-1).view(-1, 2, 3)",
            "shear_mat = convert_affinematrix_to_homography(shear_mat)",
            "transform_h = transform_h @ shear_mat"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=x), value='y')",
            "Update(target_node=ASTNode(type=identifier, text=y), value='x')",
            "Move(target_node=ASTNode(type=binary_operator), node=ASTNode(type=identifier, text=sx_tan), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=x), value='y')",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=unary_operator))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 6,
        "number": 3269,
        "neg_line": [
            "-shear_mat = torch.stack([ones, -sx_tan, sx_tan * x,  # type: ignore   # noqa: E241",
            "--sy_tan, ones + sx_tan * sy_tan, sy_tan * (-sx_tan * x + y)],  # noqa: E241"
        ],
        "pos_line": [
            "+shear_mat = torch.stack([ones, -sx_tan, sx_tan * y,  # type: ignore   # noqa: E241",
            "+-sy_tan, ones + sx_tan * sy_tan, sy_tan * (sx_tan * y + x)],  # noqa: E241"
        ],
        "core_change": "-shear_mat = torch.stack([ones, -sx_tan, sx_tan * x,  # type: ignore   # noqa: E241 --sy_tan, ones + sx_tan * sy_tan, sy_tan * (-sx_tan * x + y)],  # noqa: E241 +shear_mat = torch.stack([ones, -sx_tan, sx_tan * y,  # type: ignore   # noqa: E241 +-sy_tan, ones + sx_tan * sy_tan, sy_tan * (sx_tan * y + x)],  # noqa: E241",
        "core_API": "tan"
    },
    {
        "commit_hash": "734b7e2a5a717ccdebbda3f98da7ea947b417b65",
        "index": "e4c36109b..1e70ba773 100644",
        "commit_message": "Mask t5 relative position bias then head pruned (#17968)\n\n* add position bias head masking if heads pruned\n\n* fix pruning function in t5 encoder\n\n* make style\n\n* make fix-copies\n\n* Revert added folder\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class T5EncoderModel(T5PreTrainedModel):",
            "class PreTrainedModel",
            "\"\"\"",
            "for layer, heads in heads_to_prune.items():",
            "-            self.encoder.layer[layer].attention.prune_heads(heads)",
            "+            self.encoder.block[layer].layer[0].SelfAttention.prune_heads(heads)",
            "",
            "@add_start_docstrings_to_model_forward(T5_ENCODER_INPUTS_DOCSTRING)",
            "@replace_return_docstrings(output_type=BaseModelOutput, config_class=_CONFIG_FOR_DOC)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('subscript', None), position=0, insert_id=1192472)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1192473)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'SelfAttention'), position=2, insert_id=1192474)",
            "Insert(target_node=IN(type=subscript), node=('attribute', None), position=0, insert_id=1192475)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=1192476)",
            "Insert(target_node=IN(type=subscript), node=('integer', '0'), position=2, insert_id=1192477)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=1192478)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=subscript), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=attention), value='layer')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=attention), position=2)",
            "Update(target_node=ASTNode(type=identifier, text=layer), value='block')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 3271,
        "neg_line": [
            "-self.encoder.layer[layer].attention.prune_heads(heads)"
        ],
        "pos_line": [
            "+self.encoder.block[layer].layer[0].SelfAttention.prune_heads(heads)"
        ],
        "core_change": "-self.encoder.layer[layer].attention.prune_heads(heads) +self.encoder.block[layer].layer[0].SelfAttention.prune_heads(heads)",
        "core_API": "items"
    },
    {
        "commit_hash": "0a3afb1a4577831c199661758fbac388769af0b9",
        "index": "8035f41..a870179 100644",
        "commit_message": "bring in the pseudo 3d conv for the 3dunet from make-a-video, and add the new prediction objective from the progressive distillation paper, which allows for distillation, and noted in imagen video to improve upresoluting unets (fixes the color shifting issue)\n\n",
        "file": "imagen-pytorch.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ElucidatedImagen(nn.Module):",
            "",
            "lowres_cond_img_noisy = None",
            "if exists(lowres_cond_img):",
            "-            lowres_cond_img_noisy, _ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_aug_times, noise = torch.randn_like(lowres_cond_img))",
            "+            lowres_cond_img_noisy, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_aug_times, noise = torch.randn_like(lowres_cond_img))",
            "",
            "# get the sigmas"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=pattern_list), node=('list_splat_pattern', None), position=2, insert_id=252527)",
            "Insert(target_node=IN(type=list_splat_pattern), node=('*', '*'), position=0, insert_id=252528)",
            "Move(target_node=IN(type=list_splat_pattern), node=ASTNode(type=identifier, text=_), position=1)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 3272,
        "neg_line": [
            "-lowres_cond_img_noisy, _ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_aug_times, noise = torch.randn_like(lowres_cond_img))"
        ],
        "pos_line": [
            "+lowres_cond_img_noisy, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_aug_times, noise = torch.randn_like(lowres_cond_img))"
        ],
        "core_change": "-lowres_cond_img_noisy, _ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_aug_times, noise = torch.randn_like(lowres_cond_img)) +lowres_cond_img_noisy, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, t = lowres_aug_times, noise = torch.randn_like(lowres_cond_img))",
        "core_API": "q_sample"
    },
    {
        "commit_hash": "4f68c09c91e669d388da911475445fdbb023b011",
        "index": "92a42d14..52bd09c2 100644",
        "commit_message": "fixed some examples\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def local_guide(i, datum):",
            "",
            "def inspect_posterior_samples(i):",
            "c = local_guide(i, None)",
            "-    mean_param = Variable(torch.zeros(784), requires_grad=True)",
            "+    mean_param = Variable(torch.zeros(784, 1), requires_grad=True)",
            "# do MLE for class means",
            "m = pyro.param(\"mean_of_class_\" + str(c[0]), mean_param)",
            "sigma = Variable(torch.ones(m.size()))"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=772596)",
            "Insert(target_node=ASTNode(type=argument_list), node=('integer', '1'), position=3, insert_id=772597)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 3273,
        "neg_line": [
            "-mean_param = Variable(torch.zeros(784), requires_grad=True)"
        ],
        "pos_line": [
            "+mean_param = Variable(torch.zeros(784, 1), requires_grad=True)"
        ],
        "core_change": "-mean_param = Variable(torch.zeros(784), requires_grad=True) +mean_param = Variable(torch.zeros(784, 1), requires_grad=True)",
        "core_API": "zeros"
    },
    {
        "commit_hash": "ddebbdc53544c64a1957cf0ec04a34578e9b57ba",
        "index": "4c8694ff..f3ddd243 100644",
        "commit_message": "Make most metrics work on GPU (#3851)\n\n* Make most metrics work on GPU\n\n* Make metric tests work on both GPU and CPU\n\n* Add a test for the test utility\n\n* mypy\n\n* Update allennlp/common/testing/test_case.py\n\nCo-Authored-By: Mark Neumann <markn@allenai.org>\n\n* Fix a PR comment\n\n* flake8\n\nCo-authored-by: Mark Neumann <markn@allenai.org>\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Auc(Metric):",
            "if self._all_gold_labels.shape[0] == 0:",
            "return 0.5",
            "false_positive_rates, true_positive_rates, _ = metrics.roc_curve(",
            "-            self._all_gold_labels.numpy(),",
            "-            self._all_predictions.numpy(),",
            "+            self._all_gold_labels.cpu().numpy(),",
            "+            self._all_predictions.cpu().numpy(),",
            "pos_label=self._positive_label,",
            ")",
            "auc = metrics.auc(false_positive_rates, true_positive_rates)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=1, insert_id=20571)",
            "Insert(target_node=ASTNode(type=argument_list), node=('call', None), position=4, insert_id=20572)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=20573)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=20574)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=20575)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=20576)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'numpy'), position=2, insert_id=20577)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=20578)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=20579)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=20580)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'numpy'), position=2, insert_id=20581)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=20582)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Update(target_node=ASTNode(type=identifier, text=numpy), value='cpu')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=20583)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=20584)",
            "Update(target_node=ASTNode(type=identifier, text=numpy), value='cpu')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 20,
        "number": 3274,
        "neg_line": [
            "-self._all_gold_labels.numpy(),",
            "-self._all_predictions.numpy(),"
        ],
        "pos_line": [
            "+self._all_gold_labels.cpu().numpy(),",
            "+self._all_predictions.cpu().numpy(),"
        ],
        "core_change": "-self._all_gold_labels.numpy(), -self._all_predictions.numpy(), +self._all_gold_labels.cpu().numpy(), +self._all_predictions.cpu().numpy(),",
        "core_API": "roc_curve"
    },
    {
        "commit_hash": "83b1e6ac9e81cbb053ee272a4a4fcb0b6fac06ab",
        "index": "30853c68c..59ae8a929 100644",
        "commit_message": "fix the loss backward issue\n\n(cherry picked from commit 566468cc984c6ec7e10dfc62b5b4191781a99cd2)\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def perturb_past(",
            "print(' pplm_loss', (loss - kl_loss).data.cpu().numpy())",
            "",
            "# compute gradients",
            "-        loss.backward(retain_graph=True)",
            "+        loss.backward()",
            "",
            "# calculate gradient norms",
            "if grad_norms is not None and loss_type == PPLM_BOW:"
        ],
        "hunk_index": 4,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=retain_graph))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=true, text=True))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 4,
        "number": 3275,
        "neg_line": [
            "-loss.backward(retain_graph=True)"
        ],
        "pos_line": [
            "+loss.backward()"
        ],
        "core_change": "-loss.backward(retain_graph=True) +loss.backward()",
        "core_API": "cpu"
    },
    {
        "commit_hash": "f3c9542a2ced111f0d26a1c60d34b711d861245f",
        "index": "7eae361681..b9e042643f 100644",
        "commit_message": "Fixed failing test for statistical einsum (#6073)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def einsum(",
            "*operands: torch.Tensor,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "+    dtype = _get_promoted_type_of_operands(operands)",
            "operands = (operand.to(torch.float32) for operand in operands)",
            "-    return torch.einsum(equation, *operands)",
            "+    return torch.einsum(equation, *operands).to(dtype)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=function_definition), node=('block', None), position=6, insert_id=311028)",
            "Insert(target_node=ASTNode(type=return_statement), node=('call', None), position=1, insert_id=311029)",
            "Insert(target_node=IN(type=block), node=('expression_statement', None), position=0, insert_id=311030)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=311031)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=311032)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=311033)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=311034)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=311035)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=311036)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'dtype'), position=1, insert_id=311037)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=311038)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'dtype'), position=0, insert_id=311039)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=311040)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=311041)",
            "Insert(target_node=IN(type=call), node=('identifier', '_get_promoted_type_of_operands'), position=0, insert_id=311042)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=311043)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=311044)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'operands'), position=1, insert_id=311045)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=311046)",
            "Delete(target_node=ASTNode(type=block, text=))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 21,
        "number": 3277,
        "neg_line": [
            "-return torch.einsum(equation, *operands)"
        ],
        "pos_line": [
            "+dtype = _get_promoted_type_of_operands(operands)",
            "+return torch.einsum(equation, *operands).to(dtype)"
        ],
        "core_change": "+dtype = _get_promoted_type_of_operands(operands) -return torch.einsum(equation, *operands) +return torch.einsum(equation, *operands).to(dtype)",
        "core_API": "to"
    },
    {
        "commit_hash": "24694832c53bb1ac6cbc05a0c6e9cef014b7815f",
        "index": "df493c11..b26ce1ad 100644",
        "commit_message": "GH20: Fix bug in single label training.\n\n",
        "file": "flair.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class DocumentLSTMEmbeddings(DocumentEmbeddings):",
            "# EXTRACT EMBEDDINGS FROM LSTM",
            "# --------------------------------------------------------------------",
            "for sentence_no, length in enumerate(lengths):",
            "-            last_rep = outputs[length - 1, sentence_no, :].unsqueeze(0)",
            "+            last_rep = outputs[length - 1, sentence_no].unsqueeze(0)",
            "",
            "embedding = last_rep",
            "if self.use_first_representation:",
            "-                first_rep = outputs[0, sentence_no, :].unsqueeze(0)",
            "+                first_rep = outputs[0, sentence_no].unsqueeze(0)",
            "embedding = torch.cat([first_rep, last_rep], 1)",
            "",
            "sentence = sentences[sentence_no]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=243451)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=243452)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '0'), position=1, insert_id=243453)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=243454)",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=slice))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=slice))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=integer, text=0))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 15,
        "number": 3279,
        "neg_line": [
            "-last_rep = outputs[length - 1, sentence_no, :].unsqueeze(0)",
            "-first_rep = outputs[0, sentence_no, :].unsqueeze(0)"
        ],
        "pos_line": [
            "+last_rep = outputs[length - 1, sentence_no].unsqueeze(0)",
            "+first_rep = outputs[0, sentence_no].unsqueeze(0)"
        ],
        "core_change": "-last_rep = outputs[length - 1, sentence_no, :].unsqueeze(0) +last_rep = outputs[length - 1, sentence_no].unsqueeze(0) -first_rep = outputs[0, sentence_no, :].unsqueeze(0) +first_rep = outputs[0, sentence_no].unsqueeze(0)",
        "core_API": "cat"
    },
    {
        "commit_hash": "adbe603af1d76f2764c5c281190edd1291f0ac32",
        "index": "19d8924e..5c6c3f02 100644",
        "commit_message": "Bug fixes\n\n",
        "file": "TTS.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class AttentionRNNCell(nn.Module):",
            "memory_dim (int): memory vector (decoder autogression) feature dimension.",
            "align_model (str): 'b' for Bahdanau, 'ls' Location Sensitive alignment.",
            "\"\"\"",
            "-        super(AttentionRNN, self).__init__()",
            "+        super(AttentionRNNCell, self).__init__()",
            "self.align_model = align_model",
            "self.rnn_cell = nn.GRUCell(out_dim + memory_dim, out_dim)",
            "# pick bahdanau or location sensitive attention"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=AttentionRNN), value='AttentionRNNCell')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3282,
        "neg_line": [
            "-super(AttentionRNN, self).__init__()"
        ],
        "pos_line": [
            "+super(AttentionRNNCell, self).__init__()"
        ],
        "core_change": "-super(AttentionRNN, self).__init__() +super(AttentionRNNCell, self).__init__()",
        "core_API": "GRUCell"
    },
    {
        "commit_hash": "c7fd1d9fb18862318b133c9474d37c5085850070",
        "index": "48ea98e4..1d4fe83d 100755",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class ResNet_Cifar(ModelDesc):",
            "ce_cost = tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits)",
            "ce_cost = tf.reduce_mean(ce_cost, name='cross_entropy_loss')",
            "",
            "-        single_label = tf.to_int32(tf.argmax(label, axis=1))",
            "-        wrong = tf.to_float(tf.logical_not(tf.nn.in_top_k(logits, single_label, 1)), name='wrong_vector')",
            "+        single_label = tf.cast(tf.argmax(label, axis=1), tf.int32)",
            "+        wrong = tf.cast(tf.logical_not(tf.nn.in_top_k(logits, single_label, 1)), tf.float32, name='wrong_vector')",
            "# monitor training error",
            "add_moving_summary(tf.reduce_mean(wrong, name='train_error'), ce_cost)",
            "add_param_summary(('.*/W', ['histogram']))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=to_int32), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=2278926)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2278927)",
            "Update(target_node=ASTNode(type=identifier, text=to_float), value='cast')",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=2278928)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2278929)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2278930)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2278931)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'int32'), position=2, insert_id=2278932)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=2278933)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2278934)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'float32'), position=2, insert_id=2278935)"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 12,
        "number": 3283,
        "neg_line": [
            "-single_label = tf.to_int32(tf.argmax(label, axis=1))",
            "-wrong = tf.to_float(tf.logical_not(tf.nn.in_top_k(logits, single_label, 1)), name='wrong_vector')"
        ],
        "pos_line": [
            "+single_label = tf.cast(tf.argmax(label, axis=1), tf.int32)",
            "+wrong = tf.cast(tf.logical_not(tf.nn.in_top_k(logits, single_label, 1)), tf.float32, name='wrong_vector')"
        ],
        "core_change": "-single_label = tf.to_int32(tf.argmax(label, axis=1)) -wrong = tf.to_float(tf.logical_not(tf.nn.in_top_k(logits, single_label, 1)), name='wrong_vector') +single_label = tf.cast(tf.argmax(label, axis=1), tf.int32) +wrong = tf.cast(tf.logical_not(tf.nn.in_top_k(logits, single_label, 1)), tf.float32, name='wrong_vector')",
        "core_API": "softmax_cross_entropy_with_logits"
    },
    {
        "commit_hash": "b370cc7e99c5b8c7436154d4694c33b461ea0f08",
        "index": "3a8a9c541..21fc87323 100644",
        "commit_message": "[gpu] Fixup fdd61b19928e87a5354c36923182e801bfedb31b\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GPT2ModelTest(ModelTesterMixin, unittest.TestCase):",
            "",
            "# append to next input_ids and attn_mask",
            "next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)",
            "-            attn_mask = torch.cat([attn_mask, torch.ones((attn_mask.shape[0], 1)).long()], dim=1)",
            "+            attn_mask = torch.cat(",
            "+                [attn_mask, torch.ones((attn_mask.shape[0], 1), dtype=torch.long, device=torch_device)], dim=1",
            "+            )",
            "",
            "# get two different outputs",
            "output_from_no_past, _ = model(next_input_ids, attention_mask=attn_mask)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1241310)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1241311)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=1241312)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=1241313)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'dtype'), position=0, insert_id=1241314)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1241315)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1241316)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1241317)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1241318)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'torch_device'), position=2, insert_id=1241319)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1241320)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=long), position=2)",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3284,
        "neg_line": [
            "-attn_mask = torch.cat([attn_mask, torch.ones((attn_mask.shape[0], 1)).long()], dim=1)"
        ],
        "pos_line": [
            "+attn_mask = torch.cat(",
            "+[attn_mask, torch.ones((attn_mask.shape[0], 1), dtype=torch.long, device=torch_device)], dim=1",
            "+)"
        ],
        "core_change": "-attn_mask = torch.cat([attn_mask, torch.ones((attn_mask.shape[0], 1)).long()], dim=1) +attn_mask = torch.cat( +[attn_mask, torch.ones((attn_mask.shape[0], 1), dtype=torch.long, device=torch_device)], dim=1 +)",
        "core_API": "cat"
    },
    {
        "commit_hash": "e5a208bac69d34469652430fe42c220ed47c5a72",
        "index": "5d636c87..252ede28 100644",
        "commit_message": "fix speedup with CUDA (#2947)\n\n\n",
        "file": "nni.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def conv2d_mask(module_masks, mask):",
            "if index is None:",
            "return None, None, None",
            "else:",
            "-            index = torch.LongTensor(index).to(weight_mask.device)",
            "+            index = index.long().to(weight_mask.device)",
            "weight_cmask = CoarseMask(num_dim=4)",
            "weight_cmask.add_index_mask(dim=dim, index=index)",
            "bias_cmask = None"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=attribute), node=ASTNode(type=identifier, text=index), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=LongTensor), value='long')",
            "Delete(target_node=ASTNode(type=identifier, text=torch))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 3287,
        "neg_line": [
            "-index = torch.LongTensor(index).to(weight_mask.device)"
        ],
        "pos_line": [
            "+index = index.long().to(weight_mask.device)"
        ],
        "core_change": "-index = torch.LongTensor(index).to(weight_mask.device) +index = index.long().to(weight_mask.device)",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "c51dc4f92755c67a83f3fc8a0bd6b3e64df199e4",
        "index": "b564dcdb6..936e40656 100644",
        "commit_message": "[torch] remove deprecated uint8 in favor of bool (#21384)\n\n* uint8 -> bool\n\n* fix copies\n\n* style\n\n* update test modeling commen when checking attention buffers\n\n* style\n\n* use logical not on random mask instead of subtraction with 1\n\n* remove torch uint8\n\n* quality\n\n* remove modified modeling utils\n\n* Update based on review\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n\n---------\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CodeGenAttention(nn.Module):",
            "max_positions = config.max_position_embeddings",
            "self.register_buffer(",
            "\"causal_mask\",",
            "-            torch.tril(torch.ones((max_positions, max_positions), dtype=torch.uint8)).view(",
            "+            torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view(",
            "1, 1, max_positions, max_positions",
            "),",
            ")"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=uint8), value='bool')"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 1,
        "number": 3288,
        "neg_line": [
            "-torch.tril(torch.ones((max_positions, max_positions), dtype=torch.uint8)).view("
        ],
        "pos_line": [
            "+torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view("
        ],
        "core_change": "-torch.tril(torch.ones((max_positions, max_positions), dtype=torch.uint8)).view( +torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view(",
        "core_API": "register_buffer"
    },
    {
        "commit_hash": "7dc58bd286b1e81ca4d293f05bddff5e93361020",
        "index": "4417e5e02..427312eca 100644",
        "commit_message": "Refactor model summary + generalize example input array (#1773)\n\n* squash\n\nvariant a\n\n\nvariant b\n\n\nadd test\n\n\nrevert rename\n\n\nadd changelog\n\n\ndocs\n\n\nmove changelog entry to top\n\n\nuse hooks\n\n\nwip\n\n\nwipp\n\n\nlayer summary\n\n\nclean up, refactor\n\n\ntype hints\n\n\nrename\n\n\nremove obsolete code\n\n\nrename\n\n\nunused imports\n\n\nsimplify formatting of table and increase readability\n\n\ndoctest\n\n\nsuperclass object\n\n\nupdate examples\n\n\nprint unknown sizes\n\n\nmore docs and doctest\n\n\ntesting\n\n\nunknown layers\n\n\nadd rnn test\n\n\nremove main\n\n\nrestore train mode\n\n\ntest device wip\n\n\ndevice\n\n\nconstant\n\n\nsimplify model forward transfer\n\n\nreturn summary object in method\n\n\nextend tests\n\n\nfix summary for empty module\n\n\nextend tests\n\n\nrefactor and added hook\n\n\nvariant a\n\n\nvariant b\n\n\nadd test\n\n\nrevert rename\n\n\nadd changelog\n\n\ndocs\n\n\nmove changelog entry to top\n\n\nremove hardcoded string\n\n\nsimplify\n\n\ntest unknown shapes and all others\n\n\ncomments for tests\n\n\nfix hparams attribute\n\n* update default\n\n* unused import\n\n* clean up\n\n* replace hardcoded strings\n\n* fix doctest\n\n* fix top/full\n\n* black\n\n* fix rnn test\n\n* fix rnn\n\n* update debugging docs\n\n\nupdate docs\n\n\ntypo\n\n\nupdate docs\n\n\nupdate docs\n\n* add changelog\n\n* extract constant\n\n* setter and getter\n\n* move parity models to test folder\n\n* parameterize mode\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GAN(LightningModule):",
            "",
            "self.validation_z = torch.randn(8, self.latent_dim)",
            "",
            "+        self.example_input_array = torch.zeros(2, hparams.latent_dim)",
            "+",
            "def forward(self, z):",
            "return self.generator(z)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=2, insert_id=579688)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=579689)",
            "Insert(target_node=IN(type=assignment), node=('attribute', None), position=0, insert_id=579690)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=579691)",
            "Insert(target_node=IN(type=assignment), node=('call', None), position=2, insert_id=579692)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=579693)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=579694)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'example_input_array'), position=2, insert_id=579695)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=579696)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=579697)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=579698)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=579699)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'zeros'), position=2, insert_id=579700)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=579701)",
            "Insert(target_node=IN(type=argument_list), node=('integer', '2'), position=1, insert_id=579702)",
            "Insert(target_node=IN(type=argument_list), node=(',', ','), position=2, insert_id=579703)",
            "Insert(target_node=IN(type=argument_list), node=('attribute', None), position=3, insert_id=579704)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=4, insert_id=579705)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'hparams'), position=0, insert_id=579706)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=579707)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'latent_dim'), position=2, insert_id=579708)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 21,
        "number": 3291,
        "neg_line": [],
        "pos_line": [
            "+self.example_input_array = torch.zeros(2, hparams.latent_dim)",
            "+"
        ],
        "core_change": "+self.example_input_array = torch.zeros(2, hparams.latent_dim) +",
        "core_API": "randn"
    },
    {
        "commit_hash": "428516056abe41f135133e732a8d44af6ce9a234",
        "index": "deaf68d99..50c98f579 100644",
        "commit_message": "[RLlib] SAC Torch (incl. Atari learning) (#7984)\n\n* Policy-classes cleanup and torch/tf unification.\n- Make Policy abstract.\n- Add `action_dist` to call to `extra_action_out_fn` (necessary for PPO torch).\n- Move some methods and vars to base Policy\n  (from TFPolicy): num_state_tensors, ACTION_PROB, ACTION_LOGP and some more.\n\n* Fix `clip_action` import from Policy (should probably be moved into utils altogether).\n\n* - Move `is_recurrent()` and `num_state_tensors()` into TFPolicy (from DynamicTFPolicy).\n- Add config to all Policy c'tor calls (as 3rd arg after obs and action spaces).\n\n* Add `config` to c'tor call to TFPolicy.\n\n* Add missing `config` to c'tor call to TFPolicy in marvil_policy.py.\n\n* Fix test_rollout_worker.py::MockPolicy and BadPolicy classes (Policy base class is now abstract).\n\n* Fix LINT errors in Policy classes.\n\n* Implement StatefulPolicy abstract methods in test cases: test_multi_agent_env.py.\n\n* policy.py LINT errors.\n\n* Create a simple TestPolicy to sub-class from when testing Policies (reduces code in some test cases).\n\n* policy.py\n- Remove abstractmethod from `apply_gradients` and `compute_gradients` (these are not required iff `learn_on_batch` implemented).\n- Fix docstring of `num_state_tensors`.\n\n* Make QMIX torch Policy a child of TorchPolicy (instead of Policy).\n\n* QMixPolicy add empty implementations of abstract Policy methods.\n\n* Store Policy's config in self.config in base Policy c'tor.\n\n* - Make only compute_actions in base Policy's an abstractmethod and provide pass\nimplementation to all other methods if not defined.\n- Fix state_batches=None (most Policies don't have internal states).\n\n* Cartpole tf learning.\n\n* Cartpole tf AND torch learning (in ~ same ts).\n\n* Cartpole tf AND torch learning (in ~ same ts). 2\n\n* Cartpole tf (torch syntax-broken) learning (in ~ same ts). 3\n\n* Cartpole tf AND torch learning (in ~ same ts). 4\n\n* Cartpole tf AND torch learning (in ~ same ts). 5\n\n* Cartpole tf AND torch learning (in ~ same ts). 6\n\n* Cartpole tf AND torch learning (in ~ same ts). Pendulum tf learning.\n\n* WIP.\n\n* WIP.\n\n* SAC torch learning Pendulum.\n\n* WIP.\n\n* SAC torch and tf learning Pendulum and Cartpole after cleanup.\n\n* WIP.\n\n* LINT.\n\n* LINT.\n\n* SAC: Move policy.target_model to policy.device as well.\n\n* Fixes and cleanup.\n\n* Fix data-format of tf keras Conv2d layers (broken for some tf-versions which have data_format=\"channels_first\" as default).\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fix and LINT.\n\n* WIP.\n\n* Test fixes and LINT.\n\n* Fixes and LINT.\n\nCo-authored-by: Sven Mika <sven@Svens-MacBook-Pro.local>\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GaussianNoise(Exploration):",
            "action = action_dist.deterministic_sample()",
            "",
            "# Logp=always zero.",
            "-        logp = torch.zeros(shape=(action.size()[0], ), dtype=torch.float32)",
            "+        logp = torch.zeros(",
            "+            (action.size()[0], ), dtype=torch.float32, device=self.device)",
            "",
            "return action, logp"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=tuple), position=1)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=5, insert_id=1124839)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=6, insert_id=1124840)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'device'), position=0, insert_id=1124841)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1124842)",
            "Insert(target_node=IN(type=keyword_argument), node=('attribute', None), position=2, insert_id=1124843)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=1124844)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1124845)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'device'), position=2, insert_id=1124846)",
            "Delete(target_node=ASTNode(type=identifier, text=shape))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=keyword_argument))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 12,
        "number": 3293,
        "neg_line": [
            "-logp = torch.zeros(shape=(action.size()[0], ), dtype=torch.float32)"
        ],
        "pos_line": [
            "+logp = torch.zeros(",
            "+(action.size()[0], ), dtype=torch.float32, device=self.device)"
        ],
        "core_change": "-logp = torch.zeros(shape=(action.size()[0], ), dtype=torch.float32) +logp = torch.zeros( +(action.size()[0], ), dtype=torch.float32, device=self.device)",
        "core_API": "deterministic_sample"
    },
    {
        "commit_hash": "6ba2231d7227825dfb76e9df161824d04234e69f",
        "index": "75644260..096c083b 100644",
        "commit_message": "Reproducibility 3/3 (#1924)\n\n* make tests deterministic\n\n* run slow tests\n\n* prepare for testing\n\n* finish\n\n* refactor\n\n* add print statements\n\n* finish more\n\n* correct some test failures\n\n* more fixes\n\n* set up to correct tests\n\n* more corrections\n\n* up\n\n* fix more\n\n* more prints\n\n* add\n\n* up\n\n* up\n\n* up\n\n* uP\n\n* uP\n\n* more fixes\n\n* uP\n\n* up\n\n* up\n\n* up\n\n* up\n\n* fix more\n\n* up\n\n* up\n\n* clean tests\n\n* up\n\n* up\n\n* up\n\n* more fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* make\n\n* correct\n\n* finish\n\n* finish\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class StableDiffusionInpaintPipelineIntegrationTests(unittest.TestCase):",
            "",
            "prompt = \"Face of a yellow cat, high resolution, sitting on a park bench\"",
            "",
            "-        generator = torch.Generator(device=torch_device).manual_seed(0)",
            "+        generator = torch.manual_seed(0)",
            "_ = pipe(",
            "prompt=prompt,",
            "image=init_image,"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Delete(target_node=ASTNode(type=identifier, text=Generator))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=device))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=identifier, text=torch_device))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=., text=.))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 10,
        "number": 3295,
        "neg_line": [
            "-generator = torch.Generator(device=torch_device).manual_seed(0)"
        ],
        "pos_line": [
            "+generator = torch.manual_seed(0)"
        ],
        "core_change": "-generator = torch.Generator(device=torch_device).manual_seed(0) +generator = torch.manual_seed(0)",
        "core_API": "Generator"
    },
    {
        "commit_hash": "c0211b0bf79ee7e1009d04f11d27a061caa670b6",
        "index": "29c0be9..700012f 100644",
        "commit_message": "Swin-V2 test fixes, typo\n\n",
        "file": "pytorch-image-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class WindowAttention(nn.Module):",
            "B_, N, C = x.shape",
            "qkv_bias = None",
            "if self.q_bias is not None:",
            "-            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))",
            "+            qkv_bias = torch.cat((self.q_bias, self.k_bias, self.v_bias))",
            "qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)",
            "qkv = qkv.reshape(B_, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)",
            "q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)"
        ],
        "hunk_index": 3,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=tuple), node=('attribute', None), position=3, insert_id=893087)",
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=,, text=,), position=4)",
            "Move(target_node=ASTNode(type=tuple), node=ASTNode(type=attribute), position=5)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=893088)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=893089)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'k_bias'), position=2, insert_id=893090)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=zeros_like))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=requires_grad))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=false, text=False))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=v_bias))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 23,
        "number": 3296,
        "neg_line": [
            "-qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))"
        ],
        "pos_line": [
            "+qkv_bias = torch.cat((self.q_bias, self.k_bias, self.v_bias))"
        ],
        "core_change": "-qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias)) +qkv_bias = torch.cat((self.q_bias, self.k_bias, self.v_bias))",
        "core_API": "cat"
    },
    {
        "commit_hash": "300bda9e1a0a37fd9cb9d6d3dede9e5fb22ca153",
        "index": "4987107..6b28b23 100644",
        "commit_message": "Fix ELBO term scaling\n\n",
        "file": "generative-models.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "D_prior = tf.nn.sigmoid(D(X, z))",
            "X_samples, _ = P(z)",
            "",
            "disc = tf.reduce_mean(-D_sample)",
            "-loglike = -tf.reduce_mean(",
            "-    tf.nn.sigmoid_cross_entropy_with_logits(logits=X_logits, labels=X)",
            "+nll = tf.reduce_sum(",
            "+    tf.nn.sigmoid_cross_entropy_with_logits(logits=X_logits, labels=X),",
            "+    axis=1",
            ")",
            "+loglike = -tf.reduce_mean(nll)",
            "",
            "elbo = disc + loglike",
            "D_loss = tf.reduce_mean(log(D_q) + log(1. - D_prior))"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1363887)",
            "Insert(target_node=IN(type=expression_statement), node=('assignment', None), position=0, insert_id=1363888)",
            "Update(target_node=ASTNode(type=identifier, text=loglike), value='nll')",
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=call), position=2)",
            "Insert(target_node=IN(type=assignment), node=('identifier', 'loglike'), position=0, insert_id=1363889)",
            "Insert(target_node=IN(type=assignment), node=('=', '='), position=1, insert_id=1363890)",
            "Insert(target_node=IN(type=assignment), node=('unary_operator', None), position=2, insert_id=1363891)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1363892)",
            "Insert(target_node=IN(type=unary_operator), node=('-', '-'), position=0, insert_id=1363893)",
            "Insert(target_node=IN(type=unary_operator), node=('call', None), position=1, insert_id=1363894)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf'), position=0, insert_id=1363895)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1363896)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'reduce_sum'), position=2, insert_id=1363897)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=1363898)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=1363899)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=1363900)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'axis'), position=0, insert_id=1363901)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=1363902)",
            "Insert(target_node=IN(type=keyword_argument), node=('integer', '1'), position=2, insert_id=1363903)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1363904)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'nll'), position=1, insert_id=1363905)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1363906)",
            "Delete(target_node=ASTNode(type=-, text=-))",
            "Delete(target_node=ASTNode(type=unary_operator))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 25,
        "number": 3297,
        "neg_line": [
            "-loglike = -tf.reduce_mean(",
            "-tf.nn.sigmoid_cross_entropy_with_logits(logits=X_logits, labels=X)"
        ],
        "pos_line": [
            "+nll = tf.reduce_sum(",
            "+tf.nn.sigmoid_cross_entropy_with_logits(logits=X_logits, labels=X),",
            "+axis=1",
            "+loglike = -tf.reduce_mean(nll)"
        ],
        "core_change": "-loglike = -tf.reduce_mean( -tf.nn.sigmoid_cross_entropy_with_logits(logits=X_logits, labels=X) +nll = tf.reduce_sum( +tf.nn.sigmoid_cross_entropy_with_logits(logits=X_logits, labels=X), +axis=1 +loglike = -tf.reduce_mean(nll)",
        "core_API": "sigmoid"
    },
    {
        "commit_hash": "d67e72139e45bccbc5d356a456f41cee8f72eb1c",
        "index": "276c0721..9013a10f 100644",
        "commit_message": "Fix heuristic in util.get_token_ids_from_text_field_tensors (#4184)\n\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ELMoTokenCharactersIndexer(TokenIndexer):",
            "def padding_token():",
            "return [0] * ELMoCharacterMapper.max_word_length",
            "",
            "-        tensor_dict[\"tokens\"] = torch.LongTensor(",
            "+        tensor_dict[\"elmo_tokens\"] = torch.LongTensor(",
            "pad_sequence_to_length(",
            "-                tokens[\"tokens\"], padding_lengths[\"tokens\"], default_value=padding_token",
            "+                tokens[\"elmo_tokens\"], padding_lengths[\"elmo_tokens\"], default_value=padding_token",
            ")",
            ")",
            "return tensor_dict"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=string, text=\"tokens\"), value='\"elmo_tokens\"')",
            "Update(target_node=ASTNode(type=string, text=\"tokens\"), value='\"elmo_tokens\"')",
            "Update(target_node=ASTNode(type=string, text=\"tokens\"), value='\"elmo_tokens\"')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 3,
        "number": 3301,
        "neg_line": [
            "-tensor_dict[\"tokens\"] = torch.LongTensor(",
            "-tokens[\"tokens\"], padding_lengths[\"tokens\"], default_value=padding_token"
        ],
        "pos_line": [
            "+tensor_dict[\"elmo_tokens\"] = torch.LongTensor(",
            "+tokens[\"elmo_tokens\"], padding_lengths[\"elmo_tokens\"], default_value=padding_token"
        ],
        "core_change": "-tensor_dict[\"tokens\"] = torch.LongTensor( +tensor_dict[\"elmo_tokens\"] = torch.LongTensor( -tokens[\"tokens\"], padding_lengths[\"tokens\"], default_value=padding_token +tokens[\"elmo_tokens\"], padding_lengths[\"elmo_tokens\"], default_value=padding_token",
        "core_API": "LongTensor"
    },
    {
        "commit_hash": "61554842dd650f45b2d53daf4047d59530c2e173",
        "index": "429a4ab59..1337266d9 100644",
        "commit_message": "fix: use tf.clip_by_norm for cudnn layers on windows (#386)\n\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SquadModel(TFModel):",
            "self.opt = tf.train.AdadeltaOptimizer(learning_rate=self.lr_ph, epsilon=1e-6)",
            "grads = self.opt.compute_gradients(self.loss)",
            "gradients, variables = zip(*grads)",
            "-",
            "-            capped_grads, _ = tf.clip_by_global_norm(gradients, self.grad_clip)",
            "+            capped_grads = [tf.clip_by_norm(g, self.grad_clip) for g in gradients]",
            "self.train_op = self.opt.apply_gradients(zip(capped_grads, variables), global_step=self.global_step)",
            "",
            "def _build_feed_dict(self, c_tokens, c_chars, q_tokens, q_chars, y1=None, y2=None):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=assignment), node=ASTNode(type=identifier, text=capped_grads), position=0)",
            "Insert(target_node=ASTNode(type=assignment), node=('=', '='), position=1, insert_id=1923147)",
            "Insert(target_node=ASTNode(type=assignment), node=('list_comprehension', None), position=2, insert_id=1923148)",
            "Insert(target_node=IN(type=list_comprehension), node=('[', '['), position=0, insert_id=1923149)",
            "Move(target_node=IN(type=list_comprehension), node=ASTNode(type=call), position=1)",
            "Insert(target_node=IN(type=list_comprehension), node=('for_in_clause', None), position=2, insert_id=1923150)",
            "Insert(target_node=IN(type=list_comprehension), node=(']', ']'), position=3, insert_id=1923151)",
            "Insert(target_node=IN(type=for_in_clause), node=('for', 'for'), position=0, insert_id=1923152)",
            "Insert(target_node=IN(type=for_in_clause), node=('identifier', 'g'), position=1, insert_id=1923153)",
            "Insert(target_node=IN(type=for_in_clause), node=('in', 'in'), position=2, insert_id=1923154)",
            "Insert(target_node=IN(type=for_in_clause), node=('identifier', 'gradients'), position=3, insert_id=1923155)",
            "Update(target_node=ASTNode(type=identifier, text=clip_by_global_norm), value='clip_by_norm')",
            "Update(target_node=ASTNode(type=identifier, text=gradients), value='g')",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=_))",
            "Delete(target_node=ASTNode(type=pattern_list))",
            "Delete(target_node=ASTNode(type==, text==))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3302,
        "neg_line": [
            "-",
            "-capped_grads, _ = tf.clip_by_global_norm(gradients, self.grad_clip)"
        ],
        "pos_line": [
            "+capped_grads = [tf.clip_by_norm(g, self.grad_clip) for g in gradients]"
        ],
        "core_change": "- -capped_grads, _ = tf.clip_by_global_norm(gradients, self.grad_clip) +capped_grads = [tf.clip_by_norm(g, self.grad_clip) for g in gradients]",
        "core_API": "AdadeltaOptimizer"
    },
    {
        "commit_hash": "376355128edabdba16b9b6f14854a376478cde6e",
        "index": "d121bd8..3d402ae 100644",
        "commit_message": "change settings for res101, fix bug for the new code on bias, add a demo.\n\n",
        "file": "tf-faster-rcnn.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class SolverWrapper(object):",
            "while iter < max_iters + 1:",
            "# Learning rate",
            "if iter == cfg.TRAIN.STEPSIZE + 1:",
            "-        sess.run(tf.assign(lr, cfg.TRAIN.LEARNING_RATE * cfg.TRAIN.GAMMA))",
            "+        # Add snapshot here before reducing the learning rate",
            "self.snapshot(sess, iter)",
            "+        sess.run(tf.assign(lr, cfg.TRAIN.LEARNING_RATE * cfg.TRAIN.GAMMA))",
            "",
            "timer.tic()",
            "# Get training data, one batch at a time"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=2, insert_id=2334342)",
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=expression_statement), position=5)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=if, text=if), position=0)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=comparison_operator), position=1)",
            "Move(target_node=IN(type=if_statement), node=ASTNode(type=:, text=:), position=2)",
            "Insert(target_node=IN(type=if_statement), node=('block', ''), position=3, insert_id=2334343)",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=if_statement))"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 3306,
        "neg_line": [
            "-sess.run(tf.assign(lr, cfg.TRAIN.LEARNING_RATE * cfg.TRAIN.GAMMA))"
        ],
        "pos_line": [
            "+# Add snapshot here before reducing the learning rate",
            "+sess.run(tf.assign(lr, cfg.TRAIN.LEARNING_RATE * cfg.TRAIN.GAMMA))"
        ],
        "core_change": "-sess.run(tf.assign(lr, cfg.TRAIN.LEARNING_RATE * cfg.TRAIN.GAMMA)) +# Add snapshot here before reducing the learning rate +sess.run(tf.assign(lr, cfg.TRAIN.LEARNING_RATE * cfg.TRAIN.GAMMA))",
        "core_API": "run"
    },
    {
        "commit_hash": "d233e5a732f3d6988484b0540e2583059cb1f921",
        "index": "c2308e5e8..bc1748164 100644",
        "commit_message": "Fix format errors.\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class ConvolutionBlock(nn.Module):",
            ")",
            "self.norm = nn.BatchNorm1d(channels)",
            "self.pointwise_cov2 = nn.Conv1d(",
            "-            channels,",
            "-            channels,",
            "-            kernel_size=1,",
            "-            stride=1,",
            "-            padding=0,",
            "-            bias=bias,",
            "+            channels, channels, kernel_size=1, stride=1, padding=0, bias=bias,",
            ")",
            "self.act = activation"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 6,
        "AST_diff_line": 17,
        "number": 3307,
        "neg_line": [
            "-channels,",
            "-channels,",
            "-kernel_size=1,",
            "-stride=1,",
            "-padding=0,",
            "-bias=bias,"
        ],
        "pos_line": [
            "+channels, channels, kernel_size=1, stride=1, padding=0, bias=bias,"
        ],
        "core_change": "-channels, -channels, -kernel_size=1, -stride=1, -padding=0, -bias=bias, +channels, channels, kernel_size=1, stride=1, padding=0, bias=bias,",
        "core_API": "BatchNorm1d"
    },
    {
        "commit_hash": "91ac5f645c6359930904135d50ea71390f432e09",
        "index": "19b79ac0..80c24db4 100644",
        "commit_message": "Using cache from Transform and event_dim=1 (#1638)\n\n* Separated NN tests from flow tests\n\n* PermutationFlow\n\n* Tests for PermutationFLow\n\n* Bug fix\n\n* Renamed PermutationFlow to PermuteTransform\n\n* Added PermuteTransform to docs\n\n* Added device to permutation vectors\n\n* PEP8\n\n* Removed 'flow', link to IAF in docs, fixed other bug in docs\n\n* Removed more 'flow's\n\n* Added lazy_property to inv_permutation of PermuteTransform\n\n* Inverse operations for IAF and alternative version\n\n* Fixed docs error\n\n* Equations in docs\n\n* Fixed docstrings\n\n* Planar flow (untested)\n\n* Debugging planar flow\n\n* Working now!\n\n* Docs for PlanarFlow\n\n* Made PlanarFlow hashable, removed .module attribute hack\n\n* `event_dim=1` for vector flows, and using cache in `Transform`\n\n* Fixes\n\n* Rerunning Travis\n\n* Changed AutoIAFNormal so it doesn't mark output of IAF as being independent\n\n* Fixed up DMM example\n\n* Think I've fixed the DMM example now for new IAF event shape\n\n* Note about event shape for normalizing flow in DMM example\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PermuteTransform(Transform):",
            "vector of zeros works.",
            "\"\"\"",
            "",
            "-        return torch.zeros_like(x)",
            "+        return torch.zeros(x.size()[:-1])"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=724250)",
            "Update(target_node=ASTNode(type=identifier, text=zeros_like), value='zeros')",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=724251)",
            "Insert(target_node=IN(type=argument_list), node=('subscript', None), position=1, insert_id=724252)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=724253)",
            "Insert(target_node=IN(type=subscript), node=('call', None), position=0, insert_id=724254)",
            "Insert(target_node=IN(type=subscript), node=('[', '['), position=1, insert_id=724255)",
            "Insert(target_node=IN(type=subscript), node=('slice', None), position=2, insert_id=724256)",
            "Insert(target_node=IN(type=subscript), node=(']', ']'), position=3, insert_id=724257)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=724258)",
            "Move(target_node=IN(type=call), node=ASTNode(type=argument_list), position=1)",
            "Insert(target_node=IN(type=slice), node=(':', ':'), position=0, insert_id=724259)",
            "Insert(target_node=IN(type=slice), node=('unary_operator', '-1'), position=1, insert_id=724260)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=x), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=724261)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'size'), position=2, insert_id=724262)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 16,
        "number": 3309,
        "neg_line": [
            "-return torch.zeros_like(x)"
        ],
        "pos_line": [
            "+return torch.zeros(x.size()[:-1])"
        ],
        "core_change": "-return torch.zeros_like(x) +return torch.zeros(x.size()[:-1])",
        "core_API": "zeros_like"
    },
    {
        "commit_hash": "80155352ad99265742d03165aa78bfb279810549",
        "index": "58ca41ecb..0c575b09e 100644",
        "commit_message": "Make tutorials about encrypted DNN faster using FSS (#3933)\n\n* Improve Enc Lin Alg tutorial efficiency by 10%\n\n* Update tutorials on encryption computation to feature FSS\n\n* Update tutorial 11\n\n* Fix enc lin reg\n\n* Revert Enc lin alg to snn\n\n* Make fss keygen parallel\n\n* rm print\n",
        "file": "PySyft.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class PrimitiveStorage:",
            "assert (",
            "n_party == 2",
            "), f\"The FSS protocol only works for 2 workers, {n_party} were provided.\"",
            "-            alpha, s_00, s_01, *CW = fss_class.keygen(n_values=n_instances)",
            "+            alpha, s_00, s_01, *CW = sy.frameworks.torch.mpc.fss.keygen(n_values=n_instances, op=op)",
            "# simulate sharing TODO clean this",
            "mask = np.random.randint(0, 2 ** n, alpha.shape, dtype=alpha.dtype)",
            "return [((alpha - mask) % 2 ** n, s_00, *CW), (mask, s_01, *CW)]"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=788645)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=788646)",
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=2, insert_id=788647)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=3, insert_id=788648)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=788649)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=788650)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'fss'), position=2, insert_id=788651)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'op'), position=0, insert_id=788652)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=788653)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'op'), position=2, insert_id=788654)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=788655)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=788656)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'mpc'), position=2, insert_id=788657)",
            "Insert(target_node=IN(type=attribute), node=('attribute', None), position=0, insert_id=788658)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=788659)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=2, insert_id=788660)",
            "Update(target_node=ASTNode(type=identifier, text=fss_class), value='sy')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=fss_class), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'frameworks'), position=2, insert_id=788661)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 20,
        "number": 3310,
        "neg_line": [
            "-alpha, s_00, s_01, *CW = fss_class.keygen(n_values=n_instances)"
        ],
        "pos_line": [
            "+alpha, s_00, s_01, *CW = sy.frameworks.torch.mpc.fss.keygen(n_values=n_instances, op=op)"
        ],
        "core_change": "-alpha, s_00, s_01, *CW = fss_class.keygen(n_values=n_instances) +alpha, s_00, s_01, *CW = sy.frameworks.torch.mpc.fss.keygen(n_values=n_instances, op=op)",
        "core_API": "keygen"
    },
    {
        "commit_hash": "c215878f11d81808dfe4721795f0c105200e6601",
        "index": "ca32b19..d11df83 100644",
        "commit_message": "YOLOv5 Apple Metal Performance Shader (MPS) support (#7878)\n\n* Apple Metal Performance Shader (MPS) device support\n\nFollowing https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/\n\nShould work with Apple M1 devices with PyTorch nightly installed with command `--device mps`. Usage examples:\n```bash\npython train.py --device mps\npython detect.py --device mps\npython val.py --device mps\n```\n\n* Update device strategy to fix MPS issue\n",
        "file": "yolov5.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def select_device(device='', batch_size=0, newline=True):",
            "for i, d in enumerate(devices):",
            "p = torch.cuda.get_device_properties(i)",
            "s += f\"{'' if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / (1 << 20):.0f}MiB)\\n\"  # bytes to MB",
            "+    elif mps:",
            "+        s += 'MPS\\n'",
            "else:",
            "s += 'CPU\\n'",
            "",
            "if not newline:",
            "s = s.rstrip()",
            "LOGGER.info(s.encode().decode('ascii', 'ignore') if platform.system() == 'Windows' else s)  # emoji-safe",
            "-    return torch.device('cuda:0' if cuda else 'cpu')",
            "+    return torch.device('cuda:0' if cuda else 'mps' if mps else 'cpu')",
            "",
            "",
            "def time_sync():"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=1294109)",
            "Insert(target_node=IN(type=expression_statement), node=('augmented_assignment', None), position=0, insert_id=1294110)",
            "Insert(target_node=IN(type=augmented_assignment), node=('identifier', 'elif'), position=0, insert_id=1294111)",
            "Insert(target_node=IN(type=augmented_assignment), node=('ERROR', None), position=1, insert_id=1294112)",
            "Insert(target_node=IN(type=augmented_assignment), node=('+=', '+='), position=2, insert_id=1294113)",
            "Insert(target_node=IN(type=augmented_assignment), node=('string', \"'MPS\\\\n'\"), position=3, insert_id=1294114)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'mps'), position=0, insert_id=1294115)",
            "Insert(target_node=IN(type=ERROR), node=(':', ':'), position=1, insert_id=1294116)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 's'), position=2, insert_id=1294117)",
            "Insert(target_node=ASTNode(type=conditional_expression), node=('conditional_expression', None), position=4, insert_id=1294118)",
            "Insert(target_node=IN(type=conditional_expression), node=('string', \"'mps'\"), position=0, insert_id=1294119)",
            "Insert(target_node=IN(type=conditional_expression), node=('if', 'if'), position=1, insert_id=1294120)",
            "Insert(target_node=IN(type=conditional_expression), node=('identifier', 'mps'), position=2, insert_id=1294121)",
            "Insert(target_node=IN(type=conditional_expression), node=('else', 'else'), position=3, insert_id=1294122)",
            "Move(target_node=IN(type=conditional_expression), node=ASTNode(type=string, text='cpu'), position=4)"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 15,
        "number": 3313,
        "neg_line": [
            "-return torch.device('cuda:0' if cuda else 'cpu')"
        ],
        "pos_line": [
            "+elif mps:",
            "+s += 'MPS\\n'",
            "+return torch.device('cuda:0' if cuda else 'mps' if mps else 'cpu')"
        ],
        "core_change": "+elif mps: +s += 'MPS\\n' -return torch.device('cuda:0' if cuda else 'cpu') +return torch.device('cuda:0' if cuda else 'mps' if mps else 'cpu')",
        "core_API": "get_device_properties"
    },
    {
        "commit_hash": "621a1bbd883466bc4b29b421759fbda3df73bc1c",
        "index": "f3553535..f75e8c4d 100644",
        "commit_message": "Fix wrong variable collection in distributed training (#431)\n\n",
        "file": "tensorpack.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "def _get_cached_vs(name):",
            "@contextmanager",
            "def _enter_vs_reuse_ns(name):",
            "vs = _get_cached_vs(name)",
            "+    # XXX Not good to enter the cached vs directly, because this will clean-up custom getter",
            "+    # with tf.variable_scope(name, reuse=tf.AUTO_REUSE):    # available in 1.4 only",
            "with tf.variable_scope(vs):",
            "with tf.name_scope(vs.original_name_scope):",
            "yield vs"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 0,
        "AST_diff_line": 17,
        "number": 3315,
        "neg_line": [],
        "pos_line": [
            "+# XXX Not good to enter the cached vs directly, because this will clean-up custom getter",
            "+# with tf.variable_scope(name, reuse=tf.AUTO_REUSE):    # available in 1.4 only"
        ],
        "core_change": "+# XXX Not good to enter the cached vs directly, because this will clean-up custom getter +# with tf.variable_scope(name, reuse=tf.AUTO_REUSE):    # available in 1.4 only",
        "core_API": "variable_scope"
    },
    {
        "commit_hash": "73863e91028675fb298e426c841ea544741e61d0",
        "index": "8af2a7ca1..43dda00fc 100644",
        "commit_message": "Fix unintended zero-only target in iou example (#4262)\n\n\n",
        "file": "lightning.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def iou(",
            "",
            "Example:",
            "",
            "-        >>> target = torch.randint(0, 1, (10, 25, 25))",
            "+        >>> target = torch.randint(0, 2, (10, 25, 25))",
            ">>> pred = torch.tensor(target)",
            ">>> pred[2:5, 7:13, 9:15] = 1 - pred[2:5, 7:13, 9:15]",
            ">>> iou(pred, target)",
            "-        tensor(0.4914)",
            "+        tensor(0.9660)",
            "",
            "\"\"\"",
            "num_classes = get_num_classes(pred=pred, target=target, num_classes=num_classes)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=1), value='2')",
            "Update(target_node=ASTNode(type=float, text=0.4914), value='0.9660')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 3317,
        "neg_line": [
            "->>> target = torch.randint(0, 1, (10, 25, 25))",
            "-tensor(0.4914)"
        ],
        "pos_line": [
            "+>>> target = torch.randint(0, 2, (10, 25, 25))",
            "+tensor(0.9660)"
        ],
        "core_change": "->>> target = torch.randint(0, 1, (10, 25, 25)) +>>> target = torch.randint(0, 2, (10, 25, 25)) -tensor(0.4914) +tensor(0.9660)",
        "core_API": "randint"
    },
    {
        "commit_hash": "654c10bac8547c5a20946aef12041d98cbf3ef34",
        "index": "a4008f1c..b08c907d 100644",
        "commit_message": "fix (#4855)\n\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_input_nodes(",
            ") -> Tuple[Optional[str], Sequence]:",
            "def to_index(tensor):",
            "if isinstance(tensor, Tensor) and tensor.dtype == torch.bool:",
            "-            return torch.nonzero(as_tuple=False).view(-1)",
            "-        else:",
            "-            return tensor",
            "+            return tensor.nonzero(as_tuple=False).view(-1)",
            "+        return tensor",
            "",
            "if isinstance(data, Data):",
            "if input_nodes is None:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=module), node=ASTNode(type=return_statement), position=3)",
            "Update(target_node=ASTNode(type=identifier, text=torch), value='tensor')",
            "Delete(target_node=ASTNode(type=else, text=else))",
            "Delete(target_node=ASTNode(type=:, text=:))",
            "Delete(target_node=ASTNode(type=block))",
            "Delete(target_node=ASTNode(type=else_clause))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 6,
        "number": 3321,
        "neg_line": [
            "-return torch.nonzero(as_tuple=False).view(-1)",
            "-else:",
            "-return tensor"
        ],
        "pos_line": [
            "+return tensor.nonzero(as_tuple=False).view(-1)",
            "+return tensor"
        ],
        "core_change": "-return torch.nonzero(as_tuple=False).view(-1) -else: -return tensor +return tensor.nonzero(as_tuple=False).view(-1) +return tensor",
        "core_API": "nonzero"
    },
    {
        "commit_hash": "969859d5f67c7106de4d1098c4891c9b03694bbe",
        "index": "893d9916a..d724ac6e2 100644",
        "commit_message": "Fix doc errors and typos across the board (#8139)\n\n* Fix doc errors and typos across the board\n\n* Fix a typo\n\n* Fix the CI\n\n* Fix more typos\n\n* Fix CI\n\n* More fixes\n\n* Fix CI\n\n* More fixes\n\n* More fixes\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Distiller:",
            "# https://github.com/peterliht/knowledge-distillation-pytorch/blob/master/model/net.py#L100",
            "# https://github.com/peterliht/knowledge-distillation-pytorch/issues/2",
            "if self.params.restrict_ce_to_mask:",
            "-            mask = (lm_labels > -1).unsqueeze(-1).expand_as(s_logits)  # (bs, seq_lenth, voc_size)",
            "+            mask = (lm_labels > -1).unsqueeze(-1).expand_as(s_logits)  # (bs, seq_length, voc_size)",
            "else:",
            "-            mask = attention_mask.unsqueeze(-1).expand_as(s_logits)  # (bs, seq_lenth, voc_size)",
            "+            mask = attention_mask.unsqueeze(-1).expand_as(s_logits)  # (bs, seq_length, voc_size)",
            "s_logits_slct = torch.masked_select(s_logits, mask)  # (bs * seq_length * voc_size) modulo the 1s in mask",
            "s_logits_slct = s_logits_slct.view(-1, s_logits.size(-1))  # (bs * seq_length, voc_size) modulo the 1s in mask",
            "t_logits_slct = torch.masked_select(t_logits, mask)  # (bs * seq_length * voc_size) modulo the 1s in mask"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 17,
        "number": 3323,
        "neg_line": [
            "-mask = (lm_labels > -1).unsqueeze(-1).expand_as(s_logits)  # (bs, seq_lenth, voc_size)",
            "-mask = attention_mask.unsqueeze(-1).expand_as(s_logits)  # (bs, seq_lenth, voc_size)"
        ],
        "pos_line": [
            "+mask = (lm_labels > -1).unsqueeze(-1).expand_as(s_logits)  # (bs, seq_length, voc_size)",
            "+mask = attention_mask.unsqueeze(-1).expand_as(s_logits)  # (bs, seq_length, voc_size)"
        ],
        "core_change": "-mask = (lm_labels > -1).unsqueeze(-1).expand_as(s_logits)  # (bs, seq_lenth, voc_size) +mask = (lm_labels > -1).unsqueeze(-1).expand_as(s_logits)  # (bs, seq_length, voc_size) -mask = attention_mask.unsqueeze(-1).expand_as(s_logits)  # (bs, seq_lenth, voc_size) +mask = attention_mask.unsqueeze(-1).expand_as(s_logits)  # (bs, seq_length, voc_size)",
        "core_API": "unsqueeze"
    },
    {
        "commit_hash": "8c7bf5fa6af7cc9244ae1dc5dbf1b88605719f15",
        "index": "273426135..98412db99 100644",
        "commit_message": "Fix training/eval mode\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class E2E(STInterface, torch.nn.Module):",
            "isinstance(m, MultiHeadedAttention) and m.attn is not None",
            "):  # skip MHA for submodules",
            "ret[name] = m.attn.cpu().numpy()",
            "+        self.train()",
            "return ret"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('expression_statement', None), position=4, insert_id=153240)",
            "Insert(target_node=IN(type=expression_statement), node=('call', None), position=0, insert_id=153241)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=153242)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=153243)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'self'), position=0, insert_id=153244)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=153245)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'train'), position=2, insert_id=153246)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=153247)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=153248)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 9,
        "number": 3324,
        "neg_line": [],
        "pos_line": [
            "+self.train()"
        ],
        "core_change": "+self.train()",
        "core_API": "cpu"
    },
    {
        "commit_hash": "ce8f42b12ecd522e3ef7251e0d5f5175075a3fb4",
        "index": "3c9ceb2f..b32dc83c 100644",
        "commit_message": "Fixes to enable dev branch to move to pytorch-1.2 (#2001)\n\n* Fixes to enable dev branch to move to pytorch-1.2\n\n* change name for published cpu wheels\n\n* fix failing unit tests\n\n* change docs conf\n\n* change tracking example\n\n",
        "file": "pyro.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def test_elbo_zip(gate, rate):",
            "dist1 = dist.Delta(torch.tensor(0.))",
            "dist0 = dist.Poisson(rate)",
            "with pyro.plate(\"data\", len(data)):",
            "-            mask = pyro.sample(\"mask\", dist.Bernoulli(gate), infer={\"enumerate\": \"parallel\"}).byte()",
            "+            mask = pyro.sample(\"mask\", dist.Bernoulli(gate), infer={\"enumerate\": \"parallel\"}).bool()",
            "pyro.sample(\"obs\", dist.MaskedMixture(mask, dist0, dist1), obs=data)",
            "",
            "def guide(data):",
            "pass",
            "",
            "-    gate = pyro.param(\"gate\", torch.tensor(gate), constraint=constraints.unit_interval)",
            "-    rate = pyro.param(\"rate\", torch.tensor(rate), constraint=constraints.positive)",
            "+    pyro.param(\"gate\", torch.tensor(gate), constraint=constraints.unit_interval)",
            "+    pyro.param(\"rate\", torch.tensor(rate), constraint=constraints.positive)",
            "",
            "data = torch.tensor([0., 1., 2.])",
            "elbo = TraceEnum_ELBO(max_plate_nesting=1, strict_enumeration_warning=False)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Move(target_node=ASTNode(type=expression_statement), node=ASTNode(type=call), position=0)",
            "Update(target_node=ASTNode(type=identifier, text=byte), value='bool')",
            "Delete(target_node=ASTNode(type=identifier, text=gate))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))",
            "Delete(target_node=ASTNode(type=identifier, text=rate))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=assignment))"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 9,
        "number": 3325,
        "neg_line": [
            "-mask = pyro.sample(\"mask\", dist.Bernoulli(gate), infer={\"enumerate\": \"parallel\"}).byte()",
            "-gate = pyro.param(\"gate\", torch.tensor(gate), constraint=constraints.unit_interval)",
            "-rate = pyro.param(\"rate\", torch.tensor(rate), constraint=constraints.positive)"
        ],
        "pos_line": [
            "+mask = pyro.sample(\"mask\", dist.Bernoulli(gate), infer={\"enumerate\": \"parallel\"}).bool()",
            "+pyro.param(\"gate\", torch.tensor(gate), constraint=constraints.unit_interval)",
            "+pyro.param(\"rate\", torch.tensor(rate), constraint=constraints.positive)"
        ],
        "core_change": "-mask = pyro.sample(\"mask\", dist.Bernoulli(gate), infer={\"enumerate\": \"parallel\"}).byte() +mask = pyro.sample(\"mask\", dist.Bernoulli(gate), infer={\"enumerate\": \"parallel\"}).bool() -gate = pyro.param(\"gate\", torch.tensor(gate), constraint=constraints.unit_interval) -rate = pyro.param(\"rate\", torch.tensor(rate), constraint=constraints.positive) +pyro.param(\"gate\", torch.tensor(gate), constraint=constraints.unit_interval) +pyro.param(\"rate\", torch.tensor(rate), constraint=constraints.positive)",
        "core_API": "Delta"
    },
    {
        "commit_hash": "42d0e8c11815d9084f99563db67e5468364c1c9b",
        "index": "6b4ba3a2a..ceeed0ad5 100644",
        "commit_message": "Fix typo again...\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class Conv1dLinear(torch.nn.Module):",
            "super(Conv1dLinear, self).__init__()",
            "self.w_1 = torch.nn.Conv1d(in_chans, hidden_chans, kernel_size,",
            "stride=1, padding=(kernel_size - 1) // 2)",
            "-        self.w_2 = nn.Linear(hidden_chans, in_chans)",
            "+        self.w_2 = torch.nn.Linear(hidden_chans, in_chans)",
            "self.dropout = torch.nn.Dropout(dropout_rate)",
            "",
            "def forward(self, x):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=1340314)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=1340315)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'torch'), position=0, insert_id=1340316)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=nn), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 3327,
        "neg_line": [
            "-self.w_2 = nn.Linear(hidden_chans, in_chans)"
        ],
        "pos_line": [
            "+self.w_2 = torch.nn.Linear(hidden_chans, in_chans)"
        ],
        "core_change": "-self.w_2 = nn.Linear(hidden_chans, in_chans) +self.w_2 = torch.nn.Linear(hidden_chans, in_chans)",
        "core_API": "Conv1d"
    },
    {
        "commit_hash": "c05ace221339102abb1c8b0933b9cbfd51ddd4a3",
        "index": "6a8fc7070..c92bbdbff 100644",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario ako <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class MsrTextCompression(datasets.GeneratorBasedBuilder):",
            "data_dir = os.path.abspath(os.path.expanduser(dl_manager.manual_dir))",
            "if not os.path.exists(data_dir):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_text_compression', data_dir=...)` per the manual download instructions: {}\".format(",
            "-                    data_dir, self.manual_download_instructions",
            "-                )",
            "+                f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_text_compression', data_dir=...)` per the manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('string', 'f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\\'msr_text_compression\\', data_dir=...)` per the manual download instructions: {self.manual_download_instructions}\"'), position=1, insert_id=1781622)",
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=), text=)), position=2)",
            "Delete(target_node=ASTNode(type=string, text=\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_text_compression', data_dir=...)` per the manual download instructions: {}\"))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=format))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=data_dir))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=self))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=manual_download_instructions))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=), text=)))"
        ],
        "plus_line": 1,
        "minus_line": 3,
        "AST_diff_line": 16,
        "number": 3329,
        "neg_line": [
            "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_text_compression', data_dir=...)` per the manual download instructions: {}\".format(",
            "-data_dir, self.manual_download_instructions",
            "-)"
        ],
        "pos_line": [
            "+f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_text_compression', data_dir=...)` per the manual download instructions: {self.manual_download_instructions}\""
        ],
        "core_change": "-\"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_text_compression', data_dir=...)` per the manual download instructions: {}\".format( -data_dir, self.manual_download_instructions -) +f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('msr_text_compression', data_dir=...)` per the manual download instructions: {self.manual_download_instructions}\"",
        "core_API": "abspath"
    },
    {
        "commit_hash": "c317aaa3de289fc0a69e6b1001ffc586bb4d9c9a",
        "index": "e7961b68..1a425278 100644",
        "commit_message": "Refactor library namespaces [pre-release][0.6-rc1] (#1412)\n\n* flake fixes\n\n* initial flake8 fixeS\n\n* remove top level from kornia.color\n\n* kornia filters\n\n* kornia losses\n\n* kornia features\n\n* geomtry and all ok\n\n* removed jit module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* apply formatting and few fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add keep block for isort\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* skip init\n\n* fix the docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove old code\n\n* simplify ci workflow\n\n* fix circular dependency\n\n* few format fixes\n\n* fix code format test\n\n* remove kornia.jit imports\n\n* final fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* ci fixes\n\n* add versioneer\n\n* fix pnp import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* exclude version files from pre-commit\n\n* exclude files precommit\n\n* update to pytorch 1.10 and add fixes\n\n* Update tests_cpu.yml\n\n* Update setup_dev_env.sh\n\n* Update tests_cpu.yml\n\n* undo versioneer\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix no_grad\n\n* Apply suggestions from code review\n\n* fix skip windows tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update test_integrated.py\n\n* Apply suggestions from code review\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def linear_transform(",
            "new_order: List[int] = perm.tolist()",
            "inv_order: List[int] = perm_inv.tolist()",
            "",
            "-    feature_sizes = torch.tensor(inp_size[0:dim] + inp_size[dim + 1 : :])",
            "+    feature_sizes = torch.tensor(inp_size[0:dim] + inp_size[dim + 1::])",
            "num_features: int = int(torch.prod(feature_sizes).item())",
            "",
            "inp_permute = inp.permute(new_order)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3330,
        "neg_line": [
            "-feature_sizes = torch.tensor(inp_size[0:dim] + inp_size[dim + 1 : :])"
        ],
        "pos_line": [
            "+feature_sizes = torch.tensor(inp_size[0:dim] + inp_size[dim + 1::])"
        ],
        "core_change": "-feature_sizes = torch.tensor(inp_size[0:dim] + inp_size[dim + 1 : :]) +feature_sizes = torch.tensor(inp_size[0:dim] + inp_size[dim + 1::])",
        "core_API": "tolist"
    },
    {
        "commit_hash": "5908d2b8b86f1ed471ca0d670671fc0310abf021",
        "index": "417fe26f34..d6d47b6603 100644",
        "commit_message": "Another fix to vecdot (#4639)\n\n\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def vecdot(",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "dtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))",
            "-    x1, x2 = x1.to(dtype=torch.float32), x2.to(dtype=torch.float32)",
            "+    if dtype != \"float64\":",
            "+        x1, x2 = x1.to(dtype=torch.float32), x2.to(dtype=torch.float32)",
            "return torch.tensordot(x1, x2, dims=([axis], [axis]), out=out).to(dtype=dtype)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=module), node=('if_statement', None), position=2, insert_id=325158)",
            "Insert(target_node=IN(type=if_statement), node=('if', 'if'), position=0, insert_id=325159)",
            "Insert(target_node=IN(type=if_statement), node=('comparison_operator', None), position=1, insert_id=325160)",
            "Insert(target_node=IN(type=if_statement), node=(':', ':'), position=2, insert_id=325161)",
            "Insert(target_node=IN(type=if_statement), node=('block', None), position=3, insert_id=325162)",
            "Insert(target_node=IN(type=comparison_operator), node=('identifier', 'dtype'), position=0, insert_id=325163)",
            "Insert(target_node=IN(type=comparison_operator), node=('!=', '!='), position=1, insert_id=325164)",
            "Insert(target_node=IN(type=comparison_operator), node=('string', '\"float64\"'), position=2, insert_id=325165)",
            "Move(target_node=IN(type=block), node=ASTNode(type=expression_statement), position=0)"
        ],
        "plus_line": 2,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 3332,
        "neg_line": [
            "-x1, x2 = x1.to(dtype=torch.float32), x2.to(dtype=torch.float32)"
        ],
        "pos_line": [
            "+if dtype != \"float64\":",
            "+x1, x2 = x1.to(dtype=torch.float32), x2.to(dtype=torch.float32)"
        ],
        "core_change": "-x1, x2 = x1.to(dtype=torch.float32), x2.to(dtype=torch.float32) +if dtype != \"float64\": +x1, x2 = x1.to(dtype=torch.float32), x2.to(dtype=torch.float32)",
        "core_API": "as_native_dtype"
    },
    {
        "commit_hash": "b455b4c03ef99933543203c373f12d0692c4ae88",
        "index": "bf9f14b..a7f9818 100755",
        "commit_message": "Fix `TypeError` when using sigmoid\n\n",
        "file": "pix2pixHD.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "nn",
        "change": [
            "class NLayerDiscriminator(nn.Module):",
            "sequence += [[nn.Conv2d(nf, 1, kernel_size=kw, stride=1, padding=padw)]]",
            "",
            "if use_sigmoid:",
            "-            sequence += [nn.Sigmoid()]",
            "+            sequence += [[nn.Sigmoid()]]",
            "",
            "if getIntermFeat:",
            "for n in range(len(sequence)):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=list), node=('[', '['), position=0, insert_id=1440122)",
            "Move(target_node=ASTNode(type=list), node=ASTNode(type=list), position=1)",
            "Insert(target_node=ASTNode(type=list), node=(']', ']'), position=2, insert_id=1440123)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 3,
        "number": 3334,
        "neg_line": [
            "-sequence += [nn.Sigmoid()]"
        ],
        "pos_line": [
            "+sequence += [[nn.Sigmoid()]]"
        ],
        "core_change": "-sequence += [nn.Sigmoid()] +sequence += [[nn.Sigmoid()]]",
        "core_API": "Conv2d"
    },
    {
        "commit_hash": "6bc4b75c8eaf662301208be7bbc02c1a073d36e5",
        "index": "76133588..34dc48c4 100644",
        "commit_message": "chore(internal): fix typing issues (#2460)\n\n\n",
        "file": "BentoML.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "def create(",
            "yield res",
            "finally:",
            "res.info.freeze()",
            "+        res.flush()",
            "res.save(_model_store)",
            "",
            "track("
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('call', None), position=0, insert_id=2648501)",
            "Insert(target_node=ASTNode(type=attribute), node=('ERROR', None), position=1, insert_id=2648502)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=2, insert_id=2648503)",
            "Insert(target_node=ASTNode(type=attribute), node=('identifier', 'flush'), position=3, insert_id=2648504)",
            "Move(target_node=IN(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2648505)",
            "Insert(target_node=IN(type=ERROR), node=('identifier', 'res'), position=0, insert_id=2648506)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2648507)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=1, insert_id=2648508)"
        ],
        "plus_line": 1,
        "minus_line": 0,
        "AST_diff_line": 9,
        "number": 3338,
        "neg_line": [],
        "pos_line": [
            "+res.flush()"
        ],
        "core_change": "+res.flush()",
        "core_API": "freeze"
    },
    {
        "commit_hash": "2fb219a658a9d1abb385a4175bc94543b218e17f",
        "index": "0e06902fe3..2432f618de 100644",
        "commit_message": "[Ray RLlib] Fix tree import (#7662)\n\n* Rollback.\n\n* Fix import tree error by adding meaningful error and replacing by tf.nest wherever possible.\n\n* LINT.\n\n* LINT.\n\n* Fix.\n\n* Fix log-likelihood test case failing on travis.\n",
        "file": "ray.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFPolicy(Policy):",
            "",
            "# build output signatures",
            "output_signature = self._extra_output_signature_def()",
            "-        for i, a in enumerate(tree.flatten(self._sampled_action)):",
            "+        for i, a in enumerate(tf.nest.flatten(self._sampled_action)):",
            "output_signature[\"actions_{}\".format(i)] = \\",
            "tf.saved_model.utils.build_tensor_info(a)"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=attribute), node=('attribute', None), position=0, insert_id=2146952)",
            "Insert(target_node=ASTNode(type=attribute), node=('.', '.'), position=1, insert_id=2146953)",
            "Update(target_node=ASTNode(type=identifier, text=tree), value='tf')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=tree), position=0)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=., text=.), position=1)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'nest'), position=2, insert_id=2146954)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 6,
        "number": 3341,
        "neg_line": [
            "-for i, a in enumerate(tree.flatten(self._sampled_action)):"
        ],
        "pos_line": [
            "+for i, a in enumerate(tf.nest.flatten(self._sampled_action)):"
        ],
        "core_change": "-for i, a in enumerate(tree.flatten(self._sampled_action)): +for i, a in enumerate(tf.nest.flatten(self._sampled_action)):",
        "core_API": "_extra_output_signature_def"
    },
    {
        "commit_hash": "04a6ad1981abd959f12585f42212e947cfa96318",
        "index": "475127e8..24c2ba99 100644",
        "commit_message": "adopt torch.testing.assert_close (#1031)\n\n* adopt torch.testing.assert_close\n\n* use torch.testing.assert_close\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add TODO\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestRandomPerspective:",
            "assert len(out_perspective) == 2",
            "assert out_perspective[0].shape == x_data.shape",
            "assert out_perspective[1].shape == (1, 3, 3)",
            "-        assert_allclose(out_perspective[0], expected_output, atol=1e-4, rtol=1e-4)",
            "-        assert_allclose(out_perspective[1], expected_transform, atol=1e-4, rtol=1e-4)",
            "+        assert_close(out_perspective[0], expected_output, atol=1e-4, rtol=1e-4)",
            "+        assert_close(out_perspective[1], expected_transform, atol=1e-4, rtol=1e-4)",
            "assert aug.inverse(out_perspective).shape == x_data.shape",
            "",
            "def test_gradcheck(self, device):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=assert_allclose), value='assert_close')",
            "Update(target_node=ASTNode(type=identifier, text=assert_allclose), value='assert_close')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 3342,
        "neg_line": [
            "-assert_allclose(out_perspective[0], expected_output, atol=1e-4, rtol=1e-4)",
            "-assert_allclose(out_perspective[1], expected_transform, atol=1e-4, rtol=1e-4)"
        ],
        "pos_line": [
            "+assert_close(out_perspective[0], expected_output, atol=1e-4, rtol=1e-4)",
            "+assert_close(out_perspective[1], expected_transform, atol=1e-4, rtol=1e-4)"
        ],
        "core_change": "-assert_allclose(out_perspective[0], expected_output, atol=1e-4, rtol=1e-4) -assert_allclose(out_perspective[1], expected_transform, atol=1e-4, rtol=1e-4) +assert_close(out_perspective[0], expected_output, atol=1e-4, rtol=1e-4) +assert_close(out_perspective[1], expected_transform, atol=1e-4, rtol=1e-4)",
        "core_API": "inverse"
    },
    {
        "commit_hash": "af2bcf3e70928d379feaf46cdae1aeba71d0e424",
        "index": "2b61c284..c90cebeb 100755",
        "commit_message": "various fixes and improvements for preprocessing/exploration update, changed Sequence preprocessor\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class Categorical(Distribution):",
            "maxval=(1.0 - util.epsilon)",
            ")",
            "gumbel_distribution = -tf.log(x=-tf.log(x=uniform_distribution))",
            "-        sampled = tf.argmax(input=(logits + gumbel_distribution), axis=-1)",
            "+        sampled = tf.argmax(input=(logits + gumbel_distribution), axis=-1, output_type=util.tf_dtype('int'))",
            "",
            "return tf.where(condition=deterministic, x=definite, y=sampled)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=(',', ','), position=4, insert_id=2238570)",
            "Insert(target_node=ASTNode(type=argument_list), node=('keyword_argument', None), position=5, insert_id=2238571)",
            "Insert(target_node=ASTNode(type=argument_list), node=(')', ')'), position=6, insert_id=2238572)",
            "Insert(target_node=IN(type=keyword_argument), node=('identifier', 'output_type'), position=0, insert_id=2238573)",
            "Insert(target_node=IN(type=keyword_argument), node=('=', '='), position=1, insert_id=2238574)",
            "Insert(target_node=IN(type=keyword_argument), node=('call', None), position=2, insert_id=2238575)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=2238576)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=2238577)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'util'), position=0, insert_id=2238578)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=2238579)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'tf_dtype'), position=2, insert_id=2238580)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=2238581)",
            "Insert(target_node=IN(type=argument_list), node=('string', \"'int'\"), position=1, insert_id=2238582)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=), text=)), position=2)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 3344,
        "neg_line": [
            "-sampled = tf.argmax(input=(logits + gumbel_distribution), axis=-1)"
        ],
        "pos_line": [
            "+sampled = tf.argmax(input=(logits + gumbel_distribution), axis=-1, output_type=util.tf_dtype('int'))"
        ],
        "core_change": "-sampled = tf.argmax(input=(logits + gumbel_distribution), axis=-1) +sampled = tf.argmax(input=(logits + gumbel_distribution), axis=-1, output_type=util.tf_dtype('int'))",
        "core_API": "log"
    },
    {
        "commit_hash": "b9956a6a56372c86cdb5f23a8a288b183b123faa",
        "index": "3ed2e7ea..c007bf9f 100644",
        "commit_message": "Fix FP16 version comparison\n\n",
        "file": "fairseq.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TransformerDecoder(FairseqIncrementalDecoder):",
            "if k in state_dict:",
            "state_dict['decoder.layers.{}.{}.{}'.format(i, new, m)] = state_dict[k]",
            "del state_dict[k]",
            "-        if state_dict.get('decoder.version', torch.Tensor([1]))[0] < 2:",
            "+        if utils.item(state_dict.get('decoder.version', torch.Tensor([1]))[0]) < 2:",
            "# earlier checkpoints did not normalize after the stack of layers",
            "self.layer_norm = None",
            "self.normalize = False"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=comparison_operator), node=('call', None), position=0, insert_id=222045)",
            "Insert(target_node=IN(type=call), node=('attribute', None), position=0, insert_id=222046)",
            "Insert(target_node=IN(type=call), node=('argument_list', None), position=1, insert_id=222047)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'utils'), position=0, insert_id=222048)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=222049)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'item'), position=2, insert_id=222050)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=222051)",
            "Move(target_node=IN(type=argument_list), node=ASTNode(type=subscript), position=1)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=222052)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 9,
        "number": 3345,
        "neg_line": [
            "-if state_dict.get('decoder.version', torch.Tensor([1]))[0] < 2:"
        ],
        "pos_line": [
            "+if utils.item(state_dict.get('decoder.version', torch.Tensor([1]))[0]) < 2:"
        ],
        "core_change": "-if state_dict.get('decoder.version', torch.Tensor([1]))[0] < 2: +if utils.item(state_dict.get('decoder.version', torch.Tensor([1]))[0]) < 2:",
        "core_API": "get"
    },
    {
        "commit_hash": "6cf0abaed1c2911e5bf23e48e4908929b43d85ae",
        "index": "7e018de00..4cd22ae28 100644",
        "commit_message": "Align tqdm control/cache control with Transformers (#3897)\n\n* Align tqdm with transformers\n\n* Add tqdm utilites to docs\n\n* Introduce enable_caching / disable_caching\n\n* Fix test naming\n\n* Update src/datasets/fingerprint.py\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Style and rename _active to _tqdm_active\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n",
        "file": "datasets.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "datasets",
        "change": [
            "class BaseDatasetTest(TestCase):",
            "self.assertIn(\"tmp\", dset_test1.cache_files[0][\"filename\"])",
            "self.assertIn(\"tmp\", dset_test2.cache_files[0][\"filename\"])",
            "finally:",
            "-                datasets.set_caching_enabled(True)",
            "+                datasets.enable_caching()",
            "",
            "@require_torch",
            "def test_map_torch(self, in_memory):"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=set_caching_enabled), value='enable_caching')",
            "Delete(target_node=ASTNode(type=true, text=True))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 2,
        "number": 3349,
        "neg_line": [
            "-datasets.set_caching_enabled(True)"
        ],
        "pos_line": [
            "+datasets.enable_caching()"
        ],
        "core_change": "-datasets.set_caching_enabled(True) +datasets.enable_caching()",
        "core_API": "assertIn"
    },
    {
        "commit_hash": "9e85ffa5a4407c254139814114a63cdcdc5257ab",
        "index": "30a5b3a..3583cf7 100644",
        "commit_message": "bug fixed\n\n",
        "file": "autokeras.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "models",
        "change": [
            "class HillClimbingSearcher(Searcher):",
            "return self.load_best_model()",
            "",
            "",
            "-class BayesianSearcher(HillClimbingSearcher):",
            "+class BayesianSearcher(Searcher):",
            "",
            "def __init__(self, n_classes, input_shape, path, verbose):",
            "super().__init__(n_classes, input_shape, path, verbose)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=HillClimbingSearcher), value='Searcher')"
        ],
        "plus_line": 0,
        "minus_line": 0,
        "AST_diff_line": 1,
        "number": 3350,
        "neg_line": [
            "-class BayesianSearcher(HillClimbingSearcher):"
        ],
        "pos_line": [
            "+class BayesianSearcher(Searcher):"
        ],
        "core_change": "-class BayesianSearcher(HillClimbingSearcher): +class BayesianSearcher(Searcher):",
        "core_API": "load_best_model"
    },
    {
        "commit_hash": "8963a0ab4682642ef510540266fb2bdb9e61fa2c",
        "index": "d6ecf7947..2eae7a1a0 100644",
        "commit_message": "ci-fix\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def to_float(c):",
            "def complex_norm(c: Union[torch.Tensor, ComplexTensor]) -> torch.Tensor:",
            "if not is_complex(c):",
            "raise TypeError(\"Input is not a complex tensor.\")",
            "-    return torch.sqrt((c.real ** 2 + c.imag ** 2).sum(dim=-1, keepdim=True) + EPS)",
            "+    return torch.sqrt((c.real**2 + c.imag**2).sum(dim=-1, keepdim=True) + EPS)",
            "",
            "",
            "def einsum(equation, *operands):"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3354,
        "neg_line": [
            "-return torch.sqrt((c.real ** 2 + c.imag ** 2).sum(dim=-1, keepdim=True) + EPS)"
        ],
        "pos_line": [
            "+return torch.sqrt((c.real**2 + c.imag**2).sum(dim=-1, keepdim=True) + EPS)"
        ],
        "core_change": "-return torch.sqrt((c.real ** 2 + c.imag ** 2).sum(dim=-1, keepdim=True) + EPS) +return torch.sqrt((c.real**2 + c.imag**2).sum(dim=-1, keepdim=True) + EPS)",
        "core_API": "sqrt"
    },
    {
        "commit_hash": "d3cb28886ac68beba9a6646b422a4d727b056c0c",
        "index": "f647a13af..b2c14029a 100644",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class CausalSelfAttention(nn.Module):",
            "# [ batch_size x n_heads x sequence_length x sequence_length ]",
            "attn_weights = (torch.matmul(query, key.transpose(-2, -1))) * (1.0 / math.sqrt(key.size(-1)))",
            "attn_weights = attn_weights.masked_fill(",
            "-            self.mask[:, :, :sequence_length, :sequence_length] == 0, float(\"-inf\")",
            "+            self.mask[:, :, :sequence_length, :sequence_length] == 0, torch.finfo(attn_weights.dtype).min",
            ")",
            "attn_weights = F.softmax(attn_weights, dim=-1)",
            "self._attn_map = attn_weights.clone()"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=3, insert_id=1197059)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1197060)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'min'), position=2, insert_id=1197061)",
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1197062)",
            "Update(target_node=ASTNode(type=identifier, text=float), value='torch')",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=identifier, text=float), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1197063)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'finfo'), position=2, insert_id=1197064)",
            "Insert(target_node=ASTNode(type=argument_list), node=('attribute', None), position=1, insert_id=1197065)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'attn_weights'), position=0, insert_id=1197066)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1197067)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'dtype'), position=2, insert_id=1197068)",
            "Delete(target_node=ASTNode(type=string, text=\"-inf\"))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 14,
        "number": 3355,
        "neg_line": [
            "-self.mask[:, :, :sequence_length, :sequence_length] == 0, float(\"-inf\")"
        ],
        "pos_line": [
            "+self.mask[:, :, :sequence_length, :sequence_length] == 0, torch.finfo(attn_weights.dtype).min"
        ],
        "core_change": "-self.mask[:, :, :sequence_length, :sequence_length] == 0, float(\"-inf\") +self.mask[:, :, :sequence_length, :sequence_length] == 0, torch.finfo(attn_weights.dtype).min",
        "core_API": "matmul"
    },
    {
        "commit_hash": "b3316fa3b17a5851bfea4b8de24fe33cb30bf9ec",
        "index": "8e9f5e479..55ba9f3c9 100644",
        "commit_message": "fix: fix bags in keras_metrics class\n\n",
        "file": "DeepPavlov.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "keras",
        "change": [
            "class KerasModel(Trainable, Inferable):",
            "for i in range(len(metrics_names)):",
            "metrics_func = getattr(keras.metrics, metrics_names[i], None)",
            "if callable(metrics_func):",
            "-                metrics_funcs.append(keras.metrics.metrics_func)",
            "+                metrics_funcs.append(metrics_func)",
            "else:",
            "raise AttributeError(\"Metric %s is not defined\" % metrics_names[i])"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Move(target_node=ASTNode(type=argument_list), node=ASTNode(type=identifier, text=metrics_func), position=1)",
            "Delete(target_node=ASTNode(type=identifier, text=keras))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=metrics))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 7,
        "number": 3356,
        "neg_line": [
            "-metrics_funcs.append(keras.metrics.metrics_func)"
        ],
        "pos_line": [
            "+metrics_funcs.append(metrics_func)"
        ],
        "core_change": "-metrics_funcs.append(keras.metrics.metrics_func) +metrics_funcs.append(metrics_func)",
        "core_API": "append"
    },
    {
        "commit_hash": "06a1991d4153fda633af6228542f172c8cf8a9fd",
        "index": "fefe4405..c54f81e1 100644",
        "commit_message": ":elephant: Remove warnings during testing (#1401)\n\n* fix warnings during testing\n\n* Update test/geometry/transform/test_imgwarp.py\n\n* remove warning in clahe due to //\n\n* Apply suggestions from code review\n\nCo-authored-by: Luis Ferraz <luisferrazc@gmail.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TestHausdorffLoss:",
            "assert_close(actual, expected)",
            "",
            "@pytest.mark.parametrize(\"hd,shape\", [",
            "-        [kornia.losses.HausdorffERLoss, (10, 10)],",
            "-        [kornia.losses.HausdorffERLoss3D, (10, 10, 10)],",
            "+        [kornia.losses.HausdorffERLoss, (5, 5)],",
            "+        [kornia.losses.HausdorffERLoss3D, (5, 5, 5)],",
            "])",
            "-    @pytest.mark.skip(reason='It passed, but will take too much time to run.')",
            "def test_gradcheck(self, hd, shape, device):",
            "num_classes = 3",
            "logits = torch.rand(2, num_classes, *shape, device=device)"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=integer, text=10), value='5')",
            "Update(target_node=ASTNode(type=integer, text=10), value='5')",
            "Update(target_node=ASTNode(type=integer, text=10), value='5')",
            "Update(target_node=ASTNode(type=integer, text=10), value='5')",
            "Update(target_node=ASTNode(type=integer, text=10), value='5')",
            "Delete(target_node=ASTNode(type=@, text=@))",
            "Delete(target_node=ASTNode(type=identifier, text=pytest))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=mark))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=skip))",
            "Delete(target_node=ASTNode(type=attribute))",
            "Delete(target_node=ASTNode(type=(, text=())",
            "Delete(target_node=ASTNode(type=identifier, text=reason))",
            "Delete(target_node=ASTNode(type==, text==))",
            "Delete(target_node=ASTNode(type=string, text='It passed, but will take too much time to run.'))",
            "Delete(target_node=ASTNode(type=keyword_argument))",
            "Delete(target_node=ASTNode(type=), text=)))",
            "Delete(target_node=ASTNode(type=argument_list))",
            "Delete(target_node=ASTNode(type=call))",
            "Delete(target_node=ASTNode(type=decorator))"
        ],
        "plus_line": 2,
        "minus_line": 3,
        "AST_diff_line": 22,
        "number": 3357,
        "neg_line": [
            "-[kornia.losses.HausdorffERLoss, (10, 10)],",
            "-[kornia.losses.HausdorffERLoss3D, (10, 10, 10)],",
            "-@pytest.mark.skip(reason='It passed, but will take too much time to run.')"
        ],
        "pos_line": [
            "+[kornia.losses.HausdorffERLoss, (5, 5)],",
            "+[kornia.losses.HausdorffERLoss3D, (5, 5, 5)],"
        ],
        "core_change": "-[kornia.losses.HausdorffERLoss, (10, 10)], -[kornia.losses.HausdorffERLoss3D, (10, 10, 10)], +[kornia.losses.HausdorffERLoss, (5, 5)], +[kornia.losses.HausdorffERLoss3D, (5, 5, 5)], -@pytest.mark.skip(reason='It passed, but will take too much time to run.')",
        "core_API": "parametrize"
    },
    {
        "commit_hash": "eb5267f3778e30905dc77352195986d928156ae5",
        "index": "7401526d..1ecf27a2 100644",
        "commit_message": "Style quickfix\n\n",
        "file": "diffusers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class ModelMixin(torch.nn.Module):",
            ")",
            "",
            "if torch_dtype is not None and not isinstance(torch_dtype, torch.dtype):",
            "-            raise ValueError(f\"{torch_dtype} needs to be of type `torch.dtype`, e.g. `torch.float16`, but is {type(torch_dtype)}.\")",
            "+            raise ValueError(",
            "+                f\"{torch_dtype} needs to be of type `torch.dtype`, e.g. `torch.float16`, but is {type(torch_dtype)}.\"",
            "+            )",
            "elif torch_dtype is not None:",
            "model = model.to(torch_dtype)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 3,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3358,
        "neg_line": [
            "-raise ValueError(f\"{torch_dtype} needs to be of type `torch.dtype`, e.g. `torch.float16`, but is {type(torch_dtype)}.\")"
        ],
        "pos_line": [
            "+raise ValueError(",
            "+f\"{torch_dtype} needs to be of type `torch.dtype`, e.g. `torch.float16`, but is {type(torch_dtype)}.\"",
            "+)"
        ],
        "core_change": "-raise ValueError(f\"{torch_dtype} needs to be of type `torch.dtype`, e.g. `torch.float16`, but is {type(torch_dtype)}.\") +raise ValueError( +f\"{torch_dtype} needs to be of type `torch.dtype`, e.g. `torch.float16`, but is {type(torch_dtype)}.\" +)",
        "core_API": "to"
    },
    {
        "commit_hash": "6213695d6fd177666ba493dee8ba9da7eb9232b9",
        "index": "d74bea4b..6b1322bc 100644",
        "commit_message": "Fix variable override in random model.\n\n",
        "file": "tensorforce.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class RandomModel(Model):",
            "actions[name] = (tf.random_uniform(shape=shape) < 0.5)",
            "",
            "elif action['type'] == 'int':",
            "-                action = tf.floor(x=(tf.random_uniform(shape=shape) * action['num_actions']))",
            "-                actions[name] = tf.cast(x=action, dtype=util.tf_dtype(action['type']))",
            "+                sampled_action = tf.floor(x=(tf.random_uniform(shape=shape) * action['num_actions']))",
            "+                actions[name] = tf.cast(x=sampled_action, dtype=util.tf_dtype(action['type']))",
            "",
            "elif action['type'] == 'float':",
            "if 'min_value' in action:"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=action), value='sampled_action')",
            "Update(target_node=ASTNode(type=identifier, text=action), value='sampled_action')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 3359,
        "neg_line": [
            "-action = tf.floor(x=(tf.random_uniform(shape=shape) * action['num_actions']))",
            "-actions[name] = tf.cast(x=action, dtype=util.tf_dtype(action['type']))"
        ],
        "pos_line": [
            "+sampled_action = tf.floor(x=(tf.random_uniform(shape=shape) * action['num_actions']))",
            "+actions[name] = tf.cast(x=sampled_action, dtype=util.tf_dtype(action['type']))"
        ],
        "core_change": "-action = tf.floor(x=(tf.random_uniform(shape=shape) * action['num_actions'])) -actions[name] = tf.cast(x=action, dtype=util.tf_dtype(action['type'])) +sampled_action = tf.floor(x=(tf.random_uniform(shape=shape) * action['num_actions'])) +actions[name] = tf.cast(x=sampled_action, dtype=util.tf_dtype(action['type']))",
        "core_API": "random_uniform"
    },
    {
        "commit_hash": "0b86613a48e31a799590932aaadf788758ef2312",
        "index": "0a1f88fe6c..a61a643b31 100644",
        "commit_message": "Fix tests (#6182)\n\n* Remove frontend dependency on tf in nest.py\n\n* Remove frontend dependency on torch in Tensor.py\n",
        "file": "ivy.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class Tensor:",
            "self.data = self.abs()",
            "return self.data",
            "",
            "-    def contiguous(self, memory_format=torch.contiguous_format):",
            "+    def contiguous(self, memory_format=None):",
            "return self.data",
            "",
            "def new_ones(self, size, *, dtype=None, device=None, requires_grad=False):"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=default_parameter), node=('none', 'None'), position=2, insert_id=309784)",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=contiguous_format))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 5,
        "number": 3360,
        "neg_line": [
            "-def contiguous(self, memory_format=torch.contiguous_format):"
        ],
        "pos_line": [
            "+def contiguous(self, memory_format=None):"
        ],
        "core_change": "-def contiguous(self, memory_format=torch.contiguous_format): +def contiguous(self, memory_format=None):",
        "core_API": "abs"
    },
    {
        "commit_hash": "b6a7c304c68fe4a1300970459a3807ee7bce6c87",
        "index": "c57bab1f..3129c3e4 100644",
        "commit_message": "fixed cora example for new dataset\n\n",
        "file": "pytorch_geometric.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class TargetIndegree(object):",
            "",
            "if pseudo is not None and self.cat:",
            "pseudo = pseudo.view(-1, 1) if pseudo.dim() == 1 else pseudo",
            "-            data.weight = torch.cat([pseudo, deg.type_as(pseudo)], dim=-1)",
            "+            data.edge_attr = torch.cat([pseudo, deg.type_as(pseudo)], dim=-1)",
            "else:",
            "-            data.weight = deg",
            "+            data.edge_attr = deg",
            "",
            "return data"
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=weight), value='edge_attr')",
            "Update(target_node=ASTNode(type=identifier, text=weight), value='edge_attr')"
        ],
        "plus_line": 2,
        "minus_line": 2,
        "AST_diff_line": 2,
        "number": 3362,
        "neg_line": [
            "-data.weight = torch.cat([pseudo, deg.type_as(pseudo)], dim=-1)",
            "-data.weight = deg"
        ],
        "pos_line": [
            "+data.edge_attr = torch.cat([pseudo, deg.type_as(pseudo)], dim=-1)",
            "+data.edge_attr = deg"
        ],
        "core_change": "-data.weight = torch.cat([pseudo, deg.type_as(pseudo)], dim=-1) +data.edge_attr = torch.cat([pseudo, deg.type_as(pseudo)], dim=-1) -data.weight = deg +data.edge_attr = deg",
        "core_API": "view"
    },
    {
        "commit_hash": "2f463effb316f6c9e0ac9636327a3d7c13862f8d",
        "index": "445cb7625..f90dcd765 100644",
        "commit_message": "Fix TFDebertaV2ConvLayer in TFDebertaV2Model (#16031)\n\n* fix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "tensorflow",
        "match_word": "tf",
        "change": [
            "class TFDebertaV2ConvLayer(tf.keras.layers.Layer):",
            "else:",
            "if len(shape_list(input_mask)) != len(shape_list(layer_norm_input)):",
            "if len(shape_list(input_mask)) == 4:",
            "-                    mask = tf.squeeze(tf.squeeze(input_mask, axis=1), axis=1)",
            "-                mask = tf.cast(tf.expand_dims(input_mask, axis=2), tf.float32)",
            "+                    input_mask = tf.squeeze(tf.squeeze(input_mask, axis=1), axis=1)",
            "+                input_mask = tf.cast(tf.expand_dims(input_mask, axis=2), tf.float32)",
            "",
            "-            output_states = output * mask",
            "+            output_states = output * input_mask",
            "",
            "return output_states"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=mask), value='input_mask')",
            "Update(target_node=ASTNode(type=identifier, text=mask), value='input_mask')",
            "Update(target_node=ASTNode(type=identifier, text=mask), value='input_mask')"
        ],
        "plus_line": 3,
        "minus_line": 3,
        "AST_diff_line": 3,
        "number": 3363,
        "neg_line": [
            "-mask = tf.squeeze(tf.squeeze(input_mask, axis=1), axis=1)",
            "-mask = tf.cast(tf.expand_dims(input_mask, axis=2), tf.float32)",
            "-output_states = output * mask"
        ],
        "pos_line": [
            "+input_mask = tf.squeeze(tf.squeeze(input_mask, axis=1), axis=1)",
            "+input_mask = tf.cast(tf.expand_dims(input_mask, axis=2), tf.float32)",
            "+output_states = output * input_mask"
        ],
        "core_change": "-mask = tf.squeeze(tf.squeeze(input_mask, axis=1), axis=1) -mask = tf.cast(tf.expand_dims(input_mask, axis=2), tf.float32) +input_mask = tf.squeeze(tf.squeeze(input_mask, axis=1), axis=1) +input_mask = tf.cast(tf.expand_dims(input_mask, axis=2), tf.float32) -output_states = output * mask +output_states = output * input_mask",
        "core_API": "squeeze"
    },
    {
        "commit_hash": "db92d23866a083b6a8107f38c112df7fa52c60e3",
        "index": "3e37a19b..3143ab3f 100644",
        "commit_message": "Remove type ignore from the codebase (#2030)\n\n* Remove type ignore from the codebase\n\n* undo `NamedTuple` property with `List[Self]`\n\n- This is causing `Segmentation fault (core dumped)` when running `mypy --cobertura-xml-report ./`\n\n* add TODO for next mypy release\n\n* fix typo\n\n* fix F401\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def get_test_devices() -> Dict[str, torch.device]:",
            "",
            "devices[\"tpu\"] = xm.xla_device()",
            "if hasattr(torch.backends, 'mps'):",
            "-        if torch.backends.mps.is_available():  # type: ignore",
            "+        if torch.backends.mps.is_available():",
            "devices[\"mps\"] = torch.device(\"mps\")",
            "return devices"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3364,
        "neg_line": [
            "-if torch.backends.mps.is_available():  # type: ignore"
        ],
        "pos_line": [
            "+if torch.backends.mps.is_available():"
        ],
        "core_change": "-if torch.backends.mps.is_available():  # type: ignore +if torch.backends.mps.is_available():",
        "core_API": "xla_device"
    },
    {
        "commit_hash": "62e3d1dbdc88af9ed8afb37f7e8ecc011f5486a3",
        "index": "15ed7d9a..c64766c1 100644",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "file": "allennlp.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class GraphParser(Model):",
            "\"\"\"",
            "# Parameters",
            "",
            "-        tokens : Dict[str, torch.LongTensor], required",
            "+        tokens : TextFieldTensors, required",
            "The output of ``TextField.as_array()``.",
            "pos_tags : torch.LongTensor, optional (default = None)",
            "The output of a ``SequenceLabelField`` containing POS tags."
        ],
        "hunk_index": 2,
        "AST_diff": [
            "Update(target_node=ASTNode(type=identifier, text=Dict), value='TextFieldTensors')",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=identifier, text=Dict), position=0)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=,, text=,), position=1)",
            "Move(target_node=ASTNode(type=ERROR), node=ASTNode(type=attribute), position=11)",
            "Delete(target_node=ASTNode(type=[, text=[))",
            "Delete(target_node=ASTNode(type=identifier, text=str))",
            "Delete(target_node=ASTNode(type=], text=]))",
            "Delete(target_node=ASTNode(type=subscript))",
            "Delete(target_node=ASTNode(type=,, text=,))",
            "Delete(target_node=ASTNode(type=identifier, text=torch))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=LongTensor))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 13,
        "number": 3365,
        "neg_line": [
            "-tokens : Dict[str, torch.LongTensor], required"
        ],
        "pos_line": [
            "+tokens : TextFieldTensors, required"
        ],
        "core_change": "-tokens : Dict[str, torch.LongTensor], required +tokens : TextFieldTensors, required",
        "core_API": "as_array"
    },
    {
        "commit_hash": "da2f1325014a158be36d62bdd20293ef97e574f4",
        "index": "0e3aa01a..9f7ac29e 100644",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "file": "kornia.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "def confusion_matrix(",
            "confusion_list = []",
            "for iter_id in range(batch_size):",
            "pb: torch.Tensor = pre_bincount_vec[iter_id]",
            "-        bin_count: torch.Tensor = torch.bincount(pb, minlength=num_classes ** 2)",
            "+        bin_count: torch.Tensor = torch.bincount(pb, minlength=num_classes**2)",
            "confusion_list.append(bin_count)",
            "",
            "confusion_vec: torch.Tensor = torch.stack(confusion_list)"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "A",
            "S",
            "T",
            " ",
            "p",
            "a",
            "r",
            "s",
            "e",
            "r",
            " ",
            "f",
            "a",
            "i",
            "l",
            "e",
            "d"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 17,
        "number": 3368,
        "neg_line": [
            "-bin_count: torch.Tensor = torch.bincount(pb, minlength=num_classes ** 2)"
        ],
        "pos_line": [
            "+bin_count: torch.Tensor = torch.bincount(pb, minlength=num_classes**2)"
        ],
        "core_change": "-bin_count: torch.Tensor = torch.bincount(pb, minlength=num_classes ** 2) +bin_count: torch.Tensor = torch.bincount(pb, minlength=num_classes**2)",
        "core_API": "bincount"
    },
    {
        "commit_hash": "62230c88bb7c3f81943b0566c7903ad6679f12fa",
        "index": "bc4f44df8..3847a7e17 100644",
        "commit_message": "fixed typo and removed unnecessary comments\n\n",
        "file": "espnet.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "cuda",
        "change": [
            "def train(args):",
            "# Make a specified GPU current",
            "chainer.cuda.get_device_from_id(gpu_id).use()",
            "model.to_gpu()  # Copy the model to the GPU",
            "-        logging.info('single gpu calculatetion.')",
            "+        logging.info('single gpu calculation.')",
            "elif ngpu > 1:",
            "gpu_id = 0",
            "args.batch_size = math.ceil(args.batch_size / ngpu)",
            "-        # Make a specified GPU current",
            "-        # chainer.cuda.get_device_from_id(gpu_id).use()",
            "devices = {'main': gpu_id}",
            "for gid in six.moves.xrange(1, ngpu):",
            "devices['sub_%d' % gid] = gid",
            "-        logging.info('multi gpu calculatetion (#gpu = %d).' % ngpu)",
            "+        logging.info('multi gpu calculation (#gpus = %d).' % ngpu)",
            "else:",
            "gpu_id = -1",
            "logging.info('cpu calculation')"
        ],
        "hunk_index": 0,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1828413)",
            "Move(target_node=ASTNode(type=call), node=ASTNode(type=attribute), position=0)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'logging'), position=0, insert_id=1828414)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1828415)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'info'), position=2, insert_id=1828416)",
            "Update(target_node=ASTNode(type=string, text='single gpu calculatetion.'), value=\"'single gpu calculation.'\")",
            "Update(target_node=ASTNode(type=string, text='multi gpu calculatetion (#gpu = %d).'), value=\"'multi gpu calculation (#gpus = %d).'\")",
            "Delete(target_node=ASTNode(type=identifier, text=logging))",
            "Delete(target_node=ASTNode(type=., text=.))",
            "Delete(target_node=ASTNode(type=identifier, text=info))",
            "Delete(target_node=ASTNode(type=attribute))"
        ],
        "plus_line": 2,
        "minus_line": 4,
        "AST_diff_line": 11,
        "number": 3370,
        "neg_line": [
            "-logging.info('single gpu calculatetion.')",
            "-# Make a specified GPU current",
            "-# chainer.cuda.get_device_from_id(gpu_id).use()",
            "-logging.info('multi gpu calculatetion (#gpu = %d).' % ngpu)"
        ],
        "pos_line": [
            "+logging.info('single gpu calculation.')",
            "+logging.info('multi gpu calculation (#gpus = %d).' % ngpu)"
        ],
        "core_change": "-logging.info('single gpu calculatetion.') +logging.info('single gpu calculation.') -# Make a specified GPU current -# chainer.cuda.get_device_from_id(gpu_id).use() -logging.info('multi gpu calculatetion (#gpu = %d).' % ngpu) +logging.info('multi gpu calculation (#gpus = %d).' % ngpu)",
        "core_API": "get_device_from_id"
    },
    {
        "commit_hash": "e56e3140dddea7f06bfde5f040b4bb1f8ab3d21d",
        "index": "ab4879a71..1343da5ce 100644",
        "commit_message": "Fix integration tests (#12066)\n\n\n",
        "file": "transformers.txt.json",
        "label": "",
        "comments": "",
        "framework": "pytorch",
        "match_word": "torch",
        "change": [
            "class LukeModelIntegrationTests(unittest.TestCase):",
            "expected_shape = torch.Size((1, 1, 1024))",
            "self.assertEqual(outputs.entity_last_hidden_state.shape, expected_shape)",
            "",
            "-        expected_slice = torch.tensor([[0.0466, -0.0106, -0.0179]])",
            "+        expected_slice = torch.tensor([[0.0466, -0.0106, -0.0179]]).to(torch_device)",
            "self.assertTrue(torch.allclose(outputs.entity_last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))"
        ],
        "hunk_index": 1,
        "AST_diff": [
            "Insert(target_node=ASTNode(type=call), node=('attribute', None), position=0, insert_id=1215265)",
            "Insert(target_node=ASTNode(type=call), node=('argument_list', None), position=1, insert_id=1215266)",
            "Move(target_node=IN(type=attribute), node=ASTNode(type=call), position=0)",
            "Insert(target_node=IN(type=attribute), node=('.', '.'), position=1, insert_id=1215267)",
            "Insert(target_node=IN(type=attribute), node=('identifier', 'to'), position=2, insert_id=1215268)",
            "Insert(target_node=IN(type=argument_list), node=('(', '('), position=0, insert_id=1215269)",
            "Insert(target_node=IN(type=argument_list), node=('identifier', 'torch_device'), position=1, insert_id=1215270)",
            "Insert(target_node=IN(type=argument_list), node=(')', ')'), position=2, insert_id=1215271)"
        ],
        "plus_line": 1,
        "minus_line": 1,
        "AST_diff_line": 8,
        "number": 3372,
        "neg_line": [
            "-expected_slice = torch.tensor([[0.0466, -0.0106, -0.0179]])"
        ],
        "pos_line": [
            "+expected_slice = torch.tensor([[0.0466, -0.0106, -0.0179]]).to(torch_device)"
        ],
        "core_change": "-expected_slice = torch.tensor([[0.0466, -0.0106, -0.0179]]) +expected_slice = torch.tensor([[0.0466, -0.0106, -0.0179]]).to(torch_device)",
        "core_API": "Size"
    }
]