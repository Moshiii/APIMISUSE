[
    {
        "number": 3373,
        "comments": "",
        "commit_message": "fixed saved-model format\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def tf_function(*, num_args):",
            "results = function(self, **kwargs, **params_kwargs)",
            "return results",
            "",
            "-                function_graphs[graph_params] = tf.function(",
            "+                function_graphs[str(graph_params)] = tf.function(",
            "func=function_graph, input_signature=graph_signature.to_list(), autograph=False",
            "# experimental_implements=None, experimental_autograph_options=None,",
            "# experimental_relax_shapes=False, experimental_compile=None",
            ")",
            "",
            "# Apply function graph",
            "-            return function_graphs[graph_params](*graph_args)",
            "+            return function_graphs[str(graph_params)](*graph_args)",
            "",
            "# TensorFlow make_decorator",
            "return decorated"
        ]
    },
    {
        "number": 3376,
        "comments": "",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "Returns:",
            "\"\"\"",
            "",
            "",
            "-class Rouge(nlp.Metric):",
            "+class Rouge(datasets.Metric):",
            "def _info(self):",
            "-        return nlp.MetricInfo(",
            "+        return datasets.MetricInfo(",
            "description=_DESCRIPTION,",
            "citation=_CITATION,",
            "inputs_description=_KWARGS_DESCRIPTION,",
            "-            features=nlp.Features(",
            "+            features=datasets.Features(",
            "{",
            "-                    \"predictions\": nlp.Value(\"string\", id=\"sequence\"),",
            "-                    \"references\": nlp.Value(\"string\", id=\"sequence\"),",
            "+                    \"predictions\": datasets.Value(\"string\", id=\"sequence\"),",
            "+                    \"references\": datasets.Value(\"string\", id=\"sequence\"),",
            "}",
            "),",
            "codebase_urls=[\"https://github.com/google-research/google-research/tree/master/rouge\"],"
        ]
    },
    {
        "number": 3378,
        "comments": "",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def benchmark_iterating():",
            "]",
            "with tempfile.TemporaryDirectory() as tmp_dir:",
            "print(\"generating dataset\")",
            "-        features = nlp.Features({\"list\": nlp.Sequence(nlp.Value(\"float32\")), \"numbers\": nlp.Value(\"float32\")})",
            "+        features = datasets.Features(",
            "+            {\"list\": datasets.Sequence(datasets.Value(\"float32\")), \"numbers\": datasets.Value(\"float32\")}",
            "+        )",
            "dataset = generate_example_dataset(",
            "os.path.join(tmp_dir, \"dataset.arrow\"),",
            "features,"
        ]
    },
    {
        "number": 3379,
        "comments": "",
        "commit_message": "small format fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def unique_all(x: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:",
            "",
            "flat_tensor = tf.reshape(x, [-1])",
            "values, inverse_indices, counts = tf.unique_with_counts(flat_tensor)",
            "-    # values = tf.cast(values, 'float64') if values.dtype not in [tf.float32, tf.float64] else values",
            "tensor_list = flat_tensor.numpy().tolist()",
            "if (",
            "x.dtype.is_floating"
        ]
    },
    {
        "number": 3380,
        "comments": "",
        "commit_message": "fix random seed for testing\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_get_rtf(ch):",
            "normalized=False,",
            "onesided=True,",
            ")",
            "+    torch.random.manual_seed(0)",
            "x = random_speech[..., :ch]",
            "n = torch.rand(2, 16, ch, dtype=torch.double)",
            "ilens = torch.LongTensor([16, 12])"
        ]
    },
    {
        "number": 3383,
        "comments": "",
        "commit_message": "[`bnb`] fix `bnb` decoders bug (#21688)\n\n* fix `bnb` decoders bug\n\n* make fixup\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MixedInt8T5Test(unittest.TestCase):",
            "`flan-t5-small` uses `T5DenseGatedActDense` whereas `t5-small` uses `T5DenseReluDense`. We need to test",
            "both cases.",
            "\"\"\"",
            "+        import bitsandbytes as bnb",
            "+",
            "from transformers import T5ForConditionalGeneration",
            "",
            "# test with `t5-small`",
            "model = T5ForConditionalGeneration.from_pretrained(self.model_name, load_in_8bit=True, device_map=\"auto\")",
            "+",
            "+        # there was a bug with decoders - this test checks that it is fixed",
            "+        self.assertTrue(isinstance(model.decoder.block[0].layer[0].SelfAttention.q, bnb.nn.Linear8bitLt))",
            "+",
            "encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\").to(0)",
            "_ = model.generate(**encoded_input)"
        ]
    },
    {
        "number": 3384,
        "comments": "",
        "commit_message": "Tutorial fixing (#635)\n\n* TF bug fixing in Tutorials\n\n* Error fix in #476\n\n* Issue with Flags in Tutorials Fixed\n\n* Missing import fixed\n\n* Changelog Update\n\n* VGG19 import error fix\n\n* Error fixing in VGG tutorials\n\n* TFRecord Shape Error Fix\n\n* Sess Initialization Error Fix\n\n* Squeezenet model loading from \"models\" dir\n\n* PTB tutorials import issue fixed\n\n* mobilenet load from dir \"models\"\n\n* YAPF error fix\n\n* Missing Import fixed\n\n* Various Fixes on Tutorials\n\n* YAPF error correct\n\n* Update CHANGELOG.md\n\n* update VGG16 tutorial, auto download model\n\n* Python 3 Unicode Encoding Error\n\n* Deprecation Warning Fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "sess = tf.InteractiveSession()",
            "network.print_params(False)",
            "",
            "saver = tf.train.Saver()",
            "-if not os.path.isfile(\"inception_v3.ckpt\"):",
            "+if not os.path.isfile(MODEL_PATH):",
            "raise Exception(",
            "\"Please download inception_v3 ckpt from https://github.com/tensorflow/models/tree/master/research/slim\"",
            ")",
            "",
            "try:  # TF12+",
            "-    saver.restore(sess, \"./inception_v3.ckpt\")",
            "+    saver.restore(sess, MODEL_PATH)",
            "except Exception:  # TF11",
            "-    saver.restore(sess, \"inception_v3.ckpt\")",
            "+    saver.restore(sess, MODEL_PATH)",
            "print(\"Model Restored\")",
            "",
            "y = network.outputs"
        ]
    },
    {
        "number": 3386,
        "comments": "",
        "commit_message": "superset fix for `inv()` for `linalg.py` (#4988)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def matmul(",
            "*,",
            "transpose_a: bool = False,",
            "transpose_b: bool = False,",
            "-    out: Optional[torch.Tensor] = None,",
            "+    out: Optional[torch.Tensor] = None",
            ") -> torch.Tensor:",
            "if transpose_a is True:",
            "x1 = torch.t(x1)"
        ]
    },
    {
        "number": 3388,
        "comments": "",
        "commit_message": "FIX E523,E541,E741\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Decoder(BatchScorerInterface, torch.nn.Module):",
            "logp, states = self.forward_one_step(ys, ys_mask, xs, cache=batch_state)",
            "",
            "# transpose state of [layer, batch] into [batch, layer]",
            "-        state_list = [[states[l][b] for l in range(n_layers)] for b in range(n_batch)]",
            "+        state_list = [[states[i][b] for i in range(n_layers)] for b in range(n_batch)]",
            "return logp, state_list"
        ]
    },
    {
        "number": 3389,
        "comments": "",
        "commit_message": ":elephant: Remove warnings during testing (#1401)\n\n* fix warnings during testing\n\n* Update test/geometry/transform/test_imgwarp.py\n\n* remove warning in clahe due to //\n\n* Apply suggestions from code review\n\nCo-authored-by: Luis Ferraz <luisferrazc@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def rgb_to_hsv(image: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:",
            "if len(image.shape) < 3 or image.shape[-3] != 3:",
            "raise ValueError(f\"Input size must have a shape of (*, 3, H, W). Got {image.shape}\")",
            "",
            "-    # TODO: remove if/else statement once pytorch 1.6 is deprecated and keep first branch",
            "-    max_rgb, argmax_rgb = _compute_max_argmax(image)",
            "+    max_rgb, argmax_rgb = image.max(-3)",
            "min_rgb, argmin_rgb = image.min(-3)",
            "deltac = max_rgb - min_rgb"
        ]
    },
    {
        "number": 3392,
        "comments": "",
        "commit_message": "Export single output only (#7259)\n\n* Update\n\n* Update\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Detect(nn.Module):",
            "y = torch.cat((xy, wh, conf), 4)",
            "z.append(y.view(bs, -1, self.no))",
            "",
            "-        return x if self.training else (torch.cat(z, 1), x)",
            "+        return x if self.training else (torch.cat(z, 1),) if self.export else (torch.cat(z, 1), x)",
            "",
            "def _make_grid(self, nx=20, ny=20, i=0):",
            "d = self.anchors[i].device"
        ]
    },
    {
        "number": 3394,
        "comments": "",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def benchmark_indices_mapping():",
            "functions = (select, sort, shuffle, train_test_split, shard)",
            "with tempfile.TemporaryDirectory() as tmp_dir:",
            "print(\"generating dataset\")",
            "-        features = nlp.Features({\"text\": nlp.Value(\"string\"), \"numbers\": nlp.Value(\"float32\")})",
            "+        features = datasets.Features({\"text\": datasets.Value(\"string\"), \"numbers\": datasets.Value(\"float32\")})",
            "dataset = generate_example_dataset(",
            "os.path.join(tmp_dir, \"dataset.arrow\"), features, num_examples=SPEED_TEST_N_EXAMPLES",
            ")"
        ]
    },
    {
        "number": 3395,
        "comments": "",
        "commit_message": "Should be fixed finally\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def main():",
            "# train",
            "logging.info('backend = ' + args.backend)",
            "if args.backend == \"chainer\":",
            "-        from espnet.lmchainer.asr_chainer import train",
            "+        from espnet.asr.chainer.asr_chainer import train",
            "train(args)",
            "elif args.backend == \"pytorch\":",
            "-        from espnet.lmpytorch.asr_pytorch import train",
            "+        from espnet.asr.pytorch.asr_pytorch import train",
            "train(args)",
            "else:",
            "raise ValueError(\"Only chainer and pytorch are supported.\")"
        ]
    },
    {
        "number": 3396,
        "comments": "",
        "commit_message": "improve logging and fix wd computation in inference\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def regularize_cost_from_collection(name='regularize_cost'):",
            "\"\"\"",
            "regularization_losses = set(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))",
            "ctx = get_current_tower_context()",
            "+    if not ctx.is_training:",
            "+        # Currently cannot build the wd_cost correctly at inference,",
            "+        # because ths vs_name used in inference can be '', therefore the",
            "+        # variable filter will fail",
            "+        return None",
            "+",
            "if len(regularization_losses) > 0:",
            "# NOTE: this collection doesn't grow with towers.",
            "# It is only added with variables that are newly created."
        ]
    },
    {
        "number": 3400,
        "comments": "",
        "commit_message": "Fix TF version of imagenet loader (fix #1085)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def fbresnet_mapper(isTrain):",
            "return image",
            "",
            "def lighting(image, std, eigval, eigvec):",
            "-        v = tf.random_uniform(shape=[3]) * std * eigval",
            "+        v = tf.random_normal(shape=[3], stddev=std) * eigval",
            "inc = tf.matmul(eigvec, tf.reshape(v, [3, 1]))",
            "image = tf.cast(tf.cast(image, tf.float32) + tf.reshape(inc, [3]), image.dtype)",
            "return image"
        ]
    },
    {
        "number": 3401,
        "comments": "",
        "commit_message": "fix Average metric (#4624)\n\n* fix Average metric\n\n* try spawn\n\n* try again\n\n* oops\n\n* clean up, fix evalb too\n\n* use different start method for GPU vs CPU tests\n\n* add comment\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Average(Metric):",
            "_total_value = list(self.detach_tensors(value))[0]",
            "_count = 1",
            "if is_distributed():",
            "-            device = torch.device(\"cpu\")",
            "+            device = torch.device(\"cuda\" if dist.get_backend() == \"nccl\" else \"cpu\")",
            "count = torch.tensor(_count).to(device)",
            "total_value = torch.tensor(_total_value).to(device)",
            "dist.all_reduce(count, op=dist.ReduceOp.SUM)"
        ]
    },
    {
        "number": 3402,
        "comments": "",
        "commit_message": "[RLlib] Fix 3 flakey test cases. (#15785)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "torch, _ = try_import_torch()",
            "class TestDDPG(unittest.TestCase):",
            "@classmethod",
            "def setUpClass(cls) -> None:",
            "+        np.random.seed(42)",
            "+        torch.manual_seed(42)",
            "ray.init()",
            "",
            "@classmethod"
        ]
    },
    {
        "number": 3403,
        "comments": "",
        "commit_message": "Enabling multilingual models for translation pipelines. (#10536)\n\n* [WIP] Enabling multilingual models for translation pipelines.\n\n* decoder_input_ids -> forced_bos_token_id\n\n* Improve docstring.\n\n* Rebase\n\n* Fixing 2 bugs\n\n- Type token_ids coming from `_parse_and_tokenize`\n- Wrong index from tgt_lang.\n\n* Fixing black version.\n\n* Adding tests for _build_translation_inputs and add them for all\ntokenizers.\n\n* Mbart actually puts the lang code at the end.\n\n* Fixing m2m100.\n\n* Adding TF support to `deep_round`.\n\n* Update src/transformers/pipelines/text2text_generation.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* Adding one line comment.\n\n* Fixing M2M100 `_build_translation_input_ids`, and fix the call site.\n\n* Fixing tests + deep_round -> nested_simplify\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Pipeline(_ScikitCompat):",
            "Return:",
            ":obj:`Dict[str, torch.Tensor]`: The same as :obj:`inputs` but on the proper device.",
            "\"\"\"",
            "-        return {name: tensor.to(self.device) for name, tensor in inputs.items()}",
            "+        return {",
            "+            name: tensor.to(self.device) if isinstance(tensor, torch.Tensor) else tensor",
            "+            for name, tensor in inputs.items()",
            "+        }",
            "",
            "def check_model_type(self, supported_models: Union[List[str], dict]):",
            "\"\"\""
        ]
    },
    {
        "number": 3404,
        "comments": "",
        "commit_message": "Fix einsum transpose (#2532)\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2532\n\nReviewed By: myleott\n\nDifferential Revision: D32049520\n\nPulled By: sshleifer\n\nfbshipit-source-id: 9036c6db48c15e8a04a27a7d3660bdb2a248f0a5\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TransformerDecoderLayerBase(nn.Module):",
            "if self.c_attn is not None:",
            "tgt_len, bsz = x.size(0), x.size(1)",
            "x = x.view(tgt_len, bsz, self.nh, self.head_dim)",
            "-            x = torch.einsum('tbhd,h->tbdh', x, self.c_attn)",
            "+            x = torch.einsum('tbhd,h->tbhd', x, self.c_attn)",
            "x = x.reshape(tgt_len, bsz, self.embed_dim)",
            "if self.attn_ln is not None:",
            "x = self.attn_ln(x)"
        ]
    },
    {
        "number": 3405,
        "comments": "",
        "commit_message": "fix docs of distributed\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LoadCheckpoint(session_run_hook.SessionRunHook):",
            "def after_create_session(self, session, coord):",
            "if not self._loaded:",
            "self._loaded = True",
            "-            self._saver.restore(self._checkpoint)",
            "\\ No newline at end of file",
            "+            self._saver.restore(self._checkpoint)"
        ]
    },
    {
        "number": 3406,
        "comments": "",
        "commit_message": "Replace some fs operations by tf.gfile for other fs support. (fix #416)\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def get_checkpoint_path(model_path):",
            "logger.warn(",
            "\"Checkpoint path {} is auto-corrected to {}.\".format(model_path, new_path))",
            "model_path = new_path",
            "-    assert os.path.isfile(model_path) or os.path.isfile(model_path + '.index'), model_path",
            "+    assert tf.gfile.Exists(model_path) or tf.gfile.Exists(model_path + '.index'), model_path",
            "return model_path"
        ]
    },
    {
        "number": 3407,
        "comments": "",
        "commit_message": "Refactor library namespaces [pre-release][0.6-rc1] (#1412)\n\n* flake fixes\n\n* initial flake8 fixeS\n\n* remove top level from kornia.color\n\n* kornia filters\n\n* kornia losses\n\n* kornia features\n\n* geomtry and all ok\n\n* removed jit module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* apply formatting and few fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add keep block for isort\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* skip init\n\n* fix the docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove old code\n\n* simplify ci workflow\n\n* fix circular dependency\n\n* few format fixes\n\n* fix code format test\n\n* remove kornia.jit imports\n\n* final fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* ci fixes\n\n* add versioneer\n\n* fix pnp import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* exclude version files from pre-commit\n\n* exclude files precommit\n\n* update to pytorch 1.10 and add fixes\n\n* Update tests_cpu.yml\n\n* Update setup_dev_env.sh\n\n* Update tests_cpu.yml\n\n* undo versioneer\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix no_grad\n\n* Apply suggestions from code review\n\n* fix skip windows tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update test_integrated.py\n\n* Apply suggestions from code review\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def get_projective_transform(center: torch.Tensor, angles: torch.Tensor, scales:",
            "raise AssertionError(center.dtype, angles.dtype)",
            "",
            "# create rotation matrix",
            "-    angle_axis_rad: torch.Tensor = K.deg2rad(angles)",
            "-    rmat: torch.Tensor = K.angle_axis_to_rotation_matrix(angle_axis_rad)  # Bx3x3",
            "-    scaling_matrix: torch.Tensor = K.eye_like(3, rmat)",
            "+    angle_axis_rad: torch.Tensor = deg2rad(angles)",
            "+    rmat: torch.Tensor = angle_axis_to_rotation_matrix(angle_axis_rad)  # Bx3x3",
            "+    scaling_matrix: torch.Tensor = eye_like(3, rmat)",
            "scaling_matrix = scaling_matrix * scales.unsqueeze(dim=1)",
            "rmat = rmat @ scaling_matrix.to(rmat)"
        ]
    },
    {
        "number": 3408,
        "comments": "",
        "commit_message": "GH-462: fix cuda errors in classifier and unit tests\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TextClassifier(flair.nn.Model):",
            "self.document_embeddings.embed(sentences)",
            "",
            "text_embedding_list = [sentence.get_embedding().unsqueeze(0) for sentence in sentences]",
            "-        text_embedding_tensor = torch.cat(text_embedding_list, 0)",
            "+        text_embedding_tensor = torch.cat(text_embedding_list, 0).to(flair.device)",
            "",
            "label_scores = self.decoder(text_embedding_tensor)"
        ]
    },
    {
        "number": 3410,
        "comments": "",
        "commit_message": "fix cpu usage\n",
        "label": "",
        "answer": "no",
        "change": [
            "def reconstruct_cond_batch(c: ScheduledPromptBatch, current_step):",
            "break",
            "res[i] = cond_schedule[target_index].cond",
            "",
            "-    return res.to(shared.device)",
            "+    return res"
        ]
    },
    {
        "number": 3415,
        "comments": "",
        "commit_message": "fix gpu decoding\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def recog_v2(args):",
            "for idx, name in enumerate(js.keys(), 1):",
            "logging.info('(%d/%d) decoding ' + name, idx, len(js.keys()))",
            "batch = [(name, js[name])]",
            "-            enc = model.encode(load_inputs_and_targets(batch)[0][0])",
            "+            feat = load_inputs_and_targets(batch)[0][0]",
            "+            enc = model.encode(torch.as_tensor(feat).to(device))",
            "nbest_hyps = beam_search(",
            "x=enc,",
            "sos=model.sos,"
        ]
    },
    {
        "number": 3418,
        "comments": "",
        "commit_message": "[Testing] Fix LINT/sphinx errors. (#8874)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TorchRNNModel(TorchRNN):",
            "name,",
            "fc_size=64,",
            "lstm_state_size=256):",
            "+        nn.Module.__init__(self)",
            "super().__init__(obs_space, action_space, num_outputs, model_config,",
            "name)"
        ]
    },
    {
        "number": 3419,
        "comments": "",
        "commit_message": "Fixed error: Expected object of device type cuda but got device type cpu for argument\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DecoderRNNT(torch.nn.Module):",
            "normscore = recog_args.score_norm_transducer",
            "",
            "z_list, c_list = self.zero_state(h.unsqueeze(0))",
            "-        eys = torch.zeros((1, self.embed_dim))",
            "+        eys = to_device(self, torch.zeros((1, self.embed_dim)))",
            "",
            "_, (z_list, c_list) = self.rnn_forward(eys, None)"
        ]
    },
    {
        "number": 3421,
        "comments": "",
        "commit_message": "bugfix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def parse_npz(f):",
            "",
            "adj = sp.csr_matrix((f['adj_data'], f['adj_indices'], f['adj_indptr']),",
            "f['adj_shape']).tocoo()",
            "-    edge_index = torch.tensor([adj.row, adj.col])",
            "+    edge_index = torch.tensor([adj.row, adj.col], dtype=torch.long)",
            "edge_index, _ = remove_self_loops(edge_index)",
            "edge_index = to_undirected(edge_index, x.size(0))  # Internal coalesce."
        ]
    },
    {
        "number": 3423,
        "comments": "",
        "commit_message": "fix(data): fix sampler bug for single device (#22)\n\nCo-authored-by: liusongtao <liusongtao@megvii.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Exp(MyExp):",
            "",
            "if is_distributed:",
            "batch_size = batch_size // dist.get_world_size()",
            "-            sampler = InfiniteSampler(",
            "-                len(self.dataset), seed=self.seed if self.seed else 0",
            "-            )",
            "-        else:",
            "-            sampler = torch.utils.data.RandomSampler(self.dataset)",
            "+",
            "+        sampler = InfiniteSampler(",
            "+            len(self.dataset), seed=self.seed if self.seed else 0",
            "+        )",
            "",
            "batch_sampler = YoloBatchSampler(",
            "sampler=sampler,"
        ]
    },
    {
        "number": 3425,
        "comments": "",
        "commit_message": "[Audio datasets] Adapting all audio datasets (#3081)\n\n* up\n\n* up\n\n* up\n\n* up\n\n* up\n\n* Update src/datasets/utils/resources/readme_structure.yaml\n\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\n\n* correct\n\n* correct 2\n\n* Update datasets/covost2/README.md\n\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\n\n* Fix typo\n\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LJSpeech(datasets.GeneratorBasedBuilder):",
            "example = {",
            "\"id\": uid,",
            "\"file\": os.path.join(wav_path, filename),",
            "+                    \"audio\": os.path.join(wav_path, filename),",
            "\"text\": text,",
            "\"normalized_text\": norm_text,",
            "}"
        ]
    },
    {
        "number": 3426,
        "comments": "",
        "commit_message": "fix flake8 style in tensorpack/\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def LeakyReLU(x, alpha, name=None):",
            "if name is None:",
            "name = 'output'",
            "return tf.maximum(x, alpha * x, name=name)",
            "-    #alpha = float(alpha)",
            "-    #x = ((1 + alpha) * x + (1 - alpha) * tf.abs(x))",
            "+    # alpha = float(alpha)",
            "+    # x = ((1 + alpha) * x + (1 - alpha) * tf.abs(x))",
            "# return tf.mul(x, 0.5, name=name)"
        ]
    },
    {
        "number": 3429,
        "comments": "",
        "commit_message": "Fixed typos and spacing according to PEP 8\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ExampleModel(BaseModel):",
            "",
            "",
            "def init_saver(self):",
            "-        # here you initalize the tensorflow saver that will be used in saving the checkpoints.",
            "+        # here you initialize the tensorflow saver that will be used in saving the checkpoints.",
            "self.saver = tf.train.Saver(max_to_keep=self.config.max_to_keep)"
        ]
    },
    {
        "number": 3430,
        "comments": "",
        "commit_message": "Double variable fetch fixed, deprecated arg in softmax renamed.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Categorical(Distribution):",
            "state_value = tf.reduce_logsumexp(input_tensor=logits, axis=-1)",
            "",
            "# Softmax for corresponding probabilities",
            "-        probabilities = tf.nn.softmax(logits=logits, dim=-1)",
            "+        probabilities = tf.nn.softmax(logits=logits, axis=-1)",
            "",
            "# Min epsilon probability for numerical stability",
            "probabilities = tf.maximum(x=probabilities, y=util.epsilon)"
        ]
    },
    {
        "number": 3431,
        "comments": "",
        "commit_message": "fix small bug\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class UNetUnconditionalModel(ModelMixin, ConfigMixin):",
            "prev_output_channel = output_channel",
            "",
            "# out",
            "-        self.conv_norm_out = nn.GroupNorm(num_channels=block_channels[0], num_groups=32, eps=1e-5)",
            "+        self.conv_norm_out = nn.GroupNorm(num_channels=block_channels[0], num_groups=32, eps=resnet_eps)",
            "self.conv_act = nn.SiLU()",
            "self.conv_out = nn.Conv2d(block_channels[0], out_channels, 3, padding=1)"
        ]
    },
    {
        "number": 3435,
        "comments": "",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Trainer(Trainable):",
            "logging.getLogger(\"ray.rllib\").setLevel(self.config[\"log_level\"])",
            "",
            "def get_scope():",
            "-            if tf and not tf.executing_eagerly():",
            "-                return tf.Graph().as_default()",
            "+            if tf1 and not tf1.executing_eagerly():",
            "+                return tf1.Graph().as_default()",
            "else:",
            "return open(os.devnull)  # fake a no-op scope"
        ]
    },
    {
        "number": 3436,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PassThroughEncoder(Seq2SeqEncoder):",
            "else:",
            "# We should mask out the output instead of the input.",
            "# But here, output = input, so we directly mask out the input.",
            "-            return inputs * mask.unsqueeze(dim=-1).float()",
            "+            return inputs * mask.unsqueeze(dim=-1)"
        ]
    },
    {
        "number": 3437,
        "comments": "",
        "commit_message": "small fix as zeta of torch does support native out\n",
        "label": "",
        "answer": "no",
        "change": [
            "def zeta(",
            "*,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    return torch.special.zeta(x, q)",
            "+    return torch.special.zeta(x, q, out=out)",
            "",
            "",
            "-zeta.support_native_out = False",
            "+zeta.support_native_out = True",
            "",
            "",
            "def gradient("
        ]
    },
    {
        "number": 3441,
        "comments": "",
        "commit_message": "Adds CLIP to models exportable with ONNX (#18515)\n\n* onnx config for clip\n\n* default opset as 14\n\n* changes from the original repo\n\n* input values order fix\n\n* outputs fix\n\n* remove unused import\n\n* ran make fix-copies\n\n* black format\n\n* review comments: forward ref, import fix, model change revert, .to cleanup\n\n* make style\n\n* formatting fixes\n\n* revert groupvit\n\n* comment for cast to int32\n\n* comment fix\n\n* make .T as .t() for onnx conversion\n\n* ran make fix-copies\n\n* remove unneeded comment\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* fix copies\n\n* remove comment\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class GroupViTTextTransformer(nn.Module):",
            "",
            "# text_embeds.shape = [batch_size, sequence_length, transformer.width]",
            "# take features from the eot embedding (eot_token is the highest number in each sequence)",
            "-        pooled_output = last_hidden_state[torch.arange(last_hidden_state.shape[0]), input_ids.argmax(dim=-1)]",
            "+        # casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14",
            "+        pooled_output = last_hidden_state[",
            "+            torch.arange(last_hidden_state.shape[0]), input_ids.to(torch.int).argmax(dim=-1)",
            "+        ]",
            "",
            "if not return_dict:",
            "return (last_hidden_state, pooled_output) + encoder_outputs[1:]"
        ]
    },
    {
        "number": 3442,
        "comments": "",
        "commit_message": "gradient norm clipping should be done right before calling the optimiser - fixing run_glue and run_ner as well\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def train(args, train_dataset, model, tokenizer, labels, pad_token_label_id):",
            "if args.fp16:",
            "with amp.scale_loss(loss, optimizer) as scaled_loss:",
            "scaled_loss.backward()",
            "-                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)",
            "else:",
            "loss.backward()",
            "-                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)",
            "",
            "tr_loss += loss.item()",
            "if (step + 1) % args.gradient_accumulation_steps == 0:",
            "+                if args.fp16:",
            "+                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)",
            "+                else:",
            "+                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)",
            "+",
            "scheduler.step()  # Update learning rate schedule",
            "optimizer.step()",
            "model.zero_grad()"
        ]
    },
    {
        "number": 3443,
        "comments": "",
        "commit_message": "fix python3 pytorch0.4 errors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train(args):",
            "",
            "# Save attention weight each epoch",
            "if args.num_save_attention > 0 and args.mtlalpha != 1.0:",
            "-        data = sorted(valid_json.items()[:args.num_save_attention],",
            "+        data = sorted(list(valid_json.items())[:args.num_save_attention],",
            "key=lambda x: int(x[1]['input'][0]['shape'][1]), reverse=True)",
            "data = converter_kaldi([data], device=gpu_id)",
            "trainer.extend(PlotAttentionReport(model, data, args.outdir + \"/att_ws\"), trigger=(1, 'epoch'))"
        ]
    },
    {
        "number": 3446,
        "comments": "",
        "commit_message": "fix CI error\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def trans(args):",
            "names = [name for name in names if name]",
            "batch = [(name, js[name]) for name in names]",
            "feats = load_inputs_and_targets(batch)[0]",
            "-                nbest_hyps = model.translate_batch(",
            "-                    feats,",
            "-                    args,",
            "-                    train_args.char_list,",
            "-                )",
            "+                nbest_hyps = model.translate_batch(feats, args, train_args.char_list)",
            "",
            "for i, nbest_hyp in enumerate(nbest_hyps):",
            "name = names[i]"
        ]
    },
    {
        "number": 3452,
        "comments": "",
        "commit_message": "Improve mnist examples and fix bugs. (#382)\n\n* simplified mnist example\n\n* simplied mnist example\n\n* simplied mnist example\n\n* simplied mnist example\n\n* simplied mnist example\n\n* IMPORTANT : fixed D_TYPE bug / as we splited the layers into many file, the D_TYPE in core.py cant change in other files\n\n* update layer config.\n\n* format code\n\n* decouple set_keep.\n\n* fix hao comments.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PReluLayer(Layer):",
            "",
            "# with tf.name_scope(name) as scope:",
            "with tf.variable_scope(name):",
            "-            alphas = tf.get_variable(name='alphas', shape=w_shape, initializer=a_init, dtype=D_TYPE, **a_init_args)",
            "+            alphas = tf.get_variable(name='alphas', shape=w_shape, initializer=a_init, dtype=LayersConfig.tf_dtype, **a_init_args)",
            "try:  # TF 1.0",
            "self.outputs = tf.nn.relu(self.inputs) + tf.multiply(alphas, (self.inputs - tf.abs(self.inputs))) * 0.5",
            "except Exception:  # TF 0.12"
        ]
    },
    {
        "number": 3456,
        "comments": "",
        "commit_message": "Fix torchelastic detection with non-distributed installations (#13142)\n\n* Fix torchelastic detection under Mac\n\n* CHANGELOG\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TorchElasticEnvironment(ClusterEnvironment):",
            "def detect() -> bool:",
            "\"\"\"Returns ``True`` if the current process was launched using the torchelastic command.\"\"\"",
            "if _TORCH_GREATER_EQUAL_1_9_1:",
            "-            return torch.distributed.is_torchelastic_launched()",
            "+            # if not available (for example on MacOS), `is_torchelastic_launched` is not defined",
            "+            return torch.distributed.is_available() and torch.distributed.is_torchelastic_launched()",
            "required_env_vars = {\"RANK\", \"GROUP_RANK\", \"LOCAL_RANK\", \"LOCAL_WORLD_SIZE\"}",
            "return required_env_vars.issubset(os.environ.keys())"
        ]
    },
    {
        "number": 3459,
        "comments": "",
        "commit_message": "PEP8 fix.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def cast(x, dtype):",
            "# you need to assign it.",
            ">>> input = K.cast(input, dtype='float16')",
            ">>> input",
            "-        <tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16>",
            "+        <tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16>",
            "```",
            "\"\"\"",
            "return tf.cast(x, dtype)"
        ]
    },
    {
        "number": 3464,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestFeedforwardEncoder(AllenNlpTestCase):",
            ")",
            "",
            "# mask should work",
            "-        mask = torch.LongTensor([[1, 1, 1], [1, 0, 0]])",
            "+        mask = torch.BoolTensor([[True, True, True], [True, False, False]])",
            "output = encoder(tensor, mask)",
            "target = feedforward(tensor) * mask.unsqueeze(dim=-1).float()",
            "numpy.testing.assert_array_almost_equal("
        ]
    },
    {
        "number": 3465,
        "comments": "",
        "commit_message": "[UNet2DConditionModel, UNet2DModel] pass norm_num_groups to all the blocks (#442)\n\n* pass norm_num_groups to unet blocs and attention\n\n* fix UNet2DConditionModel\n\n* add norm_num_groups arg in vae\n\n* add tests\n\n* remove comment\n\n* Apply suggestions from code review\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SpatialTransformer(nn.Module):",
            "self.d_head = d_head",
            "self.in_channels = in_channels",
            "inner_dim = n_heads * d_head",
            "-        self.norm = torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)",
            "+        self.norm = torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)",
            "",
            "self.proj_in = nn.Conv2d(in_channels, inner_dim, kernel_size=1, stride=1, padding=0)"
        ]
    },
    {
        "number": 3466,
        "comments": "",
        "commit_message": "[rllib] Remove dependency on TensorFlow (#4764)\n\n* remove hard tf dep\n\n* add test\n\n* comment fix\n\n* fix test\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LSTM(Model):",
            "",
            "@override(Model)",
            "def _build_layers_v2(self, input_dict, num_outputs, options):",
            "+        import tensorflow.contrib.rnn as rnn",
            "+",
            "cell_size = options.get(\"lstm_cell_size\")",
            "if options.get(\"lstm_use_prev_action_reward\"):",
            "action_dim = int("
        ]
    },
    {
        "number": 3468,
        "comments": "",
        "commit_message": "fix travis + pylint tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class WaveRNN(nn.Module):",
            "",
            "def forward(self, x, mels):",
            "bsize = x.size(0)",
            "-        h1 = torch.zeros(1, bsize, self.rnn_dims).to(x.device)",
            "+        h1 = torch.zeros(1, bsize, self.rnn_dims).to(x.device)",
            "h2 = torch.zeros(1, bsize, self.rnn_dims).to(x.device)",
            "mels, aux = self.upsample(mels)"
        ]
    },
    {
        "number": 3469,
        "comments": "",
        "commit_message": "add pytorch 1.1.0 SyncBN support (#577)\n\n* add pytorch 1.1.0 SyncBN support\n\n* change BatchNorm2d to _BatchNorm and call freeze after train\n\n* add freeze back to init function\n\n* fixed indentation typo in adding freeze\n\n* use SyncBN protect member func to set ddp_gpu_num\n\n* Update README.md\n\nupdate pytorch version to 1.1\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ResNet(nn.Module):",
            "",
            "def train(self, mode=True):",
            "super(ResNet, self).train(mode)",
            "+        self._freeze_stages()",
            "if mode and self.norm_eval:",
            "for m in self.modules():",
            "# trick: eval have effect on BatchNorm only",
            "-                if isinstance(m, nn.BatchNorm2d):",
            "+                if isinstance(m, _BatchNorm):",
            "m.eval()"
        ]
    },
    {
        "number": 3470,
        "comments": "",
        "commit_message": "batchnorm refactored, bug fixed, get_variable_with_initializer changed\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Input(Layer):",
            "logging.info(\"Input  %s: %s\" % (self.name, str(shape)))",
            "",
            "shape_without_none = [_ if _ is not None else 1 for _ in shape]",
            "-        self.outputs = self.forward(tf.initializers.random_normal()(shape_without_none))",
            "+        self.outputs = self.forward(tf.compat.v1.initializers.random_normal()(shape_without_none))",
            "",
            "def __call__(self, prev_layer):",
            "# FIXME: better exception raising"
        ]
    },
    {
        "number": 3471,
        "comments": "",
        "commit_message": "\ud83d\ude9d Fixing Monorepo CI \ud83c\udf89 (#5642)\n\n* Updating GitHub CI workflow files\n\n* Fixing issue with MNIST dataset and torch downgrade from SyMPC\n\n* Fixed Syft and Grid pre-commit to be isolated\n\n* Bumped SyMPC to torch 1.8.1 branch\n\n* Moved MNIST to our own mirror on GitHub\n\n* Run PyGrid MCFL tests without random and waiting on finish app start\n\n* Added retry to grid_connect to fix CI random refused connections\n",
        "label": "",
        "answer": "no",
        "change": [
            "if TORCHVISION_VERSION < version.parse(\"0.9.1\"):",
            "\"ec29112dd5afa0611ce80d1b7f02629c\",",
            "),",
            "]",
            "-",
            "+else:",
            "+    torchvision.datasets.MNIST.mirrors.insert(0, URL)",
            "",
            "torchvision.datasets.MNIST(get_root_data_path(), train=True, download=True)",
            "torchvision.datasets.MNIST(get_root_data_path(), train=False, download=True)"
        ]
    },
    {
        "number": 3474,
        "comments": "",
        "commit_message": "Fix average_precision metric (#2319)\n\n* Fixed average_precision metric, parenthesis were missing. Added test test that failed with the old implementation\n\n* Modified CHANGELOG.md\n\n* Update CHANGELOG.md\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def average_precision(",
            "# Return the step function integral",
            "# The following works because the last entry of precision is",
            "# guaranteed to be 1, as returned by precision_recall_curve",
            "-    return -torch.sum(recall[1:] - recall[:-1] * precision[:-1])",
            "+    return -torch.sum((recall[1:] - recall[:-1]) * precision[:-1])",
            "",
            "",
            "def dice_score("
        ]
    },
    {
        "number": 3477,
        "comments": "",
        "commit_message": "Fix shards in IterableDataset.from_generator (#5233)\n\n* correctly pass the gen_kwards in Generator builder\n\n* docs\n\n* tests\n\n* style\n\n* more docs\n\n* typo\n\n* typo2\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Generator(datasets.GeneratorBasedBuilder):",
            "return datasets.DatasetInfo(features=self.config.features)",
            "",
            "def _split_generators(self, dl_manager):",
            "-        return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={})]",
            "+        return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs=self.config.gen_kwargs)]",
            "",
            "-    def _generate_examples(self):",
            "-        for idx, ex in enumerate(self.config.generator(**self.config.gen_kwargs)):",
            "+    def _generate_examples(self, **gen_kwargs):",
            "+        for idx, ex in enumerate(self.config.generator(**gen_kwargs)):",
            "yield idx, ex"
        ]
    },
    {
        "number": 3479,
        "comments": "",
        "commit_message": "[RLlib] Issue 12118: LSTM prev-a/r should be separately configurable. Fix missing prev-a one-hot encoding. (#12397)\n\n* WIP.\n\n* Fix and LINT.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def one_hot(x, depth=0, on_value=1, off_value=0):",
            "x = x.astype(np.int)",
            "depth = 2",
            "",
            "+    # If depth is not given, try to infer it from the values in the array.",
            "if depth == 0:",
            "depth = np.max(x) + 1",
            "assert np.max(x) < depth, \\"
        ]
    },
    {
        "number": 3481,
        "comments": "",
        "commit_message": "functionality for saving optimizer state (#257)\n\n* cleanup\n\n* remove print statement\n\n* fix weird merge nonsense\n\n* fix weird merge nonsense part 2\n\n* py3 tab nonsense\n\n* more tests\n\n* fl8\n\n*  py3 blah\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MaskedLinear(nn.Linear):",
            "Constructor",
            "\"\"\"",
            "super(MaskedLinear, self).__init__(in_features, out_features, bias)",
            "-        self.register_buffer('mask', mask)",
            "+        self.register_buffer('mask', mask.data)",
            "",
            "def forward(self, _input):",
            "-        masked_weight = self.weight * self.mask",
            "+        masked_weight = self.weight * torch.autograd.Variable(self.mask)",
            "return F.linear(_input, masked_weight, self.bias)"
        ]
    },
    {
        "number": 3484,
        "comments": "",
        "commit_message": "fix version + yapf (#6999)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def atomic_save(checkpoint, filepath: str):",
            "# Can't use the new zipfile serialization for 1.6.0 because there's a bug in",
            "# torch.hub.load_state_dict_from_url() that prevents it from loading the new files.",
            "# More details can be found here: https://github.com/pytorch/pytorch/issues/42239",
            "-    if LooseVersion(torch.__version__).version[:3] == [1, 6, 0]:",
            "+    if Version(torch.__version__).release[:3] == (1, 6, 0):",
            "torch.save(checkpoint, bytesbuffer, _use_new_zipfile_serialization=False)",
            "else:",
            "torch.save(checkpoint, bytesbuffer)"
        ]
    },
    {
        "number": 3485,
        "comments": "",
        "commit_message": "fix indentation issue (#4941)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PyTorchBenchmark(Benchmark):",
            ")",
            "torch.cuda.reset_max_memory_cached()",
            "",
            "-                        # run forward",
            "-                        _forward()",
            "+                    # run forward",
            "+                    _forward()",
            "elif not self.args.no_tpu and is_torch_tpu_available():",
            "# tpu",
            "raise NotImplementedError("
        ]
    },
    {
        "number": 3490,
        "comments": "",
        "commit_message": "fix errors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MultiCompilerOptimizer(BaseOptimizer):",
            ")",
            "if return_all:",
            "return optimized_models",
            "-        optimized_models.sort(key=lambda x: x[1], ascending=True)",
            "+        optimized_models.sort(key=lambda x: x[1], reverse=False)",
            "return optimized_models[0][0]"
        ]
    },
    {
        "number": 3491,
        "comments": "",
        "commit_message": "Fix tests failing on a single GPU (#11753)\n\nCo-authored-by: akihiro@grid.ai <akihiro@grid.ai>\nCo-authored-by: Carlos Mocholi <carlossmocholi@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_quantization(tmpdir, observe: str, fuse: bool, convert: bool):",
            "# todo: make it work also with strict loading",
            "qmodel2 = RegressionModel.load_from_checkpoint(model_path, strict=False)",
            "quant2_score = torch.mean(torch.tensor([mape(qmodel2(x), y) for x, y in dm.test_dataloader()]))",
            "-    assert torch.allclose(org_score, quant2_score, atol=0.45)",
            "+    assert torch.allclose(org_score, quant2_score, atol=0.47)",
            "",
            "# test without and with QAT callback",
            "trainer_args.update(max_epochs=curr_epoch + 1)"
        ]
    },
    {
        "number": 3492,
        "comments": "",
        "commit_message": "Detect.py supports running against a Triton container (#9228)\n\n* update coco128-seg comments\n\n* Enables detect.py to use Triton for inference\n\nTriton Inference Server is an open source inference serving software\nthat streamlines AI inferencing.\nhttps://github.com/triton-inference-server/server\n\nThe user can now provide a \"--triton-url\" argument to detect.py to use\na local or remote Triton server for inference.\nFor e.g., http://localhost:8000 will use http over port 8000\nand grpc://localhost:8001 will use grpc over port 8001.\nNote, it is not necessary to specify a weights file to use Triton.\n\nA Triton container can be created by first exporting the Yolov5 model\nto a Triton supported runtime. Onnx, Torchscript, TensorRT are\nsupported by both Triton and the export.py script.\n\nThe exported model can then be containerized via the OctoML CLI.\nSee https://github.com/octoml/octo-cli#getting-started for a guide.\n\n* added triton client to requirements\n\n* fixed support for TFSavedModels in Triton\n\n* reverted change\n\n* Test CoreML update\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update ci-testing.yml\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Use pathlib\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Refacto DetectMultiBackend to directly accept triton url as --weights http://...\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Deploy category\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update detect.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update common.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update common.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update predict.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update predict.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update predict.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update triton.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update triton.py\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Add printout and requirements check\n\n* Cleanup\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* triton fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fixed triton model query over grpc\n\n* Update check_requirements('tritonclient[all]')\n\n* group imports\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix likely remote URL bug\n\n* update comment\n\n* Update is_url()\n\n* Fix 2x download attempt on http://path/to/model.pt\n\nSigned-off-by: Glenn Jocher <glenn.jocher@ultralytics.com>\nCo-authored-by: glennjocher <glenn.jocher@ultralytics.com>\nCo-authored-by: Gaz Iqbal <giqbal@octoml.ai>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def run(",
            "seen, windows, dt = 0, [], (Profile(), Profile(), Profile())",
            "for path, im, im0s, vid_cap, s in dataset:",
            "with dt[0]:",
            "-            im = torch.from_numpy(im).to(device)",
            "+            im = torch.from_numpy(im).to(model.device)",
            "im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32",
            "im /= 255  # 0 - 255 to 0.0 - 1.0",
            "if len(im.shape) == 3:"
        ]
    },
    {
        "number": 3493,
        "comments": "",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Scheduler:",
            "\"\"\"",
            "Returns the state of the scheduler as a ``dict``.",
            "\"\"\"",
            "-        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}",
            "+        return {key: value for key, value in self.__dict__.items() if key != \"optimizer\"}",
            "",
            "def load_state_dict(self, state_dict: Dict[str, Any]) -> None:",
            "\"\"\""
        ]
    },
    {
        "number": 3496,
        "comments": "",
        "commit_message": "fixed trpo\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PGModel(Model):",
            "self.dist = self.policy.get_distribution()",
            "",
            "self.baseline_value_function = LinearValueFunction()",
            "-        self.saver = tf.train.Saver()",
            "+       # self.saver = tf.train.Saver()",
            "",
            "def get_action(self, state, episode=1):",
            "\"\"\""
        ]
    },
    {
        "number": 3499,
        "comments": "",
        "commit_message": "breaking changes in independent act mode, other fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class StatefulLayer(TemporalLayer):",
            "",
            "# def tf_apply(self, x, **internals):",
            "",
            "-    #     # optimization = tf.math.logical_not(x=Module.retrieve_tensor(name='optimization'))",
            "+    #     # optimization = tf.math.logical_not(x=Module.retrieve_tensor(name='???'))",
            "",
            "#     # def true_fn():",
            "#     batch_size = tf.shape("
        ]
    },
    {
        "number": 3500,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestDotProductSimilarityFunction(AllenNlpTestCase):",
            "a_vectors = numpy.random.rand(5, 4, 3, 6, 7)",
            "b_vectors = numpy.random.rand(5, 4, 3, 6, 7)",
            "desired_result = numpy.sum(a_vectors * b_vectors, axis=-1)",
            "-        result = dot_product(Variable(torch.from_numpy(a_vectors)),",
            "-                             Variable(torch.from_numpy(b_vectors))).data.numpy()",
            "+        result = dot_product(torch.from_numpy(a_vectors),",
            "+                             torch.from_numpy(b_vectors)).data.numpy()",
            "assert result.shape == (5, 4, 3, 6)",
            "# We're cutting this down here with a random partial index, so that if this test fails the",
            "# output isn't so huge and slow."
        ]
    },
    {
        "number": 3505,
        "comments": "",
        "commit_message": "[RLlib] SAC add discrete action support. (#7320)\n\n* Exploration API (+EpsilonGreedy sub-class).\n\n* Exploration API (+EpsilonGreedy sub-class).\n\n* Cleanup/LINT.\n\n* Add `deterministic` to generic Trainer config (NOTE: this is still ignored by most Agents).\n\n* Add `error` option to deprecation_warning().\n\n* WIP.\n\n* Bug fix: Get exploration-info for tf framework.\nBug fix: Properly deprecate some DQN config keys.\n\n* WIP.\n\n* LINT.\n\n* WIP.\n\n* Split PerWorkerEpsilonGreedy out of EpsilonGreedy.\nDocstrings.\n\n* Fix bug in sampler.py in case Policy has self.exploration = None\n\n* Update rllib/agents/dqn/dqn.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* Update rllib/agents/trainer.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* Change requests.\n\n* LINT\n\n* In tune/utils/util.py::deep_update() Only keep deep_updat'ing if both original and value are dicts. If value is not a dict, set\n\n* Completely obsolete syn_replay_optimizer.py's parameters schedule_max_timesteps AND beta_annealing_fraction (replaced with prioritized_replay_beta_annealing_timesteps).\n\n* Update rllib/evaluation/worker_set.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Review fixes.\n\n* Fix default value for DQN's exploration spec.\n\n* LINT\n\n* Fix recursion bug (wrong parent c'tor).\n\n* Do not pass timestep to get_exploration_info.\n\n* Update tf_policy.py\n\n* Fix some remaining issues with test cases and remove more deprecated DQN/APEX exploration configs.\n\n* Bug fix tf-action-dist\n\n* DDPG incompatibility bug fix with new DQN exploration handling (which is imported by DDPG).\n\n* Switch off exploration when getting action probs from off-policy-estimator's policy.\n\n* LINT\n\n* Fix test_checkpoint_restore.py.\n\n* Deprecate all SAC exploration (unused) configs.\n\n* Properly use `model.last_output()` everywhere. Instead of `model._last_output`.\n\n* WIP.\n\n* Take out set_epsilon from multi-agent-env test (not needed, decays anyway).\n\n* WIP.\n\n* Trigger re-test (flaky checkpoint-restore test).\n\n* WIP.\n\n* WIP.\n\n* Add test case for deterministic action sampling in PPO.\n\n* bug fix.\n\n* Added deterministic test cases for different Agents.\n\n* Fix problem with TupleActions in dynamic-tf-policy.\n\n* Separate supported_spaces tests so they can be run separately for easier debugging.\n\n* LINT.\n\n* Fix autoregressive_action_dist.py test case.\n\n* Re-test.\n\n* Fix.\n\n* Remove duplicate py_test rule from bazel.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* SAC fix.\n\n* SAC fix.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* FIX 2 examples tests.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* Renamed test file.\n\n* WIP.\n\n* Add unittest.main.\n\n* Make action_dist_class mandatory.\n\n* fix\n\n* FIX.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix explorations test case (contextlib cannot find its own nullcontext??).\n\n* Force torch to be installed for QMIX.\n\n* LINT.\n\n* Fix determine_tests_to_run.py.\n\n* Fix determine_tests_to_run.py.\n\n* WIP\n\n* Add Random exploration component to tests (fixed issue with \"static-graph randomness\" via py_function).\n\n* Add Random exploration component to tests (fixed issue with \"static-graph randomness\" via py_function).\n\n* Rename some stuff.\n\n* Rename some stuff.\n\n* WIP.\n\n* update.\n\n* WIP.\n\n* Gumbel Softmax Dist.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP\n\n* WIP.\n\n* WIP.\n\n* Hypertune.\n\n* Hypertune.\n\n* Hypertune.\n\n* Lock-in.\n\n* Cleanup.\n\n* LINT.\n\n* Fix.\n\n* Update rllib/policy/eager_tf_policy.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Update rllib/agents/sac/sac_policy.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Update rllib/agents/sac/sac_policy.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Update rllib/models/tf/tf_action_dist.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Update rllib/models/tf/tf_action_dist.py\n\nCo-Authored-By: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n\n* Fix items from review comments.\n\n* Add dm_tree to RLlib dependencies.\n\n* Add dm_tree to RLlib dependencies.\n\n* Fix DQN test cases ((Torch)Categorical).\n\n* Fix wrong pip install.\n\nCo-authored-by: Eric Liang <ekhliang@gmail.com>\nCo-authored-by: Kristian Hartikainen <kristian.hartikainen@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFPolicy(Policy):",
            "",
            "# build output signatures",
            "output_signature = self._extra_output_signature_def()",
            "-        for i, a in enumerate(tf.nest.flatten(self._sampled_action)):",
            "+        for i, a in enumerate(tree.flatten(self._sampled_action)):",
            "output_signature[\"actions_{}\".format(i)] = \\",
            "tf.saved_model.utils.build_tensor_info(a)"
        ]
    },
    {
        "number": 3506,
        "comments": "",
        "commit_message": "minor mwt fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DictTrainer(BaseTrainer):",
            "",
            "def load(self, filename):",
            "try:",
            "-            checkpoint = torch.load(filename)",
            "+            checkpoint = torch.load(filename, lambda storage, loc: storage)",
            "except BaseException:",
            "print(\"Cannot load model from {}\".format(filename))",
            "exit()"
        ]
    },
    {
        "number": 3507,
        "comments": "",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MultiStep(MetaOptimizer):",
            "deltas = [delta1 + delta2 for delta1, delta2 in zip(deltas, step_deltas)]",
            "return deltas",
            "",
            "-            deltas = tf.while_loop(",
            "+            deltas = self.while_loop(",
            "cond=util.tf_always_true, body=body, loop_vars=(deltas,),",
            "maximum_iterations=(self.num_steps - 1)",
            ")"
        ]
    },
    {
        "number": 3508,
        "comments": "",
        "commit_message": "Fix minor errors in european_option pricing module for heston models.\n\nPiperOrigin-RevId: 406103349\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def european_option_price(",
            "0.0, dtype=dtype, name='discount_rates')",
            "",
            "if dividend_rates is None:",
            "-      dividend_rates = tf.convert_to_tensor(",
            "-          0.0, dtype=dtype, name='dividend_rates')",
            "+      dividend_rates = 0.0",
            "+    dividend_rates = tf.convert_to_tensor(",
            "+        dividend_rates, dtype=dtype, name='dividend_rates')",
            "",
            "if discount_factors is None:",
            "discount_factors = tf.exp(-discount_rates * expiries)  # pylint: disable=invalid-unary-operand-type"
        ]
    },
    {
        "number": 3509,
        "comments": "",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ElmoTokenEmbedder(TokenEmbedder):",
            "The ELMo representations for the input sequence, shape",
            "``(batch_size, timesteps, embedding_dim)``",
            "\"\"\"",
            "-        elmo_output = self._elmo(inputs, word_inputs)",
            "+        elmo_output = self._elmo(tokens, word_inputs)",
            "elmo_representations = elmo_output[\"elmo_representations\"][0]",
            "if self._projection:",
            "projection = self._projection"
        ]
    },
    {
        "number": 3511,
        "comments": "",
        "commit_message": "Develop (#171)\n\n* merge (#128)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* rm print\n\n* Issue#37 and Issue #79 Save keras model/autokeras model (#122)\n\n* Issue #37 Export Keras model\n\n* Issue #79 Save autokeras model\n\n* Issue #37 and Issue#79 Fixed comments\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79 Fixed pytests\n\n* Issue #37 and Issue #79\n\n* quick fix test\n\n* Progbar (#143)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* contribute guide\n\n* Add Progress Bar\n\n* Update utils.py\n\n* Update search.py\n\n* update constant (#145)\n\n* [WIP] Issue #158 Imageregressor (#159)\n\n* Develop (#146)\n\n* merge (#128)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* rm print\n\n* Issue#37 and Issue #79 Save keras model/autokeras model (#122)\n\n* Issue #37 Export Keras model\n\n* Issue #79 Save autokeras model\n\n* Issue #37 and Issue#79 Fixed comments\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79\n\n* Issue #37 and Issue #79 Fixed pytests\n\n* Issue #37 and Issue #79\n\n* quick fix test\n\n* Progbar (#143)\n\n* 0.2.6 (#126)\n\n* 0.2.5 setup.py (#111)\n\n* [WIP] Attempts to Fix Memory Error  (#112)\n\n* Add Website Badge in README.md, apply timeout in search function in search.py\n\n* Add timeout in maximize_acq function in search.py\n\n* Update unit test to allow timeout to raise TimeoutError\n\n* Add unit test for timeout resume\n\n* Remove TimeoutError from expectation\n\n* Check Timeout exception in search() in search.py\n\n* 0.2.5 setup.py (#110)\n\n* Prevent gpu memory copy to main process after train() finished\n\n* Cast loss from tensor to float\n\n* Add pass() in MockProcess\n\n* [MRG] Search Space limited to avoid out of memory (#121)\n\n* limited the search space\n\n* limited the search space\n\n* reduce search space\n\n* test added\n\n* [MRG]Pytorch mp (#124)\n\n* Change multiprcoessing to torch.multiprocessing\n\n* Replace multiprocessing.Pool with torch.multiprocessing.Pool in tests\n\n* 0.2.6 (#125)\n\n* new release\n\n* auto deploy\n\n* auto deploy of docs\n\n* fix the docs auto deploy\n\n* Create CNAME\n\n* deploy docs fixed\n\n* update\n\n* bug fix (#127)\n\n* setup.py\n\n* contribute guide\n\n* Add Progress Bar\n\n* Update utils.py\n\n* Update search.py\n\n* update constant (#145)\n\n* Update setup.py (#147)\n\n* Update setup.py\n\n* Update setup.py\n\n* Update setup.py (#155)\n\n* requirements\n\n* Issue #158 Export ImageRegressor model\n\n* Memory (#161)\n\n* aa\n\n* limit memory\n\n* refactor to_real_layer to member functions\n\n* bug fix (#166)\n\n* doc string changed for augment (#170)\n\nI added proper documentation for class ImageSupervised  arg 'augment'. It is 'None' by default. However, if it is 'None', then it uses Constant.DATA_AUGMENTATION which is 'True'. This is misleading when trying things out.\n\n* Update constant.py\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TorchModel(torch.nn.Module):",
            "self.graph = graph",
            "self.layers = []",
            "for layer in graph.layer_list:",
            "-            self.layers.append(to_real_layer(layer))",
            "+            self.layers.append(layer.to_real_layer())",
            "if graph.weighted:",
            "for index, layer in enumerate(self.layers):",
            "set_stub_weight_to_torch(self.graph.layer_list[index], layer)"
        ]
    },
    {
        "number": 3512,
        "comments": "",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def psnr(input: torch.Tensor, target: torch.Tensor, max_val: float) -> torch.Ten",
            "if input.shape != target.shape:",
            "raise TypeError(f\"Expected tensors of equal shapes, but got {input.shape} and {target.shape}\")",
            "",
            "-    return 10. * torch.log10(max_val ** 2 / mse(input, target, reduction='mean'))",
            "+    return 10. * torch.log10(max_val**2 / mse(input, target, reduction='mean'))",
            "",
            "",
            "def psnr_loss(input: torch.Tensor, target: torch.Tensor, max_val: float) -> torch.Tensor:"
        ]
    },
    {
        "number": 3513,
        "comments": "",
        "commit_message": "Fix opt softmax small nit (#19243)\n\n* fix opt softmax nit\n\n- Use the same logic as 1eb09537550734a783c194e416029cb9bc4cb119 for consistency\n\n* Update src/transformers/models/opt/modeling_opt.py\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class OPTAttention(nn.Module):",
            "attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask",
            "attn_weights = torch.max(attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min))",
            "attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)",
            "-            dtype_attn_weights = attn_weights.dtype",
            "",
            "# upcast to fp32 if the weights are in fp16. Please see https://github.com/huggingface/transformers/pull/17437",
            "-        if dtype_attn_weights == torch.float16:",
            "-            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(dtype_attn_weights)",
            "+        if attn_weights.dtype == torch.float16:",
            "+            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(torch.float16)",
            "else:",
            "attn_weights = nn.functional.softmax(attn_weights, dim=-1)"
        ]
    },
    {
        "number": 3514,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SpanBasedF1Measure(Metric):",
            "possible roles associated with it).",
            "\"\"\"",
            "if mask is None:",
            "-            mask = torch.ones_like(gold_labels)",
            "+            mask = torch.ones_like(gold_labels).bool()",
            "",
            "predictions, gold_labels, mask, prediction_map = self.detach_tensors(",
            "predictions, gold_labels, mask, prediction_map"
        ]
    },
    {
        "number": 3519,
        "comments": "",
        "commit_message": "JAX: Fix RNNScratch b_h param\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RNNScratch(nn.Module):",
            "(self.num_inputs, self.num_hiddens))",
            "self.W_hh = self.param('W_hh', nn.initializers.normal(self.sigma),",
            "(self.num_hiddens, self.num_hiddens))",
            "-        self.b_h = self.param('b_h', nn.initializers.zeros, (num_hiddens))",
            "+        self.b_h = self.param('b_h', nn.initializers.zeros, (self.num_hiddens))",
            "",
            "def __call__(self, inputs, state=None):",
            "\"\"\"Defined in :numref:`sec_rnn-scratch`\"\"\""
        ]
    },
    {
        "number": 3521,
        "comments": "",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "eigvals.support_native_out = False",
            "",
            "",
            "def adjoint(",
            "-        x: torch.Tensor,",
            "-        /,",
            "-        *,",
            "-        out: Optional[torch.Tensor] = None,",
            "+    x: torch.Tensor,",
            "+    /,",
            "+    *,",
            "+    out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "_check_valid_dimension_size(x)",
            "return torch.adjoint(x).resolve_conj()"
        ]
    },
    {
        "number": 3523,
        "comments": "",
        "commit_message": "Fixed vsplit in the experimental API (#10210)\n\nRemoved out argument as the function returns a list of arrays\n",
        "label": "",
        "answer": "no",
        "change": [
            "def vsplit(",
            "ary: torch.Tensor,",
            "indices_or_sections: Union[int, Tuple[int]],",
            "/,",
            "-    *,",
            "-    out: Optional[torch.Tensor] = None,",
            "-) -> torch.Tensor:",
            "+) -> List[torch.Tensor]:",
            "return torch.vsplit(ary, indices_or_sections)"
        ]
    },
    {
        "number": 3525,
        "comments": "",
        "commit_message": "Fix the is_tensor check in Keras remove_squeezable_dimensions.\n\ntf.is_tensor only returns true for Tensor and native TF CompositeTensors, like RaggedTensor and SparseTensor. For user custom CompositeTensors, it returns false. Change the if condition to check both native TF types and extension types.\n\nPiperOrigin-RevId: 427492662\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def remove_squeezable_dimensions(",
            "Tuple of `labels` and `predictions`, possibly with last dim squeezed.",
            "\"\"\"",
            "with backend.name_scope(name or 'remove_squeezable_dimensions'):",
            "-    # tf.is_tensor returns True if predictions is a tensor or composite tensor.",
            "-    if not tf.is_tensor(predictions):",
            "+    if not tf_utils.is_tensor_or_extension_type(predictions):",
            "predictions = tf.convert_to_tensor(predictions)",
            "-    if not tf.is_tensor(labels):",
            "+    if not tf_utils.is_tensor_or_extension_type(labels):",
            "labels = tf.convert_to_tensor(labels)",
            "predictions_shape = predictions.shape",
            "predictions_rank = predictions_shape.ndims"
        ]
    },
    {
        "number": 3531,
        "comments": "",
        "commit_message": "[Pytorch] pytorch only timesteps (#724)\n\n* pytorch timesteps\n\n* style\n\n* get rid of if-else\n\n* fix test\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ScoreSdeVpScheduler(SchedulerMixin, ConfigMixin):",
            "self.discrete_sigmas = None",
            "self.timesteps = None",
            "",
            "-    def set_timesteps(self, num_inference_steps):",
            "-        self.timesteps = torch.linspace(1, self.config.sampling_eps, num_inference_steps)",
            "+    def set_timesteps(self, num_inference_steps, device: Union[str, torch.device] = None):",
            "+        self.timesteps = torch.linspace(1, self.config.sampling_eps, num_inference_steps, device=device)",
            "",
            "def step_pred(self, score, x, t, generator=None):",
            "if self.timesteps is None:"
        ]
    },
    {
        "number": 3533,
        "comments": "",
        "commit_message": "Support fp32 gradaccum for bf16 model (#2566)\n\n* allow bf16 model with fp32 gradient accumulation datatype\n\n* allow fp32 gradient accumulation and bfloat16 model in amp mode\n\n* alternative fix for grad accumulation type mismatch.  In the case of zero optimizer we should have grad accum type == model data type\n\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DeepSpeedEngine(Module):",
            "model_dtype = torch.bfloat16",
            "",
            "if self._config.grad_accum_dtype == None:",
            "-            if model_dtype == torch.bfloat16:",
            "+            if model_dtype == torch.bfloat16 and not self.zero_optimization():",
            "grad_accum_dtype = torch.float32",
            "else:",
            "grad_accum_dtype = model_dtype"
        ]
    },
    {
        "number": 3535,
        "comments": "",
        "commit_message": "docs: minor refactoring and docs for MorphoTagger\n\n* docs: add docs for MorphoTagger, minor refactoring\n\n* docs: morpho_tagger docstrings fixed\n\n* docs: fix morpho_tagger docstrings\n\n* docs: update morpho_tagger docstrings\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def csoftmax_for_slice(input):",
            "p = q_list[1] * (1.0 - mass_) / tf.reduce_sum(q_list[1])",
            "p_new = tf.dynamic_stitch(condition_indices, [q_list[0], p])",
            "",
            "-        # verification of the condition and modification of masks",
            "-        less_mask = tf.cast(tf.less(u, p_new), tf.int32)  # 0 when u bigger than p, 1 when u less than p",
            "+        # condition verification and mask modification",
            "+        less_mask = tf.cast(tf.less(u, p_new), tf.int32)  # 0 when u is bigger than p, 1 when u is less than p",
            "condition_indices = tf.dynamic_partition(tf.range(tf.shape(p_new)[0]), less_mask,",
            "-                                                 2)  # 0 when u bigger",
            "-        #  than p, 1 when u less than p",
            "+                                                 2)  # 0 when u is bigger than p, 1 when u is less than p",
            "",
            "split_p_new = tf.dynamic_partition(p_new, less_mask, 2)",
            "split_u = tf.dynamic_partition(u, less_mask, 2)"
        ]
    },
    {
        "number": 3536,
        "comments": "",
        "commit_message": "Fix pylint errors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ParallelDataProvider(data_provider.DataProvider):",
            "",
            "# Optionally shuffle the data",
            "if shuffle:",
            "-      shuffle_queue = data_flow_ops.RandomShuffleQueue(",
            "+      shuffle_queue = tf.RandomShuffleQueue(",
            "capacity=common_queue_capacity,",
            "min_after_dequeue=common_queue_min,",
            "dtypes=[tf.string, tf.string],",
            "seed=seed)",
            "enqueue_ops = []",
            "enqueue_ops.append(shuffle_queue.enqueue([data_source, data_target]))",
            "-      queue_runner.add_queue_runner(",
            "-          queue_runner.QueueRunner(shuffle_queue, enqueue_ops))",
            "+      tf.train.add_queue_runner(",
            "+          tf.train.QueueRunner(shuffle_queue, enqueue_ops))",
            "data_source, data_target = shuffle_queue.dequeue()",
            "",
            "# Decode source items"
        ]
    },
    {
        "number": 3537,
        "comments": "",
        "commit_message": "fix initializer mode\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "defs, block_func = cfg[DEPTH]",
            "",
            "with argscope(Conv2D, nl=tf.identity, use_bias=False,",
            "-                      W_init=tf.variance_scaling_initializer(scale=2.0, mode='FAN_OUT')), \\",
            "+                      W_init=tf.variance_scaling_initializer(scale=2.0, mode='fan_out')), \\",
            "argscope([Conv2D, MaxPooling, GlobalAvgPooling, BatchNorm], data_format='NCHW'):",
            "convmaps = (LinearWrap(image)",
            ".Conv2D('conv0', 64, 7, stride=2, nl=BNReLU)"
        ]
    },
    {
        "number": 3540,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ProductionRuleField(Field[ProductionRuleArray]):  # type: ignore",
            "@overrides",
            "def as_tensor(self,",
            "padding_lengths: Dict[str, int],",
            "-                  cuda_device: int = -1,",
            "-                  for_training: bool = True) -> ProductionRuleArray:",
            "+                  cuda_device: int = -1) -> ProductionRuleArray:",
            "# pylint: disable=unused-argument",
            "if self.is_global_rule:",
            "-            tensor = Variable(torch.LongTensor([self._rule_id]), volatile=not for_training)",
            "+            tensor = torch.LongTensor([self._rule_id])",
            "else:",
            "tensor = None",
            "return (self.rule, self.is_global_rule, tensor)"
        ]
    },
    {
        "number": 3542,
        "comments": "",
        "commit_message": "Add pyre typeshed information for Tensor.ndim and nn.ConvTranspose2d\n\nSummary: Adding some appropriate methods into pyre typeshed. Removing corresponding pyre-ignore and pyre-fixme messages.\n\nDifferential Revision: D22949138\n\nfbshipit-source-id: add8acdd4611ab698954868832594d062cd58f88\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TexturesVertex(TexturesBase):",
            "\"\"\"",
            "if isinstance(verts_features, (tuple, list)):",
            "correct_shape = all(",
            "-                # pyre-fixme[16]: `Tensor` has no attribute `ndim`.",
            "-                (torch.is_tensor(v) and v.ndim == 2)",
            "-                for v in verts_features",
            "+                (torch.is_tensor(v) and v.ndim == 2) for v in verts_features",
            ")",
            "if not correct_shape:",
            "raise ValueError("
        ]
    },
    {
        "number": 3544,
        "comments": "",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "if OUT_CH == 1:",
            "output = tf.image.grayscale_to_rgb(output)",
            "fake_output = tf.image.grayscale_to_rgb(fake_output)",
            "-        viz = (tf.concat(2, [input, output, fake_output]) + 1.0) * 128.0",
            "+        viz = (tf.concat_v2([input, output, fake_output], 2) + 1.0) * 128.0",
            "viz = tf.cast(tf.clip_by_value(viz, 0, 255), tf.uint8, name='viz')",
            "tf.summary.image('input,output,fake', viz, max_outputs=max(30, BATCH))"
        ]
    },
    {
        "number": 3546,
        "comments": "",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DownloadManager(object):",
            "\"\"\"",
            "Ship the files using Beam FileSystems to the pipeline temp dir.",
            "\"\"\"",
            "-        from nlp.utils.beam_utils import upload_local_to_remote",
            "+        from datasets.utils.beam_utils import upload_local_to_remote",
            "",
            "remote_dir = pipeline._options.get_all_options().get(\"temp_location\")",
            "if remote_dir is None:"
        ]
    },
    {
        "number": 3547,
        "comments": "",
        "commit_message": "`LightGCN`: Initialize embeddings via `xavier_uniform` (#4083)\n\n* initalized embedding with xavier uniform\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* move to reset_parameters()\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LightGCN(torch.nn.Module):",
            "self.reset_parameters()",
            "",
            "def reset_parameters(self):",
            "-        self.embedding.reset_parameters()",
            "+        torch.nn.init.xavier_uniform_(self.embedding.weight)",
            "for conv in self.convs:",
            "conv.reset_parameters()"
        ]
    },
    {
        "number": 3549,
        "comments": "",
        "commit_message": "[RLlib] Policy.compute_log_likelihoods() and SAC refactor. (issue #7107) (#7124)\n\n* Exploration API (+EpsilonGreedy sub-class).\n\n* Exploration API (+EpsilonGreedy sub-class).\n\n* Cleanup/LINT.\n\n* Add `deterministic` to generic Trainer config (NOTE: this is still ignored by most Agents).\n\n* Add `error` option to deprecation_warning().\n\n* WIP.\n\n* Bug fix: Get exploration-info for tf framework.\nBug fix: Properly deprecate some DQN config keys.\n\n* WIP.\n\n* LINT.\n\n* WIP.\n\n* Split PerWorkerEpsilonGreedy out of EpsilonGreedy.\nDocstrings.\n\n* Fix bug in sampler.py in case Policy has self.exploration = None\n\n* Update rllib/agents/dqn/dqn.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* Update rllib/agents/trainer.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* Change requests.\n\n* LINT\n\n* In tune/utils/util.py::deep_update() Only keep deep_updat'ing if both original and value are dicts. If value is not a dict, set\n\n* Completely obsolete syn_replay_optimizer.py's parameters schedule_max_timesteps AND beta_annealing_fraction (replaced with prioritized_replay_beta_annealing_timesteps).\n\n* Update rllib/evaluation/worker_set.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Review fixes.\n\n* Fix default value for DQN's exploration spec.\n\n* LINT\n\n* Fix recursion bug (wrong parent c'tor).\n\n* Do not pass timestep to get_exploration_info.\n\n* Update tf_policy.py\n\n* Fix some remaining issues with test cases and remove more deprecated DQN/APEX exploration configs.\n\n* Bug fix tf-action-dist\n\n* DDPG incompatibility bug fix with new DQN exploration handling (which is imported by DDPG).\n\n* Switch off exploration when getting action probs from off-policy-estimator's policy.\n\n* LINT\n\n* Fix test_checkpoint_restore.py.\n\n* Deprecate all SAC exploration (unused) configs.\n\n* Properly use `model.last_output()` everywhere. Instead of `model._last_output`.\n\n* WIP.\n\n* Take out set_epsilon from multi-agent-env test (not needed, decays anyway).\n\n* WIP.\n\n* Trigger re-test (flaky checkpoint-restore test).\n\n* WIP.\n\n* WIP.\n\n* Add test case for deterministic action sampling in PPO.\n\n* bug fix.\n\n* Added deterministic test cases for different Agents.\n\n* Fix problem with TupleActions in dynamic-tf-policy.\n\n* Separate supported_spaces tests so they can be run separately for easier debugging.\n\n* LINT.\n\n* Fix autoregressive_action_dist.py test case.\n\n* Re-test.\n\n* Fix.\n\n* Remove duplicate py_test rule from bazel.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* SAC fix.\n\n* SAC fix.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* FIX 2 examples tests.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* Renamed test file.\n\n* WIP.\n\n* Add unittest.main.\n\n* Make action_dist_class mandatory.\n\n* fix\n\n* FIX.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix explorations test case (contextlib cannot find its own nullcontext??).\n\n* Force torch to be installed for QMIX.\n\n* LINT.\n\n* Fix determine_tests_to_run.py.\n\n* Fix determine_tests_to_run.py.\n\n* WIP\n\n* Add Random exploration component to tests (fixed issue with \"static-graph randomness\" via py_function).\n\n* Add Random exploration component to tests (fixed issue with \"static-graph randomness\" via py_function).\n\n* Rename some stuff.\n\n* Rename some stuff.\n\n* WIP.\n\n* WIP.\n\n* Fix SAC.\n\n* Fix SAC.\n\n* Fix strange tf-error in ray core tests.\n\n* Fix strange ray-core tf-error in test_memory_scheduling test case.\n\n* Fix test_io.py.\n\n* LINT.\n\n* Update SAC yaml files' config.\n\nCo-authored-by: Eric Liang <ekhliang@gmail.com>\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SACModel(TFModelV2):",
            "shift_and_log_scale_diag = tf.keras.Sequential([",
            "tf.keras.layers.Dense(",
            "units=hidden,",
            "-                activation=getattr(tf.nn, actor_hidden_activation),",
            "+                activation=getattr(tf.nn, actor_hidden_activation, None),",
            "name=\"action_hidden_{}\".format(i))",
            "for i, hidden in enumerate(actor_hiddens)",
            "] + ["
        ]
    },
    {
        "number": 3555,
        "comments": "",
        "commit_message": "Merge `develop` branch into `master` (#3518)\n\n* update ci-testing.yml (#3322)\n\n* update ci-testing.yml\n\n* update greetings.yml\n\n* bring back os matrix\n\n* update ci-testing.yml (#3322)\n\n* update ci-testing.yml\n\n* update greetings.yml\n\n* bring back os matrix\n\n* Enable direct `--weights URL` definition (#3373)\n\n* Enable direct `--weights URL` definition\n\n@KalenMike this PR will enable direct --weights URL definition. Example use case:\n```\npython train.py --weights https://storage.googleapis.com/bucket/dir/model.pt\n```\n\n* cleanup\n\n* bug fixes\n\n* weights = attempt_download(weights)\n\n* Update experimental.py\n\n* Update hubconf.py\n\n* return bug fix\n\n* comment mirror\n\n* min_bytes\n\n* Update tutorial.ipynb (#3368)\n\nadd Open in Kaggle badge\n\n* `cv2.imread(img, -1)` for IMREAD_UNCHANGED (#3379)\n\n* Update datasets.py\n\n* comment\n\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* COCO evolution fix (#3388)\n\n* COCO evolution fix\n\n* cleanup\n\n* update print\n\n* print fix\n\n* Create `is_pip()` function (#3391)\n\nReturns `True` if file is part of pip package. Useful for contextual behavior modification.\n\n```python\ndef is_pip():\n    # Is file in a pip package?\n    return 'site-packages' in Path(__file__).absolute().parts\n```\n\n* Revert \"`cv2.imread(img, -1)` for IMREAD_UNCHANGED (#3379)\" (#3395)\n\nThis reverts commit 21a9607e00f1365b21d8c4bd81bdbf5fc0efea24.\n\n* Update FLOPs description (#3422)\n\n* Update README.md\n\n* Changing FLOPS to FLOPs.\n\nCo-authored-by: BuildTools <unconfigured@null.spigotmc.org>\n\n* Parse URL authentication (#3424)\n\n* Parse URL authentication\n\n* urllib.parse.unquote()\n\n* improved error handling\n\n* improved error handling\n\n* remove %3F\n\n* update check_file()\n\n* Add FLOPs title to table (#3453)\n\n* Suppress jit trace warning + graph once (#3454)\n\n* Suppress jit trace warning + graph once\n\nSuppress harmless jit trace warning on TensorBoard add_graph call. Also fix multiple add_graph() calls bug, now only on batch 0.\n\n* Update train.py\n\n* Update MixUp augmentation `alpha=beta=32.0` (#3455)\n\nPer VOC empirical results https://github.com/ultralytics/yolov5/issues/3380#issuecomment-853001307 by @developer0hye\n\n* Add `timeout()` class (#3460)\n\n* Add `timeout()` class\n\n* rearrange order\n\n* Faster HSV augmentation (#3462)\n\nremove datatype conversion process that can be skipped\n\n* Add `check_git_status()` 5 second timeout (#3464)\n\n* Add check_git_status() 5 second timeout\n\nThis should prevent the SSH Git bug that we were discussing @KalenMike\n\n* cleanup\n\n* replace timeout with check_output built-in timeout\n\n* Improved `check_requirements()` offline-handling (#3466)\n\nImprove robustness of `check_requirements()` function to offline environments (do not attempt pip installs when offline).\n\n* Add `output_names` argument for ONNX export with dynamic axes (#3456)\n\n* Add output names & dynamic axes for onnx export\n\nAdd output_names and dynamic_axes names for all outputs in torch.onnx.export. The first four outputs of the model will have names output0, output1, output2, output3\n\n* use first output only + cleanup\n\nCo-authored-by: Samridha Shrestha <samridha.shrestha@g42.ai>\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Revert FP16 `test.py` and `detect.py` inference to FP32 default (#3423)\n\n* fixed inference bug ,while use half precision\n\n* replace --use-half with --half\n\n* replace space and PEP8 in detect.py\n\n* PEP8 detect.py\n\n* update --half help comment\n\n* Update test.py\n\n* revert space\n\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Add additional links/resources to stale.yml message (#3467)\n\n* Update stale.yml\n\n* cleanup\n\n* Update stale.yml\n\n* reformat\n\n* Update stale.yml HUB URL (#3468)\n\n* Stale `github.actor` bug fix (#3483)\n\n* Explicit `model.eval()` call `if opt.train=False` (#3475)\n\n* call model.eval() when opt.train is False\n\ncall model.eval() when opt.train is False\n\n* single-line if statement\n\n* cleanup\n\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* check_requirements() exclude `opencv-python` (#3495)\n\nFix for 3rd party or contrib versions of installed OpenCV as in https://github.com/ultralytics/yolov5/issues/3494.\n\n* Earlier `assert` for cpu and half option (#3508)\n\n* early assert for cpu and half option\n\nearly assert for cpu and half option\n\n* Modified comment\n\nModified comment\n\n* Update tutorial.ipynb (#3510)\n\n* Reduce test.py results spacing (#3511)\n\n* Update README.md (#3512)\n\n* Update README.md\n\nMinor modifications\n\n* 850 width\n\n* Update greetings.yml\n\nrevert greeting change as PRs will now merge to master.\n\nCo-authored-by: Piotr Skalski <SkalskiP@users.noreply.github.com>\nCo-authored-by: SkalskiP <piotr.skalski92@gmail.com>\nCo-authored-by: Peretz Cohen <pizzaz93@users.noreply.github.com>\nCo-authored-by: tudoulei <34886368+tudoulei@users.noreply.github.com>\nCo-authored-by: chocosaj <chocosaj@users.noreply.github.com>\nCo-authored-by: BuildTools <unconfigured@null.spigotmc.org>\nCo-authored-by: Yonghye Kwon <developer.0hye@gmail.com>\nCo-authored-by: Sam_S <SamSamhuns@users.noreply.github.com>\nCo-authored-by: Samridha Shrestha <samridha.shrestha@g42.ai>\nCo-authored-by: edificewang <609552430@qq.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def _create(name, pretrained=True, channels=3, classes=80, autoshape=True, verbo",
            "cfg = list((Path(__file__).parent / 'models').rglob(f'{name}.yaml'))[0]  # model.yaml path",
            "model = Model(cfg, channels, classes)  # create model",
            "if pretrained:",
            "-                attempt_download(fname)  # download if not found locally",
            "-                ckpt = torch.load(fname, map_location=torch.device('cpu'))  # load",
            "+                ckpt = torch.load(attempt_download(fname), map_location=torch.device('cpu'))  # load",
            "msd = model.state_dict()  # model state_dict",
            "csd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32",
            "csd = {k: v for k, v in csd.items() if msd[k].shape == v.shape}  # filter"
        ]
    },
    {
        "number": 3556,
        "comments": "",
        "commit_message": "Revert \"[Ray Dataset] fix the type infer of pd.dataframe (when dtype is object)\" (#25809)\n\nThis reverts commit f61f60f70857888751c214d8ec280b85aeeb5932.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def convert_pandas_to_tf_tensor(",
            "# them. If the columns contain different types (for example, `float32`s",
            "# and `int32`s), then `tf.concat` raises an error.",
            "dtype: np.dtype = np.find_common_type(df.dtypes, [])",
            "-",
            "-            # if the columns are `ray.data.extensions.tensor_extension.TensorArray`,",
            "-            # the dtype will be `object`. In this case, we need to set the dtype to",
            "-            # none, and use the automatic type casting of `tf.convert_to_tensor`.",
            "-            if isinstance(dtype, object):",
            "-                dtype = None",
            "-",
            "except TypeError:",
            "# `find_common_type` fails if a series has `TensorDtype`. In this case,",
            "# don't cast any of the series and continue."
        ]
    },
    {
        "number": 3557,
        "comments": "",
        "commit_message": "Fixed MNIST dataset in integration test\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "\"train_kwargs = {\\n\",",
            "\"    \\\"batch_size\\\": args[\\\"batch_size\\\"],\\n\",",
            "\"}\\n\",",
            "-    \"train_data_ptr = remote_torchvision.datasets.MNIST('../data', train=True, download=True, transform=transforms)\\n\",",
            "+    \"train_data_ptr = remote_torchvision.datasets.MNIST(str(get_root_data_path()), train=True, download=True, transform=transforms)\\n\",",
            "\"train_loader_ptr = remote_torch.utils.data.DataLoader(train_data_ptr,**train_kwargs)\"",
            "]",
            "},"
        ]
    },
    {
        "number": 3558,
        "comments": "",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GuidedAnchorHead(AnchorHead):",
            "bbox_deltas = bbox_anchors.new_full(bbox_anchors.size(), 0)",
            "bbox_deltas[:, 2:] += shape_pred",
            "# filter out negative samples to speed-up weighted_bounded_iou_loss",
            "-        inds = torch.nonzero(anchor_weights[:, 0] > 0).squeeze(1)",
            "+        inds = torch.nonzero(",
            "+            anchor_weights[:, 0] > 0, as_tuple=False).squeeze(1)",
            "bbox_deltas_ = bbox_deltas[inds]",
            "bbox_anchors_ = bbox_anchors[inds]",
            "bbox_gts_ = bbox_gts[inds]"
        ]
    },
    {
        "number": 3559,
        "comments": "",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Model(ModelDesc):",
            "# For visualization in tensorboard",
            "padded1 = tf.pad(sampled1, [[0, 0], [HALF_DIFF, HALF_DIFF], [HALF_DIFF, HALF_DIFF], [0, 0]])",
            "padded2 = tf.pad(sampled2, [[0, 0], [HALF_DIFF, HALF_DIFF], [HALF_DIFF, HALF_DIFF], [0, 0]])",
            "-        img_orig = tf.concat(1, [image[:, :, :, 0], image[:, :, :, 1]])  # b x 2h  x w",
            "-        transform1 = tf.concat(1, [padded1[:, :, :, 0], padded1[:, :, :, 1]])",
            "-        transform2 = tf.concat(1, [padded2[:, :, :, 0], padded2[:, :, :, 1]])",
            "-        stacked = tf.concat(2, [img_orig, transform1, transform2], 'viz')",
            "+        img_orig = tf.concat_v2([image[:, :, :, 0], image[:, :, :, 1]], 1)  # b x 2h  x w",
            "+        transform1 = tf.concat_v2([padded1[:, :, :, 0], padded1[:, :, :, 1]], 1)",
            "+        transform2 = tf.concat_v2([padded2[:, :, :, 0], padded2[:, :, :, 1]], 1)",
            "+        stacked = tf.concat_v2([img_orig, transform1, transform2], 2, 'viz')",
            "tf.summary.image('visualize',",
            "tf.expand_dims(stacked, -1), max_images=30)",
            "",
            "-        sampled = tf.concat(3, [sampled1, sampled2], 'sampled_concat')",
            "+        sampled = tf.concat_v2([sampled1, sampled2], 3, 'sampled_concat')",
            "logits = (LinearWrap(sampled)",
            ".apply(symbf.batch_flatten)",
            ".FullyConnected('fc1', out_dim=256, nl=tf.nn.relu)"
        ]
    },
    {
        "number": 3560,
        "comments": "",
        "commit_message": "fix test for dtype\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LengthBonus(ScorerInterface):",
            "self.n = n_vocab",
            "",
            "def score(self, y, state, x):",
            "-        return torch.tensor([1.0], device=y.device).expand(self.n), None",
            "+        return torch.tensor([1.0], device=x.device, dtype=x.dtype).expand(self.n), None"
        ]
    },
    {
        "number": 3564,
        "comments": "",
        "commit_message": "fix linter and add docs\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _plot_and_save_attention(att_w, filename):",
            "",
            "",
            "def plot_multi_head_attention(data, attn_dict, outdir, suffix=\"png\"):",
            "+    \"\"\"Plot multi head attentions",
            "+",
            "+    :param dict data: utts info from json file",
            "+    :param dict[str, torch.Tensor] attn_dict: multi head attention dict.",
            "+        values should be torch.Tensor (head, input_length, output_length)",
            "+    :param str outdir: dir to save fig",
            "+    :param str suffix: filename suffix including image type (e.g., png)",
            "+    \"\"\"",
            "for name, att_ws in attn_dict.items():",
            "for idx, att_w in enumerate(att_ws):",
            "filename = \"%s/%s.%s.%s\" % ("
        ]
    },
    {
        "number": 3565,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AucTest(AllenNlpTestCase):",
            "",
            "predictions = torch.randn(8, device=device)",
            "labels = torch.randint(0, 2, (8,), dtype=torch.long, device=device)",
            "-        mask = torch.tensor([1, 1, 1, 1, 0, 0, 0, 0], dtype=torch.uint8, device=device)",
            "+        mask = torch.BoolTensor([True, True, True, True, False, False, False, False], device=device)",
            "",
            "auc(predictions, labels, mask)",
            "computed_auc_value = auc.get_metric(reset=True)"
        ]
    },
    {
        "number": 3567,
        "comments": "",
        "commit_message": "some fixes\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def BatchNorm(x, use_local_stat=None, decay=0.9, epsilon=1e-5):",
            "",
            "n_out = shape[-1]  # channel",
            "assert n_out is not None",
            "-    beta = tf.get_variable('beta', [n_out])",
            "+    beta = tf.get_variable('beta', [n_out],",
            "+            initializer=tf.zeros_initializer)",
            "gamma = tf.get_variable('gamma', [n_out],",
            "-        initializer=tf.ones_initializer)",
            "+            initializer=tf.ones_initializer)",
            "",
            "if len(shape) == 2:",
            "batch_mean, batch_var = tf.nn.moments(x, [0], keep_dims=False)"
        ]
    },
    {
        "number": 3568,
        "comments": "",
        "commit_message": "fixed a bug in the case of multi-gpu\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def recog(args):",
            "with open(args.recog_json, 'rb') as f:",
            "recog_json = json.load(f)['utts']",
            "",
            "+    if not torch_is_old:",
            "+        torch.set_grad_enabled(False)",
            "+",
            "new_json = {}",
            "for name in recog_json.keys():",
            "feat = kaldi_io_py.read_mat(recog_json[name]['input'][0]['feat'])"
        ]
    },
    {
        "number": 3569,
        "comments": "",
        "commit_message": "fix bug\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def densenet_block(incoming, nb_layers, growth, bottleneck=True,",
            "\"\"\"",
            "densenet = incoming",
            "",
            "-    for i in range(nb_layers):",
            "+    with tf.variable_scope(scope, default_name=name, values=[incoming],",
            "+                           reuse=reuse) as scope:",
            "",
            "-        with tf.variable_scope(scope, default_name=name, values=[incoming],",
            "-                               reuse=reuse) as scope:",
            "+        for i in range(nb_layers):",
            "",
            "# Identity",
            "conn = densenet"
        ]
    },
    {
        "number": 3570,
        "comments": "",
        "commit_message": "[RLlib] Curiosity minor fixes, do-overs, and testing. (#10143)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LSTMWrapper(RecurrentNetwork, nn.Module):",
            "wrapped_out,",
            "torch.reshape(input_dict[SampleBatch.PREV_ACTIONS].float(),",
            "[-1, self.action_dim]),",
            "-                    torch.reshape(input_dict[SampleBatch.PREV_REWARDS],",
            "+                    torch.reshape(input_dict[SampleBatch.PREV_REWARDS].float(),",
            "[-1, 1]),",
            "],",
            "dim=1)"
        ]
    },
    {
        "number": 3571,
        "comments": "",
        "commit_message": "Added tf.keras support (#513)\n\n* Added support for tf.keras\n\n* Added unit tests\n\n* Refactoring\n\n* Fixed tests\n\n* Hide implementation modules\n\n* Moved _DistributedOptimizer into the impl file and wrapped with function\n\n* Added cooperative multiple inheritance\n\n* Backwards compatability with TensorFlow versions less than 1.4.0\n\n* Removed duplicate headers\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class KerasTests(tf.test.TestCase):",
            "new_opt = new_model.optimizer",
            "os.remove(fname)",
            "",
            "-            self.assertEqual(type(new_opt).__module__, 'horovod.keras')",
            "+            self.assertEqual(type(new_opt).__module__, 'horovod.keras.impl')",
            "self.assertEqual(type(new_opt).__name__, 'TestOptimizer')",
            "self.assertEqual(K.get_value(opt.lr), K.get_value(new_opt.lr))",
            "self.assertEqual(len(opt.get_weights()), len(new_opt.get_weights()))"
        ]
    },
    {
        "number": 3572,
        "comments": "",
        "commit_message": "fix: minor fixes and refactoring NER (#539)\n\n* feat: add train from scratch\n\n* feat: add warning to train_from_scratch\n\n* fix: no cudnn rnn fixed mask\n\n* fix: session config added\n\n* fix: warning to log\n\n* fix: pass learning rate to default optimizer\n\n* fix: l2 in conv layer\n\n* fix: ner dataset reader\n\n* feat: add returning loss to the train on batch\n\n* doc: add model descriptions to the ner doc\n\n* chore: move ner metrics to metrics module\n\n* fix: remove train from scratch\n\n* fix: add dataset names\n\n* chore: log.info -> log.warning for warning\n\nCo-Authored-By: mu-arkhipov <arkhipov@yahoo.com>\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def stacked_cnn(units: tf.Tensor,",
            "padding='same',",
            "dilation_rate=dilation_rate,",
            "kernel_initializer=INITIALIZER(),",
            "-                                 kernel_regularizer=tf.nn.l2_loss)",
            "+                                 kernel_regularizer=l2_reg)",
            "if use_batch_norm:",
            "assert training_ph is not None",
            "units = tf.layers.batch_normalization(units, training=training_ph)"
        ]
    },
    {
        "number": 3573,
        "comments": "",
        "commit_message": "Fix scatter LopngTensor\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BertForQuestionAnswering(nn.Module):",
            "def compute_loss(logits, positions):",
            "max_position = positions.max().item()",
            "one_hot = torch.FloatTensor(batch_size, max(max_position, seq_length) +1, device=input_ids.device).zero_()",
            "-                one_hot = one_hot.scatter(1, positions, 1)",
            "+                one_hot = one_hot.scatter(1, positions.cpu(), 1) # Second argument need to be LongTensor and not cuda.LongTensor",
            "one_hot = one_hot[:, :seq_length]",
            "log_probs = nn.functional.log_softmax(logits, dim = -1).view(batch_size, seq_length)",
            "loss = -torch.mean(torch.sum(one_hot*log_probs), dim = -1)"
        ]
    },
    {
        "number": 3574,
        "comments": "",
        "commit_message": "fixed bug in unit test\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestChainTensor(TestCase):",
            "x.get()",
            "x.child = x.child.child",
            "",
            "-        # target = sy._PlusIsMinusTensor().on(torch.FloatTensor([1, 1]))",
            "-        target = torch.FloatTensor([1, 1])",
            "+        target = sy._PlusIsMinusTensor().on(torch.FloatTensor([1, 1]))",
            "+        # target = torch.FloatTensor([1, 1])",
            "assert torch.equal(x.grad.data, target)"
        ]
    },
    {
        "number": 3575,
        "comments": "",
        "commit_message": "fix multiband melgan inference\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MultibandMelganGenerator(MelganGenerator):",
            "def pqmf_synthesis(self, x):",
            "return self.pqmf_layer.synthesis(x)",
            "",
            "+    @torch.no_grad()",
            "def inference(self, cond_features):",
            "cond_features = cond_features.to(self.layers[1].weight.device)",
            "cond_features = torch.nn.functional.pad(",
            "cond_features,",
            "(self.inference_padding, self.inference_padding),",
            "'replicate')",
            "-        return self.pqmf.synthesis(self.layers(cond_features))",
            "+        return self.pqmf_synthesis(self.layers(cond_features))"
        ]
    },
    {
        "number": 3576,
        "comments": "",
        "commit_message": "[TEST][DO NOT MERGE] update to pytorch v1.5.0 (#533)\n\n* fix padding issue for pytorch nightly build\n\n* update requirements to pytorch v1.5.0\n\n* fix mypy error in feature laf\n\n* add dtype test type\n\n* fix tests with same dtype arithmetic\n\n* make render gaussian2d input test contiguous\n\n* add PYTORCH_VERISION environment variable to setuptools\n\n* enable grayscale jit tests again\n\n* enable conv_quad grad tests\n\n* test grayscale on float16,float64\n\nCo-authored-by: connorlee77 <connorlee77@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PatchDominantGradientOrientation(nn.Module):",
            "self.num_ang_bins = num_angular_bins",
            "self.gradient = SpatialGradient('sobel', 1)",
            "self.eps = eps",
            "-        self.angular_smooth = nn.Conv1d(1, 1, kernel_size=3, padding=2, bias=False, padding_mode=\"circular\")",
            "+        self.angular_smooth = nn.Conv1d(1, 1, kernel_size=3, padding=1, bias=False, padding_mode=\"circular\")",
            "with torch.no_grad():",
            "self.angular_smooth.weight[:] = torch.tensor([[[0.33, 0.34, 0.33]]])",
            "sigma: float = float(self.patch_size) / math.sqrt(2.0)"
        ]
    },
    {
        "number": 3577,
        "comments": "",
        "commit_message": "fix type issue\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def build_targets(p, targets, model):",
            "# Build targets for compute_loss(), input targets(image_idx,class,x,y,w,h)",
            "nt = targets.shape[0]",
            "tcls, tbox, indices, anch = [], [], [], []",
            "-    gain = torch.ones(6, device=targets.device)  # normalized to gridspace gain",
            "+    gain = torch.ones(6, device=targets.device).long()  # normalized to gridspace gain",
            "",
            "multi_gpu = type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel)",
            "for i, j in enumerate(model.yolo_layers):  # j: [89, 101, 113]"
        ]
    },
    {
        "number": 3578,
        "comments": "",
        "commit_message": "Small fixes\n\nSummary: Pull Request resolved: https://github.com/fairinternal/fairseq-py/pull/835\n\nDifferential Revision: D16904038\n\nPulled By: myleott\n\nfbshipit-source-id: 2c9d0b913f8d688297ac80fcabd905bd1397f66a\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LightconvLayer(nn.Module):",
            "weight = weight.view(1, H, K).expand(T*B, H, K).contiguous().view(T*B*H, K, 1)",
            "",
            "weight = F.dropout(weight, self.weight_dropout, training=self.training)",
            "-            output = torch.bmm(x_unfold, weight) # T*B*H x R x 1",
            "+            output = torch.bmm(x_unfold, weight)  # T*B*H x R x 1",
            "output = output.view(T, B, C)",
            "return output"
        ]
    },
    {
        "number": 3585,
        "comments": "",
        "commit_message": "Move DeiT to own file, vit getting crowded. Working towards fixing #1029, make pooling interface for transformers and mlp closer to convnets. Still working through some details...\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CrossViT(nn.Module):",
            "",
            "# NOTE: was before branch token section, move to here to assure all branch token are before layer norm",
            "xs = [norm(xs[i]) for i, norm in enumerate(self.norm)]",
            "-        return [xo[:, 0] for xo in xs]",
            "+        return xs",
            "",
            "def forward(self, x):",
            "xs = self.forward_features(x)",
            "-        ce_logits = [head(xs[i]) for i, head in enumerate(self.head)]",
            "+        ce_logits = [head(xs[i][:, 0]) for i, head in enumerate(self.head)]",
            "if not isinstance(self.head[0], nn.Identity):",
            "ce_logits = torch.mean(torch.stack(ce_logits, dim=0), dim=0)",
            "return ce_logits"
        ]
    },
    {
        "number": 3587,
        "comments": "",
        "commit_message": "Fix style\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def testtanh():",
            "3.3883e02,",
            "]",
            ")",
            "-",
            "-    #allclose function to compare the expected values and approximations with fixed precision",
            "-    assert torch.allclose(expected, Ptensor.tanh(x),1e-03)",
            "",
            "+    # allclose function to compare the expected values and approximations with fixed precision",
            "+    assert torch.allclose(expected, Ptensor.tanh(x), 1e-03)"
        ]
    },
    {
        "number": 3589,
        "comments": "",
        "commit_message": "[CI ] Remove `past` in favor of `pat_key_values` (#21443)\n\n* fix past renamed to past_key_value\n\n* update more `past`that were ski^\u00ead\n\n* fixup\n\n* remove changes made to rag\n\n* refactor `_reorder_cache` to use `past_key_values`\n\n* fix git `prepare_inputs_for_generation` to pass tests when false is needed in use_cache\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GPTNeoForCausalLM(GPTNeoPreTrainedModel):",
            "\"\"\"",
            "return tuple(",
            "tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)",
            "-            for layer_past in past",
            "+            for layer_past in past_key_values",
            ")"
        ]
    },
    {
        "number": 3593,
        "comments": "",
        "commit_message": "Fix special values overflow/underflow for amp (#3901)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ScalarMix(torch.nn.Module):",
            "variance = (",
            "torch.sum(((tensor_masked - mean) * broadcast_mask) ** 2) / num_elements_not_masked",
            ")",
            "-            return (tensor - mean) / torch.sqrt(variance + 1e-12)",
            "+            return (tensor - mean) / torch.sqrt(variance + util.tiny_value_of_dtype(variance.dtype))",
            "",
            "normed_weights = torch.nn.functional.softmax(",
            "torch.cat([parameter for parameter in self.scalar_parameters]), dim=0"
        ]
    },
    {
        "number": 3598,
        "comments": "",
        "commit_message": "cuda bugfix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def coalesce(edge_index, edge_attr=None, num_nodes=None):",
            "_, perm = unique(index)",
            "edge_index = edge_index[:, perm]",
            "else:",
            "-        sparse = getattr(torch.sparse, edge_attr.type().split('.')[-1])",
            "+        t = torch.cuda if edge_attr.is_cuda else torch",
            "+        sparse = getattr(t.sparse, edge_attr.type().split('.')[-1])",
            "n = num_nodes",
            "size = torch.Size([n, n] + list(edge_attr.size())[1:])",
            "adj = sparse(edge_index, edge_attr, size).coalesce()"
        ]
    },
    {
        "number": 3599,
        "comments": "",
        "commit_message": "Fix casting when computing negative ROI count.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def fpn_classifier_graph(rois, feature_maps,",
            "name=\"mrcnn_class_conv1\")(x)",
            "x = KL.TimeDistributed(BatchNorm(axis=3), name='mrcnn_class_bn1')(x)",
            "x = KL.Activation('relu')(x)",
            "-    # x = KL.Dropout(0.5)(x)",
            "x = KL.TimeDistributed(KL.Conv2D(1024, (1, 1)),",
            "name=\"mrcnn_class_conv2\")(x)",
            "x = KL.TimeDistributed(BatchNorm(axis=3),"
        ]
    },
    {
        "number": 3600,
        "comments": "",
        "commit_message": "Fix mypy issue with torch.linalg (#1236)\n\n* add a compatibiliy stub for torch solve\n\n* change missing _torch_solve_cast calls\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* bump pytorch version to 1.7.1\n\n* Fix mypy issue with torch.linalg\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "import torch",
            "from packaging import version",
            "",
            "if version.parse(torch.__version__) > version.parse(\"1.7.1\"):",
            "-    from torch.linalg import solve",
            "+    # TODO: remove the type: ignore once Python 3.6 is deprecated.",
            "+    # It turns out that Pytorch has no attribute `torch.linalg` for",
            "+    # Python 3.6 / PyTorch 1.7.0, 1.7.1",
            "+    from torch.linalg import solve  # type: ignore",
            "else:",
            "from torch import solve as _solve"
        ]
    },
    {
        "number": 3603,
        "comments": "",
        "commit_message": "Disable tests in TF1 using tf2.enabled().\n\nAlso fix image conversion in mnist_export_v2 to prevent float overflow in dbg mode.\n\nPiperOrigin-RevId: 307367586\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ExportTokenEmbeddingTest(tf.test.TestCase):",
            "",
            "if __name__ == \"__main__\":",
            "# This test is only supported in TF 2.0+.",
            "-  if LooseVersion(tf.__version__) >= LooseVersion(\"2.0.0-beta0\"):",
            "+  if tf.executing_eagerly():",
            "logging.info(\"Using TF version: %s\", tf.__version__)",
            "tf.test.main()",
            "else:"
        ]
    },
    {
        "number": 3604,
        "comments": "",
        "commit_message": "Fix unit tests for GERMEVAL dataset rename\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_load_no_dev_data_explicit(tasks_base_path):",
            "",
            "",
            "def test_multi_corpus(tasks_base_path):",
            "-    corpus_1 = flair.datasets.ColumnCorpus(tasks_base_path / \"germeval_14\", column_format={0: \"text\", 2: \"ner\"})",
            "+    corpus_1 = flair.datasets.ColumnCorpus(tasks_base_path / \"ner_german_germeval\", column_format={0: \"text\", 2: \"ner\"})",
            "",
            "corpus_2 = flair.datasets.ColumnCorpus(tasks_base_path / \"fashion\", column_format={0: \"text\", 2: \"ner\"})",
            "# get two corpora as one"
        ]
    },
    {
        "number": 3607,
        "comments": "",
        "commit_message": "fix CI error\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class FairSeqWav2Vec2Encoder(AbsEncoder):",
            "logging.info(\"Start fine-tuning wav2vec parameters!\")",
            "",
            "with torch.no_grad() if not ft else contextlib.nullcontext():",
            "-            enc_outputs = self.encoders(",
            "-                xs_pad,",
            "-                masks,",
            "-                features_only=True,",
            "-            )",
            "+            enc_outputs = self.encoders(xs_pad, masks, features_only=True)",
            "",
            "xs_pad = enc_outputs[\"x\"]  # (B,T,C),",
            "masks = enc_outputs[\"padding_mask\"]  # (B, T)"
        ]
    },
    {
        "number": 3609,
        "comments": "",
        "commit_message": "Fix incorrect formatting\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BertEncoder(tf.keras.layers.Layer):",
            "position_embeddings = self._position_embedding_layer(word_embeddings)",
            "type_embeddings = self._type_embedding_layer(type_ids)",
            "",
            "-        embeddings = self._add([word_embeddings, position_embeddings, type_embeddings])",
            "+        embeddings = self._add(",
            "+            [word_embeddings, position_embeddings, type_embeddings]",
            "+        )",
            "",
            "embeddings = self._layer_norm(embeddings)",
            "embeddings = self._dropout(embeddings)"
        ]
    },
    {
        "number": 3614,
        "comments": "",
        "commit_message": "Fix hyperparam and optimizer issue in distributed trainer (#431)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GraphVarParam(HyperParam):",
            "",
            "def setup_graph(self):",
            "\"\"\" Will setup the assign operator for that variable. \"\"\"",
            "-        all_vars = tf.global_variables()",
            "+        all_vars = tf.all_variables()",
            "for v in all_vars:",
            "if v.name == self.var_name:",
            "self.var = v"
        ]
    },
    {
        "number": 3615,
        "comments": "",
        "commit_message": "minor change to fix errors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GroupNorm(Layer):",
            "channels = inputs_shape[-1]",
            "self.int_shape = tf.concat(",
            "[#tf.shape(input=self.inputs)[0:3],",
            "-                inputs_shape[0:3]",
            "-                 tf.convert_to_tensor(value=[self.groups, channels // self.groups])], axis=0",
            "+                inputs_shape[0:3],",
            "+                tf.convert_to_tensor(value=[self.groups, channels // self.groups])], axis=0",
            ")",
            "elif self.data_format == 'channels_first':",
            "channels = shape[1]"
        ]
    },
    {
        "number": 3616,
        "comments": "",
        "commit_message": "fix https://github.com/Sanster/lama-cleaner/issues/230\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def md5sum(filename):",
            "",
            "",
            "def switch_mps_device(model_name, device):",
            "-    if model_name not in MPS_SUPPORT_MODELS and (",
            "-        device == \"mps\" or device == torch.device(\"mps\")",
            "-    ):",
            "+    if model_name not in MPS_SUPPORT_MODELS and str(device) == \"mps\":",
            "logger.info(f\"{model_name} not support mps, switch to cpu\")",
            "return torch.device(\"cpu\")",
            "return device"
        ]
    },
    {
        "number": 3619,
        "comments": "",
        "commit_message": "Abstract accelerator (step 3) (#2677)\n\n* Integrate accelerator abstraction interface into deepspeed/\n\n* Fix error message in fp16/fused_optimizer\n\n* fix error message in fp16/unfused_optimizer.py\n\n* assign get_accelerator().pin_memory() result to input Tensor name\n\n* no need to check cuda and whether nvtx supported\n\n* move try-except into inner most block\n\n* call Event() and Stream() in get_accelerator() for data type\n\n* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed\n\n* Apply op_builder backend api change from #2705 from @jeffra\n\n* fix tests where Builder NAME is used\n\n* keep original ...Builder.NAME interface instead of ...Builder().NAME interface\n\n* fix builder closure for installation\n\n* fix randomltd builder\n\n* add comments to clarify create_op_builder and get_op_builder\n\n* fix compatibility with pip install -e\n\nCo-authored-by: Cheng Li <pistasable@gmail.com>\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Autotuner:",
            "return False",
            "",
            "def get_gpu_memory_info(self):",
            "-        return torch.cuda.get_device_properties(0).total_memory",
            "+        return get_accelerator().total_memory()",
            "",
            "def get_activation_memory_per_gpu(self):",
            "if self.model_info and \"activation_mem_per_gpu\" in self.model_info:"
        ]
    },
    {
        "number": 3621,
        "comments": "",
        "commit_message": "fix target batch\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Set(Data):",
            "target = self.target[s1[i]:s1[i + 1]]",
            "else:",
            "target = self.target[i]",
            "-            target = target.view(1, -1).squeeze(1)",
            "+",
            "+            if torch.is_tensor(target):",
            "+                target = target.view(1, -1).squeeze(1)",
            "",
            "return Data(input, pos, index, weight, target)"
        ]
    },
    {
        "number": 3623,
        "comments": "",
        "commit_message": "Sync for 4.0 release (#961)\n\n* sync for 4.0 release\n\n* fix py2 issue in conversion of torch std op\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ImagePreprocessingPass(unittest.TestCase):",
            "x4 = mb.add(x=x1, y=x3)",
            "return mb.relu(x=x4)",
            "",
            "-        proto = converter._convert(prog, inputs=[ImageType(name=\"x\", shape=(10, 20, 30, 3), channel_first=False)], convert_from=\"mil\", convert_to=\"nn_proto\")",
            "-        model = models.MLModel(proto)",
            "-        assert model is not None",
            "-        assert len(model._spec.neuralNetwork.layers) == 3",
            "+        mlmodel = ct.convert(prog,",
            "+            inputs=[ImageType(name=\"x\", shape=(10, 20, 30, 3),",
            "+              channel_first=False)],",
            "+            source=\"mil\", convert_to=\"nn_proto\")",
            "+        assert mlmodel is not None",
            "+        assert len(mlmodel.get_spec().neuralNetwork.layers) == 3"
        ]
    },
    {
        "number": 3629,
        "comments": "",
        "commit_message": "fix flake8 style in tensorpack/\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CheckGradient(MapGradient):",
            "",
            "def _mapper(self, grad, var):",
            "# this is very slow.... see #3649",
            "-        #op = tf.Assert(tf.reduce_all(tf.is_finite(var)), [var], summarize=100)",
            "+        # op = tf.Assert(tf.reduce_all(tf.is_finite(var)), [var], summarize=100)",
            "grad = tf.check_numerics(grad, 'CheckGradient-' + var.op.name)",
            "return grad"
        ]
    },
    {
        "number": 3631,
        "comments": "",
        "commit_message": "Fixed lint formatting and type hints removal\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_categorical_crossentropy(",
            "native_array,",
            "fw,",
            "):",
            "-    y_true = ivy.array(y_true, dtype = ivy.float32)",
            "+    y_true = ivy.array(y_true, dtype=ivy.float32)",
            "dtype, y_pred = dtype_y_pred",
            "",
            "# Perform softmax on prediction if it's not a probability distribution."
        ]
    },
    {
        "number": 3633,
        "comments": "",
        "commit_message": "Fix eval_lm (fixes #2083) and a few other small things (#2100)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2100\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D21456309\n\nPulled By: myleott\n\nfbshipit-source-id: 291711589fca9f158e0fdbf01194da3e66fbd0aa\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def add_dataset_args(parser, train=False, gen=False):",
            "return group",
            "",
            "",
            "-def add_distributed_training_args(parser):",
            "+def add_distributed_training_args(parser, default_world_size=None):",
            "group = parser.add_argument_group(\"Distributed training\")",
            "# fmt: off",
            "+    if default_world_size is None:",
            "+        default_world_size = max(1, torch.cuda.device_count())",
            "group.add_argument('--distributed-world-size', type=int, metavar='N',",
            "-                       default=max(1, torch.cuda.device_count()),",
            "+                       default=default_world_size,",
            "help='total number of GPUs across all nodes (default: all visible GPUs)')",
            "group.add_argument('--distributed-rank', default=0, type=int,",
            "help='rank of the current worker')"
        ]
    },
    {
        "number": 3634,
        "comments": "",
        "commit_message": "add comments and fix error in tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def install_openvino(with_optimization: bool = True):",
            "\"\"\"Helper function for installing the OpenVino compiler.",
            "",
            "This function just works on intel machines.",
            "+",
            "+    Args:",
            "+        with_optimization (bool): Flag for installing the full openvino engine",
            "+            or limiting the installation to the tools need for inference",
            "+            models.",
            "\"\"\"",
            "processor = cpuinfo.get_cpu_info()[\"brand_raw\"].lower()",
            "if \"intel\" not in processor:"
        ]
    },
    {
        "number": 3635,
        "comments": "",
        "commit_message": "GH-462: fix cuda errors in classifier and unit tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DocumentLSTMEmbeddings(DocumentEmbeddings):",
            "for add in range(longest_token_sequence_in_batch - len(sentence.tokens)):",
            "word_embeddings.append(",
            "torch.zeros(self.length_of_all_token_embeddings,",
            "-                                dtype=torch.float, device=flair.device).unsqueeze(0)",
            "+                                dtype=torch.float).unsqueeze(0)",
            ")",
            "",
            "-            word_embeddings_tensor = torch.cat(word_embeddings, 0)",
            "+            word_embeddings_tensor = torch.cat(word_embeddings, 0).to(flair.device)",
            "",
            "sentence_states = word_embeddings_tensor"
        ]
    },
    {
        "number": 3638,
        "comments": "",
        "commit_message": "Refactor library namespaces [pre-release][0.6-rc1] (#1412)\n\n* flake fixes\n\n* initial flake8 fixeS\n\n* remove top level from kornia.color\n\n* kornia filters\n\n* kornia losses\n\n* kornia features\n\n* geomtry and all ok\n\n* removed jit module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* apply formatting and few fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add keep block for isort\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* skip init\n\n* fix the docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove old code\n\n* simplify ci workflow\n\n* fix circular dependency\n\n* few format fixes\n\n* fix code format test\n\n* remove kornia.jit imports\n\n* final fixes\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* ci fixes\n\n* add versioneer\n\n* fix pnp import\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* exclude version files from pre-commit\n\n* exclude files precommit\n\n* update to pytorch 1.10 and add fixes\n\n* Update tests_cpu.yml\n\n* Update setup_dev_env.sh\n\n* Update tests_cpu.yml\n\n* undo versioneer\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix no_grad\n\n* Apply suggestions from code review\n\n* fix skip windows tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update test_integrated.py\n\n* Apply suggestions from code review\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def compute_correspond_epilines(points: torch.Tensor, F_mat: torch.Tensor) -> to",
            "if not (len(F_mat.shape) == 3 and F_mat.shape[-2:] == (3, 3)):",
            "raise AssertionError(F_mat.shape)",
            "",
            "-    points_h: torch.Tensor = kornia.convert_points_to_homogeneous(points)",
            "+    points_h: torch.Tensor = convert_points_to_homogeneous(points)",
            "",
            "# project points and retrieve lines components",
            "a, b, c = torch.chunk(F_mat @ points_h.permute(0, 2, 1), dim=1, chunks=3)"
        ]
    },
    {
        "number": 3641,
        "comments": "",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MultiHeadSelfAttention(nn.Module):",
            "q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)",
            "scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)",
            "mask = (mask == 0).view(mask_reshp).expand_as(scores)  # (bs, n_heads, q_length, k_length)",
            "-        scores = scores.masked_fill(mask, torch.tensor(-float(\"inf\")))  # (bs, n_heads, q_length, k_length)",
            "+        scores = scores.masked_fill(",
            "+            mask, torch.tensor(torch.finfo(scores.dtype).min)",
            "+        )  # (bs, n_heads, q_length, k_length)",
            "",
            "weights = nn.functional.softmax(scores, dim=-1)  # (bs, n_heads, q_length, k_length)",
            "weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)"
        ]
    },
    {
        "number": 3643,
        "comments": "",
        "commit_message": "Adapt image datasets (#3362)\n\n* Initial commit\n\n* Add basic decoding\n\n* Replace features.Audio with Audio\n\n* Add Image to package reference\n\n* Use np.array\n\n* Update error msg\n\n* Add mode and channel decoding\n\n* Fix return value\n\n* Finish decoding\n\n* Make CI happy\n\n* Some more fixes\n\n* Minor doc fix\n\n* Remove animated option\n\n* Pin version\n\n* Remove unused imports in setup.py\n\n* Add vision requirements to setup.py\n\n* Add initial tests\n\n* Delete other formats\n\n* Make Image feature hashable\n\n* Add more tests\n\n* Support numpy array in alter data check in TypedSequence\n\n* Fix TypedSequence converion\n\n* Finish tests\n\n* Update Image - add ImageExtensionType and supporting functions\n\n* Update encoding functions\n\n* Add support in TypedSequence for ImageExtensionType\n\n* Add tests\n\n* Remove unused import\n\n* Fix doc and style\n\n* Fix doc indentation\n\n* Improve comment\n\n* Return single image instead of dict\n\n* Return PIL Image and not dict\n\n* Encode dict\n\n* Update tests\n\n* Style\n\n* np.ndarray encoding/decoding\n\n* Minor improvements\n\n* PIL Image support in cast_to_python_objects\n\n* Test cast\n\n* Doc fix\n\n* Extension type fixes\n\n* Style\n\n* Use types_mapper in Dataset.to_pandas\n\n* Add pandas extension array for image type\n\n* Update tests\n\n* image type inference\n\n* Remvoe cast_to_python test after Quentin's change\n\n* Improve tests\n\n* Add storage type\n\n* Improve tests\n\n* Test map that returns np.ndarray\n\n* Rename functions\n\n* Add streaming test\n\n* Use image struct in all situations\n\n* Update src/datasets/features/image.py - encode_example type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Update src/datasets/features/image.py -list_image_compression_formats type hint\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Use Image feature in ImageClassification task\n\n* Adapt cats_vs_dogs\n\n* Adapt beans\n\n* Adapt cifar10\n\n* Adapt cifar100\n\n* Add task templates to cifar10 and cifar100\n\n* Adapt FashionMNIST\n\n* Adapt food101\n\n* Adapt mnist datasets\n\n* Adapt head_qa\n\n* Update example in head_qa readme\n\n* Update head_qa dummy data\n\n* Fix streaming in beans and cats_vs_dogs\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CatsVsDogs(datasets.GeneratorBasedBuilder):",
            "if b\"JFIF\" in f.peek(10):",
            "yield str(i), {",
            "\"image_file_path\": str(filepath),",
            "+                        \"image\": str(filepath),",
            "\"labels\": filepath.parent.name.lower(),",
            "}",
            "continue"
        ]
    },
    {
        "number": 3644,
        "comments": "",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Attention(nn.Module):",
            "",
            "def fill_with_neg_inf(t):",
            "\"\"\"FP16-compatible function that fills a input_ids with -inf.\"\"\"",
            "-    return t.float().fill_(float(\"-inf\")).type_as(t)",
            "+    return t.float().fill_(torch.finfo(t.dtype).min).type_as(t)",
            "",
            "",
            "# Public API"
        ]
    },
    {
        "number": 3647,
        "comments": "",
        "commit_message": "refactor hdfs_path; fix more deprecation warnings; print label+preds\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def deploy(config,",
            "",
            "if total_loss is not None:",
            "# Add total_loss to summary.",
            "-      summaries.add(tf.summary.scalar('total_loss', total_loss,",
            "-                                      name='total_loss'))",
            "+      summaries.add(tf.summary.scalar('total_loss', total_loss))",
            "",
            "if summaries:",
            "# Merge all summaries together."
        ]
    },
    {
        "number": 3649,
        "comments": "",
        "commit_message": "PLY with uint face data (#1104)\n\nSummary: Fix assumption that face indices are signed in the PLY file, as reported in #1104.\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D34892598\n\nfbshipit-source-id: a8b23bfac1357bdc11bbbf752098319142239804\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _load_ply(f, *, path_manager: PathManager) -> _PlyData:",
            "if face.shape[1] < 3:",
            "raise ValueError(\"Faces must have at least 3 vertices.\")",
            "face_arrays = [face[:, [0, i + 1, i + 2]] for i in range(face.shape[1] - 2)]",
            "-        faces = torch.LongTensor(np.vstack(face_arrays))",
            "+        faces = torch.LongTensor(np.vstack(face_arrays).astype(np.int64))",
            "else:",
            "face_list = []",
            "for face_item in face:"
        ]
    },
    {
        "number": 3652,
        "comments": "",
        "commit_message": "fix: minor fixes and refactoring NER (#539)\n\n* feat: add train from scratch\n\n* feat: add warning to train_from_scratch\n\n* fix: no cudnn rnn fixed mask\n\n* fix: session config added\n\n* fix: warning to log\n\n* fix: pass learning rate to default optimizer\n\n* fix: l2 in conv layer\n\n* fix: ner dataset reader\n\n* feat: add returning loss to the train on batch\n\n* doc: add model descriptions to the ner doc\n\n* chore: move ner metrics to metrics module\n\n* fix: remove train from scratch\n\n* fix: add dataset names\n\n* chore: log.info -> log.warning for warning\n\nCo-Authored-By: mu-arkhipov <arkhipov@yahoo.com>\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFModel(NNModel, metaclass=TfModelMeta):",
            "variables_to_train.extend(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope_name))",
            "",
            "if optimizer is None:",
            "-                optimizer = tf.train.AdamOptimizer",
            "+                optimizer = tf.train.AdamOptimizer(learning_rate)",
            "",
            "# For batch norm it is necessary to update running averages",
            "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
        ]
    },
    {
        "number": 3658,
        "comments": "",
        "commit_message": "Fix formatting of last commit\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PatchEmbed(nn.Module):",
            "",
            "def forward(self, x):",
            "B, C, H, W = x.shape",
            "-        torch._assert(H == self.img_size[0], f\"Input image height ({H}) doesn't match model {self.img_size[0]}.\")",
            "-        torch._assert(W == self.img_size[1], f\"Input image width ({W}) doesn't match model ({self.img_size[1]}.\")",
            "+        torch._assert(H == self.img_size[0], f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\")",
            "+        torch._assert(W == self.img_size[1], f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\")",
            "x = self.proj(x)",
            "if self.flatten:",
            "x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC"
        ]
    },
    {
        "number": 3659,
        "comments": "",
        "commit_message": "Reland BT enablement on fairseq - fairseq change (#4513)\n\nSummary:\nPull Request resolved: https://github.com/facebookresearch/fairseq/pull/4513\nWith some fixes to torchscript using dual copies.\nReland this diff.\n\nReviewed By: erichan1\n\nDifferential Revision: D37371293\n\nfbshipit-source-id: 4fcfc4083955b6f5fc4ef8600f1b517b6ba69aae\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestJitSequenceGeneratorBase(unittest.TestCase):",
            "JIT_MSG = \"Targeting OSS scriptability for the 1.6 release\"",
            "",
            "",
            "-@unittest.skipIf(torch.__version__ < \"1.6.0\", JIT_MSG)",
            "+@unittest.skipIf(",
            "+    version_check(), \"Targeting OSS scriptability for the 1.13.0.dev20220613 release\"",
            "+)",
            "class TestJitSequenceGenerator(TestJitSequenceGeneratorBase):",
            "def test_export_transformer(self):",
            "model = self.transformer_model"
        ]
    },
    {
        "number": 3660,
        "comments": "",
        "commit_message": "[Audio datasets] Adapting all audio datasets (#3081)\n\n* up\n\n* up\n\n* up\n\n* up\n\n* up\n\n* Update src/datasets/utils/resources/readme_structure.yaml\n\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\n\n* correct\n\n* correct 2\n\n* Update datasets/covost2/README.md\n\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\n\n* Fix typo\n\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class OpenSlr(datasets.GeneratorBasedBuilder):",
            "# set absolute path for audio file",
            "path = os.path.join(path_to_datas[i], f\"{filename}.wav\")",
            "counter += 1",
            "-                        yield counter, {\"path\": path, \"sentence\": sentence}",
            "+                        yield counter, {\"path\": path, \"audio\": path, \"sentence\": sentence}"
        ]
    },
    {
        "number": 3662,
        "comments": "",
        "commit_message": "[Enhance] Added dtype flag to geometry module (#820)\n\n* Added docs and tests\n\n* Fixed bug for batch prob generator\n\n* Fixed typo\n\n* Fixed mypy\n\n* Fixed mypy\n\n* Added warning\n\n* Updated tests for geometry\n\n* Added dtype for conversions\n\n* Completed tests\n\n* Fixed homography\n\n* Fixed pinhole camera\n\n* Fixed tests\n\n* Fix geometry testing bugs\n\n* Fixed augmentation tests\n\n* code updated\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def find_homography_dlt(",
            "U, S, V = torch.svd(A)",
            "except:",
            "warnings.warn('SVD did not converge', RuntimeWarning)",
            "-        return torch.empty((points1_norm.size(0), 3, 3), device=points1.device)",
            "+        return torch.empty((points1_norm.size(0), 3, 3), device=device, dtype=dtype)",
            "",
            "H = V[..., -1].view(-1, 3, 3)",
            "H = transform2.inverse() @ (H @ transform1)"
        ]
    },
    {
        "number": 3665,
        "comments": "",
        "commit_message": "test: minor pytest fixes (#3109)\n\nAdds all frameworks to dev-dependencies as well. Skips LightGBM on arm64 because it's not supported.\n\nRemoves most of the pytest flags to make pytest output a little more sane and fix some issues caused by the importlib import method.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_inputs(framework: str | None) -> list[tuple[ModuleType, FrameworkTestMo",
            ")",
            ")",
            "except ModuleNotFoundError as e:",
            "-            raise ModuleNotFoundError(",
            "-                f\"Failed to find test module for framework {framework_name} (tests.integration.frameworks.models.{framework_name})\"",
            "-            ) from e",
            "+            logger.warning(f\"Failed to find test module for framework {framework_name} (tests.integration.frameworks.models.{framework_name})\")",
            "",
            "return [",
            "(module.framework, _model)"
        ]
    },
    {
        "number": 3667,
        "comments": "",
        "commit_message": "precommit: yapf (#5494)\n\n* precommit: yapf\n\n* align isort\n\n* fix\n\n# Conflicts:\n#\tutils/plots.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update setup.cfg\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update setup.cfg\n\n* Update setup.cfg\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update wandb_utils.py\n\n* Update augmentations.py\n\n* Update setup.cfg\n\n* Update yolo.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update val.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* simplify colorstr\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* val run fix\n\n* export.py last comma\n\n* Update export.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update hubconf.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* PyTorch Hub tuple fix\n\n* PyTorch Hub tuple fix2\n\n* PyTorch Hub tuple fix3\n\n* Update setup\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MixConv2d(nn.Module):",
            "a[0] = 1",
            "c_ = np.linalg.lstsq(a, b, rcond=None)[0].round()  # solve for equal weight indices, ax = b",
            "",
            "-        self.m = nn.ModuleList(",
            "-            [nn.Conv2d(c1, int(c_), k, s, k // 2, groups=math.gcd(c1, int(c_)), bias=False) for k, c_ in zip(k, c_)])",
            "+        self.m = nn.ModuleList([",
            "+            nn.Conv2d(c1, int(c_), k, s, k // 2, groups=math.gcd(c1, int(c_)), bias=False) for k, c_ in zip(k, c_)])",
            "self.bn = nn.BatchNorm2d(c2)",
            "self.act = nn.SiLU()"
        ]
    },
    {
        "number": 3668,
        "comments": "",
        "commit_message": "Fix some error of YOLOX (#5848)\n\n* Add YOLOX config\n\n* update\n\n* fix error\n\n* fix lr error\n\n* fix tiny config error and foreground_mask warning\n\n* fix dp train error\n\n* add comment\n\n* support browse_dataset\n\n* add comment\n\n* fix __repr__\n\n* Switch to synchronizing norm interval.\n\n* delete config\n\n* add MultiImageMixDataset unittest\n\n* add MultiImageMixDataset skip_type_keys unittest\n\n* fix type\n\n* add comment\n",
        "label": "",
        "answer": "no",
        "change": [
            "class YOLOXHead(BaseDenseHead, BBoxTestMixin):",
            "if self.use_l1:",
            "l1_target = self._get_l1_target(l1_target, bbox_target,",
            "priors[pos_inds])",
            "-        foreground_mask = torch.zeros_like(objectness).to(torch.uint8)",
            "+        foreground_mask = torch.zeros_like(objectness).to(torch.bool)",
            "foreground_mask[pos_inds] = 1",
            "return (foreground_mask, cls_target, obj_target, bbox_target,",
            "l1_target, num_pos_per_img)"
        ]
    },
    {
        "number": 3673,
        "comments": "",
        "commit_message": "Doc styler examples (#14953)\n\n* Fix bad examples\n\n* Add black formatting to style_doc\n\n* Use first nonempty line\n\n* Put it at the right place\n\n* Don't add spaces to empty lines\n\n* Better templates\n\n* Deal with triple quotes in docstrings\n\n* Result of style_doc\n\n* Enable mdx treatment and fix code examples in MDXs\n\n* Result of doc styler on doc source files\n\n* Last fixes\n\n* Break copy from\n",
        "label": "",
        "answer": "no",
        "change": [
            "class XSoftmax(torch.autograd.Function):",
            ">>> from transformers.models.deberta_v2.modeling_deberta_v2 import XSoftmax",
            "",
            ">>> # Make a tensor",
            "-    >>> x = torch.randn([4,20,100])",
            "+    >>> x = torch.randn([4, 20, 100])",
            "",
            ">>> # Create a mask",
            "-    >>> mask = (x>0).int()",
            "+    >>> mask = (x > 0).int()",
            "",
            ">>> # Specify the dimension to apply softmax",
            ">>> dim = -1"
        ]
    },
    {
        "number": 3674,
        "comments": "",
        "commit_message": "Fix df coercion in MultivariateStudentT (#2228)\n\n* Fix df coercion in MultivariateStudentT\n\n* flake8\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MultivariateStudentT(TorchDistribution):",
            "def __init__(self, df, loc, scale_tril, validate_args=None):",
            "dim = loc.size(-1)",
            "assert scale_tril.shape[-2:] == (dim, dim)",
            "-        df, = broadcast_all(df)",
            "+        if not isinstance(df, torch.Tensor):",
            "+            df = loc.new_tensor(df)",
            "batch_shape = broadcast_shape(df.shape, loc.shape[:-1], scale_tril.shape[:-2])",
            "event_shape = (dim,)",
            "self.df = df.expand(batch_shape)"
        ]
    },
    {
        "number": 3675,
        "comments": "",
        "commit_message": "small fix in nanmean torch backend\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def nanmean(",
            "dtype: Optional[torch.dtype] = None,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    return torch.nanmean(a, axis=axis, keepdim=keepdims, dtype=dtype, out=out)",
            "+    return torch.nanmean(a, dim=axis, keepdim=keepdims, dtype=dtype, out=out)",
            "",
            "",
            "nanmean_support_native_out = True"
        ]
    },
    {
        "number": 3676,
        "comments": "",
        "commit_message": "merge code in overfit (#1185)\n\n* merge code in overfit\n\n* fix\n",
        "label": "",
        "answer": "no",
        "change": [
            "argmax = tf.argmax",
            "tensor = tf.constant",
            "arange = tf.range",
            "astype = tf.cast",
            "+int32 = tf.int32",
            "+float32 = tf.float32",
            "numpy = lambda x, *args, **kwargs: x.numpy(*args, **kwargs)"
        ]
    },
    {
        "number": 3681,
        "comments": "",
        "commit_message": "fix the mask problem for multi-head attenion\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def multihead_attention(queries,",
            "outputs = tf.nn.softmax(outputs) # (h*N, T_q, T_k)",
            "",
            "# Query Masking",
            "-        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1))) # (N, T_q)",
            "+        query_masks = tf.sign(tf.reduce_sum(tf.abs(queries), axis=-1)) # (N, T_q)",
            "query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)",
            "query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)",
            "outputs *= query_masks # broadcasting. (N, T_q, C)"
        ]
    },
    {
        "number": 3684,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestStackedSelfAttention(AllenNlpTestCase):",
            "",
            "def test_pass_through_encoder_passes_through(self):",
            "encoder = PassThroughEncoder(input_dim=9)",
            "-        tensor = Variable(torch.randn([2, 3, 9]))",
            "+        tensor = torch.randn([2, 3, 9])",
            "output = encoder(tensor)",
            "-        numpy.testing.assert_array_almost_equal(tensor.data.cpu().numpy(),",
            "-                                                output.data.cpu().numpy())",
            "+        numpy.testing.assert_array_almost_equal(tensor.detach().cpu().numpy(),",
            "+                                                output.detach().cpu().numpy())"
        ]
    },
    {
        "number": 3686,
        "comments": "",
        "commit_message": "update espnet2/bin/asr_mix_inference.py; fix bugs\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ESPnetASRMixModel(AbsESPnetModel):",
            "ignore_label=self.ignore_id,",
            ")",
            ")",
            "-        loss_att = torch.mean(loss_att)",
            "+        loss_att = torch.stack(loss_att, dim=0).mean()",
            "acc_att = np.mean(acc_att)",
            "",
            "# Compute cer/wer using attention-decoder"
        ]
    },
    {
        "number": 3688,
        "comments": "",
        "commit_message": "fix tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestTRPOAgent(unittest.TestCase):",
            "def test_trpo_agent(self):",
            "config = {",
            "'batch_size': 8,",
            "+            \"cg_iterations\": 20,",
            "+            \"cg_damping\": 0.001,",
            "+            \"line_search_steps\": 20,",
            "'max_kl_divergence': 0.01,",
            "'max_episode_length': 4,",
            "'continuous': False,",
            "'state_shape': (2,),",
            "'actions': 2}",
            "+        tf.reset_default_graph()",
            "",
            "config = create_config(config)",
            "network_builder = NeuralNetwork.layered_network(layers=[{'type': 'dense', 'num_outputs': 32}])"
        ]
    },
    {
        "number": 3690,
        "comments": "",
        "commit_message": "[rllib] format with yapf (#2427)\n\n* initial yapf\n\n* manual fix yapf bugs\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def itergroups(items, group_size):",
            "def batched_weighted_sum(weights, vecs, batch_size):",
            "total = 0",
            "num_items_summed = 0",
            "-    for batch_weights, batch_vecs in zip(itergroups(weights, batch_size),",
            "-                                         itergroups(vecs, batch_size)):",
            "+    for batch_weights, batch_vecs in zip(",
            "+            itergroups(weights, batch_size), itergroups(vecs, batch_size)):",
            "assert len(batch_weights) == len(batch_vecs) <= batch_size",
            "-        total += np.dot(np.asarray(batch_weights, dtype=np.float32),",
            "-                        np.asarray(batch_vecs, dtype=np.float32))",
            "+        total += np.dot(",
            "+            np.asarray(batch_weights, dtype=np.float32),",
            "+            np.asarray(batch_vecs, dtype=np.float32))",
            "num_items_summed += len(batch_weights)",
            "return total, num_items_summed"
        ]
    },
    {
        "number": 3692,
        "comments": "",
        "commit_message": "tl.layers API Refactoring and various modifications (#667)\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* First Refactoring Pass done\n\n* cleaning second pass\n\n* Refactoring 3rd pass\n\n* Refactoring 4th Pass\n\n* Code Error fix\n\n* YAPF Formating Fix\n\n* Arguments now using self\n\n* YAPF error correction\n\n* Bug Fix in Decorator\n\n* act name bug fix\n\n* Error Correction\n\n* YAPF formating fix\n\n* Useless tf.identity removed\n\n* Error Fix\n\n* Changelog Updated\n\n* Error fix in tl.activation\n\n* Documentation error fix\n\n* Lazy Import added\n\n* Import Refactoring with LazyImport when necessary\n\n* Changelog Updated\n\n* Gitter Removed\n\n* Fixed proposed by @zsdonghao\n\n* Documentation updated\n\n* Missing requirements added\n\n* Update to TensorLayer 1.8.6rc1\n\n* Requirements error fix\n\n* Docker Files updated\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Layer_Recurrent_Test(unittest.TestCase):",
            "dropout=None, n_layer=2, return_seq_2d=True, name='Seq2seq'",
            ")",
            "",
            "-        net11 = tl.layers.DenseLayer(net11, n_units=10000, act=tf.identity, name='oo')",
            "+        net11 = tl.layers.DenseLayer(net11, n_units=10000, name='oo')",
            "",
            "# e_loss = tl.cost.cross_entropy_seq_with_mask(logits=net11.outputs, target_seqs=target_seqs, input_mask=target_mask, return_details=False, name='cost')",
            "# y = tf.nn.softmax(net11.outputs)"
        ]
    },
    {
        "number": 3693,
        "comments": "",
        "commit_message": "Reorganize repo (#8580)\n\n* Put models in subfolders\n\n* Styling\n\n* Fix imports in tests\n\n* More fixes in test imports\n\n* Sneaky hidden imports\n\n* Fix imports in doc files\n\n* More sneaky imports\n\n* Finish fixing tests\n\n* Fix examples\n\n* Fix path for copies\n\n* More fixes for examples\n\n* Fix dummy files\n\n* More fixes for example\n\n* More model import fixes\n\n* Is this why you're unhappy GitHub?\n\n* Fix imports in conver command\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class RagConfig(PretrainedConfig):",
            "decoder_config = kwargs.pop(\"generator\")",
            "decoder_model_type = decoder_config.pop(\"model_type\")",
            "",
            "-        from .configuration_auto import AutoConfig",
            "+        from ..auto.configuration_auto import AutoConfig",
            "",
            "self.question_encoder = AutoConfig.for_model(question_encoder_model_type, **question_encoder_config)",
            "self.generator = AutoConfig.for_model(decoder_model_type, **decoder_config)"
        ]
    },
    {
        "number": 3695,
        "comments": "",
        "commit_message": "Fix test scripts and refine some code\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RNNLM(nn.Module):",
            "emb = self.embed(x)",
            "h[0], c[0] = self.lstm[0](self.dropout[0](emb), (state['h'][0], state['c'][0]))",
            "for n in six.moves.range(1, self.n_layers):",
            "-            h[n], c[n] = self.lstm[n](self.dropout[n](h[n-1]), (state['h'][n], state['c'][n]))",
            "+            h[n], c[n] = self.lstm[n](self.dropout[n](h[n - 1]), (state['h'][n], state['c'][n]))",
            "y = self.lo(self.dropout[-1](h[-1]))",
            "state = {'c': c, 'h': h}",
            "return state, y"
        ]
    },
    {
        "number": 3697,
        "comments": "",
        "commit_message": "fix: tensorflow tests + cleanup (#1885)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "TENSOR_CLASS_NAMES = (",
            "\"Tensor\",",
            ")",
            "",
            "-ST = TypeVar(\"ST\")",
            "+ST = t.TypeVar(\"ST\")",
            "",
            "",
            "-def _isinstance_wrapper(obj: ST, sobj: Union[str, type, Sequence]) -> bool:",
            "+def _isinstance_wrapper(obj: ST, sobj: t.Union[str, type, t.Sequence]) -> bool:",
            "\"\"\"",
            "`isinstance` wrapper to check tensor spec"
        ]
    },
    {
        "number": 3698,
        "comments": "",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario \u0160a\u0161ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Newsqa(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(path_to_manual_folder):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('newsqa', data_dir=...)` that includes files from the Manual download instructions: {}\".format(",
            "-                    path_to_manual_folder, self.manual_download_instructions",
            "-                )",
            "+                f\"{path_to_manual_folder} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('newsqa', data_dir=...)` that includes files from the Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "",
            "if self.config.name == \"combined-csv\":"
        ]
    },
    {
        "number": 3704,
        "comments": "",
        "commit_message": "Fix loss function in TF2 synthetic benchmark (#1859)\n\nSigned-off-by: Boyuan Deng <contact@boyuandeng.me>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def benchmark_step(first_batch):",
            "# Horovod: use DistributedGradientTape",
            "with tf.GradientTape() as tape:",
            "probs = model(data, training=True)",
            "-        loss = tf.losses.categorical_crossentropy(target, probs)",
            "+        loss = tf.losses.sparse_categorical_crossentropy(target, probs)",
            "",
            "# Horovod: add Horovod Distributed GradientTape.",
            "tape = hvd.DistributedGradientTape(tape, compression=compression)"
        ]
    },
    {
        "number": 3705,
        "comments": "",
        "commit_message": "Improve quantization, move it to wavenet_ops.py, and add test (#63)\n\n* Slightly improve quantization and add test\n* Remove empty lines\n* Adjust to changed API\n* Fix comment\n* Adjust imports in test\n* Move encode/decode to wavenet_ops.py\n* Address comments\n* Fix test\n* Fix pep8 errors\n* Test that all values are produced by mu law\n* Add periods at the end of comments\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class WaveNet(object):",
            "The variables are all scoped to the given name.",
            "'''",
            "with tf.variable_scope(name):",
            "-            input_batch = self.encode(input_batch)",
            "+            input_batch = mu_law_encode(input_batch,",
            "+                                        self.quantization_channels)",
            "encoded = self._one_hot(input_batch)",
            "raw_output = self._create_network(encoded)"
        ]
    },
    {
        "number": 3706,
        "comments": "",
        "commit_message": "changes and fixes (#4643)\n\n* changes and fixes\n\n* changes:\n\n* merge conflicts\n\n* lint\n\n* lint fixes\n\n* lint fixes\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def tensordot(",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "# find the type to promote to",
            "-    dtype = torch.promote_types(x1.dtype, x2.dtype)",
            "+    dtype = ivy.as_native_dtype(ivy.promote_types(x1.dtype, x2.dtype))",
            "# type conversion to one that torch.tensordot can work with",
            "x1, x2 = x1.type(torch.float32), x2.type(torch.float32)"
        ]
    },
    {
        "number": 3708,
        "comments": "",
        "commit_message": "fixing a bf16 support issue (#1760)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PipelineEngine(DeepSpeedEngine):",
            "Returns:",
            "A tensor from torch.zeros() allocated on self.device.",
            "\"\"\"",
            "-        if \"dtype\" not in kwargs and self.fp16_enabled():",
            "-            kwargs[\"dtype\"] = torch.half",
            "+        if \"dtype\" not in kwargs:",
            "+            if self.fp16_enabled():",
            "+                kwargs[\"dtype\"] = torch.half",
            "+            if self.bfloat16_enabled():",
            "+                kwargs[\"dtype\"] = torch.bfloat16",
            "",
            "return torch.zeros(shape, device=self.device, **kwargs)"
        ]
    },
    {
        "number": 3709,
        "comments": "",
        "commit_message": "lint fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def sigmoid(x: torch.Tensor, out: Optional[torch.Tensor] = None) -> torch.Tensor",
            "return torch.sigmoid(x, out=out)",
            "",
            "",
            "-def softmax(x: torch.Tensor, axis: Optional[int] = None, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "+def softmax(",
            "+    x: torch.Tensor, axis: Optional[int] = None, out: Optional[torch.Tensor] = None",
            "+) -> torch.Tensor:",
            "exp_x = torch.exp(x, out=out)",
            "return exp_x / torch.sum(exp_x, axis, keepdims=True)"
        ]
    },
    {
        "number": 3710,
        "comments": "",
        "commit_message": "Rename LightningLite to Fabric (#16244)\n\n* Rename LightningLite to Fabric\n\n* Fix introspection test\n\n* Fix deprecated Lite tests\n\n* Undo accidental Horovod removal\n\n* Fixes\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main_train(dir_path, max_epochs: int = 20):",
            "stopping = EarlyStopping(monitor=\"val_acc\", mode=\"max\", min_delta=0.005)",
            "trainer = pl.Trainer(",
            "default_root_dir=dir_path,",
            "-        gpus=int(torch.cuda.is_available()),",
            "+        devices=int(torch.cuda.is_available()),",
            "precision=(16 if torch.cuda.is_available() else 32),",
            "callbacks=[stopping],",
            "min_epochs=3,"
        ]
    },
    {
        "number": 3712,
        "comments": "",
        "commit_message": "fixed default arguments\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LogMelFbank(AbsFeatsExtract):",
            ")",
            "",
            "def forward(",
            "-        self, input: torch.Tensor, input_lengths: torch.Tensor",
            "+        self, input: torch.Tensor, input_lengths: torch.Tensor = None",
            ") -> Tuple[torch.Tensor, torch.Tensor]:",
            "# 1. Domain-conversion: e.g. Stft: time -> time-freq",
            "input_stft, feats_lens = self.stft(input, input_lengths)"
        ]
    },
    {
        "number": 3713,
        "comments": "",
        "commit_message": "Fix `NeighborLoader` tests on Windows (#5988)\n\n`pyg-lib` is not yet available on Windows, so `dtype` tests do not pass.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_homogeneous_neighbor_loader(directed, dtype):",
            "@pytest.mark.parametrize('directed', [True])  # TODO re-enable undirected mode",
            "@pytest.mark.parametrize('dtype', [torch.int64, torch.int32])",
            "def test_heterogeneous_neighbor_loader(directed, dtype):",
            "+    if dtype != torch.int64 and not _WITH_PYG_LIB:",
            "+        return",
            "+",
            "torch.manual_seed(12345)",
            "",
            "data = HeteroData()"
        ]
    },
    {
        "number": 3716,
        "comments": "",
        "commit_message": "center transform, py 2.7 fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def coalesce(edge_index, edge_attr=None, num_nodes=None):",
            "else:",
            "sparse = getattr(torch.sparse, edge_attr.type().split('.')[-1])",
            "n = num_nodes",
            "-        size = torch.Size([n, n, *list(edge_attr.size())[1:]])",
            "+        size = torch.Size([n, n] + list(edge_attr.size())[1:])",
            "adj = sparse(edge_index, edge_attr, size).coalesce()",
            "edge_index, edge_attr = adj._indices(), adj._values()"
        ]
    },
    {
        "number": 3717,
        "comments": "",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BaseDatasetTest(TestCase):",
            "def reduce_ex(self):",
            "raise pickle.PicklingError()",
            "",
            "-        nlp.arrow_dataset.logger.__reduce_ex__ = reduce_ex",
            "+        datasets.arrow_dataset.logger.__reduce_ex__ = reduce_ex",
            "",
            "def _create_dummy_dataset(self, in_memory: bool, tmp_dir: str, multiple_columns=False):",
            "if multiple_columns:"
        ]
    },
    {
        "number": 3718,
        "comments": "",
        "commit_message": "a few more doc fixes (#4078)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BiMpmMatching(nn.Module, FromParams):",
            "",
            "# Returns",
            "",
            "-        A tuple of matching vectors for the two sentences. Each of which is a list of",
            "-        matching vectors of shape (batch, seq_len, num_perspectives or 1)",
            "+        `Tuple[List[torch.Tensor], List[torch.Tensor]]` :",
            "+            A tuple of matching vectors for the two sentences. Each of which is a list of",
            "+            matching vectors of shape (batch, seq_len, num_perspectives or 1)",
            "\"\"\"",
            "assert (not mask_2.requires_grad) and (not mask_1.requires_grad)",
            "assert context_1.size(-1) == context_2.size(-1) == self.hidden_dim"
        ]
    },
    {
        "number": 3720,
        "comments": "",
        "commit_message": "bug fix for illegal memory reach\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GravesAttention(nn.Module):",
            "",
            "def init_states(self, inputs):",
            "if self.J is None or inputs.shape[1] > self.J.shape[-1]:",
            "-            self.J = torch.arange(0, inputs.shape[1]).expand_as(torch.Tensor(inputs.shape[0], self.K, inputs.shape[1])).to(inputs.device)",
            "+            self.J = torch.arange(0, inputs.shape[1]).to(inputs.device).expand([inputs.shape[0], self.K, inputs.shape[1]])",
            "self.attention_weights = torch.zeros(inputs.shape[0], inputs.shape[1]).to(inputs.device)",
            "self.mu_prev = torch.zeros(inputs.shape[0], self.K).to(inputs.device)"
        ]
    },
    {
        "number": 3722,
        "comments": "",
        "commit_message": "Test fixtures (#4241)\n\n* fix test\n\n* typo\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* skip tests\n\n* run cron only in master repo\n\n* fix test\n\n* update\n\n* Add test\n",
        "label": "",
        "answer": "no",
        "change": [
            "from torch_geometric.utils import to_dense_adj",
            "try:",
            "rowptr = torch.tensor([0, 1])",
            "col = torch.tensor([0])",
            "-    torch.ops.torch_sparse.partition(rowptr, col, None, 1)",
            "+    torch.ops.torch_sparse.partition(rowptr, col, None, 1, True)",
            "with_metis = True",
            "except RuntimeError:",
            "with_metis = False"
        ]
    },
    {
        "number": 3723,
        "comments": "",
        "commit_message": "Fix svdvals usage (#1926)\n\n* big svdvals\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def solve_pnp_dlt(",
            "# Checking if world_points_norm (of any element of the batch) has rank = 3. This",
            "# function cannot be used if all world points (of any element of the batch) lie",
            "# on a line or if all world points (of any element of the batch) lie on a plane.",
            "-    s = torch.linalg.svdvals(world_points_norm)",
            "+    s = _torch_linalg_svdvals(world_points_norm)",
            "if torch.any(s[:, -1] < svd_eps):",
            "raise AssertionError(",
            "f\"The last singular value of one/more of the elements of the batch is smaller \""
        ]
    },
    {
        "number": 3724,
        "comments": "",
        "commit_message": "[WIP] ref: deprecated results obj, added support for simpler comms (1/n) (#3681)\n\n* ref: deprecated results obj, added support for simpler comms. Decouples logging from loops\n\n* ref: deprecated results obj, added support for simpler comms. Decouples logging from loops\n\n* ref: deprecated results obj, added support for simpler comms. Decouples logging from loops\n\n* ref: deprecated results obj, added support for simpler comms. Decouples logging from loops\n\n* ref: deprecated results obj, added support for simpler comms. Decouples logging from loops\n\n* ref: deprecated results obj, added support for simpler comms. Decouples logging from loops\n\n* fix global step err\n\n* fix global step err\n\n* fix global step err\n\n* fix global step err\n\n* fix global step err\n\n* fix typing err\n\n* fix str\n\n* fix typing err\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_train_step_epoch_end_scalar(tmpdir):",
            "train_step_out = out.training_step_output_for_epoch_end",
            "assert len(train_step_out) == 1",
            "train_step_out = train_step_out[0][0]",
            "-    assert isinstance(train_step_out, torch.Tensor)",
            "-    assert train_step_out.item() == 171",
            "+    assert isinstance(train_step_out['minimize'], torch.Tensor)",
            "+    assert train_step_out['minimize'].item() == 171",
            "",
            "# make sure the optimizer closure returns the correct things",
            "opt_closure_result = trainer.train_loop.training_step_and_backward("
        ]
    },
    {
        "number": 3725,
        "comments": "",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def build_q_losses(policy, model, _, train_batch):",
            "",
            "",
            "def adam_optimizer(policy, config):",
            "-    return tf.train.AdamOptimizer(",
            "+    return tf1.train.AdamOptimizer(",
            "learning_rate=policy.cur_lr, epsilon=config[\"adam_epsilon\"])"
        ]
    },
    {
        "number": 3730,
        "comments": "",
        "commit_message": "Bug fixed (#475)\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def rgb_to_hsv(image: torch.Tensor) -> torch.Tensor:",
            "",
            "h = (h / 6.0) % 1.0",
            "",
            "-    h = 2 * pi * h",
            "+    h = 2 * pi.to(image.device) * h",
            "return torch.stack([h, s, v], dim=-3)"
        ]
    },
    {
        "number": 3731,
        "comments": "",
        "commit_message": "Move complex tensor related functions to espnet2/enh/layers/complex_utils.py; Refactor complex support code; fix unit tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DNN_Beamformer(torch.nn.Module):",
            "",
            "if isinstance(data, ComplexTensor):",
            "complex_wrapper = FC",
            "-        elif is_torch_1_8_plus and torch.is_complex(data):",
            "+        elif is_torch_1_9_plus and torch.is_complex(data):",
            "complex_wrapper = torch",
            "else:",
            "raise ValueError("
        ]
    },
    {
        "number": 3733,
        "comments": "",
        "commit_message": "Fix bugs in tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Tester(unittest.TestCase):",
            "def test_rotation_matrix_to_angle_axis_gradcheck(self):",
            "# generate input data",
            "batch_size = 2",
            "-        rmat = torch.rand(batch_size, 4, 4)",
            "+        rmat = torch.rand(batch_size, 3, 4)",
            "rmat = utils.tensor_to_gradcheck_var(rmat)  # to var",
            "",
            "# evaluate function gradient"
        ]
    },
    {
        "number": 3735,
        "comments": "",
        "commit_message": "fix imgwarp test\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def warp_perspective(src, M, dsize, flags='bilinear', border_mode=None,",
            "- Output: :math:`(B, C, H, W)`",
            "",
            ".. note::",
            "-       See a working example `here <../../../examples/warp_perspective.ipynb>`_.",
            "+       See a working example `here <https://github.com/arraiy/torchgeometry/blob/master/examples/warp_perspective.ipynb>`_.",
            "\"\"\"",
            "if not torch.is_tensor(src):",
            "raise TypeError(\"Input src type is not a torch.Tensor. Got {}\""
        ]
    },
    {
        "number": 3740,
        "comments": "",
        "commit_message": "[rllib] General RNN support (#2299)\n\n* wip\n\n* cls\n\n* re\n\n* wip\n\n* wip\n\n* a3c working\n\n* torch support\n\n* pg works\n\n* lint\n\n* rm v2\n\n* consumer id\n\n* clean up pg\n\n* clean up more\n\n* fix python 2.7\n\n* tf session management\n\n* docs\n\n* dqn wip\n\n* fix compile\n\n* dqn\n\n* apex runs\n\n* up\n\n* impotrs\n\n* ddpg\n\n* quotes\n\n* fix tests\n\n* fix last r\n\n* fix tests\n\n* lint\n\n* pass checkpoint restore\n\n* kwar\n\n* nits\n\n* policy graph\n\n* fix yapf\n\n* com\n\n* class\n\n* pyt\n\n* vectorization\n\n* update\n\n* test cpe\n\n* unit test\n\n* fix ddpg2\n\n* changes\n\n* wip\n\n* args\n\n* faster test\n\n* common\n\n* fix\n\n* add alg option\n\n* batch mode and policy serving\n\n* multi serving test\n\n* todo\n\n* wip\n\n* serving test\n\n* doc async env\n\n* num envs\n\n* comments\n\n* thread\n\n* remove init hook\n\n* update\n\n* fix ppo\n\n* comments1\n\n* fix\n\n* updates\n\n* add jenkins tests\n\n* fix\n\n* fix pytorch\n\n* fix\n\n* fixes\n\n* fix a3c policy\n\n* fix squeeze\n\n* fix trunc on apex\n\n* fix squeezing for real\n\n* update\n\n* remove horizon test for now\n\n* multiagent wip\n\n* update\n\n* fix race condition\n\n* fix ma\n\n* t\n\n* doc\n\n* st\n\n* wip\n\n* example\n\n* wip\n\n* working\n\n* cartpole\n\n* wip\n\n* batch wip\n\n* fix bug\n\n* make other_batches None default\n\n* working\n\n* debug\n\n* nit\n\n* warn\n\n* comments\n\n* fix ppo\n\n* fix obs filter\n\n* update\n\n* wip\n\n* tf\n\n* update\n\n* fix\n\n* cleanup\n\n* cleanup\n\n* spacing\n\n* model\n\n* fix\n\n* dqn\n\n* fix ddpg\n\n* doc\n\n* keep names\n\n* update\n\n* fix\n\n* com\n\n* docs\n\n* clarify model outputs\n\n* Update torch_policy_graph.py\n\n* fix obs filter\n\n* pass thru worker index\n\n* fix\n\n* rename\n\n* vlad torch comments\n\n* fix log action\n\n* debug name\n\n* fix lstm\n\n* remove unused ddpg net\n\n* remove conv net\n\n* revert lstm\n\n* wip\n\n* wip\n\n* cast\n\n* wip\n\n* works\n\n* fix a3c\n\n* works\n\n* lstm util test\n\n* doc\n\n* clean up\n\n* update\n\n* fix lstm check\n\n* move to end\n\n* fix sphinx\n\n* fix cmd\n\n* remove bad doc\n\n* clarify\n\n* copy\n\n* async sa\n\n* fix\n\n* comments\n\n* fix a3c conf\n\n* tune lstm\n\n* fix reshape\n\n* fix\n\n* back to 16\n\n* tuned a3c update\n\n* update\n\n* tuned\n\n* optional\n\n* fix catalog\n\n* remove prep\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def normc_initializer(std=1.0):",
            "return _initializer",
            "",
            "",
            "+def get_activation_fn(name):",
            "+    return getattr(tf.nn, name)",
            "+",
            "+",
            "def conv2d(x, num_filters, name, filter_size=(3, 3), stride=(1, 1), pad=\"SAME\",",
            "dtype=tf.float32, collections=None):",
            "with tf.variable_scope(name):"
        ]
    },
    {
        "number": 3741,
        "comments": "",
        "commit_message": "fixed summarizer flush\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(object):",
            "Returns:",
            "Checkpoint path where the model was saved.",
            "\"\"\"",
            "-        self.monitored_session.run(fetches=self.summarizer_flush)",
            "+        if self.flush_summarizer is not None:",
            "+            self.monitored_session.run(fetches=self.flush_summarizer)",
            "",
            "return self.saver.save(",
            "sess=self.session,"
        ]
    },
    {
        "number": 3746,
        "comments": "",
        "commit_message": "Fix import order in model, reformatting.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Evolutionary(Optimizer):",
            "diffs_list.append(diffs)",
            "",
            "with tf.control_dependencies(control_inputs=diffs):",
            "-            diffs = [tf.add_n(inputs=[diffs[n] for diffs in diffs_list]) / self.samples for n in range(len(diffs_list[0]))]",
            "+            diffs = [tf.add_n(inputs=[diffs[n] for diffs in diffs_list]) /",
            "+                     self.samples for n in range(len(diffs_list[0]))]",
            "perturbation_diffs = [diff - pert for diff, pert in zip(diffs, previous_perturbations)]",
            "applied = self.apply_step(variables=variables, diffs=perturbation_diffs)"
        ]
    },
    {
        "number": 3747,
        "comments": "",
        "commit_message": "Fix indentation in nn/batch_norm.py\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BatchNorm(base.AbstractModule):",
            "trainable=False)",
            "",
            "self._moving_variance = tf.subtract(self._moving_second_moment,",
            "-                                   tf.square(self._moving_mean),",
            "-                                   name=\"moving_variance\")",
            "+                                        tf.square(self._moving_mean),",
            "+                                        name=\"moving_variance\")",
            "",
            "def build_batch_stats():",
            "\"\"\"Builds the batch statistics calculation ops.\"\"\""
        ]
    },
    {
        "number": 3748,
        "comments": "",
        "commit_message": "Fixed a problem with  multi_step optimizer and pg_prob_ratio model (related to PPO's optimization procedure)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RunningStandardize(Preprocessor):",
            "",
            "def later_run():",
            "# Variance update",
            "-                variance = tf.reduce_sum(input_tensor=((tensor - mean_estimate) * mean), axis=0)",
            "+                variance = tf.reduce_sum(input_tensor=((tensor - mean_estimate) * mean), axis=0)  # reduce_mean?",
            "assignment = tf.assign_add(ref=variance_sum_estimate, value=variance)",
            "with tf.control_dependencies(control_inputs=(assignment,)):",
            "variance_estimate = variance_sum_estimate / (count - 1.0)"
        ]
    },
    {
        "number": 3750,
        "comments": "",
        "commit_message": "Fix integration tests for TFWav2Vec2 and TFHubert\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFWav2Vec2ModelIntegrationTest(unittest.TestCase):",
            "",
            "input_speech = self._load_datasamples(4)",
            "",
            "-        inputs = processor(input_speech, return_tensors=\"tf\", padding=True, truncation=True)",
            "+        inputs = processor(input_speech, return_tensors=\"tf\", padding=True, sampling_rate=16000)",
            "",
            "input_values = inputs.input_values",
            "attention_mask = inputs.attention_mask",
            "",
            "logits = model(input_values, attention_mask=attention_mask).logits",
            "",
            "-        predicted_ids = tf.argmax(logits, dim=-1)",
            "+        predicted_ids = tf.argmax(logits, axis=-1)",
            "predicted_trans = processor.batch_decode(predicted_ids)",
            "",
            "EXPECTED_TRANSCRIPTIONS = ["
        ]
    },
    {
        "number": 3751,
        "comments": "",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ReformerPreTrainedModel(PreTrainedModel):",
            "\"\"\"Initialize the weights\"\"\"",
            "if isinstance(module, AxialPositionEmbeddings):",
            "for weight in module.weights:",
            "-                torch.nn.init.normal_(weight, std=self.config.axial_norm_std)",
            "+                nn.init.normal_(weight, std=self.config.axial_norm_std)",
            "elif isinstance(module, nn.Embedding):",
            "module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)",
            "if module.padding_idx is not None:"
        ]
    },
    {
        "number": 3752,
        "comments": "",
        "commit_message": "Improve mnist examples and fix bugs. (#382)\n\n* simplified mnist example\n\n* simplied mnist example\n\n* simplied mnist example\n\n* simplied mnist example\n\n* simplied mnist example\n\n* IMPORTANT : fixed D_TYPE bug / as we splited the layers into many file, the D_TYPE in core.py cant change in other files\n\n* update layer config.\n\n* format code\n\n* decouple set_keep.\n\n* fix hao comments.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SpatialTransformer2dAffineLayer(Layer):",
            "# 2.1 W",
            "n_in = int(self.theta_layer.outputs.get_shape()[-1])",
            "shape = (n_in, 6)",
            "-            W = tf.get_variable(name='W', initializer=tf.zeros(shape), dtype=D_TYPE)",
            "+            W = tf.get_variable(name='W', initializer=tf.zeros(shape), dtype=LayersConfig.tf_dtype)",
            "# 2.2 b",
            "identity = tf.constant(np.array([[1., 0, 0], [0, 1., 0]]).astype('float32').flatten())",
            "-            b = tf.get_variable(name='b', initializer=identity, dtype=D_TYPE)",
            "+            b = tf.get_variable(name='b', initializer=identity, dtype=LayersConfig.tf_dtype)",
            "# 2.3 transformation matrix",
            "self.theta = tf.nn.tanh(tf.matmul(self.theta_layer.outputs, W) + b)",
            "# 3. Spatial Transformer Sampling"
        ]
    },
    {
        "number": 3753,
        "comments": "",
        "commit_message": "Bug fixed (#475)\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def rgb_to_hls(image: torch.Tensor) -> torch.Tensor:",
            "hi[imax == 1] = (((b - r) / deltac) + 2)[imax == 1]",
            "hi[imax == 2] = (((r - g) / deltac) + 4)[imax == 2]",
            "",
            "-    h: torch.Tensor = 2. * pi * (60. * hi) / 360.  # hue [0, 2*pi]",
            "+    h: torch.Tensor = 2. * pi.to(image.device) * (60. * hi) / 360.  # hue [0, 2*pi]",
            "",
            "image_hls: torch.Tensor = torch.stack([h, l, s], dim=-3)"
        ]
    },
    {
        "number": 3756,
        "comments": "",
        "commit_message": "This PR fixes the currently broken lstm_use_prev_action_reward flag for default lstm models (model.use_lstm=True). (#8970)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Policy(metaclass=ABCMeta):",
            "episodes = [episode]",
            "if state is not None:",
            "state_batch = [",
            "-                s.unsqueeze(0)",
            "-                if torch and isinstance(s, torch.Tensor) else [s]",
            "+                s.unsqueeze(0) if torch and isinstance(s, torch.Tensor) else",
            "+                np.expand_dims(s, 0)",
            "for s in state",
            "]"
        ]
    },
    {
        "number": 3757,
        "comments": "",
        "commit_message": "Fix small type hinting error (#7820)\n\n* Fix small type hinting error\n\n* Update tokenization_utils_base.py\n\n* Update src/transformers/tokenization_utils_base.py\n\nCo-authored-by: Lysandre Debut <lysandre@huggingface.co>\n\nCo-authored-by: Lysandre Debut <lysandre@huggingface.co>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BatchEncoding(UserDict):",
            "return self",
            "",
            "@torch_required",
            "-    def to(self, device: str) -> \"BatchEncoding\":",
            "+    def to(self, device: Union[str, \"torch.device\"]) -> \"BatchEncoding\":",
            "\"\"\"",
            "Send all values to device by calling :obj:`v.to(device)` (PyTorch only)."
        ]
    },
    {
        "number": 3759,
        "comments": "",
        "commit_message": "Fix small mistake `optimizer` to `optimizers`\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(base_layer.Layer, version_utils.ModelVersionSelector):",
            "Example:",
            "",
            "```python",
            "-    model.compile(optimizer=tf.keras.optimizer.Adam(learning_rate=1e-3),",
            "+    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),",
            "loss=tf.keras.losses.BinaryCrossentropy(),",
            "metrics=[tf.keras.metrics.BinaryAccuracy(),",
            "tf.keras.metrics.FalseNegatives()])"
        ]
    },
    {
        "number": 3763,
        "comments": "",
        "commit_message": "fix the warning when training (#673)\n\nchange ByteTensor to BoolTensor\n",
        "label": "",
        "answer": "no",
        "change": [
            "def outputVar(l, voc):",
            "max_target_len = max([len(indexes) for indexes in indexes_batch])",
            "padList = zeroPadding(indexes_batch)",
            "mask = binaryMatrix(padList)",
            "-    mask = torch.ByteTensor(mask)",
            "+    mask = torch.BoolTensor(mask)",
            "padVar = torch.LongTensor(padList)",
            "return padVar, mask, max_target_len"
        ]
    },
    {
        "number": 3764,
        "comments": "",
        "commit_message": "Add TF whisper (#19378)\n\n* simplify loop\n\n* add featur extractor\n\n* add model\n\n* start conversion\n\n* add dropout\n\n* initial commit of test files\n\n* copnversion for all models\n\n* update processor for correct padding\n\n* update feature extraction\n\n* update integration test logits match\n\n* fmnt: off for the logits\n\n* on the fly mel bank\n\n* small nit\n\n* update test\n\n* update tokenizer\n\n* nit feature extraction\n\n* update\n\n* update tokenizer test\n\n* adds logit processor and update tokenizer to get supress tokens\n\n* style\n\n* clean convert\n\n* revert to original modeling tf utils\n\n* Update\n\n* update\n\n* nit\n\n* clean convert file\n\n* update tests and nits\n\n* quality\n\n* slow generation test\n\n* ffn_dim to allow customization\n\n* update readme\n\n* add to toctreee\n\n* start fixing integration tests\n\n* update tests and code\n\n* fix feature extractor\n\n* fix config tests common\n\n* update code to fix tests\n\n* fix feature exctractor\n\n* nit feature extraction\n\n* update test for new feature extractor\n\n* style\n\n* add absrtact\n\n* large logits wioth custom decoder input ids\n\n* wraap around is otrch available\n\n* fix feature extractor\n\n* correct logits for whisper small.en\n\n* nit\n\n* fix encoder_attentino_mask\n\n* some fixes\n\n* remove unnecessary inputs\n\n* nits\n\n* add normalizer file\n\n* update etst tokenization\n\n* fix attention mask not defined\n\n* fix generate\n\n* remove uncoder attention mask useless\n\n* update test modeling whisper\n\n* update condfig to add second non supress tokens\n\n* nits on feature exrtactor\n\n* nit for test tokenizers\n\n* update etsts\n\n* update tests\n\n* update tokenization test\n\n* fixup\n\n* invalidated hf token. Clean convert openai to whisper\n\n* fix logit tests\n\n* fixup\n\n* Add model to README\n\n* Fix doc tests\n\n* clean merge\n\n* revert toc_tree changes\n\n* remove useless LogitProcessor\n\n* Update whisper .mdx\n\n* update config file doc\n\n* update configuration docstring\n\n* update test tokenization\n\n* update test tokenization\n\n* update tokenization whisper\nAdded copied from where needed\n\n* update feature extraction\n\n* nit test name\n\n* style\n\n* quality\n\n* remove get suppress tokens and update non_speech tokens global variables\n\n* Update src/transformers/models/whisper/feature_extraction_whisper.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* clean modeling whisper and test\nRemoved the attention mask arguments that are deprecated\n\n* fix large test\n\n* Add multilingual audio test, and translate test\n\n* style\n\n* fix larg multilingual test\n\n* nits\n\n* add copied from for attention layer\n\n* remove attention masks in doc\n\n* add english normalizer\n\n* Update docs/source/en/model_doc/whisper.mdx\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* update tokenization test\n\n* remove copied from in whisper attention : no bias in k_proj only\n\n* wrap around dependencies in english normalizer\n\n* style\n\n* correct import generation logits\n\n* for now, wrap feature extractor with torch\n\n* remove torch depencies for feature extraction and style\n\n* Update src/transformers/models/whisper/convert_openai_whisper_to_tfms.py\n\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\n\n* Update src/transformers/models/whisper/configuration_whisper.py\n\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/whisper.mdx\n\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\n\n* fixup\n\n* nit\n\n* update logitds\n\n* style\n\n* nit\n\n* nits and fix final tests\n\n* add `is_more_itertools_available` to utils\n\n* quality\n\n* add begin supress tokens, supress tokens to generate args and config\n\n* clean supressTokensLogitProcessor in generation logits\n\n* Nit naming\n\n* add supressTokensAtBegin\n\n* udpate tests, supress tokens to None or correct values\n\n* nit and style\n\n* update RAG to fit test and generate_logit\n\n* add copy pasted statment on english normalizer\n\n* add arguments to config_common_kwargs\n\n* Update src/transformers/generation_utils.py\n\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\n\n* Update src/transformers/generation_logits_process.py\n\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\n\n* revert changes based on reviews\n\n* update doc and nits\n\n* Update src/transformers/models/whisper/configuration_whisper.py\n\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\n\n* more nits\n\n* last nits\n\n* update test configuration common\n\n* add BART name in decoder attention mask documentation\n\n* Update src/transformers/models/whisper/modeling_whisper.py\n\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\n\n* style\n\n* nit\n\n* nit\n\n* add english.json file to git\n\n* nits on documentation\n\n* nit\n\n* nits\n\n* last styling\n\n* add main toctree file\n\n* remove sentence piece dependency\n\n* clean init file\n\n* fix tokenizer that has no dependencies on sentencepiece\n\n* update whisper init file, nit\n\n* remove english.json file\n\n* add get decoder prompt id\n\n* All weights loading\n\n* Remove hanging pdb\n\n* Fixup and tidy up\n\n* Use same copied from as PT model\n\n* Remove whitespace changes\n\n* Remove torch references\n\n* Tie embeddings\n\n* Remove logits processor input to generate\n\n* Update logit values\n\n* revert changes and add forced logit processor\n\n* nit\n\n* clean normalizer\n\n* remove protected\n\n* Add logit processors and update generation code & tests\n\n* Some tidy up\n\n* Update docstring\n\n* update\n\n* update based on review\n\n* Update src/transformers/models/whisper/configuration_whisper.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* Update src/transformers/models/whisper/configuration_whisper.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* Update to reflect changes on the PT model branch\n\n* Tidy up\n\n* Remove extra whitespace\n\n* Fix test - make input ids small enough we can append\n\n* Include upstream changes on main\n\n* PR comments - add batch tests, remove comments & defaults\n\n* Fix model output imports\n\n* Update src/transformers/models/whisper/modeling_tf_whisper.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/generation_tf_logits_process.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/models/whisper/modeling_tf_whisper.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/models/whisper/modeling_tf_whisper.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update tests/models/whisper/test_modeling_tf_whisper.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/models/whisper/modeling_tf_whisper.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/models/whisper/modeling_tf_whisper.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update docstring example\n\n* Update src/transformers/models/whisper/modeling_tf_whisper.py\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\n\n* Remove changes to adjust_logits_during_generation function\n\n* Update src/transformers/models/whisper/modeling_tf_whisper.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Tidy up imports that don't require TF\n\n* Update tests - skip and no more skip\n\n* Update tests/generation/test_generation_tf_logits_process.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/models/whisper/modeling_tf_whisper.py\n\n* Update src/transformers/models/whisper/modeling_tf_whisper.py\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\n\n* Add training flags\n\n* Add (skipped) XLA generation tests\n\n* Add embedding correctness test\n\n* Add constant ids for generation tests\n\n* Make logits finding a bit tidier\n\n* Remove unused args\n\n* xla generation enabled\n\n* Don't skip XLA tests anymore\n\n* Fix tests - add position ids to expected signature and update rag generation\n\n* Undo method reorder\n\n* Remove added whitespace\n\n* Remove copy-paste gradient checkopint ref\n\n* Remove\n\n* Trigger CI - (issue with refs when pulling)\n\nCo-authored-by: Arthur Zucker <arthur.zucker@gmail.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: NielsRogge <niels.rogge1@gmail.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\nCo-authored-by: Joao Gante <joao@huggingface.co>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFRagTokenForGeneration(TFRagPreTrainedModel, TFCausalLanguageModelingLoss",
            "eos_token_id=eos_token_id,",
            "forced_bos_token_id=None,",
            "forced_eos_token_id=None,",
            "+                input_ids_seq_length=tf.shape(decoder_input_ids)[-1],",
            ")",
            "model_kwargs[\"attention_mask\"] = context_attention_mask"
        ]
    },
    {
        "number": 3767,
        "comments": "",
        "commit_message": "New TF GLUE example (#12028)\n\n* Pushing partially-complete new GLUE example\n\n* First draft of the new TF GLUE example! Needs a little more testing to be sure but it's almost ready.\n\n* Fix to the fit() call\n\n* Bugfixes, making sure TPU and multi-GPU support is ready\n\n* Remove logger line that depends on Pytorch\n\n* Style pass\n\n* Deleting old TF GLUE example\n\n* Include label2id and id2label in the saved model config\n\n* Don't clobber the existing model.config.label2id\n\n* Style fixes\n\n* Update examples/tensorflow/text-classification/run_glue.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main():",
            "model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)",
            "# endregion",
            "",
            "-        # region Convert data to TF format",
            "+        # region Convert data to a tf.data.Dataset",
            "",
            "-        # Convert data to a tf.keras.utils.Sequence object for training if we're not using a TPU",
            "-        # For TPU, convert to a tf.data.Dataset",
            "tf_data = dict()",
            "max_samples = {",
            "\"train\": data_args.max_train_samples,"
        ]
    },
    {
        "number": 3768,
        "comments": "",
        "commit_message": "Fixing RNN compute_dtype in v1.\n\nIn v1, since there isn't a global policy, the layer compute_dtype will be \"_inferred\" from input, and the inferred dtype are actually populate on the cell.\n\nPiperOrigin-RevId: 394779149\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RNN(Layer):",
            "else:",
            "initial_state = self.states",
            "initial_state = tf.nest.map_structure(",
            "-          lambda v: tf.cast(v, self.compute_dtype), initial_state",
            "+          # When the layer has a inferred dtype, use the dtype from the cell.",
            "+          lambda v: tf.cast(v, self.compute_dtype or self.cell.compute_dtype),",
            "+          initial_state",
            ")",
            "elif initial_state is None:",
            "initial_state = self.get_initial_state(inputs)"
        ]
    },
    {
        "number": 3771,
        "comments": "",
        "commit_message": "Fix tests on CUDA (#2098)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestDexiNed:",
            "dtype=dtype,",
            ")",
            "",
            "-        out = model(img)",
            "-        assert_close(out[-1][:, :1], expect, atol=1e-4, rtol=1e-4)",
            "+        out = model(img)[-1]",
            "+        assert_close(out, expect, atol=3e-4, rtol=3e-4)",
            "",
            "def test_jit(self, device, dtype):",
            "op = kornia.filters.DexiNed(pretrained=False).to(device, dtype)"
        ]
    },
    {
        "number": 3774,
        "comments": "",
        "commit_message": "tutorial fixes from #336. Wheels with cuda10.1.\n\nSummary:\nAdd a document to explain how to run the tutorials.\nFix API of TexturesVertex in fit_textured_mesh.\nPrepare cuda 10.1 wheels (not 10.2) for linux to be available on pypi - this matches what colab has.\nChange the tutorials to use these new wheels.\n\nReviewed By: gkioxari\n\nDifferential Revision: D23324479\n\nfbshipit-source-id: 60e92a3f46a2d878f811b7703638f8d1dae143d9\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "\"outputs\": [],",
            "\"source\": [",
            "\"!pip install torch torchvision\\n\",",
            "-    \"!pip install 'git+https://github.com/facebookresearch/pytorch3d.git'\"",
            "+    \"import sys\\n\",",
            "+    \"if torch.__version__=='1.6.0+cu101' and sys.platform.startswith('linux'):\\n\",",
            "+    \"    !pip install pytorch3d\\n\",",
            "+    \"else:\\n\",",
            "+    \"    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\"",
            "]",
            "},",
            "{"
        ]
    },
    {
        "number": 3775,
        "comments": "",
        "commit_message": "Dog and fix features (#591)\n\n* match OpenCV DoG except two things: single iteration quad interp and no edge filtering\n\n* Added DoG to the docs\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def laf_is_inside_image(laf: torch.Tensor, images: torch.Tensor) -> torch.Tensor",
            "raise_error_if_laf_is_not_valid(laf)",
            "n, ch, h, w = images.size()",
            "pts: torch.Tensor = laf_to_boundary_points(laf, 12)",
            "-    good_lafs_mask: torch.Tensor = (pts[..., 0] >= 0) * (pts[..., 0] <= w) * (pts[..., 1] >= 0) * (pts[..., 1] <= h)",
            "+    good_lafs_mask: torch.Tensor = (pts[..., 0] >= border) *\\",
            "+        (pts[..., 0] <= w - border) *\\",
            "+        (pts[..., 1] >= border) *\\",
            "+        (pts[..., 1] <= h - border)",
            "good_lafs_mask = good_lafs_mask.min(dim=2)[0]",
            "return good_lafs_mask"
        ]
    },
    {
        "number": 3779,
        "comments": "",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NaturalGradient(Optimizer):",
            "return [tf.zeros_like(tensor=delta) for delta in deltas]",
            "",
            "# Natural gradient step only works if constant > 0",
            "-        return tf.cond(pred=(constant > 0.0), true_fn=natural_gradient_step, false_fn=zero_step)",
            "+        return self.cond(pred=(constant > 0.0), true_fn=natural_gradient_step, false_fn=zero_step)"
        ]
    },
    {
        "number": 3780,
        "comments": "",
        "commit_message": "Fix GPU device placement when calling tensor.cuda() (#624)\n\n* Correctly set cuda device when constructing cuda tensors\n\n* Fix .enumerate_support() methods\n\n* Revert to older invocation of torch.arange(-, -)\n\n* Support pytorch 0.2 invocation of .expand()\n\n* Fix type of Bernoulli sample\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class OneHotCategorical(Distribution):",
            "sample. The last dimension is used for the one-hot encoding.",
            ":rtype: torch.autograd.Variable.",
            "\"\"\"",
            "-        return Variable(torch.stack([t.expand_as(self.ps) for t in torch_eye(*self.event_shape())]))",
            "+        result = torch.stack([t.expand_as(self.ps) for t in torch_eye(*self.event_shape())])",
            "+        if self.ps.is_cuda:",
            "+            result = result.cuda(self.ps.get_device())",
            "+        return Variable(result)",
            "",
            "def analytic_mean(self):",
            "\"\"\""
        ]
    },
    {
        "number": 3781,
        "comments": "",
        "commit_message": "fix bugs for data_loading_tutorial and dcgan_faces_tutorial (#1092)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "for epoch in range(num_epochs):",
            "# Format batch",
            "real_cpu = data[0].to(device)",
            "b_size = real_cpu.size(0)",
            "-        label = torch.full((b_size,), real_label, device=device)",
            "+        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)",
            "# Forward pass real batch through D",
            "output = netD(real_cpu).view(-1)",
            "# Calculate loss on all-real batch"
        ]
    },
    {
        "number": 3785,
        "comments": "",
        "commit_message": "Fix tf boolean mask in graph mode (#6741)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFCausalLanguageModelingLoss:",
            ")",
            "# make sure only labels that are not equal to -100",
            "# are taken into account as loss",
            "-        active_loss = tf.reshape(labels, (-1,)) != -100",
            "+        active_loss = tf.not_equal(tf.reshape(labels, (-1,)), -100)",
            "reduced_logits = tf.boolean_mask(tf.reshape(logits, (-1, shape_list(logits)[2])), active_loss)",
            "labels = tf.boolean_mask(tf.reshape(labels, (-1,)), active_loss)",
            "return loss_fn(labels, reduced_logits)"
        ]
    },
    {
        "number": 3786,
        "comments": "",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFHubertPreTrainedModel(TFPreTrainedModel):",
            "input_signature=[",
            "{",
            "\"input_values\": tf.TensorSpec((None, None), tf.float32, name=\"input_values\"),",
            "-                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "-                \"token_type_ids\": tf.TensorSpec((None, None), tf.int64, name=\"token_type_ids\"),",
            "+                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "+                \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),",
            "}",
            "]",
            ")"
        ]
    },
    {
        "number": 3787,
        "comments": "",
        "commit_message": "[Compression] fix typehints (#4800)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DistMetricsCalculator(MetricsCalculator):",
            "if len(across_dim) == 0:",
            "dist_sum = torch.abs(reorder_tensor - other).sum()",
            "else:",
            "-                    dist_sum = torch.norm((reorder_tensor - other), p=self.p, dim=across_dim).sum()",
            "+                    dist_sum = torch.norm((reorder_tensor - other), p=self.p, dim=across_dim).sum()  # type: ignore",
            "# NOTE: this place need refactor when support layer level pruning.",
            "tmp_metric = metric",
            "for i in idx[:-1]:"
        ]
    },
    {
        "number": 3788,
        "comments": "",
        "commit_message": "Fix return_seq=True for dynamic RNN. (#229)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def bidirectional_rnn(incoming, rnncell_fw, rnncell_bw, return_seq=False,",
            "tf.add_to_collection(tf.GraphKeys.ACTIVATIONS, outputs[-1])",
            "",
            "if dynamic:",
            "-        outputs = tf.transpose(tf.pack(outputs), [1, 0, 2])",
            "-        o = advanced_indexing_op(outputs, sequence_length)",
            "+        if return_seq:",
            "+            o = outputs",
            "+        else:",
            "+            outputs = tf.transpose(tf.pack(outputs), [1, 0, 2])",
            "+            o = advanced_indexing_op(outputs, sequence_length)",
            "else:",
            "o = outputs if return_seq else outputs[-1]"
        ]
    },
    {
        "number": 3789,
        "comments": "",
        "commit_message": "improved global tensor handling, various other fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class EpsilonDecay(Exploration):",
            "",
            "pred = tf.logical_or(x=(timestep < self.start_timestep),",
            "y=(timestep > self.start_timestep + int(self.timesteps)))",
            "-        return tf.fill(dims=shape, value=tf.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))",
            "+        return tf.fill(dims=shape, value=self.cond(pred=pred, true_fn=true_fn, false_fn=false_fn))"
        ]
    },
    {
        "number": 3795,
        "comments": "",
        "commit_message": "Fix and improve DPG agent, more policy/value-function changes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SignatureDict(NestedDict):",
            "elif name in kwargs:",
            "arg = kwargs[name]",
            "if isinstance(spec, self.value_type):",
            "-                    assert isinstance(arg, (tf.IndexedSlices, tf.Tensor, tf.Variable)), (name, spec, arg)",
            "+                    assert isinstance(arg, (tf.IndexedSlices, tf.Tensor, tf.Variable))",
            "if isinstance(arg, tf.IndexedSlices):",
            "# spec = tf.IndexedSlicesSpec(",
            "#     shape=spec.shape, dtype=spec.dtype, indices_dtype=arg.indices.dtype"
        ]
    },
    {
        "number": 3796,
        "comments": "",
        "commit_message": "Fix init weights\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def init_seq2seq_weights(module):",
            "",
            "Defined in :numref:`sec_seq2seq`\"\"\"",
            "if type(module) == nn.Linear:",
            "-         nn.init.xavier_uniform_(layer.weight)",
            "+         nn.init.xavier_uniform_(module.weight)",
            "if type(module) == nn.GRU:",
            "-        for param in layer._flat_weights_names:",
            "+        for param in module._flat_weights_names:",
            "if \"weight\" in param:",
            "nn.init.xavier_uniform_(module._parameters[param])"
        ]
    },
    {
        "number": 3800,
        "comments": "",
        "commit_message": "fix heatconv\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class HEATConv(MessagePassing):",
            "alpha = softmax(alpha, index, ptr, size_i)",
            "alpha = F.dropout(alpha, p=self.dropout, training=self.training)",
            "",
            "-        out = self.lin(torch.cat([x_i, edge_attr], dim=-1)).unsqueeze(-2)",
            "-        out = out * alpha.unsqueeze(-1)",
            "-        return out",
            "+        out = self.lin(torch.cat([x_j, edge_attr], dim=-1)).unsqueeze(-2)",
            "+        return out * alpha.unsqueeze(-1)",
            "",
            "def __repr__(self) -> str:",
            "return (f'{self.__class__.__name__}({self.in_channels}, '"
        ]
    },
    {
        "number": 3803,
        "comments": "",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class EntropyTest(AllenNlpTestCase):",
            "def test_masked_case(self):",
            "metric = Entropy()",
            "# This would have non-zero entropy without the mask.",
            "-        logits = torch.Tensor([[1, 1, 1, 1],",
            "-                               [10000, -10000, -10000, -1000]])",
            "+        logits = torch.Tensor([[1, 1, 1, 1], [10000, -10000, -10000, -1000]])",
            "mask = torch.Tensor([0, 1])",
            "metric(logits, mask)",
            "assert metric.get_metric() == 0.0"
        ]
    },
    {
        "number": 3804,
        "comments": "",
        "commit_message": "Fix longformer attention mask type casting when using apex (#4574)\n\n* Fix longformer attention mask casting when using apex\n\n* remove extra type casting\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LongformerSelfAttention(nn.Module):",
            "]",
            "attn[extra_attention_mask_nonzeros[::-1]] = nonzero_selected_attn.view(",
            "len(selection_padding_mask_nonzeros[0]), -1",
            "-            ).type_as(hidden_states)",
            "+            )",
            "",
            "context_layer = attn.transpose(0, 1)",
            "if self.output_attentions:"
        ]
    },
    {
        "number": 3805,
        "comments": "",
        "commit_message": "Fix warning for empty tensor without type\n\nSummary:\nFairseq create an empty tensor without type.\nIt will create warning for torchscript model.\nWarning: Creating a tensor from an empty intlist will create a tensor of default floating point type  (currently Float) in python but a tensor of type int in torchscript.\n\nThis diff adds definition of the type.\n\nReviewed By: myleott\n\nDifferential Revision: D29081170\n\nfbshipit-source-id: 5c32aae65c9998b245eac43bfedc820bea509338\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NGramRepeatBlock(nn.Module):",
            "]",
            "for bbsz_idx in range(bsz * beam_size):",
            "lprobs[bbsz_idx][",
            "-                torch.tensor(banned_tokens[bbsz_idx]).long()",
            "+                torch.tensor(banned_tokens[bbsz_idx], dtype=torch.int64)",
            "] = torch.tensor(-math.inf).to(lprobs)",
            "return lprobs"
        ]
    },
    {
        "number": 3809,
        "comments": "",
        "commit_message": "Following griegler's suggestion, fixed a bug in NLayerDiscriminator that always applied sigmoid to the output\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NLayerDiscriminator(nn.Module):",
            "]",
            "",
            "sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=1)]",
            "-        sequence += [nn.Sigmoid()]",
            "+",
            "+        if use_sigmoid:",
            "+            sequence += [nn.Sigmoid()]",
            "",
            "self.model = nn.Sequential(*sequence)"
        ]
    },
    {
        "number": 3811,
        "comments": "",
        "commit_message": "[rllib] Fix TF2 import of EagerVariableStore (#5625)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DistributionalQModel(TFModelV2):",
            "return state_score",
            "",
            "if tf.executing_eagerly():",
            "+            from tensorflow.python.ops import variable_scope",
            "# Have to use a variable store to reuse variables in eager mode",
            "-            import tensorflow.contrib as tfc",
            "-            store = tfc.eager.EagerVariableStore()",
            "+            store = variable_scope.EagerVariableStore()",
            "",
            "# Save the scope objects, since in eager we will execute this",
            "# path repeatedly and there is no guarantee it will always be run"
        ]
    },
    {
        "number": 3813,
        "comments": "",
        "commit_message": "Fix vector normalization bug in semantic_similarity_with_tf_hub_unversal_encoder colab\n\nPiperOrigin-RevId: 191657545\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "\"\\n\",",
            "\"# For evaluation we use exactly normalized rather than\\n\",",
            "\"# approximately normalized.\\n\",",
            "-        \"sts_encode1 = tf.nn.l2_normalize(embed(sts_input1))\\n\",",
            "-        \"sts_encode2 = tf.nn.l2_normalize(embed(sts_input2))\\n\",",
            "-        \"sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\"",
            "+        \"sts_encode1 = tf.nn.l2_normalize(embed(sts_input1), axis=1)\\n\",",
            "+        \"sts_encode2 = tf.nn.l2_normalize(embed(sts_input2), axis=1)\\n\",",
            "+        \"sim_scores = -tf.acos(tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1))\"",
            "]",
            "},",
            "{"
        ]
    },
    {
        "number": 3817,
        "comments": "",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def nanmedian(",
            "overwrite_input: Optional[bool] = False,",
            "out: Optional[torch.tensor] = None,",
            ") -> torch.tensor:",
            "-    return torch.nanmedian(input, axis=axis, keepdims=keepdims, overwrite_input=overwrite_input, out=out)",
            "+    return torch.nanmedian(",
            "+        input, axis=axis, keepdims=keepdims, overwrite_input=overwrite_input, out=out",
            "+    )"
        ]
    },
    {
        "number": 3819,
        "comments": "",
        "commit_message": "formatting fixes for Array API submodule in Torch backend.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def cos(x: torch.Tensor)\\",
            "return torch.cos(x)",
            "",
            "",
            "-def logical_not(x: torch.Tensor) -> torch.Tensor:",
            "+def logical_not(x: torch.Tensor)\\",
            "+        -> torch.Tensor:",
            "return torch.logical_not(x.type(torch.bool))"
        ]
    },
    {
        "number": 3820,
        "comments": "",
        "commit_message": "[Feat] Jit warp perspective (#574)\n\n* Torchscriptable warp_perspective()\n\n* Simplify device and type casting\n\n* fix tests\n\n* Remove redundant parameter and update docs of warp_grid\n\n* Update docs\n\n* fix typing errors\n\n* Fixed homography warper tests\n\n* Added jit warp perspective tests\n\n* Added jit warp perspective tests\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def transform_points(trans_01: torch.Tensor,",
            ">>> trans_01 = torch.eye(4).view(1, 4, 4)  # Bx4x4",
            ">>> points_0 = kornia.transform_points(trans_01, points_1)  # BxNx3",
            "\"\"\"",
            "-    if not torch.is_tensor(trans_01) or not torch.is_tensor(points_1):",
            "-        raise TypeError(\"Input type is not a torch.Tensor\")",
            "+    check_is_tensor(trans_01)",
            "+    check_is_tensor(points_1)",
            "if not trans_01.device == points_1.device:",
            "raise TypeError(\"Tensor must be in the same device\")",
            "if not trans_01.shape[0] == points_1.shape[0] and trans_01.shape[0] != 1:"
        ]
    },
    {
        "number": 3821,
        "comments": "",
        "commit_message": "Heterogeneous Graph Support + GraphGym (#3068)\n\n* added HGT DBLP example\n\n* typo\n\n* Merge PyG master (#52)\n\n* Adding the Facebok Page-Page dataset\n\n* type hints\n\n* documentation CI\n\n* py 3.8\n\n* fix links\n\n* fix links\n\n* fail on warning\n\n* fail on warning\n\n* fix doc\n\nCo-authored-by: benedekrozemberczki <benedek.rozemberczki@gmail.com>\n\n* revert\n\n* Fix Documentation Rendering (#51)\n\n* fix doc rendering\n\n* fix linting\n\n* retrigger checks\n\n* remove pytorch 1.7.0 legacy code (#50)\n\n* Fix `copy.deepcopy` within lazy `nn.dense.Linear` (#44)\n\n* fix deepcopy within lazy Linear\n\n* fix merge\n\n* assert exception\n\n* example to doc\n\n* resolve conflict\n\n* resolve conflict\n\n* Add Figure and Equation to `to_hetero` docstring (#60)\n\n* add tex\n\n* add svg + docstring\n\n* typo\n\n* added equation\n\n* Message Passing Hooks (#53)\n\n* add hooks\n\n* docstring\n\n* add docstring\n\n* allow modification of inputs/output\n\n* add test for modifying output\n\n* add additional asserts for modifying output test\n\n* Rename `HeteroData.get_edges` and `HeteroData.get_nodes` (#58)\n\n* rename to_edges and to_nodes\n\n* typo\n\n* `HeteroConv` (#64)\n\n* clean heteroconv\n\n* init\n\n* init\n\n* clean up\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix documentation\n\n* bipartite function\n\n* fix test CI\n\n* remove pillow version\n\n* clean up for merge\n\n* Merge PyG master (#69)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\n\n* re-merge\n\n* add lazy column to GNN cheatsheet (#70)\n\n* `to_hetero_with_bases(model)` (#63)\n\n* update\n\n* fix linting\n\n* basisconv\n\n* add ValueError\n\n* to_hetero_with_bases impl done\n\n* add test\n\n* add comments\n\n* add comments\n\n* docstring\n\n* typo\n\n* update figure\n\n* svg\n\n* typo\n\n* add test\n\n* update\n\n* add rgcn equality test\n\n* typos\n\n* update\n\n* typos\n\n* update figures\n\n* generate new svgs\n\n* fix assignment\n\n* rename\n\n* delete sorted edge types\n\n* rename\n\n* add legend\n\n* fix typo\n\n* Test: Check equal outputs of `to_hetero` and `RGCNConv` (#59)\n\n* check equal output\n\n* add sparsetensor test\n\n* check equal output\n\n* add sparsetensor test\n\n* rename\n\n* linting\n\n* add missing import\n\n* `HeteroData` support for `T.NormalizeFeatures` (#56)\n\n* normalize features\n\n* allow normalization of any feature\n\n* in-place div\n\n* normalize features\n\n* allow normalization of any feature\n\n* in-place div\n\n* fix test\n\n* no need to re-assign\n\n* `HeteroData` support for `T.AddSelfLoops` (#54)\n\n* hetero support for AddSelfLoops\n\n* check for edge_index attribute\n\n* f-string\n\n* retrigger checks\n\n* revert bipartite changes\n\n* hetero support for AddSelfLoops\n\n* check for edge_index attribute\n\n* f-string\n\n* retrigger checks\n\n* revert bipartite changes\n\n* merge master\n\n* merge master\n\n* `HeteroData` support for `T.ToSparseTensor` (#55)\n\n* hetero support for ToSparseTensor\n\n* add test\n\n* customize the attribute of SparseTensor.value\n\n* rework sort_edge_index\n\n* hetero support for ToSparseTensor\n\n* add test\n\n* customize the attribute of SparseTensor.value\n\n* rework sort_edge_index\n\n* linting\n\n* `HeteroData` support for `T.ToUndirected` (#57)\n\n* to_undirected\n\n* revert bipartite changes\n\n* coalesce + undirected enhancement\n\n* merge master\n\n* revert bipartite changes\n\n* coalesce + undirected enhancement\n\n* merge master\n\n* clean up\n\n* new default relation type\n\n* fix tests\n\n* resolve merge conflicts\n\n* resolve merge conflicts 2\n\n* resolve merge conflicts 3\n\n* Merge PyG master (#74)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\n* clean up doc for to_dense_batch\n\n* clean up\n\n* add legend to cheatsheet\n\n* Improve terminology (#2837)\n\nI think the previous version of the document uses the term 'symmetric' incorrectly. A symmetric matrix is a square matrix that is is equal to its transpose (https://en.wikipedia.org/wiki/Symmetric_matrix). However, the text is only talking about the shape of the matrix, not its content. Hence, 'square (matrix)' would be the correct term to use.\n\n* Add batch_size input to to_dense_batch (#2838)\n\n* Add batch_size input to to_dense_batch\n\n* to_dense_batch fix typo in batch_size param use\n\n* add typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Added return_attention_weights to TransformerConv. (#2807)\n\n* added return_weights functionality to tranformer\n\n* added return attn weights tests\n\n* flake8\n\n* added typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* MD17 (#2843)\n\n* Added MD17 dataset\n\n* Updated Documentation\n\n* Added link to sGDML website in doc\n\n* fixed typos in doc and made train variable description clearer\n\n* clean up\n\n* fix linting\n\n* fix doc warning\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* update doc\n\n* remove forward doc\n\n* add static graph support info to cheatsheet\n\n* fix num_nodes in case edge_index is empty\n\n* fix math formula\n\n* faster GDC import\n\n* lazy import\n\n* lazy import for datasets\n\n* lazy import for nn\n\n* Sequential jittable + traceable\n\n* typo\n\n* typo\n\n* update doc\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Sch\u00e4fer <moritz.schaefer@protonmail.com>\n\n* re-add\n\n* GraphGym cleaned version (#82)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Update .gitignore\n\n* Integrate GraphGym into PyG (#85)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Integrate graphgym into pyg, keep user API in project root\n\n* fix merge conflict\n\n* fix lint errors\n\n* Make optional dependencies\n\n* merge LICENSE from GraphGym\n\n* add import\n\n* clean up LICENSE\n\n* fix import\n\n* resolve merge conflicts\n\n* resolve merge conflicts 2\n\n* Merge PyG master (#87)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\n* clean up doc for to_dense_batch\n\n* clean up\n\n* add legend to cheatsheet\n\n* Improve terminology (#2837)\n\nI think the previous version of the document uses the term 'symmetric' incorrectly. A symmetric matrix is a square matrix that is is equal to its transpose (https://en.wikipedia.org/wiki/Symmetric_matrix). However, the text is only talking about the shape of the matrix, not its content. Hence, 'square (matrix)' would be the correct term to use.\n\n* Add batch_size input to to_dense_batch (#2838)\n\n* Add batch_size input to to_dense_batch\n\n* to_dense_batch fix typo in batch_size param use\n\n* add typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Added return_attention_weights to TransformerConv. (#2807)\n\n* added return_weights functionality to tranformer\n\n* added return attn weights tests\n\n* flake8\n\n* added typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* MD17 (#2843)\n\n* Added MD17 dataset\n\n* Updated Documentation\n\n* Added link to sGDML website in doc\n\n* fixed typos in doc and made train variable description clearer\n\n* clean up\n\n* fix linting\n\n* fix doc warning\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* update doc\n\n* remove forward doc\n\n* add static graph support info to cheatsheet\n\n* fix num_nodes in case edge_index is empty\n\n* fix math formula\n\n* faster GDC import\n\n* lazy import\n\n* lazy import for datasets\n\n* lazy import for nn\n\n* Sequential jittable + traceable\n\n* typo\n\n* typo\n\n* update doc\n\n* Simple models (#2869)\n\n* Inclusion of new backbone models\n\n* Eliminating head from asap.py\n\n* small correction\n\n* Create test_gcn.py\n\n* Update __init__.py\n\n* Update test_gcn.py\n\n* Left only the convolutional simple models\n\n* Tests included\n\n* update\n\n* clean up\n\n* clean up v2\n\n* fix activation\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* Example for MemPooling. (#2729)\n\n* example for mem pooling\n\n* backprop on kl loss is done at the end of an epoch. Keys in memory layers are trained only on kl loss.\n\n* added learning rate decay. Using PROTIENS_full\n\n* flake8\n\n* reduced lr. increased weight decay\n\n* changed download location\n\n* added comments\n\n* clean up\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typos\n\n* fix removeisolatednodes transform in case 'data.num_nodes' is present\n\n* fix XConv with dilation > 1\n\n* fix XConv with dilation > 1\n\n* rgcn link prediction  (#2734)\n\n* implemented LinkPrediction dataset for loading FB15k237\n\n* implemented evaluation for relational link prediction\n\n* implemented R-GCNConf link prediction example\n\n* fixed bug: wrong initial objects in negative_sampling\n\n* changed file downloader urllib.request.urlretrieve  to pytorch.data.download_url; renamed LinkPrediction class to RelationalLinkPredictionDataset\n\n* update dataset\n\n* update example script\n\n* rename\n\nCo-authored-by: Moritz <moritzblum>\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix gnnexplainer draw kwargs\n\n* remove python-louvain dependency\n\n* allow customization of output in MP jit mode\n\n* fix test for py3.6\n\n* changed normalisation to same norm from instance norm to be robust to small var (#2917)\n\n* add CITATION.cff\n\n* format\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* add basetransform ABC (#2924)\n\n* clean up BaseTransform\n\n* clean up GATConv and add comments\n\n* add max_num_neighbors as an additional argument\n\n* fix jit GATConv on PyTorch 1.8.0\n\n* fix doc\n\n* fix gnn explainer with existing self-loops\n\n* Rgcn link pred fix (#2946)\n\n* added regularization, removed typo in test\n\n* clean up\n\nCo-authored-by: Moritz <moritzblum>\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Correct gini coefficient mathcal formula (#2932)\n\n* typo\n\n* typo\n\n* Update from_networkx (#2923)\n\n* Update from_networkx\n\n* Update test\n\n* Update convert.py\n\n* Minor corrections\n\n* Update test_convert.py\n\n* Corrections\n\n* Update test_convert.py\n\n* Case where there are no edges\n\n* Correcting how edge_attr are concatenated\n\n* clean up + new test\n\n* remove unused code\n\n* add union type\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix deterministic ordering in from_networkx\n\n* recursive-include *.jinja files\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Sch\u00e4fer <moritz.schaefer@protonmail.com>\nCo-authored-by: PabloAMC <pmorenocf@alumnos.unex.es>\nCo-authored-by: Moritz Blum <31183934+moritzblum@users.noreply.github.com>\nCo-authored-by: fbragman <fbragman@users.noreply.github.com>\nCo-authored-by: Christopher Lee <2824685+CCInc@users.noreply.github.com>\nCo-authored-by: Tim Daubensch\u00fctz <tim@daubenschuetz.de>\n\n* resolve merge conflicts 3\n\n* resolve merge conflicts 4\n\n* Implementation of the `HGTLoader` + `ogbn-mag` example (#73)\n\n* first try\n\n* update\n\n* HGT Loader\n\n* typo\n\n* first try\n\n* update\n\n* HGT Loader\n\n* typo\n\n* bugfixes\n\n* lazy GATConv\n\n* bugfix\n\n* bugfix\n\n* full working pipeline\n\n* update\n\n* rename\n\n* docstring\n\n* typos\n\n* update\n\n* typo\n\n* typo\n\n* typo\n\n* added comments\n\n* add test\n\n* add tests\n\n* fix example\n\n* rename\n\n* linting\n\n* Random split functionalities (#72)\n\n* link split\n\n* create split\n\n* example tests\n\n* link split tests\n\n* fix linting\n\n* update docstring\n\n* undirected option, refactor and docs\n\n* add num nodes as argument to neg sampling\n\n* clean up + remove single object\n\n* update example\n\n* typo\n\n* fix compose\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* add basetransform\n\n* typo\n\n* typo\n\n* fix test\n\n* Improve `torch_geometric.data` Documentation (#98)\n\n* update data doc\n\n* typo\n\n* typo\n\n* note\n\n* typo\n\n* add docstring\n\n* only show inherited members for data and hetero_data\n\n* documentation update for batch and dataset\n\n* update doc\n\n* update\n\n* fix\n\n* record_stream\n\n* update\n\n* typo\n\n* add/fix data functionality\n\n* linting\n\n* typo\n\n* `_parent` memory leak fix (#103)\n\n* memory leak fix\n\n* Clean up\n\n* clean up\n\n* bugfix tests\n\n* typos\n\n* fix test\n\n* fix test\n\n* rename reverse\n\n* (Heterogeneous) `NeighborLoader` (#92)\n\n* initial commit\n\n* typo\n\n* neighbor loader functionality + tests\n\n* docstring\n\n* fix docstring\n\n* skip tests\n\n* fix share_memory_\n\n* typo\n\n* typo\n\n* update example\n\n* typo\n\n* share_strategy\n\n* fix cuda calls\n\n* better print\n\n* fix size\n\n* fix print\n\n* final commit\n\n* fix\n\n* some todos\n\n* preprocessed features\n\n* fix to_undirected\n\n* more documentation\n\n* update doc\n\n* fix doc\n\n* fix doc\n\n* Add benchmark code and the example with existing graph classification examples (#93)\n\n* add benchmarking utilities\n\n* update graph classification benchmark\n\n* improve code style\n\n* add pytorch-memlab for benchmark code\n\n* skip some tests when cuda is not available\n\n* add type hint when appropriate\n\n* add seed_everything to improve code\n\n* code refactoring\n\n* code refactoring\n\n* code refactoring\n\n* code improvement\n\n* remove unnecessary dataloader import\n\n* change benchmark interface with decorator\n\n* documentation improvement\n\n* linting\n\n* linting part 2\n\n* linting part 3\n\n* seed_everything\n\n* create utils file\n\n* update\n\n* use utils functions\n\n* fix test\n\n* update the profiler to the latest torch (1.8.1+)\n\n* refactor profiler and add more documentation\n\n* refactor profiler and add more documentation\n\n* resolve lint errors\n\n* resolve lint errors\n\n* update\n\n* clean up test and profile\n\n* fix linting\n\n* add to doc\n\n* fix doc\n\n* typo\n\n* update benchmark\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* Move `HGTLoader` to `torch_geometric.loader` + clean up (#104)\n\n* move files\n\n* use utils functions\n\n* fix example\n\n* update\n\n* fix tests\n\n* fix seed\n\n* fix linear test\n\n* rename\n\n* Support GraphGym custom modules outside PyG package (#102)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Integrate graphgym into pyg, keep user API in project root\n\n* fix merge conflict\n\n* fix lint errors\n\n* Make optional dependencies\n\n* merge LICENSE from GraphGym\n\n* Enable adding GraphGym customized modules outside PyG package\n\n* lint\n\n* Rename `AddTrainValTestMask` to `RandomNodeSplit` (#108)\n\n* initial commit\n\n* rename example\n\n* remove AddTrainValTestMask\n\n* fix linting\n\n* create optimizer config and scheduler config separately (#113)\n\n* create optimizer config and scheduler config separately\n\n* fix format\n\n* import explicitly\n\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\n\n* Heterogeneous Graph Tutorial (#83)\n\n* add HG tutorial roadmap\n\n* started working on hg tutorial\n\n* hg_tutorial, some text and .tex figure\n\n* added svg\n\n* hg tutorial content\n\n* fix CI\n\n* text and structure\n\n* finished first draft\n\n* fixed one code example\n\n* fixing conventions\n\n* fixing links\n\n* update svg\n\n* some smaller improvements of tutorial\n\n* improvements on tutorial\n\n* hg-tutorial: fixed compiling issue, added detailed content\n\n* added absolute links\n\n* fixed warnings\n\n* streamlined dataset section\n\n* update svg\n\n* update tutorial\n\n* update 2\n\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\n\n* typo\n\n* Move data loaders to `torch_geometric.loader` (#110)\n\n* move graphsaint\n\n* deprecations\n\n* move clusterloader\n\n* deprecations\n\n* type hints\n\n* move shadow\n\n* typo\n\n* typo\n\n* move datalistloader\n\n* dense data loader\n\n* random node sampler\n\n* fix doc\n\n* Lazy GNN operators (#89)\n\n* lazy cheb conv\n\n* lazy GraphConv\n\n* lazy GATv2Conv\n\n* lazy TAGConv\n\n* lazy FAConv\n\n* lazy FeaStConv\n\n* lazy NNConv\n\n* typo\n\n* fix tests\n\n* lazy SuperGATConv\n\n* lazy SuperGATConv fix\n\n* lazy SplineConv\n\n* fix lazy check\n\n* lazy GravNetConv\n\n* arma conv lazy\n\n* dense linear in gmmconv\n\n* typo\n\n* add test\n\n* lazy GMMConv\n\n* doc\n\n* rename (#116)\n\n* Revisit `MetaPath2Vec` (#114)\n\n* revisit metapath2vec\n\n* update\n\n* typo\n\n* update\n\n* fix doc\n\n* update\n\n* check for attributes rather than key\n\n* Clean up `torch_geometric.profile` further (#111)\n\n* remove print_layer_stats\n\n* typos\n\n* update\n\n* readme highlights and quick tour (#99)\n\n* readme highlights and quick tour\n\n* arch\n\n* arch image\n\n* arch overview\n\n* list categories\n\n* categorization\n\n* category description\n\n* Update README.md\n\nfrom Matthias\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* improved highlights\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* minor\n\n* update readme\n\n* update\n\n* update\n\n* update\n\n* update\n\n* fix url\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* move ops\n\n* toc\n\n* typo\n\n* typo\n\n* add svgs\n\n* update figure\n\n* fix links\n\n* fix size\n\n* fix size\n\n* typo\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* fix broken links\n\n* fix links\n\n* Heterogeneous Graph Sampler Tutorial (#117)\n\n* initial commit\n\n* address comments\n\n* remove todo\n\n* typo\n\n* Conversion between heterogenous and homogeneous graph objects (#115)\n\n* temp checkpoint (wip, will remove)\n\n* (wip) typed graph conversion\n\n* (wip) typed graph conversion\n\n* (wip) typed graph conversion\n\n* update\n\n* typo\n\n* delete examples\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix test\n\n* update doc\n\n* deprecate NeighborSampler (#119)\n\n* Move `torch_geometric.data.DataLoader` to `torch_geometric.loader.DataLoader` (#120)\n\n* move dataloader\n\n* rename\n\n* typos\n\n* typos\n\n* fix __cat_dim__\n\n* updategp\n\n* Deprecate `train_test_split_edges` + Modifications to `RandomLinkSplit` (#121)\n\n* deprecate train_test_split_edges\n\n* to device transform\n\n* fix example\n\n* add split_labels argument\n\n* fix autoencoder example\n\n* typos\n\n* add docstring\n\n* ARGVA\n\n* seal\n\n* adress comments\n\n* Create example to load `*.csv` and transfer to `HeteroData` (#76)\n\n* create example to load csv file and transfer to heter-data\n\n* add ipython notebook version load csv with documentation\n\n* address comment\n\n* first version of csv loading doc\n\n* first version of csv loading doc\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestions csv tutorial\n\n* example script load csv + extract fix\n\n* fixed edge index stacking dimension in example and jupyter nb\n\n* linting\n\n* linting2\n\n* rename\n\n* update\n\n* update\n\n* update\n\n* typo\n\n* typo\n\n* update\n\n* rename\n\n* update tutorial\n\n* typo\n\n* address comments\n\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* fix\n\n* typo\n\n* update\n\n* fix\n\n* fix\n\nCo-authored-by: benedekrozemberczki <benedek.rozemberczki@gmail.com>\nCo-authored-by: Rex Ying <rexying@stanford.edu>\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Sch\u00e4fer <moritz.schaefer@protonmail.com>\nCo-authored-by: Jiaxuan <youjiaxuan@gmail.com>\nCo-authored-by: PabloAMC <pmorenocf@alumnos.unex.es>\nCo-authored-by: Moritz Blum <31183934+moritzblum@users.noreply.github.com>\nCo-authored-by: fbragman <fbragman@users.noreply.github.com>\nCo-authored-by: Christopher Lee <2824685+CCInc@users.noreply.github.com>\nCo-authored-by: Tim Daubensch\u00fctz <tim@daubenschuetz.de>\nCo-authored-by: Yue Zhao <yzhao062@gmail.com>\nCo-authored-by: Dong Wang <dongw89@gmail.com>\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_metapath2vec():",
            "z = model('author', torch.arange(2))",
            "assert z.size() == (2, 16)",
            "",
            "-    pos_rw, neg_rw = model.sample(torch.arange(3))",
            "+    pos_rw, neg_rw = model._sample(torch.arange(3))",
            "",
            "loss = model.loss(pos_rw, neg_rw)",
            "assert 0 <= loss.item()"
        ]
    },
    {
        "number": 3825,
        "comments": "",
        "commit_message": "Fix attention shape\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AttentionDecoder(RNNDecoder):",
            "logits=self.vocab_size,",
            "predicted_ids=tf.TensorShape([]),",
            "cell_output=self.cell.output_size,",
            "-        attention_scores=tf.concat(",
            "-            [[0], tf.shape(self.attention_values)[1:-1]], 0),",
            "+        attention_scores=tf.shape(self.attention_values)[1:-1],",
            "attention_context=self.attention_values.get_shape()[-1])",
            "",
            "@property"
        ]
    },
    {
        "number": 3828,
        "comments": "",
        "commit_message": "fix coalesce on CUDA\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def coalesce(",
            "return edge_index",
            "",
            "dim_size = edge_index.size(1)",
            "-    idx = torch.arange(0, nnz).sub_(mask.logical_not_().cumsum(dim=0))",
            "+    idx = torch.arange(0, nnz, device=edge_index.device)",
            "+    idx.sub_(mask.logical_not_().cumsum(dim=0))",
            "",
            "if isinstance(edge_attr, Tensor):",
            "edge_attr = scatter(edge_attr, idx, 0, None, dim_size, reduce)"
        ]
    },
    {
        "number": 3830,
        "comments": "",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SparseGPRegression(GPModel):",
            "W_Dinv = W / D",
            "K = W_Dinv.matmul(W.t()).contiguous()",
            "K.view(-1)[::M + 1] += 1  # add identity matrix to K",
            "-        L = K.cholesky()",
            "+        L = torch.linalg.cholesky(K)",
            "",
            "# get y_residual and convert it into 2D tensor for packing",
            "y_residual = self.y - self.mean_function(self.X)"
        ]
    },
    {
        "number": 3831,
        "comments": "",
        "commit_message": "\ud83d\udd27 Fix \ud83d\udc1b #109, don't call _build() if you want save into pb.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFFastSpeech2(TFFastSpeech):",
            "duration_outputs = self.duration_predictor(",
            "[last_encoder_hidden_states, speaker_ids, attention_mask]",
            ")  # [batch_size, length]",
            "-        duration_outputs = tf.math.exp(duration_outputs) - 1.0",
            "+        duration_outputs = tf.nn.relu(tf.math.exp(duration_outputs) - 1.0)",
            "duration_outputs = tf.cast(",
            "tf.math.round(duration_outputs * speed_ratios), tf.int32",
            ")"
        ]
    },
    {
        "number": 3832,
        "comments": "",
        "commit_message": "Fix save spec bug with the call function kwargs.\n\nPiperOrigin-RevId: 422951302\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Layer(tf.Module, version_utils.LayerVersionSelector):",
            "flat_specs = [tf_utils.get_tensor_spec(x) for x in flat_kwarg]",
            "if any(s is None for s in flat_specs):",
            "continue",
            "-      kwargs[key] = args_spec.append(",
            "-          tf.nest.pack_sequence_as(kwarg, flat_specs))",
            "+      kwargs_spec[key] = tf.nest.pack_sequence_as(kwarg, flat_specs)",
            "",
            "self._saved_model_inputs_spec = inputs_spec",
            "self._saved_model_arg_spec = ([inputs_spec] + args_spec, kwargs_spec)"
        ]
    },
    {
        "number": 3833,
        "comments": "",
        "commit_message": "fix dropout configuration\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TextCNN(object):",
            "# \u5168\u8fde\u63a5\u5c42\uff0c\u540e\u9762\u63a5dropout\u4ee5\u53carelu\u6fc0\u6d3b",
            "fc = tf.layers.dense(gmp, self.config.hidden_dim, name='fc1')",
            "fc = tf.contrib.layers.dropout(fc,",
            "-                self.config.dropout_keep_prob)",
            "+                self.keep_prob)",
            "fc = tf.nn.relu(fc)",
            "",
            "# \u5206\u7c7b\u5668"
        ]
    },
    {
        "number": 3837,
        "comments": "",
        "commit_message": "fix D413\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def pixel_wise_softmax(x, name='pixel_wise_softmax'):",
            "References",
            "- `tf.reverse <https://www.tensorflow.org/versions/master/api_docs/python/array_ops.html#reverse>`_",
            "+",
            "\"\"\"",
            "with tf.name_scope(name):",
            "return tf.nn.softmax(x)"
        ]
    },
    {
        "number": 3842,
        "comments": "",
        "commit_message": "fix lint error\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class InverseDepthSmoothnessLoss(nn.Module):",
            "assert len(img.shape) == 4, img.shape",
            "return img[:, :, :-1, :] - img[:, :, 1:, :]",
            "",
            "-    def forward(self, idepth: torch.Tensor, image: torch.Tensor) -> torch.Tensor:",
            "+    def forward(",
            "+            self,",
            "+            idepth: torch.Tensor,",
            "+            image: torch.Tensor) -> torch.Tensor:",
            "if not torch.is_tensor(idepth):",
            "raise TypeError(\"Input idepth type is not a torch.Tensor. Got {}\"",
            ".format(type(idepth)))"
        ]
    },
    {
        "number": 3843,
        "comments": "",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFLayoutLMv3PreTrainedModel(TFPreTrainedModel):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "-                \"bbox\": tf.TensorSpec((None, None, 4), tf.int64, name=\"bbox\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "+                \"bbox\": tf.TensorSpec((None, None, 4), tf.int32, name=\"bbox\"),",
            "\"pixel_values\": tf.TensorSpec((None, None, None, None), tf.float32, name=\"pixel_values\"),",
            "-                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "+                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "}",
            "]",
            ")"
        ]
    },
    {
        "number": 3846,
        "comments": "",
        "commit_message": "Fix inference on cpu device (#241)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Tacotron(nn.Module):",
            "with open(path, \"a\") as f:",
            "print(msg, file=f)",
            "",
            "-    def load(self, path, optimizer=None):",
            "+    def load(self, path, device, optimizer=None):",
            "# Use device of model params as location for loaded state",
            "-        checkpoint = torch.load(str(path))",
            "+        checkpoint = torch.load(str(path), map_location=device)",
            "self.load_state_dict(checkpoint[\"model_state\"], strict=False)",
            "",
            "if \"optimizer_state\" in checkpoint and optimizer is not None:"
        ]
    },
    {
        "number": 3847,
        "comments": "",
        "commit_message": "Fix ddp tests + .test() (#2512)\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* fix deprecation warnings\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* Update pytorch_lightning/trainer/trainer.py\n\nCo-authored-by: Jeremy Jordan <13970565+jeremyjordan@users.noreply.github.com>\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\n* added base tests for tpu\n\nCo-authored-by: Jirka <jirka@pytorchlightning.ai>\nCo-authored-by: Jeremy Jordan <13970565+jeremyjordan@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DummyDataset(Dataset):",
            "return 12",
            "",
            "def __getitem__(self, idx):",
            "-        return np.array([0.5, 1.0, 2.0])",
            "+        return torch.tensor([0.5, 1.0, 2.0])"
        ]
    },
    {
        "number": 3849,
        "comments": "",
        "commit_message": "Fixed failing test for elementwise floormod (#4187)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "floor_divide.support_native_out = True",
            "",
            "",
            "def floormod(",
            "-    x: torch.Tensor, y: torch.Tensor, *, out: Optional[torch.Tensor] = None",
            "+    x: torch.Tensor, y: torch.Tensor, /, *, out: Optional[torch.Tensor] = None",
            ") -> torch.Tensor:",
            "+    x, y = ivy.promote_types_of_inputs(x, y)",
            "return x % y"
        ]
    },
    {
        "number": 3856,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestAutoRegressiveSeqDecoder(AllenNlpTestCase):",
            ").eval()",
            "",
            "encoded_state = torch.randn(batch_size, time_steps, decoder_inout_dim)",
            "-        source_mask = torch.ones(batch_size, time_steps).long()",
            "+        source_mask = torch.ones(batch_size, time_steps).bool()",
            "target_tokens = {\"tokens\": {\"tokens\": torch.ones(batch_size, time_steps).long()}}",
            "-        source_mask[0, 1:] = 0",
            "+        source_mask[0, 1:] = False",
            "encoder_out = {\"source_mask\": source_mask, \"encoder_outputs\": encoded_state}",
            "",
            "auto_regressive_seq_decoder.forward(encoder_out, target_tokens)"
        ]
    },
    {
        "number": 3861,
        "comments": "",
        "commit_message": "Fix distributed/rpc/rnn unused import, typos, formatting (#875)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RNNModel(nn.Module):",
            "# pass input to the remote embedding table and fetch emb tensor back",
            "emb = _remote_method(EmbeddingTable.forward, self.emb_table_rref, input)",
            "output, hidden = self.rnn(emb, hidden)",
            "-        # pass output to the rremote decoder and get the decoded output back",
            "+        # pass output to the remote decoder and get the decoded output back",
            "decoded = _remote_method(Decoder.forward, self.decoder_rref, output)",
            "return decoded, hidden"
        ]
    },
    {
        "number": 3863,
        "comments": "",
        "commit_message": "Add loss evaluator (#678)\n\n* Fix license in setup.py\n\n* Add code for loss evaluator\n\n* Configs support loss evaluator\n\n* Fix a little bug\n\n* Fix flake8\n\n* return revised bbox to reg\n\n* return revised bbox to reg\n\n* revision according to comments\n\n* fix flake8\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class FCNMaskHead(nn.Module):",
            "def loss(self, mask_pred, mask_targets, labels):",
            "loss = dict()",
            "if self.class_agnostic:",
            "-            loss_mask = mask_cross_entropy(mask_pred, mask_targets,",
            "-                                           torch.zeros_like(labels))",
            "+            loss_mask = self.loss_mask(mask_pred, mask_targets,",
            "+                                       torch.zeros_like(labels))",
            "else:",
            "-            loss_mask = mask_cross_entropy(mask_pred, mask_targets, labels)",
            "+            loss_mask = self.loss_mask(mask_pred, mask_targets, labels)",
            "loss['loss_mask'] = loss_mask",
            "return loss"
        ]
    },
    {
        "number": 3864,
        "comments": "",
        "commit_message": "Small fix for Cuda Torch DQN. (#10177)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def build_q_losses(policy, model, _, train_batch):",
            "q_tp1_best_one_hot_selection = F.one_hot(q_tp1_best_using_online_net,",
            "policy.action_space.n)",
            "q_tp1_best = torch.sum(",
            "-            torch.where(q_tp1 > -float(\"inf\"), q_tp1, torch.tensor(0.0)) *",
            "+            torch.where(q_tp1 > -float(\"inf\"), q_tp1,",
            "+                        torch.tensor(0.0, device=policy.device)) *",
            "q_tp1_best_one_hot_selection, 1)",
            "q_probs_tp1_best = torch.sum(",
            "q_probs_tp1 * torch.unsqueeze(q_tp1_best_one_hot_selection, -1), 1)"
        ]
    },
    {
        "number": 3865,
        "comments": "",
        "commit_message": "[refactor] Training of densehead (#6315)\n\n* Refactor one-stage get_bboxes logic (#5317)\n\n* revert batch to single\n\n* update anchor_head\n\n* replace preds with bboxes\n\n* add point_bbox_coder\n\n* FCOS add get_selected_priori\n\n* unified anchor-free and anchor-based get_bbox_single\n\n* update code\n\n* update reppoints and sabl\n\n* add sparse priors\n\n* add mlvlpointsgenerator\n\n* revert __init__ of core\n\n* refactor reppoints\n\n* delete label channal\n\n* add docstr\n\n* fix typo\n\n* fix args\n\n* fix typo\n\n* fix doc\n\n* fix stride_h\n\n* add offset\n\n* Unified bbox coder\n\n* add offset\n\n* remove point_bbox_coder.py\n\n* fix docstr\n\n* new interface of single_proir\n\n* fix device\n\n* add unitest\n\n* add cuda unitest\n\n* add more cuda unintest\n\n* fix reppoints\n\n* fix device\n\n* update all prior\n\n* update vfnet\n\n* add unintest for ssd and yolo and rename prior_idxs\n\n* add docstr for MlvlPointGenerator\n\n* update reppoints and rpnhead\n\n* add space\n\n* add num_base_priors\n\n* update some model\n\n* update docstr\n\n* fixAugFPN test and lint.\n\n* Fix autoassign\n\n* add docs\n\n* Unified fcos decoding\n\n* update docstr\n\n* fix train error\n\n* Fix Vfnet\n\n* Fix some\n\n* update centernet\n\n* revert\n\n* add warnings\n\n* fix unittest error\n\n* delete duplicated\n\n* fix comment\n\n* fix docs\n\n* fix type\n\nCo-authored-by: zhangshilong <2392587229zsl@gmail.com>\n\n* support onnx export for fcos\n\n* support onnx export for fcos fsaf retina and ssd\n\n* resolve comments\n\n* resolve comments\n\n* add with nms\n\n* support cornernet\n\n* resolve comments\n\n* add default with nms\n\n* fix trt arrange should be int\n\n* refactor anchor head anchor free head\n\n* add dtype to single_level_grid_priors\n\n* atss fcos autoassign\n\n* fovea\n\n* fsaf free anchor\n\n* suport more\n\n* suport more\n\n* support all\n\n* resolve conversation\n\n* fix point generator\n\n* fix device\n\n* change to distancecoder\n\n* resolve conversation\n\n* fix grid prior\n\n* fix typos in autoassgin\n\n* fix typos\n\n* fix doc\n\nCo-authored-by: Haian Huang(\u6df1\u5ea6\u7738) <1286304229@qq.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RPNHead(AnchorHead):",
            "self.rpn_conv = nn.Conv2d(",
            "self.in_channels, self.feat_channels, 3, padding=1)",
            "self.rpn_cls = nn.Conv2d(self.feat_channels,",
            "-                                 self.num_anchors * self.cls_out_channels, 1)",
            "-        self.rpn_reg = nn.Conv2d(self.feat_channels, self.num_anchors * 4, 1)",
            "+                                 self.num_base_priors * self.cls_out_channels,",
            "+                                 1)",
            "+        self.rpn_reg = nn.Conv2d(self.feat_channels, self.num_base_priors * 4,",
            "+                                 1)",
            "",
            "def forward_single(self, x):",
            "\"\"\"Forward feature map of a single scale level.\"\"\""
        ]
    },
    {
        "number": 3867,
        "comments": "",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "msort_support_native_out = True",
            "",
            "# lexsort",
            "def lexsort(",
            "-    keys: torch.Tensor,",
            "-    /,",
            "-    *,",
            "-    axis: int = -1,",
            "-    out: Optional[torch.Tensor] = None",
            "+    keys: torch.Tensor, /, *, axis: int = -1, out: Optional[torch.Tensor] = None",
            ") -> torch.Tensor:",
            "shape = keys.size()",
            "if len(shape) == 1:",
            "_, result = torch.sort(keys, dim=axis, stable=True)",
            "return result",
            "if shape[0] == 0:",
            "-        raise TypeError('need sequence of keys with len > 0 in lexsort')",
            "+        raise TypeError(\"need sequence of keys with len > 0 in lexsort\")",
            "if len(shape) == 2 and shape[1] == 1:",
            "return torch.tensor([0])",
            "_, result = torch.sort(keys[0], dim=axis, stable=True)"
        ]
    },
    {
        "number": 3868,
        "comments": "",
        "commit_message": "Fix metric state reset (#5273)\n\n* Fix metric state reset\n\n* Fix test\n\n* Improve formatting\n\nCo-authored-by: Ananya Harsh Jha <ananya@pytorchlightning.ai>\n(cherry picked from commit 4913cbb987a0516f8b33c016134b19c0588d107a)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Metric(nn.Module, ABC):",
            "\"\"\"",
            "for attr, default in self._defaults.items():",
            "current_val = getattr(self, attr)",
            "-            if isinstance(current_val, torch.Tensor):",
            "+            if isinstance(default, torch.Tensor):",
            "setattr(self, attr, deepcopy(default).to(current_val.device))",
            "else:",
            "setattr(self, attr, deepcopy(default))"
        ]
    },
    {
        "number": 3869,
        "comments": "",
        "commit_message": "Bug fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DistributedFusedAdam(torch.optim.Optimizer):",
            "if torch.distributed.get_rank() in ranks:",
            "self._rs_pg.append(grp)",
            "if self._compute_L2_grad_norm and torch.distributed.get_rank() in ranks:",
            "-                #self._l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)",
            "-                self._l2_grad_norm_pg = self._rs_pg[-1]",
            "+                self._l2_grad_norm_pg = torch.distributed.new_group(ranks=ranks)",
            "self._rs_st = [torch.cuda.Stream() for _ in range(self._num_rs_pg)]",
            "if self._num_ag_pg == 0:",
            "self._ag_pg = self._rs_pg"
        ]
    },
    {
        "number": 3870,
        "comments": "",
        "commit_message": "Fix tests in networks_test.py\n\ntf.zeros_initializer is now a function that needs to be called to get\nthe initializer.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class KernelDeepLSTMTest(tf.test.TestCase):",
            "@parameterized.expand([",
            "[\"zeros\"],",
            "[{\"w\": \"zeros\", \"b\": \"zeros\", \"bad\": \"bad\"}],",
            "-      [{\"w\": tf.zeros_initializer, \"b\": np.array([0])}],",
            "-      [{\"linear\": {\"w\": tf.zeros_initializer, \"b\": \"zeros\"}}]",
            "+      [{\"w\": tf.zeros_initializer(), \"b\": np.array([0])}],",
            "+      [{\"linear\": {\"w\": tf.zeros_initializer(), \"b\": \"zeros\"}}]",
            "])",
            "def testResults(self, initializer):",
            "\"\"\"Tests zero updates when last layer is initialized to zero.\"\"\""
        ]
    },
    {
        "number": 3874,
        "comments": "",
        "commit_message": "fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def negative_sampling(edge_index, num_nodes=None, num_neg_samples=None,",
            "",
            "if force_undirected:",
            "# (-sqrt((2 * N + 1)^2 - 8 * perm) + 2 * N + 1) / 2",
            "-        row = torch.floor((-torch.sqrt((2 * num_nodes + 1)**2 - 8 * perm) +",
            "+        row = torch.floor((-torch.sqrt((2. * num_nodes + 1.)**2 - 8. * perm) +",
            "2 * num_nodes + 1) / 2)",
            "col = perm - row * (2 * num_nodes - row - 1) // 2",
            "neg_edge_index = torch.stack([row, col], dim=0).long()"
        ]
    },
    {
        "number": 3877,
        "comments": "",
        "commit_message": "Fix when _stable_1d_sort to work when n >= N (#6177)\n\n* Fix when _stable_1d_sort to work when n >= N\n\n* Apply suggestions\n\nCo-authored-by: Carlos Mocholi <carlossmocholi@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _stable_1d_sort(x: torch, N: int = 2049):",
            "n = x.numel()",
            "if N - n > 0:",
            "x_max = x.max()",
            "-        x_pad = torch.cat([x, (x_max + 1) * torch.ones(2049 - n, dtype=x.dtype, device=x.device)], 0)",
            "-    x_sort = x_pad.sort()",
            "-    return x_sort.values[:n], x_sort.indices[:n]",
            "+        x = torch.cat([x, (x_max + 1) * torch.ones(N - n, dtype=x.dtype, device=x.device)], 0)",
            "+    x_sort = x.sort()",
            "+    i = min(N, n)",
            "+    return x_sort.values[:i], x_sort.indices[:i]"
        ]
    },
    {
        "number": 3880,
        "comments": "",
        "commit_message": "Tf model outputs (#6247)\n\n* TF outputs and test on BERT\n\n* Albert to DistilBert\n\n* All remaining TF models except T5\n\n* Documentation\n\n* One file forgotten\n\n* TF outputs and test on BERT\n\n* Albert to DistilBert\n\n* All remaining TF models except T5\n\n* Documentation\n\n* One file forgotten\n\n* Add new models and fix issues\n\n* Quality improvements\n\n* Add T5\n\n* A bit of cleanup\n\n* Fix for slow tests\n\n* Style\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def _prepare_output_docstrings(output_type, config_class):",
            "",
            "# Add the return introduction",
            "full_output_type = f\"{output_type.__module__}.{output_type.__name__}\"",
            "-    intro = RETURN_INTRODUCTION.format(full_output_type=full_output_type, config_class=config_class)",
            "+    intro = TF_RETURN_INTRODUCTION if output_type.__name__.startswith(\"TF\") else PT_RETURN_INTRODUCTION",
            "+    intro = intro.format(full_output_type=full_output_type, config_class=config_class)",
            "return intro + docstrings"
        ]
    },
    {
        "number": 3881,
        "comments": "",
        "commit_message": "fix ci check(2): missed over-indent\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train(args):",
            "rho=0.95, eps=args.eps,",
            "weight_decay=args.weight_decay)",
            "elif args.opt == 'adam':",
            "-            optimizer = torch.optim.Adam(params,",
            "-                                         weight_decay=args.weight_decay)",
            "+        optimizer = torch.optim.Adam(params,",
            "+                                     weight_decay=args.weight_decay)",
            "else:",
            "raise NotImplementedError(\"unknown optimizer: \" + args.opt)"
        ]
    },
    {
        "number": 3886,
        "comments": "",
        "commit_message": "Full GraphGym + PyG integration (#3076)\n\n* Full GraphGym + PyG integration\n\n* fix torch.allclose\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_copy_linear(lazy):",
            "assert torch.allclose(copied_lin.weight, lin.weight)",
            "assert id(copied_lin.bias) != id(lin.bias)",
            "assert copied_lin.bias.data_ptr() != lin.bias.data_ptr()",
            "-    assert torch.allclose(copied_lin.bias, lin.bias)",
            "+    assert torch.allclose(copied_lin.bias, lin.bias, atol=1e-6)"
        ]
    },
    {
        "number": 3887,
        "comments": "",
        "commit_message": "Fix CI errors for torch 1.9.1\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_neural_beamformer_bf_output(",
            "assert others[\"mask_spk{}\".format(n)].shape[-2] == ch",
            "assert specs[n - 1].shape == others[\"mask_spk{}\".format(n)][..., 0, :].shape",
            "assert specs[n - 1].shape == input_spectrum[..., 0, :].shape",
            "-        if is_torch_1_9_plus and torch.is_complex(specs[n - 1]):",
            "+        if is_torch_complex_tensor(specs[n - 1]):",
            "assert specs[n - 1].dtype == torch.complex64",
            "else:",
            "assert specs[n - 1].dtype == torch.float"
        ]
    },
    {
        "number": 3889,
        "comments": "",
        "commit_message": "[RLlib] Bug fix: when on GPU, sample_batch.to_device() only converts the device and does not convert float64 to float32. (#25460)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SampleBatch(dict):",
            "assert torch is not None",
            "for k, v in self.items():",
            "if isinstance(v, np.ndarray) and v.dtype != object:",
            "-                    self[k] = torch.from_numpy(v).to(device)",
            "+                    self[k] = convert_to_torch_tensor(v, device)",
            "else:",
            "raise NotImplementedError",
            "return self"
        ]
    },
    {
        "number": 3890,
        "comments": "",
        "commit_message": "Empty val batch CUDA device fix (#7539)\n\nVerified fix for https://github.com/ultralytics/yolov5/pull/7525#issuecomment-1106081123\n",
        "label": "",
        "answer": "no",
        "change": [
            "def run(",
            "",
            "if npr == 0:",
            "if nl:",
            "-                    stats.append((correct, *torch.zeros((3, 0))))",
            "+                    stats.append((correct, *torch.zeros((3, 0), device=device)))",
            "continue",
            "",
            "# Predictions"
        ]
    },
    {
        "number": 3891,
        "comments": "",
        "commit_message": "pytorch 1.0 (#2165)\n\n* fixes\n\n* allenlp\n\n* fix bug\n\n* reduce=False -> reduction='none'\n\n* fix _sparse_mask\n\n* remove print statement\n\n* fix more tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def make_model(num_layers: int = 6,",
            "# Initialize parameters with Glorot / fan_avg.",
            "for p in model.parameters():",
            "if p.dim() > 1:",
            "-            torch.nn.init.xavier_uniform(p)",
            "+            torch.nn.init.xavier_uniform_(p)",
            "return model"
        ]
    },
    {
        "number": 3892,
        "comments": "",
        "commit_message": "fixes and clean-up\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class VGG2L(torch.nn.Module):",
            "if self.output is not None:",
            "sequence = self.output(sequence)",
            "",
            "-        if mask is not None:",
            "-            return sequence, self.create_new_mask(mask)",
            "-",
            "-        return sequence, mask",
            "+        return sequence, self.create_new_mask(mask)",
            "",
            "def create_new_conformer_mask(self, mask: torch.Tensor) -> torch.Tensor:",
            "\"\"\"Create a new conformer mask for output sequences."
        ]
    },
    {
        "number": 3901,
        "comments": "",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def setup_mixins(policy, obs_space, action_space, config):",
            "ValueNetworkMixin.__init__(policy)",
            "# Set up a tf-var for the moving avg (do this here to make it work with",
            "# eager mode).",
            "-    policy._ma_adv_norm = tf.get_variable(",
            "+    policy._ma_adv_norm = tf1.get_variable(",
            "name=\"moving_average_of_advantage_norm\",",
            "dtype=tf.float32,",
            "initializer=100.0,"
        ]
    },
    {
        "number": 3902,
        "comments": "",
        "commit_message": "fix cascade r-cnn in TF1. tf.zeros([tensor]) is supported only in TF2 (fix #1525)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CascadeRCNNHead(object):",
            "max_iou_per_box = tf.reduce_max(iou, axis=1)  # N",
            "best_iou_ind = tf.cond(tf.shape(iou)[1] > 0,",
            "lambda: tf.argmax(iou, axis=1),   # #proposal, each in 0~m-1",
            "-                                       lambda: tf.zeros([tf.shape(iou)[0]], dtype=tf.int64))",
            "+                                       lambda: tf.zeros(tf.shape(iou)[0], dtype=tf.int64))",
            "labels_per_box = tf.gather(self.gt_labels, best_iou_ind)",
            "fg_mask = max_iou_per_box >= iou_threshold",
            "fg_inds_wrt_gt = tf.boolean_mask(best_iou_ind, fg_mask)"
        ]
    },
    {
        "number": 3905,
        "comments": "",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFTransfoXLPreTrainedModel(TFPreTrainedModel):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "}",
            "]",
            ")"
        ]
    },
    {
        "number": 3907,
        "comments": "",
        "commit_message": "Fix amp autocast  (#6080)\n\n* precision fixes\n\n* add amp test model\n\n* fix test\n\n* revert\n\n* move assert to training step\n\n* fix test\n\n* fix test\n\n* remove unrelated changes\n\n* add changelog\n\n* remove unused import\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NativeMixedPrecisionPlugin(MixedPrecisionPlugin):",
            "@contextmanager",
            "def train_step_context(self) -> Generator[autocast, None, None]:",
            "\"\"\"Enable autocast context\"\"\"",
            "-        yield torch.cuda.amp.autocast()",
            "+        with torch.cuda.amp.autocast():",
            "+            yield"
        ]
    },
    {
        "number": 3908,
        "comments": "",
        "commit_message": "quickfix batch shape\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class VPGModel(PGModel):",
            "self.log_probabilities = self.dist.log_prob(self.policy.get_policy_variables(), self.actions)",
            "",
            "# Concise: Get log likelihood of actions, weigh by advantages, compute gradient on that",
            "-            self.loss = -tf.reduce_mean(self.log_probabilities * self.advantage, name=\"loss_op\")",
            "+            self.loss = -tf.reduce_mean(self.log_probabilities * self.advantage, name=\"loss_op\", axis=1)",
            "",
            "self.optimize_op = self.optimizer.minimize(self.loss)"
        ]
    },
    {
        "number": 3913,
        "comments": "",
        "commit_message": "annotat unused vars (#5017)\n\n* annotate all unused vars\n\n* rank_zero_warn\n\n* Apply suggestions from code review\n\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n\n* f1 fixed\n\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def bleu_score(",
            "assert len(translate_corpus) == len(reference_corpus)",
            "numerator = torch.zeros(n_gram)",
            "denominator = torch.zeros(n_gram)",
            "-    precision_scores = torch.zeros(n_gram)",
            "c = 0.0",
            "r = 0.0"
        ]
    },
    {
        "number": 3915,
        "comments": "",
        "commit_message": "[RLlib] DQN torch version. (#7597)\n\n* Fix.\n\n* Rollback.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Test case fixes.\n\n* Test case fixes and LINT.\n\n* Test case fixes and LINT.\n\n* Rollback.\n\n* WIP.\n\n* WIP.\n\n* Test case fixes.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* Add regression test for DQN w/ param noise.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Comment\n\n* Regression test case.\n\n* WIP.\n\n* WIP.\n\n* LINT.\n\n* LINT.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* LINT.\n\n* Fix (SAC does currently not support eager).\n\n* Fix.\n\n* WIP.\n\n* LINT.\n\n* Update rllib/evaluation/sampler.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/evaluation/sampler.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/utils/exploration/exploration.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/utils/exploration/exploration.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* LINT.\n\n* Fix and LINT.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* Fix.\n\n* Fix and LINT.\n\n* Update rllib/utils/exploration/exploration.py\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Fixes.\n\n* WIP.\n\n* LINT.\n\n* Fixes and LINT.\n\n* LINT and fixes.\n\n* LINT.\n\n* Move action_dist back into torch extra_action_out_fn and LINT.\n\n* Working SimpleQ learning cartpole on both torch AND tf.\n\n* Working Rainbow learning cartpole on tf.\n\n* Working Rainbow learning cartpole on tf.\n\n* WIP.\n\n* LINT.\n\n* LINT.\n\n* Update docs and add torch to APEX test.\n\n* LINT.\n\n* Fix.\n\n* LINT.\n\n* Fix.\n\n* Fix.\n\n* Fix and docstrings.\n\n* Fix broken RLlib tests in master.\n\n* Split BAZEL learning tests into cartpole and pendulum (reached the 60min barrier).\n\n* Fix error_outputs option in BAZEL for RLlib regression tests.\n\n* Fix.\n\n* Tune param-noise tests.\n\n* LINT.\n\n* Fix.\n\n* Fix.\n\n* test\n\n* test\n\n* test\n\n* Fix.\n\n* Fix.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* LINT.\n\n* WIP.\n\nCo-authored-by: Eric Liang <ekhliang@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class RolloutWorker(EvaluatorInterface, ParallelIteratorWorker):",
            "policy_config = policy_config or {}",
            "if (tf and policy_config.get(\"eager\")",
            "and not policy_config.get(\"no_eager_on_workers\")):",
            "-            tf.enable_eager_execution()",
            "+            # This check is necessary for certain all-framework tests that",
            "+            # use tf's eager_mode() context generator.",
            "+            if not tf.executing_eagerly():",
            "+                tf.enable_eager_execution()",
            "",
            "if log_level:",
            "logging.getLogger(\"ray.rllib\").setLevel(log_level)"
        ]
    },
    {
        "number": 3918,
        "comments": "",
        "commit_message": "fix typo\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DeepQNetwork:",
            "t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_net')",
            "e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='eval_net')",
            "",
            "-        with tf.variable_scope('soft_replacement'):",
            "+        with tf.variable_scope('hard_replacement'):",
            "self.target_replace_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)]",
            "",
            "self.sess = tf.Session()"
        ]
    },
    {
        "number": 3925,
        "comments": "",
        "commit_message": "fixing \"No space allowed before...\" compile errors\n\nfixing \"No space allowed before...\" compile errors\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Synthesizer(object):",
            "wav = self.pwgan.inference(vocoder_input, hop_size=self.ap.hop_length)",
            "elif self.wavernn:",
            "vocoder_input = None",
            "-                if self.tts_config.model == \"Tacotron\" :",
            "-                    vocoder_input = torch.FloatTensor(self.ap.out_linear_to_mel(linear_spec = postnet_output.T).T).T.unsqueeze(0)",
            "+                if self.tts_config.model == \"Tacotron\":",
            "+                    vocoder_input = torch.FloatTensor(self.ap.out_linear_to_mel(linear_spec=postnet_output.T).T).T.unsqueeze(0)",
            "else:",
            "vocoder_input = torch.FloatTensor(postnet_output.T).unsqueeze(0)"
        ]
    },
    {
        "number": 3927,
        "comments": "",
        "commit_message": "Fix CUDA tests on dev (#1484)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NUTS(HMC):",
            "else:",
            "new_tree_prob = new_tree.weight / tree_weight",
            "rand = pyro.sample(\"rand_t={}_treedepth={}\".format(self._t, tree_depth),",
            "-                                   dist.Uniform(torch.zeros(1), torch.ones(1)))",
            "+                                   dist.Uniform(new_tree_prob.new_tensor(0.),",
            "+                                                new_tree_prob.new_tensor(1.)))",
            "if rand < new_tree_prob:",
            "accepted = True",
            "z = new_tree.z_proposal"
        ]
    },
    {
        "number": 3930,
        "comments": "",
        "commit_message": "Check `'onnxruntime-gpu' if torch.has_cuda` (#5087)\n\n* Check `'onnxruntime-gpu' if torch.has_cuda`\n\n* fix indent\n",
        "label": "",
        "answer": "no",
        "change": [
            "def run(weights=ROOT / 'yolov5s.pt',  # model.pt path(s)",
            "# check_requirements(('opencv-python>=4.5.4',))",
            "net = cv2.dnn.readNetFromONNX(w)",
            "else:",
            "-            check_requirements(('onnx', 'onnxruntime'))",
            "+            check_requirements(('onnx', 'onnxruntime-gpu' if torch.has_cuda else 'onnxruntime'))",
            "import onnxruntime",
            "session = onnxruntime.InferenceSession(w, None)",
            "else:  # TensorFlow models"
        ]
    },
    {
        "number": 3932,
        "comments": "",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestDrawRectangle:",
            "points_list.append([])",
            "for n in range(N):",
            "points_list[b].append([])",
            "-                points_list[b][n].append(int(torch.randint(0, w - 1, (1,))))",
            "-                points_list[b][n].append(int(torch.randint(0, h - 1, (1,))))",
            "-                points_list[b][n].append(int(torch.randint(points_list[b][n][-2] + 1, w, (1,))))",
            "-                points_list[b][n].append(int(torch.randint(points_list[b][n][-2] + 1, h, (1,))))",
            "+                points_list[b][n].append(int(torch.randint(0, w - 1, (1, ))))",
            "+                points_list[b][n].append(int(torch.randint(0, h - 1, (1, ))))",
            "+                points_list[b][n].append(int(torch.randint(points_list[b][n][-2] + 1, w, (1, ))))",
            "+                points_list[b][n].append(int(torch.randint(points_list[b][n][-2] + 1, h, (1, ))))",
            "",
            "points = torch.tensor(points_list).to(device)"
        ]
    },
    {
        "number": 3934,
        "comments": "",
        "commit_message": "[Bugbash] fix bug in compression (#4259)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "if __name__ == '__main__':",
            "evaluator(model)",
            "",
            "pruner._unwrap_model()",
            "-    ModelSpeedup(model, dummy_input=torch.rand(10, 3, 32, 32).to(device), masks_file='simple_masks.pth').speedup_model()",
            "+    ModelSpeedup(model, dummy_input=torch.rand(10, 3, 32, 32).to(device), masks_file=masks).speedup_model()",
            "",
            "print('\\nThe accuracy after speed up:')",
            "evaluator(model)"
        ]
    },
    {
        "number": 3938,
        "comments": "",
        "commit_message": "Fix valid ratio for Deformable Detr (#20958)\n\n* fix: valid ratio has right value\n\n* chore: remove unnecessary line\n\nCo-authored-by: Jeongyeon Nam <jy.nam@navercorp.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DeformableDetrModel(DeformableDetrPreTrainedModel):",
            "spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=source_flatten.device)",
            "level_start_index = torch.cat((spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))",
            "valid_ratios = torch.stack([self.get_valid_ratio(m) for m in masks], 1)",
            "-",
            "-        # revert valid_ratios",
            "-        valid_ratios = ~valid_ratios.bool()",
            "valid_ratios = valid_ratios.float()",
            "",
            "# Fourth, sent source_flatten + mask_flatten + lvl_pos_embed_flatten (backbone + proj layer output) through encoder"
        ]
    },
    {
        "number": 3942,
        "comments": "",
        "commit_message": "lint fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "trunc.unsupported_dtypes = (\"float16\",)",
            "",
            "",
            "def abs(",
            "-    x: Union[float, torch.Tensor],",
            "-    *,",
            "-    out: Optional[torch.Tensor] = None",
            "+    x: Union[float, torch.Tensor], *, out: Optional[torch.Tensor] = None",
            ") -> torch.Tensor:",
            "x = _cast_for_unary_op(x)",
            "return torch.abs(x, out=out)"
        ]
    },
    {
        "number": 3947,
        "comments": "",
        "commit_message": "Fixing exploration tensors. Needs templating and get variables\nfor compatibility with train scaffold.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Model(object):",
            "",
            "elif action_spec['type'] == 'float':",
            "if 'min_value' in action_spec:",
            "-                    exploration = tf.clip_by_value(",
            "+                    exploration_value = tf.clip_by_value(",
            "t=exploration_value,",
            "clip_value_min=action_spec['min_value'],",
            "clip_value_max=action_spec['max_value']",
            ")",
            "",
            "-                action += tf.reshape(exploration, tf.shape(action))",
            "+                action += tf.reshape(exploration_value, tf.shape(action))",
            "",
            "return action"
        ]
    },
    {
        "number": 3948,
        "comments": "",
        "commit_message": "Fix some TF GPT-J CI testings (#16454)\n\n* Fix for test_mixed_precision\n\n* Fix test_saved_model_creation by using shape_list instead of shape\n\n* skit test_model_from_pretrained on GPU for now to avoid GPU OOM\n\n* skip test_gptj_sample_max_time for now\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFGPTJModelLanguageGenerationTest(unittest.TestCase):",
            ")  # token_type_ids should change output",
            "",
            "@slow",
            "+    @unittest.skip(reason=\"TF generate currently has no time-based stopping criteria\")",
            "def test_gptj_sample_max_time(self):",
            "tokenizer = AutoTokenizer.from_pretrained(\"anton-l/gpt-j-tiny-random\")",
            "model = TFGPTJForCausalLM.from_pretrained(\"anton-l/gpt-j-tiny-random\", from_pt=True)"
        ]
    },
    {
        "number": 3949,
        "comments": "",
        "commit_message": "Add option to keep Viterbi scores when predicting (#1314)\n\n* fix torch version\n\n* fix tensorboard version\n\n* add keep_scores option\n\n* remove extra space\n\n* change to always return scores\n\n* rename to avoid type conflict\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ConditionalRandomField(torch.nn.Module):",
            "tag_sequence[sequence_length + 1, end_tag] = 0.",
            "",
            "# We pass the tags and the transitions to ``viterbi_decode``.",
            "-            viterbi_path, _ = util.viterbi_decode(tag_sequence[:(sequence_length + 2)], transitions)",
            "+            viterbi_path, viterbi_score = util.viterbi_decode(tag_sequence[:(sequence_length + 2)], transitions)",
            "# Get rid of START and END sentinels and append.",
            "-            all_tags.append(viterbi_path[1:-1])",
            "+            viterbi_path = viterbi_path[1:-1]",
            "+            best_paths.append((viterbi_path, viterbi_score.item()))",
            "",
            "-        return all_tags",
            "+        return best_paths"
        ]
    },
    {
        "number": 3951,
        "comments": "",
        "commit_message": "RANSAC improvements (#1435)\n\n- Added LU solver for RANSAC-homography, it speeds-up the minimal-solver part x10\n- Now find_fundamental weights is optional, as it was for homography\n- Fix test precision and made one xFail test to be compulsory\n- Fixed tests for fundamental matrix, as one have to have >= 8 points (also added to docstring)\n- updates deprecated torch.svd to torch.linalg.svd\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def zca_mean(",
            "else:",
            "cov = cov / float(N)",
            "",
            "-    U, S, _ = _torch_svd_cast(cov)",
            "+    U, S, _ = torch.linalg.svd(cov)",
            "",
            "S = S.reshape(-1, 1)",
            "S_inv_root: torch.Tensor = torch.rsqrt(S + eps)"
        ]
    },
    {
        "number": 3952,
        "comments": "",
        "commit_message": "Fixed style\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def testLog():",
            "",
            "Ptensor = PolynomialTensor()",
            "",
            "-    x = torch.randn(50000)",
            "-",
            "+    x = torch.randn(50000)",
            "+",
            "print(x)",
            "",
            "m = torch.nn.tanh()",
            "ten = m(x)",
            "",
            "assert (EvalError(ten, Ptensor.log(x))) == True",
            "-",
            "-testExp()",
            "",
            "+",
            "+testExp()"
        ]
    },
    {
        "number": 3953,
        "comments": "",
        "commit_message": "fixed categorical bug\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Bernoulli(Distribution):",
            "Reparameterized Bernoulli sampler.",
            "\"\"\"",
            "_ps = self._sanitize_input(ps)",
            "-        return torch.bernoulli(_ps)",
            "+        return torch.bernoulli(_ps).type_as(_ps)",
            "",
            "def log_pdf(self, x, ps=None, batch_size=1, *args, **kwargs):",
            "\"\"\""
        ]
    },
    {
        "number": 3955,
        "comments": "",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def normalize_min_max(x: torch.Tensor, min_val: float = 0., max_val: float = 1.,",
            "x_min: torch.Tensor = x.view(B, C, -1).min(-1)[0].view(B, C, 1)",
            "x_max: torch.Tensor = x.view(B, C, -1).max(-1)[0].view(B, C, 1)",
            "",
            "-    x_out: torch.Tensor = (",
            "-        (max_val - min_val) * (x.view(B, C, -1) - x_min) / (x_max - x_min + eps) + min_val",
            "-    )",
            "+    x_out: torch.Tensor = ((max_val - min_val) * (x.view(B, C, -1) - x_min) / (x_max - x_min + eps) + min_val)",
            "return x_out.view(shape)"
        ]
    },
    {
        "number": 3960,
        "comments": "",
        "commit_message": "Fixed nightly errors (#886)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TestRandomPerspective:",
            "assert out_perspective[0].shape == x_data.shape",
            "assert out_perspective[1].shape == (1, 3, 3)",
            "assert_allclose(out_perspective[0], x_data)",
            "-        assert_allclose(out_perspective[1], torch.eye(3, device=device))",
            "+        assert_allclose(out_perspective[1], torch.eye(3, device=device)[None])",
            "",
            "def test_transform_module_should_return_expected_transform(self, device):",
            "torch.manual_seed(0)"
        ]
    },
    {
        "number": 3963,
        "comments": "",
        "commit_message": "bugfix: persist collection order\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def average_grads(all_grads):",
            "for grad_and_vars in zip(*all_grads):",
            "# Ngpu * 2",
            "v = grad_and_vars[0][1]",
            "-            all_grads = [g for (g, _) in grad_and_vars]",
            "+            grads = [g for (g, _) in grad_and_vars]",
            "",
            "with tf.device(v.device):       # colocate summed grad with var",
            "grad = tf.multiply(",
            "-                    tf.add_n(all_grads), 1.0 / nr_tower)",
            "+                    tf.add_n(grads), 1.0 / nr_tower)",
            "ret.append((grad, v))",
            "return ret"
        ]
    },
    {
        "number": 3966,
        "comments": "",
        "commit_message": "fix seed generator\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SD(DiffusionInpaintModel):",
            "callback=self.callback,",
            "height=img_h,",
            "width=img_w,",
            "+            generator=torch.manual_seed(config.sd_seed)",
            ").images[0]",
            "",
            "output = (output * 255).round().astype(\"uint8\")"
        ]
    },
    {
        "number": 3968,
        "comments": "",
        "commit_message": "Fix #4098\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Attention(nn.Module):",
            "w = w / (v.size(-1) ** 0.5)",
            "nd, ns = w.size(-2), w.size(-1)",
            "mask = self.bias[:, :, ns - nd : ns, :ns]",
            "-        w = torch.where(mask, w, self.masked_bias)",
            "+        w = torch.where(mask, w, self.masked_bias.to(w.dtype))",
            "",
            "if attention_mask is not None:",
            "# Apply the attention mask"
        ]
    },
    {
        "number": 3969,
        "comments": "",
        "commit_message": "fix: Highway network layer fixed\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def highway_convolutional_network(input_units,",
            "kernel_initializer=xavier_initializer())",
            "if use_batch_norm:",
            "units = tf.layers.batch_normalization(units, training=training_ph)",
            "-        sigmoid_gate = tf.layers.dense(input_units, activation=tf.sigmoid, kernel_initializer=xavier_initializer())",
            "+        sigmoid_gate = tf.layers.dense(input_units, 1, activation=tf.sigmoid, kernel_initializer=xavier_initializer())",
            "input_units = sigmoid_gate * input_units + (1 - sigmoid_gate) * units",
            "input_units = tf.nn.relu(input_units)",
            "return input_units"
        ]
    },
    {
        "number": 3971,
        "comments": "",
        "commit_message": "replace remote_torch.Tensor([R]) with R.reshape(1) to fix proxy error\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "\"        \\n\",",
            "\"        # calculate critic (value) loss using L1 smooth loss\\n\",",
            "\"        value_losses.append(remote_torch.nn.functional.smooth_l1_loss(value,\\n\",",
            "-    \"                                                                remote_torch.Tensor([R])))\\n\",",
            "+    \"                                                                R.reshape(1)))\\n\",",
            "\"    # reset gradients    \\n\",",
            "\"    optimizer.zero_grad()\\n\",",
            "\"    \\n\","
        ]
    },
    {
        "number": 3974,
        "comments": "",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def elastic_transform2d(image: torch.Tensor,",
            "# Warp image based on displacement matrix",
            "b, c, h, w = image.shape",
            "grid = kornia.utils.create_meshgrid(h, w, device=image.device).to(image.dtype)",
            "-    warped = F.grid_sample(",
            "-        image, (grid + disp).clamp(-1, 1), align_corners=align_corners, mode=mode)",
            "+    warped = F.grid_sample(image, (grid + disp).clamp(-1, 1), align_corners=align_corners, mode=mode)",
            "",
            "return warped"
        ]
    },
    {
        "number": 3975,
        "comments": "",
        "commit_message": "Formatting fixes to meshgrid on all backends (#7579)\n\nCo-authored-by: @simonetgordon <simonegordon12@icloud.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def linspace(",
            "",
            "@with_unsupported_dtypes({\"2.9.1 and below\": (\"bool\",)}, backend_version)",
            "def meshgrid(",
            "-    *arrays: Union[tf.Tensor, tf.Variable], sparse: bool = False, indexing: str = \"xy\"",
            "+    *arrays: Union[tf.Tensor, tf.Variable],",
            "+    sparse: bool = False,",
            "+    indexing: str = \"xy\",",
            ") -> List[Union[tf.Tensor, tf.Variable]]:",
            "if not sparse:",
            "return tf.meshgrid(*arrays, indexing=indexing)"
        ]
    },
    {
        "number": 3976,
        "comments": "",
        "commit_message": "Add colorstr() (#1887)\n\n* Add colorful()\n\n* update\n\n* newline fix\n\n* add git description\n\n* --always\n\n* update loss scaling\n\n* update loss scaling 2\n\n* rename to colorstr()\n",
        "label": "",
        "answer": "no",
        "change": [
            "def select_device(device='', batch_size=None):",
            "p = torch.cuda.get_device_properties(i)",
            "s += f\"{'' if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / 1024 ** 2}MB)\\n\"  # bytes to MB",
            "else:",
            "-        s += 'CPU'",
            "+        s += 'CPU\\n'",
            "",
            "-    logger.info(f'{s}\\n')  # skip a line",
            "+    logger.info(s)  # skip a line",
            "return torch.device('cuda:0' if cuda else 'cpu')"
        ]
    },
    {
        "number": 3978,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestCnnHighwayEncoder(AllenNlpTestCase):",
            "encoder = TimeDistributed(encoder)",
            "",
            "embedding = torch.from_numpy(np.random.randn(5, 6, 50, 4)).float()",
            "-        mask = torch.ones(5, 6, 50).long()",
            "+        mask = torch.ones(5, 6, 50).bool()",
            "token_embedding = encoder(embedding, mask)",
            "",
            "assert list(token_embedding.size()) == [5, 6, 16]"
        ]
    },
    {
        "number": 3979,
        "comments": "",
        "commit_message": "Fix(core): fix memory leak issue and switch to subprocess backend (#216)\n\n* fix RAM leak error\n\n* fix another memory leak\n\n* update boxes.py\n\n* make flake8 happy\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main():",
            "log_level=trt.Logger.INFO,",
            "max_workspace_size=(1 << 32),",
            ")",
            "-    torch.save(model_trt.state_dict(), os.path.join(file_name, 'model_trt.pth'))",
            "+    torch.save(model_trt.state_dict(), os.path.join(file_name, \"model_trt.pth\"))",
            "logger.info(\"Converted TensorRT model done.\")",
            "-    engine_file = os.path.join(file_name, 'model_trt.engine')",
            "-    engine_file_demo = os.path.join('demo', 'TensorRT', 'cpp', 'model_trt.engine')",
            "-    with open(engine_file, 'wb') as f:",
            "+    engine_file = os.path.join(file_name, \"model_trt.engine\")",
            "+    engine_file_demo = os.path.join(\"demo\", \"TensorRT\", \"cpp\", \"model_trt.engine\")",
            "+    with open(engine_file, \"wb\") as f:",
            "f.write(model_trt.engine.serialize())",
            "",
            "shutil.copyfile(engine_file, engine_file_demo)"
        ]
    },
    {
        "number": 3981,
        "comments": "",
        "commit_message": "Fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "if K.backend() == 'cntk':",
            "supports_sparse = False",
            "elif K.backend() == 'theano' and not KTH.th_sparse_module:",
            "supports_sparse = False",
            "-else:",
            "-    supports_sparse = True",
            "+elif K.backend() == 'tensorflow':",
            "+    # Must wait for tf.keras to support sparse ops.",
            "+    supports_sparse = False",
            "",
            "",
            "def check_dtype(var, dtype):"
        ]
    },
    {
        "number": 3982,
        "comments": "",
        "commit_message": "small fix to tensorflow variable function.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "from ivy.container import Container",
            "",
            "",
            "def variable(x):",
            "-    with ivy.dev(x, as_native=True):",
            "+    with tf.device(ivy.dev(x, as_native=True)):",
            "return tf.Variable(x, trainable=True)"
        ]
    },
    {
        "number": 3983,
        "comments": "",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MetricInfo:",
            "for key, value in self.features.items():",
            "if not isinstance(value, Value):",
            "raise ValueError(",
            "-                        f\"When using 'numpy' format, all features should be a `nlp.Value` feature. \"",
            "+                        f\"When using 'numpy' format, all features should be a `datasets.Value` feature. \"",
            "f\"Here {key} is an instance of {value.__class__.__name__}\"",
            ")"
        ]
    },
    {
        "number": 3986,
        "comments": "",
        "commit_message": "Fix training/eval mode\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class E2E(STInterface, torch.nn.Module):",
            "att_ws = self.dec.calculate_all_attentions(",
            "hpad, hlens, ys_pad, lang_ids=tgt_lang_ids",
            ")",
            "-",
            "+        self.train()",
            "return att_ws",
            "",
            "def subsample_frames(self, x):"
        ]
    },
    {
        "number": 3987,
        "comments": "",
        "commit_message": "Fix tests which will break once MirroredStrategy switch to collective ops\n\nPiperOrigin-RevId: 515152548\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "from keras.testing_infra import test_utils",
            "def create_mirrored_strategy():",
            "# The test creates two virtual CPUs, and we use both of them to test with",
            "# multiple devices.",
            "+    # pylint: disable=protected-access",
            "+    tf.distribute.MirroredStrategy._collective_key_base += 1",
            "return tf.distribute.MirroredStrategy([\"cpu:0\", \"cpu:1\"])"
        ]
    },
    {
        "number": 3988,
        "comments": "",
        "commit_message": "fix import issue\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_frontend_backward_multi_channel(train, use_wpe, use_beamformer):",
            "frontend.train()",
            "else:",
            "frontend.eval()",
            "-    torch.random.manual_seed(14)",
            "+    set_all_random_seed(14)",
            "x = torch.randn(2, 1000, 2, requires_grad=True)",
            "x_lengths = torch.LongTensor([1000, 980])",
            "y, y_lengths = frontend(x, x_lengths)"
        ]
    },
    {
        "number": 3989,
        "comments": "",
        "commit_message": "Fix: Use .size to get integer dimension\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def attn(x, scope, n_state, *, past, params, block_offset=0, train=False):",
            "return mtf.transformer.attention.visibility_mask_to_attention_bias(vis, dtype)",
            "",
            "with tf.variable_scope(scope):",
            "-        dim_qkv = mtf.Dimension(\"qkv\", n_state * 3)",
            "+        dim_qkv = mtf.Dimension(\"qkv\", n_state.size * 3)",
            "c = conv1d(x, 'c_attn', dim_qkv, params=params)",
            "",
            "conv_output_channels = c.shape[2]  # should be equal to dim_qkv"
        ]
    },
    {
        "number": 3991,
        "comments": "",
        "commit_message": "Cast rewards as tf.float32 to fix error in DQN in tf2 (#28384)\n\n* Cast rewards as tf.float32 to fix error in DQN in tf2\n\nSigned-off-by: mgerstgrasser <matthias@gerstgrasser.net>\n\n* Add test case for DQN with integer rewards\n\nSigned-off-by: mgerstgrasser <matthias@gerstgrasser.net>\n\nSigned-off-by: mgerstgrasser <matthias@gerstgrasser.net>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def build_q_losses(policy: Policy, model, _, train_batch: SampleBatch) -> Tensor",
            "q_tp1_best,",
            "q_dist_tp1_best,",
            "train_batch[PRIO_WEIGHTS],",
            "-        train_batch[SampleBatch.REWARDS],",
            "+        tf.cast(train_batch[SampleBatch.REWARDS], tf.float32),",
            "tf.cast(train_batch[SampleBatch.DONES], tf.float32),",
            "config[\"gamma\"],",
            "config[\"n_step\"],"
        ]
    },
    {
        "number": 3992,
        "comments": "",
        "commit_message": "[RLlib] Exploration API: Policy changes needed for forward pass noisifications. (#7798)\n\n* Rollback.\n\n* WIP.\n\n* WIP.\n\n* LINT.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* LINT.\n\n* Fix (SAC does currently not support eager).\n\n* Fix.\n\n* WIP.\n\n* LINT.\n\n* Update rllib/evaluation/sampler.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/evaluation/sampler.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/utils/exploration/exploration.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/utils/exploration/exploration.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* LINT.\n\n* Fix and LINT.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* Fix.\n\n* Fix and LINT.\n\n* Update rllib/utils/exploration/exploration.py\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\nCo-authored-by: Eric Liang <ekhliang@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Deterministic(TFActionDistribution):",
            "return self.inputs",
            "",
            "@override(TFActionDistribution)",
            "-    def sampled_action_logp(self):",
            "-        return 0.0",
            "+    def logp(self, x):",
            "+        return tf.zeros_like(self.inputs)",
            "",
            "@override(TFActionDistribution)",
            "def _build_sample_op(self):"
        ]
    },
    {
        "number": 3993,
        "comments": "",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def find_fundamental(points1: torch.Tensor, points2: torch.Tensor, weights: torc",
            "",
            "# reconstruct and force the matrix to have rank2",
            "U, S, V = torch.svd(F_mat)",
            "-    rank_mask = torch.tensor([1.0, 1.0, 0.0],",
            "-                             device=F_mat.device,",
            "-                             dtype=F_mat.dtype)",
            "+    rank_mask = torch.tensor([1.0, 1.0, 0.0], device=F_mat.device, dtype=F_mat.dtype)",
            "",
            "F_projected = U @ (torch.diag_embed(S * rank_mask) @ V.transpose(-2, -1))",
            "F_est = transform2.transpose(-2, -1) @ (F_projected @ transform1)"
        ]
    },
    {
        "number": 4000,
        "comments": "",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_cholesky_transform(batch_shape, dim, transform):",
            "assert_close(log_det, torch.slogdet(jacobian)[1])",
            "",
            "assert log_det.shape == batch_shape",
            "-    assert_close(y, x_mat.cholesky())",
            "+    assert_close(y, torch.linalg.cholesky(x_mat))",
            "assert_close(transform.inv(y), x_mat)"
        ]
    },
    {
        "number": 4004,
        "comments": "",
        "commit_message": "Drop torch 1.6 testing (#10390)\n\n* Drop torch 1.6 support\n\n* Drop 1.6 support\n\n* Update CHANGELOG\n\n* Fixes\n\n* Split change\n\n* Undo change\n\n* 1.7 -> 1.7.1\n\nhttps://github.com/pytorch/pytorch/issues/47354\n\n* Force trigger nightly\n\n* Update .github/workflows/events-nightly.yml\n\nCo-authored-by: Aki Nitta <nitta@akihironitta.com>\n\n* Revert 1.7.1 change - try wildcard\n\n* Update adjust versions and test it\n\n* Undo test changes\n\n* Revert \"Undo test changes\"\n\nThis reverts commit 3a6acadd115e86f02d83a788f1978372ab6764f3.\n\n* Update CHANGELOG.md\n\nCo-authored-by: Aki Nitta <nitta@akihironitta.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LightningDistributed:",
            "if self.rank != 0:",
            "obj = [None] * len(obj)",
            "",
            "-        broadcast_object_list(obj, 0, group=group or _group.WORLD)",
            "+        torch.distributed.broadcast_object_list(obj, 0, group=group or _group.WORLD)",
            "",
            "return obj[0]"
        ]
    },
    {
        "number": 4006,
        "comments": "",
        "commit_message": "GH-407: fix training on cuda without CRF\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SequenceTagger(flair.nn.Model):",
            "for sentence_feats, sentence_tags, sentence_length in zip(features, tags, lengths):",
            "sentence_feats = sentence_feats[:sentence_length]",
            "",
            "-                # print(sentence_tags)",
            "-                # tag_tensor = torch.LongTensor(sentence_tags)",
            "-                # tag_tensor = tag_tensor.to(flair.device)",
            "-",
            "score += torch.nn.functional.cross_entropy(sentence_feats, sentence_tags)",
            "",
            "return score"
        ]
    },
    {
        "number": 4008,
        "comments": "",
        "commit_message": "`enable_model_cpu_offload` (#2285)\n\n* enable_model_offload PoC\n\nIt's surprisingly more involved than expected, see comments in the PR.\n\n* Rename final_offload_hook\n\n* Invoke the vae forward hook manually.\n\n* Completely remove decoder.\n\n* Style\n\n* apply_forward_hook decorator\n\n* Rename method.\n\n* Style\n\n* Copy enable_model_cpu_offload\n\n* Fix copies.\n\n* Remove comment.\n\n* Fix copies\n\n* Missing import\n\n* Fix doc-builder style.\n\n* Merge main and fix again.\n\n* Add docs\n\n* Fix docs.\n\n* Add a couple of tests.\n\n* style\n",
        "label": "",
        "answer": "no",
        "change": [
            "class VersatileDiffusionImageVariationPipeline(DiffusionPipeline):",
            "`pipeline.enable_sequential_cpu_offload()` the execution device can only be inferred from Accelerate's module",
            "hooks.",
            "\"\"\"",
            "-        if self.device != torch.device(\"meta\") or not hasattr(self.image_unet, \"_hf_hook\"):",
            "+        if not hasattr(self.image_unet, \"_hf_hook\"):",
            "return self.device",
            "for module in self.image_unet.modules():",
            "if ("
        ]
    },
    {
        "number": 4011,
        "comments": "",
        "commit_message": "lint fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def remainder(",
            "return tf.math.floormod(x1, x2)",
            "",
            "",
            "-def round(",
            "-    x: Union[tf.Tensor, tf.Variable]",
            "-) -> Union[tf.Tensor, tf.Variable]:",
            "+def round(x: Union[tf.Tensor, tf.Variable]) -> Union[tf.Tensor, tf.Variable]:",
            "if \"int\" in str(x.dtype):",
            "return x",
            "else:"
        ]
    },
    {
        "number": 4016,
        "comments": "",
        "commit_message": "fix error messages not appearing\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "else:",
            "sd_model = sd_model.to(device)",
            "",
            "model_hijack = StableDiffusionModelHijack()",
            "-model_hijack.hijack(sd_model)",
            "+#model_hijack.hijack(sd_model)",
            "",
            "with open(os.path.join(script_path, \"style.css\"), \"r\", encoding=\"utf8\") as file:",
            "css = file.read()"
        ]
    },
    {
        "number": 4017,
        "comments": "",
        "commit_message": "fix kw_only missmatch for diff function with out arg missing for jax, torch and tensorflow as opposed to np backend and ivy functional\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def diff(",
            "axis: Optional[int] = -1,",
            "prepend: Optional[Union[torch.Tensor, int, float, list, tuple]] = None,",
            "append: Optional[Union[torch.Tensor, int, float, list, tuple]] = None,",
            "+    out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "x = x if type(x) == torch.Tensor else torch.Tensor(x)",
            "prepend = ("
        ]
    },
    {
        "number": 4018,
        "comments": "",
        "commit_message": "fixed broken tests due to tensor broadcasting\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Normal(Distribution):",
            "def _sanitize_input(self, mu, sigma):",
            "if mu is not None:",
            "# stateless distribution",
            "-            mu = torch.unsqueeze(mu, 1)",
            "return mu, sigma",
            "elif self.mu is not None:",
            "# stateful distribution"
        ]
    },
    {
        "number": 4019,
        "comments": "",
        "commit_message": "fix typing\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def random_hflip(input, p=0.5):",
            "probs: torch.Tensor = torch.empty(input.shape[0], device=device).uniform_(0, 1)",
            "else:",
            "input = input.unsqueeze(0)",
            "-        probs: torch.Tensor = torch.empty(1, device=device).uniform_(0, 1)",
            "+        probs = torch.empty(1, device=device).uniform_(0, 1)",
            "",
            "to_flip: torch.Tensor = probs < p",
            "",
            "if input[to_flip].nelement() != 0:",
            "",
            "-        flipped: torch.Tensor = flips.hflip(input[to_flip])",
            "+        flipped: torch.Tensor = hflip(input[to_flip])",
            "trans_mat: torch.Tensor = torch.zeros((3, 3))",
            "trans_mat[0][0] = -1",
            "trans_mat[1][1] = 1"
        ]
    },
    {
        "number": 4025,
        "comments": "",
        "commit_message": "use dicts for namespaces within text fields (#25)\n\n* switch text and list fields to use dicts\n\n* unify namespace attribute in token indexers\n\n* switch dataset, iterators and tagger to new dict based textfields\n\n* fix unrecognised arg in char indexer\n\n* improve docs\n\n* make dataset iteration clearer\n\n* change to as_arrays, remove list options from data/\n\n* remove non-defaults from dataset test\n\n* add generic types to fields\n\n* move TextField internals to be dicts\n\n* fix mypy by removing DataArray type annotation\n\n* use new dict for dataset loop\n\n* Revert \"use new dict for dataset loop\"\n\nThis reverts commit 6447f7a4607ec5725e92e53689e469f73794e0cf.\n\n* address matts comments\n\n* fix typo\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class SimpleTaggerTest(AllenNlpTestCase):",
            "training_arrays = self.dataset.as_arrays()",
            "",
            "# TODO(Mark): clean this up once the Trainer is finalised.",
            "-        sequence = training_arrays[\"tokens\"][0]",
            "+        sequence = training_arrays[\"tokens\"][\"tokens\"]",
            "tags = training_arrays[\"tags\"]",
            "-        training_arrays = {\"tokens\": Variable(torch.from_numpy(sequence)),  # pylint: disable=no-member",
            "+        training_arrays = {\"tokens\": {\"tokens\": Variable(torch.from_numpy(sequence))},  # pylint: disable=no-member",
            "\"tags\": Variable(torch.from_numpy(tags))}  # pylint: disable=no-member",
            "_ = self.model.forward(**training_arrays)",
            "",
            "def test_tag_returns_distributions_per_token(self):",
            "-        text = TextField([\"This\", \"is\", \"a\", \"sentence\"], token_indexers=[SingleIdTokenIndexer()])",
            "+        text = TextField([\"This\", \"is\", \"a\", \"sentence\"], token_indexers={\"tokens\": SingleIdTokenIndexer()})",
            "output = self.model.tag(text)",
            "possible_tags = self.vocab.get_index_to_token_vocabulary(\"tags\").values()",
            "for tag in output[\"tags\"]:"
        ]
    },
    {
        "number": 4029,
        "comments": "",
        "commit_message": "Add useful errors when model is not configured correctly (#1199)\n\n* add check_model_configuration method\n\n* trying to fix errors\n\n* trying to fix tests\n\n* added test_epoch_end to lightning template\n\n* fix tests\n\n* fix new test after rebase\n\n* fix spelling\n\n* added more checks\n\n* updated formating\n\n* added tests\n\n* fixed CHANGELOG\n\n* Apply suggestions from code review\n\n* move test to new module\n\n* change check on configure_optimizers\n\nCo-authored-by: Nicki Skafte <nugginea@gmail.com>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LightTestStepMultipleDataloadersMixin:",
            "class LightTestFitSingleTestDataloadersMixin:",
            "\"\"\"Test fit single test dataloaders mixin.\"\"\"",
            "",
            "+    def test_dataloader(self):",
            "+        return self._dataloader(train=False)",
            "+",
            "def test_step(self, batch, batch_idx, *args, **kwargs):",
            "\"\"\"",
            "Lightning calls this inside the validation loop"
        ]
    },
    {
        "number": 4031,
        "comments": "",
        "commit_message": "use torch.matmul instead of einsum in attnetion. (#445)\n\n* use torch.matmul instead of einsum\n\n* fix softmax\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CrossAttention(nn.Module):",
            "for i in range(hidden_states.shape[0] // slice_size):",
            "start_idx = i * slice_size",
            "end_idx = (i + 1) * slice_size",
            "-            attn_slice = (",
            "-                torch.einsum(\"b i d, b j d -> b i j\", query[start_idx:end_idx], key[start_idx:end_idx]) * self.scale",
            "-            )",
            "+            attn_slice = torch.matmul(query[start_idx:end_idx], key[start_idx:end_idx].transpose(1, 2)) * self.scale",
            "attn_slice = attn_slice.softmax(dim=-1)",
            "-            attn_slice = torch.einsum(\"b i j, b j d -> b i d\", attn_slice, value[start_idx:end_idx])",
            "+            attn_slice = torch.matmul(attn_slice, value[start_idx:end_idx])",
            "",
            "hidden_states[start_idx:end_idx] = attn_slice"
        ]
    },
    {
        "number": 4033,
        "comments": "",
        "commit_message": "[RLlib] Minor fixes (torch GPU bugs + some cleanup). (#11609)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def build_vtrace_loss(policy, model, dist_class, train_batch):",
            "if isinstance(policy.action_space, gym.spaces.Discrete):",
            "is_multidiscrete = False",
            "output_hidden_shape = [policy.action_space.n]",
            "-    elif isinstance(policy.action_space,",
            "-                    gym.spaces.multi_discrete.MultiDiscrete):",
            "+    elif isinstance(policy.action_space, gym.spaces.MultiDiscrete):",
            "is_multidiscrete = True",
            "output_hidden_shape = policy.action_space.nvec.astype(np.int32)",
            "else:"
        ]
    },
    {
        "number": 4034,
        "comments": "",
        "commit_message": "test is fixed now\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestTorchVariable(TestCase):",
            "x = Var(torch.FloatTensor([1, 2, -3, 4, 5])).send(remote)",
            "assert torch.equal(x.ceil().get(), Var(torch.FloatTensor([1, 2, -3, 4, 5])))",
            "assert torch.equal(x.ceil_().get(), Var(torch.FloatTensor([1, 2, -3, 4, 5])))",
            "-        assert torch.equal(x.cpu().get(), Var(torch.FloatTensor([1, 2, -3, 4, 5])))",
            "\\ No newline at end of file",
            "+        assert torch.equal(x.cpu().get(), Var(torch.FloatTensor([1, 2, -3, 4, 5])))"
        ]
    },
    {
        "number": 4035,
        "comments": "",
        "commit_message": "fix many pylint issues.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BatchNormLayer(Layer):",
            "params_shape = x_shape[-1:]",
            "",
            "from tensorflow.python.training import moving_averages",
            "-        from tensorflow.python.ops import control_flow_ops",
            "",
            "-        with tf.variable_scope(name) as vs:",
            "+        with tf.variable_scope(name):",
            "axis = list(range(len(x_shape) - 1))",
            "",
            "# 1. beta, gamma"
        ]
    },
    {
        "number": 4037,
        "comments": "",
        "commit_message": "fix label mapping (#3180)\n\n* fix label mapping\n\n* add pretty_name\n\n* update infos\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class HLGD(datasets.GeneratorBasedBuilder):",
            "\"date_b\": datasets.Value(\"string\"),",
            "\"url_a\": datasets.Value(\"string\"),",
            "\"url_b\": datasets.Value(\"string\"),",
            "-                \"label\": datasets.features.ClassLabel(names=[\"different_event\", \"same_event\"]),",
            "+                \"label\": datasets.features.ClassLabel(names=[\"same_event\", \"different_event\"]),",
            "}",
            ")"
        ]
    },
    {
        "number": 4038,
        "comments": "",
        "commit_message": "Fix ResNet based models to work w/ norm layers w/o affine params. Reformat long arg lists into vertical form.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ResNestBottleneck(nn.Module):",
            "self.downsample = downsample",
            "",
            "def zero_init_last(self):",
            "-        nn.init.zeros_(self.bn3.weight)",
            "+        if getattr(self.bn3, 'weight', None) is not None:",
            "+            nn.init.zeros_(self.bn3.weight)",
            "",
            "def forward(self, x):",
            "shortcut = x"
        ]
    },
    {
        "number": 4040,
        "comments": "",
        "commit_message": "fix some errors for distributed lm_finetuning\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def main():",
            "global_step += 1",
            "",
            "# Save a trained model",
            "-        logger.info(\"** ** * Saving fine - tuned model ** ** * \")",
            "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self",
            "output_model_file = os.path.join(args.output_dir, WEIGHTS_NAME)",
            "output_config_file = os.path.join(args.output_dir, CONFIG_NAME)",
            "-        if args.do_train:",
            "+        if args.do_train and torch.distributed.get_rank() == 0:",
            "+            logger.info(\"** ** * Saving fine - tuned model ** ** * \")",
            "torch.save(model_to_save.state_dict(), output_model_file)",
            "model_to_save.config.to_json_file(output_config_file)",
            "tokenizer.save_vocabulary(args.output_dir)"
        ]
    },
    {
        "number": 4042,
        "comments": "",
        "commit_message": "fix typo\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LatentDiffusion(DiffusionPipeline):",
            "# 3. optionally sample variance",
            "variance = 0",
            "if eta > 0:",
            "-                noise = torch.randn(image.shape, generator=generator)to(image.device)",
            "+                noise = torch.randn(image.shape, generator=generator).to(image.device)",
            "variance = self.noise_scheduler.get_variance(t, num_inference_steps).sqrt() * eta * noise",
            "",
            "# 4. set current image to prev_image: x_t -> x_t-1"
        ]
    },
    {
        "number": 4046,
        "comments": "",
        "commit_message": "[Fix] gpu tests for crop3d and flip (#727)\n\n* fix gpu tests for crop3d and flip\n\n* add 0.4.1 pytorch dependency\n\n* add missing docs for warp3d functions\n\n* disable get_perspective_transform3d test for a while\n\n* Fixed 3D crop GPU test\n\n* Fixed partial projwarp\n\n* Recomputed the tests for 3D rotation\n\n* Fixed one more test\n\n* Fixed lint\n\n* Update gradcheck for float64\n\n* Fixed lint\n\nCo-authored-by: shijianjian <sj8716643@126.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def vflip(input: torch.Tensor) -> torch.Tensor:",
            "\"\"\"",
            "",
            "h = input.shape[-2]",
            "-    return input[..., torch.arange(h - 1, -1, -1), :]",
            "+    return input[..., torch.arange(h - 1, -1, -1, device=input.device), :]"
        ]
    },
    {
        "number": 4048,
        "comments": "",
        "commit_message": "Fixed indentation\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main(argv=None):",
            "facenet.plot_roc(fpr, tpr, 'NN4')",
            "",
            "if __name__ == '__main__':",
            "-  tf.app.run()",
            "+    tf.app.run()"
        ]
    },
    {
        "number": 4054,
        "comments": "",
        "commit_message": "Make most metrics work on GPU (#3851)\n\n* Make most metrics work on GPU\n\n* Make metric tests work on both GPU and CPU\n\n* Add a test for the test utility\n\n* mypy\n\n* Update allennlp/common/testing/test_case.py\n\nCo-Authored-By: Mark Neumann <markn@allenai.org>\n\n* Fix a PR comment\n\n* flake8\n\nCo-authored-by: Mark Neumann <markn@allenai.org>\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BLEU(Metric):",
            "",
            "None",
            "\"\"\"",
            "-        predictions, gold_targets = self.unwrap_to_tensors(predictions, gold_targets)",
            "+        predictions, gold_targets = self.detach_tensors(predictions, gold_targets)",
            "for ngram_size, _ in enumerate(self._ngram_weights, start=1):",
            "precision_matches, precision_totals = self._get_modified_precision_counts(",
            "predictions, gold_targets, ngram_size"
        ]
    },
    {
        "number": 4056,
        "comments": "",
        "commit_message": "Update quality tooling for formatting (#21480)\n\n* Result of black 23.1\n\n* Update target to Python 3.7\n\n* Switch flake8 to ruff\n\n* Configure isort\n\n* Configure isort\n\n* Apply isort with line limit\n\n* Put the right black version\n\n* adapt black in check copies\n\n* Fix copies\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFGenerationMixin:",
            "generated, finished_sequences, cur_len, model_kwargs, next_step_cached_variables",
            "):",
            "maximum_iterations = max_length - cur_len",
            "-            generated, _, cur_len, _, _, = tf.while_loop(",
            "+            (",
            "+                generated,",
            "+                _,",
            "+                cur_len,",
            "+                _,",
            "+                _,",
            "+            ) = tf.while_loop(",
            "contrastive_search_cond_fn,",
            "contrastive_search_body_fn,",
            "(generated, finished_sequences, cur_len, model_kwargs, next_step_cached_variables),"
        ]
    },
    {
        "number": 4058,
        "comments": "",
        "commit_message": "fix 'logits' naming in A3C (fix #197)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GANModelDesc(ModelDesc):",
            "d_pos_acc = tf.reduce_mean(tf.cast(score_real > 0.5, tf.float32), name='accuracy_real')",
            "d_neg_acc = tf.reduce_mean(tf.cast(score_fake < 0.5, tf.float32), name='accuracy_fake')",
            "",
            "-                self.d_accuracy = tf.add(.5 * d_pos_acc, .5 * d_neg_acc, name='accuracy')",
            "+                d_accuracy = tf.add(.5 * d_pos_acc, .5 * d_neg_acc, name='accuracy')",
            "self.d_loss = tf.add(.5 * d_loss_pos, .5 * d_loss_neg, name='loss')",
            "",
            "with tf.name_scope(\"gen\"):",
            "self.g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(",
            "logits=logits_fake, labels=tf.ones_like(logits_fake)), name='loss')",
            "-                self.g_accuracy = tf.reduce_mean(tf.cast(score_fake > 0.5, tf.float32), name='accuracy')",
            "+                g_accuracy = tf.reduce_mean(tf.cast(score_fake > 0.5, tf.float32), name='accuracy')",
            "",
            "-            add_moving_summary(self.g_loss, self.d_loss, self.d_accuracy, self.g_accuracy)",
            "+            add_moving_summary(self.g_loss, self.d_loss, d_accuracy, g_accuracy)",
            "",
            "",
            "class GANTrainer(FeedfreeTrainerBase):"
        ]
    },
    {
        "number": 4060,
        "comments": "",
        "commit_message": "fix build errors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ReshapeLayer(Layer):",
            "",
            "",
            "class TransposeLayer(Layer):",
            "-    \"\"\"",
            "-    The :class:`TransposeLayer` class transposes the dimension of a teneor, see `tf.transpose() <https://www.tensorflow.org/api_docs/python/tf/transpose>`__ .",
            "+    \"\"\"A layer that transposes the dimension of a tensor.",
            "+",
            "+    See `tf.transpose() <https://www.tensorflow.org/api_docs/python/tf/transpose>`__ .",
            "",
            "Parameters"
        ]
    },
    {
        "number": 4064,
        "comments": "",
        "commit_message": "fix speaker-embeddings dimension during inference\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Tacotron2(TacotronAbstract):",
            "if self.num_speakers > 1:",
            "if not self.embeddings_per_sample:",
            "speaker_embeddings = self.speaker_embedding(speaker_ids)[:, None]",
            "+                speaker_embeddings = torch.unsqueeze(speaker_embeddings, 0).transpose(1, 2)",
            "encoder_outputs = self._concat_speaker_embedding(encoder_outputs, speaker_embeddings)",
            "",
            "mel_outputs, alignments, stop_tokens = self.decoder.inference_truncated(encoder_outputs)"
        ]
    },
    {
        "number": 4066,
        "comments": "",
        "commit_message": "[rllib] Try fixing torch GPU and masking errors (#10168)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TorchParametricActionsModel(DQNTorchModel):",
            "# Mask out invalid actions (use -inf to tag invalid).",
            "# These are then recognized by the EpsilonGreedy exploration component",
            "# as invalid actions that are not to be chosen.",
            "-        inf_mask = torch.clamp(",
            "-            torch.log(action_mask), -float(\"inf\"), float(\"inf\"))",
            "+        inf_mask = torch.clamp(torch.log(action_mask), FLOAT_MIN, FLOAT_MAX)",
            "+",
            "return action_logits + inf_mask, state",
            "",
            "def value_function(self):"
        ]
    },
    {
        "number": 4067,
        "comments": "",
        "commit_message": "Update to PyTorch 1.11.0 (#3045)\n\n* Run black\n\n* Fix ProvenanceTensor\n\n* Replace torch.triangular_solve -> torch.linalg.solve_triangular\n\n* More fixes to torch.linalg.solve_triangular\n\n* Bump torch version\n\n* Bump Python version 3.6 -> 3.7\n\n* Fix some tests\n\n* Decrease tolerance on stable tests\n\n* Remove obsolete xfail\n\n* Resolve #3032\n\n* Fix solve_triangular in ops.gaussian, revert test weakening\n\n* Fix catching of singular matrices in hmc\n\n* xfail funsor tests\n\n* Work around pandas 1.3 bug\n\n* Allow mypy to install missing stubs\n\n* Clarify triangular_solve test\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def triu_inverse(x):",
            "B_Dinv = B / x.bottom_diag.unsqueeze(-2)",
            "",
            "identity = torch.eye(head_size, dtype=A.dtype, device=A.device)",
            "-    top_left = torch.triangular_solve(identity, A, upper=True)[0]",
            "+    top_left = torch.linalg.solve_triangular(A, identity, upper=True)",
            "top_right = -top_left.matmul(B_Dinv)  # complexity: head_size^2 x N",
            "top = torch.cat([top_left, top_right], -1)",
            "bottom_diag = x.bottom_diag.reciprocal()"
        ]
    },
    {
        "number": 4068,
        "comments": "",
        "commit_message": "weight_decay fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def train():",
            "",
            "# Compute loss",
            "loss, loss_items = compute_loss(pred, targets, model)",
            "+            if torch.isnan(loss):",
            "+                print('WARNING: nan loss detected, skipping batch ', loss_items)",
            "+                continue",
            "",
            "# Scale loss by nominal batch_size of 64",
            "loss *= batch_size / 64"
        ]
    },
    {
        "number": 4070,
        "comments": "",
        "commit_message": "Fixed failing test for manipulation concat due to bfloat16 casting (#4598)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def concat(",
            "xs = list(xs)",
            "highest_dtype = xs[0].dtype",
            "for i in xs:",
            "-        highest_dtype = tf.experimental.numpy.promote_types(highest_dtype, i.dtype)",
            "+        highest_dtype = ivy.as_native_dtype(ivy.promote_types(highest_dtype, i.dtype))",
            "",
            "for i in range(len(xs)):",
            "if is_axis_none:"
        ]
    },
    {
        "number": 4071,
        "comments": "",
        "commit_message": "[RLlib] Minor cleanup in preparation to tf2.x support. (#9130)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* Fixes.\n\n* Fixes and LINT.\n\n* WIP.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PPOLoss:",
            "self.mean_policy_loss = reduce_mean_valid(-surrogate_loss)",
            "",
            "if use_gae:",
            "-            vf_loss1 = tf.square(value_fn - value_targets)",
            "+            vf_loss1 = tf.math.square(value_fn - value_targets)",
            "vf_clipped = vf_preds + tf.clip_by_value(",
            "value_fn - vf_preds, -vf_clip_param, vf_clip_param)",
            "-            vf_loss2 = tf.square(vf_clipped - value_targets)",
            "+            vf_loss2 = tf.math.square(vf_clipped - value_targets)",
            "vf_loss = tf.maximum(vf_loss1, vf_loss2)",
            "self.mean_vf_loss = reduce_mean_valid(vf_loss)",
            "loss = reduce_mean_valid("
        ]
    },
    {
        "number": 4072,
        "comments": "",
        "commit_message": "Fix setup.py for cpu-only environment installation (#538)\n\n* Add guard to not using `torch.version.cuda` above no-CUDA environment.\n* Fix several typos on setup.py.\n\nSigned-off-by: Seunghwan Hong <seunghwan@scatterlab.co.kr>\n\nCo-authored-by: Jeff Rasley <jerasley@microsoft.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "else:",
            "version_str += f'+{git_hash}'",
            "",
            "torch_version = \".\".join([TORCH_MAJOR, TORCH_MINOR])",
            "-cuda_version = \".\".join(torch.version.cuda.split('.')[:2])",
            "+# Set cuda_version to 0.0 if cpu-only",
            "+cuda_version = \"0.0\"",
            "+if torch.version.cuda is not None:",
            "+    cuda_version = \".\".join(torch.version.cuda.split('.')[:2])",
            "torch_info = {\"version\": torch_version, \"cuda_version\": cuda_version}",
            "",
            "print(f\"version={version_str}, git_hash={git_hash}, git_branch={git_branch}\")"
        ]
    },
    {
        "number": 4074,
        "comments": "",
        "commit_message": "Fix compatibility of Retiarii CGO (#4621)\n\n* fix compatibility of CGO\n* remove old test files for CGO\n* revert to fit pytorch-lightning 1.5.x\n* update PyTorch-lightning version in doc\n* fix nni.trace issue\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _new_trainer():",
            "train_dataset = serialize(MNIST, root='data/mnist', train=True, download=True, transform=transform)",
            "test_dataset = serialize(MNIST, root='data/mnist', train=False, download=True, transform=transform)",
            "",
            "-    multi_module = MultiModelSupervisedLearningModule(nn.CrossEntropyLoss, {'acc': pl._AccuracyWithLogits})",
            "+    multi_module = _MultiModelSupervisedLearningModule(nn.CrossEntropyLoss, {'acc': pl._AccuracyWithLogits})",
            "",
            "lightning = pl.Lightning(multi_module, cgo_trainer.Trainer(use_cgo=True,",
            "max_epochs=1,"
        ]
    },
    {
        "number": 4076,
        "comments": "",
        "commit_message": "Fixed code based on flake8\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Decoder(torch.nn.Module):",
            "if self.labeldist is not None:",
            "if self.vlabeldist is None:",
            "self.vlabeldist = to_cuda(self, Variable(torch.from_numpy(self.labeldist)))",
            "-            loss_reg = - torch.sum((functional.log_softmax(y_all, dim=1) * self.vlabeldist).view(-1), dim=0) / len(ys_in)",
            "+            loss_reg = - torch.sum((functional.log_softmax(y_all, dim=1) *",
            "+                                    self.vlabeldist).view(-1), dim=0) / len(ys_in)",
            "self.loss = (1. - self.lsm_weight) * self.loss + self.lsm_weight * loss_reg",
            "",
            "return self.loss, acc, att_weight_all"
        ]
    },
    {
        "number": 4078,
        "comments": "",
        "commit_message": "Reproducibility 3/3 (#1924)\n\n* make tests deterministic\n\n* run slow tests\n\n* prepare for testing\n\n* finish\n\n* refactor\n\n* add print statements\n\n* finish more\n\n* correct some test failures\n\n* more fixes\n\n* set up to correct tests\n\n* more corrections\n\n* up\n\n* fix more\n\n* more prints\n\n* add\n\n* up\n\n* up\n\n* up\n\n* uP\n\n* uP\n\n* more fixes\n\n* uP\n\n* up\n\n* up\n\n* up\n\n* up\n\n* fix more\n\n* up\n\n* up\n\n* clean tests\n\n* up\n\n* up\n\n* up\n\n* more fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* make\n\n* correct\n\n* finish\n\n* finish\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PaintByExamplePipelineIntegrationTests(unittest.TestCase):",
            "image_slice = image[0, -3:, -3:, -1]",
            "",
            "assert image.shape == (1, 512, 512, 3)",
            "-        expected_slice = np.array(",
            "-            [0.47455794, 0.47086594, 0.47683704, 0.51024145, 0.5064255, 0.5123164, 0.532502, 0.5328063, 0.5428694]",
            "-        )",
            "+        expected_slice = np.array([0.4834, 0.4811, 0.4874, 0.5122, 0.5081, 0.5144, 0.5291, 0.5290, 0.5374])",
            "+",
            "assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2"
        ]
    },
    {
        "number": 4081,
        "comments": "",
        "commit_message": "fix docs of customized (#1508)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "number of neurons is tunable.",
            "class SingleDenseLayerBlock(ak.Block):",
            "def build(self, hp, inputs=None):",
            "# Get the input_node from inputs.",
            "-        input_node = tf.python.util.nest.flatten(inputs)[0]",
            "+        input_node = tf.nest.flatten(inputs)[0]",
            "layer = tf.keras.layers.Dense(",
            "hp.Int(\"num_units\", min_value=32, max_value=512, step=32)",
            ")"
        ]
    },
    {
        "number": 4082,
        "comments": "",
        "commit_message": "Fix val.py 'no labels found bug' (#8806)\n\nResolves https://github.com/ultralytics/yolov5/issues/8791\n\nBug first introduced in #8782\n",
        "label": "",
        "answer": "no",
        "change": [
            "def run(",
            "tp, fp, p, r, f1, ap, ap_class = ap_per_class(*stats, plot=plots, save_dir=save_dir, names=names)",
            "ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95",
            "mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()",
            "-        nt = np.bincount(stats[3].astype(int), minlength=nc)  # number of targets per class",
            "-    else:",
            "-        nt = torch.zeros(1)",
            "+    nt = np.bincount(stats[3].astype(int), minlength=nc)  # number of targets per class",
            "",
            "# Print results",
            "pf = '%20s' + '%11i' * 2 + '%11.3g' * 4  # print format"
        ]
    },
    {
        "number": 4083,
        "comments": "",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "label": "",
        "answer": "no",
        "change": [
            "from .tokenization_utils import BatchEncoding",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "-TF_OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"openai-gpt\": \"https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-tf_model.h5\"",
            "-}",
            "+TF_OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = {\"openai-gpt\": \"https://cdn.huggingface.co/openai-gpt-tf_model.h5\"}",
            "",
            "",
            "def gelu(x):"
        ]
    },
    {
        "number": 4084,
        "comments": "",
        "commit_message": "Spacy token indexer (#3040)\n\n* add a tokenizer to ud\n\n* add spacy indexer\n\n* allow token_indexers to specify their own type\n\n* dumb hack to allow a whitespace spacy tokenizer...\n\n* pass through token embedder\n\n* add ndarray to TokenType, tests for pass through embedder\n\n* add doc\n\n* remove todo, test\n\n* fix docs\n\n* why is this test flaky\n\n* fix the correct test\n\n* add as_padded_tensor method\n\n* better place for depreciation stuff\n\n* add warning for calling inherited get_padding_token\n\n* ignore type for backward compatability\n\n* mattg comments\n\n* pylint\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TestElmoTokenRepresentation(ElmoTestCase):",
            "for k in range(10):",
            "char_indices = indices[\"elmo\"][(k * 50):((k + 1) * 50)]",
            "sentences.append(",
            "-                    indexer.pad_token_sequence(",
            "+                    indexer.as_padded_tensor(",
            "{'key': char_indices}, desired_num_tokens={'key': 50}, padding_lengths={}",
            ")['key']",
            ")",
            "-        batch = torch.from_numpy(numpy.array(sentences))",
            "+        batch = torch.stack(sentences)",
            "",
            "elmo_token_embedder = _ElmoCharacterEncoder(self.options_file, self.weight_file)",
            "elmo_token_embedder_output = elmo_token_embedder(batch)"
        ]
    },
    {
        "number": 4086,
        "comments": "",
        "commit_message": "Reimplement \"FCOS: Fully Convolutional One-Stage Object Detection\" (#586)\n\n* add fcos\n\n* use P5 instead of C5\n\n* add relu before extra convs in FPN\n\n* add singleclass_nms, use caffe2 lr\n\n* fix log interval\n\n* use caffe2init and relu in extra layers\n\n* fix scale layer, use p5 instead of c5\n\n* fix focs target\n\n* refactor code\n\n* delete useless file\n\n* clean\n\n* refactor code\n\n* change num_classes to cls_out_channels\n\n* fix bug of in get_bboxes\n\n* fix bug in test\n\n* add r101 2x cfg\n\n* ms use value mode, add x101-64x4d cfg\n\n* add more comment and rename some variable\n\n* rename centers to points, modify doc string of distance2bbox\n\n* add fcos detector, replace frozen with requires_grad\n\n* add README.md\n\n* add r101-1x performance, rename cfg, add detector FCOS\n\n* update fcos r50 2x performance, remove fpn caffe2 initialize\n\n* fix flake8 error\n\n* rename cfg\n\n* fix grammar error of some comments\n\n* minor fix comment\n\n* change work_dir to be consistent with config name\n\n* add FCOS support in README\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def multiclass_nms(multi_bboxes, multi_scores, score_thr, nms_cfg, max_num=-1):",
            "else:",
            "_bboxes = multi_bboxes[cls_inds, i * 4:(i + 1) * 4]",
            "_scores = multi_scores[cls_inds, i]",
            "+        if score_factors is not None:",
            "+            _scores *= score_factors[cls_inds]",
            "cls_dets = torch.cat([_bboxes, _scores[:, None]], dim=1)",
            "cls_dets, _ = nms_op(cls_dets, **nms_cfg_)",
            "-        cls_labels = multi_bboxes.new_full(",
            "-            (cls_dets.shape[0], ), i - 1, dtype=torch.long)",
            "+        cls_labels = multi_bboxes.new_full((cls_dets.shape[0], ),",
            "+                                           i - 1,",
            "+                                           dtype=torch.long)",
            "bboxes.append(cls_dets)",
            "labels.append(cls_labels)",
            "if bboxes:"
        ]
    },
    {
        "number": 4087,
        "comments": "",
        "commit_message": "[common attributes] Fix previous commit for transfo-xl\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CommonTestCases:",
            "model = model_class(config)",
            "self.assertIsInstance(",
            "model.get_input_embeddings(),",
            "-                    torch.nn.Embedding",
            "+                    (torch.nn.Embedding, AdaptiveEmbedding)",
            ")",
            "model.set_input_embeddings(torch.nn.Embedding(10, 10))",
            "x = model.get_output_embeddings()"
        ]
    },
    {
        "number": 4091,
        "comments": "",
        "commit_message": "Fix BERT/MobileBERT classifier dropout\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BertForMultipleChoice(BertPreTrainedModel):",
            "super().__init__(config)",
            "",
            "self.bert = BertModel(config)",
            "-        self.dropout = nn.Dropout(config.classifier_dropout_prob)",
            "+        self.dropout = nn.Dropout(config.classifier_dropout)",
            "self.classifier = nn.Linear(config.hidden_size, 1)",
            "",
            "self.init_weights()"
        ]
    },
    {
        "number": 4092,
        "comments": "",
        "commit_message": "Fix enumeration notebook tiny problem (#2621)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "\"@config_enumerate\\n\",",
            "\"def model(data, num_components=3):\\n\",",
            "\"    print('Running model with {} data points'.format(len(data)))\\n\",",
            "-    \"    p = pyro.sample(\\\"p\\\", dist.Dirichlet(0.5 * torch.ones(3)))\\n\",",
            "+    \"    p = pyro.sample(\\\"p\\\", dist.Dirichlet(0.5 * torch.ones(num_components)))\\n\",",
            "\"    scale = pyro.sample(\\\"scale\\\", dist.LogNormal(0, num_components))\\n\",",
            "\"    with pyro.plate(\\\"components\\\", num_components):\\n\",",
            "\"        loc = pyro.sample(\\\"loc\\\", dist.Normal(0, 10))\\n\","
        ]
    },
    {
        "number": 4094,
        "comments": "",
        "commit_message": "quickfix batch shape\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PGModel(Model):",
            "advantage = episode['returns'] - baseline",
            "",
            "if self.normalize_advantage:",
            "-            return zero_mean_unit_variance(advantage)",
            "+            return np.squeeze(zero_mean_unit_variance(advantage))",
            "else:",
            "-            return advantage",
            "+            return np.squeeze(advantage)"
        ]
    },
    {
        "number": 4095,
        "comments": "",
        "commit_message": "Add docstrings of core modules and methods (#3120)\n\n* Add docstrings of core modules and methods\n\n* Update docs and fix comments\n\n* Complete docstrings\n\n* Resolve comments\n\n* reformat docstrings\n\n* resolve comments\n",
        "label": "",
        "answer": "no",
        "change": [
            "def bbox2roi(bbox_list):",
            "",
            "",
            "def roi2bbox(rois):",
            "+    \"\"\"Convert rois to bounding box format",
            "+",
            "+    Args:",
            "+        rois (torch.Tensor): RoIs with the shape (n, 5) where the first",
            "+            column indicates batch id of each RoI.",
            "+",
            "+    Returns:",
            "+        list[torch.Tensor]: Converted boxes of corresponding rois.",
            "+    \"\"\"",
            "bbox_list = []",
            "img_ids = torch.unique(rois[:, 0].cpu(), sorted=True)",
            "for img_id in img_ids:"
        ]
    },
    {
        "number": 4100,
        "comments": "",
        "commit_message": "[RLlib] Minor fixes (torch GPU bugs + some cleanup). (#11609)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DQNTorchModel(TorchModelV2, nn.Module):",
            "if self.num_atoms > 1:",
            "# Distributional Q-learning uses a discrete support z",
            "# to represent the action value distribution",
            "-            z = torch.range(0.0, self.num_atoms - 1, dtype=torch.float32)",
            "+            z = torch.range(",
            "+                0.0, self.num_atoms - 1,",
            "+                dtype=torch.float32).to(action_scores.device)",
            "z = self.v_min + \\",
            "z * (self.v_max - self.v_min) / float(self.num_atoms - 1)"
        ]
    },
    {
        "number": 4103,
        "comments": "",
        "commit_message": "fix flake8\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class EntityLinker(flair.nn.DefaultClassifier[Sentence]):",
            "if len(embedding_list) > 0:",
            "embedded_entity_pairs = torch.cat(embedding_list, 0)",
            "",
            "-            return embedded_entity_pairs,",
            "+            return (embedded_entity_pairs,)",
            "else:",
            "-            return torch.zeros(0, self.word_embeddings.embedding_length, device=flair.device),",
            "+            return (torch.zeros(0, self.word_embeddings.embedding_length, device=flair.device),)",
            "",
            "def _get_state_dict(self):",
            "model_state = {"
        ]
    },
    {
        "number": 4107,
        "comments": "",
        "commit_message": "support for TF 1.13, improved while loops, fixed lstm problem\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class InternalLstm(InternalLayer, TransformationBase):",
            ")",
            "",
            "def tf_apply(self, x, state):",
            "-        state = tf.contrib.rnn.LSTMStateTuple(c=state[:, 0, :], h=state[:, 1, :])",
            "+        state = tf.nn.rnn_cell.LSTMStateTuple(c=state[:, 0, :], h=state[:, 1, :])",
            "",
            "x, state = self.cell(inputs=x, state=state)",
            "state = tf.stack(values=(state.c, state.h), axis=1)"
        ]
    },
    {
        "number": 4110,
        "comments": "",
        "commit_message": "[Tests] Fix UnCLIP cpu offload tests (#1769)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class UnCLIPPipelineIntegrationTests(unittest.TestCase):",
            ")",
            "",
            "mem_bytes = torch.cuda.max_memory_allocated()",
            "-        # make sure that less than 1.5 GB is allocated",
            "-        assert mem_bytes < 1.5 * 10**9",
            "+        # make sure that less than 7 GB is allocated",
            "+        assert mem_bytes < 7 * 10**9"
        ]
    },
    {
        "number": 4112,
        "comments": "",
        "commit_message": "[torch] remove deprecated uint8 in favor of bool (#21384)\n\n* uint8 -> bool\n\n* fix copies\n\n* style\n\n* update test modeling commen when checking attention buffers\n\n* style\n\n* use logical not on random mask instead of subtraction with 1\n\n* remove torch uint8\n\n* quality\n\n* remove modified modeling utils\n\n* Update based on review\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n\n---------\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GPTNeoXJapaneseAttention(nn.Module):",
            "batch_size, num_attention_heads, query_length, attn_head_size = query.size()",
            "key_length = key.size(-2)",
            "",
            "-        causal_mask = self._create_casual_mask(key_length, query_length)",
            "+        causal_mask = self._create_causal_mask(key_length, query_length)",
            "",
            "query = query.view(batch_size * num_attention_heads, query_length, attn_head_size)",
            "key = key.view(batch_size * num_attention_heads, key_length, attn_head_size)"
        ]
    },
    {
        "number": 4113,
        "comments": "",
        "commit_message": "Adding neural HMM TTS Model (#2272)\n\n* Adding neural HMM TTS\n\n* Adding tests\n\n* Adding neural hmm on readme\n\n* renaming training recipe\n\n* Removing overflow\\s decoder parameters from the config\n\n* Update the Trainer requirement version for a compatible one (#2276)\n\n* Bump up to v0.10.2\n\n* Adding neural HMM TTS\n\n* Adding tests\n\n* Adding neural hmm on readme\n\n* renaming training recipe\n\n* Removing overflow\\s decoder parameters from the config\n\n* fixing documentation\n\nCo-authored-by: Edresson Casanova <edresson1@gmail.com>\nCo-authored-by: Eren G\u00f6lge <erogol@hotmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Overflow(BaseTTS):",
            "self.register_buffer(\"mean\", torch.tensor(0))",
            "self.register_buffer(\"std\", torch.tensor(1))",
            "",
            "-        # self.mean = nn.Parameter(torch.zeros(1), requires_grad=False)",
            "-        # self.std = nn.Parameter(torch.ones(1), requires_grad=False)",
            "-",
            "def update_mean_std(self, statistics_dict: Dict):",
            "self.mean.data = torch.tensor(statistics_dict[\"mean\"])",
            "self.std.data = torch.tensor(statistics_dict[\"std\"])"
        ]
    },
    {
        "number": 4114,
        "comments": "",
        "commit_message": "Fix kernel dimensions for LeNet model code example (#2192)\n\n* Change kernel to 5x5 in  1st Conv2d layer in model init\n\nSigned-off-by: Kiersten Stokes <kierstenstokes@gmail.com>\n\n* Change kernel to 5x5 in 2nd Conv2d layer in model init\n\n* Fix dimensions of 1st Linear layer to match new expected size\n---------\nSigned-off-by: Kiersten Stokes <kierstenstokes@gmail.com>\nCo-authored-by: Suraj Subramanian <5676233+suraj813@users.noreply.github.com>\nCo-authored-by: Svetlana Karslioglu <svekars@fb.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LeNet(nn.Module):",
            "",
            "def __init__(self):",
            "super(LeNet, self).__init__()",
            "-        # 1 input image channel (black & white), 6 output channels, 3x3 square convolution",
            "+        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution",
            "# kernel",
            "-        self.conv1 = nn.Conv2d(1, 6, 3)",
            "-        self.conv2 = nn.Conv2d(6, 16, 3)",
            "+        self.conv1 = nn.Conv2d(1, 6, 5)",
            "+        self.conv2 = nn.Conv2d(6, 16, 5)",
            "# an affine operation: y = Wx + b",
            "-        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension",
            "+        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension",
            "self.fc2 = nn.Linear(120, 84)",
            "self.fc3 = nn.Linear(84, 10)"
        ]
    },
    {
        "number": 4117,
        "comments": "",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def quantile(",
            "",
            "temp = a.reshape((-1,) + tuple(desired_shape))",
            "",
            "-        return torch.quantile(temp, q, dim=0, keepdim=keepdims, interpolation=interpolation)",
            "+        return torch.quantile(",
            "+            temp, q, dim=0, keepdim=keepdims, interpolation=interpolation",
            "+        )",
            "",
            "return torch.quantile(a, q, dim=axis, keepdim=keepdims, interpolation=interpolation)"
        ]
    },
    {
        "number": 4119,
        "comments": "",
        "commit_message": "Deprecate prepare_module (#3166)\n\n* Refactor prepare_module\n\n* Add deprecation warning in prepare_module\n\n* Remove prepare_module in inspect\n\n* Remove prepare_module in patching\n\n* Remove prepare_module in dummy_data\n\n* Remove prepare_module in run_beam\n\n* Remove prepare_module in test_dataset_common\n\n* Fix hash in run_beam\n\n* Remove prepare_module from test_load\n\n* Remove prepare_module from test_metric_common\n\n* Remove prepare_module from test_hf_gcp\n\n* Use deprecated function instead\n\n* Add deprecation to docstring\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class LocalMetricTest(parameterized.TestCase):",
            "@slow",
            "def test_load_real_metric(self, metric_name):",
            "doctest.ELLIPSIS_MARKER = \"[...]\"",
            "-        metric_module = importlib.import_module(datasets.load.prepare_module(os.path.join(\"metrics\", metric_name))[0])",
            "+        metric_module = importlib.import_module(",
            "+            datasets.load.metric_module_factory(os.path.join(\"metrics\", metric_name)).module_path",
            "+        )",
            "# run doctest",
            "with self.use_local_metrics():",
            "results = doctest.testmod(metric_module, verbose=True, raise_on_error=True)"
        ]
    },
    {
        "number": 4125,
        "comments": "",
        "commit_message": "fix mulenc asr\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CustomConverterMulEnc(object):",
            "self.dtype = dtype",
            "self.num_encs = len(subsamping_factors)",
            "",
            "-    def __call__(self, batch, device):",
            "+    def __call__(self, batch, device=torch.device('cpu')):",
            "\"\"\"Transform a batch and send it to a device.",
            "",
            "Args:"
        ]
    },
    {
        "number": 4128,
        "comments": "",
        "commit_message": "[hotfix] Lint formatting for new Tune optimizer ZOOpt (#8040)\n\n* formatting\n\n* removedill\n\n* lint\n",
        "label": "",
        "answer": "no",
        "change": [
            "MOCK_MODULES = [",
            "\"tensorflow.contrib.rnn\", \"tensorflow.contrib.slim\", \"tensorflow.core\",",
            "\"tensorflow.core.util\", \"tensorflow.python\", \"tensorflow.python.client\",",
            "\"tensorflow.python.util\", \"torch\", \"torch.distributed\", \"torch.nn\",",
            "-    \"torch.nn.parallel\", \"torch.utils.data\", \"torch.utils.data.distributed\"",
            "+    \"torch.nn.parallel\", \"torch.utils.data\", \"torch.utils.data.distributed\",",
            "+    \"zoopt\"",
            "]",
            "for mod_name in MOCK_MODULES:",
            "sys.modules[mod_name] = mock.Mock()"
        ]
    },
    {
        "number": 4132,
        "comments": "",
        "commit_message": "Fix tflongformer int dtype (#18907)\n\n* Use int64 throughout TFLongFormer\n\n* make style\n\n* Do some more fixed casting in TFLongFormer\n\n* Fix some wonky \"is None\" conditionals\n\n* Cast all the dtypes, salt the earth\n\n* Fix copies to TFLED as well and do some casting there\n\n* dtype fix in TFLongformer test\n\n* Make fixup\n\n* Expand tolerances on the LED tests too (I think this is a TF32 thing)\n\n* Expand test tolerances for LED a tiny bit (probably a Tensorfloat thing again)\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFLEDModelIntegrationTest(unittest.TestCase):",
            "expected_slice = tf.convert_to_tensor(",
            "[[33.6507, 6.4572, 16.8089], [5.8739, -2.4238, 11.2902], [-3.2139, -4.3149, 4.2783]],",
            ")",
            "-        tf.debugging.assert_near(output[:, :3, :3], expected_slice, atol=TOLERANCE)",
            "+        tf.debugging.assert_near(output[:, :3, :3], expected_slice, atol=1e-3, rtol=1e-3)"
        ]
    },
    {
        "number": 4134,
        "comments": "",
        "commit_message": "FIX small bugs in `run_classifier_pytorch.py`\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main():",
            "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)",
            "",
            "model.train()",
            "-        for epoch in args.num_train_epochs:",
            "+        for epoch in range(args.num_train_epochs):",
            "for input_ids, input_mask, segment_ids, label_ids in train_dataloader:",
            "input_ids = input_ids.to(device)",
            "input_mask = input_mask.float().to(device)"
        ]
    },
    {
        "number": 4135,
        "comments": "",
        "commit_message": "Add Cuda tests for pyro.distributions; fix errors (#297)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Dirichlet(Distribution):",
            "alpha = self._sanitize_input(alpha)",
            "if alpha.dim() not in (1, 2):",
            "raise ValueError('Expected alpha.dim() in (1,2), actual: {}'.format(alpha.dim()))",
            "-        alpha_np = alpha.data.numpy()",
            "+        alpha_np = alpha.data.cpu().numpy()",
            "if alpha.dim() == 1:",
            "x_np = spr.dirichlet.rvs(alpha_np)[0]",
            "else:",
            "x_np = np.empty_like(alpha_np)",
            "for i in range(alpha_np.shape[0]):",
            "x_np[i, :] = spr.dirichlet.rvs(alpha_np[i, :])[0]",
            "-        x = Variable(torch.Tensor(x_np))",
            "+        x = Variable(type(alpha.data)(x_np))",
            "return x",
            "",
            "# TODO Remove the batch_size argument."
        ]
    },
    {
        "number": 4136,
        "comments": "",
        "commit_message": "[rllib] Use RLlib preprocessors in DQN (fixes PongDeterministic-v4) (#1124)\n\n* fix pong\n\n* rename\n\n* update\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DQNGraph(object):",
            "with tf.variable_scope(\"q_func\", reuse=True):",
            "q_tp1_using_online_net = _build_q_network(",
            "self.obs_tp1, num_actions, config)",
            "-            q_tp1_best_using_online_net = tf.arg_max(q_tp1_using_online_net, 1)",
            "+            q_tp1_best_using_online_net = tf.argmax(q_tp1_using_online_net, 1)",
            "q_tp1_best = tf.reduce_sum(",
            "self.q_tp1 * tf.one_hot(",
            "q_tp1_best_using_online_net, num_actions), 1)"
        ]
    },
    {
        "number": 4137,
        "comments": "",
        "commit_message": "Several micro optimizations (#4833)\n\n* benchmark transfers\n\n* create tensors directl on device when possible\n\n* fix\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ROUGE(Metric):",
            "sequence_count = len(predictions)",
            "if is_distributed():",
            "device = predictions.device",
            "-            _sequence_count = torch.tensor(sequence_count).to(device)",
            "+            _sequence_count = torch.tensor(sequence_count, device=device)",
            "dist.all_reduce(_sequence_count, op=dist.ReduceOp.SUM)",
            "sequence_count = _sequence_count.item()",
            "self._total_sequence_count += sequence_count"
        ]
    },
    {
        "number": 4138,
        "comments": "",
        "commit_message": "fix integration test levit (#17555)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LevitModelIntegrationTest(unittest.TestCase):",
            "expected_shape = torch.Size((1, 1000))",
            "self.assertEqual(outputs.logits.shape, expected_shape)",
            "",
            "-        expected_slice = torch.tensor([0.0096, -1.0084, -1.4318]).to(torch_device)",
            "+        expected_slice = torch.tensor([1.0448, -0.3745, -1.8317]).to(torch_device)",
            "",
            "self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))"
        ]
    },
    {
        "number": 4140,
        "comments": "",
        "commit_message": "Fix the CI (#4903)\n\n* Fix CI\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ModelTesterMixin:",
            "model.to(torch_device)",
            "model.eval()",
            "with torch.no_grad():",
            "-                outputs = model(**inputs_dict)",
            "+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))",
            "attentions = outputs[-1]",
            "self.assertEqual(model.config.output_hidden_states, False)",
            "self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)"
        ]
    },
    {
        "number": 4141,
        "comments": "",
        "commit_message": "Add linting with black (#2678)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2678\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D32653381\n\nPulled By: dianaml0\n\nfbshipit-source-id: 2810d14867cd7d64f4d340740e2b590b82de47fe\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def setup_model_loss_criterion(cfg, args, rank, is_cuda):",
            "loss_fn = loss_fn.cuda()",
            "",
            "optimizer = optim.sgd.SGD(args, model.parameters())",
            "-    optimizer = optim.FairseqBMUF(",
            "-        cfg=cfg.bmuf,",
            "-        optimizer=optimizer",
            "-    )",
            "+    optimizer = optim.FairseqBMUF(cfg=cfg.bmuf, optimizer=optimizer)",
            "",
            "return model, loss_fn, optimizer"
        ]
    },
    {
        "number": 4142,
        "comments": "",
        "commit_message": "Heterogeneous Graph Support + GraphGym (#3068)\n\n* added HGT DBLP example\n\n* typo\n\n* Merge PyG master (#52)\n\n* Adding the Facebok Page-Page dataset\n\n* type hints\n\n* documentation CI\n\n* py 3.8\n\n* fix links\n\n* fix links\n\n* fail on warning\n\n* fail on warning\n\n* fix doc\n\nCo-authored-by: benedekrozemberczki <benedek.rozemberczki@gmail.com>\n\n* revert\n\n* Fix Documentation Rendering (#51)\n\n* fix doc rendering\n\n* fix linting\n\n* retrigger checks\n\n* remove pytorch 1.7.0 legacy code (#50)\n\n* Fix `copy.deepcopy` within lazy `nn.dense.Linear` (#44)\n\n* fix deepcopy within lazy Linear\n\n* fix merge\n\n* assert exception\n\n* example to doc\n\n* resolve conflict\n\n* resolve conflict\n\n* Add Figure and Equation to `to_hetero` docstring (#60)\n\n* add tex\n\n* add svg + docstring\n\n* typo\n\n* added equation\n\n* Message Passing Hooks (#53)\n\n* add hooks\n\n* docstring\n\n* add docstring\n\n* allow modification of inputs/output\n\n* add test for modifying output\n\n* add additional asserts for modifying output test\n\n* Rename `HeteroData.get_edges` and `HeteroData.get_nodes` (#58)\n\n* rename to_edges and to_nodes\n\n* typo\n\n* `HeteroConv` (#64)\n\n* clean heteroconv\n\n* init\n\n* init\n\n* clean up\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix documentation\n\n* bipartite function\n\n* fix test CI\n\n* remove pillow version\n\n* clean up for merge\n\n* Merge PyG master (#69)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\n\n* re-merge\n\n* add lazy column to GNN cheatsheet (#70)\n\n* `to_hetero_with_bases(model)` (#63)\n\n* update\n\n* fix linting\n\n* basisconv\n\n* add ValueError\n\n* to_hetero_with_bases impl done\n\n* add test\n\n* add comments\n\n* add comments\n\n* docstring\n\n* typo\n\n* update figure\n\n* svg\n\n* typo\n\n* add test\n\n* update\n\n* add rgcn equality test\n\n* typos\n\n* update\n\n* typos\n\n* update figures\n\n* generate new svgs\n\n* fix assignment\n\n* rename\n\n* delete sorted edge types\n\n* rename\n\n* add legend\n\n* fix typo\n\n* Test: Check equal outputs of `to_hetero` and `RGCNConv` (#59)\n\n* check equal output\n\n* add sparsetensor test\n\n* check equal output\n\n* add sparsetensor test\n\n* rename\n\n* linting\n\n* add missing import\n\n* `HeteroData` support for `T.NormalizeFeatures` (#56)\n\n* normalize features\n\n* allow normalization of any feature\n\n* in-place div\n\n* normalize features\n\n* allow normalization of any feature\n\n* in-place div\n\n* fix test\n\n* no need to re-assign\n\n* `HeteroData` support for `T.AddSelfLoops` (#54)\n\n* hetero support for AddSelfLoops\n\n* check for edge_index attribute\n\n* f-string\n\n* retrigger checks\n\n* revert bipartite changes\n\n* hetero support for AddSelfLoops\n\n* check for edge_index attribute\n\n* f-string\n\n* retrigger checks\n\n* revert bipartite changes\n\n* merge master\n\n* merge master\n\n* `HeteroData` support for `T.ToSparseTensor` (#55)\n\n* hetero support for ToSparseTensor\n\n* add test\n\n* customize the attribute of SparseTensor.value\n\n* rework sort_edge_index\n\n* hetero support for ToSparseTensor\n\n* add test\n\n* customize the attribute of SparseTensor.value\n\n* rework sort_edge_index\n\n* linting\n\n* `HeteroData` support for `T.ToUndirected` (#57)\n\n* to_undirected\n\n* revert bipartite changes\n\n* coalesce + undirected enhancement\n\n* merge master\n\n* revert bipartite changes\n\n* coalesce + undirected enhancement\n\n* merge master\n\n* clean up\n\n* new default relation type\n\n* fix tests\n\n* resolve merge conflicts\n\n* resolve merge conflicts 2\n\n* resolve merge conflicts 3\n\n* Merge PyG master (#74)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\n* clean up doc for to_dense_batch\n\n* clean up\n\n* add legend to cheatsheet\n\n* Improve terminology (#2837)\n\nI think the previous version of the document uses the term 'symmetric' incorrectly. A symmetric matrix is a square matrix that is is equal to its transpose (https://en.wikipedia.org/wiki/Symmetric_matrix). However, the text is only talking about the shape of the matrix, not its content. Hence, 'square (matrix)' would be the correct term to use.\n\n* Add batch_size input to to_dense_batch (#2838)\n\n* Add batch_size input to to_dense_batch\n\n* to_dense_batch fix typo in batch_size param use\n\n* add typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Added return_attention_weights to TransformerConv. (#2807)\n\n* added return_weights functionality to tranformer\n\n* added return attn weights tests\n\n* flake8\n\n* added typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* MD17 (#2843)\n\n* Added MD17 dataset\n\n* Updated Documentation\n\n* Added link to sGDML website in doc\n\n* fixed typos in doc and made train variable description clearer\n\n* clean up\n\n* fix linting\n\n* fix doc warning\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* update doc\n\n* remove forward doc\n\n* add static graph support info to cheatsheet\n\n* fix num_nodes in case edge_index is empty\n\n* fix math formula\n\n* faster GDC import\n\n* lazy import\n\n* lazy import for datasets\n\n* lazy import for nn\n\n* Sequential jittable + traceable\n\n* typo\n\n* typo\n\n* update doc\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Sch\u00e4fer <moritz.schaefer@protonmail.com>\n\n* re-add\n\n* GraphGym cleaned version (#82)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Update .gitignore\n\n* Integrate GraphGym into PyG (#85)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Integrate graphgym into pyg, keep user API in project root\n\n* fix merge conflict\n\n* fix lint errors\n\n* Make optional dependencies\n\n* merge LICENSE from GraphGym\n\n* add import\n\n* clean up LICENSE\n\n* fix import\n\n* resolve merge conflicts\n\n* resolve merge conflicts 2\n\n* Merge PyG master (#87)\n\n* renaming: PointConv to PointNetConv\n\n* Fix a broken link in datasets/gdelt.py (#2800)\n\n* fix test\n\n* re-add batching of strings\n\n* add quick start table\n\n* gnn cheatsheet\n\n* remove pillow version\n\n* clean up doc for to_dense_batch\n\n* clean up\n\n* add legend to cheatsheet\n\n* Improve terminology (#2837)\n\nI think the previous version of the document uses the term 'symmetric' incorrectly. A symmetric matrix is a square matrix that is is equal to its transpose (https://en.wikipedia.org/wiki/Symmetric_matrix). However, the text is only talking about the shape of the matrix, not its content. Hence, 'square (matrix)' would be the correct term to use.\n\n* Add batch_size input to to_dense_batch (#2838)\n\n* Add batch_size input to to_dense_batch\n\n* to_dense_batch fix typo in batch_size param use\n\n* add typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Added return_attention_weights to TransformerConv. (#2807)\n\n* added return_weights functionality to tranformer\n\n* added return attn weights tests\n\n* flake8\n\n* added typehints\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* MD17 (#2843)\n\n* Added MD17 dataset\n\n* Updated Documentation\n\n* Added link to sGDML website in doc\n\n* fixed typos in doc and made train variable description clearer\n\n* clean up\n\n* fix linting\n\n* fix doc warning\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* update doc\n\n* remove forward doc\n\n* add static graph support info to cheatsheet\n\n* fix num_nodes in case edge_index is empty\n\n* fix math formula\n\n* faster GDC import\n\n* lazy import\n\n* lazy import for datasets\n\n* lazy import for nn\n\n* Sequential jittable + traceable\n\n* typo\n\n* typo\n\n* update doc\n\n* Simple models (#2869)\n\n* Inclusion of new backbone models\n\n* Eliminating head from asap.py\n\n* small correction\n\n* Create test_gcn.py\n\n* Update __init__.py\n\n* Update test_gcn.py\n\n* Left only the convolutional simple models\n\n* Tests included\n\n* update\n\n* clean up\n\n* clean up v2\n\n* fix activation\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* Example for MemPooling. (#2729)\n\n* example for mem pooling\n\n* backprop on kl loss is done at the end of an epoch. Keys in memory layers are trained only on kl loss.\n\n* added learning rate decay. Using PROTIENS_full\n\n* flake8\n\n* reduced lr. increased weight decay\n\n* changed download location\n\n* added comments\n\n* clean up\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typos\n\n* fix removeisolatednodes transform in case 'data.num_nodes' is present\n\n* fix XConv with dilation > 1\n\n* fix XConv with dilation > 1\n\n* rgcn link prediction  (#2734)\n\n* implemented LinkPrediction dataset for loading FB15k237\n\n* implemented evaluation for relational link prediction\n\n* implemented R-GCNConf link prediction example\n\n* fixed bug: wrong initial objects in negative_sampling\n\n* changed file downloader urllib.request.urlretrieve  to pytorch.data.download_url; renamed LinkPrediction class to RelationalLinkPredictionDataset\n\n* update dataset\n\n* update example script\n\n* rename\n\nCo-authored-by: Moritz <moritzblum>\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix gnnexplainer draw kwargs\n\n* remove python-louvain dependency\n\n* allow customization of output in MP jit mode\n\n* fix test for py3.6\n\n* changed normalisation to same norm from instance norm to be robust to small var (#2917)\n\n* add CITATION.cff\n\n* format\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* [ci skip]\n\n* add basetransform ABC (#2924)\n\n* clean up BaseTransform\n\n* clean up GATConv and add comments\n\n* add max_num_neighbors as an additional argument\n\n* fix jit GATConv on PyTorch 1.8.0\n\n* fix doc\n\n* fix gnn explainer with existing self-loops\n\n* Rgcn link pred fix (#2946)\n\n* added regularization, removed typo in test\n\n* clean up\n\nCo-authored-by: Moritz <moritzblum>\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* Correct gini coefficient mathcal formula (#2932)\n\n* typo\n\n* typo\n\n* Update from_networkx (#2923)\n\n* Update from_networkx\n\n* Update test\n\n* Update convert.py\n\n* Minor corrections\n\n* Update test_convert.py\n\n* Corrections\n\n* Update test_convert.py\n\n* Case where there are no edges\n\n* Correcting how edge_attr are concatenated\n\n* clean up + new test\n\n* remove unused code\n\n* add union type\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix deterministic ordering in from_networkx\n\n* recursive-include *.jinja files\n\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Sch\u00e4fer <moritz.schaefer@protonmail.com>\nCo-authored-by: PabloAMC <pmorenocf@alumnos.unex.es>\nCo-authored-by: Moritz Blum <31183934+moritzblum@users.noreply.github.com>\nCo-authored-by: fbragman <fbragman@users.noreply.github.com>\nCo-authored-by: Christopher Lee <2824685+CCInc@users.noreply.github.com>\nCo-authored-by: Tim Daubensch\u00fctz <tim@daubenschuetz.de>\n\n* resolve merge conflicts 3\n\n* resolve merge conflicts 4\n\n* Implementation of the `HGTLoader` + `ogbn-mag` example (#73)\n\n* first try\n\n* update\n\n* HGT Loader\n\n* typo\n\n* first try\n\n* update\n\n* HGT Loader\n\n* typo\n\n* bugfixes\n\n* lazy GATConv\n\n* bugfix\n\n* bugfix\n\n* full working pipeline\n\n* update\n\n* rename\n\n* docstring\n\n* typos\n\n* update\n\n* typo\n\n* typo\n\n* typo\n\n* added comments\n\n* add test\n\n* add tests\n\n* fix example\n\n* rename\n\n* linting\n\n* Random split functionalities (#72)\n\n* link split\n\n* create split\n\n* example tests\n\n* link split tests\n\n* fix linting\n\n* update docstring\n\n* undirected option, refactor and docs\n\n* add num nodes as argument to neg sampling\n\n* clean up + remove single object\n\n* update example\n\n* typo\n\n* fix compose\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* add basetransform\n\n* typo\n\n* typo\n\n* fix test\n\n* Improve `torch_geometric.data` Documentation (#98)\n\n* update data doc\n\n* typo\n\n* typo\n\n* note\n\n* typo\n\n* add docstring\n\n* only show inherited members for data and hetero_data\n\n* documentation update for batch and dataset\n\n* update doc\n\n* update\n\n* fix\n\n* record_stream\n\n* update\n\n* typo\n\n* add/fix data functionality\n\n* linting\n\n* typo\n\n* `_parent` memory leak fix (#103)\n\n* memory leak fix\n\n* Clean up\n\n* clean up\n\n* bugfix tests\n\n* typos\n\n* fix test\n\n* fix test\n\n* rename reverse\n\n* (Heterogeneous) `NeighborLoader` (#92)\n\n* initial commit\n\n* typo\n\n* neighbor loader functionality + tests\n\n* docstring\n\n* fix docstring\n\n* skip tests\n\n* fix share_memory_\n\n* typo\n\n* typo\n\n* update example\n\n* typo\n\n* share_strategy\n\n* fix cuda calls\n\n* better print\n\n* fix size\n\n* fix print\n\n* final commit\n\n* fix\n\n* some todos\n\n* preprocessed features\n\n* fix to_undirected\n\n* more documentation\n\n* update doc\n\n* fix doc\n\n* fix doc\n\n* Add benchmark code and the example with existing graph classification examples (#93)\n\n* add benchmarking utilities\n\n* update graph classification benchmark\n\n* improve code style\n\n* add pytorch-memlab for benchmark code\n\n* skip some tests when cuda is not available\n\n* add type hint when appropriate\n\n* add seed_everything to improve code\n\n* code refactoring\n\n* code refactoring\n\n* code refactoring\n\n* code improvement\n\n* remove unnecessary dataloader import\n\n* change benchmark interface with decorator\n\n* documentation improvement\n\n* linting\n\n* linting part 2\n\n* linting part 3\n\n* seed_everything\n\n* create utils file\n\n* update\n\n* use utils functions\n\n* fix test\n\n* update the profiler to the latest torch (1.8.1+)\n\n* refactor profiler and add more documentation\n\n* refactor profiler and add more documentation\n\n* resolve lint errors\n\n* resolve lint errors\n\n* update\n\n* clean up test and profile\n\n* fix linting\n\n* add to doc\n\n* fix doc\n\n* typo\n\n* update benchmark\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* Move `HGTLoader` to `torch_geometric.loader` + clean up (#104)\n\n* move files\n\n* use utils functions\n\n* fix example\n\n* update\n\n* fix tests\n\n* fix seed\n\n* fix linear test\n\n* rename\n\n* Support GraphGym custom modules outside PyG package (#102)\n\n* GraphGym cleaned version\n\n* remove deepsnap dependency\n\n* fix lint errors, part 1\n\n* fix all lint errors\n\n* fix all lint errors\n\n* fix all lint errors\n\n* apply yapf\n\n* Integrate graphgym into pyg, keep user API in project root\n\n* fix merge conflict\n\n* fix lint errors\n\n* Make optional dependencies\n\n* merge LICENSE from GraphGym\n\n* Enable adding GraphGym customized modules outside PyG package\n\n* lint\n\n* Rename `AddTrainValTestMask` to `RandomNodeSplit` (#108)\n\n* initial commit\n\n* rename example\n\n* remove AddTrainValTestMask\n\n* fix linting\n\n* create optimizer config and scheduler config separately (#113)\n\n* create optimizer config and scheduler config separately\n\n* fix format\n\n* import explicitly\n\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\n\n* Heterogeneous Graph Tutorial (#83)\n\n* add HG tutorial roadmap\n\n* started working on hg tutorial\n\n* hg_tutorial, some text and .tex figure\n\n* added svg\n\n* hg tutorial content\n\n* fix CI\n\n* text and structure\n\n* finished first draft\n\n* fixed one code example\n\n* fixing conventions\n\n* fixing links\n\n* update svg\n\n* some smaller improvements of tutorial\n\n* improvements on tutorial\n\n* hg-tutorial: fixed compiling issue, added detailed content\n\n* added absolute links\n\n* fixed warnings\n\n* streamlined dataset section\n\n* update svg\n\n* update tutorial\n\n* update 2\n\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\n\n* typo\n\n* Move data loaders to `torch_geometric.loader` (#110)\n\n* move graphsaint\n\n* deprecations\n\n* move clusterloader\n\n* deprecations\n\n* type hints\n\n* move shadow\n\n* typo\n\n* typo\n\n* move datalistloader\n\n* dense data loader\n\n* random node sampler\n\n* fix doc\n\n* Lazy GNN operators (#89)\n\n* lazy cheb conv\n\n* lazy GraphConv\n\n* lazy GATv2Conv\n\n* lazy TAGConv\n\n* lazy FAConv\n\n* lazy FeaStConv\n\n* lazy NNConv\n\n* typo\n\n* fix tests\n\n* lazy SuperGATConv\n\n* lazy SuperGATConv fix\n\n* lazy SplineConv\n\n* fix lazy check\n\n* lazy GravNetConv\n\n* arma conv lazy\n\n* dense linear in gmmconv\n\n* typo\n\n* add test\n\n* lazy GMMConv\n\n* doc\n\n* rename (#116)\n\n* Revisit `MetaPath2Vec` (#114)\n\n* revisit metapath2vec\n\n* update\n\n* typo\n\n* update\n\n* fix doc\n\n* update\n\n* check for attributes rather than key\n\n* Clean up `torch_geometric.profile` further (#111)\n\n* remove print_layer_stats\n\n* typos\n\n* update\n\n* readme highlights and quick tour (#99)\n\n* readme highlights and quick tour\n\n* arch\n\n* arch image\n\n* arch overview\n\n* list categories\n\n* categorization\n\n* category description\n\n* Update README.md\n\nfrom Matthias\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* improved highlights\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* Update README.md\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* minor\n\n* update readme\n\n* update\n\n* update\n\n* update\n\n* update\n\n* fix url\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* move ops\n\n* toc\n\n* typo\n\n* typo\n\n* add svgs\n\n* update figure\n\n* fix links\n\n* fix size\n\n* fix size\n\n* typo\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* fix broken links\n\n* fix links\n\n* Heterogeneous Graph Sampler Tutorial (#117)\n\n* initial commit\n\n* address comments\n\n* remove todo\n\n* typo\n\n* Conversion between heterogenous and homogeneous graph objects (#115)\n\n* temp checkpoint (wip, will remove)\n\n* (wip) typed graph conversion\n\n* (wip) typed graph conversion\n\n* (wip) typed graph conversion\n\n* update\n\n* typo\n\n* delete examples\n\nCo-authored-by: rusty1s <matthias.fey@tu-dortmund.de>\n\n* fix test\n\n* update doc\n\n* deprecate NeighborSampler (#119)\n\n* Move `torch_geometric.data.DataLoader` to `torch_geometric.loader.DataLoader` (#120)\n\n* move dataloader\n\n* rename\n\n* typos\n\n* typos\n\n* fix __cat_dim__\n\n* updategp\n\n* Deprecate `train_test_split_edges` + Modifications to `RandomLinkSplit` (#121)\n\n* deprecate train_test_split_edges\n\n* to device transform\n\n* fix example\n\n* add split_labels argument\n\n* fix autoencoder example\n\n* typos\n\n* add docstring\n\n* ARGVA\n\n* seal\n\n* adress comments\n\n* Create example to load `*.csv` and transfer to `HeteroData` (#76)\n\n* create example to load csv file and transfer to heter-data\n\n* add ipython notebook version load csv with documentation\n\n* address comment\n\n* first version of csv loading doc\n\n* first version of csv loading doc\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestion docs/source/notes/loading_csv.rst\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* suggestions csv tutorial\n\n* example script load csv + extract fix\n\n* fixed edge index stacking dimension in example and jupyter nb\n\n* linting\n\n* linting2\n\n* rename\n\n* update\n\n* update\n\n* update\n\n* typo\n\n* typo\n\n* update\n\n* rename\n\n* update tutorial\n\n* typo\n\n* address comments\n\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n\n* typo\n\n* fix\n\n* typo\n\n* update\n\n* fix\n\n* fix\n\nCo-authored-by: benedekrozemberczki <benedek.rozemberczki@gmail.com>\nCo-authored-by: Rex Ying <rexying@stanford.edu>\nCo-authored-by: Dongkwan Kim <todoaskit@gmail.com>\nCo-authored-by: Markus <markus.zopf@outlook.com>\nCo-authored-by: Jimmie <jimmiebtlr@gmail.com>\nCo-authored-by: Jinu Sunil <jinu.sunil@gmail.com>\nCo-authored-by: Moritz R Sch\u00e4fer <moritz.schaefer@protonmail.com>\nCo-authored-by: Jiaxuan <youjiaxuan@gmail.com>\nCo-authored-by: PabloAMC <pmorenocf@alumnos.unex.es>\nCo-authored-by: Moritz Blum <31183934+moritzblum@users.noreply.github.com>\nCo-authored-by: fbragman <fbragman@users.noreply.github.com>\nCo-authored-by: Christopher Lee <2824685+CCInc@users.noreply.github.com>\nCo-authored-by: Tim Daubensch\u00fctz <tim@daubenschuetz.de>\nCo-authored-by: Yue Zhao <yzhao062@gmail.com>\nCo-authored-by: Dong Wang <dongw89@gmail.com>\nCo-authored-by: Dong Wang <dongwang@yannis-air.lan>\nCo-authored-by: Jan Eric Lenssen <janeric.lenssen@tu-dortmund.de>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Net(torch.nn.Module):",
            "x = global_max_pool(x, batch)",
            "x = self.lin(x).view(-1)",
            "",
            "-        attn_loss = F.kl_div(",
            "-            torch.log(score + 1e-14), data.attn[perm], reduction='none')",
            "+        attn_loss = F.kl_div(torch.log(score + 1e-14), data.attn[perm],",
            "+                             reduction='none')",
            "attn_loss = scatter_mean(attn_loss, batch)",
            "",
            "return x, attn_loss, ratio"
        ]
    },
    {
        "number": 4143,
        "comments": "",
        "commit_message": "Bump torch, torchvision, python versions (#2663)\n\n* Bump torch, torchvision, python versions\n\n* Fix .nonzero() usage\n\n* Add missing licenses\n\n* Fix more .nonzero() errors\n\n* Fix errors\n\n* Fix pyplot.hist invocation\n\n* Work around torch 1.6 incompatibility with pyplot.hist\n\n* Fix more pyplot.hist() errors\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ForecastingModel):",
            "",
            "# Sample global parameters.",
            "noise_scale = pyro.sample(\"noise_scale\",",
            "-                                  dist.LogNormal(torch.full((dim,), -3), 1).to_event(1))",
            "+                                  dist.LogNormal(torch.full((dim,), -3.), 1.).to_event(1))",
            "assert noise_scale.shape[-1:] == (dim,)",
            "trans_timescale = pyro.sample(\"trans_timescale\",",
            "dist.LogNormal(torch.zeros(dim), 1).to_event(1))"
        ]
    },
    {
        "number": 4147,
        "comments": "",
        "commit_message": "fix pts scale, save ply\n\nSummary:\nFix:\n* Scaling of point clouds for scalars\n* save_ply compatible cat\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D22298609\n\nfbshipit-source-id: abe94a5b64baf325587202d20adfc36912cc1478\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Pointclouds(object):",
            "self.",
            "\"\"\"",
            "if not torch.is_tensor(scale):",
            "-            scale = torch.full(len(self), scale)",
            "+            scale = torch.full((len(self),), scale, device=self.device)",
            "new_points_list = []",
            "points_list = self.points_list()",
            "for i, old_points in enumerate(points_list):"
        ]
    },
    {
        "number": 4148,
        "comments": "",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TranslationVariableLanguages:",
            "",
            "# At construction time:",
            "",
            "-        nlp.features.Translation(languages=['en', 'fr', 'de'])",
            "+        datasets.features.Translation(languages=['en', 'fr', 'de'])",
            "",
            "# During data generation:"
        ]
    },
    {
        "number": 4153,
        "comments": "",
        "commit_message": "py2.7 fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def stack(sequence, horizontal=True, vertical=True):",
            "# Concat all indices and values to one new large sparse matrix.",
            "indices = torch.cat(indices, dim=1)",
            "values = torch.cat([mat._values() for mat in sequence])",
            "-    size = torch.Size([y_sum, x_sum, *(sequence[0].size()[2:])])",
            "+    size = torch.Size([y_sum, x_sum, *list(sequence[0].size()[2:])])",
            "slices = torch.LongTensor(slices)",
            "",
            "return torch.sparse.FloatTensor(indices, values, size), slices"
        ]
    },
    {
        "number": 4156,
        "comments": "",
        "commit_message": "Fix float zeros format (#5491)\n\n* Fix float zeros format\n\n* 255 to integer\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Sum(nn.Module):",
            "self.weight = weight  # apply weights boolean",
            "self.iter = range(n - 1)  # iter object",
            "if weight:",
            "-            self.w = nn.Parameter(-torch.arange(1., n) / 2, requires_grad=True)  # layer weights",
            "+            self.w = nn.Parameter(-torch.arange(1.0, n) / 2, requires_grad=True)  # layer weights",
            "",
            "def forward(self, x):",
            "y = x[0]  # no weight"
        ]
    },
    {
        "number": 4159,
        "comments": "",
        "commit_message": "Fix serialization error due to EagerTensor constant\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def EfficientNet(",
            "# original implementation.",
            "# See https://github.com/tensorflow/tensorflow/issues/49930 for more",
            "# details",
            "-        x = layers.Rescaling(1.0 / tf.math.sqrt(IMAGENET_STDDEV_RGB))(x)",
            "+        x = layers.Rescaling(",
            "+            [1.0 / math.sqrt(stddev) for stddev in IMAGENET_STDDEV_RGB]",
            "+        )(x)",
            "",
            "x = layers.ZeroPadding2D(",
            "padding=imagenet_utils.correct_pad(x, 3), name=\"stem_conv_pad\""
        ]
    },
    {
        "number": 4160,
        "comments": "",
        "commit_message": "Fix padding_idx logical error in Adaptive Input (#1629)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\n\nI think if we keep pass **padding index of vocabulary** as `padding_idx` to adaptive embedding layers,\nthere will be no chance to train some words.\n\ne.g. If `cut_off` is (20000,60000) and vocab is larger than 60000,\nwe can't learn[**20,000+padding_idx**]th word and [**60,000+padding_idx**]th word.\nBecause those words' ids will be **padding_idx** by subtraction logic and eventually get zero tensors.\n\nSo, I changed `self.padding_idx` to `None` after assign vocab's `padding_idx`\n**for the first time at head embedding representation**.\nPull Request resolved: https://github.com/pytorch/fairseq/pull/1629\n\nDifferential Revision: D19557340\n\nPulled By: myleott\n\nfbshipit-source-id: e0c3b38862374d422a46dc62c248b2ecfbf08fd2\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AdaptiveInput(nn.Module):",
            "size = self.cutoff[i] - prev",
            "dim = int(initial_dim // (factor ** i))",
            "seq = nn.Sequential(",
            "-                nn.Embedding(size, dim, padding_idx),",
            "+                nn.Embedding(size, dim, self.padding_idx),",
            "nn.Linear(dim, output_dim, bias=False)",
            ")",
            "self.embeddings.append(seq)",
            "+            self.padding_idx = None",
            "+        self.padding_idx = padding_idx",
            "",
            "def init_weights(m):",
            "if isinstance(m, nn.Embedding):"
        ]
    },
    {
        "number": 4164,
        "comments": "",
        "commit_message": "[Hotfix] Fix Swin model outputs (#15414)\n\n* Fix Swin model outputs\n\n* Rename pooler\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SwinModelIntegrationTest(unittest.TestCase):",
            "expected_shape = torch.Size((1, 1000))",
            "self.assertEqual(outputs.logits.shape, expected_shape)",
            "",
            "-        expected_slice = torch.tensor([-0.2952, -0.4777, 0.2025]).to(torch_device)",
            "+        expected_slice = torch.tensor([-0.0948, -0.6454, -0.0921]).to(torch_device)",
            "",
            "self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))"
        ]
    },
    {
        "number": 4167,
        "comments": "",
        "commit_message": "fixed lstm, added gru, other fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(Module):",
            "#     raise TensorForceError(\"Invalid model directory/file.\")",
            "",
            "self.saver.restore(sess=self.session, save_path=file)",
            "-        self.session.run(fetches=self.list_buffer_index_reset_op)",
            "+        self.session.run(fetches=self.reset_buffer_indices)",
            "",
            "def get_components(self):",
            "\"\"\""
        ]
    },
    {
        "number": 4169,
        "comments": "",
        "commit_message": "fix norm issues in cvt\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CvT(nn.Module):",
            "",
            "layers.append(nn.Sequential(",
            "nn.Conv2d(dim, config['emb_dim'], kernel_size = config['emb_kernel'], padding = (config['emb_kernel'] // 2), stride = config['emb_stride']),",
            "+                LayerNorm(config['emb_dim']),",
            "Transformer(dim = config['emb_dim'], proj_kernel = config['proj_kernel'], kv_proj_stride = config['kv_proj_stride'], depth = config['depth'], heads = config['heads'], mlp_mult = config['mlp_mult'], dropout = dropout)",
            "))"
        ]
    },
    {
        "number": 4179,
        "comments": "",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario \u0160a\u0161ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class JigsawToxicityPred(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(data_dir):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('jigsaw_toxicity_pred', data_dir=...)`. Manual download instructions: {}\".format(",
            "-                    data_dir, self.manual_download_instructions",
            "-                )",
            "+                f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('jigsaw_toxicity_pred', data_dir=...)`. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "",
            "return ["
        ]
    },
    {
        "number": 4180,
        "comments": "",
        "commit_message": "Update to PyTorch 1.11.0 (#3045)\n\n* Run black\n\n* Fix ProvenanceTensor\n\n* Replace torch.triangular_solve -> torch.linalg.solve_triangular\n\n* More fixes to torch.linalg.solve_triangular\n\n* Bump torch version\n\n* Bump Python version 3.6 -> 3.7\n\n* Fix some tests\n\n* Decrease tolerance on stable tests\n\n* Remove obsolete xfail\n\n* Resolve #3032\n\n* Fix solve_triangular in ops.gaussian, revert test weakening\n\n* Fix catching of singular matrices in hmc\n\n* xfail funsor tests\n\n* Work around pandas 1.3 bug\n\n* Allow mypy to install missing stubs\n\n* Clarify triangular_solve test\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LowerCholeskyAffine(Transform):",
            "",
            "Inverts y => x.",
            "\"\"\"",
            "-        return torch.triangular_solve(",
            "-            (y - self.loc).unsqueeze(-1), self.scale_tril, upper=False, transpose=False",
            "-        )[0].squeeze(-1)",
            "+        return torch.linalg.solve_triangular(",
            "+            self.scale_tril, (y - self.loc).unsqueeze(-1), upper=False",
            "+        ).squeeze(-1)",
            "",
            "def log_abs_det_jacobian(self, x, y):",
            "\"\"\""
        ]
    },
    {
        "number": 4185,
        "comments": "",
        "commit_message": "more fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main():",
            "",
            "",
            "# Save the trained model and the tokenizer",
            "-    if args.do_train and args.local_rank == -1 or torch.distributed.get_rank() == 0:",
            "+    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):",
            "# Create output directory if needed",
            "if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:",
            "os.makedirs(args.output_dir)"
        ]
    },
    {
        "number": 4186,
        "comments": "",
        "commit_message": "[Fix] MotionBlur bug fix and doctest update (#782)\n\n* Fixed #779\n\n* Added tests for _extract_device_dtype\n\n* Fixed broken tests\n\n* Added tests against functional\n\n* Fixed doctests\n\n* bug fix\n",
        "label": "",
        "answer": "no",
        "change": [
            "def homography_i_H_ref(pinhole_i, pinhole_ref):",
            "- Output: :math:`(N, 4, 4)`",
            "",
            "Example:",
            "-        >>> pinhole_i = torch.rand(1, 12)    # Nx12",
            "-        >>> pinhole_ref = torch.rand(1, 12)  # Nx12",
            "-        >>> homography_i_H_ref(pinhole_i, pinhole_ref)  # Nx4x4",
            "+        pinhole_i = torch.rand(1, 12)    # Nx12",
            "+        pinhole_ref = torch.rand(1, 12)  # Nx12",
            "+        homography_i_H_ref(pinhole_i, pinhole_ref)  # Nx4x4",
            "\"\"\"",
            "+    # TODO: Add doctest once having `rtvec_to_pose`.",
            "assert len(",
            "pinhole_i.shape) == 2 and pinhole_i.shape[1] == 12, pinhole.shape",
            "assert pinhole_i.shape == pinhole_ref.shape, pinhole_ref.shape"
        ]
    },
    {
        "number": 4189,
        "comments": "",
        "commit_message": "Improve snt.Embed performance in distributed training.\n\nAdds a fix to avoid excess computation on parameter servers.\n\nPiperOrigin-RevId: 179912285\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Embed(base.AbstractModule):",
            "regularizer=self._regularizers.get(self.EMBEDDINGS, None),",
            "trainable=self._trainable)",
            "",
            "+    # On the backwards pass, we want to convert the gradient from",
            "+    # indexed-slices to a regular tensor before sending it back to the",
            "+    # parameter server. This avoids excess computation on the parameter server.",
            "+",
            "+    embeddings = util.convert_gradient_to_tensor(self._embeddings)",
            "+",
            "# Lookup embeddings",
            "-    return tf.nn.embedding_lookup(",
            "-        self._embeddings, ids, name=\"embedding_lookup\")",
            "+    return tf.nn.embedding_lookup(embeddings, ids, name=\"embedding_lookup\")",
            "",
            "@property",
            "def vocab_size(self):"
        ]
    },
    {
        "number": 4192,
        "comments": "",
        "commit_message": "wandb.init(): When the user passes an \"id\" attribute but no \"name\", use the id value for name, but make WANDB_NAME take precedence.\n\nFix a TensorFlow test that was depending on run history being a shared global.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_hook(history):",
            "tf.summary.scalar('c1', c1)",
            "summary_op = tf.summary.merge_all()",
            "",
            "-        hook = wandb_tensorflow.WandbHook(summary_op)",
            "+        hook = wandb_tensorflow.WandbHook(summary_op, history=history)",
            "with tf.train.MonitoredTrainingSession(hooks=[hook]) as sess:",
            "summary, acc = sess.run([summary_op, c1])",
            "",
            "assert wandb_tensorflow.tf_summary_to_dict(summary) == {'c1': 42.0}",
            "+    print(history.rows)",
            "+    # TODO(adrian): there is still some kind of bug here where the history",
            "+    # is being shared with another test that manages to add rows before this one.",
            "assert history.rows[0]['c1'] == 42.0"
        ]
    },
    {
        "number": 4196,
        "comments": "",
        "commit_message": "Fix momentum and epsilon values (#19454)\n\nThe momentum value for PyTorch and TensorFlow batch normalization layers is not equivalent. The TensorFlow value should be (1 - pytorch_momentum) in order to ensure the correct updates are applied to the running mean and running variance calculations. We wouldn't observe a difference loading a pretrained model and performing inference, but evaluation outputs would change after some training steps.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFResNetShortCut(tf.keras.layers.Layer):",
            "out_channels, kernel_size=1, strides=stride, use_bias=False, name=\"convolution\"",
            ")",
            "# Use same default momentum and epsilon as PyTorch equivalent",
            "-        self.normalization = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.1, name=\"normalization\")",
            "+        self.normalization = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.9, name=\"normalization\")",
            "",
            "def call(self, x: tf.Tensor, training: bool = False) -> tf.Tensor:",
            "hidden_state = x"
        ]
    },
    {
        "number": 4197,
        "comments": "",
        "commit_message": "Generalize TorchHook to FrameworkHook (#2561)\n\n* create FrameworkHook and move generic hooking functionality over\n\n* PEP-ify import statements all around\n\n* PR comment - fix docstring\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "if dependency_check.torch_available:",
            "import torch",
            "",
            "framework_tensors.append(torch.Tensor)",
            "+    framework_tensors.append(torch.nn.Parameter)",
            "framework_shapes.append(torch.Size)",
            "",
            "framework_tensors = tuple(framework_tensors)"
        ]
    },
    {
        "number": 4205,
        "comments": "",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "",
        "answer": "no",
        "change": [
            "class OpusState:",
            "load_layers_(model.model.decoder.layers, state_dict, BART_CONVERTER, is_decoder=True)",
            "",
            "# handle tensors not associated with layers",
            "-        wemb_tensor = torch.nn.Parameter(torch.FloatTensor(self.wemb))",
            "-        bias_tensor = torch.nn.Parameter(torch.FloatTensor(self.final_bias))",
            "+        wemb_tensor = nn.Parameter(torch.FloatTensor(self.wemb))",
            "+        bias_tensor = nn.Parameter(torch.FloatTensor(self.final_bias))",
            "model.model.shared.weight = wemb_tensor",
            "model.model.encoder.embed_tokens = model.model.decoder.embed_tokens = model.model.shared"
        ]
    },
    {
        "number": 4208,
        "comments": "",
        "commit_message": "fixed lint errors.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def trace(",
            "return ret",
            "",
            "",
            "-def det(",
            "-        x: torch.Tensor,",
            "-        out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:",
            "+def det(x: torch.Tensor, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "return torch.linalg.det(x, out=out)"
        ]
    },
    {
        "number": 4210,
        "comments": "",
        "commit_message": "fix a bug for save weights when multi-GPU training\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main(args):",
            "tb_writer.add_scalar(tags[1], acc, epoch)",
            "tb_writer.add_scalar(tags[2], optimizer.param_groups[0][\"lr\"], epoch)",
            "",
            "-            torch.save(model.state_dict(), \"./weights/model-{}.pth\".format(epoch))",
            "+            torch.save(model.module.state_dict(), \"./weights/model-{}.pth\".format(epoch))",
            "",
            "# \u5220\u9664\u4e34\u65f6\u7f13\u5b58\u6587\u4ef6",
            "if rank == 0:"
        ]
    },
    {
        "number": 4212,
        "comments": "",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class EpsilonGreedy(Exploration):",
            "),",
            "false_fn=lambda: exploit_action)",
            "",
            "-        assign_op = tf.assign(self.last_timestep, timestep)",
            "-        with tf.control_dependencies([assign_op]):",
            "+        assign_op = tf1.assign(self.last_timestep, timestep)",
            "+        with tf1.control_dependencies([assign_op]):",
            "return action, tf.zeros_like(action, dtype=tf.float32)",
            "",
            "def _get_torch_exploration_action(self, q_values, explore, timestep):"
        ]
    },
    {
        "number": 4214,
        "comments": "",
        "commit_message": "[MOD] fix typo, add tests for ldconv\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class LightweightConvolution2D(nn.Module):",
            "# convolution along frequency axis",
            "weight_f = F.softmax(self.weight_f, dim=-1)",
            "weight_f = F.dropout(weight_f, self.dropout_rate, training=self.training)",
            "-        weight_new = torch.zeros(B * T, 1, self.kernel_size, device=x.device).copy_(weight_f)",
            "+        weight_new = torch.zeros(B * T, 1, self.kernel_size, device=x.device, dtype=x.dtype).copy_(weight_f)",
            "xf = F.conv1d(x.view(1, B * T, C), weight_new, padding=self.padding_size, groups=B * T).view(B, T, C)",
            "",
            "# lightconv"
        ]
    },
    {
        "number": 4216,
        "comments": "",
        "commit_message": "Fixed automl APIs to work with remote filesystems (#2650)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ECD(BaseModel):",
            "\"\"\"Loads the model from the given path.\"\"\"",
            "weights_save_path = os.path.join(save_path, MODEL_WEIGHTS_FILE_NAME)",
            "device = torch.device(get_torch_device())",
            "-        self.load_state_dict(torch.load(weights_save_path, map_location=device))",
            "+        with open_file(weights_save_path, \"rb\") as f:",
            "+            self.load_state_dict(torch.load(f, map_location=device))",
            "",
            "def get_args(self):",
            "\"\"\"Returns init arguments for constructing this model.\"\"\""
        ]
    },
    {
        "number": 4221,
        "comments": "",
        "commit_message": "Fix torch meshgrid warnings (#20475)\n\n* fix torch meshgrid warnings\n\n* support lower torch versions\n\n* don't edit examples\n\n* dont edit examples\n\n* fix ci\n\n* fix style\n\n* rebase cleanup\n\n* fix ci again\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ViltEmbeddings(nn.Module):",
            "x = x.flatten(2).transpose(1, 2)",
            "# Set `device` here, otherwise `patch_index` will always be on `CPU` and will fail near the end for torch>=1.13",
            "patch_index = torch.stack(",
            "-            torch.meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1",
            "+            meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1",
            ").to(device=x_mask.device)",
            "patch_index = patch_index[None, None, :, :, :]",
            "patch_index = patch_index.expand(x_mask.shape[0], x_mask.shape[1], -1, -1, -1)"
        ]
    },
    {
        "number": 4223,
        "comments": "",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "",
        "answer": "no",
        "change": [
            "class VisualBertForQuestionAnswering(VisualBertPreTrainedModel):",
            "",
            "loss = None",
            "if labels is not None:",
            "-            loss_fct = torch.nn.KLDivLoss(reduction=\"batchmean\")",
            "-            log_softmax = torch.nn.LogSoftmax(dim=-1)",
            "+            loss_fct = nn.KLDivLoss(reduction=\"batchmean\")",
            "+            log_softmax = nn.LogSoftmax(dim=-1)",
            "reshaped_logits = log_softmax(reshaped_logits)",
            "loss = loss_fct(reshaped_logits, labels.contiguous())",
            "if not return_dict:"
        ]
    },
    {
        "number": 4225,
        "comments": "",
        "commit_message": "Fix tf-nightly-gpu break (#1124)\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "from common import mpi_env_rank_and_size",
            "config = tf.ConfigProto()",
            "config.gpu_options.allow_growth = True",
            "",
            "+if _has_eager:",
            "+    # Specifies the config to use with eager execution. Does not preclude",
            "+    # tests from running in the graph mode.",
            "+    tf.enable_eager_execution(config=config)",
            "+",
            "# MLSL supports only byte, float and double data types",
            "mlsl_supported_types = set([tf.float32, tf.float64])"
        ]
    },
    {
        "number": 4227,
        "comments": "",
        "commit_message": "small fix to test_torch_tan, with num_positional_args now determined correctly.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "import ivy.functional.backends.torch as ivy_torch",
            "set(ivy_torch.valid_float_dtypes)))),",
            "as_variable=st.booleans(),",
            "with_out=st.booleans(),",
            "-    num_positional_args=helpers.num_positional_args(fn_name=\"tan\"),",
            "+    num_positional_args=helpers.num_positional_args(",
            "+        fn_name=\"functional.frontends.torch.tan\"),",
            "native_array=st.booleans(),",
            ")",
            "def test_torch_tan("
        ]
    },
    {
        "number": 4229,
        "comments": "",
        "commit_message": "[Examples] TPU-based training of a language model using TensorFlow (#21657)\n\n* add: tokenizer training script for TF TPU LM training.\n\n* add: script for preparing the TFRecord shards.\n\n* add: sequence of execution to readme.\n\n* remove limit from the tfrecord shard name.\n\n* Add initial train_model.py\n\n* Add basic training arguments and model init\n\n* Get up to the point of writing the data collator\n\n* Pushing progress so far!\n\n* Complete first draft of model training code\n\n* feat: grouping of texts efficiently.\n\nCo-authored-by: Matt <rocketknight1@gmail.com>\n\n* Add proper masking collator and get training loop working\n\n* fix: things.\n\n* Read sample counts from filenames\n\n* Read sample counts from filenames\n\n* Draft README\n\n* Improve TPU warning\n\n* Use distribute instead of distribute.experimental\n\n* Apply suggestions from code review\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\n\n* Modularize loading and add MLM probability as arg\n\n* minor refactoring to better use the cli args.\n\n* readme fillup.\n\n* include tpu and inference sections in the readme.\n\n* table of contents.\n\n* parallelize maps.\n\n* polish readme.\n\n* change script name to run_mlm.py\n\n* address PR feedback (round I).\n\n---------\n\nCo-authored-by: Matt <rocketknight1@gmail.com>\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DataCollatorForLanguageModeling(DataCollatorMixin):",
            "inputs = tf.where(indices_replaced, mask_token_id, inputs)",
            "",
            "# 10% of the time, we replace masked input tokens with random word",
            "-        indices_random = self.tf_bernoulli(input_shape, 0.5) & masked_indices & ~indices_replaced",
            "-        random_words = tf.random.uniform(input_shape, maxval=vocab_size, dtype=tf.int64)",
            "+        indices_random = self.tf_bernoulli(input_shape, 0.1) & masked_indices & ~indices_replaced",
            "+        random_words = tf.random.uniform(input_shape, maxval=vocab_size, dtype=inputs.dtype)",
            "+",
            "inputs = tf.where(indices_random, random_words, inputs)",
            "",
            "# The rest of the time (10% of the time) we keep the masked input tokens unchanged"
        ]
    },
    {
        "number": 4235,
        "comments": "",
        "commit_message": "move summary_op from trainer to callbacks. fix #125\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Trainer(object):",
            "logger.info(\"Epoch {} (global_step {}) finished, time:{:.2f} sec.\".format(",
            "self.epoch_num, self.global_step, time.time() - start_time))",
            "",
            "-                    # trigger epoch outside the timing region.",
            "-                    self.trigger_epoch()",
            "+                    self.trigger_epoch()  # trigger epoch outside the timing region.",
            "except StopTraining:",
            "logger.info(\"Training was stopped.\")",
            "except KeyboardInterrupt:"
        ]
    },
    {
        "number": 4236,
        "comments": "",
        "commit_message": "Black preview (#17217)\n\n* Black preview\n\n* Fixup too!\n\n* Fix check copies\n\n* Use the same version as the CI\n\n* Bump black\n",
        "label": "",
        "answer": "no",
        "change": [
            "def load_tf2_weights_in_bert(model, tf_checkpoint_path, config):",
            "pointer.data = torch.from_numpy(array)",
            "else:",
            "raise ValueError(",
            "-                f\"Shape mismatch in layer {full_name}: Model expects shape {pointer.shape} but layer contains shape: {array.shape}\"",
            "+                f\"Shape mismatch in layer {full_name}: Model expects shape {pointer.shape} but layer contains shape:\"",
            "+                f\" {array.shape}\"",
            ")",
            "logger.info(f\"Successfully set variable {full_name} to PyTorch layer {trace}\")",
            "return model"
        ]
    },
    {
        "number": 4238,
        "comments": "",
        "commit_message": "Fix doc formatting and improve doc styling (#4072)\n\n* switch to pydoc-markdown with custom processor\n\n* add some extra css\n\n* fixes\n\n* fixes\n\n* minor tweaks\n\n* fixes\n\n* add breadcrumbs\n\n* fixes\n\n* fix arg formatting\n\n* fix\n\n* fixes\n\n* more fixes\n\n* fix\n\n* fix cross-refs within module\n\n* fix dev requirements\n\n* pin pydoc-markdown to latest commit\n\n* small refactor and docstring fixes\n\n* more small fixes\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PretrainedTransformerMismatchedEmbedder(TokenEmbedder):",
            "segment_concat_mask: Optional[torch.BoolTensor]",
            "See `PretrainedTransformerEmbedder`.",
            "",
            "-        # Returns:",
            "+        # Returns",
            "",
            "-        Shape: [batch_size, num_orig_tokens, embedding_size].",
            "+        `torch.Tensor`",
            "+            Shape: [batch_size, num_orig_tokens, embedding_size].",
            "\"\"\"",
            "# Shape: [batch_size, num_wordpieces, embedding_size].",
            "embeddings = self._matched_embedder("
        ]
    },
    {
        "number": 4241,
        "comments": "",
        "commit_message": "fixed a typo\n",
        "label": "",
        "answer": "no",
        "change": [
            "def knn(x, y, k, batch_x=None, batch_y=None, cosine=False, num_workers=1):",
            "x = torch.Tensor([[-1, -1], [-1, 1], [1, -1], [1, 1]])",
            "batch_x = torch.tensor([0, 0, 0, 0])",
            "y = torch.Tensor([[-1, 0], [1, 0]])",
            "-        batch_x = torch.tensor([0, 0])",
            "+        batch_y = torch.tensor([0, 0])",
            "assign_index = knn(x, y, 2, batch_x, batch_y)",
            "\"\"\"",
            "if torch_cluster is None:"
        ]
    },
    {
        "number": 4243,
        "comments": "",
        "commit_message": "lint fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def vecdot(",
            "*,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    dtype = torch.promote_types(x1.dtype, x2.dtype)",
            "x1, x2 = x1.to(torch.float32), x2.to(torch.float32)",
            "return torch.tensordot(x1, x2, dims=([axis], [axis]), out=out)"
        ]
    },
    {
        "number": 4245,
        "comments": "",
        "commit_message": "Feature/log computational graph (#3003)\n\n* add methods\n\n* log in trainer\n\n* add tests\n\n* changelog\n\n* fix tests\n\n* fix tests\n\n* fix tests\n\n* fix tests\n\n* fix tests\n\n* fix tests\n\n* fix tests\n\n* text\n\n* added argument\n\n* update tests\n\n* fix styling\n\n* improve testing\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_tbptt_cpu_model_result_auto_reduce(tmpdir):",
            ")",
            "",
            "model = BpttTestModel(**hparams)",
            "+    model.example_input_array = torch.randn(5, truncated_bptt_steps)",
            "",
            "# fit model",
            "trainer = Trainer("
        ]
    },
    {
        "number": 4247,
        "comments": "",
        "commit_message": "fix tf random multinomial test (#7098)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_multinomial(",
            "):",
            "prob_dtype, batch_size, population_size, num_samples, replace, probs = everything",
            "# tensorflow does not support multinomial without replacement",
            "-    if backend_fw == \"tensorflow\":",
            "-        assume(replace is True)",
            "+    if backend_fw == ivy.functional.backends.tensorflow:",
            "+        assume(replace)",
            "",
            "def call():",
            "return helpers.test_function("
        ]
    },
    {
        "number": 4248,
        "comments": "",
        "commit_message": "Align tqdm control/cache control with Transformers (#3897)\n\n* Align tqdm with transformers\n\n* Add tqdm utilites to docs\n\n* Introduce enable_caching / disable_caching\n\n* Fix test naming\n\n* Update src/datasets/fingerprint.py\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n\n* Style and rename _active to _tqdm_active\n\nCo-authored-by: Quentin Lhoest <42851186+lhoestq@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def set_test_cache_config(tmp_path_factory, monkeypatch):",
            "",
            "@pytest.fixture(autouse=True, scope=\"session\")",
            "def disable_tqdm_output():",
            "-    datasets.set_progress_bar_enabled(False)",
            "+    datasets.disable_progress_bar()",
            "",
            "",
            "@pytest.fixture(autouse=True)"
        ]
    },
    {
        "number": 4249,
        "comments": "",
        "commit_message": "Fix some of the tests so that they pass for Bazel.\n\nPiperOrigin-RevId: 268469295\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MonotoneConvexTest(tf.test.TestCase):",
            "test_time = tf.constant([1.1, 2.7], dtype=dtype)",
            "interpolated, _ = monotone_convex.interpolate(test_time, interval_values,",
            "interval_times)",
            "-    gradient_1y = self.evaluate(tf.gradients(interpolated[0], knot_1y)[0])",
            "-    gradient_zero = self.evaluate(tf.gradients(interpolated[1], knot_1y)[0])",
            "+    gradient_1y = self.evaluate(tf.convert_to_tensor(",
            "+        tf.gradients(interpolated[0], knot_1y)[0]))",
            "+    gradient_zero = self.evaluate(tf.convert_to_tensor(",
            "+        tf.gradients(interpolated[1], knot_1y)[0]))",
            "+",
            "self.assertAlmostEqual(gradient_1y[0], 0.42)",
            "self.assertAlmostEqual(gradient_zero[0], 0.0)"
        ]
    },
    {
        "number": 4250,
        "comments": "",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "# weight decay on all W of fc layers",
            "wd_w = tf.train.exponential_decay(0.00004, get_global_step_var(),",
            "80000, 0.7, True)",
            "-        wd_cost = tf.mul(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss')",
            "+        wd_cost = tf.multiply(wd_w, regularize_cost('.*/W', tf.nn.l2_loss), name='l2_regularize_loss')",
            "",
            "self.cost = tf.add_n([0.4 * loss1, loss2, wd_cost], name='cost')",
            "add_moving_summary(loss1, loss2, wd_cost, self.cost)"
        ]
    },
    {
        "number": 4256,
        "comments": "",
        "commit_message": "Fix multi-class prediction\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def mask_to_image(mask: np.ndarray, mask_values):",
            "if isinstance(mask_values[0], list):",
            "out = np.zeros((mask.shape[-2], mask.shape[-1], len(mask_values[0])), dtype=np.uint8)",
            "elif mask_values == [0, 1]:",
            "-        out = np.zeros((mask.shape[-2], mask.shape[-1]), dtype=np.bool)",
            "+        out = np.zeros((mask.shape[-2], mask.shape[-1]), dtype=bool)",
            "else:",
            "out = np.zeros((mask.shape[-2], mask.shape[-1]), dtype=np.uint8)"
        ]
    },
    {
        "number": 4258,
        "comments": "",
        "commit_message": "fix review comments\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def train(args):",
            "rnn = RNNLM(args.n_vocab, args.layer, args.unit, args.type, args.dropout_rate)",
            "model = ClassifierWithState(rnn)",
            "if args.ngpu > 0:",
            "-        model = torch.nn.DataParallel(model).cuda()",
            "+        model = torch.nn.DataParallel(model, device_ids=list(range(args.ngpu))).cuda()",
            "setattr(model, \"reporter\", model.module.reporter)",
            "gpu_id = 0",
            "else:"
        ]
    },
    {
        "number": 4267,
        "comments": "",
        "commit_message": "[PyTorch][Bug Fix] RNN Scratch train_ch8 step once in a batch (#1347)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train_ch8(model, train_iter, vocab, lr, num_epochs, device,",
            "animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',",
            "legend=['train'], xlim=[1, num_epochs])",
            "if isinstance(model, nn.Module):",
            "-        trainer = torch.optim.SGD(model.parameters(), lr)",
            "-        updater = lambda batch_size: trainer.step()",
            "+        updater = torch.optim.SGD(model.parameters(), lr)",
            "else:",
            "updater = lambda batch_size: d2l.sgd(model.params, lr, batch_size)",
            "predict = lambda prefix: predict_ch8(prefix, 50, model, vocab, device)"
        ]
    },
    {
        "number": 4269,
        "comments": "",
        "commit_message": "fix \"some elements of the input tensor and the written-to tensor refer to a single memory location\" error in ddp_pipeline tutorial (#2002)\n\nCo-authored-by: Svetlana Karslioglu <svekars@fb.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class PositionalEncoding(nn.Module):",
            "pe[:, 0::2] = torch.sin(position * div_term)",
            "pe[:, 1::2] = torch.cos(position * div_term)",
            "pe = pe.unsqueeze(0).transpose(0, 1)",
            "-        self.register_buffer('pe', pe)",
            "+        self.pe = nn.Parameter(pe, requires_grad=False)",
            "",
            "def forward(self, x):",
            "x = x + self.pe[:x.size(0), :]"
        ]
    },
    {
        "number": 4273,
        "comments": "",
        "commit_message": "Support more batch distributions in HaarReparam (#2731)\n\n* Support more batch distributions in HaarReparam\n\n* Reorder comment\n\n* Avoid draconian warning\n\n* Fix strictness check\n\n* Work around torch.view_as_complex() error\n\n* Add support for dist.Uniform\n",
        "label": "",
        "answer": "no",
        "change": [
            "def idct(x, dim=-1):",
            "X = torch.stack([x[..., :M], xi], dim=-1)",
            "coef_real = torch.cos(torch.linspace(0, 0.5 * math.pi, N + 1, dtype=x.dtype, device=x.device))",
            "coef = torch.stack([coef_real[:M], coef_real[-M:].flip(-1)], dim=-1)",
            "-    Y = torch.view_as_complex(coef) * torch.view_as_complex(X)",
            "+    Y = as_complex(coef) * as_complex(X)",
            "# Step 2",
            "y = irfft(Y, n=N)",
            "# Step 3"
        ]
    },
    {
        "number": 4274,
        "comments": "",
        "commit_message": "Fix some pydocstyle lints (#784)\n\n* RTD with Mock Imports\n\n* cleanup docs (fix D208)\n\n* fix D300\n\n* fix D402\n\n* fix D412\n\n* fix D204\n\n* fix D403\n\n* fix D207\n\n* fix D301\n\n* fix D200\n\n* fix format\n\n* disable D302\n\n* fix D210\n\n* fix D202\n\n* fix doc test and make it happen earlier\n\n* Issue with Deprecated Decorator Fixed\n\n* Changelog updated + YAPF cleaning\n\n* Update .readthedocs.yml\n\n* disable D202, since it has conflict with yapf\n\n* TF 1.6.0 Errors fixed\n\n* YAPF Cleaning\n\n* Doc Mock Import for ROI Pooling\n\n* TL init if protected\n\n* Error fix - Test Documentation\n\n* fix documentation test\n\n* update changelog\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def retrieve_seq_length_op3(data, pad_val=0):  # HangSheng: return tensor for se",
            "",
            "",
            "def target_mask_op(data, pad_val=0):  # HangSheng: return tensor for mask,if input is tf.string",
            "-    \"\"\"Return tensor for mask, if input is ``tf.string``.",
            "-",
            "-    \"\"\"",
            "+    \"\"\"Return tensor for mask, if input is ``tf.string``.\"\"\"",
            "data_shape_size = data.get_shape().ndims",
            "if data_shape_size == 3:",
            "return tf.cast(tf.reduce_any(tf.not_equal(data, pad_val), axis=2), dtype=tf.int32)"
        ]
    },
    {
        "number": 4276,
        "comments": "",
        "commit_message": "fix float16 for torch.ctc by casting float32\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CTC(torch.nn.Module):",
            "# expected shape of seqLength x batchSize x alphabet_size",
            "dtype = ys_hat.dtype",
            "ys_hat = ys_hat.transpose(0, 1)",
            "-        if self.ctc_type == \"warpctc\":",
            "+        if self.ctc_type == \"warpctc\" or dtype == torch.float16:",
            "# warpctc only supports float32",
            "ys_hat = ys_hat.to(dtype=torch.float32)",
            "else:"
        ]
    },
    {
        "number": 4282,
        "comments": "",
        "commit_message": "Added coverage to WikiTables ERM parser (#1181)\n\n* wikitables coverage parser\n\n* minor comment updates and bug fixes\n\n* updated table question kg test and coverage computation\n\n* removed unused imports\n\n* copy decoder step's output projection weights properly\n\n* fixed new variable creation\n\n* addressed pr comments\n\n* mypy issues\n\n* addressed remaining PR comments\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class ExpectedRiskMinimization(DecoderTrainer[Callable[[StateType], torch.Tensor",
            "state.score,",
            "state.action_history):",
            "if self._normalize_by_length:",
            "-                    path_length = nn_util.new_variable_with_data(model_score,",
            "-                                                                 torch.Tensor([len(history)]))",
            "+                    path_length = Variable(model_score.data.new([len(history)]))",
            "model_score = model_score / path_length",
            "batch_scores[batch_index].append(model_score)",
            "return batch_scores"
        ]
    },
    {
        "number": 4284,
        "comments": "",
        "commit_message": "ci: Redo `format.sh --all` script & backfill lint fixes (#9956)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def build_eager_tf_policy(name,",
            "dummy_batch[\"seq_lens\"] = np.array([1], dtype=np.int32)",
            "",
            "# Convert everything to tensors.",
            "-            dummy_batch = tf.nest.map_structure(",
            "-                tf1.convert_to_tensor, dummy_batch)",
            "+            dummy_batch = tf.nest.map_structure(tf1.convert_to_tensor,",
            "+                                                dummy_batch)",
            "",
            "# for IMPALA which expects a certain sample batch size.",
            "def tile_to(tensor, n):"
        ]
    },
    {
        "number": 4286,
        "comments": "",
        "commit_message": "Fix send/get in chain Tensors and partial fix of torch func\n- All functionalities claimed have unit tests\n- torch func with args=FloatTensor and output=FloatTensor work remotely\n- So, output=IntTensors or output=float fails so far\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class _TorchObject(object):",
            "def create_pointer(self, register=False):",
            "return self.child.create_pointer(parent=self, register=register)",
            "",
            "-    def footprint(self):",
            "-        return self.child.footprint()",
            "-",
            "def get(self):",
            "new_child_obj = self.child.get(parent=self)",
            "return self"
        ]
    },
    {
        "number": 4290,
        "comments": "",
        "commit_message": "Fix unit tests\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_load_no_dev_data_explicit(tasks_base_path):",
            "",
            "def test_multi_corpus(tasks_base_path):",
            "",
            "-    corpus_1 = flair.datasets.NER_GERMAN_GERMEVAL(tasks_base_path)",
            "+    corpus_1 = flair.datasets.ColumnCorpus(tasks_base_path  / \"germeval_14\", column_format={0: \"text\", 2: \"ner\"})",
            "",
            "corpus_2 = flair.datasets.ColumnCorpus(tasks_base_path / \"fashion\", column_format={0: \"text\", 2: \"ner\"})",
            "# get two corpora as one"
        ]
    },
    {
        "number": 4291,
        "comments": "",
        "commit_message": "fix lint and docstring failures\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def full(",
            "fill_value: Union[int, float],",
            "*,",
            "dtype: tf.DType = None,",
            "-    device: str",
            "+    device: str,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "with tf.device(as_native_dev(default_device(device))):",
            "return tf.fill("
        ]
    },
    {
        "number": 4292,
        "comments": "",
        "commit_message": "Rename compute_loss in TF models (#15207)\n\n* Rename compute_loss to hf_compute_loss to avoid conflicts with the new Keras method\n\n* make style\n\n* Adding deprecation warning to `compute_loss`\n\n* Fix sneaky reference to compute_loss\n\n* Replace logger.warning with warnings.warn\n\n* Clarifying warning and deprecation timeline\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFGPT2ForSequenceClassification(TFGPT2PreTrainedModel, TFSequenceClassific",
            "if not tf.is_tensor(sequence_lengths):",
            "in_logits = logits[0 : logits_shape[0], sequence_lengths]",
            "",
            "-            loss = self.compute_loss(tf.reshape(inputs[\"labels\"], [-1]), tf.reshape(in_logits, [-1, self.num_labels]))",
            "+            loss = self.hf_compute_loss(",
            "+                tf.reshape(inputs[\"labels\"], [-1]), tf.reshape(in_logits, [-1, self.num_labels])",
            "+            )",
            "pooled_logits = in_logits if in_logits is not None else logits",
            "",
            "if not inputs[\"return_dict\"]:"
        ]
    },
    {
        "number": 4296,
        "comments": "",
        "commit_message": "fixes #484 (#495)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SummaryWriter(object):",
            "make_tsv(metadata, save_path, metadata_header=metadata_header)",
            "if label_img is not None:",
            "assert mat.shape[0] == label_img.shape[0], '#images should equal with #data points'",
            "+            assert label_img.shape[2] == label_img.shape[3], 'Image should be square, see tensorflow/tensorboard#670'",
            "make_sprite(label_img, save_path)",
            "assert mat.ndim == 2, 'mat should be 2D, where mat.size(0) is the number of data points'",
            "make_mat(mat, save_path)"
        ]
    },
    {
        "number": 4297,
        "comments": "",
        "commit_message": "update and fix grad bug\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "l = tf.nn.dropout(l, keep_prob)",
            "",
            "# fc will have activation summary by default. disable this for the output layer",
            "-        logits = FullyConnected('fc1', l, out_dim=10,",
            "-                             summary_activation=False, nl=tf.identity)",
            "+        logits = FullyConnected('fc1', l, out_dim=10, nl=tf.identity)",
            "prob = tf.nn.softmax(logits, name='prob')",
            "",
            "y = one_hot(label, 10)"
        ]
    },
    {
        "number": 4300,
        "comments": "",
        "commit_message": "stricter typing for Optional[T] types, improve handling of Lazy params (#4743)\n\n* stricter typing for Optional[T] types\n\n* fix linting error\n\n* fix checkpointer test\n\n* fix add_field method\n\n* fix '_extract_token_and_type_ids' method\n\n* fix typing on Lazy\n\n* improve Lazy API\n\n* add notes about Lazy to CHANGELOG\n\n* fix CHANGELOG\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ElmoLstm(_EncoderBase):",
            "# the final states for all the layers.",
            "final_states.append(",
            "(",
            "-                    torch.cat([forward_state[0], backward_state[0]], -1),",
            "-                    torch.cat([forward_state[1], backward_state[1]], -1),",
            "+                    torch.cat([forward_state[0], backward_state[0]], -1),  # type: ignore",
            "+                    torch.cat([forward_state[1], backward_state[1]], -1),  # type: ignore",
            ")",
            ")"
        ]
    },
    {
        "number": 4301,
        "comments": "",
        "commit_message": "fixed test\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_forward(use_token_averaged_energy, reduction_factor):",
            "es, elens = layer(xs, torch.LongTensor([384, 128]))",
            "assert es.shape[1] == max(elens)",
            "else:",
            "-        ds = torch.LongTensor([[3, 3, 1], [3, 0, 0]])",
            "+        ds = torch.LongTensor([[3, 3, 1], [3, 0, 0]]) // reduction_factor",
            "dlens = torch.LongTensor([3, 1])",
            "es, _ = layer(",
            "xs, torch.LongTensor([384, 128]), durations=ds, durations_lengths=dlens"
        ]
    },
    {
        "number": 4302,
        "comments": "",
        "commit_message": "Fixed PyTorch MNIST dataset (#2707)\n\nSigned-off-by: Travis Addair <tgaddair@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train_fn():",
            "transforms.Normalize((0.1307, ), (0.3081, ))])",
            "",
            "test_dataset = datasets.MNIST(",
            "-        'data-%d' % hvd.rank(), train=False, transform=transformations)",
            "+        data_dir, train=False, transform=transformations)",
            "# Horovod: use DistributedSampler to partition the test data.",
            "test_sampler = torch.utils.data.distributed.DistributedSampler(",
            "test_dataset, num_replicas=hvd.size(), rank=hvd.rank())"
        ]
    },
    {
        "number": 4304,
        "comments": "",
        "commit_message": "[Refactor] Remove set_seed (#289)\n\n* [Refactor] Remove set_seed and class attributes\n\n* apply anton's suggestiosn\n\n* fix\n\n* Apply suggestions from code review\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* up\n\n* update\n\n* make style\n\n* Apply suggestions from code review\n\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\n\n* make fix-copies\n\n* make style\n\n* make style and new copies\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\nCo-authored-by: Anton Lozhkov <anton@huggingface.co>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ScoreSdeVePipeline(DiffusionPipeline):",
            "# correction step",
            "for _ in range(self.scheduler.correct_steps):",
            "model_output = self.unet(sample, sigma_t)[\"sample\"]",
            "-                sample = self.scheduler.step_correct(model_output, sample)[\"prev_sample\"]",
            "+                sample = self.scheduler.step_correct(model_output, sample, generator=generator)[\"prev_sample\"]",
            "",
            "# prediction step",
            "model_output = model(sample, sigma_t)[\"sample\"]",
            "-            output = self.scheduler.step_pred(model_output, t, sample)",
            "+            output = self.scheduler.step_pred(model_output, t, sample, generator=generator)",
            "",
            "sample, sample_mean = output[\"prev_sample\"], output[\"prev_sample_mean\"]"
        ]
    },
    {
        "number": 4307,
        "comments": "",
        "commit_message": "fix linter and mypy error for laplacian\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Laplacian(nn.Module):",
            "computed = (kernel_size - 1) // 2",
            "return computed",
            "",
            "-    def forward(self, x: torch.Tensor):",
            "+    def forward(self, x: torch.Tensor):  # type: ignore",
            "if not torch.is_tensor(x):",
            "raise TypeError(\"Input x type is not a torch.Tensor. Got {}\"",
            ".format(type(x)))"
        ]
    },
    {
        "number": 4308,
        "comments": "",
        "commit_message": "Adds quick fix for pretrained models not loading by modifying state_dict keys on load. (#2911)\n\n* Adds quick fix for pretrained models not loading.  Fix modifies state_dict keys before loading.\n\n* Shortens keys in test equally.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ECD(BaseModel):",
            "weights_save_path = os.path.join(save_path, MODEL_WEIGHTS_FILE_NAME)",
            "device = torch.device(get_torch_device())",
            "with open_file(weights_save_path, \"rb\") as f:",
            "-            self.load_state_dict(torch.load(f, map_location=device))",
            "+            state_dict = torch.load(f, map_location=device)",
            "+            self.load_state_dict(update_state_dict(state_dict))",
            "",
            "def get_args(self):",
            "\"\"\"Returns init arguments for constructing this model.\"\"\""
        ]
    },
    {
        "number": 4310,
        "comments": "",
        "commit_message": "DDP `torch.jit.trace()` `--sync-bn` fix (#4615)\n\n* Remove assert\n\n* debug0\n\n* trace=not opt.sync\n\n* sync to sync_bn fix\n\n* Cleanup\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Loggers():",
            "if self.wandb:",
            "self.wandb.log({\"Labels\": [wandb.Image(str(x), caption=x.name) for x in paths]})",
            "",
            "-    def on_train_batch_end(self, ni, model, imgs, targets, paths, plots):",
            "+    def on_train_batch_end(self, ni, model, imgs, targets, paths, plots, sync_bn):",
            "# Callback runs on train batch end",
            "if plots:",
            "if ni == 0:",
            "-                with warnings.catch_warnings():",
            "-                    warnings.simplefilter('ignore')  # suppress jit trace warning",
            "-                    self.tb.add_graph(torch.jit.trace(de_parallel(model), imgs[0:1], strict=False), [])",
            "+                if not sync_bn:  # tb.add_graph() --sync known issue https://github.com/ultralytics/yolov5/issues/3754",
            "+                    with warnings.catch_warnings():",
            "+                        warnings.simplefilter('ignore')  # suppress jit trace warning",
            "+                        self.tb.add_graph(torch.jit.trace(de_parallel(model), imgs[0:1], strict=False), [])",
            "if ni < 3:",
            "f = self.save_dir / f'train_batch{ni}.jpg'  # filename",
            "Thread(target=plot_images, args=(imgs, targets, paths, f), daemon=True).start()"
        ]
    },
    {
        "number": 4311,
        "comments": "",
        "commit_message": "fix model: relu/normalize after every conv1d\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Model:",
            "enc = tf.layers.max_pooling1d(enc, 2, 1, padding=\"same\")  # (N, T, K * E / 2)",
            "",
            "### Conv1D projections",
            "-            enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_1\")  # (N, T, E/2)",
            "-            enc = normalize(enc, type=hp.norm_type, is_training=self.is_training, activation_fn=tf.nn.relu)",
            "-            enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_2\")  # (N, T, E/2)",
            "+            enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_1\", activation_fn=tf.nn.relu)  # (N, T, E/2)",
            "+            enc = normalize(enc, type=hp.norm_type, is_training=self.is_training)",
            "+            enc = conv1d(enc, hp.hidden_units // 2, 3, scope=\"conv1d_2\", activation_fn=None)  # (N, T, E/2)",
            "enc += prenet_out  # (N, T, E/2) # residual connections",
            "",
            "### Highway Nets"
        ]
    },
    {
        "number": 4312,
        "comments": "",
        "commit_message": "bugfix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def eval_on_ILSVRC12(model, model_file, dataflow):",
            "",
            "def image_preprocess(image, bgr=True):",
            "if image.dtype.base_dtype != tf.float32:",
            "-        image = tf.case(image, tf.float32)",
            "+        image = tf.cast(image, tf.float32)",
            "image = image * (1.0 / 255)",
            "",
            "mean = [0.485, 0.456, 0.406]    # rgb"
        ]
    },
    {
        "number": 4313,
        "comments": "",
        "commit_message": "Add test-cuda make target, and fix cuda bugs (#338)\n\n* Add test-cuda make target; fix cuda bugs\n\n* Fix more cuda bugs; distributions now passes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Multinomial(Distribution):",
            "n = int(n.data.cpu()[0][0])",
            "else:",
            "n = int(n.data.cpu()[0])",
            "-        return Variable(torch.multinomial(ps.data, n, replacement=True))",
            "+        return Variable(torch_multinomial(ps.data, n, replacement=True))",
            "",
            "def batch_log_pdf(self, x, ps=None, n=None):",
            "ps, n = self._sanitize_input(ps, n)"
        ]
    },
    {
        "number": 4314,
        "comments": "",
        "commit_message": "bug fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def build_targets(p, targets, model):",
            "tcls, tbox, indices, av = [], [], [], []",
            "reject, use_all_anchors = True, True",
            "gain = torch.ones(6, device=targets.device)  # normalized to gridspace gain",
            "+",
            "+    # m = list(model.modules())[-1]",
            "+    # for i in range(m.nl):",
            "+    #     anchor_vec = m.anchor_vec[i]",
            "multi_gpu = type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel)",
            "for i, j in enumerate(model.yolo_layers):",
            "# get number of grid points and anchor vec for this yolo layer",
            "anchor_vec = model.module.module_list[j].anchor_vec if multi_gpu else model.module_list[j].anchor_vec",
            "",
            "# iou of targets-anchors",
            "-        gain[2:] = torch.tensor(p[i].shape)[[2, 3, 2, 3]]  # xyxy gain",
            "+        gain[2:] = torch.tensor(p[i].shape)[[3, 2, 3, 2]]  # xyxy gain",
            "t, a = targets * gain, []",
            "gwh = t[:, 4:6]",
            "if nt:"
        ]
    },
    {
        "number": 4316,
        "comments": "",
        "commit_message": "Fix default dtype in HJM, HullWhite, and Heston model, as well as in PiecewiseConstant class.\n\nPiperOrigin-RevId: 387639335\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GaussianHJM(quasi_gaussian_hjm.QuasiGaussianHJM):",
            "",
            "self._exact_discretization_setup(dim)",
            "super(quasi_gaussian_hjm.QuasiGaussianHJM,",
            "-            self).__init__(dim, _drift_fn, _vol_fn, dtype, self._name)",
            "+            self).__init__(dim, _drift_fn, _vol_fn, self._dtype, self._name)",
            "",
            "def sample_paths(self,",
            "times,"
        ]
    },
    {
        "number": 4317,
        "comments": "",
        "commit_message": "Drop support for PyTorch 1.10 (#16492)\n\n* Drop support for PyTorch 1.10\n\n* CHANGELOG\n\n* READMEs\n\n* mypy\n\n* ls\n\n* New poplar version\n\n* Fixed tests\n\n* links\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* skip azure badges\n\n* Table\n\n* Matching dockerfiles\n\n* Drop unnecessary channels and packages\n\n* Push nightly\n\n* Undo unrelated changes\n\n* Revert \"Push nightly\"\n\nThis reverts commit 9618f737c4dc65331fef4bb11fe46a61513d220a.\n\n---------\n\nCo-authored-by: Jirka <jirka.borovec@seznam.cz>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AcceleratorConnector:",
            "",
            "def _init_deterministic(self, deterministic: Optional[Union[bool, _LITERAL_WARN]]) -> None:",
            "self.deterministic = deterministic or False  # default to False if not set",
            "-        if _TORCH_GREATER_EQUAL_1_11 and deterministic == \"warn\":",
            "+        if deterministic == \"warn\":",
            "torch.use_deterministic_algorithms(True, warn_only=True)",
            "-        else:",
            "-            torch.use_deterministic_algorithms(self.deterministic)",
            "if self.deterministic:",
            "# https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility",
            "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\""
        ]
    },
    {
        "number": 4318,
        "comments": "",
        "commit_message": "lint fixes\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def linear_resample(x, num_samples, axis=-1):",
            "num_x_dims = len(x_shape)",
            "axis = axis % num_x_dims",
            "num_vals = x.shape[axis]",
            "-    if x.dtype not in ['float16','float32','float64']:",
            "-        x=tf.cast(x,tf.float32)",
            "+    if x.dtype not in [\"float16\", \"float32\", \"float64\"]:",
            "+        x = tf.cast(x, tf.float32)",
            "xp = tf.range(num_vals, dtype=tf.float32)",
            "x_coords = tf.range(num_samples, dtype=tf.float32) * (",
            "-                (num_vals - 1) / (num_samples - 1)",
            "+            (num_vals - 1) / (num_samples - 1)",
            ")",
            "else:",
            "xp = tf.range(num_vals, dtype=x.dtype)"
        ]
    },
    {
        "number": 4320,
        "comments": "",
        "commit_message": "made _load_pretrained_model_low_mem static + bug fix (#16548)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix",
            "resolved_archive_file = [resolved_archive_file]",
            "",
            "for archive_file in resolved_archive_file:",
            "-            state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")",
            "+            state_dict = torch.load(archive_file, map_location=\"cpu\")",
            "",
            "# materialize state_dict entries one by one on CPU",
            "for k in loaded_state_dict_keys:"
        ]
    },
    {
        "number": 4323,
        "comments": "",
        "commit_message": "fix to ensure that returned tensors after the tokenization is Long (#7039)\n\n* fix to ensure that returned tensors after the tokenization is Long\n\n* fix to ensure that returned tensors after the tokenization is Long\n\nCo-authored-by: Ashwin Geet Dsa <adsa@grvingt-6.nancy.grid5000.fr>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DataCollatorForLanguageModeling:",
            ") -> torch.Tensor:",
            "# In order to accept both lists of lists and lists of Tensors",
            "if isinstance(examples[0], (list, tuple)):",
            "-            examples = [torch.Tensor(e) for e in examples]",
            "+            examples = [torch.tensor(e, dtype=torch.long) for e in examples]",
            "length_of_first = examples[0].size(0)",
            "are_tensors_same_length = all(x.size(0) == length_of_first for x in examples)",
            "if are_tensors_same_length:"
        ]
    },
    {
        "number": 4326,
        "comments": "",
        "commit_message": "[release] fix long running horovod tune tests. (#29505)\n\n\n\nCo-authored-by: Amog Kamsetty <amogkam@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train_loop_per_worker(config):",
            "import horovod.torch as hvd",
            "",
            "hvd.init()",
            "-    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
            "+    device = ray.train.torch.get_device()",
            "mode = config[\"mode\"]",
            "net = Net(mode).to(device)",
            "optimizer = torch.optim.SGD("
        ]
    },
    {
        "number": 4327,
        "comments": "",
        "commit_message": "Fix comment.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ExpandDims(Preprocessor):",
            "return shape[:position] + (1,) + shape[position:]",
            "",
            "def tf_process(self, tensor):",
            "-        # Flatten tensor",
            "-        return tf.expand_dims(tensor, self.axis)",
            "+        # Expand tensor.",
            "+        return tf.expand_dims(input=tensor, axis=self.axis)"
        ]
    },
    {
        "number": 4329,
        "comments": "",
        "commit_message": "small fix to unravel_index of torch\n",
        "label": "",
        "answer": "no",
        "change": [
            "def unravel_index(",
            "*,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    temp = indices.to(\"int64\")",
            "+    temp = indices.detach()",
            "output = []",
            "for dim in reversed(shape):",
            "output.append(temp % dim)",
            "temp = temp // dim",
            "-    return torch.tensor(reversed(output))",
            "+    return torch.tensor(reversed(output), dtype=torch.int64)",
            "",
            "",
            "unravel_index.support_native_out = False"
        ]
    },
    {
        "number": 4330,
        "comments": "",
        "commit_message": "Fix indent\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class E2E(ASRInterface, torch.nn.Module):",
            "lpz = None",
            "",
            "word_eds, word_ref_lens, char_eds, char_ref_lens = [], [], [], []",
            "-            nbest_hyps = self.dec.recognize_beam_batch(hs_pad, torch.tensor(hlens), lpz,",
            "-                                                       self.recog_args, self.char_list,",
            "-                                                       self.rnnlm,",
            "-                                                       tgt_lang_ids=tgt_lang_ids.squeeze(1).tolist() if self.replace_sos else None)",
            "+            nbest_hyps = self.dec.recognize_beam_batch(",
            "+                hs_pad, torch.tensor(hlens), lpz,",
            "+                self.recog_args, self.char_list,",
            "+                self.rnnlm,",
            "+                tgt_lang_ids=tgt_lang_ids.squeeze(1).tolist() if self.replace_sos else None)",
            "# remove <sos> and <eos>",
            "y_hats = [nbest_hyp[0]['yseq'][1:-1] for nbest_hyp in nbest_hyps]",
            "for i, y_hat in enumerate(y_hats):"
        ]
    },
    {
        "number": 4331,
        "comments": "",
        "commit_message": "Upgrade black to version ~=22.0 (#15565)\n\n* Upgrade black to version ~=22.0\n\n* Check copies\n\n* Fix code\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _calculate_expected_result(",
            "# PyTorch does not currently support Huber loss with custom delta so we define it ourself",
            "def huber_loss(input, target, delta: float = 1.0):",
            "errors = torch.abs(input - target)  # shape (batch_size,)",
            "-    return torch.where(errors < delta, 0.5 * errors ** 2, errors * delta - (0.5 * delta ** 2))",
            "+    return torch.where(errors < delta, 0.5 * errors**2, errors * delta - (0.5 * delta**2))",
            "",
            "",
            "def _calculate_regression_loss("
        ]
    },
    {
        "number": 4332,
        "comments": "",
        "commit_message": "update deprecated tensorflow casting (#12367)\n\n* update deprecated tensorflow casting\n\n* Fix PEP8\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,",
            "the log probability of each decoded sequence.",
            "\"\"\"",
            "y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())",
            "-    input_length = tf.to_int32(input_length)",
            "+    input_length = tf.cast(input_length, tf.int32)",
            "",
            "if greedy:",
            "(decoded, log_prob) = ctc.ctc_greedy_decoder("
        ]
    },
    {
        "number": 4334,
        "comments": "",
        "commit_message": "fix indentation\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class local(BaseOp):",
            "tf.nn.conv2d(tij, kij,",
            "padding = 'VALID',",
            "strides = [1] * 4))",
            "-                out += [tf.concat(row_i, 2)]",
            "+            out += [tf.concat(row_i, 2)]",
            "",
            "-            self.out = tf.concat(out, 1)",
            "+        self.out = tf.concat(out, 1)",
            "",
            "def speak(self):",
            "l = self.lay"
        ]
    },
    {
        "number": 4335,
        "comments": "",
        "commit_message": "Fixed postnet for GST.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TacotronGST(nn.Module):",
            "forward_attn, trans_agent, forward_attn_mask,",
            "location_attn, separate_stopnet)",
            "self.postnet = PostCBHG(mel_dim)",
            "-        self.last_linear = nn.Sequential(",
            "-            nn.Linear(self.postnet.cbhg.gru_features * 2, linear_dim),",
            "-            nn.Sigmoid())",
            "+        self.last_linear = nn.Linear(self.postnet.cbhg.gru_features * 2, linear_dim)",
            "+",
            "",
            "def forward(self, characters, text_lengths, mel_specs, speaker_ids=None):",
            "B = characters.size(0)"
        ]
    },
    {
        "number": 4337,
        "comments": "",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "",
        "answer": "no",
        "change": [
            "def set_model_weights_in_torch(weights, torch_model, hidden_size):",
            "assert (",
            "position_embeddings.weights[emb_idx].shape == emb_weights.shape",
            "), f\"{position_embeddings[emb_idx]} emb does not match\"",
            "-            position_embeddings.weights[emb_idx] = torch.nn.Parameter(torch.tensor(emb_weights))",
            "+            position_embeddings.weights[emb_idx] = nn.Parameter(torch.tensor(emb_weights))",
            "",
            "trax_layer_weights = weights[5]",
            "assert len(torch_model_reformer.encoder.layers) * 4 == len("
        ]
    },
    {
        "number": 4340,
        "comments": "",
        "commit_message": "`HeteroData` support for `Batch.from_datalist`  (#3080)\n\n* new dataloader\n\n* fix test and linting\n\n* update\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def split(data, batch):",
            "if data.x is not None:",
            "slices['x'] = node_slice",
            "else:",
            "-        data.num_nodes = torch.bincount(batch).tolist()",
            "-        slices['num_nodes'] = torch.arange(len(data.num_nodes) + 1)",
            "+        # Imitate `collate` functionality:",
            "+        data._num_nodes = torch.bincount(batch).tolist()",
            "+        data.num_nodes = batch.numel()",
            "if data.edge_attr is not None:",
            "slices['edge_attr'] = edge_slice",
            "if data.y is not None:"
        ]
    },
    {
        "number": 4344,
        "comments": "",
        "commit_message": "densenet fixes\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def densenet_block(incoming, nb_layers, growth, bottleneck=True,",
            "\"\"\"",
            "densenet = incoming",
            "",
            "-    with tf.variable_scope(scope, default_name=name, values=[incoming],",
            "-                           reuse=reuse) as scope:",
            "+    for i in range(nb_layers):",
            "",
            "-        for i in range(nb_layers):",
            "+        with tf.variable_scope(scope, default_name=name, values=[incoming],",
            "+                               reuse=reuse) as scope:",
            "",
            "# Identity",
            "conn = densenet"
        ]
    },
    {
        "number": 4345,
        "comments": "",
        "commit_message": "lint fixes (#5332)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def sinc(",
            "def vorbis_window(",
            "window_length: Union[tf.Tensor, tf.Variable],",
            "*,",
            "-    dtype:Optional[tf.DType] = tf.dtypes.float32,",
            "-    out: Optional[Union[tf.Tensor, tf.Variable]] = None",
            "+    dtype: Optional[tf.DType] = tf.dtypes.float32,",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    return tf.signal.vorbis_window(",
            "-    window_length,",
            "-    dtype=dtype,",
            "-    name=None",
            "-    )",
            "\\ No newline at end of file",
            "+    return tf.signal.vorbis_window(window_length, dtype=dtype, name=None)"
        ]
    },
    {
        "number": 4346,
        "comments": "",
        "commit_message": "fix typo in test\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GPT2ModelLanguageGenerationTest(unittest.TestCase):",
            "@slow",
            "def test_lm_generate_distilgpt2(self):",
            "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")",
            "-        input_ids = torch.tensor([[463, 1893]], dtype=torch.long, device=torch_device)  # The president",
            "+        input_ids = torch.tensor([[464, 1893]], dtype=torch.long, device=torch_device)  # The president",
            "expected_output_ids = [",
            "464,",
            "1893,"
        ]
    },
    {
        "number": 4349,
        "comments": "",
        "commit_message": "Fixed Slice proto to allow for optional entries\n\n- Added tests for using slice on Tensors and Pandas DataFrame\n- Rebuilt protos\n- Added a new pytest category called duet for integration tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "_RETURNTYPES = _descriptor.Descriptor(",
            "syntax=\"proto3\",",
            "extension_ranges=[],",
            "oneofs=[],",
            "-    serialized_start=122,",
            "-    serialized_end=197,",
            "+    serialized_start=83,",
            "+    serialized_end=158,",
            ")",
            "",
            "_RETURNTYPES.fields_by_name["
        ]
    },
    {
        "number": 4350,
        "comments": "",
        "commit_message": "Fix CTRL past\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CTRLModel(CTRLPreTrainedModel):",
            "inputs_embeds = self.w(input_ids)",
            "# inputs_embeds = embedded.unsqueeze(0) if len(input_ids.shape)<2 else embedded",
            "seq_len = input_ids.shape[-1]",
            "-        mask = torch.triu(torch.ones(seq_len, seq_len), 1).to(inputs_embeds.device)",
            "+        mask = torch.triu(torch.ones(seq_len + past_length, seq_len + past_length), 1).to(inputs_embeds.device)",
            "",
            "inputs_embeds *= np.sqrt(self.d_model_size)"
        ]
    },
    {
        "number": 4351,
        "comments": "",
        "commit_message": "[lm examples] fix overflow in perplexity calc (#11855)\n\n* fix overflow in perplexity calc\n\n* use inf\n\n* fix\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main():",
            "",
            "losses = torch.cat(losses)",
            "losses = losses[: len(eval_dataset)]",
            "-        perplexity = math.exp(torch.mean(losses))",
            "+        try:",
            "+            perplexity = math.exp(torch.mean(losses))",
            "+        except OverflowError:",
            "+            perplexity = float(\"inf\")",
            "",
            "logger.info(f\"epoch {epoch}: perplexity: {perplexity}\")"
        ]
    },
    {
        "number": 4353,
        "comments": "",
        "commit_message": "Add Pre-commit bot fixes (#3722)\n\n* flake8\n\n* flake8\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_deep_graph_infomax():",
            "loss = model.loss(pos_z, neg_z, summary)",
            "assert 0 <= loss.item()",
            "",
            "-    acc = model.test(",
            "-        torch.ones(20, 16), torch.randint(10, (20, )), torch.ones(20, 16),",
            "-        torch.randint(10, (20, )))",
            "+    acc = model.test(torch.ones(20, 16), torch.randint(10, (20, )),",
            "+                     torch.ones(20, 16), torch.randint(10, (20, )))",
            "assert 0 <= acc and acc <= 1"
        ]
    },
    {
        "number": 4356,
        "comments": "",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            ".BatchNorm('lastbn')",
            ".apply(nonlin)",
            ".GlobalAvgPooling('gap')",
            "-                      .tf.mul(49)  # this is due to a bug in our model design",
            "+                      .tf.multiply(49)  # this is due to a bug in our model design",
            ".FullyConnected('fct', 1000)())",
            "prob = tf.nn.softmax(logits, name='output')",
            "wrong = prediction_incorrect(logits, label, 1, name='wrong-top1')"
        ]
    },
    {
        "number": 4358,
        "comments": "",
        "commit_message": "fixed trpo\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def shape(x, unknown=-1):",
            "",
            "",
            "def no_operation():",
            "+    # return tf.constant(value=False, dtype=tf_dtype(dtype='bool'))",
            "return identity_operation(x=tf.constant(value=False, dtype=tf_dtype(dtype='bool')))"
        ]
    },
    {
        "number": 4360,
        "comments": "",
        "commit_message": "Fixed tests and docs (#654)\n\n* Fixed tests\n\n* Fixed doc error and warnings\n\n* Fixed lint\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Affine(nn.Module):",
            "self.translation = translation",
            "",
            "if scale_factor is None:",
            "-            scale_factor = torch.ones(batch_size)",
            "+            scale_factor = torch.ones(batch_size, 2)",
            "self.scale_factor = scale_factor",
            "",
            "self.shear = shear"
        ]
    },
    {
        "number": 4363,
        "comments": "",
        "commit_message": "Fixed confidence_penalty for newer versions of pytorch (#3156)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def rmspe_loss(targets: torch.Tensor, predictions: torch.Tensor) -> torch.Tensor",
            "def mean_confidence_penalty(probabilities: torch.Tensor, num_classes: int) -> torch.Tensor:",
            "max_entropy = torch.log(torch.tensor(num_classes))",
            "# clipping needed for avoiding log(0) = -inf",
            "-    entropy_per_class = torch.maximum(-probabilities * torch.log(torch.clamp(probabilities, 1e-10, 1)), 0)",
            "+    entropy_per_class, _ = torch.max(-probabilities * torch.log(torch.clamp(probabilities, 1e-10, 1)), dim=0)",
            "entropy = torch.sum(entropy_per_class, -1)",
            "penalty = (max_entropy - entropy) / max_entropy",
            "return torch.mean(penalty)"
        ]
    },
    {
        "number": 4367,
        "comments": "",
        "commit_message": "More clarification fixes for update_freq documentation\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DistributedTrainingTest(tf.test.TestCase):",
            "strategy.cluster_resolver",
            "and strategy.cluster_resolver.task_type == \"worker\"",
            "):",
            "-            # Workaround for an issue with",
            "-            # `tf.distribute.MultiWorkerMirroredStrategy`",
            "+            # The below assertion is run by both chief and workers when using",
            "+            # `tf.distribute.MultiWorkerMirroredStrategy`, but only the chief",
            "+            # will log events.",
            "events_expected = []",
            "",
            "self.assertEqual(events_got, events_expected)"
        ]
    },
    {
        "number": 4370,
        "comments": "",
        "commit_message": "Rename config and environment variable for in memory max size (#2454)\n\n* Rename config and env variable IN_MEMORY_MAX_SIZE\n\n* Rename also in documentation\n\n* Fix style\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_is_small_dataset(",
            "dataset_size, config_max_in_memory_dataset_size, env_max_in_memory_dataset_size, monkeypatch",
            "):",
            "if config_max_in_memory_dataset_size != \"default\":",
            "-        monkeypatch.setattr(",
            "-            datasets.config, \"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", config_max_in_memory_dataset_size",
            "-        )",
            "+        monkeypatch.setattr(datasets.config, \"IN_MEMORY_MAX_SIZE\", config_max_in_memory_dataset_size)",
            "",
            "-    max_in_memory_dataset_size = datasets.config.HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES",
            "+    max_in_memory_dataset_size = datasets.config.IN_MEMORY_MAX_SIZE",
            "if config_max_in_memory_dataset_size == \"default\":",
            "if env_max_in_memory_dataset_size:",
            "assert max_in_memory_dataset_size == env_max_in_memory_dataset_size"
        ]
    },
    {
        "number": 4372,
        "comments": "",
        "commit_message": "fix CI/PEP\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def check_batch_state(state, max_len, pad_token):",
            "",
            "",
            "def custom_torch_load(model_path, model, training=True):",
            "-    \"\"\"Custom torch loading for transducer-models modules and parameters.",
            "+    \"\"\"Load transducer model modules and parameters with training-only ones removed.",
            "",
            "Args:",
            "-        model_path (str): Model path.",
            "-        model (torch.nn.Module): The model with pretrained modules.",
            "+        model_path (str): Model path",
            "+        model (torch.nn.Module): The model with pretrained modules",
            "",
            "\"\"\"",
            "if \"snapshot\" in os.path.basename(model_path):"
        ]
    },
    {
        "number": 4374,
        "comments": "",
        "commit_message": "Fix #139. Broken SKResNets after BlurPool addition, as a plus, SKResNets support AA now too.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ConvBnAct(nn.Module):",
            "x = self.drop_block(x)",
            "if self.act is not None:",
            "x = self.act(x)",
            "+        if self.aa is not None:",
            "+            x = self.aa(x)",
            "return x"
        ]
    },
    {
        "number": 4376,
        "comments": "",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PearsonCorrelation(Metric):",
            "labels_variance = self._labels_variance.get_metric(reset=reset)",
            "if reset:",
            "self.reset()",
            "-        denominator = (math.sqrt(predictions_variance) * math.sqrt(labels_variance))",
            "+        denominator = math.sqrt(predictions_variance) * math.sqrt(labels_variance)",
            "if np.around(denominator, decimals=5) == 0:",
            "pearson_r = 0",
            "else:"
        ]
    },
    {
        "number": 4377,
        "comments": "",
        "commit_message": "Fixing image segmentation with inference mode. (#14204)\n\n* Fixing image segmentation for inference mode.\n\n* Update src/transformers/pipelines/base.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ImageSegmentationPipeline(Pipeline):",
            "",
            "return super().__call__(*args, **kwargs)",
            "",
            "+    def get_inference_context(self):",
            "+        return torch.no_grad",
            "+",
            "def preprocess(self, image):",
            "image = self.load_image(image)",
            "target_size = torch.IntTensor([[image.height, image.width]])"
        ]
    },
    {
        "number": 4378,
        "comments": "",
        "commit_message": "[Docs] Improve docs for geometry transform (#913)\n\n* fixes linter in clahe transpose\n\n* add tolerance in add_weighted jit test\n\n* update exclude in github matrix\n\n* fix docs in camera matrix shape for unproject_points\n\n* fix transpose in clahe\n\n* reorganize geometry transform docs\n\n* improve geometry documentation links\n\n* improve image transforms docs\n\n* fix mypy in affine3d\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def _compute_equalized_tiles(interp_tiles: torch.Tensor, luts: torch.Tensor) ->",
            "tiles_equalized: torch.Tensor = torch.zeros_like(interp_tiles, dtype=torch.long)",
            "",
            "# compute the interpolation weights (shapes are 2 x TH x TW because they must be applied to 2 interp tiles)",
            "-    ih = torch.arange(2 * th - 1, -1, -1, device=interp_tiles.device).div(2. * th - 1)[None].T.expand(2 * th, tw)",
            "+    ih = torch.arange(2 * th - 1, -1, -1, device=interp_tiles.device).div(",
            "+        2. * th - 1)[None].transpose(-2, -1).expand(2 * th, tw)",
            "ih = ih.unfold(0, th, th).unfold(1, tw, tw)  # 2 x 1 x TH x TW",
            "iw = torch.arange(2 * tw - 1, -1, -1, device=interp_tiles.device).div(2. * tw - 1).expand(th, 2 * tw)",
            "iw = iw.unfold(0, th, th).unfold(1, tw, tw)  # 1 x 2 x TH x TW"
        ]
    },
    {
        "number": 4381,
        "comments": "",
        "commit_message": "fix assert message (#1040)\n\n* fix assert\n\nThe current assert \"Model must initialized in fp16 mode for ZeRO Stage 3.\" needs TLC - I rewrote it completely to match its cousen assert, so now we have 2 consistent matching asserts:\n\n- f\"fp16 is enabled but one or several model parameters have dtype that is not fp16\"\n- f\"fp16 is not enabled but one or several model parameters have dtype of fp16\"\n\n* remove f\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DeepSpeedEngine(Module):",
            "if self.zero_optimization_partition_weights() and any(",
            "[hasattr(param,",
            "'ds_id') for param in self.module.parameters()]):",
            "-                assert all([param.dtype == torch.half for param in self.module.parameters()]), f\"Model must initialized in fp16 mode for ZeRO Stage 3.\"",
            "+                assert all([param.dtype == torch.half for param in self.module.parameters()]), \"fp16 is enabled but one or several model parameters have dtype that is not fp16\"",
            "self.module.half()",
            "else:",
            "-            assert all([param.dtype == torch.float for param in self.module.parameters()]), f\"fp16 is not enabled but one or several model parameters have dtype of fp16\"",
            "+            assert all([param.dtype == torch.float for param in self.module.parameters()]), \"fp16 is not enabled but one or several model parameters have dtype of fp16\"",
            "",
            "if not self.dont_change_device:",
            "self.module.to(self.device)"
        ]
    },
    {
        "number": 4387,
        "comments": "",
        "commit_message": "fix pytorch RNNLM, and chime5 recipt to remove long/short utt\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Decoder(torch.nn.Module):",
            "local_best_scores, joint_best_ids = torch.topk(local_scores, beam, dim=1)",
            "local_best_ids = local_best_ids[:, joint_best_ids[0]]",
            "else:",
            "-                    local_best_scores, local_best_ids = torch.topk(local_att_scores, beam, dim=1)",
            "+                    local_best_scores, local_best_ids = torch.topk(local_scores, beam, dim=1)",
            "",
            "for j in six.moves.range(beam):",
            "new_hyp = {}"
        ]
    },
    {
        "number": 4391,
        "comments": "",
        "commit_message": "Loss bug fix - target_flat vs target\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class L1LossMasked(nn.Module):",
            "# target_flat: (batch * max_len, dim)",
            "target_flat = target.view(-1, target.shape[-1])",
            "# losses_flat: (batch * max_len, dim)",
            "-        losses_flat = functional.l1_loss(input, target, size_average=False,",
            "+        losses_flat = functional.l1_loss(input, target_flat, size_average=False,",
            "reduce=False)",
            "# losses: (batch, max_len, dim)",
            "losses = losses_flat.view(*target.size())"
        ]
    },
    {
        "number": 4393,
        "comments": "",
        "commit_message": "annotat unused vars (#5017)\n\n* annotate all unused vars\n\n* rank_zero_warn\n\n* Apply suggestions from code review\n\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n\n* f1 fixed\n\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def stat_scores_multiple_classes(",
            "",
            "tps = torch.zeros((num_classes + 1,), device=pred.device)",
            "fps = torch.zeros((num_classes + 1,), device=pred.device)",
            "-        tns = torch.zeros((num_classes + 1,), device=pred.device)",
            "fns = torch.zeros((num_classes + 1,), device=pred.device)",
            "sups = torch.zeros((num_classes + 1,), device=pred.device)"
        ]
    },
    {
        "number": 4396,
        "comments": "",
        "commit_message": "misc small changes and fix #688\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "if __name__ == '__main__':",
            "parser.add_argument(dest='output', help='output model file, can be npz or TF checkpoint')",
            "args = parser.parse_args()",
            "",
            "-    tf.train.import_meta_graph(args.meta)",
            "+    tf.train.import_meta_graph(args.meta, clear_devices=True)",
            "",
            "# loading...",
            "init = get_model_loader(args.input)"
        ]
    },
    {
        "number": 4397,
        "comments": "",
        "commit_message": "fixes tpu data loader bug (#957)\n\n* fixes tpu data loader bug\n\n* fixes tpu data loader bug\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TrainerTrainLoopMixin(ABC):",
            "if self.reload_dataloaders_every_epoch:",
            "self.reset_train_dataloader(self.get_model())",
            "",
            "+        # track local dataloader so TPU can wrap each epoch",
            "+        train_dataloader = self.train_dataloader",
            "+",
            "# on TPU we have to wrap it under the ParallelLoader",
            "if self.use_tpu:",
            "device = xm.xla_device()",
            "-            self.train_dataloader = xla_pl.ParallelLoader(self.train_dataloader, [device])",
            "-            self.train_dataloader = self.train_dataloader.per_device_loader(device)",
            "+            train_dataloader = xla_pl.ParallelLoader(train_dataloader, [device])",
            "+            train_dataloader = train_dataloader.per_device_loader(device)",
            "",
            "# run epoch",
            "for batch_idx, batch in self.profiler.profile_iterable(",
            "-            enumerate(self.train_dataloader), \"get_train_batch\"",
            "+            enumerate(train_dataloader), \"get_train_batch\"",
            "):",
            "# stop epoch if we limited the number of training batches",
            "if batch_idx >= self.num_training_batches:"
        ]
    },
    {
        "number": 4398,
        "comments": "",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFGroupViTModel(TFGroupViTPreTrainedModel):",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "\"pixel_values\": tf.TensorSpec((None, None, None, None), tf.float64, name=\"pixel_values\"),",
            "-                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "+                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "}",
            "]",
            ")"
        ]
    },
    {
        "number": 4400,
        "comments": "",
        "commit_message": "black fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class StochasticDurationPredictor(torch.nn.Module):",
            "z, logdet = flow(z, x_mask, g=x, inverse=inverse)",
            "logdet_tot = logdet_tot + logdet",
            "nll = (",
            "-                torch.sum(0.5 * (math.log(2 * math.pi) + (z ** 2)) * x_mask, [1, 2])",
            "+                torch.sum(0.5 * (math.log(2 * math.pi) + (z**2)) * x_mask, [1, 2])",
            "- logdet_tot",
            ")",
            "return nll + logq  # (B,)"
        ]
    },
    {
        "number": 4401,
        "comments": "",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RPNHead(AnchorHead):",
            "if cfg.min_bbox_size > 0:",
            "w = proposals[:, 2] - proposals[:, 0]",
            "h = proposals[:, 3] - proposals[:, 1]",
            "-            valid_inds = torch.nonzero((w >= cfg.min_bbox_size)",
            "-                                       & (h >= cfg.min_bbox_size)).squeeze()",
            "+            valid_inds = torch.nonzero(",
            "+                (w >= cfg.min_bbox_size)",
            "+                & (h >= cfg.min_bbox_size),",
            "+                as_tuple=False).squeeze()",
            "if valid_inds.sum().item() != len(proposals):",
            "proposals = proposals[valid_inds, :]",
            "scores = scores[valid_inds]"
        ]
    },
    {
        "number": 4403,
        "comments": "",
        "commit_message": "Fix mypy check\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RandomResizedCrop(AugmentationBase):",
            "else:",
            "batch_shape = input.shape",
            "params = RandomResizedCrop.get_params(",
            "-                batch_size, batch_shape[-2:], self.size, self.scale, self.ratio)  # type: ignore",
            "+                batch_size, batch_shape[-2:], self.size, self.scale, self.ratio)",
            "return super().forward(input, params)"
        ]
    },
    {
        "number": 4404,
        "comments": "",
        "commit_message": "fix initializer mode\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def resnet_group(l, name, block_func, features, count, stride):",
            "",
            "def resnet_backbone(image, num_blocks, group_func, block_func):",
            "with argscope(Conv2D, nl=tf.identity, use_bias=False,",
            "-                  W_init=tf.variance_scaling_initializer(scale=2.0, mode='FAN_OUT')):",
            "+                  W_init=tf.variance_scaling_initializer(scale=2.0, mode='fan_out')):",
            "logits = (LinearWrap(image)",
            ".Conv2D('conv0', 64, 7, stride=2, nl=BNReLU)",
            ".MaxPooling('pool0', shape=3, stride=2, padding='SAME')"
        ]
    },
    {
        "number": 4407,
        "comments": "",
        "commit_message": "Fix LKJ sample shape bug (#2617)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class LKJCorrCholesky(TorchDistribution):",
            "super().__init__(torch.Size(), torch.Size((d, d)), validate_args=validate_args)",
            "",
            "def sample(self, sample_shape=torch.Size()):",
            "-        y = self._gen.sample(sample_shape=self.batch_shape + sample_shape).detach()",
            "+        with torch.no_grad():",
            "+            y = self._gen.sample(sample_shape=sample_shape + self.batch_shape)",
            "z = y.mul(2).add(-1.0)",
            "return _vector_to_l_cholesky(z)"
        ]
    },
    {
        "number": 4411,
        "comments": "",
        "commit_message": "Fix uploading to s3 error handling\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def upload_to_s3_using_presigned(",
            ")  # maintain list of part no and ETag",
            "",
            "# Step 4 - Send a message to PyGrid informing about dataset upload complete!",
            "-        upload_response = client.datasets.perform_request(",
            "+        client.datasets.perform_request(",
            "syft_msg=UploadDataCompleteMessage,",
            "content={",
            "\"upload_id\": upload_response.payload.upload_id,"
        ]
    },
    {
        "number": 4416,
        "comments": "",
        "commit_message": "fix a broken link and minor update doc example (#425)\n\n* fix a broken link.\n\n* fix a broken link.\n\n* fix a broken link.\n\n* add placeholder for easy example reference.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ExpandDimsLayer(Layer):",
            "class TileLayer(Layer):",
            "\"\"\"",
            "The :class:`TileLayer` class constructs a tensor by tiling a given tensor,",
            "-    see `tf.tile() <https://www.tensorflow.org/api_docs/python/array_ops/slicing_and_joining#tile>`__ .",
            "+    see `tf.tile() <https://www.tensorflow.org/api_docs/python/tf/tile>`__ .",
            "",
            "Parameters"
        ]
    },
    {
        "number": 4417,
        "comments": "",
        "commit_message": "Fix optional typehint (continued) (#11923)\n\nCo-authored-by: @AnnaTz\n",
        "label": "",
        "answer": "no",
        "change": [
            "def thresholded_relu(",
            "x: Tensor,",
            "/,",
            "*,",
            "-    threshold: Optional[Union[int, float]] = 0,",
            "+    threshold: Union[int, float] = 0,",
            "out: Optional[Tensor] = None,",
            ") -> Tensor:",
            "return tf.where(x > threshold, x, 0)"
        ]
    },
    {
        "number": 4420,
        "comments": "",
        "commit_message": "Layer API Refactoring (#675)\n\n* Layer API Refactoring\n\n* private method decorator added\n\n* Input Layers added\n\n* Lazy Import on ROI pooling\n\n* @zsdonghao change proposal added\n\n* UTF-8 header added\n\n* gitignore updated\n\n* Python2 error correction\n\n* Changelog Updated\n\n* Python Headers Uniformized + Codacy Error Fix + LSTMStateTuple missing import\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TimeDistributedLayer(Layer):",
            "",
            "self.outputs = tf.stack(x, axis=1, name=name)",
            "",
            "-        self.all_layers.append(self.outputs)",
            "-        self.all_params.extend(variables)",
            "+        self._add_layers(self.outputs)",
            "+        self._add_params(variables)"
        ]
    },
    {
        "number": 4421,
        "comments": "",
        "commit_message": "Fixing example code\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def linear_layer(x, size):",
            "",
            "",
            "def dense_layer(x, size):",
            "-    with tf.variable_scope('linear'):",
            "+    with tf.variable_scope('dense'):",
            "weights = tf.Variable(initial_value=tf.random_normal(shape=(x.get_shape()[1].value, size), stddev=sqrt(2.0 / (x.get_shape()[1].value + size))))",
            "x = tf.matmul(a=x, b=weights)",
            "x = tf.nn.relu(features=x)"
        ]
    },
    {
        "number": 4424,
        "comments": "",
        "commit_message": "Fix other PyTorch models\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DistilBertModel(DistilBertPreTrainedModel):",
            "else:",
            "raise ValueError(\"You have to specify either input_ids or inputs_embeds\")",
            "",
            "+        device = input_ids.device if input_ids is not None else inputs_embeds.device",
            "+",
            "if attention_mask is None:",
            "-            attention_mask = torch.ones(input_shape) # (bs, seq_length)",
            "+            attention_mask = torch.ones(input_shape, device=device) # (bs, seq_length)",
            "",
            "# Prepare head mask if needed",
            "# 1.0 in head_mask indicate we keep the head"
        ]
    },
    {
        "number": 4425,
        "comments": "",
        "commit_message": "fix noise device\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LatentDiffusion(DiffusionPipeline):",
            "# 3. optionally sample variance",
            "variance = 0",
            "if eta > 0:",
            "-                noise = torch.randn(image.shape, generator=generator, device=image.device)",
            "+                noise = torch.randn(image.shape, generator=generator)to(image.device)",
            "variance = self.noise_scheduler.get_variance(t, num_inference_steps).sqrt() * eta * noise",
            "",
            "# 4. set current image to prev_image: x_t -> x_t-1"
        ]
    },
    {
        "number": 4428,
        "comments": "",
        "commit_message": "Fix EmbedSequence\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class EmbedSequence(Layer):",
            "self.dropout = None",
            "",
            "def call(self, inputs, training=None, mask=None):",
            "-        embedded = self.embed(",
            "-            inputs, training=None, mask=None",
            "+        embedded = tf.nn.embedding_lookup(",
            "+            self.embeddings, inputs, name='embeddings_lookup'",
            ")",
            "",
            "# TODO use tf2 mechanism for masking"
        ]
    },
    {
        "number": 4429,
        "comments": "",
        "commit_message": "lstsq fix in circle fitting for old PyTorch\n\nSummary: the pytorch3d.compat.lstsq function needs a 2D rhs.\n\nReviewed By: patricklabatut\n\nDifferential Revision: D36195826\n\nfbshipit-source-id: 9dbafea2057035cc04973f56729dc97b47dcac83\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def fit_circle_in_2d(",
            "n_provided = points2d.shape[0]",
            "if n_provided < 3:",
            "raise ValueError(f\"{n_provided} points are not enough to determine a circle\")",
            "-    solution = lstsq(design, rhs)",
            "-    center = solution[:2] / 2",
            "-    radius = torch.sqrt(solution[2] + (center ** 2).sum())",
            "+    solution = lstsq(design, rhs[:, None])",
            "+    center = solution[:2, 0] / 2",
            "+    radius = torch.sqrt(solution[2, 0] + (center ** 2).sum())",
            "if n_points > 0:",
            "if angles is not None:",
            "warnings.warn(\"n_points ignored because angles provided\")"
        ]
    },
    {
        "number": 4431,
        "comments": "",
        "commit_message": "fix deprecation warnings; remove pydoop dependency; update README\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _generate_image_and_label_batch(image, label, min_queue_examples,",
            "capacity=min_queue_examples + 3 * batch_size)",
            "",
            "# Display the training images in the visualizer.",
            "-  tf.image_summary('images', images)",
            "+  tf.summary.image('images', images)",
            "",
            "return images, tf.reshape(label_batch, [batch_size])"
        ]
    },
    {
        "number": 4434,
        "comments": "",
        "commit_message": "fix issue where is_initialized is not available in single-worker paradigm (#2801)\n\nSummary:\n# Before submitting\n\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes https://github.com/pytorch/fairseq/issues/1205\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/2801\n\nReviewed By: alexeib\n\nDifferential Revision: D24579193\n\nPulled By: myleott\n\nfbshipit-source-id: bcb14bb588d4538398bff4114e0a387fd29818c5\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def distributed_init(cfg: FairseqConfig):",
            "cfg = convert_namespace_to_omegaconf(cfg)",
            "",
            "if not cfg.common.tpu:",
            "-        if torch.distributed.is_initialized():",
            "+        if torch.distributed.is_available() and torch.distributed.is_initialized():",
            "warnings.warn(",
            "\"Distributed is already initialized, cannot initialize twice!\"",
            ")"
        ]
    },
    {
        "number": 4436,
        "comments": "",
        "commit_message": "Quick fix :) (#606)\n\n* Changing the name\n\n* style + quality\n\n* update doc and logo\n\n* clean up\n\n* circle-CI on the branche for now\n\n* fix daily dialog dataset\n\n* fix urls\n\nCo-authored-by: Quentin Lhoest <lhoest.q@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def disable_progress_bar():",
            "",
            "Usage:",
            "",
            "-    nlp.disable_progress_bar()",
            "+    datasets.disable_progress_bar()",
            "\"\"\"",
            "# Replace tqdm",
            "global _active"
        ]
    },
    {
        "number": 4437,
        "comments": "",
        "commit_message": "Use NewSessionCreate in PredictConfig, so that variables are initialized (fix #1225)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PredictConfig(object):",
            "assert_type(self.session_init, SessionInit, 'session_init')",
            "",
            "if session_creator is None:",
            "-            self.session_creator = tf.train.ChiefSessionCreator(config=get_default_sess_config())",
            "+            self.session_creator = NewSessionCreator(config=get_default_sess_config())",
            "else:",
            "self.session_creator = session_creator"
        ]
    },
    {
        "number": 4445,
        "comments": "",
        "commit_message": "[RLLib] Fix MultiDiscrete not being one-hotted correctly (#26558)\n\nCo-authored-by: Jun Gong <jungong@anyscale.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ComplexInputNetwork(TorchModelV2, nn.Module):",
            "cnn_out, _ = self.cnns[i](SampleBatch({SampleBatch.OBS: component}))",
            "outs.append(cnn_out)",
            "elif i in self.one_hot:",
            "-                if component.dtype in [torch.int32, torch.int64, torch.uint8]:",
            "+                if component.dtype in [",
            "+                    torch.int8,",
            "+                    torch.int16,",
            "+                    torch.int32,",
            "+                    torch.int64,",
            "+                    torch.uint8,",
            "+                ]:",
            "one_hot_in = {",
            "SampleBatch.OBS: one_hot(",
            "component, self.flattened_input_space[i]"
        ]
    },
    {
        "number": 4446,
        "comments": "",
        "commit_message": "fix(activation): add missing comma\n",
        "label": "",
        "answer": "no",
        "change": [
            "CUSTOM_FNS = {'lrelu001': lambda x: mtf.leaky_relu(x, alpha=0.01),",
            "'spike2': lambda x: mtf.exp(-x ** 2),",
            "'tanhshrink': lambda x: x - tanh(x),",
            "'softsign': lambda x: x / (mtf.abs(x) + 1),",
            "-              'softmax': lambda x: mtf.softmax(x, x.shape[-1])",
            "+              'softmax': lambda x: mtf.softmax(x, x.shape[-1]),",
            "'logsoftmax': lambda x: mtf.log_softmax(x, x.shape[-1]),",
            "'bipolarsigmoid': lambda x: mtf.sigmoid(x) * 2 - 1,",
            "'rrelu': _rrelu,"
        ]
    },
    {
        "number": 4448,
        "comments": "",
        "commit_message": "fix: renamed variable name (#18850)\n\nThe sequence_masked variable is actually the part of the sequence that is kept unmasked for the encoder. This commit renames the variable.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ViTMAEEmbeddings(nn.Module):",
            "# unshuffle to get the binary mask",
            "mask = torch.gather(mask, dim=1, index=ids_restore)",
            "",
            "-        return sequence_masked, mask, ids_restore",
            "+        return sequence_unmasked, mask, ids_restore",
            "",
            "def forward(self, pixel_values, noise=None):",
            "batch_size, num_channels, height, width = pixel_values.shape"
        ]
    },
    {
        "number": 4450,
        "comments": "",
        "commit_message": "Fix TVLT (torch device issue) (#21710)\n\n* fix tvlt ci\n\n* fix tvlt ci\n\n* fix tvlt ci\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def generate_audio_mask_noise(audio_values, audio_mask=None, mask_ratio=0.75, ma",
            "if mask_type == \"frame-level\":",
            "num_time_patches = seq_len // freq_len",
            "noise = (",
            "-            torch.rand(batch_size, num_time_patches).unsqueeze(-1).repeat(1, 1, freq_len).view(batch_size, seq_len)",
            "+            torch.rand(batch_size, num_time_patches, device=audio_values.device)",
            "+            .unsqueeze(-1)",
            "+            .repeat(1, 1, freq_len)",
            "+            .view(batch_size, seq_len)",
            ")  # noise in [0, 1]",
            "elif mask_type == \"patch-level\":",
            "-        noise = torch.rand(batch_size, seq_len)  # noise in [0, 1]",
            "+        noise = torch.rand(batch_size, seq_len, device=audio_values.device)  # noise in [0, 1]",
            "len_keep = int(seq_len * (1 - mask_ratio))",
            "return noise, len_keep"
        ]
    },
    {
        "number": 4451,
        "comments": "",
        "commit_message": "[RLlib] DDPG PyTorch version. (#7953)\n\nThe DDPG/TD3 algorithms currently do not have a PyTorch implementation. This PR adds PyTorch support for DDPG/TD3 to RLlib.\nThis PR:\n- Depends on the re-factor PR for DDPG (Functional Algorithm API).\n- Adds learning regression tests for the PyTorch version of DDPG and a DDPG (torch)\n- Updates the documentation to reflect that DDPG and TD3 now support PyTorch.\n\n* Learning Pendulum-v0 on torch version (same config as tf). Wall time a little slower (~20% than tf).\n* Fix GPU target model problem.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class OrnsteinUhlenbeckNoise(GaussianNoise):",
            "ou_new = self.ou_theta * -self.ou_state + \\",
            "self.ou_sigma * gaussian_sample",
            "self.ou_state += ou_new",
            "-                high_low = torch.from_numpy(self.action_space.high -",
            "-                                            self.action_space.low).to(",
            "-                                                self.device)",
            "-                noise = scale * self.ou_base_scale * self.ou_state * high_low",
            "+                high_m_low = torch.from_numpy(",
            "+                    self.action_space.high - self.action_space.low). \\",
            "+                    to(self.device)",
            "+                noise = scale * self.ou_base_scale * self.ou_state * high_m_low",
            "action = torch.clamp(det_actions + noise,",
            "self.action_space.low[0],",
            "self.action_space.high[0])"
        ]
    },
    {
        "number": 4456,
        "comments": "",
        "commit_message": "Fix error in mixed precision training of `TFCvtModel` (#22267)\n\n* Make sure CVT can be trained using mixed precision\n\n* Add test for keras-fit with mixed-precision\n\n* Update tests/models/cvt/test_modeling_tf_cvt.py\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\n\n---------\n\nCo-authored-by: gcuder <Gerald.Cuder@iacapps.com>\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFCvtDropPath(tf.keras.layers.Layer):",
            "return x",
            "keep_prob = 1 - self.drop_prob",
            "shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)",
            "-        random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)",
            "+        random_tensor = keep_prob + tf.random.uniform(shape, 0, 1, dtype=self.compute_dtype)",
            "random_tensor = tf.floor(random_tensor)",
            "return (x / keep_prob) * random_tensor"
        ]
    },
    {
        "number": 4459,
        "comments": "",
        "commit_message": "fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class FrameScoreFeats(AbsFeatsExtract):",
            "pad = self.win_length // 2",
            "input_lengths = input_lengths + 2 * pad",
            "",
            "-            olens = (",
            "-                torch.div(",
            "-                    (input_lengths - self.win_length),",
            "-                    self.hop_length,",
            "-                    rounding_mode=\"floor\",",
            "-                )",
            "-                + 1",
            "-            )",
            "+            olens = (input_lengths - self.win_length) // self.hop_length + 1",
            "output.masked_fill_(make_pad_mask(olens, output, 1), 0.0)",
            "else:",
            "olens = None"
        ]
    },
    {
        "number": 4460,
        "comments": "",
        "commit_message": "fix some issue with torch.pi and torch jit script\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def beta_linear_log_snr(t):",
            "",
            "@torch.jit.script",
            "def alpha_cosine_log_snr(t, s: float = 0.008):",
            "-    return -log((torch.cos((t + s) / (1 + s) * torch.pi * 0.5) ** -2) - 1)",
            "+    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1)",
            "",
            "def log_snr_to_alpha_sigma(log_snr):",
            "return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))"
        ]
    },
    {
        "number": 4464,
        "comments": "",
        "commit_message": "[WIP] Reduction when batch size < num gpus (#1609)\n\n* reduce if <= num_gpus\n\n* add test with explanation\n\n* chlog\n\n* fix changelog\n\nCo-authored-by: J. Borovec <jirka.borovec@seznam.cz>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TrainerLoggingMixin(ABC):",
            "elif isinstance(output[k], torch.Tensor) and output[k].dim() == 0:",
            "pass",
            "",
            "-            # reduce only metrics that have the same number of gpus",
            "-            elif output[k].size(0) == num_gpus:",
            "-                reduced = torch.mean(output[k])",
            "-                output[k] = reduced",
            "+            # do not reduce metrics that have batch size > num gpus",
            "+            elif output[k].size(0) <= num_gpus:",
            "+                output[k] = torch.mean(output[k])",
            "+",
            "return output"
        ]
    },
    {
        "number": 4466,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestEncoderBase(AllenNlpTestCase):",
            ")",
            "",
            "# Check that error is raised if mask has wrong batch size.",
            "-        bad_mask = torch.FloatTensor([1, 1, 0])",
            "+        bad_mask = torch.BoolTensor([True, True, False])",
            "with self.assertRaises(ValueError):",
            "self.encoder_base.reset_states(bad_mask)"
        ]
    },
    {
        "number": 4467,
        "comments": "",
        "commit_message": "Fix CIs for PyTorch 1.13 (#20686)\n\n* fix 1\n\n* fix 2\n\n* fix 3\n\n* fix 4\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CLIPSegTextTransformer(nn.Module):",
            "# take features from the eot embedding (eot_token is the highest number in each sequence)",
            "# casting to torch.int for onnx compatibility: argmax doesn't support int64 inputs with opset 14",
            "pooled_output = last_hidden_state[",
            "-            torch.arange(last_hidden_state.shape[0], device=input_ids.device), input_ids.to(torch.int).argmax(dim=-1)",
            "+            torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),",
            "+            input_ids.to(dtype=torch.int, device=last_hidden_state.device).argmax(dim=-1),",
            "]",
            "",
            "if not return_dict:"
        ]
    },
    {
        "number": 4474,
        "comments": "",
        "commit_message": "Fix parsing Keras version identifiers like `2.2.4-tf` (#3794)\n\nSigned-off-by: Max H. Gerlach <git@maxgerlach.de>\n",
        "label": "",
        "answer": "no",
        "change": [
            "from horovod.common.util  import is_version_greater_equal_than",
            "",
            "if is_version_greater_equal_than(tf.__version__, \"2.6.0\"):",
            "from keras import backend as K",
            "-    if version.parse(keras.__version__) < version.parse(\"2.9.0\"):",
            "+    if version.parse(keras.__version__.replace(\"-tf\", \"+tf\")) < version.parse(\"2.9.0\"):",
            "from keras.optimizer_v2 import optimizer_v2",
            "else:",
            "from keras.optimizers.optimizer_v2 import optimizer_v2"
        ]
    },
    {
        "number": 4476,
        "comments": "",
        "commit_message": "fix mypy error\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def tversky_loss(input: torch.Tensor, target: torch.Tensor,",
            "# compute the actual dice score",
            "dims = (1, 2, 3)",
            "intersection = torch.sum(input_soft * target_one_hot, dims)",
            "-    fps = torch.sum(input_soft * (1. - target_one_hot), dims)",
            "-    fns = torch.sum((1. - input_soft) * target_one_hot, dims)",
            "+    fps = torch.sum(input_soft * (-target_one_hot + 1.), dims)",
            "+    fns = torch.sum((-input_soft + 1.) * target_one_hot, dims)",
            "",
            "numerator = intersection",
            "denominator = intersection + alpha * fps + beta * fns",
            "tversky_loss = numerator / (denominator + eps)",
            "-    return torch.mean(1. - tversky_loss)",
            "+    return torch.mean(-tversky_loss + 1.)",
            "",
            "",
            "class TverskyLoss(nn.Module):"
        ]
    },
    {
        "number": 4478,
        "comments": "",
        "commit_message": "PoC: Accelerator refactor (#5743)\n\n* restoring the result from subprocess\n\n* fix queue.get() order for results\n\n* add missing \"block_backward_sync\" context manager\n\n* add missing \"block_backward_sync\" context manager\n\n* fix sync_batchnorm\n\n* fix supported gpu-ids for tuple\n\n* fix clip gradients and inf recursion\n\n* accelerator selection: added cluster_environment plugin\n\n* fix torchelastic test\n\n* fix reduce early stopping decision for DDP\n\n* fix tests: callbacks, conversion to lightning optimizer\n\n* fix lightning optimizer does not pickle\n\n* fix setting benchmark and deterministic option\n\n* fix slurm amp test\n\n* fix prepare_data test and determine node_rank\n\n* fix retrieving last path when testing\n\n* remove obsolete plugin argument\n\n* fix test: test_trainer_config\n\n* fix torchscript tests\n\n* fix trainer.model access\n\n* move properties\n\n* fix test_transfer_batch_hook\n\n* fix auto_select_gpus\n\n* fix omegaconf test\n\n* fix test that needs to simulate slurm ddp\n\n* add horovod plugin\n\n* fix test with named arguments\n\n* clean up whitespace\n\n* fix datamodules test\n\n* remove old accelerators\n\n* fix naming\n\n* move old plugins\n\n* move to plugins\n\n* create precision subpackage\n\n* create training_type subpackage\n\n* fix all new import errors\n\n* fix wrong arguments order passed to test\n\n* fix LR finder\n\n* Added sharded training type and amp plugin\n\n* Move clip grad to precision plugin\n\n* Added sharded spawn, select accelerators based on distributed_backend + enable custom fp16 plugin automatically\n\n* Fix import issue, attempting to fix tests\n\n* Fix initial test\n\n* Reflect hook logic from master, should wrap model after move to device\n\n* Optional state consolidation, since master has optimizers not wrapped\n\n* change attribute for instance test\n\n* reset optimizers\n\noptimizers are not used in main process, so state would be wrong.\n\n* legacy\n\n* imports in accel\n\n* legacy2\n\n* trainer imports\n\n* fix import errors after rebase\n\n* move hook to new setup location\n\n* provide unwrapping logic\n\n* fix trainer callback system\n\n* added ddp2 implementation\n\n* fix imports .legacy\n\n* move plugins\n\n* restore legacy\n\n* drop test.py from root\n\n* add tpu accelerator and plugins\n\n* fixes\n\n* fix lightning optimizer merge\n\n* reset bugreportmodel\n\n* unwrapping\n\n* step routing forward\n\n* model access\n\n* unwrap\n\n* opt\n\n* integrate distrib_type\n\n* sync changes\n\n* sync\n\n* fixes\n\n* add forgotten generators\n\n* add missing logic\n\n* update\n\n* import\n\n* missed imports\n\n* import fixes\n\n* isort\n\n* mv f\n\n* changelog\n\n* format\n\n* move helper to parallel plugin\n\n* d\n\n* add world size\n\n* clean up\n\n* duplicate\n\n* activate ddp_sharded and tpu\n\n* set nvidia flags\n\n* remove unused colab var\n\n* use_tpu <-> on_tpu attrs\n\n* make some ddp_cpu and clusterplugin tests pass\n\n* Ref/accelerator connector (#5742)\n\n* final cleanup\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* connector cleanup\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* trainer cleanup\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* accelerator cleanup + missing logic in accelerator connector\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* add missing changes to callbacks\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* reflect accelerator changes to lightning module\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* clean cluster envs\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* cleanup plugins\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* add broadcasting\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* yapf\n\n* remove plugin connector\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* plugins\n\n* manual optimization\n\n* update optimizer routing\n\n* add rank to torchelastic\n\n* fix memory mixed precision\n\n* setstate on trainer for pickling in ddp spawn\n\n* add predict method\n\n* add back commented accelerator code\n\n* adapt test for sync_batch_norm to new plugin\n\n* fix deprecated tests\n\n* fix ddp cpu choice when no num_processes are given\n\n* yapf format\n\n* skip a memory test that cannot pass anymore\n\n* fix pickle error in spawn plugin\n\n* x\n\n* avoid\n\n* x\n\n* fix cyclic import in docs build\n\n* add support for sharded\n\n* update typing\n\n* add sharded and sharded_spawn to distributed types\n\n* make unwrap model default\n\n* refactor LightningShardedDataParallel similar to LightningDistributedDataParallel\n\n* update sharded spawn to reflect changes\n\n* update sharded to reflect changes\n\n* Merge 1.1.5 changes\n\n* fix merge\n\n* fix merge\n\n* yapf isort\n\n* fix merge\n\n* yapf isort\n\n* fix indentation in test\n\n* copy over reinit scheduler implementation from dev1.2\n\n* fix apex tracking calls with dev_debugger\n\n* reduce diff to dev1.2, clean up\n\n* fix trainer config test  when gpus>0 and num_processes >0 and ddp_cpu\n\n* sort plugin tests legacy/new\n\n* fix error handling for amp on cpu\n\n* fix merge\n\n\nfix merge\n\n\nfix merge\n\n* [Feat] Resolve manual_backward (#5837)\n\n* resolve manual_backward\n\n* resolve flake8\n\n* update\n\n* resolve for ddp_spawn\n\n* resolve flake8\n\n* resolve flake8\n\n* resolve flake8\n\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-88-60.ec2.internal>\n\n* fix tests/accelerator tests on cpu\n\n* [BugFix] Resolve manual optimization (#5852)\n\n* resolve manual_optimization\n\n* update\n\n* update\n\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-88-60.ec2.internal>\n\n* Remove copy trainer parameters to happen earlier within the loop and add safe guard to get ref model (#5856)\n\n* resovle a bug\n\n* Accelerator refactor sharded rpc (#5854)\n\n* rpc branch\n\n* merge\n\n* update handling of rpc\n\n* make devices etc. Optional in RPC\n\n* set devices etc. later if necessary\n\n* remove devices from sequential\n\n* make devices optional in rpc\n\n* fix import\n\n* uncomment everything\n\n* fix cluster selection\n\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-88-60.ec2.internal>\n\n* resolve bug\n\n* fix assert in rpc test\n\n* resolve a test\n\n* fix docs compilation\n\n* accelerator refactor - fix for sharded parity test (#5866)\n\n* fix memory issue with ddp_spawn\n\n* x\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n* x\n\n* Remove DDP2 as this does not apply\n\n* Add missing pre optimizer hook to ensure lambda closure is called\n\n* fix apex docstring\n\n* [accelerator][BugFix] Resolve some test for 1 gpu (#5863)\n\n* update\n\n* revert init\n\n* resolve a bug\n\n* update\n\n* resolve flake8\n\n* update\n\n* update\n\n* update\n\n* revert init\n\n* resolve a bug\n\n* update\n\n* resolve flake8\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* revert init\n\n* resolve a bug\n\n* update\n\n* resolve flake8\n\n* update\n\n* update\n\n* update\n\n* revert init\n\n* update\n\n* resolve flake8\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* all_gather\n\n* update\n\n* make plugins work, add misconfig for RPC\n\n* update\n\n* update\n\n* remove breaking test\n\n* resolve some tests\n\n* resolve flake8\n\n* revert to ddp_spawn\n\nCo-authored-by: root <root@ip-172-31-88-60.ec2.internal>\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-88-60.ec2.internal>\nCo-authored-by: Justus Schock <justus.schock@rwth-aachen.de>\n\n* yapf isort\n\n* resolve flake8\n\n* fix apex doctests\n\n* fix apex doctests 2\n\n* resolve docs\n\n* update drone\n\n* clean env\n\n* update\n\n* update\n\n* update\n\n* update\n\n* merge\n\n* Fix RPC related tests, clean out old API, update for new accelerator API [skip ci] (#5881)\n\n* Fix RPC related tests, clean out old API, update for new accelerator API\n\n* Move tests out of legacy folder, update paths and names\n\n* Update test_remove_1-4.py\n\n* Expose properties for tpu cores/gpus/num_gpus\n\n* Add root GPU property\n\n* Move properties to properties.py\n\n* move tests that were previously in drone\n\n* Fix root GPU property (#5908)\n\n* Move root GPU to property, remove horovod set as this is handled in horovod plugin, ensure we mock correctly to set GPU accelerator\n\n* Add missing tests back\n\n* fix best model path transfer when no checkpoint callback available\n\n* Fix setup hook order [wip] (#5858)\n\n* Call trainer setup hook before accelerator setup\n\n* Add test case\n\n* add new test\n\n* typo\n\n* fix callback order in test\n\nCo-authored-by: tchaton <thomas@grid.ai>\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* rename ddp sequential -> rpc sequential for special test\n\n* revert\n\n* fix stupid merge problem\n\n* Use property in connector for sampler (#5913)\n\n* merge the import conflicts\n\n* fix spawning of processes in slurm\n\n* [wip] Fix some bugs for TPU [skip ci] (#5878)\n\n* fixed for single tpu\n\n* fixed spawn\n\n* fixed spawn\n\n* update\n\n* update\n\n* wip\n\n* resolve bugs\n\n* resolve bug\n\n* update on comment\n\n* removed decorator\n\n* resolve comments\n\n* set to 4\n\n* update\n\n* update\n\n* need cleaning\n\n* update\n\n* update\n\n* update\n\n* resolve flake8\n\n* resolve bugs\n\n* exclude broadcast\n\n* resolve bugs\n\n* change test\n\n* update\n\n* update\n\n* skip if meet fails\n\n* properly raise trace\n\n* update\n\n* add catch\n\n* wrap test\n\n* resolve typo\n\n* update\n\n* typo\n\nCo-authored-by: Lezwon Castelino <lezwon@gmail.com>\nCo-authored-by: Your Name <you@example.com>\n\n* resolve some tests\n\n* update\n\n* fix imports\n\n* update\n\n* resolve flake8\n\n* update azure pipeline\n\n* skip a sharded test on cpu that requires a gpu\n\n* resolve tpus\n\n* resolve bug\n\n* resolve flake8\n\n* update\n\n* updat utils\n\n* revert permission change on files\n\n* suggestions from carlos\n\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n\n* remove unrelated formatting changes\n\n* remove incomplete comment\n\n* Update pytorch_lightning/accelerators/__init__.py\n\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n\n* remove unrelated formatting change\n\n* add types\n\n* warn 1.7 ddp manual backward only if ddp kwarg unset\n\n* yapf + isort\n\n* pep8 unused imports\n\n* fix cyclic import in docs\n\n* Apply suggestions from code review\n\n* typer in accelerator.py\n\n* typo\n\n* Apply suggestions from code review\n\n* formatting\n\n* update on comments\n\n* update typo\n\n* Update pytorch_lightning/trainer/properties.py\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\n\n* update\n\n* suggestion from code review\n\n* suggestion from code review\n\nCo-authored-by: Adrian W\u00e4lchli <aedu.waelchli@gmail.com>\nCo-authored-by: SeanNaren <sean@grid.ai>\nCo-authored-by: Jirka Borovec <jirka.borovec@seznam.cz>\nCo-authored-by: chaton <thomas@grid.ai>\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-88-60.ec2.internal>\nCo-authored-by: Sean Naren <sean.narenthiran@gmail.com>\nCo-authored-by: root <root@ip-172-31-88-60.ec2.internal>\nCo-authored-by: Lezwon Castelino <lezwon@gmail.com>\nCo-authored-by: Your Name <you@example.com>\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: mergify[bot] <37929162+mergify[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MetricsHolder:",
            "else:",
            "current = torch.tensor(current, device=device, dtype=torch.float)",
            "",
            "-        if use_tpu and _TPU_AVAILABLE:",
            "+        if isinstance(current, torch.Tensor) and current.device.type == \"xla\":",
            "current = current.cpu()",
            "",
            "return current"
        ]
    },
    {
        "number": 4483,
        "comments": "",
        "commit_message": ":pencil2: Fix typo (#9020)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFEncoderLayer(tf.keras.layers.Layer):",
            "if self.normalize_before:",
            "x = self.final_layer_norm(x)",
            "x = self.activation_fn(self.fc1(x))",
            "-        x = tf.nn.dropout(x, rate=self.self.activation_dropout if training else 0)",
            "+        x = tf.nn.dropout(x, rate=self.activation_dropout if training else 0)",
            "x = self.fc2(x)",
            "x = tf.nn.dropout(x, rate=self.dropout if training else 0)",
            "x = residual + x"
        ]
    },
    {
        "number": 4484,
        "comments": "",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_mean_var(batch_shape):",
            "dim = 2",
            "loc = torch.randn(batch_shape + (dim,))",
            "A = torch.randn(batch_shape + (dim, dim + dim))",
            "-    scale_tril = A.matmul(A.transpose(-2, -1)).cholesky()",
            "+    scale_tril = torch.linalg.cholesky(A.matmul(A.transpose(-2, -1)))",
            "df = torch.randn(batch_shape).exp() + 4",
            "num_samples = 100000",
            "d = MultivariateStudentT(df, loc, scale_tril)"
        ]
    },
    {
        "number": 4485,
        "comments": "",
        "commit_message": "fix warper.compute_subpixel_step and _compute_projection to not crash\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DepthWarper(nn.Module):",
            "1.0 - delta_d)",
            "xy_p1 = self._compute_projection(self.width / 2, self.height / 2,",
            "1.0 + delta_d)",
            "-        dx = torch.norm((xy_p1 - xy_m1), 2, dim=2) / 2.0",
            "+        dx = torch.norm((xy_p1 - xy_m1), 2, dim=-1) / 2.0",
            "dxdd = dx / (delta_d)  # pixel*(1/meter)",
            "# half pixel sampling, we're interested in the min for all cameras",
            "return torch.min(0.5 / dxdd)"
        ]
    },
    {
        "number": 4487,
        "comments": "",
        "commit_message": "Fix BN momentum in inception*\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BasicConv2d(nn.Module):",
            "",
            "def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):",
            "super(BasicConv2d, self).__init__()",
            "-        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False) # verify bias false",
            "-        self.bn = nn.BatchNorm2d(out_planes, eps=0.001, momentum=0, affine=True)",
            "+        self.conv = nn.Conv2d(in_planes, out_planes,",
            "+                              kernel_size=kernel_size, stride=stride,",
            "+                              padding=padding, bias=False) # verify bias false",
            "+        self.bn = nn.BatchNorm2d(out_planes,",
            "+                                 eps=0.001, # value found in tensorflow",
            "+                                 momentum=0.1, # default pytorch value",
            "+                                 affine=True)",
            "self.relu = nn.ReLU(inplace=True)",
            "",
            "def forward(self, x):"
        ]
    },
    {
        "number": 4490,
        "comments": "",
        "commit_message": "Fix inference CI device error (#2824)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestAutoTensorParallelism(DistributedTest):",
            "",
            "# We have to load these large models on CPU with pipeline because not",
            "# enough GPU memory",
            "-        pipe = pipeline(task, model=model, device=-1, framework=\"pt\")",
            "+        pipe = pipeline(task, model=model, device=torch.device(\"cpu\"), framework=\"pt\")",
            "bs_output = pipe(query, **inf_kwargs)",
            "",
            "pipe.model = deepspeed.init_inference(pipe.model,"
        ]
    },
    {
        "number": 4492,
        "comments": "",
        "commit_message": "added fixes\n\nPiperOrigin-RevId: 325186954\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "\"source\": [",
            "\"model.compile(optimizer='adam',\\n\",",
            "\"              loss=tf.losses.BinaryCrossentropy(from_logits=True),\\n\",",
            "-        \"              metrics=['accuracy'])\"",
            "+        \"              metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])\"",
            "]",
            "},",
            "{"
        ]
    },
    {
        "number": 4493,
        "comments": "",
        "commit_message": "Remove type ignore from the codebase (#2030)\n\n* Remove type ignore from the codebase\n\n* undo `NamedTuple` property with `List[Self]`\n\n- This is causing `Segmentation fault (core dumped)` when running `mypy --cobertura-xml-report ./`\n\n* add TODO for next mypy release\n\n* fix typo\n\n* fix F401\n",
        "label": "",
        "answer": "no",
        "change": [
            "def random_intrinsics(low: Union[float, torch.Tensor], high: Union[float, torch.",
            "the random camera matrix with the shape of :math:`(1, 3, 3)`.",
            "\"\"\"",
            "sampler = torch.distributions.Uniform(low, high)",
            "-    fx, fy, cx, cy = (sampler.sample((1,)) for _ in range(4))",
            "+    fx, fy, cx, cy = (sampler.sample(torch.Size((1,))) for _ in range(4))",
            "zeros, ones = torch.zeros_like(fx), torch.ones_like(fx)",
            "camera_matrix: torch.Tensor = torch.cat([fx, zeros, cx, zeros, fy, cy, zeros, zeros, ones])",
            "return camera_matrix.view(1, 3, 3)"
        ]
    },
    {
        "number": 4494,
        "comments": "",
        "commit_message": "Moving bbox-related functionality to bbox module (#1103)\n\n* Moving bbox-related functionality to bbox module\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* With transform boxes test moved to test/bbox\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix importing bbox methods across the library\n\n* With backward compatibility and deprecation warnings included\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Backward compatibility with fixed names\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* With lost import\n\n* With docs entry for bbox module\n\n* Naming changed from _3d to 3d\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AugmentationSequential(ImageSequential):",
            ") -> torch.Tensor:",
            "if isinstance(module, GeometricAugmentationBase2D):",
            "transform = module.compute_inverse_transformation(module.get_transformation_matrix(input, param))",
            "-            input = transform_boxes(torch.as_tensor(transform, device=input.device, dtype=input.dtype), input, mode)",
            "+            input = transform_bbox(torch.as_tensor(transform, device=input.device, dtype=input.dtype), input, mode)",
            "return input",
            "",
            "def inverse_keypoints("
        ]
    },
    {
        "number": 4495,
        "comments": "",
        "commit_message": "Fixed failing tests for meta torch (#3652)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def divide(",
            "x1, x2 = ivy.promote_types_of_inputs(x1, x2)",
            "ret = torch.div(x1, x2)",
            "if ivy.is_float_dtype(x1.dtype):",
            "-        ret = torch.tensor(ret, dtype=x1.dtype)",
            "+        ret = ret.to(x1.dtype)",
            "else:",
            "-        ret = torch.tensor(ret, dtype=ivy.default_float_dtype(as_native=True))",
            "+        ret = ret.to(ivy.default_float_dtype(as_native=True))",
            "return ret"
        ]
    },
    {
        "number": 4498,
        "comments": "",
        "commit_message": "Copy to local before loading checkpoint\n\nSummary: Follow-up for \"Fix FSDP optim state loading (#1819)\". Update for remote file systems.\n\nReviewed By: sshleifer\n\nDifferential Revision: D28088088\n\nfbshipit-source-id: 5d2f3ea5084fbbb21564d053317d2c07565cf2bc\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Trainer(object):",
            "last_optim_state = state.get(\"last_optimizer_state\", None)",
            "if last_optim_state == -1:",
            "master_path = re.sub(\"shard[0-9]+\", \"shard0\", filename)",
            "-                    last_optim_state = torch.load(master_path, map_location='cpu')['last_optimizer_state']",
            "+                    local_master_path = PathManager.get_local_path(master_path)",
            "+                    last_optim_state = torch.load(local_master_path, map_location='cpu')['last_optimizer_state']",
            "",
            "# If doing zero_sharding, do not broadcast global optimizer",
            "# state. Later we will broadcast sharded states to each rank"
        ]
    },
    {
        "number": 4499,
        "comments": "",
        "commit_message": "Fix pylint errors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ImageCaptioningInputPipeline(InputPipeline):",
            "self.params[\"caption_tokens_field\"]),",
            "\"target_len\": tfexample_decoder.ItemHandlerCallback(",
            "keys=[self.params[\"caption_tokens_field\"]],",
            "-            func=lambda dict: tf.size(dict[self.params[\"caption_tokens_field\"]]))",
            "+            func=lambda dict: tf.size(",
            "+                dict[self.params[\"caption_tokens_field\"]]))",
            "}",
            "",
            "decoder = TFSEquenceExampleDecoder("
        ]
    },
    {
        "number": 4501,
        "comments": "",
        "commit_message": "Add docstrings of core modules and methods (#3120)\n\n* Add docstrings of core modules and methods\n\n* Update docs and fix comments\n\n* Complete docstrings\n\n* Resolve comments\n\n* reformat docstrings\n\n* resolve comments\n",
        "label": "",
        "answer": "no",
        "change": [
            "from .base_bbox_coder import BaseBBoxCoder",
            "",
            "@BBOX_CODERS.register_module()",
            "class PseudoBBoxCoder(BaseBBoxCoder):",
            "+    \"\"\"Pseudo bounding box coder\"\"\"",
            "",
            "def __init__(self, **kwargs):",
            "super(BaseBBoxCoder, self).__init__(**kwargs)",
            "",
            "def encode(self, bboxes, gt_bboxes):",
            "+        \"\"\"torch.Tensor: return the given ``bboxes``\"\"\"",
            "return gt_bboxes",
            "",
            "def decode(self, bboxes, pred_bboxes):",
            "+        \"\"\"torch.Tensor: return the given ``pred_bboxes``\"\"\"",
            "return pred_bboxes"
        ]
    },
    {
        "number": 4502,
        "comments": "",
        "commit_message": "Improve reproduceability 2/3 (#1906)\n\n* [Repro] Correct reproducability\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* need better image\n\n* allow conversion from no state dict checkpoints\n\n* up\n\n* up\n\n* up\n\n* up\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* next try\n\n* up\n\n* up\n\n* better name\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\n* correct more\n\n* up\n\n* replace all torch randn\n\n* fix\n\n* correct\n\n* correct\n\n* finish\n\n* fix more\n\n* up\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PNDMPipeline(DiffusionPipeline):",
            "# the official paper: https://arxiv.org/pdf/2202.09778.pdf",
            "",
            "# Sample gaussian noise to begin loop",
            "-        image = torch.randn(",
            "+        image = randn_tensor(",
            "(batch_size, self.unet.in_channels, self.unet.sample_size, self.unet.sample_size),",
            "generator=generator,",
            "+            device=self.device,",
            ")",
            "-        image = image.to(self.device)",
            "",
            "self.scheduler.set_timesteps(num_inference_steps)",
            "for t in self.progress_bar(self.scheduler.timesteps):"
        ]
    },
    {
        "number": 4503,
        "comments": "",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def load_data():",
            "(X_np, Y), _ = multi_mnist(inpath, max_digits=2, canvas_size=50, seed=42)",
            "X_np = X_np.astype(np.float32)",
            "X_np /= 255.0",
            "-    X = Variable(torch.from_numpy(X_np))",
            "+    X = torch.from_numpy(X_np)",
            "# Using FloatTensor to allow comparison with values sampled from",
            "# Bernoulli.",
            "counts = torch.FloatTensor([len(objs) for objs in Y])"
        ]
    },
    {
        "number": 4507,
        "comments": "",
        "commit_message": "Added Bfloat16Finfo to tensorflow and fixed failing tests for dtype finfo (#4577)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def finfo(type: Union[DType, str, tf.Tensor, tf.Variable]) -> Finfo:",
            "if isinstance(type, tf.Tensor):",
            "type = type.dtype",
            "if ivy.as_native_dtype(type) == tf.bfloat16:",
            "-        return Finfo(tf.experimental.numpy.finfo(tf.float32))",
            "+        return Finfo(Bfloat16Finfo())",
            "return Finfo(tf.experimental.numpy.finfo(ivy.as_native_dtype(type)))"
        ]
    },
    {
        "number": 4508,
        "comments": "",
        "commit_message": "Fix segment_ops to use int32 when calling scatter_nd. This is needed because scatter_nd does not support boolean updates when running on GPU.\n\nPiperOrigin-RevId: 268456498\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def segment_diff(x,",
            "",
            "needs_fix = tf.scatter_nd(",
            "fix_indices,",
            "-        tf.reshape(tf.ones_like(fix_indices, dtype=tf.bool), [-1]),",
            "+        # Unfortunately, scatter_nd doesn't support bool on GPUs so we need to",
            "+        # do ints here and then convert to bool.",
            "+        tf.reshape(tf.ones_like(fix_indices, dtype=tf.int32), [-1]),",
            "shape=tf.shape(x))",
            "# If exclusive is False, then needs_fix means we need to replace the values",
            "# in raw_diffs at those locations with the values in x.",
            "+    needs_fix = tf.cast(needs_fix, dtype=tf.bool)",
            "if not exclusive:",
            "return tf.where(needs_fix, x, raw_diffs)"
        ]
    },
    {
        "number": 4510,
        "comments": "",
        "commit_message": "Fix BERT\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BertModel(BertPreTrainedModel):",
            "device = input_ids.device if input_ids is not None else inputs_embeds.device",
            "",
            "if attention_mask is None:",
            "-            attention_mask = torch.ones(input_shape)",
            "+            attention_mask = torch.ones(input_shape, device=device)",
            "if encoder_attention_mask is None:",
            "-            encoder_attention_mask = torch.ones(input_shape)",
            "+            encoder_attention_mask = torch.ones(input_shape, device=device)",
            "if token_type_ids is None:",
            "-            token_type_ids = torch.zeros(input_shape, dtype=torch.long)",
            "+            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)",
            "",
            "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]",
            "# ourselves in which case we just need to make it broadcastable to all heads."
        ]
    },
    {
        "number": 4511,
        "comments": "",
        "commit_message": "fix frequency in rotary vision transformer\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AxialRotaryEmbedding(nn.Module):",
            "def __init__(self, dim, max_freq = 10):",
            "super().__init__()",
            "self.dim = dim",
            "-        scales = torch.logspace(1., log(max_freq / 2) / log(2), self.dim // 4, base = 2)",
            "+        scales = torch.logspace(0., log(max_freq / 2) / log(2), self.dim // 4, base = 2)",
            "self.register_buffer('scales', scales)",
            "",
            "def forward(self, x):"
        ]
    },
    {
        "number": 4515,
        "comments": "",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFSpeech2TextPreTrainedModel(TFPreTrainedModel):",
            "input_signature=[",
            "{",
            "\"input_features\": tf.TensorSpec((None, None, None), tf.float32, name=\"input_features\"),",
            "-                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "-                \"decoder_input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"decoder_input_ids\"),",
            "-                \"decoder_attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"decoder_attention_mask\"),",
            "+                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "+                \"decoder_input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"decoder_input_ids\"),",
            "+                \"decoder_attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"decoder_attention_mask\"),",
            "}",
            "]",
            ")"
        ]
    },
    {
        "number": 4518,
        "comments": "",
        "commit_message": "Traced models serialization and torchscripting fix (#17206)\n\n* Fix torch.jit.script and pickling issues\n\n* Fix get_attr issues\n\n* Fix import in function\n\n* Fix GPT-J and T5 tracing for torch=1.11\n\n* Gate graph surgery on torch version\n\n* Modeling minor changes to enable TorchScripting\n\n* Model serialization / deserialization test\n\n* Remove _assert_is_none users\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MobileBertEmbeddings(nn.Module):",
            "# dimensional output.",
            "inputs_embeds = torch.cat(",
            "[",
            "-                    nn.functional.pad(inputs_embeds[:, 1:], [0, 0, 0, 1, 0, 0], value=0),",
            "+                    nn.functional.pad(inputs_embeds[:, 1:], [0, 0, 0, 1, 0, 0], value=0.0),",
            "inputs_embeds,",
            "-                    nn.functional.pad(inputs_embeds[:, :-1], [0, 0, 1, 0, 0, 0], value=0),",
            "+                    nn.functional.pad(inputs_embeds[:, :-1], [0, 0, 1, 0, 0, 0], value=0.0),",
            "],",
            "dim=2,",
            ")"
        ]
    },
    {
        "number": 4520,
        "comments": "",
        "commit_message": "Fix GPU device placement when calling tensor.cuda() (#624)\n\n* Correctly set cuda device when constructing cuda tensors\n\n* Fix .enumerate_support() methods\n\n* Revert to older invocation of torch.arange(-, -)\n\n* Support pytorch 0.2 invocation of .expand()\n\n* Fix type of Bernoulli sample\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Bernoulli(Distribution):",
            "sample.",
            ":rtype: torch.autograd.Variable.",
            "\"\"\"",
            "-        return Variable(torch.stack([torch.Tensor([t]).expand_as(self.ps) for t in [0, 1]]))",
            "+        result = torch.arange(0, 2)  # TODO Convert to LongTensor or ByteTensor.",
            "+        result = result.view((-1,) + (1,) * self.ps.dim()).expand((2,) + self.ps.size())",
            "+        if self.ps.is_cuda:",
            "+            result = result.cuda(self.ps.get_device())",
            "+        return Variable(result)",
            "",
            "def analytic_mean(self):",
            "\"\"\""
        ]
    },
    {
        "number": 4521,
        "comments": "",
        "commit_message": "GH-1624: fix numpy warning\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class FastTextEmbeddings(TokenEmbeddings):",
            "word_embedding = np.zeros(self.embedding_length, dtype=\"float\")",
            "",
            "word_embedding = torch.tensor(",
            "-            word_embedding, device=flair.device, dtype=torch.float",
            "+            word_embedding.tolist(), device=flair.device, dtype=torch.float",
            ")",
            "return word_embedding"
        ]
    },
    {
        "number": 4524,
        "comments": "",
        "commit_message": "[Fix] Fixes jit tests in feature for pytorch 1.6 (#858)\n\n* fix test jit issues in feature module\n\n* add github actions job for fast test pytorch versions\n\n* fixed orientation module mypy issues\n\n* add torch.jit.annotation for mypy issues\n\n* remove import cast\n\n* preset self.angle_detector as nn.Module\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class HardNet(nn.Module):",
            "# training totally unstable.",
            "return (x - mp.detach()) / (sp.detach() + eps)",
            "",
            "-    def forward(self, input: torch.Tensor) -> torch.Tensor:   # type: ignore",
            "+    def forward(self, input: torch.Tensor) -> torch.Tensor:",
            "x_norm: torch.Tensor = self._normalize_input(input)",
            "x_features: torch.Tensor = self.features(x_norm)",
            "x_out = x_features.view(x_features.size(0), -1)"
        ]
    },
    {
        "number": 4528,
        "comments": "",
        "commit_message": "[RLlib] DQN torch version. (#7597)\n\n* Fix.\n\n* Rollback.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* Test case fixes.\n\n* Test case fixes and LINT.\n\n* Test case fixes and LINT.\n\n* Rollback.\n\n* WIP.\n\n* WIP.\n\n* Test case fixes.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* Add regression test for DQN w/ param noise.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* Comment\n\n* Regression test case.\n\n* WIP.\n\n* WIP.\n\n* LINT.\n\n* LINT.\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n\n* LINT.\n\n* Fix (SAC does currently not support eager).\n\n* Fix.\n\n* WIP.\n\n* LINT.\n\n* Update rllib/evaluation/sampler.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/evaluation/sampler.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/utils/exploration/exploration.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/utils/exploration/exploration.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* LINT.\n\n* Fix and LINT.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* Fix.\n\n* LINT.\n\n* Fix.\n\n* Fix and LINT.\n\n* Update rllib/utils/exploration/exploration.py\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Update rllib/policy/dynamic_tf_policy.py\n\nCo-Authored-By: Eric Liang <ekhliang@gmail.com>\n\n* Fixes.\n\n* WIP.\n\n* LINT.\n\n* Fixes and LINT.\n\n* LINT and fixes.\n\n* LINT.\n\n* Move action_dist back into torch extra_action_out_fn and LINT.\n\n* Working SimpleQ learning cartpole on both torch AND tf.\n\n* Working Rainbow learning cartpole on tf.\n\n* Working Rainbow learning cartpole on tf.\n\n* WIP.\n\n* LINT.\n\n* LINT.\n\n* Update docs and add torch to APEX test.\n\n* LINT.\n\n* Fix.\n\n* LINT.\n\n* Fix.\n\n* Fix.\n\n* Fix and docstrings.\n\n* Fix broken RLlib tests in master.\n\n* Split BAZEL learning tests into cartpole and pendulum (reached the 60min barrier).\n\n* Fix error_outputs option in BAZEL for RLlib regression tests.\n\n* Fix.\n\n* Tune param-noise tests.\n\n* LINT.\n\n* Fix.\n\n* Fix.\n\n* test\n\n* test\n\n* test\n\n* Fix.\n\n* Fix.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* WIP.\n\n* LINT.\n\n* WIP.\n\nCo-authored-by: Eric Liang <ekhliang@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def do_test_log_likelihood(run,",
            "layer_key[0])])",
            "else:",
            "expected_mean_logstd = fc(",
            "-                        fc(obs_batch,",
            "-                           vars[\"_hidden_layers.0._model.0.weight\"]),",
            "-                        vars[\"_logits._model.0.weight\"])",
            "+                        fc(",
            "+                            obs_batch,",
            "+                            np.transpose(",
            "+                                vars[\"_hidden_layers.0._model.0.weight\"])),",
            "+                        np.transpose(vars[\"_logits._model.0.weight\"]))",
            "mean, log_std = np.split(expected_mean_logstd, 2, axis=-1)",
            "if logp_func is None:",
            "expected_logp = np.log(norm.pdf(a, mean, np.exp(log_std)))"
        ]
    },
    {
        "number": 4530,
        "comments": "",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFEncoderDecoderModel(TFPreTrainedModel, TFCausalLanguageModelingLoss):",
            "`Dict[str, tf.Tensor]`: The dummy inputs.",
            "\"\"\"",
            "# Add `decoder_input_ids` because `self.decoder` requires it.",
            "-        input_ids = tf.constant(DUMMY_INPUTS)",
            "+        input_ids = tf.constant(DUMMY_INPUTS, dtype=tf.int32)",
            "dummy = {\"input_ids\": input_ids, \"decoder_input_ids\": input_ids}",
            "return dummy"
        ]
    },
    {
        "number": 4531,
        "comments": "",
        "commit_message": "attempting to fix issue with deepspeed fp16 seeing overflowing gradient\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def discretized_gaussian_log_likelihood(x, *, means, log_scales, thres = 0.999):",
            "log_cdf_plus,",
            "torch.where(x > thres,",
            "log_one_minus_cdf_min,",
            "-            log(cdf_delta)))",
            "+            log(cdf_delta, eps = eps)))",
            "",
            "return log_probs"
        ]
    },
    {
        "number": 4532,
        "comments": "",
        "commit_message": "WIP: Support for Training with BF16 (#13207)\n\n* started bf16 integration\n\n* minor changes\n\n* code now runs\n\n* style\n\n* lay foundation for bf16 testing\n\n* lay foundation for bf16 testing\n\n* start the tests\n\n* better bf16 check\n\n* style\n\n* 2 separate checkers - one for bf16 support, another for bf16+autocast\n\n* Update src/transformers/training_args.py\n\nCo-authored-by: Stas Bekman <stas00@users.noreply.github.com>\n\n* a couple of comment resolutions\n\n* more comment resolutions\n\n* resolved a small bug\n\n* just some print statemtns\n\n* added todo marking\n\n* added a todo\n\n* adjust for API change s/fast_dtype/dtype/\n\n* fix style\n\n* merge 2 bf16 util functions\n\n* bf16 now does scaling too\n\n* Add support for bfloat16\n\n* Revert T5 layernorm to float32\n\nThis is based on the comment at https://github.com/huggingface/transformers/pull/14448/files#r752660929 and the PyTorch PR https://github.com/pytorch/pytorch/pull/66920 .\n\n* Add comment about conversion to float32 before returning the numpy data\n\n* Add comment about AMP-bfloat16 incompatibility\n\n* Fix formatting\n\n* typo\n\n* reformer / bf16\n\n* cleanup\n\n* require at least pt-1.10\n\n* fix\n\n* will deal with deepspeed separately\n\n* cleanup\n\n* revert\n\n* cleanup\n\n* fp16_full_eval and bf16_full_eval are separate modes\n\n* proper deprecation\n\n* cleanup\n\n* test and fixes\n\n* spelling\n\n* cleanup\n\n* add a note that this API is experimental\n\nCo-authored-by: jamie <jamie@cortx.com>\nCo-authored-by: Stas Bekman <stas@stason.org>\nCo-authored-by: Stas Bekman <stas00@users.noreply.github.com>\nCo-authored-by: suriya <suriya@cortx.com>\nCo-authored-by: Manuel R. Ciosici <manuelrciosici@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class T5LayerNorm(nn.Module):",
            "variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)",
            "hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)",
            "",
            "-        # convert into float16 if necessary",
            "-        if self.weight.dtype == torch.float16:",
            "-            hidden_states = hidden_states.to(torch.float16)",
            "+        # convert into half-precision if necessary",
            "+        if self.weight.dtype in [torch.float16, torch.bfloat16]:",
            "+            hidden_states = hidden_states.to(self.weight.dtype)",
            "+",
            "return self.weight * hidden_states"
        ]
    },
    {
        "number": 4533,
        "comments": "",
        "commit_message": "Fix voice conversion inference (#1583)\n\n* Add voice conversion zoo test\n\n* Fix style\n\n* Fix unit test\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestVits(unittest.TestCase):",
            "args = VitsArgs(num_speakers=num_speakers, use_speaker_embedding=True)",
            "model = Vits(args)",
            "",
            "-        ref_inp = torch.randn(1, spec_len, 513)",
            "+        ref_inp = torch.randn(1, 513, spec_len)",
            "ref_inp_len = torch.randint(1, spec_effective_len, (1,))",
            "ref_spk_id = torch.randint(1, num_speakers, (1,))",
            "tgt_spk_id = torch.randint(1, num_speakers, (1,))"
        ]
    },
    {
        "number": 4537,
        "comments": "",
        "commit_message": "ds_report bug fix on cpu and guard torch import in setup.py (#524)\n\n* on cpu box error gracefully if cuda home doesn't exist\n\n* gaurd against torch import issue\n\n* fix sytax error\n\n* fix import\n",
        "label": "",
        "answer": "no",
        "change": [
            "def ninja_installed():",
            "def nvcc_version():",
            "import torch.utils.cpp_extension",
            "cuda_home = torch.utils.cpp_extension.CUDA_HOME",
            "+    if cuda_home is None:",
            "+        return f\"{RED} [FAIL] cannot find CUDA_HOME via torch.utils.cpp_extension.CUDA_HOME={torch.utils.cpp_extension.CUDA_HOME} {END}\"",
            "try:",
            "output = subprocess.check_output([cuda_home + \"/bin/nvcc\",",
            "\"-V\"],"
        ]
    },
    {
        "number": 4538,
        "comments": "",
        "commit_message": "Fix beta log likelihood\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Beta(Distribution):",
            "def log_probability(self, action):",
            "action = (action - self.min_value) / (self.max_value - self.min_value)",
            "action = tf.minimum(x=action, y=(1.0 - util.epsilon))",
            "-        return (self.alpha - 1.0) * tf.log(action) + (self.beta - 1.0) * tf.log1p(-action) - self.log_norm",
            "+        return (self.alpha - 1.0) * tf.log(action + util.epsilon) +\\",
            "+               (self.beta - 1.0) * tf.log1p(-action) - self.log_norm",
            "",
            "def kl_divergence(self, other):",
            "assert isinstance(other, Beta)"
        ]
    },
    {
        "number": 4540,
        "comments": "",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "from allennlp.modules.attention.dot_product_attention import DotProductAttention",
            "",
            "",
            "class TestDotProductAttention(AllenNlpTestCase):",
            "-",
            "def test_can_init_dot(self):",
            "legacy_attention = Attention.from_params(Params({\"type\": \"dot_product\"}))",
            "isinstance(legacy_attention, DotProductAttention)",
            "",
            "def test_dot_product_similarity(self):",
            "linear = DotProductAttention(normalize=False)",
            "-        output = linear(torch.FloatTensor([[0, 0, 0], [1, 1, 1]]),",
            "-                        torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]))",
            "+        output = linear(",
            "+            torch.FloatTensor([[0, 0, 0], [1, 1, 1]]),",
            "+            torch.FloatTensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]),",
            "+        )",
            "",
            "-        assert_almost_equal(output.numpy(),",
            "-                            numpy.array([[0.0, 0.0], [24.0, 33.0]]), decimal=2)",
            "+        assert_almost_equal(output.numpy(), numpy.array([[0.0, 0.0], [24.0, 33.0]]), decimal=2)"
        ]
    },
    {
        "number": 4541,
        "comments": "",
        "commit_message": "Fix typo in test\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TokenClassificationIntegrationTest(test_combinations.TestCase):",
            "keras.layers.Conv1D(4, 5, padding='same', activation='relu'),",
            "keras.layers.Conv1D(8, 5, padding='same'),",
            "keras.layers.BatchNormalization(),",
            "-        keras.layers.Conv2D(3, 5, padding='same', activation='softmax'),",
            "+        keras.layers.Conv1D(3, 5, padding='same', activation='softmax'),",
            "]",
            "model = test_utils.get_model_from_layers(",
            "layers, input_shape=(None,))"
        ]
    },
    {
        "number": 4545,
        "comments": "",
        "commit_message": "fix model(Remove unwanted relu layer.) (#1644)\n\nA relu layer was inserted just before the cross entropy error was calculated, and it seemed that the loss was not calculated correctly, so I removed the relu layer.\nAccordingly, the structure of the NeuralNetwork class in buildmodel_tutorial.py was also changed.\n\nFYI:\nAfter fixing the model, we tried the task in the environment at hand, and the accuracy went from about 52% to about 71%.\n\nIf you have time, I hope you will take a look.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NeuralNetwork(nn.Module):",
            "nn.Linear(512, 512),",
            "nn.ReLU(),",
            "nn.Linear(512, 10),",
            "-            nn.ReLU()",
            ")",
            "",
            "def forward(self, x):"
        ]
    },
    {
        "number": 4555,
        "comments": "",
        "commit_message": "Fix issue #452 for PyTorch 0.4.0\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class roibatchLoader(data.Dataset):",
            "# for ratio cross 1, we make it to be 1.",
            "target_ratio = 1",
            "",
            "-        self.ratio_list_batch[left_idx:(right_idx+1)] = target_ratio",
            "+        self.ratio_list_batch[left_idx:(right_idx+1)] = torch.tensor(target_ratio.astype(np.float64)) # trainset ratio list ,each batch is same number",
            "",
            "",
            "def __getitem__(self, index):"
        ]
    },
    {
        "number": 4559,
        "comments": "",
        "commit_message": "Fixed scale parameter generation (#1400)\n\n* Fixed scale parameter generation\n\n* Minor fix\n\n* Added test for Random Affine 3d transformation\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Flake issue fixed\n\n* Refactored random affine 3d test into TestRandomAffine3D class\n\n* Newline addition\n\n* Addressing PR comments\n\n* Minor fixes\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def random_affine_generator3d(",
            "dim=1,",
            ")",
            "else:",
            "-        scale = torch.ones(batch_size, device=device, dtype=dtype).repeat(1, 3)",
            "+        scale = torch.ones(batch_size, device=device, dtype=dtype).reshape(batch_size, 1).repeat(1, 3)",
            "",
            "if translate is not None:",
            "if translate.shape != torch.Size([3]):"
        ]
    },
    {
        "number": 4561,
        "comments": "",
        "commit_message": "Fix #30\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CapsNet(object):",
            "assert self.masked_v.get_shape() == [cfg.batch_size, 1, 16, 1]",
            "# Method 2. masking with true label, default mode",
            "else:",
            "-                self.masked_v = tf.matmul(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)), transpose_a=True)",
            "+                # self.masked_v = tf.matmul(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)), transpose_a=True)",
            "+                self.masked_v = tf.multply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)))",
            "self.v_length = tf.sqrt(tf.reduce_sum(tf.square(self.caps2), axis=2, keep_dims=True) + epsilon)",
            "",
            "# 2. Reconstructe the MNIST images with 3 FC layers"
        ]
    },
    {
        "number": 4563,
        "comments": "",
        "commit_message": "fix bugs shift_rgb (#1861)\n\n* fix bugs shift_rgb\n\n* add tests for TestRGBShift and TestRandomRGBShift\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def shift_rgb(image: torch.Tensor, r_shift: torch.Tensor, g_shift: torch.Tensor,",
            "",
            "shifts = [r_shift, g_shift, b_shift]",
            "",
            "-    shifted = (image + torch.Tensor(shifts).view(1, 3, 1, 1).to(image)).clamp_(min=0, max=1)",
            "+    shifted = (image + torch.stack(shifts).view(-1, 3, 1, 1).to(image)).clamp_(min=0, max=1)",
            "",
            "return shifted"
        ]
    },
    {
        "number": 4564,
        "comments": "",
        "commit_message": "Adding normalization bias verification (#4990)\n\n* adding batchnorm verification\n\n* Adding trainer callback\n\n* updating changelog\n\n* renaming class\n\n* detailed message for sanity check\n\n* run sanity checks by default\n\n* fix normalization bias issue in image embeddings\n\n* update docstring\n\n* fix test\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TestImageFeatureEmbeddings(AllenNlpTestCase):",
            "super().__init__()",
            "",
            "self.image_embeddings = torch.nn.Linear(feature_size, embedding_size)",
            "-                self.image_location_embeddings = torch.nn.Linear(4, embedding_size)",
            "+                self.image_location_embeddings = torch.nn.Linear(4, embedding_size, bias=False)",
            "self.layer_norm = torch.nn.LayerNorm(embedding_size, eps=1e-12)",
            "self.dropout = torch.nn.Dropout(dropout)"
        ]
    },
    {
        "number": 4565,
        "comments": "",
        "commit_message": "fix bug in last commit\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class FastRCNNHead(object):",
            "",
            "@memoized",
            "def decoded_output_boxes_class_agnostic(self):",
            "+        \"\"\" Returns: Nx4 \"\"\"",
            "assert self._bbox_class_agnostic",
            "box_logits = tf.reshape(self.box_logits, [-1, 4])",
            "decoded = decode_bbox_target("
        ]
    },
    {
        "number": 4567,
        "comments": "",
        "commit_message": "Fix a few last paths for the new repo org (#8666)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def make_support(question, source=\"wiki40b\", method=\"dense\", n_results=10):",
            "return question_doc, support_list",
            "",
            "",
            "-@st.cache(hash_funcs={torch.Tensor: (lambda _: None), transformers.tokenization_bart.BartTokenizer: (lambda _: None)})",
            "+@st.cache(",
            "+    hash_funcs={",
            "+        torch.Tensor: (lambda _: None),",
            "+        transformers.models.bart.tokenization_bart.BartTokenizer: (lambda _: None),",
            "+    }",
            "+)",
            "def answer_question(",
            "question_doc, s2s_model, s2s_tokenizer, min_len=64, max_len=256, sampling=False, n_beams=2, top_p=0.95, temp=0.8",
            "):"
        ]
    },
    {
        "number": 4568,
        "comments": "",
        "commit_message": "small fix related to entropy summary\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DistributionModel(MemoryModel):",
            "collapsed_size = util.prod(util.shape(entropy)[1:])",
            "entropy = tf.reshape(tensor=entropy, shape=(-1, collapsed_size))",
            "entropies.append(entropy)",
            "-",
            "entropy_per_instance = tf.reduce_mean(input_tensor=tf.concat(values=entropies, axis=1), axis=1)",
            "entropy = tf.reduce_mean(input_tensor=entropy_per_instance, axis=0)",
            "-            if 'entropy' in self.summary_labels:",
            "-                tf.contrib.summary.scalar(name='entropy', tensor=entropy)",
            "+",
            "+        if 'entropy' in self.summary_labels:",
            "+            tf.contrib.summary.scalar(name='entropy', tensor=entropy)",
            "+        if self.entropy_regularization is not None and self.entropy_regularization > 0.0:",
            "losses['entropy'] = -self.entropy_regularization * entropy",
            "",
            "return losses"
        ]
    },
    {
        "number": 4569,
        "comments": "",
        "commit_message": "[torch] remove deprecated uint8 in favor of bool (#21384)\n\n* uint8 -> bool\n\n* fix copies\n\n* style\n\n* update test modeling commen when checking attention buffers\n\n* style\n\n* use logical not on random mask instead of subtraction with 1\n\n* remove torch uint8\n\n* quality\n\n* remove modified modeling utils\n\n* Update based on review\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n\n---------\n\nCo-authored-by: sgugger <sylvain.gugger@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _compute_global_attention_mask(input_ids, sep_token_id, before_sep_token=Tru",
            "# bool attention mask with True in locations of global attention",
            "attention_mask = torch.arange(input_ids.shape[1], device=input_ids.device)",
            "if before_sep_token is True:",
            "-        attention_mask = (attention_mask.expand_as(input_ids) < question_end_index).to(torch.uint8)",
            "+        attention_mask = (attention_mask.expand_as(input_ids) < question_end_index).to(torch.bool)",
            "else:",
            "# last token is separation token and should not be counted and in the middle are two separation tokens",
            "-        attention_mask = (attention_mask.expand_as(input_ids) > (question_end_index + 1)).to(torch.uint8) * (",
            "+        attention_mask = (attention_mask.expand_as(input_ids) > (question_end_index + 1)).to(torch.bool) * (",
            "attention_mask.expand_as(input_ids) < input_ids.shape[-1]",
            "-        ).to(torch.uint8)",
            "+        ).to(torch.bool)",
            "",
            "return attention_mask"
        ]
    },
    {
        "number": 4573,
        "comments": "",
        "commit_message": "New YOLOv5 Classification Models (#8956)\n\n* Update\n\n* Logger step fix: Increment step with epochs (#8654)\n\n* enhance\n\n* revert\n\n* allow training from scratch\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update --img argument from train.py \n\nsingle line\n\n* fix image size from 640 to 128\n\n* suport custom dataloader and augmentation\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* format\n\n* Update dataloaders.py\n\n* Single line return, single line comment, remove unused argument\n\n* address PR comments\n\n* fix spelling\n\n* don't augment eval set\n\n* use fstring\n\n* update augmentations.py\n\n* new maning convention for transforms\n\n* reverse if statement, inline ops\n\n* reverse if statement, inline ops\n\n* updates\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update dataloaders\n\n* Remove additional if statement\n\n* Remove is_train as redundant\n\n* Cleanup\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Cleanup2\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update classifier.py\n\n* Update augmentations.py\n\n* fix: imshow clip warning\n\n* update\n\n* Revert ToTensorV2 removal\n\n* Update classifier.py\n\n* Update normalize values, revert uint8\n\n* normalize image using cv2\n\n* remove dedundant comment\n\n* Update classifier.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* replace print with logger\n\n* commit steps\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Allow logging models from GenericLogger (#8676)\n\n* enhance\n\n* revert\n\n* allow training from scratch\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update --img argument from train.py \n\nsingle line\n\n* fix image size from 640 to 128\n\n* suport custom dataloader and augmentation\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* format\n\n* Update dataloaders.py\n\n* Single line return, single line comment, remove unused argument\n\n* address PR comments\n\n* fix spelling\n\n* don't augment eval set\n\n* use fstring\n\n* update augmentations.py\n\n* new maning convention for transforms\n\n* reverse if statement, inline ops\n\n* reverse if statement, inline ops\n\n* updates\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update dataloaders\n\n* Remove additional if statement\n\n* Remove is_train as redundant\n\n* Cleanup\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Cleanup2\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update classifier.py\n\n* Update augmentations.py\n\n* fix: imshow clip warning\n\n* update\n\n* Revert ToTensorV2 removal\n\n* Update classifier.py\n\n* Update normalize values, revert uint8\n\n* normalize image using cv2\n\n* remove dedundant comment\n\n* Update classifier.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* replace print with logger\n\n* commit steps\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* support final model logging\n\n* update\n\n* update\n\n* update\n\n* update\n\n* remove curses\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update classifier.py\n\n* Update __init__.py\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Glenn Jocher <glenn.jocher@ultralytics.com>\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update dataset download\n\n* Update dataset download\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Pass imgsz to classify_transforms()\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Cos scheduler\n\n* Cos scheduler\n\n* Remove unused args\n\n* Update\n\n* Add seed\n\n* Add seed\n\n* Update\n\n* Update\n\n* Add run(), main()\n\n* Merge master\n\n* Merge master\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Create YOLOv5 BaseModel class (#8829)\n\n* Create BaseModel\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\n* Hub load device fix\n\n* Update\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n\n* Add experiment\n\n* Merge master\n\n* Attach names\n\n* weight decay = 1e-4\n\n* weight decay = 5e-5\n\n* update smart_optimizer console printout\n\n* fashion-mnist fix\n\n* Merge master\n\n* Update Table\n\n* Update Table\n\n* Remove destroy process group\n\n* add kwargs to forward()\n\n* fuse fix for resnet50\n\n* nc, names fix for resnet50\n\n* nc, names fix for resnet50\n\n* ONNX CPU inference fix\n\n* revert\n\n* cuda\n\n* if augment or visualize\n\n* if augment or visualize\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* New smart_inference_mode()\n\n* Update README\n\n* Refactor into /classify dir\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* reset defaults\n\n* reset defaults\n\n* fix gpu predict\n\n* warmup\n\n* ema half fix\n\n* spacing\n\n* remove data\n\n* remove cache\n\n* remove denormalize\n\n* save run settings\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* verbose false on initial plots\n\n* new save_yaml() function\n\n* Update ci-testing.yml\n\n* Path(data) CI fix\n\n* Separate classification CI\n\n* fix val\n\n* fix val\n\n* fix val\n\n* smartCrossEntropyLoss\n\n* skip validation on hub load\n\n* autodownload with working dir root\n\n* str(data)\n\n* Dataset usage example\n\n* im_show normalize\n\n* im_show normalize\n\n* add imagenet simple names to multibackend\n\n* Add validation speeds\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* 24-space names\n\n* Update bash scripts\n\n* Update permissions\n\n* Add bash script arguments\n\n* remove verbose\n\n* TRT data fix\n\n* names generator fix\n\n* optimize if names\n\n* update usage\n\n* Add local loading\n\n* Verbose=False\n\n* update names printing\n\n* Add Usage examples\n\n* Add Usage examples\n\n* Add Usage examples\n\n* Add Usage examples\n\n* named_children\n\n* reshape_classifier_outputs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* update\n\n* update\n\n* fix CI\n\n* fix incorrect class substitution\n\n* fix incorrect class substitution\n\n* remove denormalize\n\n* ravel fix\n\n* cleanup\n\n* update opt file printing\n\n* update opt file printing\n\n* update defaults\n\n* add opt to checkpoint\n\n* Add warning\n\n* Add comment\n\n* plot half bug fix\n\n* Use NotImplementedError\n\n* fix export shape report\n\n* Fix TRT load\n\n* cleanup CI\n\n* profile comment\n\n* CI fix\n\n* Add cls models\n\n* avoid inplace error\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fix usage examples\n\n* Update README\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\n* Update README\n\nCo-authored-by: Ayush Chaurasia <ayush.chaurarsia@gmail.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def attempt_load(weights, device=None, inplace=True, fuse=True):",
            "elif t is nn.Upsample and not hasattr(m, 'recompute_scale_factor'):",
            "m.recompute_scale_factor = None  # torch 1.11.0 compatibility",
            "",
            "+    # Return model",
            "if len(model) == 1:",
            "-        return model[-1]  # return model",
            "+        return model[-1]",
            "+",
            "+    # Return detection ensemble",
            "print(f'Ensemble created with {weights}\\n')",
            "for k in 'names', 'nc', 'yaml':",
            "setattr(model, k, getattr(model[0], k))",
            "model.stride = model[torch.argmax(torch.tensor([m.stride.max() for m in model])).int()].stride  # max stride",
            "assert all(model[0].nc == m.nc for m in model), f'Models have different class counts: {[m.nc for m in model]}'",
            "-    return model  # return ensemble",
            "+    return model"
        ]
    },
    {
        "number": 4575,
        "comments": "",
        "commit_message": "Fix index_lookup_test in tf v1 setting.\n\nPiperOrigin-RevId: 394765449\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class IndexLookupOutputTest(keras_parameterized.TestCase,",
            ")  # pyformat: disable",
            "def test_int_output(self, shape, input_array, expected_output):",
            "vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]",
            "+    if callable(input_array):",
            "+      input_array = input_array()",
            "",
            "layer = index_lookup.IndexLookup(",
            "max_tokens=None,"
        ]
    },
    {
        "number": 4580,
        "comments": "",
        "commit_message": "Fix for creating tensor from tensors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def perform_analysis_torch(preds, indices, noise_eps=0.1, delta=1e-5, moments=8,",
            "data_ind_eps_list = (data_ind_log_mgf - math.log(delta)) / l_list",
            "",
            "return min(eps_list_nm), min(data_ind_eps_list)",
            "+",
            "+",
            "+",
            "+"
        ]
    },
    {
        "number": 4583,
        "comments": "",
        "commit_message": "fix a potential bug in GroupSampler (#955)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GroupSampler(Sampler):",
            "range(len(indices) // self.samples_per_gpu))",
            "]",
            "indices = np.concatenate(indices)",
            "-        indices = torch.from_numpy(indices).long()",
            "+        indices = indices.astype(np.int64).tolist()",
            "assert len(indices) == self.num_samples",
            "return iter(indices)"
        ]
    },
    {
        "number": 4584,
        "comments": "",
        "commit_message": "Adds CLIP to models exportable with ONNX (#18515)\n\n* onnx config for clip\n\n* default opset as 14\n\n* changes from the original repo\n\n* input values order fix\n\n* outputs fix\n\n* remove unused import\n\n* ran make fix-copies\n\n* black format\n\n* review comments: forward ref, import fix, model change revert, .to cleanup\n\n* make style\n\n* formatting fixes\n\n* revert groupvit\n\n* comment for cast to int32\n\n* comment fix\n\n* make .T as .t() for onnx conversion\n\n* ran make fix-copies\n\n* remove unneeded comment\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* fix copies\n\n* remove comment\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CLIPModel(CLIPPreTrainedModel):",
            "# cosine similarity as logits",
            "logit_scale = self.logit_scale.exp()",
            "logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale",
            "-        logits_per_image = logits_per_text.T",
            "+        logits_per_image = logits_per_text.t()",
            "",
            "loss = None",
            "if return_loss:"
        ]
    },
    {
        "number": 4585,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class BooleanAccuracy(Metric):",
            "",
            "# We want to skip predictions that are completely masked;",
            "# so we'll keep predictions that aren't.",
            "-            keep = mask.view(batch_size, -1).max(dim=1)[0].float()",
            "+            keep = mask.view(batch_size, -1).max(dim=1)[0]",
            "else:",
            "-            keep = torch.ones(batch_size, device=predictions.device).float()",
            "+            keep = torch.ones(batch_size, device=predictions.device).bool()",
            "",
            "predictions = predictions.view(batch_size, -1)",
            "gold_labels = gold_labels.view(batch_size, -1)"
        ]
    },
    {
        "number": 4586,
        "comments": "",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario \u0160a\u0161ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class StyleChangeDetection(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(train_dir):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('style_change_detection', data_dir=...)` that includes {}. Manual download instructions: {}\".format(",
            "-                    train_dir, train_dir, self.manual_download_instructions",
            "-                )",
            "+                f\"{train_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('style_change_detection', data_dir=...)` that includes {train_dir}. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "",
            "return ["
        ]
    },
    {
        "number": 4588,
        "comments": "",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def cross_product_matrix(x: torch.Tensor) -> torch.Tensor:",
            "",
            "# construct the matrix, reshape to 3x3 and return",
            "zeros = torch.zeros_like(x0)",
            "-    cross_product_matrix_flat = torch.stack([",
            "-        zeros, -x2, x1,",
            "-        x2, zeros, -x0,",
            "-        -x1, x0, zeros], dim=-1)",
            "+    cross_product_matrix_flat = torch.stack([zeros, -x2, x1, x2, zeros, -x0, -x1, x0, zeros], dim=-1)",
            "return cross_product_matrix_flat.view(-1, 3, 3)"
        ]
    },
    {
        "number": 4589,
        "comments": "",
        "commit_message": "Add HF_ prefix to env var MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES (#2409)\n\n* add HF_ prefix to env var MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\n\n* update tests\n\n* style\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_load_from_disk_with_default_in_memory(",
            "current_dataset_size = 512  # arrow file size = 512, in-memory dataset size = 148",
            "if max_in_memory_dataset_size == \"default\":",
            "# default = 250 * 2 ** 20",
            "-        max_in_memory_dataset_size = datasets.config.MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES",
            "+        max_in_memory_dataset_size = datasets.config.HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES",
            "else:",
            "-        monkeypatch.setattr(datasets.config, \"MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", max_in_memory_dataset_size)",
            "+        monkeypatch.setattr(datasets.config, \"HF_MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES\", max_in_memory_dataset_size)",
            "if max_in_memory_dataset_size:",
            "expected_in_memory = current_dataset_size < max_in_memory_dataset_size",
            "else:"
        ]
    },
    {
        "number": 4591,
        "comments": "",
        "commit_message": "fixed the issues #372 (#379)\n\n\u4fee\u590d\u4e86\u4e00\u4e9b\u53c2\u6570\u4f20\u9012\u9020\u6210\u7684\u95ee\u9898\uff0c\u628a\u8fc7\u65f6\u7684torch.nn.functional.tanh()\u6539\u6210\u4e86torch.tanh()\n",
        "label": "",
        "answer": "no",
        "change": [
            "class STL(nn.Module):",
            "def forward(self, inputs):",
            "N = inputs.size(0)",
            "query = inputs.unsqueeze(1)  # [N, 1, E//2]",
            "-        keys = tFunctional.tanh(self.embed).unsqueeze(0).expand(N, -1, -1)  # [N, token_num, E // num_heads]",
            "+        keys = torch.tanh(self.embed).unsqueeze(0).expand(N, -1, -1)  # [N, token_num, E // num_heads]",
            "style_embed = self.attention(query, keys)",
            "",
            "return style_embed"
        ]
    },
    {
        "number": 4593,
        "comments": "",
        "commit_message": "Fix bug in half precision for DPMSolverMultistepScheduler (#1349)\n\n* cast to float for quantile method\n\n* add fp16 test for DPMSolverMultistepScheduler fix\n\n* formatting update\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DPMSolverMultistepScheduler(SchedulerMixin, ConfigMixin):",
            "self.config.sample_max_value * torch.ones_like(dynamic_max_val).to(dynamic_max_val.device),",
            ")[(...,) + (None,) * (x0_pred.ndim - 1)]",
            "x0_pred = torch.clamp(x0_pred, -dynamic_max_val, dynamic_max_val) / dynamic_max_val",
            "+                x0_pred = x0_pred.type(orig_dtype)",
            "return x0_pred",
            "# DPM-Solver needs to solve an integral of the noise prediction model.",
            "elif self.config.algorithm_type == \"dpmsolver\":"
        ]
    },
    {
        "number": 4596,
        "comments": "",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def nms(boxes: torch.Tensor, scores: torch.Tensor, iou_threshold: float) -> torc",
            "xx2 = torch.min(x2[i], x2[order[1:]])",
            "yy2 = torch.min(y2[i], y2[order[1:]])",
            "",
            "-        w = torch.clamp(xx2 - xx1, min=0.)",
            "-        h = torch.clamp(yy2 - yy1, min=0.)",
            "+        w = torch.clamp(xx2 - xx1, min=0.0)",
            "+        h = torch.clamp(yy2 - yy1, min=0.0)",
            "inter = w * h",
            "ovr = inter / (areas[i] + areas[order[1:]] - inter)"
        ]
    },
    {
        "number": 4600,
        "comments": "",
        "commit_message": "bug fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def create_attn_mask(N, T, g=0.05):",
            "M[n, t] = val",
            "e_x = np.exp(M - np.max(M))",
            "M = e_x / e_x.sum(axis=0) # only difference",
            "-    M = Variable(torch.FloatTensor(M).t()).cuda()",
            "+    M = torch.FloatTensor(M).t().cuda()",
            "M = torch.stack([M]*32)",
            "return M"
        ]
    },
    {
        "number": 4609,
        "comments": "",
        "commit_message": "Fixed failing ivy tests for test_vmap by using handle_test and adding fn_tree (#11356)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _fn3(x, y):",
            "",
            "",
            "# vmap",
            "-@given(",
            "+@handle_test(",
            "+    fn_tree=\"functional.ivy.vmap\",",
            "func=st.sampled_from([_fn1, _fn2, _fn3]),",
            "dtype_and_arrays_and_axes=helpers.arrays_and_axes(",
            "allow_none=False,"
        ]
    },
    {
        "number": 4611,
        "comments": "",
        "commit_message": "Fix test scripts and refine some code\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def transfer_lm(ch_rnnlm, th_rnnlm):",
            "",
            "def test_lm():",
            "n_vocab = 3",
            "+    n_layers = 2",
            "n_units = 2",
            "batchsize = 5",
            "-    rnnlm_ch = lm_chainer.ClassifierWithState(lm_chainer.RNNLM(n_vocab, n_units))",
            "-    rnnlm_th = lm_pytorch.ClassifierWithState(lm_pytorch.RNNLM(n_vocab, n_units))",
            "+    rnnlm_ch = lm_chainer.ClassifierWithState(lm_chainer.RNNLM(n_vocab, n_layers, n_units))",
            "+    rnnlm_th = lm_pytorch.ClassifierWithState(lm_pytorch.RNNLM(n_vocab, n_layers, n_units))",
            "transfer_lm(rnnlm_ch.predictor, rnnlm_th.predictor)",
            "import numpy",
            "# TODO(karita) implement weight transfer"
        ]
    },
    {
        "number": 4613,
        "comments": "",
        "commit_message": "fixed dropbox update\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class FrozenCLIPEmbedderWithCustomWords(torch.nn.Module):",
            "image_embs_name = None",
            "if image_embs_name is not None and self.image_embs_name != image_embs_name:",
            "self.image_embs_name = image_embs_name",
            "-            self.image_embs = torch.load(aesthetic_embeddings[self.image_embs_name], map_location=device)",
            "+            self.image_embs = torch.load(shared.aesthetic_embeddings[self.image_embs_name], map_location=device)",
            "self.image_embs /= self.image_embs.norm(dim=-1, keepdim=True)",
            "self.image_embs.requires_grad_(False)"
        ]
    },
    {
        "number": 4614,
        "comments": "",
        "commit_message": "[Feat] improve testing framework (#560)\n\n* Parametrize device and dtype fixtures using CLI options\n\n* improve tests parametrise n MakeFile\n\n* add coverage tot tests and update ci config file\n\n* test pep8 using pytest-flak8\n\n* add pytest-mypy, update failing tests and remove verify script\n\n* update build-docs command in MakeFile\n\n* fix performance test issue with non cuda device\n\n* add pydocstyle, adapt normalisation module to pydocsyle\n\n* fix augmentation  docs rendering\n\nCo-authored-by: Aiden Nibali <dismaldenizen@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestCenterCrop:",
            "def test_jit_trace(self, device):",
            "@torch.jit.script",
            "def op_script(input: torch.Tensor,",
            "-                      size: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:",
            "+                      size: Tuple[int, int]) -> torch.Tensor:",
            "return kornia.center_crop(input, size)",
            "# 1. Trace op",
            "batch_size, channels, height, width = 1, 2, 5, 4"
        ]
    },
    {
        "number": 4615,
        "comments": "",
        "commit_message": "codemod for 0.4 (#331) (#336)\n\n* codemod for 0.4\n\n* better IN key removal\n\n* fix ac\n\n* update to use some device\n\n* rnn flatten_param\n",
        "label": "",
        "answer": "no",
        "change": [
            "class UpsampleConvLayer(torch.nn.Module):",
            "super(UpsampleConvLayer, self).__init__()",
            "self.upsample = upsample",
            "if upsample:",
            "-            self.upsample_layer = torch.nn.UpsamplingNearest2d(scale_factor=upsample)",
            "+            self.upsample_layer = torch.nn.Upsample(mode='nearest', scale_factor=upsample)",
            "reflection_padding = kernel_size // 2",
            "self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)",
            "self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)"
        ]
    },
    {
        "number": 4617,
        "comments": "",
        "commit_message": "Layer API Refactoring (#675)\n\n* Layer API Refactoring\n\n* private method decorator added\n\n* Input Layers added\n\n* Lazy Import on ROI pooling\n\n* @zsdonghao change proposal added\n\n* UTF-8 header added\n\n* gitignore updated\n\n* Python2 error correction\n\n* Changelog Updated\n\n* Python Headers Uniformized + Codacy Error Fix + LSTMStateTuple missing import\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PReluLayer(Layer):",
            "except Exception:  # TF 0.12",
            "self.outputs = tf.nn.relu(self.inputs) + tf.mul(alphas, (self.inputs - tf.abs(self.inputs))) * 0.5",
            "",
            "-        self.all_layers.append(self.outputs)",
            "-        self.all_params.extend([alphas])",
            "+        self._add_layers(self.outputs)",
            "+        self._add_params([alphas])"
        ]
    },
    {
        "number": 4620,
        "comments": "",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def scope_vars(scope, trainable_only=False):",
            "vars: [tf.Variable]",
            "list of variables in `scope`.",
            "\"\"\"",
            "-    return tf.get_collection(",
            "-        tf.GraphKeys.TRAINABLE_VARIABLES",
            "-        if trainable_only else tf.GraphKeys.VARIABLES,",
            "+    return tf1.get_collection(",
            "+        tf1.GraphKeys.TRAINABLE_VARIABLES",
            "+        if trainable_only else tf1.GraphKeys.VARIABLES,",
            "scope=scope if isinstance(scope, str) else scope.name)"
        ]
    },
    {
        "number": 4622,
        "comments": "",
        "commit_message": "Fix keras docstrings\n\nPiperOrigin-RevId: 424275818\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Optimizer(_BaseOptimizer):",
            ">>> opt = tf.keras.optimizers.experimental.SGD(learning_rate=1, clipvalue=1)",
            ">>> var1, var2 = tf.Variable(2.0), tf.Variable(2.0)",
            ">>> with tf.GradientTape() as tape:",
            "-    ... loss = 2 * var1 + 2 * var2",
            "+  ...   loss = 2 * var1 + 2 * var2",
            ">>> grads = tape.gradient(loss, [var1, var2])",
            ">>> print([grads[0].numpy(), grads[1].numpy()])",
            "-  [2.0., 2.0]",
            "+  [2.0, 2.0]",
            ">>> opt.apply_gradients(zip(grads, [var1, var2]))",
            ">>> # Without clipping, we should get [0, 0], but as gradients are clipped to",
            ">>> # have max value 1, we get [1.0, 1.0]."
        ]
    },
    {
        "number": 4623,
        "comments": "",
        "commit_message": "Fix layer choice on IT and deprecate \"choices\" and \"length\" (#2386)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Mutator(BaseMutator):",
            "\"\"\"",
            "if self._connect_all:",
            "return self._all_connect_tensor_reduction(mutable.reduction,",
            "-                                                      [op(*args, **kwargs) for op in mutable.choices]), \\",
            "-                torch.ones(mutable.length)",
            "+                                                      [op(*args, **kwargs) for op in mutable]), \\",
            "+                torch.ones(len(mutable))",
            "",
            "def _map_fn(op, args, kwargs):",
            "return op(*args, **kwargs)",
            "",
            "mask = self._get_decision(mutable)",
            "-        assert len(mask) == len(mutable.choices), \\",
            "-            \"Invalid mask, expected {} to be of length {}.\".format(mask, len(mutable.choices))",
            "-        out = self._select_with_mask(_map_fn, [(choice, args, kwargs) for choice in mutable.choices], mask)",
            "+        assert len(mask) == len(mutable), \\",
            "+            \"Invalid mask, expected {} to be of length {}.\".format(mask, len(mutable))",
            "+        out = self._select_with_mask(_map_fn, [(choice, args, kwargs) for choice in mutable], mask)",
            "return self._tensor_reduction(mutable.reduction, out), mask",
            "",
            "def on_forward_input_choice(self, mutable, tensor_list):"
        ]
    },
    {
        "number": 4624,
        "comments": "",
        "commit_message": "Fix MHA with bias=False (fixes #1527)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1528\n\nDifferential Revision: D19178445\n\nPulled By: myleott\n\nfbshipit-source-id: 2e08012205de825ded334222d29797f2c125f15e\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MultiheadAttention(nn.Module):",
            "nn.init.xavier_uniform_(self.q_proj.weight)",
            "",
            "nn.init.xavier_uniform_(self.out_proj.weight)",
            "-        nn.init.constant_(self.out_proj.bias, 0.)",
            "+        if self.out_proj.bias is not None:",
            "+            nn.init.constant_(self.out_proj.bias, 0.)",
            "if self.bias_k is not None:",
            "nn.init.xavier_normal_(self.bias_k)",
            "if self.bias_v is not None:"
        ]
    },
    {
        "number": 4625,
        "comments": "",
        "commit_message": "upgrade detection / some v2 compat fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def encode_bbox_target(boxes, anchors):",
            "",
            "# Note that here not all boxes are valid. Some may be zero",
            "txty = (xbyb - xaya) / waha",
            "-    twth = tf.log(wbhb / waha)  # may contain -inf for invalid boxes",
            "+    twth = tf.math.log(wbhb / waha)  # may contain -inf for invalid boxes",
            "encoded = tf.concat([txty, twth], axis=1)  # (-1x2x2)",
            "return tf.reshape(encoded, tf.shape(boxes))"
        ]
    },
    {
        "number": 4626,
        "comments": "",
        "commit_message": "Fix Leaky ReLU activation (#784)\n\n* Fix leaky ReLU activation\n\n- fix bug in activations.leaky_relu that is making it act like an ordinary ReLU\n\n* Add tests for leaky_relu\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def leaky_relu(x, alpha=0.1, name=\"LeakyReLU\"):",
            "\"\"\"",
            "",
            "with tf.name_scope(name) as scope:",
            "-        x = tf.nn.relu(x)",
            "m_x = tf.nn.relu(-x)",
            "+        x = tf.nn.relu(x)",
            "x -= alpha * m_x",
            "",
            "x.scope = scope"
        ]
    },
    {
        "number": 4631,
        "comments": "",
        "commit_message": "Fix DDIM on Windows not using int64 for timesteps (#819)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DDIMScheduler(SchedulerMixin, ConfigMixin):",
            "step_ratio = self.config.num_train_timesteps // self.num_inference_steps",
            "# creates integer timesteps by multiplying by ratio",
            "# casting to int to avoid issues when num_inference_step is power of 3",
            "-        timesteps = (np.arange(0, num_inference_steps) * step_ratio).round()[::-1].copy()",
            "+        timesteps = (np.arange(0, num_inference_steps) * step_ratio).round()[::-1].copy().astype(np.int64)",
            "self.timesteps = torch.from_numpy(timesteps).to(device)",
            "self.timesteps += offset"
        ]
    },
    {
        "number": 4640,
        "comments": "",
        "commit_message": "Copy tokenizer files in each of their repo (#10624)\n\n* Move tokenizer files in each repo\n\n* Fix mBART50 tests\n\n* Fix mBART tests\n\n* Fix Marian tests\n\n* Update templates\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MarianTokenizationTest(TokenizerTesterMixin, unittest.TestCase):",
            "vocab = [\"</s>\", \"<unk>\", \"\u2581This\", \"\u2581is\", \"\u2581a\", \"\u2581t\", \"est\", \"\\u0120\", \"<pad>\"]",
            "vocab_tokens = dict(zip(vocab, range(len(vocab))))",
            "save_dir = Path(self.tmpdirname)",
            "-        save_json(vocab_tokens, save_dir / vocab_files_names[\"vocab\"])",
            "-        save_json(mock_tokenizer_config, save_dir / vocab_files_names[\"tokenizer_config_file\"])",
            "-        if not (save_dir / vocab_files_names[\"source_spm\"]).exists():",
            "-            copyfile(SAMPLE_SP, save_dir / vocab_files_names[\"source_spm\"])",
            "-            copyfile(SAMPLE_SP, save_dir / vocab_files_names[\"target_spm\"])",
            "+        save_json(vocab_tokens, save_dir / VOCAB_FILES_NAMES[\"vocab\"])",
            "+        save_json(mock_tokenizer_config, save_dir / VOCAB_FILES_NAMES[\"tokenizer_config_file\"])",
            "+        if not (save_dir / VOCAB_FILES_NAMES[\"source_spm\"]).exists():",
            "+            copyfile(SAMPLE_SP, save_dir / VOCAB_FILES_NAMES[\"source_spm\"])",
            "+            copyfile(SAMPLE_SP, save_dir / VOCAB_FILES_NAMES[\"target_spm\"])",
            "",
            "tokenizer = MarianTokenizer.from_pretrained(self.tmpdirname)",
            "tokenizer.save_pretrained(self.tmpdirname)"
        ]
    },
    {
        "number": 4642,
        "comments": "",
        "commit_message": "tensorflow: fix bug in broadcast_variables (#416)\n\nWhen there's only one rank in total, broadcast_variables should still\nreturn a tf operation.\n\nSigned-off-by: Yulu Jia <yulu.jia@bytedance.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def broadcast_variables(variables, root_rank, scope=''):",
            "scope: the graph name scope",
            "\"\"\"",
            "if size() <= 1:",
            "-        return variables",
            "+        return tf.group(*variables)",
            "_assign = tf.assign if hasattr(tf, 'assign') else tf.compat.v1.assign",
            "return tf.group(*[_assign(var, broadcast(var, root_rank, scope))",
            "for var in variables])"
        ]
    },
    {
        "number": 4644,
        "comments": "",
        "commit_message": "Fix seed unittest, internal Tensorforce model improvements\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Queue(Memory):",
            "return lengths, successor_indices, mask",
            "",
            "lengths = tf.ones_like(input=indices, dtype=tf_util.get_dtype(type='int'))",
            "-        successor_indices = tf.expand_dims(input=indices, axis=1)",
            "+        successor_indices = tf.math.mod(x=tf.expand_dims(input=indices, axis=1), y=capacity)",
            "mask = tf.ones_like(input=successor_indices, dtype=tf_util.get_dtype(type='bool'))",
            "shape = tf.TensorShape(dims=((None, None)))"
        ]
    },
    {
        "number": 4646,
        "comments": "",
        "commit_message": "[Feature] Support efficientnet in mmdetection. (#7514)\n\n* Initial implementation\n\n* Add missing import\n\n* Add MemoryEfficientSwishImplementation. Add docstrings\n\n* Add efficientnet2mmdet tool\n\n* Add config folder\n\n* Flake8\n\n* Flake8\n\n* Flake8\n\n* Fix config\n\n* Requested changes\n\n* docformatter\n\n* Update train config from https://github.com/google/automl/blob/master/efficientdet\n\n* Run pre-commit\n\n* Fix schedule\n\n* Set by_epoch=False in scheduler\n\n* Train 80 epochs\n\n* Remove duplicated arg\n\n* Update README.md\n\n* efficient3 efficient0\n\n* efficientNet imports\n\n* efficientNet\n\n* config edit path for eff3 and dropout for eff0\n\n* efficientnet review2\n\n* fix model_converter location and drop path\n\n* fix model converter  and efficientnet import\n\n* register memoryefficietnswish\n\n* eff0, eff3\n\n* fix  flake8 yapf isort\n\n* same padding in tensorflow and edit drop path rate\n\n* fix init of utils\n\n* Align mmdet utils with mmcls\n\n* Align mmdet.models.utils with mmcls\n\n* Use mmcls efficientnet backbone\n\n* Update\n\n* Update\n\n* Update metafile\n\nCo-authored-by: David de la Iglesia Castro <daviddelaiglesiacastro@gmail.com>\nCo-authored-by: David de la Iglesia Castro <diglesia@gradiant.org>\nCo-authored-by: jiangyitong <jiangyitong1@sensetime.com>\nCo-authored-by: jiangyitong <jiangyitong1998@outlook.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class InvertedResidual(BaseModule):",
            "out = self.linear_conv(out)",
            "",
            "if self.with_res_shortcut:",
            "-                return x + out",
            "+                return x + self.drop_path(out)",
            "else:",
            "return out"
        ]
    },
    {
        "number": 4647,
        "comments": "",
        "commit_message": "Change to use F-string (#2531)\n\n* Change .format to f-string\n\n* Resolve comments\n\n* Fix missing change\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_nn_op_forward_called():",
            "torch.__version__ = '1.4.1'",
            "",
            "for m in ['Conv2d', 'ConvTranspose2d', 'MaxPool2d']:",
            "-        with patch('torch.nn.{}.forward'.format(m)) as nn_module_forward:",
            "+        with patch(f'torch.nn.{m}.forward') as nn_module_forward:",
            "# randn input",
            "x_empty = torch.randn(0, 3, 10, 10)",
            "wrapper = eval(m)(3, 2, 1)"
        ]
    },
    {
        "number": 4650,
        "comments": "",
        "commit_message": "Fix classification with only one class\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def compute_loss(predictions, targets, model):  # predictions, targets, model",
            "tobj[b, anchor, grid_j, grid_i] = (1.0 - model.gr) + model.gr * iou.detach().clamp(0).type(tobj.dtype)  # iou ratio",
            "",
            "# Classification",
            "-            t = torch.full_like(ps[:, 5:], cn, device=device)  # targets",
            "-            t[range(num_targets), tcls[layer_index]] = cp",
            "-            lcls += BCEcls(ps[:, 5:], t)  # BCE",
            "+            if ps.size(1) - 5 > 1:",
            "+                t = torch.full_like(ps[:, 5:], cn, device=device)  # targets",
            "+                t[range(num_targets), tcls[layer_index]] = cp",
            "+                lcls += BCEcls(ps[:, 5:], t)  # BCE",
            "",
            "lobj += BCEobj(layer_predictions[..., 4], tobj) * balance[layer_index]  # obj loss"
        ]
    },
    {
        "number": 4654,
        "comments": "",
        "commit_message": "fix policy gradients for mujoco domains (#589)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Categorical(object):",
            "class DiagGaussian(object):",
            "def __init__(self, flat):",
            "self.flat = flat",
            "-    mean, logstd = tf.split(1, 2, flat)",
            "+    mean, logstd = tf.split(flat, 2, axis=1)",
            "self.mean = mean",
            "self.logstd = logstd",
            "self.std = tf.exp(logstd)"
        ]
    },
    {
        "number": 4655,
        "comments": "",
        "commit_message": "small fix to torch.device to fix torch related tests (#2230)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "torch_scatter = None",
            "def dev(x: torch.Tensor, as_native: bool = False) -> Union[ivy.Device, torch.device]:",
            "dv = x.device",
            "if as_native:",
            "-        return torch.device(dv.replace(\"gpu\", \"cuda\"))",
            "+        return torch.device(dv.type.replace(\"gpu\", \"cuda\"))",
            "return as_ivy_dev(dv)"
        ]
    },
    {
        "number": 4661,
        "comments": "",
        "commit_message": "Add name for shuflle layer.\n\nfix a bug.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Shuffle(Layer):",
            "assert in_channel % self.group == 0",
            "temp = tf.reshape(inputs, [-1, h, w, in_channel // self.group, self.group])",
            "temp = tf.transpose(temp, [0, 1, 2, 4, 3])",
            "-        outputs = tf.reshape(temp, [-1, h, w, in_channel])",
            "+        outputs = tf.reshape(temp, [-1, h, w, in_channel],name=self.name)",
            "return outputs"
        ]
    },
    {
        "number": 4662,
        "comments": "",
        "commit_message": "tutorial fixes from #336. Wheels with cuda10.1.\n\nSummary:\nAdd a document to explain how to run the tutorials.\nFix API of TexturesVertex in fit_textured_mesh.\nPrepare cuda 10.1 wheels (not 10.2) for linux to be available on pypi - this matches what colab has.\nChange the tutorials to use these new wheels.\n\nReviewed By: gkioxari\n\nDifferential Revision: D23324479\n\nfbshipit-source-id: 60e92a3f46a2d878f811b7703638f8d1dae143d9\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "\"outputs\": [],",
            "\"source\": [",
            "\"!pip install torch torchvision\\n\",",
            "-    \"!pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\"",
            "+    \"import sys\\n\",",
            "+    \"if torch.__version__=='1.6.0+cu101' and sys.platform.startswith('linux'):\\n\",",
            "+    \"    !pip install pytorch3d\\n\",",
            "+    \"else:\\n\",",
            "+    \"    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\"",
            "]",
            "},",
            "{"
        ]
    },
    {
        "number": 4666,
        "comments": "",
        "commit_message": "re-organize predict/; fix TF incompatibile change of sparse_softmax_cross_entropy_loss\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "",
            "logits = FullyConnected('linear', l, out_dim=1000, nl=tf.identity)",
            "prob = tf.nn.softmax(logits, name='output')",
            "-        loss3 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)",
            "+        loss3 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)",
            "loss3 = tf.reduce_mean(loss3, name='loss3')",
            "",
            "cost = tf.add_n([loss3, 0.3 * loss2, 0.3 * loss1], name='weighted_cost')"
        ]
    },
    {
        "number": 4667,
        "comments": "",
        "commit_message": "Fix BCE loss issue (#1872)\n\n* Fix BCE loss issue\n\n* Remove import\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BCELossMasked(nn.Module):",
            "",
            "def __init__(self, pos_weight: float = None):",
            "super().__init__()",
            "-        self.pos_weight = torch.tensor([pos_weight])",
            "+        self.pos_weight = nn.Parameter(torch.tensor([pos_weight]), requires_grad=False)",
            "",
            "def forward(self, x, target, length):",
            "\"\"\""
        ]
    },
    {
        "number": 4672,
        "comments": "",
        "commit_message": "Fix beam search input transform\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AttentionDecoder(DecoderBase):",
            "initial_call=False,",
            "predictions=outputs)",
            "",
            "-    next_inputs = tf.concat([next_inputs, attention_context], 1)",
            "+    next_inputs = self.transform_inputs(next_inputs, outputs)",
            "",
            "return (outputs, cell_state, next_inputs, finished)"
        ]
    },
    {
        "number": 4673,
        "comments": "",
        "commit_message": "[Enhance] pytorch nightly fixes (#861)\n\n* Fixed Uniform bug for validation and added extra dimensions\n\n* Fixed geometry tests\n\n* Fixed beta distribution bug\n\n* Fixed linting\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestTransformBoxes:",
            "",
            "boxes = torch.tensor([[139.2640, 103.0150, 397.3120, 410.5225]], device=device, dtype=dtype)",
            "",
            "-        expected = torch.tensor([372.7360, 103.0150, 114.6880, 410.5225], device=device, dtype=dtype)",
            "+        expected = torch.tensor([[372.7360, 103.0150, 114.6880, 410.5225]], device=device, dtype=dtype)",
            "",
            "trans_mat = torch.tensor([[[-1., 0., 512.],",
            "[0., 1., 0.],"
        ]
    },
    {
        "number": 4674,
        "comments": "",
        "commit_message": "fix tests and warnings (#1834)\n\n* fix tests and warnings\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix tests and warnings\n\n* fix tests and warnings\n\n* replace `torch.testinig.assert_close` with `assert_close`\n\nCo-authored-by: Anton Shevtsov <aeshevtsov@avito.ru>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestConvDistanceTransform:",
            "output1 = kornia.contrib.distance_transform(sample1, kernel_size, h)",
            "assert_close(expected_output1, output1)",
            "",
            "-    def test_gradcheck(self, device, dtype):",
            "+    def test_gradcheck(self, device):",
            "B, C, H, W = 1, 1, 32, 32",
            "-        sample1 = torch.ones(B, C, H, W, device=device, dtype=dtype, requires_grad=True)",
            "+        sample1 = torch.ones(B, C, H, W, device=device, dtype=torch.float64, requires_grad=True)",
            "assert gradcheck(kornia.contrib.distance_transform, (sample1), raise_exception=True)",
            "",
            "def test_loss_grad(self, device, dtype):"
        ]
    },
    {
        "number": 4675,
        "comments": "",
        "commit_message": "Fix(core): fix memory leak issue and switch to subprocess backend (#216)\n\n* fix RAM leak error\n\n* fix another memory leak\n\n* update boxes.py\n\n* make flake8 happy\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def all_reduce(py_dict, op=\"sum\", group=None):",
            "flatten_tensor /= world_size",
            "",
            "split_tensors = [",
            "-        x.reshape(shape) for x, shape in zip(",
            "-            torch.split(flatten_tensor, tensor_numels), tensor_shapes",
            "-        )",
            "+        x.reshape(shape)",
            "+        for x, shape in zip(torch.split(flatten_tensor, tensor_numels), tensor_shapes)",
            "]",
            "return OrderedDict({k: v for k, v in zip(py_key, split_tensors)})"
        ]
    },
    {
        "number": 4676,
        "comments": "",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "",
        "answer": "no",
        "change": [
            "def sqrt(x):",
            "# is upper triangular) using some `flip` operators:",
            "#   flip(cholesky(flip(schur_complement)))",
            "try:",
            "-            top_left = torch.flip(torch.cholesky(torch.flip(schur_complement, (-2, -1))), (-2, -1))",
            "+            top_left = torch.flip(",
            "+                torch.linalg.cholesky(torch.flip(schur_complement, (-2, -1))), (-2, -1)",
            "+            )",
            "break",
            "except RuntimeError:",
            "B = B / 2"
        ]
    },
    {
        "number": 4677,
        "comments": "",
        "commit_message": "Changes and improvements to how we initialize transformer modules from pretrained models (#5200)\n\n* updates\n\n* rename 'load_state_dict' -> 'read_state_dict'\n\n* fix TransformerStack\n\n* more fixes\n\n* fix embeddings\n\n* fix toolkit tests\n\n* fix self attention\n\n* fix bimodal encoder tests\n\n* fix more tests\n\n* fix T5!\n\n* fixes\n\n* fix backbone\n\n* fix\n\n* fixes\n\n* fix\n\n* doc fixes\n\n* name changes\n\n* patch models branch temporarily\n\n* update CHANGELOG\n\n* change default dist loading strategy to 'MEM_EFFICIENT' for T5\n\n* fix distilbert test\n\n* always use memory efficient distributed loading strategy\n\n* Update .github/workflows/ci.yml\n\nCo-authored-by: Pete <petew@allenai.org>\n\nCo-authored-by: Akshita Bhagia <akshita23bhagia@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "import torch",
            "from allennlp.common import FromParams",
            "",
            "from allennlp.modules.transformer.transformer_module import TransformerModule",
            "+from allennlp.modules.transformer.layer_norm import LayerNorm",
            "",
            "",
            "class OutputLayer(TransformerModule, FromParams):",
            "",
            "-    _huggingface_mapping = {\"LayerNorm\": \"layer_norm\"}",
            "+    _pretrained_mapping = {\"LayerNorm\": \"layer_norm\"}",
            "",
            "def __init__(self, input_size: int, hidden_size: int, dropout: float):",
            "super().__init__()",
            "self.dense = torch.nn.Linear(input_size, hidden_size)",
            "-        self.layer_norm = torch.nn.LayerNorm(hidden_size, eps=1e-12)",
            "+        self.layer_norm = LayerNorm(hidden_size, eps=1e-12)",
            "self.dropout = torch.nn.Dropout(dropout)",
            "",
            "def forward(self, hidden_states, input_tensor):"
        ]
    },
    {
        "number": 4679,
        "comments": "",
        "commit_message": "Doc styler examples (#14953)\n\n* Fix bad examples\n\n* Add black formatting to style_doc\n\n* Use first nonempty line\n\n* Put it at the right place\n\n* Don't add spaces to empty lines\n\n* Better templates\n\n* Deal with triple quotes in docstrings\n\n* Result of style_doc\n\n* Enable mdx treatment and fix code examples in MDXs\n\n* Result of doc styler on doc source files\n\n* Last fixes\n\n* Break copy from\n",
        "label": "",
        "answer": "no",
        "change": [
            "class XLMForQuestionAnswering(XLMPreTrainedModel):",
            ">>> from transformers import XLMTokenizer, XLMForQuestionAnswering",
            ">>> import torch",
            "",
            "-        >>> tokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')",
            "-        >>> model = XLMForQuestionAnswering.from_pretrained('xlm-mlm-en-2048')",
            "+        >>> tokenizer = XLMTokenizer.from_pretrained(\"xlm-mlm-en-2048\")",
            "+        >>> model = XLMForQuestionAnswering.from_pretrained(\"xlm-mlm-en-2048\")",
            "",
            "-        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1",
            "+        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(",
            "+        ...     0",
            "+        >>> )  # Batch size 1",
            ">>> start_positions = torch.tensor([1])",
            ">>> end_positions = torch.tensor([3])"
        ]
    },
    {
        "number": 4680,
        "comments": "",
        "commit_message": "Add Homography Tracker API (#1389)\n\n* rebase commit\n\n* init\n\n* added test data for loftr and image registrator\n\n* real ransac tests\n\n* save\n\n* works\n\n* lint\n\n* \u00fcpdated example\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* docs\n\n* docs\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* stupid CI OOM\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* lint\n\n* fix dtype in tests\n\n* fix test dtype\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* no-grad for homotest\n\n* fix\n\n* fix\n\n* fix random seed for ransac test\n\n* some formatting\n\n* codespell\n\n* fix signature defaults\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add fixture with data\n\n* remove files\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix deepsource\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* deepsource\n\n* fix super\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* identation fix\n\n* fix positional\n\n* fix mypy\n\n* refactor some tests\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* remove doctest\n\n* fix ransac tests\n\n* fix similarity tests\n\n* fix sha path\n\n* fix loftr precision\n\n* xfail some ransac tests\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "+import pytest",
            "+import torch",
            "+",
            "+",
            "+@pytest.fixture",
            "+def data_loftr():",
            "+    url = 'https://github.com/kornia/data_test/blob/main/loftr_outdoor_and_homography_data.pt?raw=true'",
            "+    return torch.hub.load_state_dict_from_url(url)"
        ]
    },
    {
        "number": 4681,
        "comments": "",
        "commit_message": "Fixed some of the model definitions\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main(args):",
            "",
            "# Start running operations on the Graph.",
            "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)",
            "-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))",
            "+        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))",
            "sess.run(tf.initialize_all_variables())",
            "sess.run(tf.initialize_local_variables())",
            "summary_writer = tf.train.SummaryWriter(log_dir, sess.graph)"
        ]
    },
    {
        "number": 4682,
        "comments": "",
        "commit_message": "TF Seq2Seq int dtype fix (#13496)\n\nFixes problems with passing int64 input to TF Seq2Seq models.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFMarianMTModel(TFMarianPreTrainedModel, TFCausalLanguageModelingLoss):",
            "if inputs[\"labels\"] is not None:",
            "inputs[\"labels\"] = tf.where(",
            "inputs[\"labels\"] == self.config.pad_token_id,",
            "-                tf.fill(shape_list(inputs[\"labels\"]), -100),",
            "+                tf.fill(shape_list(inputs[\"labels\"]), tf.cast(-100, inputs[\"labels\"].dtype)),",
            "inputs[\"labels\"],",
            ")",
            "inputs[\"use_cache\"] = False"
        ]
    },
    {
        "number": 4685,
        "comments": "",
        "commit_message": "[tune] Fix bug in example where config hyperparameters were ignored (#2860)\n\nA fix to an example for tune (`python/ray/tune/examples/pbt_tune_cifar10_with_keras.py`) where the hyperparameters for the optimizer, learning rate and decay, were not being passed into the optimizer. \n\nThis means that the current optimizer uses default values for the hyperparameters no matter the config.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Cifar10Model(Trainable):",
            "x_train = self.train_data[0]",
            "model = self._build_model(x_train.shape[1:])",
            "",
            "-        opt = tf.keras.optimizers.Adadelta()",
            "+        opt = tf.keras.optimizers.Adadelta(",
            "+            lr=self.config[\"lr\"], decay=self.config[\"decay\"])",
            "model.compile(",
            "loss=\"categorical_crossentropy\",",
            "optimizer=opt,"
        ]
    },
    {
        "number": 4693,
        "comments": "",
        "commit_message": "update expected results of slow tests (#268)\n\n* update expected results of slow tests\n\n* relax sum and mean tests\n\n* Print shapes when reporting exception\n\n* formatting\n\n* fix sentence\n\n* relax test_stable_diffusion_fast_ddim for gpu fp16\n\n* relax flakey tests on GPU\n\n* added comment on large tolerences\n\n* black\n\n* format\n\n* set scheduler seed\n\n* added generator\n\n* use np.isclose\n\n* set num_inference_steps to 50\n\n* fix dep. warning\n\n* update expected_slice\n\n* preprocess if image\n\n* updated expected results\n\n* updated expected from CI\n\n* pass generator to VAE\n\n* undo change back to orig\n\n* use orignal\n\n* revert back the expected on cpu\n\n* revert back values for CPU\n\n* more undo\n\n* update result after using gen\n\n* update mean\n\n* set generator for mps\n\n* update expected on CI server\n\n* undo\n\n* use new seed every time\n\n* cpu manual seed\n\n* reduce num_inference_steps\n\n* style\n\n* use generator for randn\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class VQModelTests(ModelTesterMixin, unittest.TestCase):",
            "# fmt: off",
            "expected_output_slice = torch.tensor([-0.0153, -0.4044, -0.1880, -0.5161, -0.2418, -0.4072, -0.1612, -0.0633, -0.0143])",
            "# fmt: on",
            "-        self.assertTrue(torch.allclose(output_slice, expected_output_slice, rtol=1e-2))",
            "+        self.assertTrue(torch.allclose(output_slice, expected_output_slice, atol=1e-3))"
        ]
    },
    {
        "number": 4694,
        "comments": "",
        "commit_message": "fix cluster gcn test\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_cluster_gcn_conv():",
            "",
            "t = '(Tensor, SparseTensor, Size) -> Tensor'",
            "jit = torch.jit.script(conv.jittable(t))",
            "-    assert torch.allclose(jit(x, adj.t()), out)",
            "+    assert torch.allclose(jit(x, adj.t()), out, atol=1e-5)"
        ]
    },
    {
        "number": 4703,
        "comments": "",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def batch_flatten(x):",
            "shape = x.get_shape().as_list()[1:]",
            "if None not in shape:",
            "return tf.reshape(x, [-1, int(np.prod(shape))])",
            "-    return tf.reshape(x, tf.pack([tf.shape(x)[0], -1]))",
            "+    return tf.reshape(x, tf.stack([tf.shape(x)[0], -1]))",
            "",
            "",
            "def class_balanced_cross_entropy(pred, label, name='cross_entropy_loss'):"
        ]
    },
    {
        "number": 4706,
        "comments": "",
        "commit_message": "Revert \"Fix Conv1D block for FastSpeech\"\n\nThis reverts commit 5f5fa7d8d0c3e74bf9c5460eb6ca3754dec708eb.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MultiLayeredConv1d(torch.nn.Module):",
            "",
            "\"\"\"",
            "x = torch.relu(self.w_1(x.transpose(-1, 1))).transpose(-1, 1)",
            "-        x = torch.relu(self.w_2(x.transpose(-1, 1))).transpose(-1, 1)",
            "-        return self.dropout(x)",
            "+        return self.w_2(self.dropout(x).transpose(-1, 1)).transpose(-1, 1)"
        ]
    },
    {
        "number": 4707,
        "comments": "",
        "commit_message": "[SGD] Fix process group timeout units (#12477)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TorchTrainer:",
            "if backend == \"auto\":",
            "backend = \"nccl\" if use_gpu else \"gloo\"",
            "",
            "+        if backend == \"nccl\":",
            "+            timeout_s = NCCL_TIMEOUT_S",
            "+",
            "logger.debug(f\"Using {backend} as backend.\")",
            "self.backend = backend",
            "self.num_cpus_per_worker = num_cpus_per_worker"
        ]
    },
    {
        "number": 4708,
        "comments": "",
        "commit_message": "fixed problem with multi-state/action networks and scoping\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Bernoulli(Distribution):",
            "self.shape = shape",
            "action_size = util.prod(self.shape)",
            "",
            "-        with tf.name_scope(name=scope):",
            "-            self.logit = Linear(size=action_size, bias=log(probability), scope='logit')",
            "+        self.logit = Linear(size=action_size, bias=log(probability), scope='logit')",
            "",
            "super(Bernoulli, self).__init__(scope, summary_labels)"
        ]
    },
    {
        "number": 4710,
        "comments": "",
        "commit_message": "Reproducibility 3/3 (#1924)\n\n* make tests deterministic\n\n* run slow tests\n\n* prepare for testing\n\n* finish\n\n* refactor\n\n* add print statements\n\n* finish more\n\n* correct some test failures\n\n* more fixes\n\n* set up to correct tests\n\n* more corrections\n\n* up\n\n* fix more\n\n* more prints\n\n* add\n\n* up\n\n* up\n\n* up\n\n* uP\n\n* uP\n\n* more fixes\n\n* uP\n\n* up\n\n* up\n\n* up\n\n* up\n\n* fix more\n\n* up\n\n* up\n\n* clean tests\n\n* up\n\n* up\n\n* up\n\n* more fixes\n\n* Apply suggestions from code review\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n\n* make\n\n* correct\n\n* finish\n\n* finish\n\nCo-authored-by: Suraj Patil <surajp815@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class VersatileDiffusionTextToImagePipelineIntegrationTests(unittest.TestCase):",
            "image_slice = image[0, 253:256, 253:256, -1]",
            "",
            "assert image.shape == (1, 512, 512, 3)",
            "-        expected_slice = np.array([0.0408, 0.0181, 0.0, 0.0388, 0.0046, 0.0461, 0.0411, 0.0, 0.0222])",
            "+        expected_slice = np.array([0.3493, 0.3757, 0.4093, 0.4495, 0.4233, 0.4102, 0.4507, 0.4756, 0.4787])",
            "+",
            "assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2"
        ]
    },
    {
        "number": 4711,
        "comments": "",
        "commit_message": "Removing old BERT files (#3746)\n\n* Removed old bert files\n\n* removing pretrained model caching\n\n* fix bug\n\n* black, flake\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestBasicTextFieldEmbedder(AllenNlpTestCase):",
            "token_embedder = BasicTextFieldEmbedder.from_params(vocab=self.vocab, params=params)",
            "inputs = {",
            "\"bert\": {",
            "-                \"input_ids\": (torch.rand(3, 5) * 10).long(),",
            "-                \"offsets\": (torch.rand(3, 5) * 1).long(),",
            "+                \"token_ids\": (torch.rand(3, 5) * 10).long(),",
            "+                \"mask\": (torch.rand(3, 5) * 1).long(),",
            "},",
            "\"token_characters\": {\"token_characters\": (torch.rand(3, 5, 5) * 1).long()},",
            "}"
        ]
    },
    {
        "number": 4714,
        "comments": "",
        "commit_message": "Fixed matrix_rank implementation of backends to adhere to the standard (#4827)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def matrix_rank(",
            ") -> torch.Tensor:",
            "# ToDo: add support for default rtol value here, for the case where None is provided",
            "ret = torch.linalg.matrix_rank(x, rtol=rtol, out=out)",
            "-    return torch.tensor(ret, dtype=ivy.default_int_dtype(as_native=True))",
            "+    return ret.to(dtype=x.dtype)",
            "",
            "",
            "matrix_rank.unsupported_dtypes = ("
        ]
    },
    {
        "number": 4715,
        "comments": "",
        "commit_message": "fix cuda tests failing (#1941)\n\n* fix cuda tests failing\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def adalam_core(",
            "final_matches, idxs, counts = torch.unique(final_matches, dim=0, return_inverse=True, return_counts=True)",
            "_, ind_sorted = torch.sort(idxs)",
            "cum_sum = counts.cumsum(0)",
            "-        cum_sum = torch.cat((torch.tensor([0]), cum_sum[:-1]))",
            "+        cum_sum = torch.cat((torch.tensor([0], dtype=cum_sum.dtype, device=cum_sum.device), cum_sum[:-1]))",
            "first_indicies = ind_sorted[cum_sum]",
            "accepted_dist = accepted_dist[first_indicies]",
            "if return_dist:"
        ]
    },
    {
        "number": 4716,
        "comments": "",
        "commit_message": "Keras training: Give up on inferring steps if the dataset is not a `tf.data.Dataset` such as a per-worker dataset (thus doesn't have a variant tensor).\n\nThis fixes the cases where the dataset is trained with a per-worker dataset.\n\nPiperOrigin-RevId: 463620470\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DataHandler:",
            "if adapter_steps is not None:",
            "return adapter_steps",
            "",
            "+        # tf.distribute's `PerWorkerDataset` does not inherit from",
            "+        # `tf.data.Dataset` and in those cases we give up on inferring steps.",
            "+        if not isinstance(dataset, tf.data.Dataset):",
            "+            return None",
            "+",
            "size = tf.data.experimental.cardinality(dataset)",
            "if size == tf.data.experimental.INFINITE_CARDINALITY and steps is None:",
            "raise ValueError("
        ]
    },
    {
        "number": 4718,
        "comments": "",
        "commit_message": "Use codecs utf-8 writer for decode_and_evaluate. Fix #38.\n\nPiperOrigin-RevId: 164206518\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def decode_and_evaluate(name,",
            "",
            "start_time = time.time()",
            "num_sentences = 0",
            "-    with tf.gfile.GFile(trans_file, mode=\"w\") as trans_f:",
            "+    with codecs.getwriter(\"utf-8\")(",
            "+        tf.gfile.GFile(trans_file, mode=\"w\")) as trans_f:",
            "trans_f.write(\"\")  # Write empty string to ensure file is created.",
            "",
            "while True:"
        ]
    },
    {
        "number": 4722,
        "comments": "",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestRandomCropSizeGen(RandomGeneratorBaseTests):",
            "",
            "def test_same_on_batch(self, device, dtype):",
            "torch.manual_seed(42)",
            "-        degrees = torch.tensor([10, 20])",
            "res = random_crop_size_generator(",
            "batch_size=8,",
            "size=(100, 100),"
        ]
    },
    {
        "number": 4725,
        "comments": "",
        "commit_message": "Add linting with black (#2678)\n\nSummary:\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes # (issue).\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2678\n\nReviewed By: Mortimerp9\n\nDifferential Revision: D32653381\n\nPulled By: dianaml0\n\nfbshipit-source-id: 2810d14867cd7d64f4d340740e2b590b82de47fe\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TransformerEncoderBase(FairseqEncoder):",
            "# `forward` so we use a dictionary instead.",
            "# TorchScript does not support mixed values so the values are all lists.",
            "# The empty list is equivalent to None.",
            "-        src_lengths = src_tokens.ne(self.padding_idx).sum(dim=1, dtype=torch.int32).reshape(-1, 1).contiguous()",
            "+        src_lengths = (",
            "+            src_tokens.ne(self.padding_idx)",
            "+            .sum(dim=1, dtype=torch.int32)",
            "+            .reshape(-1, 1)",
            "+            .contiguous()",
            "+        )",
            "return {",
            "\"encoder_out\": [x],  # T x B x C",
            "\"encoder_padding_mask\": [encoder_padding_mask],  # B x T"
        ]
    },
    {
        "number": 4727,
        "comments": "",
        "commit_message": "Fix TorchScript support in `DNAConv` and `FAConv` (#6754)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_fa_conv():",
            "",
            "result = conv(x, x_0, adj2.t(), return_attention_weights=True)",
            "assert torch.allclose(result[0], out)",
            "-    assert result[1].size() == torch.Size([4, 4]) and result[1]._nnz() == 10",
            "+    assert result[1][0].size() == torch.Size([4, 4])",
            "+    assert result[1][0]._nnz() == 10",
            "assert conv._alpha is None",
            "",
            "if is_full_test():"
        ]
    },
    {
        "number": 4728,
        "comments": "",
        "commit_message": "Drop JIT support for `core.check`, `Boxes`, and others (#2219)\n\n* Drop JIT support for `core.check` API\n\n- Consequently for this, we drop support of JIT on the following items: (in of dynamo)\n  - enhance\n    - AdjustSigmoid\n    - AdjustLog\n    - AddWeighted\n  - geometry\n    - UndistortPoints\n    - bbox and Boxes - follow up on #2218\n    - EuclideanDistance\n    - TransformPoints\n    - HomographyWarper\n    - WarpPerspective\n    - UpscaleDouble\n  - losses\n\n* Update typing with pyupgrade\n* drop all jit related from bbox and boxes\n\nfrom #2218\n* fix/skip failing dynamo tests\n* fix loss hd\n* fix typing\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestEuclideanDistance(BaseTester):",
            "pt2 = torch.rand(2, 3, device=device, dtype=torch.float64, requires_grad=True)",
            "assert gradcheck(kgl.euclidean_distance, (pt1, pt2), raise_exception=True, fast_mode=True)",
            "",
            "-    def test_jit(self, device, dtype):",
            "+    def test_dynamo(self, device, dtype, torch_optimizer):",
            "pt1 = torch.rand(2, 3, device=device, dtype=dtype)",
            "pt2 = torch.rand(2, 3, device=device, dtype=dtype)",
            "op = kgl.euclidean_distance",
            "-        op_jit = torch.jit.script(op)",
            "-        self.assert_close(op(pt1, pt2), op_jit(pt1, pt2))",
            "+        op_optimized = torch_optimizer(op)",
            "+        self.assert_close(op(pt1, pt2), op_optimized(pt1, pt2))",
            "",
            "def test_module(self, device, dtype):",
            "pass"
        ]
    },
    {
        "number": 4729,
        "comments": "",
        "commit_message": "Fix bug Issue4592 (#4614)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MNASNet(nn.Module):",
            "nn.ReLU(inplace=True),",
            "]",
            "self.layers = nn.Sequential(*layers)",
            "-        self.classifier = nn.Sequential(nn.Dropout(p=dropout, inplace=True),",
            "+        self.classifier = nn.Sequential(nn.Dropout(p=dropout),",
            "nn.Linear(1280, num_classes))",
            "self._initialize_weights()",
            "#self.for_test = 10"
        ]
    },
    {
        "number": 4734,
        "comments": "",
        "commit_message": "Fix mnist sgd jenkins tests on master (#4168)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def sum_grad_and_var_all_reduce(grad_and_vars,",
            "#   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))",
            "scaled_grads = [g for g, _ in grad_and_vars]",
            "if alg == 'nccl':",
            "-            summed_grads = nccl.all_sum(scaled_grads)",
            "+            from tensorflow.python.ops import nccl_ops",
            "+            summed_grads = nccl_ops.all_sum(scaled_grads)",
            "elif alg == 'simple':",
            "summed_grads = build_reduce_sum(scaled_grads)",
            "elif alg == 'trivial':"
        ]
    },
    {
        "number": 4738,
        "comments": "",
        "commit_message": "Fix lstm return state shape.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Lstm(Layer):",
            "",
            "# This distinction is so we can stack multiple LSTM layers",
            "if self.return_final_state:",
            "-            return tf.stack(values=(state.c, state.h), axis=1)",
            "+            return tf.concat(values=(state.c, state.h), axis=1)",
            "else:",
            "-            return x",
            "\\ No newline at end of file",
            "+            return x"
        ]
    },
    {
        "number": 4742,
        "comments": "",
        "commit_message": "Update trainer.py (#1098)\n\nfixes #1093\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Trainer:",
            "return loss.item()",
            "",
            "def _run_epoch(self, epoch: int, dataloader: DataLoader, train: bool = True):",
            "-        self.dataloader.sampler.set_epoch(epoch)",
            "+        dataloader.sampler.set_epoch(epoch)",
            "for iter, (source, targets) in enumerate(dataloader):",
            "step_type = \"Train\" if train else \"Eval\"",
            "source = source.to(self.local_rank)"
        ]
    },
    {
        "number": 4743,
        "comments": "",
        "commit_message": "Fixed failing test for clip_vector_norm and a bug with execute_with_gradients (#7582)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def execute_with_gradients(",
            "return grads",
            "",
            "if isinstance(y, ivy.NativeArray):",
            "-        grads = grad_func(torch.clone(y))",
            "+        grads = _set_duplicates(",
            "+            grad_func(torch.clone(y)), required_duplicate_index_chains",
            "+        )",
            "else:",
            "# ToDo: use functorch.jacrev if it fixes the issue with broken memory reference",
            "array_idxs = ivy.nested_argwhere(y, lambda x: ivy.is_native_array(x))"
        ]
    },
    {
        "number": 4750,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestCnnEncoder(AllenNlpTestCase):",
            "num_filters=13,",
            "ngram_filter_sizes=(1, 2, 3, 4, 5),",
            "output_dim=30)",
            "-        tensor = Variable(torch.rand(4, 8, 7))",
            "+        tensor = torch.rand(4, 8, 7)",
            "assert encoder(tensor, None).size() == (4, 30)"
        ]
    },
    {
        "number": 4755,
        "comments": "",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def as_ivy_dtype(dtype_in: Union[torch.dtype, str, bool, int, float], /) -> ivy.",
            "",
            "",
            "@with_unsupported_dtypes({\"1.11.0 and below\": (\"uint16\",)}, backend_version)",
            "-def as_native_dtype(dtype_in: Union[torch.dtype, str, bool, int, float],",
            "-                    /) -> torch.dtype:",
            "+def as_native_dtype(",
            "+    dtype_in: Union[torch.dtype, str, bool, int, float], /",
            "+) -> torch.dtype:",
            "if dtype_in is int:",
            "return ivy.default_int_dtype(as_native=True)",
            "if dtype_in is float:"
        ]
    },
    {
        "number": 4756,
        "comments": "",
        "commit_message": "fix image summary\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "W_init=tf.truncated_normal_initializer(stddev=0.02)):",
            "with tf.variable_scope('gen'):",
            "image_gen = self.generator(z)",
            "-                tf.summary.image('gen', image_gen, max_images=30)",
            "+                tf.summary.image('gen', image_gen, max_outputs=30)",
            "with tf.variable_scope('discrim'):",
            "vecpos, _ = self.discriminator(image_pos)",
            "with tf.variable_scope('discrim', reuse=True):"
        ]
    },
    {
        "number": 4757,
        "comments": "",
        "commit_message": "Remove duplicated functional of gp (#1607)\n\n* clean gp\n\n* rearange sgpr\n\n* kernel add -> sum in test_benchmark\n\n* nit\n\n* fix error\n\n* fix error during clean\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def svgp_multiclass(num_steps, whiten):",
            "whiten=whiten)",
            "",
            "gpmodel.fix_param(\"Xu\")",
            "-    gpmodel.kernel.get_subkernel(\"WhiteNoise\").fix_param(\"variance\")",
            "+    gpmodel.kernel.kern1.fix_param(\"variance\")",
            "",
            "gpmodel.optimize(optim.Adam({\"lr\": 0.0001}), num_steps=num_steps)"
        ]
    },
    {
        "number": 4759,
        "comments": "",
        "commit_message": "Fixed allclose to give consisten output of boolean arrays, fixed the docstring examples and added the correct decorators\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def allclose(",
            "equal_nan: Optional[bool] = False,",
            "out: Optional[torch.Tensor] = None,",
            ") -> bool:",
            "-    return torch.allclose(x1, x2, rtol=rtol, atol=atol, equal_nan=equal_nan)",
            "+    ret = torch.allclose(x1, x2, rtol=rtol, atol=atol, equal_nan=equal_nan)",
            "+    return torch.tensor(ret)",
            "",
            "",
            "@with_unsupported_dtypes({\"1.11.0 and below\": (\"float16\",)}, backend_version)"
        ]
    },
    {
        "number": 4764,
        "comments": "",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ResNetFPNModel(DetectionModel):",
            "maskrcnn_head_func = getattr(model_mrcnn, cfg.FPN.MRCNN_HEAD_FUNC)",
            "mask_logits = maskrcnn_head_func(",
            "'maskrcnn', roi_feature_maskrcnn, cfg.DATA.NUM_CATEGORY)   # #fg x #cat x 28 x 28",
            "-                indices = tf.stack([tf.range(tf.size(final_labels)), tf.to_int32(final_labels) - 1], axis=1)",
            "+                indices = tf.stack([tf.range(tf.size(final_labels)), tf.cast(final_labels, tf.int32) - 1], axis=1)",
            "final_mask_logits = tf.gather_nd(mask_logits, indices)   # #resultx28x28",
            "tf.sigmoid(final_mask_logits, name='output/masks')",
            "return []"
        ]
    },
    {
        "number": 4768,
        "comments": "",
        "commit_message": "Fix #235 replaying map_data (via dist.Subsample) (#239)\n\n* Add failing tests for #235\n\n* Implement iarange via dist.Subsample\n\n* Add documentation to subsample.py\n\n* Do not treat warnings as errors\n\n* Use pyro.util.log_gamma\n\n* Fix type error in Subsample.batch_log_pdf\n\n* Fix failing test that inspected sample sites\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def iarange(name, size, subsample_size=0):",
            "yield Variable(torch.LongTensor(list(range(size))))",
            "return",
            "",
            "-    subsample = Variable(torch.randperm(size)[0:subsample_size])",
            "+    subsample = sample(name, Subsample(size, subsample_size))",
            "if len(_PYRO_STACK) == 0:",
            "yield subsample",
            "else:"
        ]
    },
    {
        "number": 4769,
        "comments": "",
        "commit_message": "more numpy tests fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def argmin(",
            "keepdims: bool = False,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    x = torch.tensor(x)",
            "return torch.argmin(x, axis=axis, keepdim=keepdims, out=out)"
        ]
    },
    {
        "number": 4770,
        "comments": "",
        "commit_message": "Quick fix\n",
        "label": "",
        "answer": "no",
        "change": [
            "def SubpixelConv2d(net, scale=2, n_out_channel=None, act=tf.identity, name='subp",
            "bsize = tf.shape(X)[0] # Handling Dimension(None) type for undefined batch dim",
            "Xs=tf.split(X,r,3) #b*h*w*r*r",
            "Xr=tf.concat(Xs,2) #b*h*(r*w)*r",
            "-            X=tf.reshape(Xr,(b,r*h,r*w,c)) # b*(r*h)*(r*w)*c",
            "+            X=tf.reshape(Xr,(b,r*a,r*b,c)) # b*(r*h)*(r*w)*c",
            "else:",
            "print(_err_log)",
            "return X"
        ]
    },
    {
        "number": 4773,
        "comments": "",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def resnet_fpn_backbone(image, num_blocks):",
            "freeze_at = cfg.BACKBONE.FREEZE_AT",
            "shape2d = tf.shape(image)[2:]",
            "mult = float(cfg.FPN.RESOLUTION_REQUIREMENT)",
            "-    new_shape2d = tf.to_int32(tf.ceil(tf.to_float(shape2d) / mult) * mult)",
            "+    new_shape2d = tf.cast(tf.ceil(tf.cast(shape2d, tf.float32) / mult) * mult, tf.int32)",
            "pad_shape2d = new_shape2d - shape2d",
            "assert len(num_blocks) == 4, num_blocks",
            "with backbone_scope(freeze=freeze_at > 0):"
        ]
    },
    {
        "number": 4774,
        "comments": "",
        "commit_message": "fix entropy term in distributions.py (wrong isinstance check) (#3120)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ExpandedDistribution(TorchDistribution):",
            "log_prob = log_prob.expand(shape)",
            "if isinstance(score_function, torch.Tensor):",
            "score_function = score_function.expand(shape)",
            "-            if isinstance(score_function, torch.Tensor):",
            "+            if isinstance(entropy_term, torch.Tensor):",
            "entropy_term = entropy_term.expand(shape)",
            "return ScoreParts(log_prob, score_function, entropy_term)"
        ]
    },
    {
        "number": 4778,
        "comments": "",
        "commit_message": "Fix typo in train split name (#3751)\n\n* Fix typo in README guide\n\n* Fix split naming and table alignment in README guide\n\n* Fix split naming and table alignment in all datasets\n\n* Fix previously malformed dataset cards\n\n* Fix previously malformed dataset cards\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DatasetBuilder:",
            "Please follow the manual download instructions:",
            "{self.manual_download_instructions}",
            "Manual data can be loaded with:",
            "-                     datasets.load_dataset({self.name}, data_dir='<path/to/manual/data>')\"\"\"",
            "+                     datasets.load_dataset(\"{self.name}\", data_dir=\"<path/to/manual/data>\")\"\"\"",
            ")",
            ")"
        ]
    },
    {
        "number": 4780,
        "comments": "",
        "commit_message": "Fix a few issues loading pretrained vit/bit npz weights w/ num_classes=0 __init__ arg. Missed a few other small classifier handling detail on Mlp, GhostNet, Levit. Should fix #713\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def _load_weights(model: VisionTransformer, checkpoint_path: str, prefix: str =",
            "model.pos_embed.copy_(pos_embed_w)",
            "model.norm.weight.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/scale']))",
            "model.norm.bias.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/bias']))",
            "-    if model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:",
            "+    if isinstance(model.head, nn.Linear) and model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:",
            "model.head.weight.copy_(_n2p(w[f'{prefix}head/kernel']))",
            "model.head.bias.copy_(_n2p(w[f'{prefix}head/bias']))",
            "for i, block in enumerate(model.blocks.children()):"
        ]
    },
    {
        "number": 4783,
        "comments": "",
        "commit_message": "fix typo in freeze_variables\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def remap_variables(fn):",
            "def freeze_variables():",
            "\"\"\"",
            "Return a context, where all variables (reused or not) returned by",
            "-    ``get_variable`` will have no gradients (they  will be followed by ``tf.stop_gradient``).",
            "+    ``get_variable`` will have no gradients (they will be wrapped by ``tf.stop_gradient``).",
            "But they will still be in ``TRAINABLE_VARIABLES`` collections so they will get",
            "saved correctly. This is useful to fix certain variables for fine-tuning.",
            "",
            "Example:",
            ".. code-block:: python",
            "",
            "-            with varreplace.freeze_get_variable():",
            "+            with varreplace.freeze_variable():",
            "x = FullyConnected('fc', x, 1000)   # fc/* will not be trained",
            "\"\"\"",
            "return remap_variables(lambda v: tf.stop_gradient(v))"
        ]
    },
    {
        "number": 4794,
        "comments": "",
        "commit_message": "[Metrics] Unification of regression (#4166)\n\n* moved to utility\n\n* add files\n\n* unify\n\n* add desc\n\n* update\n\n* end of line\n\n* Apply suggestions from code review\n\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\n\n* add back functional test in new interface\n\n* pep8\n\n* doctest fix\n\n* test name fix\n\n* unify psnr + add class psnr, TODO: psnr test refactor ala mean squared error\n\n* unify psnr\n\n* rm unused code\n\n* pep8\n\n* docs\n\n* unify ssim\n\n* lower tolerance for ssim\n\n* fix import\n\n* pep8\n\n* docs\n\n* flake8\n\n* test smaller images\n\n* trying to fix test\n\n* no ddp test for ssim\n\n* pep8\n\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\nCo-authored-by: Teddy Koker <teddy.koker@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MeanSquaredLogError(Metric):",
            "preds: Predictions from model",
            "target: Ground truth values",
            "\"\"\"",
            "-        self._check_same_shape(preds, target)",
            "-        squared_log_error = torch.pow(torch.log1p(preds) - torch.log1p(target), 2)",
            "+        sum_squared_log_error, n_obs = _mean_squared_log_error_update(preds, target)",
            "",
            "-        self.sum_squared_log_error += torch.sum(squared_log_error)",
            "-        self.total += target.numel()",
            "+        self.sum_squared_log_error += sum_squared_log_error",
            "+        self.total += n_obs",
            "",
            "def compute(self):",
            "\"\"\"",
            "Compute mean squared logarithmic error over state.",
            "\"\"\"",
            "-        return self.sum_squared_log_error / self.total",
            "+        return _mean_squared_log_error_compute(self.sum_squared_log_error, self.total)"
        ]
    },
    {
        "number": 4795,
        "comments": "",
        "commit_message": "don't use sigmoid output for tacotron, fix bug for memory queue handling, remove maxout\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Tacotron(nn.Module):",
            "forward_attn, trans_agent, forward_attn_mask,",
            "location_attn, separate_stopnet)",
            "self.postnet = PostCBHG(mel_dim)",
            "-        self.last_linear = nn.Sequential(",
            "-            nn.Linear(self.postnet.cbhg.gru_features * 2, linear_dim),",
            "-            nn.Sigmoid())",
            "+        self.last_linear = nn.Linear(self.postnet.cbhg.gru_features * 2, linear_dim)",
            "",
            "def forward(self, characters, text_lengths, mel_specs, speaker_ids=None):",
            "B = characters.size(0)"
        ]
    },
    {
        "number": 4796,
        "comments": "",
        "commit_message": "Fix TorchScript support in `DNAConv` and `FAConv` (#6754)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_dna_conv1():",
            "assert out.size() == (num_nodes, channels)",
            "",
            "if is_full_test():",
            "-        jit = torch.jit.script(conv.jittable())",
            "+        t = '(Tensor, Tensor, OptTensor) -> Tensor'",
            "+        jit = torch.jit.script(conv.jittable(t))",
            "assert jit(x, edge_index).tolist() == out.tolist()"
        ]
    },
    {
        "number": 4797,
        "comments": "",
        "commit_message": "summaries updated and fixed\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class InternalLstm(TransformationBase):",
            "assert False",
            "",
            "specification[self.name] = dict(",
            "-            type='float', shape=(2, self.size), initial_state=tf.identity(x=self.initial_state)",
            "+            type='float', shape=(2, self.size), initial_state=self.initial_state",
            ")",
            "",
            "return specification"
        ]
    },
    {
        "number": 4798,
        "comments": "",
        "commit_message": "Fix logits masking bug introduced in #2469 (#2774)\n\nSummary:\n- Masking logits with float(inf) significantly degrades deconding\nperformance with language model and flashlight decoder. This fixes it.\n\n# Before submitting\n\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\n- [ ] Did you make sure to update the docs?\n- [ ] Did you write any new necessary tests?\n\n## What does this PR do?\nFixes logits masking bug introduced in https://github.com/fairinternal/fairseq-py/issues/2469\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding \ufffd\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2774\n\nReviewed By: arbabu123\n\nDifferential Revision: D33003148\n\nPulled By: alexeib\n\nfbshipit-source-id: 19bf3da4f5104b33357fd4941e5e76b95174ee28\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Wav2VecCtc(BaseFairseqModel):",
            "",
            "if net_output[\"padding_mask\"] is not None and net_output[\"padding_mask\"].any():",
            "number_of_classes = logits.size(-1)",
            "-            masking_tensor = torch.ones(number_of_classes) * float(\"-inf\")",
            "-            masking_tensor[0] = float(\"inf\")",
            "+            masking_tensor = torch.ones(",
            "+                number_of_classes, device=logits.device",
            "+            ) * float(\"-inf\")",
            "+            masking_tensor[0] = 0",
            "logits[net_output[\"padding_mask\"].T] = masking_tensor.type_as(logits)",
            "",
            "if normalize:"
        ]
    },
    {
        "number": 4799,
        "comments": "",
        "commit_message": "dtype fix in LayerScale to support mixed precision\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LayerScale(layers.Layer):",
            "",
            "def build(self, input_shape):",
            "self.gamma = tf.Variable(",
            "-            self.init_values * tf.ones((self.projection_dim,))",
            "+            self.init_values * tf.ones((self.projection_dim,)),",
            "+            dtype=self._compute_dtype_object",
            ")",
            "",
            "def call(self, x):"
        ]
    },
    {
        "number": 4802,
        "comments": "",
        "commit_message": "Fix the device of label in multiclass_nms (#5673)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def multiclass_nms(multi_bboxes,",
            "",
            "scores = multi_scores[:, :-1]",
            "",
            "-    labels = torch.arange(num_classes, dtype=torch.long)",
            "+    labels = torch.arange(num_classes, dtype=torch.long, device=scores.device)",
            "labels = labels.view(1, -1).expand_as(scores)",
            "",
            "bboxes = bboxes.reshape(-1, 4)"
        ]
    },
    {
        "number": 4803,
        "comments": "",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def BilinearUpSample(x, shape):",
            "\"\"\"",
            "# inp_shape = tf.shape(x)",
            "# return tf.image.resize_bilinear(x,",
            "-    # tf.pack([inp_shape[1]*shape,inp_shape[2]*shape]),",
            "+    # tf.stack([inp_shape[1]*shape,inp_shape[2]*shape]),",
            "# align_corners=True)",
            "",
            "inp_shape = x.get_shape().as_list()"
        ]
    },
    {
        "number": 4821,
        "comments": "",
        "commit_message": "Sync for 4.0b4 release (#950)\n\n* sync for 4.0b4 release\n\n* fix extra space character in build.sh and add a simple prediction test for smoke testing on older macs\n",
        "label": "",
        "answer": "no",
        "change": [
            "def run_compare_tf(",
            "graph, feed_dict, output_nodes, frontend, backend",
            ")",
            "else:",
            "-        with tf.Session(graph=graph) as sess:",
            "-            sess.run(tf.global_variables_initializer())",
            "-            tf_outputs = sess.run(output_nodes, feed_dict=feed_dict)",
            "+        if not tf_outputs:",
            "+            with tf.Session(graph=graph) as sess:",
            "+                sess.run(tf.global_variables_initializer())",
            "+                tf_outputs = sess.run(output_nodes, feed_dict=feed_dict)",
            "expected_outputs = {name: val for name, val in zip(output_names, tf_outputs)}",
            "",
            "for k,v in input_key_values.items():"
        ]
    },
    {
        "number": 4830,
        "comments": "",
        "commit_message": "[Deepspeed Wav2vec2] integration (#11638)\n\n* wip\n\n* wip - but working with https://github.com/microsoft/DeepSpeed/pull/1044\n\n* cleanup\n\n* workaround\n\n* working 5/8 modes\n\n* solve fp32 distributed zero3\n\n* style\n\n* sync\n\n* sync\n\n* rework\n\n* deprecation\n\n* cleanup\n\n* https://github.com/microsoft/DeepSpeed/pull/1044 pr was merged\n\n* clean up\n\n* add a guide\n\n* more prose\n\n* more prose\n\n* fix\n\n* more prose\n\n* sub_group_size was too big\n\n* Apply suggestions from code review\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* refactor\n\n* bug fix\n\n* make the true check explicit\n\n* new deepspeed release\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Trainer:",
            "\"\"\"",
            "for k, v in inputs.items():",
            "if isinstance(v, torch.Tensor):",
            "-                inputs[k] = v.to(self.args.device)",
            "+                kwargs = dict(device=self.args.device)",
            "+                if self.deepspeed and inputs[k].dtype != torch.int64:",
            "+                    # NLP models inputs are int64 and those get adjusted to the right dtype of the",
            "+                    # embedding. Other models such as wav2vec2's inputs are already float and thus",
            "+                    # may need special handling to match the dtypes of the model",
            "+                    kwargs.update(dict(dtype=self.args.hf_deepspeed_config.dtype()))",
            "+",
            "+                inputs[k] = v.to(**kwargs)",
            "",
            "if self.args.past_index >= 0 and self._past is not None:",
            "inputs[\"mems\"] = self._past"
        ]
    },
    {
        "number": 4831,
        "comments": "",
        "commit_message": "Fix RNNP\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RNNP(torch.nn.Module):",
            "xs_pack = pack_padded_sequence(xs_pad, ilens, batch_first=True)",
            "rnn = getattr(self, (\"birnn\" if self.bidir else \"rnn\") + str(layer))",
            "rnn.flatten_parameters()",
            "-            if prev_state is not None and self.nbrnn.bidirectional:",
            "+            if prev_state is not None and rnn.bidirectional:",
            "prev_state = zero_backward_rnn_state(prev_state)",
            "ys, states = rnn(xs_pack, hx=prev_state)",
            "elayer_states.append(states)"
        ]
    },
    {
        "number": 4837,
        "comments": "",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestAutoRegressiveSeqDecoder(AllenNlpTestCase):",
            "",
            "encoded_state = torch.randn(batch_size, time_steps, decoder_inout_dim)",
            "source_mask = torch.ones(batch_size, time_steps).long()",
            "-        target_tokens = {\"tokens\": torch.ones(batch_size, time_steps).long()}",
            "+        target_tokens = {\"tokens\": {\"tokens\": torch.ones(batch_size, time_steps).long()}}",
            "source_mask[0, 1:] = 0",
            "encoder_out = {\"source_mask\": source_mask, \"encoder_outputs\": encoded_state}"
        ]
    },
    {
        "number": 4838,
        "comments": "",
        "commit_message": "fix flake8\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_encoder_cache(normalize_before):",
            "dropout_rate=0.0,",
            "input_layer=\"embed\")",
            "elayer = encoder.encoders[0]",
            "-    memory = torch.randn(2, 5, adim)",
            "-",
            "x = torch.randn(2, 5, adim)",
            "mask = subsequent_mask(x.shape[1]).unsqueeze(0)",
            "prev_mask = mask[:, :-1, :-1]"
        ]
    },
    {
        "number": 4839,
        "comments": "",
        "commit_message": "Metrics in distributed setting (#4525)\n\n* initial commit to ensure that metrics work correctly in distributed setting\n\n* updating global_distributed_metric to take metric object\n\n* adding distributed f1 score\n\n* adding distributed attachment scores\n\n* bug fix\n\n* adding distributed boolean accuracy\n\n* adding distributed entropy\n\n* adding distributed evalb\n\n* adding distributed mean_absolute_error\n\n* adding distributed sequence accuracy\n\n* adding distributed unigram recall\n\n* making models compatible with distributed metrics\n\n* adding distributed auc\n\n* adding distributed bleu\n\n* adding missing argument\n\n* initial commit to ensure that metrics work correctly in distributed setting\n\n* updating global_distributed_metric to take metric object\n\n* adding distributed f1 score\n\n* adding distributed attachment scores\n\n* bug fix\n\n* adding distributed boolean accuracy\n\n* adding distributed entropy\n\n* adding distributed evalb\n\n* adding distributed mean_absolute_error\n\n* adding distributed sequence accuracy\n\n* adding distributed unigram recall\n\n* making models compatible with distributed metrics\n\n* adding distributed auc\n\n* adding distributed bleu\n\n* adding missing argument\n\n* changing start method\n\n* removing unnecessary argument\n\n* adding remaining metrics, removing extra argument\n\n* allowing float values\n\n* bug fix\n\n* more bug fixes\n\n* changing average to return float\n\n* adding timeout for distributed test\n\n* testing unequal batches\n\n* adding distributed auc\n\n* adding distributed spearman correlation\n\n* adding distributed covariance and pearson correlation\n\n* changing distributed test to function, misc changes\n\n* checking batch lengths explicitly to raise errors\n\nCo-authored-by: Dirk Groeneveld <dirkg@allenai.org>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SequenceAccuracy(Metric):",
            "accuracy = self.correct_count / self.total_count",
            "else:",
            "accuracy = 0",
            "-",
            "if reset:",
            "self.reset()",
            "-        return accuracy",
            "+        return {\"accuracy\": accuracy}",
            "",
            "@overrides",
            "def reset(self):"
        ]
    },
    {
        "number": 4845,
        "comments": "",
        "commit_message": "Fix docstring issues (#2072)\n\n* Fix docstring module name of filesystems\n\n* Fix docstring missing blank after comma\n\n* Fix docstring missing trailing dot\n\n* Fix docstring cross-referencing in Returns\n\n* Fix docstring cross-referencing in description\n\n* Fix docstring non-rendered args descriptions\n\n* Fix docstring cross-reference content prefix\n\n* Fix docstring line length\n\n* Fix docstring of SplitGenerator\n\n* Fix docstring of Split\n\n* Fix docstring document cross-reference in Split\n\n* Fix docstring of DatasetDict.shuffle\n\n* Fix docstring of DatasetBuilder\n\n* Add docstring attributes of DatasetInfo\n\n* Change docstring default rendering of Attributes in conf.py\n\n* Fix docstring of list_datasets\n\n* Fix docstring of load_dataset\n",
        "label": "",
        "answer": "no",
        "change": [
            "def is_remote_filesystem(fs: fsspec.spec.AbstractFileSystem) -> bool:",
            "Validates if filesystem has remote protocol.",
            "",
            "Args:",
            "-        fs (``fsspec.spec.AbstractFileSystem``): An abstract super-class for pythonic file-systems, e.g. :code:`fsspec.filesystem(\\'file\\')` or :class:`datasets.filesystem.S3FileSystem`",
            "+        fs (``fsspec.spec.AbstractFileSystem``): An abstract super-class for pythonic file-systems, e.g. :code:`fsspec.filesystem(\\'file\\')` or :class:`datasets.filesystems.S3FileSystem`",
            "\"\"\"",
            "if fs is not None and fs.protocol != \"file\":",
            "return True"
        ]
    },
    {
        "number": 4848,
        "comments": "",
        "commit_message": "MBartForConditionalGeneration (#6441)\n\n* add MBartForConditionalGeneration\n\n* style\n\n* rebase and fixes\n\n* add mbart test in TEST_FILES_WITH_NO_COMMON_TESTS\n\n* fix docs\n\n* don't ignore mbart\n\n* doc\n\n* fix mbart fairseq link\n\n* put mbart before bart\n\n* apply doc suggestions\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BartConfig(PretrainedConfig):",
            "if self.normalize_before or self.add_final_layer_norm or self.scale_embedding:",
            "logger.info(\"This configuration is a mixture of MBART and BART settings\")",
            "return False",
            "-",
            "-",
            "-class MBartConfig(BartConfig):",
            "-    model_type = \"mbart\"",
            "-    \"\"\"See real config values at https://s3.amazonaws.com/models.huggingface.co/bert/facebook/mbart-large-en-ro/config.json.\"\"\""
        ]
    },
    {
        "number": 4851,
        "comments": "",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "label": "",
        "answer": "no",
        "change": [
            "from .tokenization_utils import BatchEncoding",
            "logger = logging.getLogger(__name__)",
            "",
            "TF_TRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"transfo-xl-wt103\": \"https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-tf_model.h5\",",
            "+    \"transfo-xl-wt103\": \"https://cdn.huggingface.co/transfo-xl-wt103-tf_model.h5\",",
            "}"
        ]
    },
    {
        "number": 4853,
        "comments": "",
        "commit_message": "[RLlib] Add testing framework_iterator. (#7852)\n\n* Add testing framework_iterator.\n\n* LINT.\n\n* WIP.\n\n* Fix and LINT.\n\n* LINT fix.\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class QLoss:",
            "# priority is robust and insensitive to `prioritized_replay_alpha`",
            "self.td_error = tf.nn.softmax_cross_entropy_with_logits(",
            "labels=m, logits=q_logits_t_selected)",
            "-            self.loss = tf.reduce_mean(self.td_error * importance_weights)",
            "+            self.loss = tf.reduce_mean(",
            "+                self.td_error * tf.cast(importance_weights, tf.float32))",
            "self.stats = {",
            "# TODO: better Q stats for dist dqn",
            "\"mean_td_error\": tf.reduce_mean(self.td_error),"
        ]
    },
    {
        "number": 4861,
        "comments": "",
        "commit_message": "Fix the bug that torch version less than 1.12 throws TypeError (#1671)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "try:",
            "if _torch_available:",
            "import torch",
            "",
            "-        if torch.__version__ < version.Version(\"1.12\"):",
            "+        if version.Version(torch.__version__) < version.Version(\"1.12\"):",
            "raise ValueError(\"PyTorch should be >= 1.12\")",
            "logger.debug(f\"Successfully imported xformers version {_xformers_version}\")",
            "except importlib_metadata.PackageNotFoundError:"
        ]
    },
    {
        "number": 4862,
        "comments": "",
        "commit_message": "Refac module factory + avoid etag requests for hub datasets (#2986)\n\n* refac module factory + avoid etag requests for hub datasets\n\n* fix tests\n\n* typing\n\n* fixes\n\n* prepare timeout\n\n* fix offline simulator with hugginggace_hub\n\n* add module factory tests (1/N)\n\n* add module factory test (2/N)\n\n* add data files tests (1/N)\n\n* add data fiels tests (2/N)\n\n* add data files tests (3/N)\n\n* style\n\n* docstrings\n\n* don't update counts when running tests\n\n* nump huggingface_hub\n\n* add timeouts for offline mode\n\n* minor\n\n* minor bis\n\n* install ruamel-yaml properly in the CI for windows\n\n* fix windows test\n\n* style\n\n* fix comet intensive calls patcher\n\n* warning message when loading from the master branch\n\n* style\n\n* albert's comments\n\n* remove unnecessary check\n\n* don't use master if HF_SCRIPTS_VERSION is specified\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LocalMetricTest(parameterized.TestCase):",
            "",
            "def test_load_metric(self, metric_name):",
            "doctest.ELLIPSIS_MARKER = \"[...]\"",
            "-        metric_module = importlib.import_module(datasets.load.prepare_module(os.path.join(\"metrics\", metric_name))[0])",
            "+        metric_module = importlib.import_module(",
            "+            datasets.load.prepare_module(os.path.join(\"metrics\", metric_name), dataset=False)[0]",
            "+        )",
            "metric = datasets.load.import_main_class(metric_module.__name__, dataset=False)",
            "# check parameters",
            "parameters = inspect.signature(metric._compute).parameters"
        ]
    },
    {
        "number": 4864,
        "comments": "",
        "commit_message": "[Feat] Add elastic transform 2d (#853)\n\n* first\n\n* Update elastic_transform.py\n\n* updated elastic transform\n\n* now gradients flow through the gaussian function\n\n* cleaner elastic transform\n\n* minor fixes in augmentation documentation\n\n* implement functional elastic transform\n\n* set device and dtype for gaussian kernel\n\nCo-authored-by: IssamLaradji <issam.laradji@gmail.com>\nCo-authored-by: Issam H. Laradji <IssamLaradji@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestSimpleKD:",
            "def test_jit(self, device, dtype):",
            "batch_size, channels, ps = 1, 1, 19",
            "patches = torch.rand(batch_size, channels, ps, ps).to(device)",
            "-        model =  SimpleKD(patch_size=ps, kernel_type='polar', whitening='lw').to(patches.device, patches.dtype).eval()  # noqa",
            "-        model_jit = torch.jit.script( SimpleKD(patch_size=ps, kernel_type='polar', whitening='lw').to(patches.device, patches.dtype).eval())  # noqa",
            "+        model = SimpleKD(patch_size=ps, kernel_type='polar', whitening='lw').to(patches.device, patches.dtype).eval()  # noqa",
            "+        model_jit = torch.jit.script(SimpleKD(patch_size=ps, kernel_type='polar', whitening='lw').to(patches.device, patches.dtype).eval())  # noqa",
            "assert_allclose(model(patches), model_jit(patches))"
        ]
    },
    {
        "number": 4868,
        "comments": "",
        "commit_message": "[TF 2.2 compat]  use tf.VariableAggregation.ONLY_FIRST_REPLICA (#4283)\n\n* Fix the issue to properly run the accumulator with TF 2.2\n\n* Apply style\n\n* Fix training_args_tf for TF 2.2\n\n* Fix the TF training args when only one GPU is available\n\n* Remove the fixed version of TF in setup.py\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GradientAccumulator(object):",
            "self._gradients.extend(",
            "[",
            "tf.Variable(",
            "-                        tf.zeros_like(gradient), trainable=False, synchronization=tf.VariableSynchronization.ON_READ,",
            "+                        tf.zeros_like(gradient),",
            "+                        trainable=False,",
            "+                        synchronization=tf.VariableSynchronization.ON_READ,",
            "+                        aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA,",
            ")",
            "for gradient in gradients",
            "]"
        ]
    },
    {
        "number": 4871,
        "comments": "",
        "commit_message": "Disable non blocking to device with MPS (#14368)\n\n* disable non-blocking for mps due to race condition bug\n\n* fixed typo\n\n* fixed: unknown mps device for non arm systems\n\n* Removed unrobust test case\n\n* moved _MPS_DEVICES such that we used in apply_func\n\n* Resolve circular dependencies\n\n* Comment rewording\n\n* changed torchElasticEnvironment to a global import\n\n* simplified if statement to blocking device type\n\n* Added change to CHANGELOG\n\n* Update src/pytorch_lightning/utilities/apply_func.py\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fixed mypy not detecting casting of device\n\n* Moved check into if statement to mainain original behavior\n\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\nCo-authored-by: Justus Schock <12886177+justusschock@users.noreply.github.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Jirka Borovec <Borda@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def move_data_to_device(batch: Any, device: Union[str, torch.device]) -> Any:",
            "",
            "kwargs = {}",
            "# Don't issue non-blocking transfers to CPU",
            "-        if isinstance(data, Tensor) and device not in _CPU_DEVICES:",
            "+        # Same with MPS due to a race condition bug: https://github.com/pytorch/pytorch/issues/83015",
            "+        if isinstance(data, Tensor) and isinstance(device, torch.device) and device.type not in _BLOCKING_DEVICE_TYPES:",
            "kwargs[\"non_blocking\"] = True",
            "data_output = data.to(device, **kwargs)",
            "if data_output is not None:"
        ]
    },
    {
        "number": 4874,
        "comments": "",
        "commit_message": "Fix `select_device()` for Multi-GPU (#6434)\n\n* Fix `select_device()` for Multi-GPU\n\nPossible fix for https://github.com/ultralytics/yolov5/issues/6431\n\n* Update torch_utils.py\n\n* Update torch_utils.py\n\n* Update torch_utils.py\n\n* Update torch_utils.py\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n\n* Update\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def select_device(device='', batch_size=0, newline=True):",
            "if cpu:",
            "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force torch.cuda.is_available() = False",
            "elif device:  # non-cpu device requested",
            "-        nd = torch.cuda.device_count()  # number of CUDA devices",
            "-        assert torch.cuda.is_available(), 'CUDA is not available, use `--device cpu` or do not pass a --device'",
            "+        nd = device_count()  # number of CUDA devices",
            "assert nd > int(max(device.split(','))), f'Invalid `--device {device}` request, valid devices are 0 - {nd - 1}'",
            "-        os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable (must be after asserts)",
            "+        os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable - must be before assert is_available()",
            "+        assert torch.cuda.is_available(), 'CUDA is not available, use `--device cpu` or do not pass a --device'",
            "",
            "cuda = not cpu and torch.cuda.is_available()",
            "if cuda:"
        ]
    },
    {
        "number": 4876,
        "comments": "",
        "commit_message": "fix some typos in docs, comments, logging/errors (#11432)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def symmetric_linear_quantization_params(num_bits, saturation_min, saturation_ma",
            "`saturation_max`.",
            "\"\"\"",
            "# in this part, we do not need any gradient computation,",
            "-    # in order to enfore this, we put torch.no_grad()",
            "+    # in order to enforce this, we put torch.no_grad()",
            "with torch.no_grad():",
            "n = 2 ** (num_bits - 1) - 1"
        ]
    },
    {
        "number": 4878,
        "comments": "",
        "commit_message": "More unit test fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main(args):",
            "print(args)",
            "",
            "use_cuda = torch.cuda.is_available() and not args.cpu",
            "-    if hasattr(torch, 'set_grad_enabled'):",
            "-        torch.set_grad_enabled(False)",
            "",
            "# Load dataset",
            "if args.replace_unk is None:"
        ]
    },
    {
        "number": 4881,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestTimeDistributed(AllenNlpTestCase):",
            "def test_time_distributed_works_with_multiple_inputs(self):",
            "module = lambda x, y: x + y",
            "distributed = TimeDistributed(module)",
            "-        x_input = Variable(torch.LongTensor([[[1, 2], [3, 4]]]))",
            "-        y_input = Variable(torch.LongTensor([[[4, 2], [9, 1]]]))",
            "+        x_input = torch.LongTensor([[[1, 2], [3, 4]]])",
            "+        y_input = torch.LongTensor([[[4, 2], [9, 1]]])",
            "output = distributed(x_input, y_input)",
            "assert_almost_equal(output.data.numpy(), [[[5, 4], [12, 5]]])"
        ]
    },
    {
        "number": 4882,
        "comments": "",
        "commit_message": "revert padding bug fix for now\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_pred_input(params, enc = None):",
            "bos = tf.constant(1, shape=[1, 1], dtype=tf.int64)",
            "src_seq = tf.random.uniform(shape=[1, length], minval=4, maxval=(params['n_vocab'] - 1), dtype=tf.int64)",
            "seq = tf.concat([bos, src_seq], axis=1)",
            "-    seq = tf.pad(seq, [[0, 0], [0, remaining]], constant_values=params['padding_id'])",
            "+    seq = tf.pad(seq, [[0, 0], [0, remaining]])",
            "dataset = tf.data.Dataset.from_tensors(seq)",
            "",
            "dataset = dataset.map(_dummy_labels)"
        ]
    },
    {
        "number": 4885,
        "comments": "",
        "commit_message": "[MidBlock] Fix mid block (#78)\n\n* upload files\n\n* finish\n",
        "label": "",
        "answer": "no",
        "change": [
            "class UNetModel(ModelMixin, ConfigMixin):",
            "hs.append(self.down[i_level].downsample(hs[-1]))",
            "",
            "# middle",
            "-        h = self.mid(hs[-1], temb)",
            "-        #        h = self.mid.block_1(h, temb)",
            "-        #        h = self.mid.attn_1(h)",
            "-        #        h = self.mid.block_2(h, temb)",
            "+        h = self.mid_new(hs[-1], temb)",
            "",
            "# upsampling",
            "for i_level in reversed(range(self.num_resolutions)):"
        ]
    },
    {
        "number": 4888,
        "comments": "",
        "commit_message": "[OPT/Galactica] Load large `galactica` models (#20390)\n\n* fix `opt` bias\n\n* revert unneeded assignment\n",
        "label": "",
        "answer": "no",
        "change": [
            "class OPTDecoder(OPTPreTrainedModel):",
            "# with checkpoints that have been fine-tuned before transformers v4.20.1",
            "# see https://github.com/facebookresearch/metaseq/pull/164",
            "if config.do_layer_norm_before and not config._remove_final_layer_norm:",
            "-            self.final_layer_norm = nn.LayerNorm(config.hidden_size)",
            "+            self.final_layer_norm = nn.LayerNorm(",
            "+                config.hidden_size, elementwise_affine=config.layer_norm_elementwise_affine",
            "+            )",
            "else:",
            "self.final_layer_norm = None"
        ]
    },
    {
        "number": 4889,
        "comments": "",
        "commit_message": "max points bugfix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NormalizeRotation(object):",
            "C = torch.matmul(pos.t(), pos)",
            "e, v = torch.eig(C, eigenvectors=True)  # v[:,j] is j-th eigenvector",
            "",
            "-        data.pos = torch.matmul(pos, v)",
            "+        data.pos = torch.matmul(data.pos, v)",
            "",
            "if 'norm' in data:",
            "data.norm = F.normalize(torch.matmul(data.norm, v))"
        ]
    },
    {
        "number": 4897,
        "comments": "",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_deg2rad(batch_shape, device, dtype, atol, rtol):",
            "",
            "assert_close(x_deg, x_rad_to_deg, atol=atol, rtol=rtol)",
            "",
            "-    eps = torch.finfo(dtype).eps",
            "assert gradcheck(kornia.deg2rad, (tensor_to_gradcheck_var(x_deg),), raise_exception=True)"
        ]
    },
    {
        "number": 4898,
        "comments": "",
        "commit_message": "fixed_line (#1072)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NewDataset(datasets.GeneratorBasedBuilder):",
            "# data = datasets.load_dataset('my_dataset', 'first_domain')",
            "# data = datasets.load_dataset('my_dataset', 'second_domain')",
            "BUILDER_CONFIGS = [",
            "-        datasets.BuilderConfig(name=\"first_domain\", description=\"This part of my dataset covers a first domain\"),",
            "-        datasets.BuilderConfig(name=\"second_domain\", description=\"This part of my dataset covers a second domain\"),",
            "+        datasets.BuilderConfig(name=\"first_domain\", version=VERSION, description=\"This part of my dataset covers a first domain\"),",
            "+        datasets.BuilderConfig(name=\"second_domain\", version=VERSION, description=\"This part of my dataset covers a second domain\"),",
            "]",
            "",
            "DEFAULT_CONFIG_NAME = \"first_domain\"  # It's not mandatory to have a default configuration. Just use one if it make sense."
        ]
    },
    {
        "number": 4899,
        "comments": "",
        "commit_message": "Fix HolidayCalendar tests on Travis\n\nPiperOrigin-RevId: 290737915\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class HolidayCalendarTest(tf.test.TestCase, parameterized.TestCase):",
            "def test_skip_eager_reset(self):",
            "cal = dates.HolidayCalendar(start_year=2020, end_year=2021)",
            "cal.is_business_day(dates.DateTensor.from_tuples([]))  # Trigger caching.",
            "-    tf.reset_default_graph()",
            "+    tf.compat.v1.reset_default_graph()",
            "cal.reset()",
            "date_tensor = dates.DateTensor.from_tuples([(2020, 1, 3), (2020, 1, 4),",
            "(2021, 12, 24), (2021, 12, 25)])"
        ]
    },
    {
        "number": 4901,
        "comments": "",
        "commit_message": "RandomRotation3D cuda fix (#810)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestRandomRotation3D:",
            "",
            "def test_same_on_batch(self, device, dtype):",
            "f = RandomRotation3D(degrees=40, same_on_batch=True)",
            "-        input = torch.eye(6, device=device, dtype=dtype).unsqueeze(dim=0).unsqueeze(dim=0).repeat(2, 3, 1, 1, 1)",
            "+        input = torch.eye(6, device=device, dtype=dtype).unsqueeze(dim=0).unsqueeze(dim=0).repeat(2, 3, 6, 1, 1)",
            "res = f(input)",
            "assert (res[0] == res[1]).all()"
        ]
    },
    {
        "number": 4906,
        "comments": "",
        "commit_message": "Fix momentum and epsilon values (#19454)\n\nThe momentum value for PyTorch and TensorFlow batch normalization layers is not equivalent. The TensorFlow value should be (1 - pytorch_momentum) in order to ensure the correct updates are applied to the running mean and running variance calculations. We wouldn't observe a difference loading a pretrained model and performing inference, but evaluation outputs would change after some training steps.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFSegformerDecodeHead(TFSegformerPreTrainedModel):",
            "self.linear_fuse = tf.keras.layers.Conv2D(",
            "filters=config.decoder_hidden_size, kernel_size=1, use_bias=False, name=\"linear_fuse\"",
            ")",
            "-        self.batch_norm = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.1, name=\"batch_norm\")",
            "+        self.batch_norm = tf.keras.layers.BatchNormalization(epsilon=1e-5, momentum=0.9, name=\"batch_norm\")",
            "self.activation = tf.keras.layers.Activation(\"relu\")",
            "",
            "self.dropout = tf.keras.layers.Dropout(config.classifier_dropout_prob)"
        ]
    },
    {
        "number": 4907,
        "comments": "",
        "commit_message": "fix consistency CrossEntropyLoss in modeling_bart (#6265)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BartForSequenceClassification(PretrainedBartModel):",
            "",
            "loss = None",
            "if labels is not None:",
            "-            loss = F.cross_entropy(logits.view(-1, self.config.num_labels), labels.view(-1))",
            "+            loss_fct = CrossEntropyLoss()",
            "+            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))",
            "",
            "if not return_dict:",
            "output = (logits,) + outputs[1:]"
        ]
    },
    {
        "number": 4910,
        "comments": "",
        "commit_message": "Work on Special Activations Functions (#690)\n\n* work on special activations\n\n* Special activation functions fixed\n\n* Changelog modified\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def leaky_twice_relu6(x, alpha_low=0.2, alpha_high=0.2, name=\"leaky_relu6\"):",
            "",
            "\"\"\"",
            "",
            "-    if not (0 < alpha_high <= 1):",
            "+    if not isinstance(alpha_high, tf.Tensor) and not (0 < alpha_high <= 1):",
            "raise ValueError(\"`alpha_high` value must be in [0, 1]`\")",
            "",
            "-    if not (0 < alpha_low <= 1):",
            "+    if not isinstance(alpha_low, tf.Tensor) and not (0 < alpha_low <= 1):",
            "raise ValueError(\"`alpha_low` value must be in [0, 1]`\")",
            "",
            "with tf.name_scope(name, \"leaky_twice_relu6\") as name_scope:"
        ]
    },
    {
        "number": 4911,
        "comments": "",
        "commit_message": "Make BentoService.api decorator required for user model (#14)\n\n* lazily config service apis, avoid __init__ in BentoService base class\n\n* refactor - @property apis => get_service_api method\n\n* fix bentoml_config scope issue in model load\n\n* fix module save/load\n\n* remove 'predict' magic, enforce \"@api\" decorator\n\n* skip test_save_and_load_model_from_s3 for now\n\n* fix all linter errors\n\n* improve save/load from s3 location\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TfKerasModelArtifact(Artifact):",
            "def model_file_path(self, base_path):",
            "return os.path.join(base_path, self.name + self._model_extension)",
            "",
            "-    def pack(self, model):",
            "+    def pack(self, model):  # pylint:disable=arguments-differ",
            "self.model = model",
            "",
            "def get(self):",
            "return self.model",
            "",
            "-    def load(self, base_path):  # pylint:disable=arguments-differ",
            "-        from tensorflow.keras.models import load_model",
            "+    def load(self, base_path):",
            "+        try:",
            "+            from tensorflow.keras.models import load_model",
            "+        except ImportError:",
            "+            raise ImportError(\"tensorflow package is required to use TfKerasModelArtifact\")",
            "self.model = load_model(self.model_file_path(base_path))",
            "",
            "def save(self, base_path):"
        ]
    },
    {
        "number": 4913,
        "comments": "",
        "commit_message": "Adding simple HMC Kernel  (#690)\n\n* Adding HMC kernel\n\n* support cuda\n\n* add more tests\n\n* fix lint\n\n* relax tolerance on test_mcmc\n\n* add whitespace\n\n* correct test\n\n* adjust tolerance\n\n* address comments\n\n* remove clone kwarg\n\n* fix rng seed\n\n* adjust eps\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_mcmc_interface():",
            "samples.append(marginal.sample(data))",
            "sample_mean = torch.mean(torch.stack(samples), 0)",
            "sample_std = torch.std(torch.stack(samples), 0)",
            "-    assert_equal(sample_mean.data, torch.Tensor([0.0]), prec=5e-2)",
            "-    assert_equal(sample_std.data, torch.Tensor([1.0]), prec=5e-2)",
            "+    assert_equal(sample_mean.data, torch.Tensor([0.0]), prec=0.08)",
            "+    assert_equal(sample_std.data, torch.Tensor([1.0]), prec=0.08)"
        ]
    },
    {
        "number": 4917,
        "comments": "",
        "commit_message": "Add Cuda tests for pyro.distributions; fix errors (#297)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Multinomial(Distribution):",
            "if var.data.dim() == 1:",
            "return var.data",
            "# nested tensor arrays because of batches\"",
            "-        return var.data.numpy()[0]",
            "+        return var.data.cpu().numpy()[0]",
            "",
            "def analytic_mean(self, ps=None, n=None):",
            "ps, n = self._sanitize_input(ps, n)"
        ]
    },
    {
        "number": 4927,
        "comments": "",
        "commit_message": "[RLlib] CQL for HalfCheetah-Random-v0 + Hopper-Random-v0 + CQL Bug Fixes (#14243)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def cql_loss(policy: Policy, model: ModelV2,",
            "# CQL Loss (We are using Entropy version of CQL (the best version))",
            "rand_actions = convert_to_torch_tensor(",
            "torch.FloatTensor(actions.shape[0] * num_actions,",
            "-                          actions.shape[-1]).uniform_(action_low, action_high))",
            "+                          actions.shape[-1]).uniform_(action_low, action_high),",
            "+        policy.device)",
            "curr_actions, curr_logp = policy_actions_repeat(model, action_dist_class,",
            "obs, num_actions)",
            "next_actions, next_logp = policy_actions_repeat(model, action_dist_class,",
            "next_obs, num_actions)",
            "+",
            "curr_logp = curr_logp.view(actions.shape[0], num_actions, 1)",
            "next_logp = next_logp.view(actions.shape[0], num_actions, 1)"
        ]
    },
    {
        "number": 4929,
        "comments": "",
        "commit_message": "fixed a bunch of linting errors...\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SimplePoutineTests(TestCase):",
            "",
            "def guide():",
            "latent = pyro.sample(\"latent\",",
            "-                                DiagNormal(Variable(torch.zeros(1)),",
            "+                                 DiagNormal(Variable(torch.zeros(1)),",
            "5 * Variable(torch.ones(1))))",
            "-            #x_dist = DiagNormal(latent, Variable(torch.ones(1)))",
            "+            # x_dist = DiagNormal(latent, Variable(torch.ones(1)))",
            "return latent",
            "",
            "self.guide = guide",
            "",
            "-",
            "def test_trace_replay(self):",
            "\"\"\"",
            "some simple invariants on a single example, but woefully incomplete"
        ]
    },
    {
        "number": 4930,
        "comments": "",
        "commit_message": "`attempt_load()` deserialize fix (#8051)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def attempt_load(weights, device=None, inplace=True, fuse=True):",
            "# Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a",
            "model = Ensemble()",
            "for w in weights if isinstance(weights, list) else [weights]:",
            "-        ckpt = torch.load(attempt_download(w))",
            "-        ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float()  # FP32 model",
            "+        ckpt = torch.load(attempt_download(w), map_location=device)",
            "+        ckpt = (ckpt.get('ema') or ckpt['model']).float()  # FP32 model",
            "model.append(ckpt.fuse().eval() if fuse else ckpt.eval())  # fused or un-fused model in eval mode",
            "",
            "# Compatibility updates"
        ]
    },
    {
        "number": 4931,
        "comments": "",
        "commit_message": "Add AutoResumeTrainConfig to really do auto resuming from log dir (fix #660)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Trainer(object):",
            "session_creator (tf.train.SessionCreator):",
            "session_init (sessinit.SessionInit):",
            "\"\"\"",
            "+        assert isinstance(session_creator, tf.train.SessionCreator), session_creator",
            "+        assert isinstance(session_init, SessionInit), session_init",
            "session_init._setup_graph()",
            "",
            "logger.info(\"Creating the session ...\")"
        ]
    },
    {
        "number": 4936,
        "comments": "",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def count_nonzero(",
            "def _dtype_count_nonzero(a, axis, dtype):",
            "if dtype is None:",
            "return torch.count_nonzero(a, dim=axis)",
            "-        return torch.tensor(torch.count_nonzero(a, dim=axis),",
            "-                            dtype=ivy.as_native_dtype(dtype))",
            "+        return torch.tensor(",
            "+            torch.count_nonzero(a, dim=axis), dtype=ivy.as_native_dtype(dtype)",
            "+        )",
            "",
            "x = _dtype_count_nonzero(a, axis, dtype)",
            "if not keepdims:"
        ]
    },
    {
        "number": 4938,
        "comments": "",
        "commit_message": "skipping remainder and fixing torch and tf backends for remainder - deviates from standard\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def remainder(",
            ") -> torch.Tensor:",
            "x1, x2 = _cast_for_binary_op(x1, x2)",
            "ret = torch.remainder(x1, x2, out=out)",
            "-    ret[torch.isnan(ret)] = 0",
            "if ivy.exists(out):",
            "return ivy.inplace_update(out, ret)",
            "return ret"
        ]
    },
    {
        "number": 4941,
        "comments": "",
        "commit_message": "Fixed failing test for one_hot (#4192)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def one_hot(",
            "device: torch.device,",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "-    return torch.nn.functional.one_hot(indices.type(torch.int64), depth).to(device)",
            "+    return torch.nn.functional.one_hot(indices.to(torch.int64), depth).to(",
            "+        device, indices.dtype",
            "+    )",
            "",
            "",
            "def shape(x: torch.Tensor, as_array: bool = False) -> Union[ivy.Shape, ivy.Array]:"
        ]
    },
    {
        "number": 4942,
        "comments": "",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def linear_transform(",
            "new_order: List[int] = perm.tolist()",
            "inv_order: List[int] = perm_inv.tolist()",
            "",
            "-    feature_sizes = torch.tensor(inp_size[0:dim] + inp_size[dim + 1::])",
            "+    feature_sizes = torch.tensor(inp_size[0:dim] + inp_size[dim + 1 : :])",
            "num_features: int = int(torch.prod(feature_sizes).item())",
            "",
            "inp_permute = inp.permute(new_order)"
        ]
    },
    {
        "number": 4944,
        "comments": "",
        "commit_message": "Fix doctest for `TFDeiTForImageClassification` (#19173)\n\n* Fix doctest for TFDeiTForImageClassification\n\n* Remove unnecessary tf.random.set_seed\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFDeiTForImageClassification(TFDeiTPreTrainedModel, TFSequenceClassificati",
            ">>> # model predicts one of the 1000 ImageNet classes",
            ">>> predicted_class_idx = tf.math.argmax(logits, axis=-1)[0]",
            ">>> print(\"Predicted class:\", model.config.id2label[int(predicted_class_idx)])",
            "-        Predicted class: ptarmigan",
            "+        Predicted class: little blue heron, Egretta caerulea",
            "```\"\"\"",
            "return_dict = return_dict if return_dict is not None else self.config.use_return_dict"
        ]
    },
    {
        "number": 4946,
        "comments": "",
        "commit_message": "Future 4/n: test & legacy in test/ folder (#13295)\n\n* move: legacy >> test/\n\n* move: tests >> test/\n\n* rename unittests\n\n* update CI\n\n* tests4pl\n\n* tests_pytorch\n\n* proxi\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* ci\n\n* link\n\n* cli\n\n* standalone\n\n* fixing\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* .\n\n* Apply suggestions from code review\n\nCo-authored-by: Akihiro Nitta <nitta@akihironitta.com>\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* alone\n\n* test -> tests\n\n* Standalone fixes\n\n* ci\n\n* Update\n\n* More fixes\n\n* Fix coverage\n\n* Fix mypy\n\n* mypy\n\n* Empty-Commit\n\n* Fix\n\n* mypy just for pl\n\n* Fix standalone\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Akihiro Nitta <nitta@akihironitta.com>\nCo-authored-by: Carlos Mochol\u00ed <carlossmocholi@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def init_checkpoint_callback(logger):",
            "",
            "",
            "def pl_multi_process_test(func):",
            "-    \"\"\"Wrapper for running multi-processing tests.\"\"\"",
            "+    \"\"\"Wrapper for running multi-processing tests_pytorch.\"\"\"",
            "",
            "@functools.wraps(func)",
            "def wrapper(*args, **kwargs):"
        ]
    },
    {
        "number": 4948,
        "comments": "",
        "commit_message": "Input alias deprecation warning fixes - related to issue #479 (#480)\n\n* __init__ import error message repositioned at a more appropriate location\n\n* gitignore updated with venv environment\n\n* Typo fixed in tensorlayer.layers.convolution.py\n\n* Deprecation warning added for tl.layer.deconv2d with backward compatibility restored - Issue #479\n\n* Deprecation warning added for Layer API change: `layer` argument changed to `prev_layer` - Issue #479\nAdditional Modification using the syntax super for inheritance - more pythonic\n\n* test layers extend with argument names precised\n\n* tl.layers.core.py forgotten Classes with deprecation\n\n* Error fix in deprecation warning tl.layers.spatial_transformer.py\n\n* Test refactored and fix with arguments missing added\n\n* ReshapeLayer error fix\n\n* test_layers_normalization argument name missing fixed\n\n* error in tl.layers.ReshapeLayer test if shape is not empty fixed\n\n* test_layers_special_activation missing argument name fixed\n\n* test_layers_stack missing argument name fixed\n\n* test_layers_super_resolution missing argument name fixed\n\n* test_models missing argument name fixed\n\n* Formating error fixed\n\n* Decorator for deprecated argument added\n\n* Deprecation API Change - Use of newly implemented decorator. Docstring updated\n\n* Codacy Issues Fix\n\n* Unnecessary PR changes removed - PR Cleaned & Refactored\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "net.print_layers()",
            "net.print_params(False)",
            "",
            "if len(net.all_layers) != 6:",
            "-    raise Exception(\"layers dont match\")",
            "+    raise Exception(\"layers do not match\")",
            "",
            "if len(net.all_params) != 12:",
            "-    raise Exception(\"params dont match\")",
            "+    raise Exception(\"params do not match\")",
            "",
            "if net.count_params() != 60560:",
            "-    raise Exception(\"params dont match\")",
            "+    raise Exception(\"params do not match\")"
        ]
    },
    {
        "number": 4952,
        "comments": "",
        "commit_message": "fix #11724 (#11897)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFMarianSinusoidalPositionalEmbedding(tf.keras.layers.Layer):",
            "position_enc = np.array(",
            "[[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]",
            ")",
            "+        table = np.zeros_like(position_enc)",
            "# index 0 is all zero",
            "-        position_enc[:, 0 : dim // 2] = np.sin(position_enc[:, 0::2])",
            "-        position_enc[:, dim // 2 :] = np.cos(position_enc[:, 1::2])",
            "+        table[:, 0 : dim // 2] = np.sin(position_enc[:, 0::2])",
            "+        table[:, dim // 2 :] = np.cos(position_enc[:, 1::2])",
            "# convert to tensor",
            "-        table = tf.convert_to_tensor(position_enc)",
            "+        table = tf.convert_to_tensor(table)",
            "tf.stop_gradient(table)",
            "return table"
        ]
    },
    {
        "number": 4953,
        "comments": "",
        "commit_message": "Add docs for MobileViT (#1430)\n\n* Fix typing in MobileViT\n\n* Fix bibtex citation\n\n* Add docs for MobileViT\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PreNorm(nn.Module):",
            "self.norm = nn.LayerNorm(dim)",
            "self.fn = fn",
            "",
            "-    def forward(self, x: torch.Tensor, **kwargs) -> nn.Module:",
            "+    def forward(self, x: torch.Tensor, **kwargs) -> torch.Tensor:",
            "return self.fn(self.norm(x), **kwargs)"
        ]
    },
    {
        "number": 4957,
        "comments": "",
        "commit_message": "Several micro optimizations (#4833)\n\n* benchmark transfers\n\n* create tensors directl on device when possible\n\n* fix\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SmoothGradient(SaliencyInterpreter):",
            "def forward_hook(module, inputs, output):",
            "# Random noise = N(0, stdev * (max-min))",
            "scale = output.detach().max() - output.detach().min()",
            "-            noise = torch.randn(output.shape).to(output.device) * stdev * scale",
            "+            noise = torch.randn(output.shape, device=output.device) * stdev * scale",
            "",
            "# Add the random noise",
            "output.add_(noise)"
        ]
    },
    {
        "number": 4958,
        "comments": "",
        "commit_message": "Changed dtype to depend on spots, textbook title fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def barrier_price(*,",
            "raise ValueError('At most one of continuous_dividends and cost of carries '",
            "'may be supplied')",
            "with tf.name_scope(name or \"barrier_price\"):",
            "-    strikes = tf.convert_to_tensor(strikes, dtype=dtype, name=\"strikes\")",
            "-    dtype = strikes.dtype",
            "spots = tf.convert_to_tensor(spots, dtype=dtype, name=\"spots\")",
            "+    dtype = spots.dtype",
            "+    strikes = tf.convert_to_tensor(strikes, dtype=dtype, name=\"strikes\")",
            "volatilities = tf.convert_to_tensor(",
            "volatilities, dtype=dtype, name=\"volatilities\")",
            "expiries = tf.convert_to_tensor(expiries, dtype=dtype, name=\"expiries\")"
        ]
    },
    {
        "number": 4966,
        "comments": "",
        "commit_message": "Logger connector re-design `_Metadata.reduce_fx` fixes. (#7890)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _setup_ddp(rank, worldsize):",
            "def _ddp_test_fn(rank, worldsize):",
            "_setup_ddp(rank, worldsize)",
            "tensor = torch.tensor([1.0])",
            "-    sync = _Sync(sync_ddp_if_available, should=True, op=torch.distributed.ReduceOp.SUM)",
            "+    sync = _Sync(sync_ddp_if_available, should=True, op='SUM')",
            "actual = sync(tensor)",
            "assert actual.item() == dist.get_world_size(), \"Result-Log does not work properly with DDP and Tensors\""
        ]
    },
    {
        "number": 4969,
        "comments": "",
        "commit_message": "Add set_epoch and fix args in DDP-series example (#1076)\n\nAdd set_epoch for shuffling inputs, fix arg order\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Trainer:",
            "return loss.item()",
            "",
            "def _run_epoch(self, epoch: int, dataloader: DataLoader, train: bool = True):",
            "+        self.dataloader.sampler.set_epoch(epoch)",
            "for iter, (source, targets) in enumerate(dataloader):",
            "step_type = \"Train\" if train else \"Eval\"",
            "source = source.to(self.local_rank)"
        ]
    },
    {
        "number": 4971,
        "comments": "",
        "commit_message": "fix qm9 dataset\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class QM9(InMemoryDataset):",
            "bond_idx += 2 * [self.bonds[bond.GetBondType()]]",
            "",
            "edge_index = torch.tensor([row, col], dtype=torch.long)",
            "-            edge_attr = F.one_hot(torch.tensor(bond_idx),",
            "-                                  num_classes=len(self.bonds)).to(torch.float)",
            "+            edge_attr = F.one_hot(",
            "+                torch.tensor(bond_idx),",
            "+                num_classes=len(self.bonds)).to(torch.float)",
            "edge_index, edge_attr = coalesce(edge_index, edge_attr, N, N)",
            "",
            "y = target[i].unsqueeze(0)"
        ]
    },
    {
        "number": 4973,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NlvrCoverageSemanticParserTest(ModelTestCase):",
            "('e -> 6', True, None)]",
            "# Of the actions above, those at indices 0 and 4 are on the agenda, and there are padding",
            "# indices at the end.",
            "-        test_agenda = Variable(torch.Tensor([[0], [4], [-1], [-1]]))",
            "+        test_agenda = torch.Tensor([[0], [4], [-1], [-1]])",
            "checklist_info = self.model._get_checklist_info(test_agenda, all_actions)",
            "target_checklist, terminal_actions, checklist_mask = checklist_info",
            "assert_almost_equal(target_checklist.data.numpy(), [[1], [0], [1]])"
        ]
    },
    {
        "number": 4977,
        "comments": "",
        "commit_message": "small fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DistributedPGModel(object):",
            "# TODO write summaries",
            "# self.summary_writer = tf.summary.FileWriter('log' + \"_%d\" % self.task_index)",
            "if not self.optimizer:",
            "-                self.optimizer = tf.train.AdamOptimizer(self.alpha)",
            "+                self.optimizer = tf.train.AdamOptimizer(self.learning_rate)",
            "",
            "else:",
            "optimizer_cls = get_function(self.optimizer)",
            "-                self.optimizer = optimizer_cls(self.alpha, *self.optimizer_args, **self.optimizer_kwargs)",
            "+                self.optimizer = optimizer_cls(self.learning_rate, *self.optimizer_args, **self.optimizer_kwargs)",
            "",
            "self.optimize_op = tf.group(self.optimizer.apply_gradients(grad_var_list),",
            "global_step_inc)"
        ]
    },
    {
        "number": 4981,
        "comments": "",
        "commit_message": "upgrade to pytorch 1.2 (#3182)\n\n* first attempt at pytorch 1.2\n\n* explicit is better than implicit\n\n* more explicit\n\n* attempt to fix flaky tests\n\n* pylint\n\n* no disable dropout\n\n* disable dropout by default\n\n* restore dropout, don't deepcopy\n\n* change batch size for biaffine_dependency_parser_multilang_test, maybe that will make it pass? :(\n\n* try batch size 10\n\n* ignore bad gradient parameter\n\n* cleanup\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BiaffineDependencyParser(Model):",
            "attended_arcs = attended_arcs + torch.diag(attended_arcs.new(mask.size(1)).fill_(-numpy.inf))",
            "# Mask padded tokens, because we only want to consider actual words as heads.",
            "if mask is not None:",
            "-            minus_mask = (1 - mask).byte().unsqueeze(2)",
            "+            minus_mask = (1 - mask).to(dtype=torch.bool).unsqueeze(2)",
            "attended_arcs.masked_fill_(minus_mask, -numpy.inf)",
            "",
            "# Compute the heads greedily."
        ]
    },
    {
        "number": 4982,
        "comments": "",
        "commit_message": "Fixes to device in augmentations (#1546)\n\n* add RandomPosterize test and fix device\n\n* fix other device transfer issues\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PosterizeGenerator(RandomGeneratorBase):",
            "def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, torch.Tensor]:  # type:ignore",
            "batch_size = batch_shape[0]",
            "_common_param_check(batch_size, same_on_batch)",
            "-        _device, _dtype = _extract_device_dtype([self.bits if isinstance(self.bits, torch.Tensor) else None])",
            "+        _device, _ = _extract_device_dtype([self.bits if isinstance(self.bits, torch.Tensor) else None])",
            "bits_factor = _adapted_rsampling((batch_size,), self.bit_sampler, same_on_batch)",
            "return dict(bits_factor=bits_factor.to(device=_device, dtype=torch.int32))"
        ]
    },
    {
        "number": 4988,
        "comments": "",
        "commit_message": "fix: change load/save logic in ner\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DstcSlotFillingNetwork(Inferable, Trainable):",
            "self._slot_vals = json.load(f)",
            "",
            "self._ner_network = ner_network",
            "-        self.load()",
            "-",
            "",
            "@overrides",
            "def load(self):",
            "# Check presence of the model files",
            "-        if tf.train.get_checkpoint_state(str(path)) is not None:",
            "+        if tf.train.get_checkpoint_state(str(self.ser_path)) is not None:",
            "print(\"\\n:: initializing `{}` from saved\"\\",
            ".format(self.__class__.__name__))",
            "self._ner_network.load()"
        ]
    },
    {
        "number": 4992,
        "comments": "",
        "commit_message": "Avoid accessing .dataset of a DataLoader in Trainer (#16451)\n\n* Avoid accessing .dataset of a dataloader\n\n* style\n\n* fix\n\n* cleaning up, reverting some misunderstandings\n\n* black\n\n* add train_dataset argument to get_train_dataloader, and fix other instances of length checks\n\n* flake8\n\n* address comments\n\n* fix bug\n\n* cleanup\n\n* add test\n\n* Update tests/trainer/test_trainer.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* under torch\n\n* merge\n\n* stylistic suggestion\n\nCo-authored-by: Sander Land <sander@chatdesk.com>\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ProgressCallback(TrainerCallback):",
            "self.current_step = state.global_step",
            "",
            "def on_prediction_step(self, args, state, control, eval_dataloader=None, **kwargs):",
            "-        if state.is_local_process_zero and has_length(eval_dataloader.dataset):",
            "+        if state.is_local_process_zero and has_length(eval_dataloader):",
            "if self.prediction_bar is None:",
            "self.prediction_bar = tqdm(total=len(eval_dataloader), leave=self.training_bar is None)",
            "self.prediction_bar.update(1)"
        ]
    },
    {
        "number": 5002,
        "comments": "",
        "commit_message": "Fix bugs in tensorrt and openvino\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def get_outputs_sizes_torch(",
            "",
            "",
            "def create_model_inputs_torch(",
            "-    batch_size: int, input_infos: List[InputInfo]",
            "+    input_infos: List[InputInfo],",
            ") -> List[torch.Tensor]:",
            "input_tensors = (",
            "-        torch.randn((batch_size, *input_info.size))",
            "+        torch.randn(*input_info.size)",
            "if input_info.dtype is DataType.FLOAT32",
            "else torch.randint(",
            "-            size=(batch_size, *input_info.size),",
            "+            size=input_info.size,",
            "low=input_info.min_value or 0,",
            "high=input_info.max_value or 100,",
            ")"
        ]
    },
    {
        "number": 5004,
        "comments": "",
        "commit_message": "GH-316: minor fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LanguageModel(nn.Module):",
            "hidden = hidden",
            "",
            "input = torch.tensor(self.dictionary.get_idx_for_item(prefix[-1])).unsqueeze(0).unsqueeze(0)",
            "-            if torch.cuda.is_available():",
            "-                input = input.cuda()",
            "",
            "for i in range(number_of_characters):",
            "+",
            "+                if torch.cuda.is_available():",
            "+                    input = input.cuda()",
            "+",
            "prediction, _, hidden = self.forward(input, hidden)",
            "prediction /= temperature",
            "word_weights = prediction.squeeze().data.div(1.0).exp().cpu()"
        ]
    },
    {
        "number": 5005,
        "comments": "",
        "commit_message": "[transformer] Format & Test Refactoring (#1325)\n\n* try PyTorch custom TestCase class\n\n* revert\n\n* initial working example\n\n* update\n\n* data utils\n\n* fix imports\n\n* hardcode backend to nccl\n\n* fix signature\n\n* fix typo\n\n* mapping\n\n* set device\n\n* init\n\n* refactor x entropy\n\n* remove unused import & destroy model parallel\n\n* refactor random\n\n* fix test\n\n* remove migrated tests\n\n* refactor\n\n* init\n\n* separate affine weight init\n\n* init model parallel\n\n* split more\n\n* weight init fix part 1\n\n* use cpu init for consistency btwn native and tensor parallel\n\n* black\n\n* add col parallel\n\n* use a 3D tensor of square matrix for column parallel linear\n\n* skip the failing cases\n\n* migrate layers test\n\n* pipeline parallel forward/backward\n\n* fix typo\n\n* fix typo\n\n* fix\n\n* fix pipeline world size\n\n* black\n\n* rm `run_pipeline_parallel_test` in favor of test_pipeline_parallel_fwd_bwd.py\n\n* stop logging\n\n* set log level\n\n* black\n\n* license and format\n\n* fix\n\n* skip tf32 as matrices are small\n\n* remove potentially inappropriate license\n\n* Apply suggestions from code review\n\n* remove `TODO` comment\n\n* `torch.testing.assert_allclose` -> `torch.testing.assert_close`\n\n* remove comment-outs\n\n* remote unused import\n\n* minor fix\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestBatchSamplerBehavior(unittest.TestCase):",
            "_micro_batch_size=_micro_batch_size,",
            "_global_batch_size=global_batch_size,",
            "))",
            "-            # print(batch)",
            "-            # print(microbatches)",
            "self.assertEqual(len(microbatches), global_batch_size // _micro_batch_size)",
            "self.assertEqual(len(microbatches[0][0]), _micro_batch_size)"
        ]
    },
    {
        "number": 5006,
        "comments": "",
        "commit_message": "fix tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TransformerEmbeddings(TransformerBaseEmbeddings):",
            "def _forward_tensors(self, tensors) -> Dict[str, torch.Tensor]:",
            "return self.forward(**tensors)",
            "",
            "-    def export_onnx(self, path: Union[str, Path], example_sentences: List[Sentence], **kwargs) -> TransformerOnnxEmbeddings:",
            "+    def export_onnx(",
            "+        self, path: Union[str, Path], example_sentences: List[Sentence], **kwargs",
            "+    ) -> TransformerOnnxEmbeddings:",
            "\"\"\"",
            "Export TransformerEmbeddings to OnnxFormat.",
            ":param example_sentences: a list of sentences that will be used for tracing. It is recommended to take 2-4"
        ]
    },
    {
        "number": 5007,
        "comments": "",
        "commit_message": "fixed failing core.container unit tests.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def random_uniform(low: float = 0.0, high: float = 1.0, shape: Optional[List[int",
            "true_shape: List[int] = []",
            "else:",
            "true_shape: List[int] = shape",
            "-    return _torch.rand(true_shape, device=default_device(dev).replace('gpu', 'cuda')) * rand_range + low",
            "+    return _torch.rand(true_shape, device=default_device(dev)) * rand_range + low",
            "",
            "",
            "def random_normal(mean: float = 0.0, std: float = 1.0, shape: Optional[List[int]] = None, dev: ivy.Device = None):"
        ]
    },
    {
        "number": 5013,
        "comments": "",
        "commit_message": "Fixed minor error while calculating span accuracy (#2923)\n\n* Fixed minor error while calculating span accuracy\n\n* Added checks for input shape in BooleanAccuracy\n\n* Fixed label shape in QaNet\n\n* Use f-string\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class QaNet(Model):",
            "self._span_start_accuracy(span_start_logits, span_start.squeeze(-1))",
            "loss += nll_loss(util.masked_log_softmax(span_end_logits, passage_mask), span_end.squeeze(-1))",
            "self._span_end_accuracy(span_end_logits, span_end.squeeze(-1))",
            "-            self._span_accuracy(best_span, torch.stack([span_start, span_end], -1))",
            "+            self._span_accuracy(best_span, torch.cat([span_start, span_end], -1))",
            "output_dict[\"loss\"] = loss",
            "",
            "# Compute the EM and F1 on SQuAD and add the tokenized input to the output."
        ]
    },
    {
        "number": 5014,
        "comments": "",
        "commit_message": "fixed bugs chatbot\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def chat():",
            "model = ChatBotModel(True, batch_size=1)",
            "model.build_graph()",
            "",
            "-    # saver = tf.train.import_meta_graph('checkpoints/chatbot-30.meta')",
            "-",
            "saver = tf.train.Saver()",
            "",
            "with tf.Session() as sess:"
        ]
    },
    {
        "number": 5018,
        "comments": "",
        "commit_message": "pytorch 1.0 (#2165)\n\n* fixes\n\n* allenlp\n\n* fix bug\n\n* reduce=False -> reduction='none'\n\n* fix _sparse_mask\n\n* remove print statement\n\n* fix more tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def get_dropout_mask(dropout_probability: float, tensor_for_masking: torch.Tenso",
            "This scaling ensures expected values and variances of the output of applying this mask",
            "and the original tensor are the same.",
            "\"\"\"",
            "-    binary_mask = tensor_for_masking.new_tensor(torch.rand(tensor_for_masking.size()) > dropout_probability)",
            "+    binary_mask = (torch.rand(tensor_for_masking.size()) > dropout_probability).to(tensor_for_masking.device)",
            "# Scale mask by 1/keep_prob to preserve output statistics.",
            "dropout_mask = binary_mask.float().div(1.0 - dropout_probability)",
            "return dropout_mask"
        ]
    },
    {
        "number": 5021,
        "comments": "",
        "commit_message": "Various fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TextCNN(object):",
            "# Final (unnormalized) scores and predictions",
            "with tf.name_scope(\"output\"):",
            "W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")",
            "-            b = tf.Variable(tf.constant(0.1, shape=[num_classes]))",
            "+            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")",
            "self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")",
            "self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")"
        ]
    },
    {
        "number": 5023,
        "comments": "",
        "commit_message": "fix rnn inference\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NaiveRNNDP(AbsSVS):",
            "before_outs.transpose(1, 2)",
            ").transpose(1, 2)",
            "",
            "-        return after_outs, None, None  # outs, probs, att_ws",
            "+        return dict(",
            "+            feat_gen=after_outs[0], prob=None, att_w=None",
            "+        )  # outs, probs, att_ws",
            "",
            "def _integrate_with_spk_embed(",
            "self, hs: torch.Tensor, spembs: torch.Tensor"
        ]
    },
    {
        "number": 5024,
        "comments": "",
        "commit_message": "define and apply YAPF formatting  (#1039)\n\n* define YAPF\n\n* yapf kornia -rip\n\n* yapf examples -rip\n\n* yapf test -rip\n\n* Update CONTRIBUTING.rst\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n\n* update config\n\n* fix 4lint\n\n* mypy\n\n* mypy\n\n* mypy\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class UnsharpMask(nn.Module):",
            "torch.Size([2, 4, 5, 5])",
            "\"\"\"",
            "",
            "-    def __init__(self, kernel_size: Tuple[int, int],",
            "-                 sigma: Tuple[float, float],",
            "-                 border_type: str = 'reflect') -> None:",
            "+    def __init__(self, kernel_size: Tuple[int, int], sigma: Tuple[float, float], border_type: str = 'reflect') -> None:",
            "super(UnsharpMask, self).__init__()",
            "self.kernel_size: Tuple[int, int] = kernel_size",
            "self.sigma: Tuple[float, float] = sigma"
        ]
    },
    {
        "number": 5025,
        "comments": "",
        "commit_message": "[MaskFormer] Add support for ResNet backbone (#20483)\n\n* Add SwinBackbone\n\n* Add hidden_states_before_downsampling support\n\n* Fix Swin tests\n\n* Improve conversion script\n\n* Add id2label mappings\n\n* Add vistas mapping\n\n* Update comments\n\n* Fix backbone\n\n* Improve tests\n\n* Extend conversion script\n\n* Add Swin conversion script\n\n* Fix style\n\n* Revert config attribute\n\n* Remove SwinBackbone from main init\n\n* Remove unused attribute\n\n* Use encoder for ResNet backbone\n\n* Improve conversion script and add integration test\n\n* Apply suggestion\n\nCo-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RegNetPreTrainedModel(PreTrainedModel):",
            "main_input_name = \"pixel_values\"",
            "supports_gradient_checkpointing = True",
            "",
            "+    # Copied from transformers.models.resnet.modeling_resnet.ResNetPreTrainedModel._init_weights",
            "def _init_weights(self, module):",
            "if isinstance(module, nn.Conv2d):",
            "nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")"
        ]
    },
    {
        "number": 5029,
        "comments": "",
        "commit_message": "fix torch.nonzero usage in pytorch 1.5 (#2602)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BBoxHead(nn.Module):",
            "",
            "bboxes_list = []",
            "for i in range(len(img_metas)):",
            "-            inds = torch.nonzero(rois[:, 0] == i).squeeze(dim=1)",
            "+            inds = torch.nonzero(",
            "+                rois[:, 0] == i, as_tuple=False).squeeze(dim=1)",
            "num_rois = inds.numel()",
            "",
            "bboxes_ = rois[inds, 1:]"
        ]
    },
    {
        "number": 5030,
        "comments": "",
        "commit_message": "Implement VitsAudioConfig (#1556)\n\n* Implement VitsAudioConfig\n\n* Update VITS LJSpeech recipe\n\n* Update VITS VCTK recipe\n\n* Make style\n\n* Add missing decorator\n\n* Add missing param\n\n* Make style\n\n* Update recipes\n\n* Fix test\n\n* Bug fix\n\n* Exclude tests folder\n\n* Make linter\n\n* Make style\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SSIMLoss(torch.nn.Module):",
            "",
            "if ssim_loss.item() < 0.0:",
            "print(f\" > SSIM loss is out-of-range {ssim_loss.item()}, setting it 0.0\")",
            "-            ssim_loss =  torch.tensor([0.0])",
            "+            ssim_loss = torch.tensor([0.0])",
            "",
            "return ssim_loss"
        ]
    },
    {
        "number": 5036,
        "comments": "",
        "commit_message": "Fix RaggedTensorSpec loading when reviving model form SavedModel.\n\nA bunch of arguments were not passed to the __init__ making the spec incorrect.\n\nPiperOrigin-RevId: 382692661\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def infer_inputs_from_restored_call_function(fn):",
            "if isinstance(x, tf.SparseTensorSpec):",
            "return tf.SparseTensorSpec(common_shape, x.dtype)",
            "elif isinstance(x, tf.RaggedTensorSpec):",
            "-      return tf.RaggedTensorSpec(common_shape, x.dtype)",
            "+      return tf.RaggedTensorSpec(",
            "+          common_shape,",
            "+          x.dtype,",
            "+          ragged_rank=x.ragged_rank,",
            "+          row_splits_dtype=x.row_splits_dtype,",
            "+          flat_values_spec=x.flat_values_spec)",
            "return tf.TensorSpec(common_shape, x.dtype, x.name)",
            "",
            "spec = fn.concrete_functions[0].structured_input_signature"
        ]
    },
    {
        "number": 5037,
        "comments": "",
        "commit_message": "Fix masked losses.\n\nMasked losses with the default \"auto\" reduction were giving outputs that are\ninconsistent with what you would get from a ragged input. Masked and Ragged are two different representations of the same thing (when it can be represented as ragged).  These should match.\n\nThe (input_type='masked', reduction='auto') case fails (doesn't match the ragged case) before this change.\n\nThe existing tests, where I'm changing the expected values are because I believe the old values are incorrect.\n\nPiperOrigin-RevId: 493003876\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def apply_mask(y_p, sw, mask):",
            "if mask is not None:",
            "mask = tf.cast(mask, y_p.dtype)",
            "if sw is not None:",
            "+            sw = tf.cast(sw, mask.dtype)",
            "mask, _, sw = squeeze_or_expand_dimensions(mask, sample_weight=sw)",
            "sw *= mask",
            "else:"
        ]
    },
    {
        "number": 5039,
        "comments": "",
        "commit_message": "Input alias deprecation warning fixes - related to issue #479 (#480)\n\n* __init__ import error message repositioned at a more appropriate location\n\n* gitignore updated with venv environment\n\n* Typo fixed in tensorlayer.layers.convolution.py\n\n* Deprecation warning added for tl.layer.deconv2d with backward compatibility restored - Issue #479\n\n* Deprecation warning added for Layer API change: `layer` argument changed to `prev_layer` - Issue #479\nAdditional Modification using the syntax super for inheritance - more pythonic\n\n* test layers extend with argument names precised\n\n* tl.layers.core.py forgotten Classes with deprecation\n\n* Error fix in deprecation warning tl.layers.spatial_transformer.py\n\n* Test refactored and fix with arguments missing added\n\n* ReshapeLayer error fix\n\n* test_layers_normalization argument name missing fixed\n\n* error in tl.layers.ReshapeLayer test if shape is not empty fixed\n\n* test_layers_special_activation missing argument name fixed\n\n* test_layers_stack missing argument name fixed\n\n* test_layers_super_resolution missing argument name fixed\n\n* test_models missing argument name fixed\n\n* Formating error fixed\n\n* Decorator for deprecated argument added\n\n* Deprecation API Change - Use of newly implemented decorator. Docstring updated\n\n* Codacy Issues Fix\n\n* Unnecessary PR changes removed - PR Cleaned & Refactored\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "with tf.Graph().as_default() as graph:",
            "# vgg1.restore_params(sess)",
            "",
            "if len(vgg1.all_layers) != 21:",
            "-        raise Exception(\"layers dont match\")",
            "+        raise Exception(\"layers do not match\")",
            "",
            "if len(vgg1.all_params) != 30:",
            "-        raise Exception(\"params dont match\")",
            "+        raise Exception(\"params do not match\")"
        ]
    },
    {
        "number": 5042,
        "comments": "",
        "commit_message": "[CI/Data] Fix `test_huggingface` failing (#30965)\n\nThe 'emotion' dataset used before has been removed from its upload location. This PR replaces it with an alternative dataset.\n",
        "label": "",
        "answer": "no",
        "change": [
            "from ray.tests.conftest import *  # noqa",
            "",
            "",
            "def test_huggingface(ray_start_regular_shared):",
            "-    data = datasets.load_dataset(\"emotion\")",
            "+    data = datasets.load_dataset(\"tweet_eval\", \"emotion\")",
            "",
            "assert isinstance(data, datasets.DatasetDict)"
        ]
    },
    {
        "number": 5043,
        "comments": "",
        "commit_message": "Fixes wrong imports\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main():",
            "np.random.seed(args.seed)",
            "",
            "if args.backend == \"pytorch\":",
            "-        from tts.pytorch.tts_pytorch import train",
            "+        fromespnet.lmpytorch.tts_pytorch import train",
            "train(args)",
            "else:",
            "raise NotImplementedError(\"Only pytorch is supported.\")"
        ]
    },
    {
        "number": 5045,
        "comments": "",
        "commit_message": "upgrade detection / some v2 compat fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SyncMultiGPUReplicatedBuilder(DataParallelBuilder):",
            "Copy values of variables on GPU 0 to other GPUs.",
            "\"\"\"",
            "# literally all variables, because it's better to sync optimizer-internal variables as well",
            "-        all_vars = tf.global_variables() + tf.local_variables()",
            "+        all_vars = tfv1.global_variables() + tfv1.local_variables()",
            "var_by_name = {v.name: v for v in all_vars}",
            "-        trainable_names = {x.name for x in tf.trainable_variables()}",
            "+        trainable_names = {x.name for x in tfv1.trainable_variables()}",
            "post_init_ops = []",
            "",
            "def log_failure(name, reason):"
        ]
    },
    {
        "number": 5048,
        "comments": "",
        "commit_message": "Add doc (#3125)\n\n* add doc\n\n* fix some typos\n\n* fix some doc\n\n* fix property doc\n\n* update doc\n\n* complete free anchor doc\n",
        "label": "",
        "answer": "no",
        "change": [
            "import torch",
            "",
            "",
            "def cast_tensor_type(inputs, src_type, dst_type):",
            "+    \"\"\"Recursively convert Tensor in inputs from src_type to dst_type.",
            "+",
            "+    Args:",
            "+        inputs: Inputs that to be casted.",
            "+        src_type (torch.dtype): Source type..",
            "+        dst_type (torch.dtype): Destination type.",
            "+",
            "+    Returns:",
            "+        The same type with inputs, but all contained Tensors have been cast.",
            "+    \"\"\"",
            "if isinstance(inputs, torch.Tensor):",
            "return inputs.to(dst_type)",
            "elif isinstance(inputs, str):"
        ]
    },
    {
        "number": 5052,
        "comments": "",
        "commit_message": "fix typing callable in load storage (#1768)\n\n* fix typing callable in load storage\n\n* missing import\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def _torch_solve_cast(input: torch.Tensor, A: torch.Tensor) -> Tuple[torch.Tenso",
            "if dtype not in (torch.float32, torch.float64):",
            "dtype = torch.float32",
            "",
            "-    out = solve(A.to(dtype), input.to(dtype))",
            "+    out = torch.linalg.solve(A.to(dtype), input.to(dtype))",
            "",
            "return (out.to(input.dtype), out)"
        ]
    },
    {
        "number": 5056,
        "comments": "",
        "commit_message": "Fix test\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_calculate_plot_attention():",
            "",
            "train_args = make_train_args(report_cer=True)",
            "",
            "-    model, x, ilens, y, data = prepare(train_args)",
            "+    model, x, ilens, y, data, uttid_list = prepare(train_args)",
            "",
            "attn_dict = model.calculate_all_attentions(x[0:1], ilens[0:1], y[0:1])",
            "-    plot.plot_multi_head_attention(data, attn_dict, \"/tmp/espnet-test\")",
            "+    plot.plot_multi_head_attention(data, uttid_list, attn_dict, \"/tmp/espnet-test\")",
            "",
            "",
            "def test_invalid_input_layer_type():"
        ]
    },
    {
        "number": 5062,
        "comments": "",
        "commit_message": "fix torch frontend test `swapdims` (#3026)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_torch_permute(",
            "),",
            "native_array=st.booleans(),",
            ")",
            "-@handle_cmd_line_args",
            "def test_torch_swapdims(",
            "dtype_and_values,",
            "dim0,"
        ]
    },
    {
        "number": 5064,
        "comments": "",
        "commit_message": "Fix tensorflow bugs\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TensorflowBaseInferenceLearner(BaseInferenceLearner, ABC):",
            "return \".npy\"",
            "",
            "def free_gpu_memory(self):",
            "-        tf.keras.clear_session()",
            "+        tf.keras.backend.clear_session()",
            "self._is_gpu_ready = False",
            "",
            "def set_model_on_gpu(self):"
        ]
    },
    {
        "number": 5070,
        "comments": "",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def run_timeline(sess, ops, debug_name, feed_dict={}, timeline_dir=None):",
            "if timeline_dir:",
            "from tensorflow.python.client import timeline",
            "",
            "-        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)",
            "-        run_metadata = tf.RunMetadata()",
            "+        run_options = tf1.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)",
            "+        run_metadata = tf1.RunMetadata()",
            "start = time.time()",
            "fetches = sess.run(",
            "ops,"
        ]
    },
    {
        "number": 5072,
        "comments": "",
        "commit_message": "fix dynamic quantize test for torch < 1.5\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_dynamic_quantization(train_dic, recog_dic, quantize_dic):",
            "train_args = get_default_train_args(**train_dic)",
            "recog_args = get_default_recog_args(**recog_dic)",
            "",
            "+    if not is_torch_1_5_plus:",
            "+        q_dtype = torch.qint8",
            "+    else:",
            "+        q_dtype = quantize_dic[\"mod\"]",
            "+",
            "model = E2E(idim, odim, train_args)",
            "model = torch.quantization.quantize_dynamic(",
            "-        model, quantize_dic[\"mod\"], dtype=quantize_dic[\"dtype\"]",
            "+        model, q_dtype, dtype=quantize_dic[\"dtype\"]",
            ")",
            "",
            "beam_search = BeamSearchTransducer("
        ]
    },
    {
        "number": 5074,
        "comments": "",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario \u0160a\u0161ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class KannadaNews(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('kannada_news', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format(",
            "-                    path_to_manual_file, _TRAIN_FILENAME, self.manual_download_instructions",
            "-                )",
            "+                f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('kannada_news', data_dir=...)` that includes a file name {_TRAIN_FILENAME}. Manual download instructions: {self.manual_download_instructions})\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ]
    },
    {
        "number": 5077,
        "comments": "",
        "commit_message": "some small fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def is_training_name(name):",
            "This is only used to improve logging.",
            ":returns: guess whether this tensor is something only used in training.",
            "\"\"\"",
            "-    # TODO: maybe simply check against TRAINABLE_VARIABLES and EXTRA_SAVE_VARS_KEY ?",
            "+    # TODO: maybe simply check against TRAINABLE_VARIABLES and MODEL_VARIABLES?",
            "# TODO or use get_slot_names()",
            "name = get_op_tensor_name(name)[0]",
            "if name.endswith('/Adam') or name.endswith('/Adam_1'):"
        ]
    },
    {
        "number": 5078,
        "comments": "",
        "commit_message": "fix ner_model_dir not in args bug\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def optimize_ner_model(args, num_labels,  logger=None):",
            "",
            "with tf.Session() as sess:",
            "sess.run(tf.global_variables_initializer())",
            "-                saver.restore(sess, tf.train.latest_checkpoint(args.ner_model_dir))",
            "+                saver.restore(sess, tf.train.latest_checkpoint(args.model_dir))",
            "logger.info('freeze...')",
            "from tensorflow.python.framework import graph_util",
            "tmp_g = graph_util.convert_variables_to_constants(sess, graph.as_graph_def(), ['pred_ids'])"
        ]
    },
    {
        "number": 5079,
        "comments": "",
        "commit_message": "fixed nonzero deprecation warnings\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GDC(object):",
            "kwargs['eps'] = self.__calculate_eps__(edge_weight, num_nodes,",
            "kwargs['avg_degree'])",
            "",
            "-            remaining_edge_idx = torch.nonzero(",
            "-                edge_weight >= kwargs['eps']).flatten()",
            "+            remaining_edge_idx = (edge_weight >= kwargs['eps']).nonzero(",
            "+                as_tuple=False).flatten()",
            "edge_index = edge_index[:, remaining_edge_idx]",
            "edge_weight = edge_weight[remaining_edge_idx]",
            "elif method == 'topk':"
        ]
    },
    {
        "number": 5080,
        "comments": "",
        "commit_message": "viusalize_subgraph fix for graph explanation\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class GNNExplainer(torch.nn.Module):",
            "if node_idx == -1:",
            "hard_edge_mask = torch.BoolTensor([True] * edge_index.size(1),",
            "device=edge_mask.device)",
            "-            subset = torch.arange(",
            "-                edge_index.max() + 1,",
            "-                device=edge_index.device if y is None else y.device)",
            "+            subset = torch.arange(edge_index.max().item() + 1,",
            "+                                  device=edge_index.device)",
            "+            y = None",
            "+",
            "else:",
            "# Only operate on a k-hop subgraph around `node_idx`.",
            "subset, edge_index, _, hard_edge_mask = k_hop_subgraph("
        ]
    },
    {
        "number": 5082,
        "comments": "",
        "commit_message": "Small fixes.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_srelu():",
            "",
            "",
            "if __name__ == '__main__':",
            "-    # pytest.main([__file__])",
            "-    test_srelu()",
            "+    pytest.main([__file__])"
        ]
    },
    {
        "number": 5084,
        "comments": "",
        "commit_message": "bug fix in summary. fix TF comptability break. update PTB readme\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def huber_loss(x, delta=1, name='huber_loss'):",
            "\"\"\"",
            "sqrcost = tf.square(x)",
            "abscost = tf.abs(x)",
            "-    return tf.select(abscost < delta,",
            "-                     sqrcost * 0.5,",
            "-                     abscost * delta - 0.5 * delta ** 2,",
            "-                     name=name)",
            "+    return tf.where(abscost < delta,",
            "+                    sqrcost * 0.5,",
            "+                    abscost * delta - 0.5 * delta ** 2,",
            "+                    name=name)",
            "",
            "",
            "def get_scalar_var(name, init_value, summary=False, trainable=False):"
        ]
    },
    {
        "number": 5091,
        "comments": "",
        "commit_message": "Use nearest neighbour interpolation for masks (#1630)\n\n* If mask, use always nearest neighbour interpolation\n\n* added parameter override\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fixed pickle\n\n* fixed errors\n\n* Update colorjiggle\n\n* fixed lint\n\n* bug fix\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* try again\n\n* Fixed augmentation container\n\n* Bug fixes\n\n* bug fixes\n\n* Fixed lint\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Make it work\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* Fixed mypy\n\n* Fixed mypy\n\nCo-authored-by: shijianjian <sj8716643@126.com>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\nCo-authored-by: Jian Shi <shij0c@kaust.edu.sa>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RandomInvert(IntensityAugmentationBase2D):",
            "self.flags = dict(max_val=max_val)",
            "",
            "def apply_transform(",
            "-        self, input: Tensor, params: Dict[str, Tensor], transform: Optional[Tensor] = None",
            "+        self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None",
            ") -> Tensor:",
            "-        return invert(input, torch.as_tensor(self.flags[\"max_val\"], device=input.device, dtype=input.dtype))",
            "+        return invert(input, torch.as_tensor(flags[\"max_val\"], device=input.device, dtype=input.dtype))"
        ]
    },
    {
        "number": 5095,
        "comments": "",
        "commit_message": "fixes to docstrings,signatures\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "clip.support_native_out = True",
            "clip.unsupported_dtypes = (\"float16\",)",
            "",
            "",
            "-def unstack(x: torch.Tensor, axis: int, keepdims: bool = False) -> List[torch.Tensor]:",
            "+def unstack(",
            "+    x: torch.Tensor, /, *, axis: int = 0, keepdims: bool = False",
            "+) -> List[torch.Tensor]:",
            "if x.shape == ():",
            "return [x]",
            "ret = list(torch.unbind(x, axis))"
        ]
    },
    {
        "number": 5101,
        "comments": "",
        "commit_message": "[style] consistent nn. and nn.functional (#12124)\n\n* consistent nn. and nn.functional\n\n* fix glitch\n\n* fix glitch #2\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LightningModel(pl.LightningModule):",
            "super().__init__()",
            "self.model = model",
            "self.num_labels = 2",
            "-        self.qa_outputs = torch.nn.Linear(self.model.config.hidden_size, self.num_labels)",
            "+        self.qa_outputs = nn.Linear(self.model.config.hidden_size, self.num_labels)",
            "",
            "# implement only because lightning requires to do so",
            "def forward(self):"
        ]
    },
    {
        "number": 5102,
        "comments": "",
        "commit_message": "Not use -1e4 as attn mask (#17306)\n\n* Use torch.finfo(self.dtype).min\n\n* for GPTNeoX\n\n* for Albert\n\n* For Splinter\n\n* Update src/transformers/models/data2vec/modeling_data2vec_audio.py\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n\n* fix -inf used in Bart-like models\n\n* Fix a few remaining -inf\n\n* more fix\n\n* clean up\n\n* For CLIP\n\n* For FSMT\n\n* clean up\n\n* fix test\n\n* Add dtype argument and use it for LayoutLMv3\n\n* update FlaxLongT5Attention\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class FSMTHeadTests(unittest.TestCase):",
            "config, *_ = self._get_config_and_data()",
            "input_ids = _long_tensor(([4, 4, 2]))",
            "decoder_input_ids = _long_tensor([[26388, 2, config.pad_token_id]])",
            "-        ignore = float(\"-inf\")",
            "+        causal_mask_dtype = torch.float32",
            "+        ignore = torch.finfo(causal_mask_dtype).min",
            "decoder_input_ids, decoder_attn_mask, causal_mask = _prepare_fsmt_decoder_inputs(",
            "-            config, input_ids, decoder_input_ids",
            "+            config, input_ids, decoder_input_ids, causal_mask_dtype=causal_mask_dtype",
            ")",
            "expected_causal_mask = torch.tensor(",
            "[[0, ignore, ignore], [0, 0, ignore], [0, 0, 0]]  # never attend to the final token, because its pad"
        ]
    },
    {
        "number": 5106,
        "comments": "",
        "commit_message": "fix prediction on run-squad.py example\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def main():",
            "if args.do_train:",
            "torch.save(model_to_save.state_dict(), output_model_file)",
            "",
            "-    # Load a trained model that you have fine-tuned",
            "-    model_state_dict = torch.load(output_model_file)",
            "-    model = BertForQuestionAnswering.from_pretrained(args.bert_model, state_dict=model_state_dict)",
            "+        # Load a trained model that you have fine-tuned",
            "+        model_state_dict = torch.load(output_model_file)",
            "+        model = BertForQuestionAnswering.from_pretrained(args.bert_model, state_dict=model_state_dict)",
            "+    else:",
            "+        model = BertForQuestionAnswering.from_pretrained(args.bert_model)",
            "+",
            "model.to(device)",
            "",
            "if args.do_predict and (args.local_rank == -1 or torch.distributed.get_rank() == 0):"
        ]
    },
    {
        "number": 5107,
        "comments": "",
        "commit_message": "Fix hyperlstm\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class HyperLSTMCell(tf.contrib.rnn.RNNCell):",
            "x_size = x.get_shape().as_list()[-1]",
            "embedding_size = self.hyper_embedding_size",
            "num_units = self.num_units",
            "-      batch_size = x.get_shape().as_list()[0]",
            "+      batch_size = tf.shape(x)[0]",
            "",
            "W_xh = tf.get_variable('W_xh',",
            "[x_size, 4*num_units], initializer=w_init)"
        ]
    },
    {
        "number": 5108,
        "comments": "",
        "commit_message": "fix: fix BF16_Optimizer compatibility issue with optimizer state 0-dim tensor (#2152)\n\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BF16_Optimizer(ZeROOptimizer):",
            "hp_frag_address.numel)",
            "for key,",
            "value in self.optimizer.state[flat_hp_partition].items()",
            "-                if torch.is_tensor(value)",
            "+                if torch.is_tensor(value) and value.dim() > 0",
            "}",
            "",
            "lp_frag_address = fragment_address(start=fragment_start - lp_start,"
        ]
    },
    {
        "number": 5111,
        "comments": "",
        "commit_message": "Fix bugs in tensorrt and openvino\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TensorflowONNXTensorRTInferenceLearner(",
            "else None",
            ")",
            "out_arrays = self._predict_array(cuda_input_arrays, input_shapes)",
            "-        return tuple(tf.convert_to_tensor(array[0]) for array in out_arrays)",
            "+        return tuple(tf.convert_to_tensor(array) for array in out_arrays)",
            "",
            "",
            "class NumpyONNXTensorRTInferenceLearner("
        ]
    },
    {
        "number": 5112,
        "comments": "",
        "commit_message": "[GPT2] Correct gradient checkpointing (#9308)\n\n* correct gpt2\n\n* fix gpt2\n\n* fix use_cache ordering\n\n* correct past tolerance\n\n* fix for all cases\n\n* style\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFGPT2ModelTester:",
            "output_from_past_slice = output_from_past[:, :, random_slice_idx]",
            "",
            "# test that outputs are equal for slice",
            "-        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-6)",
            "+        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-3)",
            "",
            "def create_and_check_gpt2_lm_head(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):",
            "model = TFGPT2LMHeadModel(config=config)"
        ]
    },
    {
        "number": 5118,
        "comments": "",
        "commit_message": "fixed trpo model distribution object creation\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TRPOModel(PolicyGradientModel):",
            "gradients = [grad for grad in gradients if grad is not None]",
            "self.policy_gradient = tf.concat(values=[tf.reshape(grad, (-1,)) for grad in gradients], axis=0)  # util.prod(util.shape(v))",
            "",
            "-            fixed_distribution = distribution.__class__([tf.stop_gradient(x) for x in distribution])",
            "+            fixed_distribution = distribution.__class__.from_tensors(parameters=[tf.stop_gradient(x) for x in distribution])",
            "fixed_kl_divergence = fixed_distribution.kl_divergence(distribution)",
            "",
            "self.tangent = tf.placeholder(tf.float32, shape=(None,))"
        ]
    },
    {
        "number": 5124,
        "comments": "",
        "commit_message": "Fix T5/mT5 tests (#18029)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFT5ModelIntegrationTests(unittest.TestCase):",
            "labels = tokenizer(\"Hi I am\", return_tensors=\"tf\").input_ids",
            "",
            "loss = model(input_ids, labels=labels).loss",
            "-        mtf_score = -tf.math.reduce_sum(loss).numpy()",
            "+        mtf_score = -tf.math.reduce_mean(loss).numpy()",
            "",
            "-        EXPECTED_SCORE = -60.7397",
            "+        EXPECTED_SCORE = -7.594554",
            "self.assertTrue(abs(mtf_score - EXPECTED_SCORE) < 1e-4)",
            "",
            "@slow"
        ]
    },
    {
        "number": 5127,
        "comments": "",
        "commit_message": "lint fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def log2(",
            "x: Union[tf.Tensor, tf.Variable], *, out: Union[tf.Tensor, tf.Variable] = None",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "if ivy.is_int_dtype(x):",
            "-        x=tf.cast(x,dtype=ivy.default_float_dtype())",
            "+        x = tf.cast(x, dtype=ivy.default_float_dtype())",
            "return tf.math.log(x) / tf.math.log(tf.constant(2.0, x.dtype))"
        ]
    },
    {
        "number": 5129,
        "comments": "",
        "commit_message": "two minor fixes for parallel mode\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PolicyModel(Model):",
            "is_frequency = tf.math.equal(x=tf.mod(x=episode, y=frequency), y=zero)",
            "# Only update once per episode increment",
            "false = tf.constant(value=False, dtype=util.tf_dtype(dtype='bool'))",
            "-                terminal = tf.concat(values=(false, terminal), axis=0)",
            "+                terminal = tf.concat(values=((false,), terminal), axis=0)",
            "is_terminal = terminal[-1]",
            "is_frequency = tf.math.logical_and(x=is_frequency, y=is_terminal)",
            "at_least_start = tf.math.greater_equal(x=episode, y=start)"
        ]
    },
    {
        "number": 5130,
        "comments": "",
        "commit_message": "Initial support for TensorFlow 2.0 (#1193)\n\n* Initial support for TensorFlow 2.0\n\n* Support for TensorFlow 2.0 in `horovod.tensorflow`\n* Replace RMSProp with Adam (RMSProp doesn't scale)\n* Minor example bugfixes\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Bugfixes, enable subgraph caching\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Bugfixes & performance improvements\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Revert RMSprop changes in Keras tests\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Renamed examples tensorflow_20_* -> tensorflow2_*\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Support for TensorFlow 2.0 tests\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Bugfix Keras test exclusion logic\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Address review comments\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main(_):",
            "predict, loss = conv_model(image, label, tf.estimator.ModeKeys.TRAIN)",
            "",
            "# Horovod: adjust learning rate based on number of GPUs.",
            "-    opt = tf.train.RMSPropOptimizer(0.001 * hvd.size())",
            "+    opt = tf.train.AdamOptimizer(0.001 * hvd.size())",
            "",
            "# Horovod: add Horovod Distributed Optimizer.",
            "opt = hvd.DistributedOptimizer(opt)"
        ]
    },
    {
        "number": 5132,
        "comments": "",
        "commit_message": "add a missing output in dorefa (fix #800)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ImageNetModel):",
            ".apply(nonlin)",
            ".FullyConnected('fct', 1000, use_bias=True)())",
            "add_param_summary(('.*/W', ['histogram', 'rms']))",
            "+        tf.nn.softmax(logits, name='output')  # for prediction",
            "return logits",
            "",
            "def optimizer(self):"
        ]
    },
    {
        "number": 5133,
        "comments": "",
        "commit_message": "fix elementwise, fix bitwise backends and tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def bitwise_right_shift(",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "x1, x2 = ivy.promote_types_of_inputs(x1, x2)",
            "-    x1, x2 = _clamp_bits(x1, x2)",
            "+    ivy.assertions.check_all(x2 >= 0, message=\"shifts must be non-negative\")",
            "return tf.bitwise.right_shift(x1, x2)"
        ]
    },
    {
        "number": 5135,
        "comments": "",
        "commit_message": "Fix unit test failures with torch==1.1.0 (#1840)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class _OMTMVNSample(Function):",
            "loc_grad = sum_leftmost(grad_output, -1)",
            "",
            "identity = eye_like(g, dim)",
            "-        R_inv = torch.trtrs(identity, L.t(), transpose=False, upper=True)[0]",
            "+        R_inv = torch.triangular_solve(identity, L.t(), transpose=False, upper=True)[0]",
            "",
            "z_ja = z.unsqueeze(-1)",
            "g_R_inv = torch.matmul(g, R_inv).unsqueeze(-2)"
        ]
    },
    {
        "number": 5141,
        "comments": "",
        "commit_message": "fix incomplete RunningMean (#1309)\n\n* fix RunningMean\n\n* changelog\n\n* fix none\n\n* Update supporters.py\n\njust needed to multiply by zero for init\n\n* Revert \"Update supporters.py\"\n\nThis reverts commit 7e0da6c6\n\n* fix NaN\n\n* formatting\n\nCo-authored-by: William Falcon <waf2107@columbia.edu>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def info_system():",
            "",
            "def info_cuda():",
            "return {",
            "-        'GPU': set([torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]),",
            "+        'GPU': [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())],",
            "# 'nvidia_driver': get_nvidia_driver_version(run_lambda),",
            "'available': torch.cuda.is_available(),",
            "'version': torch.version.cuda,"
        ]
    },
    {
        "number": 5145,
        "comments": "",
        "commit_message": "CDN urls (#4030)\n\n* [file_utils] use_cdn + documentation\n\n* Move to cdn. urls for weights\n\n* [urls] Hotfix for bert-base-japanese\n",
        "label": "",
        "answer": "no",
        "change": [
            "from .tokenization_utils import BatchEncoding",
            "logger = logging.getLogger(__name__)",
            "",
            "TF_XLNET_PRETRAINED_MODEL_ARCHIVE_MAP = {",
            "-    \"xlnet-base-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-tf_model.h5\",",
            "-    \"xlnet-large-cased\": \"https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-large-cased-tf_model.h5\",",
            "+    \"xlnet-base-cased\": \"https://cdn.huggingface.co/xlnet-base-cased-tf_model.h5\",",
            "+    \"xlnet-large-cased\": \"https://cdn.huggingface.co/xlnet-large-cased-tf_model.h5\",",
            "}"
        ]
    },
    {
        "number": 5147,
        "comments": "",
        "commit_message": "[RLlib] Fix crash when using StochasticSampling exploration (most PG-style algos) w/ tf and numpy > 1.19.5 (#18366)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class OrnsteinUhlenbeckNoise(GaussianNoise):",
            "true_fn=lambda: exploration_actions,",
            "false_fn=lambda: deterministic_actions)",
            "# Logp=always zero.",
            "-        logp = tf.zeros_like(deterministic_actions, dtype=tf.float32)[:, 0]",
            "+        logp = zero_logps_from_actions(deterministic_actions)",
            "",
            "# Increment `last_timestep` by 1 (or set to `timestep`).",
            "if self.framework in [\"tf2\", \"tfe\"]:"
        ]
    },
    {
        "number": 5149,
        "comments": "",
        "commit_message": "fix initializer :fire:\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BaseOp(object):",
            "self.lay.w[var] = tf.get_variable(var,",
            "shape = self.lay.wshape[var],",
            "dtype = tf.float32,",
            "-                initializer = tf.contrib.layers.xavier_initializer())",
            "+                initializer = self.lay.w[var])",
            "",
            "def wrap_pholder(self, ph, feed):",
            "\"\"\"wrap layer.h into placeholders\"\"\""
        ]
    },
    {
        "number": 5150,
        "comments": "",
        "commit_message": "Add deterministic argument for Agent.act, lots of other small changes and fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Recent(Queue):",
            "# Check whether memory contains at least one valid timestep",
            "num_timesteps = tf.math.minimum(x=self.buffer_index, y=capacity)",
            "num_timesteps -= (past_horizon + future_horizon)",
            "+        num_timesteps = tf.maximum(x=num_timesteps, y=self.episode_count)",
            "",
            "# Check whether memory contains at least one timestep",
            "assertions = list()"
        ]
    },
    {
        "number": 5159,
        "comments": "",
        "commit_message": "update expected results of slow tests (#268)\n\n* update expected results of slow tests\n\n* relax sum and mean tests\n\n* Print shapes when reporting exception\n\n* formatting\n\n* fix sentence\n\n* relax test_stable_diffusion_fast_ddim for gpu fp16\n\n* relax flakey tests on GPU\n\n* added comment on large tolerences\n\n* black\n\n* format\n\n* set scheduler seed\n\n* added generator\n\n* use np.isclose\n\n* set num_inference_steps to 50\n\n* fix dep. warning\n\n* update expected_slice\n\n* preprocess if image\n\n* updated expected results\n\n* updated expected from CI\n\n* pass generator to VAE\n\n* undo change back to orig\n\n* use orignal\n\n* revert back the expected on cpu\n\n* revert back values for CPU\n\n* more undo\n\n* update result after using gen\n\n* update mean\n\n* set generator for mps\n\n* update expected on CI server\n\n* undo\n\n* use new seed every time\n\n* cpu manual seed\n\n* reduce num_inference_steps\n\n* style\n\n* use generator for randn\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class AutoencoderKL(ModelMixin, ConfigMixin):",
            "x = sample",
            "posterior = self.encode(x).latent_dist",
            "if sample_posterior:",
            "-            z = posterior.sample()",
            "+            z = posterior.sample(generator=generator)",
            "else:",
            "z = posterior.mode()",
            "dec = self.decode(z).sample"
        ]
    },
    {
        "number": 5160,
        "comments": "",
        "commit_message": "Sift test fix (#341)\n\n* fix filter2d docs\n\n* speed-up patch local feature tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestLAFAffineShapeEstimator:",
            "assert_allclose(new_laf, expected, atol=1e-4, rtol=1e-4)",
            "",
            "def test_gradcheck(self):",
            "-        batch_size, channels, height, width = 1, 1, 100, 100",
            "+        batch_size, channels, height, width = 1, 1, 40, 40",
            "patches = torch.rand(batch_size, channels, height, width)",
            "patches = utils.tensor_to_gradcheck_var(patches)  # to var",
            "-        laf = torch.tensor([[[[20., 0., 56.], [0., 20., 56.]]]])",
            "+        laf = torch.tensor([[[[5., 0., 26.], [0., 5., 26.]]]])",
            "laf = utils.tensor_to_gradcheck_var(laf)  # to var",
            "assert gradcheck(LAFAffineShapeEstimator(11), (laf, patches),",
            "raise_exception=True, rtol=1e-3, atol=1e-3)"
        ]
    },
    {
        "number": 5162,
        "comments": "",
        "commit_message": "num features to dataset, num_classes bugfix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "train_dataset = MNISTSuperpixels(path, True, transform=transform)",
            "test_dataset = MNISTSuperpixels(path, False, transform=transform)",
            "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)",
            "test_loader = DataLoader(test_dataset, batch_size=64)",
            "+d = train_dataset",
            "",
            "",
            "class Net(torch.nn.Module):",
            "def __init__(self):",
            "super(Net, self).__init__()",
            "-        self.conv1 = SplineConv(1, 32, dim=2, kernel_size=5)",
            "+        self.conv1 = SplineConv(d.num_features, 32, dim=2, kernel_size=5)",
            "self.conv2 = SplineConv(32, 64, dim=2, kernel_size=5)",
            "self.conv3 = SplineConv(64, 64, dim=2, kernel_size=5)",
            "self.fc1 = torch.nn.Linear(4 * 64, 128)",
            "-        self.fc2 = torch.nn.Linear(128, 10)",
            "+        self.fc2 = torch.nn.Linear(128, d.num_classes)",
            "",
            "def forward(self, data):",
            "data.x = F.elu(self.conv1(data.x, data.edge_index, data.edge_attr))"
        ]
    },
    {
        "number": 5164,
        "comments": "",
        "commit_message": "fix pep8 style in /scripts\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "with tf.Graph().as_default() as G:",
            "logger.info(\"Variables to dump:\")",
            "logger.info(\", \".join(var_dict.keys()))",
            "saver = tf.train.Saver(",
            "-                    var_list=var_dict,",
            "-                    write_version=tf.train.SaverDef.V2)",
            "+                var_list=var_dict,",
            "+                write_version=tf.train.SaverDef.V2)",
            "saver.save(sess, args.output, write_meta_graph=False)"
        ]
    },
    {
        "number": 5167,
        "comments": "",
        "commit_message": "Add DeiT (PyTorch) (#11056)\n\n* First draft of deit\n\n* More improvements\n\n* Remove DeiTTokenizerFast from init\n\n* Conversion script works\n\n* Add DeiT to ViT conversion script\n\n* Add tests, add head model, add support for deit in vit conversion script\n\n* Update model checkpoint names\n\n* Update image_mean and image_std, set resample to bicubic\n\n* Improve docs\n\n* Docs improvements\n\n* Add DeiTForImageClassificationWithTeacher to init\n\n* Address comments by @sgugger\n\n* Improve feature extractors\n\n* Make fix-copies\n\n* Minor fixes\n\n* Address comments by @patil-suraj\n\n* All models uploaded\n\n* Fix tests\n\n* Remove labels argument from DeiTForImageClassificationWithTeacher\n\n* Fix-copies, style and quality\n\n* Fix tests\n\n* Fix typo\n\n* Multiple docs improvements\n\n* More docs fixes\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ViTModelIntegrationTest(unittest.TestCase):",
            "inputs = feature_extractor(images=image, return_tensors=\"pt\").to(torch_device)",
            "",
            "# forward pass",
            "-        # currently failing",
            "-        # see https://discuss.pytorch.org/t/runtimeerror-expected-object-of-scalar-type-double-but-got-scalar-type-float-for-argument-2-weight/38961/2",
            "-        outputs = model(inputs[\"pixel_values\"])",
            "-        # outputs = model(**inputs)",
            "+        outputs = model(**inputs)",
            "",
            "# verify the logits",
            "expected_shape = torch.Size((1, 1000))"
        ]
    },
    {
        "number": 5171,
        "comments": "",
        "commit_message": "fix owlvit tests, update docstring examples (#18586)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class OwlViTModelIntegrationTest(unittest.TestCase):",
            "",
            "num_queries = int((model.config.vision_config.image_size / model.config.vision_config.patch_size) ** 2)",
            "self.assertEqual(outputs.pred_boxes.shape, torch.Size((1, num_queries, 4)))",
            "+",
            "expected_slice_boxes = torch.tensor(",
            "-            [[0.0948, 0.0471, 0.1915], [0.3194, 0.0583, 0.6498], [0.1441, 0.0452, 0.2197]]",
            "+            [[0.0691, 0.0445, 0.1373], [0.1592, 0.0456, 0.3192], [0.1632, 0.0423, 0.2478]]",
            ").to(torch_device)",
            "self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4))"
        ]
    },
    {
        "number": 5173,
        "comments": "",
        "commit_message": "[Fix] Missing device initializations (#672)\n\n* Fix device initialization in several variables\n\n* Fix device initialization in several variables\n\n* Skip CUDA tests for #666\n\n* Lint\n\n* Fix device initialization in several variables\n\n* Skip CUDA tests for #666\n\n* Lint\n\n* Remove half-precision tests (#649) and add a jit test\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _scale_channel(im):",
            "# and then normalization by step.",
            "lut = (torch.cumsum(histo, 0) + (step // 2)) // step",
            "# Shift lut, prepending with 0.",
            "-        lut = torch.cat([torch.zeros(1), lut[:-1]])",
            "+        lut = torch.cat([torch.zeros(1, device=lut.device), lut[:-1]])",
            "# Clip the counts to be in range.  This is done",
            "# in the C code for image.point.",
            "return torch.clamp(lut, 0, 255)"
        ]
    },
    {
        "number": 5174,
        "comments": "",
        "commit_message": "Fix pylint errors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MetadataCaptureHook(TrainingHook):",
            "return tf.train.SessionRunArgs(self._global_step)",
            "else:",
            "tf.logging.info(\"Performing full trace on next step.\")",
            "-      run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)",
            "+      run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE) #pylint: disable=E1101",
            "return tf.train.SessionRunArgs(self._global_step, options=run_options)",
            "",
            "def after_run(self, _run_context, run_values):"
        ]
    },
    {
        "number": 5176,
        "comments": "",
        "commit_message": "fix UserWarning from slow tensor conversion (#1948)\n\n* fix UserWarning from slow tensor conversion\n\n* Add latest docstring and tutorial changes\n\nCo-authored-by: github-actions[bot] <41898282+github-actions[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def convert_features_to_dataset(features):",
            "\"Converting now to a tensor of default type long.\")",
            "",
            "# Convert all remaining python objects to torch long tensors",
            "-        cur_tensor = torch.tensor([sample[t_name] for sample in features], dtype=torch.long)",
            "+        cur_tensor = torch.as_tensor(np.array([sample[t_name] for sample in features]), dtype=torch.long)",
            "",
            "all_tensors.append(cur_tensor)"
        ]
    },
    {
        "number": 5177,
        "comments": "",
        "commit_message": "minor fixes (#14026)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CLIPModelTester:",
            "",
            "def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):",
            "model = CLIPModel(config).to(torch_device).eval()",
            "-        result = model(input_ids, pixel_values, attention_mask)",
            "+        with torch.no_grad():",
            "+            result = model(input_ids, pixel_values, attention_mask)",
            "self.parent.assertEqual(",
            "result.logits_per_image.shape, (self.vision_model_tester.batch_size, self.text_model_tester.batch_size)",
            ")"
        ]
    },
    {
        "number": 5178,
        "comments": "",
        "commit_message": "fix indices in inverse_pose\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def inverse_pose(pose):",
            "",
            "pose_inv = pose.clone()",
            "pose_inv[..., :3, 0:3] = torch.transpose(pose[..., :3, :3], 1, 2)",
            "-    pose_inv[..., :3, 2:3] = torch.matmul(",
            "-        -1.0 * pose_inv[..., :3, :3], pose[..., :3, 2:3])",
            "+    pose_inv[..., :3, 3:4] = torch.matmul(",
            "+        -1.0 * pose_inv[..., :3, :3], pose[..., :3, 3:4])",
            "",
            "if len(pose_shape) == 2:",
            "pose_inv = torch.squeeze(pose_inv, dim=0)"
        ]
    },
    {
        "number": 5179,
        "comments": "",
        "commit_message": "Rename analytic_mean, analytic_var, enumerable (#932)\n\n* Rename analytic_mean, analytic_var, enumerable\n\n* Fix typo in Histogram\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Parameterized(nn.Module):",
            "p = pyro.sample(param_name, prior)",
            "else:  # prior != None and mode = \"guide\"",
            "MAP_param_name = param_name + \"_MAP\"",
            "-            MAP_param_0 = torch.tensor(prior.analytic_mean().data.clone(), requires_grad=True)",
            "+            MAP_param_0 = torch.tensor(prior.mean.data.clone(), requires_grad=True)",
            "MAP_param = pyro.param(MAP_param_name, MAP_param_0)",
            "p = pyro.sample(param_name, dist.Delta(MAP_param))"
        ]
    },
    {
        "number": 5180,
        "comments": "",
        "commit_message": "[Fix] Fix a lot of typos (#6190)\n\n* pre-commit: Add codespell to look for typos\n\n* fixup! Indentation\n\n* Update lint\n\n* Fix lint\n\n* Fix typo\n\n* Fix comments\n\nCo-authored-by: Christian Clauss <cclauss@me.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_patch_merging():",
            "assert out_size == (2, 1)",
            "assert x_out.size(1) == out_size[0] * out_size[1]",
            "",
            "-        # test different kernel_size with diffrent stride",
            "+        # test different kernel_size with different stride",
            "input_size = (6, 5)",
            "kernel_size = (6, 2)",
            "stride = (6, 2)"
        ]
    },
    {
        "number": 5183,
        "comments": "",
        "commit_message": "Fix coordinate system conventions in renderer\n\nSummary:\n## Updates\n\n- Defined the world and camera coordinates according to this figure. The world coordinates are defined as having +Y up, +X left and +Z in.\n\n{F230888499}\n\n- Removed all flipping from blending functions.\n- Updated the rasterizer to return images with +Y up and +X left.\n- Updated all the mesh rasterizer tests\n    - The expected values are now defined in terms of the default +Y up, +X left\n    - Added tests where the triangles in the meshes are non symmetrical so that it is clear which direction +X and +Y are\n\n## Questions:\n- Should we have **scene settings** instead of raster settings?\n    - To be more correct we should be [z clipping in the rasterizer based on the far/near clipping planes](https://github.com/ShichenLiu/SoftRas/blob/master/soft_renderer/cuda/soft_rasterize_cuda_kernel.cu#L400) - these values are also required in the blending functions so should we make these scene level parameters and have a scene settings tuple which is available to the rasterizer and shader?\n\nReviewed By: gkioxari\n\nDifferential Revision: D20208604\n\nfbshipit-source-id: 55787301b1bffa0afa9618f0a0886cc681da51f3\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def softmax_blend_naive(colors, fragments, blend_params):",
            "pixel_colors[n, h, w, :3] += (delta / denom) * bk_color",
            "pixel_colors[n, h, w, 3] = 1.0 - alpha",
            "",
            "-    return torch.flip(pixel_colors, [1])",
            "+    return pixel_colors",
            "",
            "",
            "class TestBlending(unittest.TestCase):"
        ]
    },
    {
        "number": 5187,
        "comments": "",
        "commit_message": "Ensure compatibility with torch>=1.13 torchvision>=0.14 (#3155)\n\n* Bump dependency versions to torch>1.13 torchvision>=0.14\n\n* Avoid setuptools>=60\n\n* Replace Tensor.lu() -> torch.linalg.lu_factor()\n\n* Fix test\n\n* Revert \"Bump dependency versions to torch>1.13 torchvision>=0.14\"\n\nThis reverts commit 90ec2066e783a6d1a0485394fe58320ce8065a46.\n\n* Add comment on lap\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GeneralizedChannelPermute(ConditionedGeneralizedChannelPermute, TransformM",
            "W, _ = torch.linalg.qr(torch.randn(channels, channels))",
            "",
            "# Construct the partially pivoted LU-form and the pivots",
            "-        LU, pivots = W.lu()",
            "+        LU, pivots = torch.linalg.lu_factor(W)",
            "",
            "# Convert the pivots into the permutation matrix",
            "if permutation is None:"
        ]
    },
    {
        "number": 5190,
        "comments": "",
        "commit_message": "Integration testing v1 (#71)\n\n* new version with bug fix for gpu\n\n* hotfix\n\n* current data type default to TEXT in order to avoid null data types (#67)\n\n* Cleanup (#68)\n\n* removed a duplicated helper funtion and moved it to helpers with a better name\n\n* removed unused code and files, fixed a small isue in the encoder that would have resulted in a crash\n\n* fixed signature of tryCastToNumber\n\n* added scraps file and two functions we want to keep in it\n\n* remoed test files\n\n* added comment about funtion psotioning\n\n* added integration testing and better csv sniffing\n\n* added test data to gitignore\n\n* finalized the predict part of testing\n\n* commented out problematic line, added logging\n\n* added some test files to gitignore\n\n* rasigin exceptions where appropriate\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class EncoderRNN(nn.Module):",
            "return output, hidden",
            "",
            "def initHidden(self):",
            "-        return torch.zeros(1, 1, self.hidden_size), device=device)",
            "+        return torch.zeros(1, 1, self.hidden_size, device=device)"
        ]
    },
    {
        "number": 5191,
        "comments": "",
        "commit_message": "fix a bug in tutorial 1\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Input(Layer):",
            "logging.info(\"Input  %s: %s\" % (self.name, str(shape)))",
            "",
            "shape_without_none = [_ if _ is not None else 1 for _ in shape]",
            "-        self.outputs = self.forward(tf.initializers.constant(value=0.0)(shape_without_none), is_train=False)",
            "+        self.outputs = self.forward(tf.initializers.random_normal()(shape_without_none), is_train=False)",
            "",
            "def __call__(self, prev_layer):",
            "# FIXME: better exception raising"
        ]
    },
    {
        "number": 5192,
        "comments": "",
        "commit_message": "Fixes for 2.7\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def read_image_file(path):",
            "row = []",
            "img.append(row)",
            "for c in range(num_cols):",
            "-                    row.append(data[idx])",
            "+                    row.append(parse_byte(data[idx]))",
            "idx += 1",
            "assert len(images) == length",
            "-        out = torch.FloatTensor(images)",
            "-        return out",
            "+        return torch.FloatTensor(images)",
            "",
            "print(\"Loading training set\")",
            "training_set = ("
        ]
    },
    {
        "number": 5194,
        "comments": "",
        "commit_message": "Fixed bugs about cascade_roi_head. (#3244)\n\n* Fixed bugs about cascade_roi_head.\n\n* Fixed the bug of refining bg rois at training when reg_class_agnostic is False.\n\n* Fixed the bug of potentially infering roi label to be bg at inference.\n\n* Reformatted code.\n\n* Ensured foreground roi_labels when refining bboxes at training.\n\n* Fixed typo.\n\n* Fixed incompatibility between mmdet2.2(ops/conv_ws.py) and mmcv1.0.0\n\n* Fixed typo.\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CascadeRoIHead(BaseRoIHead, BBoxTestMixin, MaskTestMixin):",
            "ms_scores.append(bbox_results['cls_score'])",
            "",
            "if i < self.num_stages - 1:",
            "-                    bbox_label = bbox_results['cls_score'].argmax(dim=1)",
            "+                    bbox_label = bbox_results['cls_score'][:, :-1].argmax(",
            "+                        dim=1)",
            "rois = self.bbox_head[i].regress_by_class(",
            "rois, bbox_label, bbox_results['bbox_pred'],",
            "img_meta[0])"
        ]
    },
    {
        "number": 5198,
        "comments": "",
        "commit_message": "fixed hpc save, load. cleaned apu\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_model_saving_loading():",
            "# make prediction",
            "# assert that both predictions are the same",
            "new_pred = model_2(x)",
            "-    assert torch.eq(pred_before_saving, new_pred)",
            "+    assert torch.all(torch.eq(pred_before_saving, new_pred)).item() == 1",
            "",
            "clear_save_dir()"
        ]
    },
    {
        "number": 5201,
        "comments": "",
        "commit_message": "Fix distributed trainer (fix #671)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def override_to_local_variable(enable=True):",
            "ns = orig_vs.original_name_scope",
            "with tf.variable_scope(",
            "orig_vs, custom_getter=custom_getter):",
            "-                with tf.name_scope(ns + '/'):",
            "+                with tf.name_scope(ns + '/' if ns else ''):",
            "yield",
            "else:",
            "yield"
        ]
    },
    {
        "number": 5202,
        "comments": "",
        "commit_message": "fixed problem with multi-state/action networks and scoping\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Gaussian(Distribution):",
            "self.shape = shape",
            "action_size = util.prod(self.shape)",
            "",
            "-        with tf.name_scope(name=scope):",
            "-            self.mean = Linear(size=action_size, bias=mean, scope='mean')",
            "-            self.log_stddev = Linear(size=action_size, bias=log_stddev, scope='log-stddev')",
            "+        self.mean = Linear(size=action_size, bias=mean, scope='mean')",
            "+        self.log_stddev = Linear(size=action_size, bias=log_stddev, scope='log-stddev')",
            "",
            "super(Gaussian, self).__init__(scope, summary_labels)"
        ]
    },
    {
        "number": 5204,
        "comments": "",
        "commit_message": "Fix PReLU alphas reuse_variables error\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def prelu(x, channel_shared=False, weights_init='zeros', restore=True, name=\"PRe",
            "i_scope = \"\"",
            "if hasattr(x, 'scope'):",
            "if x.scope: i_scope = x.scope",
            "-    with tf.name_scope(i_scope + name) as scope:",
            "+    with tf.variable_scope(i_scope + name) as scope:",
            "W_init = initializations.get(weights_init)()",
            "alphas = va.variable(shape=w_shape, initializer=W_init,",
            "-                             restore=restore, name=scope + \"alphas\")",
            "+                             restore=restore, name=\"alphas\")",
            "",
            "x = tf.nn.relu(x) + tf.mul(alphas, (x - tf.abs(x))) * 0.5"
        ]
    },
    {
        "number": 5205,
        "comments": "",
        "commit_message": "Fix symbol which no longer exists in tf2 root.\n\nThis was leading to a weird error if the filesystem threw an OpError\nwhen finding out if a path exists.\n\nPiperOrigin-RevId: 303131046\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PathResolver(Resolver):",
            "def is_supported(self, handle):",
            "try:",
            "return tf_v1.gfile.Exists(handle)",
            "-    except tf.OpError:",
            "+    except tf.errors.OpError:",
            "return False",
            "",
            "def __call__(self, handle):"
        ]
    },
    {
        "number": 5208,
        "comments": "",
        "commit_message": "Fix #873\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Trainer:",
            "# main model optimizer step",
            "loss_dict[\"loss\"].backward()",
            "if grad_clip > 0:",
            "-                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip, error_if_nonfinite=False)",
            "+                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)",
            "optimizer.step()",
            "",
            "step_time = time.time() - step_start_time"
        ]
    },
    {
        "number": 5211,
        "comments": "",
        "commit_message": "[Refactor]: support batch input in bbox coder (#4721)\n\n* bbox coder support batch infer\n\n* Add unit test\n\n* get img_shape as tensor in single_stage\n\n* Fix max_shape\n\n* Update docs and unittest\n\nCo-authored-by: maningsheng <maningsheng@sensetime.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SingleStageDetector(BaseDetector):",
            "outs = self.bbox_head(x)",
            "# get origin input shape to support onnx dynamic shape",
            "if torch.onnx.is_in_onnx_export():",
            "-            img_metas[0]['img_shape_for_onnx'] = img.shape[2:]",
            "+            # get shape as tensor",
            "+            img_shape = torch._shape_as_tensor(img)[2:]",
            "+            img_metas[0]['img_shape_for_onnx'] = img_shape",
            "bbox_list = self.bbox_head.get_bboxes(",
            "*outs, img_metas, rescale=rescale)",
            "# skip post-processing when exporting to ONNX"
        ]
    },
    {
        "number": 5214,
        "comments": "",
        "commit_message": "release switch norm layer (#737)\n\n* release switch norm layer\n\n* change log\n\n* fix john suggestion\n\n* refactoring\n\n* codacy fix\n\n* typo fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Layer_Normalization_Test(unittest.TestCase):",
            "tf.reset_default_graph()",
            "",
            "def test_all_layers(self):",
            "-        self.assertEqual(len(self.data[\"train_network\"][\"layers\"]), 7)",
            "-        self.assertEqual(len(self.data[\"eval_network\"][\"layers\"]), 7)",
            "+        self.assertEqual(len(self.data[\"train_network\"][\"layers\"]), 8)",
            "+        self.assertEqual(len(self.data[\"eval_network\"][\"layers\"]), 8)",
            "",
            "def test_all_params(self):",
            "-        self.assertEqual(len(self.data[\"train_network\"][\"params\"]), 12)",
            "+        self.assertEqual(len(self.data[\"train_network\"][\"params\"]), 16)",
            "",
            "def test_n_params(self):",
            "-        self.assertEqual(self.data[\"train_network\"][\"n_params\"], 60560)",
            "+        self.assertEqual(self.data[\"train_network\"][\"n_params\"], 60726)",
            "",
            "",
            "if __name__ == '__main__':"
        ]
    },
    {
        "number": 5217,
        "comments": "",
        "commit_message": "fix normalization\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_decoder_cache():",
            "attention_dim=adim,",
            "linear_units=3,",
            "num_blocks=2,",
            "+        normalize_before=normalize_before,",
            "dropout_rate=0.0)",
            "dlayer = decoder.decoders[0]",
            "memory = torch.randn(2, 5, adim)",
            "",
            "-    x = torch.randn(2, 5, adim)",
            "+    x = torch.randn(2, 5, adim) * 100",
            "mask = subsequent_mask(x.shape[1]).unsqueeze(0)",
            "prev_mask = mask[:, :-1, :-1]",
            "decoder.eval()"
        ]
    },
    {
        "number": 5218,
        "comments": "",
        "commit_message": "small fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MLPValueFunction(ValueFunction):",
            "with tf.variable_scope(\"mlp_value_function\"):",
            "self.input = tf.placeholder(tf.float32, shape=[None, self.get_features_size(state_size)], name=\"input\")",
            "",
            "-            define_network = NeuralNetwork.layered_network((",
            "+            network_builder = NeuralNetwork.layered_network((",
            "{'type': 'dense', 'num_outputs': layer_size},",
            "{'type': 'dense', 'num_outputs': 1}))",
            "-            network = NeuralNetwork(define_network=define_network, inputs=[self.input])",
            "+            network = NeuralNetwork(network_builder=network_builder, inputs=[self.input])",
            "",
            "# hidden_1 = dense(layer_input=self.input, {'num_outputs': input_shape}, scope='hidden_1')",
            "# hidden_2 = dense(hidden_1, {'num_outputs': self.layer_size}, scope='hidden_2')",
            "# out = dense(hidden_2, {'num_outputs': 1}, scope='out')",
            "-            self.mlp = tf.reshape(network.output, (-1,))",
            "+            self.mlp = tf.reshape(network.output, (-1, 1))",
            "",
            "l2 = tf.nn.l2_loss(self.mlp - self.labels)",
            "self.update = tf.train.AdamOptimizer().minimize(l2)"
        ]
    },
    {
        "number": 5219,
        "comments": "",
        "commit_message": "[RLlib] Fix all example scripts to run on GPUs. (#11105)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TorchCustomLossModel(TorchModelV2, nn.Module):",
            "",
            "# Compute the IL loss.",
            "action_dist = TorchCategorical(logits, self.model_config)",
            "-        imitation_loss = torch.mean(",
            "-            -action_dist.logp(torch.from_numpy(batch[\"actions\"])))",
            "+        imitation_loss = torch.mean(-action_dist.logp(",
            "+            torch.from_numpy(batch[\"actions\"]).to(policy_loss[0].device)))",
            "self.imitation_loss_metric = imitation_loss.item()",
            "self.policy_loss_metric = np.mean([l.item() for l in policy_loss])"
        ]
    },
    {
        "number": 5220,
        "comments": "",
        "commit_message": "Doc styler v2 (#14950)\n\n* New doc styler\n\n* Fix issue with args at the start\n\n* Code sample fixes\n\n* Style code examples in MDX\n\n* Fix more patterns\n\n* Typo\n\n* Typo\n\n* More patterns\n\n* Do without black for now\n\n* Get more info in error\n\n* Docstring style\n\n* Re-enable check\n\n* Quality\n\n* Fix add_end_docstring decorator\n\n* Fix docstring\n",
        "label": "",
        "answer": "no",
        "change": [
            "PT_SPEECH_SEQ_CLASS_SAMPLE = r\"\"\"",
            "",
            ">>> # audio file is decoded on the fly",
            ">>> inputs = feature_extractor(dataset[0][\"audio\"][\"array\"], return_tensors=\"pt\")",
            "-    >>> logits = model(**inputs).logits >>> predicted_class_ids = torch.argmax(logits, dim=-1)",
            "+    >>> logits = model(**inputs).logits",
            "+    >>> predicted_class_ids = torch.argmax(logits, dim=-1)",
            ">>> predicted_label = model.config.id2label[predicted_class_ids]",
            "",
            ">>> # compute loss - target_label is e.g. \"down\""
        ]
    },
    {
        "number": 5221,
        "comments": "",
        "commit_message": "Several micro optimizations (#4833)\n\n* benchmark transfers\n\n* create tensors directl on device when possible\n\n* fix\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Covariance(Metric):",
            "",
            "#     # Note: this gives an approximate aggregation of the covariance.",
            "#     device = gold_labels.device",
            "-        #     delta_mean_prediction = torch.tensor(delta_mean_prediction).to(device)",
            "-        #     delta_mean_label = torch.tensor(delta_mean_label).to(device)",
            "-        #     delta_co_moment = torch.tensor(delta_co_moment).to(device)",
            "-        #     _total_count = torch.tensor(updated_count).to(device)",
            "+        #     delta_mean_prediction = torch.tensor(delta_mean_prediction, device=device)",
            "+        #     delta_mean_label = torch.tensor(delta_mean_label, device=device)",
            "+        #     delta_co_moment = torch.tensor(delta_co_moment, device=device)",
            "+        #     _total_count = torch.tensor(updated_count, device=device)",
            "#     dist.all_reduce(delta_mean_prediction, op=dist.ReduceOp.SUM)",
            "#     dist.all_reduce(delta_mean_label, op=dist.ReduceOp.SUM)",
            "#     dist.all_reduce(delta_co_moment, op=dist.ReduceOp.SUM)"
        ]
    },
    {
        "number": 5223,
        "comments": "",
        "commit_message": "fix test for cuda conversion\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_pytorch_np():",
            "",
            "# CUDA variable",
            "if torch.cuda.device_count()>0:",
            "-            assert isinstance(x2num.makenp(torch.autograd.variable.Variable(tensor)).cuda(), np.ndarray)",
            "+            assert isinstance(x2num.makenp(torch.autograd.variable.Variable(tensor).cuda()), np.ndarray)",
            "",
            "# python primitive type",
            "assert(isinstance(x2num.makenp(0), np.ndarray))"
        ]
    },
    {
        "number": 5225,
        "comments": "",
        "commit_message": "bug fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LookAheadWordLM(nn.Module):",
            "elif xi == self.space:",
            "y[:, self.space] = self.zero",
            "y[:, self.eos] = self.zero",
            "-            return (wlm_state, cumsum_probs, new_node), torch.log(log_y)",
            "+            return (wlm_state, cumsum_probs, new_node), torch.log(y)",
            "else:  # if no path in the tree, transition probability is one",
            "log_y = torch.zeros(1, self.subword_dict_size)",
            "return (wlm_state, cumsum_probs, new_node), log_y"
        ]
    },
    {
        "number": 5226,
        "comments": "",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def quantile(",
            "temp = a.detach()",
            "dimension = len(a.size())",
            "for x in axis:",
            "-            axis1 = x",
            "-            for axis2 in range(x+1,dimension):",
            "+            axis1 = x",
            "+            for axis2 in range(x + 1, dimension):",
            "temp = torch.transpose(temp, axis1, axis2)",
            "axis1 = axis2",
            "-        temp = torch.flatten(temp, start_dim=dimension-len(axis))",
            "+        temp = torch.flatten(temp, start_dim=dimension - len(axis))",
            "return torch.quantile(",
            "temp, q, dim=-1, keepdim=keepdims, interpolation=interpolation, out=out",
            ")",
            "-    return torch.quantile(a, q, dim=axis, keepdim=keepdims, interpolation=interpolation, out=out)",
            "+    return torch.quantile(",
            "+        a, q, dim=axis, keepdim=keepdims, interpolation=interpolation, out=out",
            "+    )",
            "",
            "",
            "quantile.support_native_out = True"
        ]
    },
    {
        "number": 5227,
        "comments": "",
        "commit_message": "Consistent ner features (#636)\n\n* update lince features\n\n* update xtreme pan-x features\n\n* update lince dataset_info\n\n* fix lince\n\n* update lince dataset_info\n\n* update xtreme dataset_info\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Xtreme(datasets.GeneratorBasedBuilder):",
            "for line in f:",
            "if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":",
            "if words:",
            "-                            yield guid_index, {\"words\": words, \"ner_tags\": ner_tags, \"langs\": langs}",
            "+                            yield guid_index, {\"words\": words, \"ner\": ner_tags, \"langs\": langs}",
            "guid_index += 1",
            "words = []",
            "ner_tags = []"
        ]
    },
    {
        "number": 5230,
        "comments": "",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario \u0160a\u0161ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TurkishMovieSentiment(datasets.GeneratorBasedBuilder):",
            "path_to_manual_file = os.path.abspath(os.path.expanduser(dl_manager.manual_dir))",
            "if not os.path.exists(path_to_manual_file):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('turkishmoviesentiment', data_dir=...)` that includes a file name {}. Manual download instructions: {})\".format(",
            "-                    path_to_manual_file, _FILENAME, self.manual_download_instructions",
            "-                )",
            "+                f\"{path_to_manual_file} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('turkishmoviesentiment', data_dir=...)` that includes a file name {_FILENAME}. Manual download instructions: {self.manual_download_instructions})\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ]
    },
    {
        "number": 5232,
        "comments": "",
        "commit_message": "CI: precommit - docformatter (#8584)\n\n* CI: precommit - docformatter\n* fix deprecated\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DDPSpawnPlugin(ParallelPlugin):",
            "self.model.to(self.root_device)",
            "",
            "def pre_backward(self, closure_loss: torch.Tensor) -> None:",
            "-        \"\"\"Run before precision plugin executes backward\"\"\"",
            "+        \"\"\"Run before precision plugin executes backward.\"\"\"",
            "if not self.lightning_module.automatic_optimization:",
            "prepare_for_backward(self.model, closure_loss)",
            "",
            "def reduce(self, tensor, group: Optional[Any] = None, reduce_op: Union[ReduceOp, str] = \"mean\") -> torch.Tensor:",
            "-        \"\"\"",
            "-        Reduces a tensor from several distributed processes to one aggregated tensor.",
            "+        \"\"\"Reduces a tensor from several distributed processes to one aggregated tensor.",
            "",
            "Args:",
            "tensor: the tensor to sync and reduce"
        ]
    },
    {
        "number": 5234,
        "comments": "",
        "commit_message": "bugfix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SHREC2016(InMemoryDataset):",
            "'{}_{}_shape_{}'.format(self.part, self.cat, i))",
            "data = read_off('{}.off'.format(path))",
            "y = read_txt_array('{}.baryc_gt'.format(path))",
            "-            data.y_idx = y[:, 0].to(torch.long)",
            "+            data.y = y[:, 0].to(torch.long) - 1",
            "data.y_baryc = y[:, 1:]",
            "train_list.append(data)"
        ]
    },
    {
        "number": 5235,
        "comments": "",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def Deconv2D(x, out_shape, kernel_shape,",
            "if use_bias:",
            "b = tf.get_variable('b', [out_channel], initializer=b_init)",
            "",
            "-    out_shape_dyn = tf.pack([tf.shape(x)[0]] + shp3_dyn)",
            "+    out_shape_dyn = tf.stack([tf.shape(x)[0]] + shp3_dyn)",
            "conv = tf.nn.conv2d_transpose(x, W, out_shape_dyn, stride4d, padding=padding)",
            "conv.set_shape(tf.TensorShape([None] + shp3_static))",
            "return nl(tf.nn.bias_add(conv, b) if use_bias else conv, name='output')"
        ]
    },
    {
        "number": 5237,
        "comments": "",
        "commit_message": "Flax Big Bird (#11967)\n\n* add flax bert\n\n* bert -> bigbird\n\n* original_full ported\n\n* add debugger\n\n* init block sparse\n\n* fix copies ; gelu_fast -> gelu_new\n\n* block sparse port\n\n* fix block sparse\n\n* block sparse working\n\n* all ckpts working\n\n* fix-copies\n\n* make quality\n\n* init tests\n\n* temporary fix for FlaxBigBirdForMultipleChoice\n\n* skip test_attention_outputs\n\n* fix\n\n* gelu_fast -> gelu_new ; fix multiple choice model\n\n* remove nsp\n\n* fix sequence classifier\n\n* fix\n\n* make quality\n\n* make fix-copies\n\n* finish\n\n* Delete debugger.ipynb\n\n* Update src/transformers/models/big_bird/modeling_flax_big_bird.py\n\n* make style\n\n* finish\n\n* bye bye jit flax tests\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BigBirdForMultipleChoice(BigBirdPreTrainedModel):",
            "return_dict=return_dict,",
            ")",
            "",
            "-        sequence_output = outputs[0]",
            "+        pooled_output = outputs[1]",
            "",
            "-        pooled_output = self.sequence_summary(sequence_output)",
            "+        pooled_output = self.dropout(pooled_output)",
            "logits = self.classifier(pooled_output)",
            "reshaped_logits = logits.view(-1, num_choices)"
        ]
    },
    {
        "number": 5242,
        "comments": "",
        "commit_message": "fix #1692\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFXLNetMainLayer(tf.keras.layers.Layer):",
            "assert input_mask is None or attention_mask is None, \"You can only use one of input_mask (uses 1 for padding) \" \\",
            "\"or attention_mask (uses 0 for padding, added for compatbility with BERT). Please choose one.\"",
            "if input_mask is None and attention_mask is not None:",
            "-            input_mask = 1.0 - attention_mask",
            "+            input_mask = 1.0 - tf.cast(attention_mask, dtype=dtype_float)",
            "if input_mask is not None and perm_mask is not None:",
            "data_mask = input_mask[None] + perm_mask",
            "elif input_mask is not None and perm_mask is None:"
        ]
    },
    {
        "number": 5243,
        "comments": "",
        "commit_message": "fixing docs (#2227)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def roc(",
            "",
            ">>> x = torch.tensor([0, 1, 2, 3])",
            ">>> y = torch.tensor([0, 1, 2, 2])",
            "-        >>> fpr, tpr, thresholds = roc(x,y)",
            "+        >>> fpr, tpr, thresholds = roc(x, y)",
            ">>> fpr",
            "tensor([0.0000, 0.3333, 0.6667, 0.6667, 1.0000])",
            ">>> tpr"
        ]
    },
    {
        "number": 5244,
        "comments": "",
        "commit_message": "Improvements on syft Hook & Plans (#3080)\n\n* Fix hook by removing unused and confusing functions\n\n* Fix hook of PlaceHolder and add tracer to th.tensor\n\n* Update list of ambiguous and hook-excluded functions\n\n* Improve instantiation of placeholder in Operation response\n\n* Fix extract_key bug, add checks and explicative failures\n\n* Add tests for plans with comparison and funcs with no args\n\n* Add dtype serialization and small fix in ptr plan\n\n* Add checks to AdditiveSharedTensor\n\n* Add clarification error to Plan\n\n* Add support of multi pointer tensors on plan ptr with>1 locations\n\n* Import placeholder instantiation\n\n* Add a tag index for worker and a find_by_tag method\n\n* Clean object storge rm_obj and use tag index\n\n* Fix search behavior, optimize search using tag index\n\n* Update plans & pointerplans to support tag serialization\n\n* Fix tests on plans\n\n* Fix Protocol to pass tests\n\n* Fix baseworker search to handle empty response and empty query\n\n* Basic FL training plan example\n\n* Improve plan representation\n\n* Run black code formatter\n\n* Update training plan example notebooks\n\n* update order of params in FL training plan example\n\n* Fix serde test issues\n\n* Fix stale tests using worker.search + fix issues related to tags\n\n* Fix module for AST conv2d\n\n* Update tutorial using worker.search\n\n* Small fix on tests and autograd tensor\n\n* Fix test serde\n\n* pip dep fix\n\n* Address PR comments and clean code\n\n* Add a check and fix a typo\n\n* Add comment note\n\nCo-authored-by: Vova Manannikov <vmanannikov@broadsoft.com>\nCo-authored-by: Karl Higley <kmhigley@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def make_autogradtensor(**kwargs):",
            "\"simplified\": (",
            "CODE[syft.frameworks.torch.tensors.interpreters.autograd.AutogradTensor],",
            "(",
            "-                    None,  # owner",
            "agt.id,  # (int)",
            "msgpack.serde._simplify(",
            "syft.hook.local_worker, agt.child"
        ]
    },
    {
        "number": 5245,
        "comments": "",
        "commit_message": "[Feat] Enabled Torch1.5.1 cpu support (#796)\n\n* Added py151 support\n\n* Enabled 1.5.1 CI\n\n* Typo fix\n\n* Fixed Equalize\n\n* Update setup.py\n\n* Bug fix\n\n* typo fix\n\n* Fixed mypy check\n\n* Update tests_cpu_versions.yml\n\nCo-authored-by: Edgar Riba <edgar.riba@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def rgb_to_hsv(image: torch.Tensor) -> torch.Tensor:",
            "",
            "# avoid division by zero",
            "deltac = torch.where(",
            "-        deltac == 0, torch.ones_like(deltac), deltac)",
            "+        deltac == 0, torch.ones_like(deltac, device=deltac.device, dtype=deltac.dtype), deltac)",
            "",
            "maxc_tmp = maxc.unsqueeze(-3) - image",
            "rc: torch.Tensor = maxc_tmp[..., 0, :, :]"
        ]
    },
    {
        "number": 5246,
        "comments": "",
        "commit_message": "bug fixes, under construction\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NormalizedAdvantageFunctions(ValueFunction):",
            ":param batch:=",
            ":return:",
            "\"\"\"",
            "-        float_terminals = tf.to_float(batch['terminals'])",
            "+        float_terminals = batch['terminals'].astype(float)",
            "+",
            "q_targets = batch['rewards'] + (1. - float_terminals) \\",
            "* self.gamma * self.get_target_value_estimate(batch['next_states'])"
        ]
    },
    {
        "number": 5247,
        "comments": "",
        "commit_message": "Fix CI with change of name of nlp (#7054)\n\n* nlp -> datasets\n\n* More nlp -> datasets\n\n* Woopsie\n\n* More nlp -> datasets\n\n* One last\n",
        "label": "",
        "answer": "yes",
        "change": [
            "from tqdm import tqdm",
            "",
            "",
            "def download_wmt_dataset(src_lang=\"ro\", tgt_lang=\"en\", dataset=\"wmt16\", save_dir=None) -> None:",
            "-    \"\"\"Download a dataset using the nlp package and save it to the format expected by finetune.py",
            "+    \"\"\"Download a dataset using the datasets package and save it to the format expected by finetune.py",
            "Format of save_dir: train.source, train.target, val.source, val.target, test.source, test.target.",
            "",
            "Args:",
            "src_lang: <str> source language",
            "tgt_lang: <str> target language",
            "-        dataset: <str> wmt16, wmt17, etc. wmt16 is a good start as it's small. To get the full list run `import nlp; print([d.id for d in nlp.list_datasets() if \"wmt\" in d.id])`",
            "+        dataset: <str> wmt16, wmt17, etc. wmt16 is a good start as it's small. To get the full list run `import datasets; print([d.id for d in datasets.list_datasets() if \"wmt\" in d.id])`",
            "save_dir: <str>, where to save the datasets, defaults to f'{dataset}-{src_lang}-{tgt_lang}'",
            "",
            "Usage:",
            ">>> download_wmt_dataset('ro', 'en', dataset='wmt16') # saves to wmt16-ro-en",
            "\"\"\"",
            "try:",
            "-        import nlp",
            "+        import datasets",
            "except (ModuleNotFoundError, ImportError):",
            "-        raise ImportError(\"run pip install nlp\")",
            "+        raise ImportError(\"run pip install datasets\")",
            "pair = f\"{src_lang}-{tgt_lang}\"",
            "print(f\"Converting {dataset}-{pair}\")",
            "-    ds = nlp.load_dataset(dataset, pair)",
            "+    ds = datasets.load_dataset(dataset, pair)",
            "if save_dir is None:",
            "save_dir = f\"{dataset}-{pair}\"",
            "save_dir = Path(save_dir)"
        ]
    },
    {
        "number": 5249,
        "comments": "",
        "commit_message": "fixed log std shape in Gaussian policy and KL-divergence axis\"\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PGModel(Model):",
            "size = 1",
            "for dims in self.state_shape:",
            "size *= dims",
            "-        self.baseline_value_function = MLPValueFunction(self.session, state_size=size, layer_size=100)  # LinearValueFunction()",
            "+        self.baseline_value_function = LinearValueFunction()",
            "# self.saver = tf.train.Saver()",
            "",
            "def get_action(self, state, episode=1):"
        ]
    },
    {
        "number": 5251,
        "comments": "",
        "commit_message": "Fix linting issues\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def nonzero(x: torch.Tensor) -> Tuple[torch.Tensor]:",
            "return torch.nonzero(x, as_tuple=True)",
            "",
            "",
            "-def where(condition: torch.Tensor, x1: torch.Tensor, x2: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "+def where(",
            "+    condition: torch.Tensor,",
            "+    x1: torch.Tensor,",
            "+    x2: torch.Tensor,",
            "+    *,",
            "+    out: Optional[torch.Tensor] = None",
            "+) -> torch.Tensor:",
            "promoted_type = torch.promote_types(x1.dtype, x2.dtype)",
            "x1 = x1.to(promoted_type)",
            "x2 = x2.to(promoted_type)"
        ]
    },
    {
        "number": 5254,
        "comments": "",
        "commit_message": "[torch] Enable GPU training (#1508)\n\n* Added more descriptive description_summary, fixed bug in TabNet combiner\n\n* Updated modules to do proper GPU conversion\n\n* Functional GPU training\n\n* Tabnet working, removed RayRemoteTrainer\n\n* Temporary changes to unblock GPU training\n\n* Functional test_seq_encoder\n\n* Cleaned up PR\n\n* Fixed failing CPU tests\n\n* Responded to comments\n\n* Fix failing GPU tests in test_api.py\n\n* [torch] Fix GPU tests for test_visualizations (#1539)\n\nUpdated torch.aranges to registered buffers in embedding_modules.py\n\n* [torch] Fix test_encoder.py for GPU (#1544)\n\n* Accept and move model to GPU as part of Predictor init (#1559)\n\n* [torch] Fixed tests in test_experiment.py (#1562)\n\n* Fixed tests in tests/integration_tests/test_experiment.py\n\n* Moved set embed and inputs to correct device in tests/ludwig/encoders/test_set_encoders.py\n\n* Moved set date and inputs to correct device in tests/ludwig/encoders/test_date_encoders.py\n\n* Moved bag embed and inputs to correct device in tests/ludwig/encoders/test_bag_encoders.py\n\n* Moved sequence embed and inputs to correct device in tests/ludwig/features/test_sequence_features.py\n\n* [torch] Fixed GPU issues in combiners.py (#1568)\n\n* Fix GPU tests in test_collect.py (#1569)\n\n* Added model to Predictor initialization\n\n* Fixed embedding modules tests (#1554)\n\n* Fixed embedding modules tests\n* Moved embedding modules to device in tests, removed duplicate argument\n\nCo-authored-by: Shreya Rajpal <shreya.rajpal@gmail.com>\n\n* Fixed encoders tests except test_h3_rnn_embed (#1556)\n\n* Fixed encoders tests except test_h3_rnn_embed\n\n* Addressed Shreya's comments\n\n* Moved embedding modules to device, used registered buffers\n\nCo-authored-by: Shreya Rajpal <shreya.rajpal@gmail.com>\n\n* Pass ECD object to Predictor\n\n* Removed model arg from predict function\n\nCo-authored-by: Jeffrey Tang <jeff@predibase.com>\nCo-authored-by: Jeffrey Tang <810895+jeffreyftang@users.noreply.github.com>\nCo-authored-by: Animesh Kumar <anmshkmr@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MSEMetric(MeanSquaredError, LudwigMetric):",
            "super().__init__()",
            "",
            "def update(self, preds: Tensor, target: Tensor) -> None:",
            "-        super().update(preds.detach(), target)",
            "+        super().update(preds, target)",
            "",
            "@classmethod",
            "def get_objective(cls):"
        ]
    },
    {
        "number": 5255,
        "comments": "",
        "commit_message": "[RLlib] Minor cleanup in preparation to tf2.x support. (#9130)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* Fixes.\n\n* Fixes and LINT.\n\n* WIP.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class QLoss:",
            "r_tau = tf.clip_by_value(r_tau, v_min, v_max)",
            "b = (r_tau - v_min) / ((v_max - v_min) / float(num_atoms - 1))",
            "lb = tf.floor(b)",
            "-            ub = tf.ceil(b)",
            "+            ub = tf.math.ceil(b)",
            "# indispensable judgement which is missed in most implementations",
            "# when b happens to be an integer, lb == ub, so pr_j(s', a*) will",
            "# be discarded because (ub-b) == (b-lb) == 0",
            "-            floor_equal_ceil = tf.to_float(tf.less(ub - lb, 0.5))",
            "+            floor_equal_ceil = tf.cast(tf.less(ub - lb, 0.5), tf.float32)",
            "",
            "l_project = tf.one_hot(",
            "tf.cast(lb, dtype=tf.int32),"
        ]
    },
    {
        "number": 5257,
        "comments": "",
        "commit_message": "[metrics] Accuracy num_classes error fix (#3764)\n\n* change accuracy error to warning\n\n* changelog\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_multilabel_accuracy():",
            "assert torch.allclose(accuracy(y2, torch.logical_not(y2), class_reduction='none'), torch.tensor([0., 0.]))",
            "assert torch.allclose(accuracy(y1, torch.logical_not(y1), class_reduction='none'), torch.tensor([0., 0.]))",
            "",
            "-    with pytest.raises(RuntimeError):",
            "-        accuracy(y2, torch.zeros_like(y2), class_reduction='none')",
            "+    # num_classes does not match extracted number from input we expect a warning",
            "+    with pytest.warns(RuntimeWarning,",
            "+                      match=r'You have set .* number of classes which is'",
            "+                            r' different from predicted (.*) and'",
            "+                            r' target (.*) number of classes'):",
            "+        _ = accuracy(y2, torch.zeros_like(y2), num_classes=3)",
            "",
            "",
            "def test_accuracy():"
        ]
    },
    {
        "number": 5263,
        "comments": "",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class EagerModel(TFModelV2):",
            "",
            "def lambda_(x):",
            "eager_out = tf.py_function(self.forward_eager, [x], tf.float32)",
            "-            with tf.control_dependencies([eager_out]):",
            "+            with tf1.control_dependencies([eager_out]):",
            "eager_out.set_shape(x.shape)",
            "return eager_out"
        ]
    },
    {
        "number": 5264,
        "comments": "",
        "commit_message": "fixed style errors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestTorchVariable(TestCase):",
            "\"\"\"Linear transformation of x by w\"\"\"",
            "return x.mm(w)",
            "",
            "-        x = Var(torch.FloatTensor([[1,1],[2,2]]), requires_grad=True)",
            "-        y = Var(torch.FloatTensor([[1,1],[2,2]]), requires_grad=True)",
            "+        x = Var(torch.FloatTensor([[1, 1], [2, 2]]), requires_grad=True)",
            "+        y = Var(torch.FloatTensor([[1, 1], [2, 2]]), requires_grad=True)",
            "",
            "z = linear(x, y)"
        ]
    },
    {
        "number": 5265,
        "comments": "",
        "commit_message": "fix linting and tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_base_storage():",
            "assert len(storage) == 1",
            "assert storage.x is not None",
            "",
            "-    storage = BaseStorage(key='key', parent={}, x=torch.zeros(1))",
            "-",
            "+    storage = BaseStorage(x=torch.zeros(1))",
            "copied_storage = copy.copy(storage)",
            "assert storage == copied_storage",
            "assert id(storage) != id(copied_storage)"
        ]
    },
    {
        "number": 5270,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class IndexField(Field[torch.Tensor]):",
            "@overrides",
            "def as_tensor(self,",
            "padding_lengths: Dict[str, int],",
            "-                  cuda_device: int = -1,",
            "-                  for_training: bool = True) -> torch.Tensor:",
            "+                  cuda_device: int = -1) -> torch.Tensor:",
            "# pylint: disable=unused-argument",
            "-        tensor = Variable(torch.LongTensor([self.sequence_index]), volatile=not for_training)",
            "+        tensor = torch.LongTensor([self.sequence_index])",
            "return tensor if cuda_device == -1 else tensor.cuda(cuda_device)",
            "",
            "@overrides"
        ]
    },
    {
        "number": 5271,
        "comments": "",
        "commit_message": "fixed lm backward compatibility\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def recog(args):",
            "rnnlm_args = get_model_conf(args.word_rnnlm, args.word_rnnlm_conf)",
            "word_dict = rnnlm_args.char_list_dict",
            "char_dict = {x: i for i, x in enumerate(train_args.char_list)}",
            "-        word_rnnlm = lm_pytorch.ClassifierWithState(lm_pytorch.RNNLM(",
            "-            len(word_dict), rnnlm_args.layer, rnnlm_args.unit, rnnlm_args.embed_unit))",
            "+        word_rnnlm = lm_pytorch.ClassifierWithState(",
            "+            lm_pytorch.RNNLM(",
            "+                len(word_dict), rnnlm_args.layer, rnnlm_args.unit,",
            "+                getattr(rnnlm_args, \"embed_unit\", None),  # for backward compatibility",
            "+            )",
            "+        )",
            "torch_load(args.word_rnnlm, word_rnnlm)",
            "word_rnnlm.eval()"
        ]
    },
    {
        "number": 5274,
        "comments": "",
        "commit_message": "bugfix & optional tensorflow implementation (#96)\n\n* bugfix & optional tensorflow implementation\n\n* removed torch max version\n\n* bugfix compressor\n\n* bugfix metric\n\nCo-authored-by: Valerio Sofi <v.sofi@nebuly.ai>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def run_tf_model(",
            "model: tf.Module, input_tensors: Tuple[tf.Tensor]",
            ") -> Tuple[tf.Tensor]:",
            "pred = model.predict(*input_tensors)",
            "-    if isinstance(pred, tf.Module):",
            "+    if isinstance(pred, tf.Module) and pred is not None:",
            "pred = (pred,)",
            "return pred"
        ]
    },
    {
        "number": 5276,
        "comments": "",
        "commit_message": "Fix module reloading during tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def patch(save=True, tensorboardX=tensorboardX_loaded):",
            "writer.EventFileWriter.add_event = add_event(",
            "writer.EventFileWriter.add_event)",
            "wandb.patched[\"tensorboard\"].append(tensorboard_py_module)",
            "-        wandb.patched[\"tensorboard\"].append(\"tensorflow.summary\")",
            "",
            "# This configures TensorFlow 2 style Tensorboard logging",
            "c_writer = wandb.util.get_module(tensorboard_c_module)"
        ]
    },
    {
        "number": 5277,
        "comments": "",
        "commit_message": "fix type\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class FaceAlignment:",
            "out += flip(self.face_alignment_net(flip(inp)).detach(), is_label=True)",
            "out = out.cpu().numpy()",
            "",
            "-            pts, pts_img = get_preds_fromhm(out, center, scale)",
            "-            pts, pts_img = pts.view(68, 2) * 4, pts_img.view(68, 2)",
            "+            pts, pts_img = get_preds_fromhm(out, center.numpy(), scale)",
            "pts, pts_img = torch.from_numpy(pts), torch.from_numpy(pts_img)",
            "+            pts, pts_img = pts.view(68, 2) * 4, pts_img.view(68, 2)",
            "",
            "if self.landmarks_type == LandmarksType._3D:",
            "heatmaps = np.zeros((68, 256, 256), dtype=np.float32)"
        ]
    },
    {
        "number": 5285,
        "comments": "",
        "commit_message": "fix minor bug\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class _AnchorTargetLayer(nn.Module):",
            "disable_inds = bg_inds[rand_num[:bg_inds.size(0)-num_bg]]",
            "labels[i][disable_inds] = -1",
            "",
            "-        offset = torch.arange(0, batch_size)*20",
            "+        offset = torch.arange(0, batch_size)*gt_boxes.size(1)",
            "+",
            "argmax_overlaps = argmax_overlaps + offset.view(batch_size, 1).type_as(argmax_overlaps)",
            "bbox_targets = _compute_targets_batch(anchors, gt_boxes.view(-1,5)[argmax_overlaps.view(-1), :].view(batch_size, -1, 5))"
        ]
    },
    {
        "number": 5287,
        "comments": "",
        "commit_message": "for discussion: incorporate black code formatter (#3308)\n\n* setup files\n\n* run black\n\n* undo\n\n* update CONTRIBUTING.md\n\n* fix quotes in test_other_modules\n\n* make flake8 happy\n\n* set black to 100 characters per line\n\n* move type: ignore to where mypy wants them\n\n* more flake8\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "from allennlp.state_machines import util",
            "",
            "class TestStateMachinesUtil(AllenNlpTestCase):",
            "def test_create_allowed_transitions(self):",
            "-        targets = torch.Tensor([[[2, 3, 4], [1, 3, 4], [1, 2, 4]], [[3, 4, 0], [2, 3, 4], [0, 0, 0]]])",
            "-        target_mask = torch.Tensor([[[1, 1, 1], [1, 1, 1], [1, 1, 1]], [[1, 1, 0], [1, 1, 1], [0, 0, 0]]])",
            "+        targets = torch.Tensor(",
            "+            [[[2, 3, 4], [1, 3, 4], [1, 2, 4]], [[3, 4, 0], [2, 3, 4], [0, 0, 0]]]",
            "+        )",
            "+        target_mask = torch.Tensor(",
            "+            [[[1, 1, 1], [1, 1, 1], [1, 1, 1]], [[1, 1, 0], [1, 1, 1], [0, 0, 0]]]",
            "+        )",
            "prefix_tree = util.construct_prefix_tree(targets, target_mask)",
            "",
            "# There were two instances in this batch."
        ]
    },
    {
        "number": 5289,
        "comments": "",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DeFMO(nn.Module):",
            "# use torch.hub to load pretrained model",
            "if pretrained:",
            "storage_fcn: Callable = lambda storage, loc: storage",
            "-            pretrained_dict = torch.hub.load_state_dict_from_url(",
            "-                urls['defmo_encoder'], map_location=storage_fcn",
            "-            )",
            "+            pretrained_dict = torch.hub.load_state_dict_from_url(urls['defmo_encoder'], map_location=storage_fcn)",
            "self.encoder.load_state_dict(pretrained_dict, strict=True)",
            "-            pretrained_dict_ren = torch.hub.load_state_dict_from_url(",
            "-                urls['defmo_rendering'], map_location=storage_fcn",
            "-            )",
            "+            pretrained_dict_ren = torch.hub.load_state_dict_from_url(urls['defmo_rendering'], map_location=storage_fcn)",
            "self.rendering.load_state_dict(pretrained_dict_ren, strict=True)",
            "self.eval()"
        ]
    },
    {
        "number": 5295,
        "comments": "",
        "commit_message": "[RLlib] Fix all the CI tests that were broken by is_training and replay buffer changes; re-comment-in the failing RLlib tests (#19809)\n\n* Fix DDPG, since it is based on GenericOffPolicyTrainer.\n\n* Fix QMix, SAC, and MADDPA too.\n\n* Undo QMix change.\n\n* Fix DQN input batch type. Always use SampleBatch.\n\n* apex ddpg should not use replay_buffer_config yet.\n\n* Make eager tf policy to use SampleBatch.\n\n* lint\n\n* LINT.\n\n* Re-enable RLlib broken tests to make sure things work ok now.\n\n* fixes.\n\nCo-authored-by: sven1977 <svenmika1977@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def build_eager_tf_policy(",
            "dist_inputs, dist_class, _ = action_distribution_fn(",
            "self,",
            "self.model,",
            "-                    input_dict[SampleBatch.CUR_OBS],",
            "+                    input_batch,",
            "explore=False,",
            "is_training=False)",
            "# Default log-likelihood calculation.",
            "else:",
            "-                dist_inputs, _ = self.model(input_dict, state_batches,",
            "+                dist_inputs, _ = self.model(input_batch, state_batches,",
            "seq_lens)",
            "dist_class = self.dist_class"
        ]
    },
    {
        "number": 5296,
        "comments": "",
        "commit_message": "Minor fix to image retraining example code in retrieving training files\n\nPiperOrigin-RevId: 240981917\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def create_image_lists(image_dir, testing_percentage, validation_percentage):",
            "extensions = sorted(set(os.path.normcase(ext)  # Smash case on Windows.",
            "for ext in ['JPEG', 'JPG', 'jpeg', 'jpg', 'png']))",
            "file_list = []",
            "-    dir_name = os.path.basename(sub_dir)",
            "+    dir_name = os.path.basename(",
            "+        # tf.gfile.Walk() returns sub-directory with trailing '/' when it is in",
            "+        # Google Cloud Storage, which confuses os.path.basename().",
            "+        sub_dir[:-1] if sub_dir.endswith('/') else sub_dir)",
            "+",
            "if dir_name == image_dir:",
            "continue",
            "tf.logging.info(\"Looking for images in '\" + dir_name + \"'\")"
        ]
    },
    {
        "number": 5298,
        "comments": "",
        "commit_message": "fixed ornstein-uhlenbeck exploration variable; pass action_spec instead of action_shape to tf_exploration\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GaussianNoise(Exploration):",
            "",
            "super(GaussianNoise, self).__init__(scope=scope, summary_labels=summary_labels)",
            "",
            "-    def tf_explore(self, episode, timestep, action_shape):",
            "-        return tf.random_normal(shape=action_shape[1:], mean=self.mu, stddev=self.sigma)",
            "+    def tf_explore(self, episode, timestep, action_spec):",
            "+        return tf.random_normal(shape=action_spec['shape'], mean=self.mu, stddev=self.sigma)"
        ]
    },
    {
        "number": 5299,
        "comments": "",
        "commit_message": "Fix train_step, test_step and tests for CLIP (#18684)\n\n* Fix train_step and test_step, correctly enable CLIP fit test\n\n* Stop using get_args on older Python versions\n\n* Don't use get_origin either\n\n* UnionType is actually even newer, don't use that either\n\n* Apply the same fix to test_loss_computation\n\n* Just realized I was accidentally skipping a bunch of tests!\n\n* Fix test_loss_computation for models without separable labels\n\n* Fix scalar losses in test_step and train_step\n\n* Stop committing your breakpoints\n\n* Fix Swin loss shape\n\n* Fix Tapas loss shape\n\n* Shape fixes for TAPAS, DeIT, HuBERT and ViTMAE\n\n* Add loss computation to TFMobileBertForPreTraining\n\n* make fixup and move copied from statement\n\n* make fixup and move copied from statement\n\n* Correct copied from\n\n* Add labels and next_sentence_label inputs to TFMobileBERT\n\n* Make sure total_loss is always defined\n\n* Update tests/test_modeling_tf_common.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Fix copied from\n\n* Ensure CTC models get labels in tests\n\n* Ensure CTC models get labels in tests\n\n* Fix tests for vit_mae\n\n* Fix tests for vit_mae\n\n* Fix tests for vit_mae\n\n* Reduce batch size for wav2vec2 testing because it was causing OOM\n\n* Skip some TAPAS tests that are failing\n\n* Skip a failing HuBERT test\n\n* make style\n\n* Fix mobilebertforpretraining test\n\n* Skip Wav2Vec2 tests that use huge amounts of mem\n\n* Skip keras_fit for Wav2Vec2 as well\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFViTMAEForPreTraining(TFViTMAEPreTrainedModel):",
            "loss = tf.reduce_mean(loss, axis=-1)  # [batch_size, num_patches], mean loss per patch",
            "",
            "loss = tf.reduce_sum(loss * mask) / tf.reduce_sum(mask)  # mean loss on removed patches",
            "+        loss = tf.reshape(loss, (1,))",
            "return loss",
            "",
            "@unpack_inputs"
        ]
    },
    {
        "number": 5301,
        "comments": "",
        "commit_message": "Fix label name in DataCollatorForNextSentencePrediction test (#8048)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DataCollatorIntegrationTest(unittest.TestCase):",
            "total_samples = batch[\"input_ids\"].shape[0]",
            "self.assertEqual(batch[\"input_ids\"].shape, torch.Size((total_samples, 512)))",
            "self.assertEqual(batch[\"token_type_ids\"].shape, torch.Size((total_samples, 512)))",
            "-        self.assertEqual(batch[\"masked_lm_labels\"].shape, torch.Size((total_samples, 512)))",
            "+        self.assertEqual(batch[\"labels\"].shape, torch.Size((total_samples, 512)))",
            "self.assertEqual(batch[\"next_sentence_label\"].shape, torch.Size((total_samples,)))",
            "",
            "@slow"
        ]
    },
    {
        "number": 5302,
        "comments": "",
        "commit_message": "Code for \"Double-Head RCNN: Rethinking Classification and Localization for Object Detection\" (#809)\n\n* add major components for training double head\n\ndouble head only with two losses and no attention\n\nremove double_head detector for now, merge upchannel to double_head bbox head\n\nchange the stype using yapl\n\nremove uncessary comment#\n\nto pass check\n\nto pass v2\n\nto pass v3\n\nline too long and style again\n\nreuse bottlenet\n\n* refactoring\n\n* bug fix\n\n* bug fix for weight initialization\n\n* add reg roi scale factor and modify loss weights\n\n* rescale the roi after mapping to fpn levels\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SingleRoIExtractor(nn.Module):",
            "target_lvls = self.map_roi_levels(rois, num_levels)",
            "roi_feats = feats[0].new_zeros(rois.size()[0], self.out_channels,",
            "out_size, out_size)",
            "+        if roi_scale_factor is not None:",
            "+            rois = self.roi_rescale(rois, roi_scale_factor)",
            "for i in range(num_levels):",
            "inds = target_lvls == i",
            "if inds.any():"
        ]
    },
    {
        "number": 5306,
        "comments": "",
        "commit_message": "Fix various LossScaleOptimizer issues.\n\nIn particular, fix crash when saving LossScaleOptimizer with h5, and when passing LossScaleOptimizer to convert_to_legacy_optimizer(). Also change mixed_precision/model_test.py to use the new optimizer instead of the legacy optimizer when TF2 is used.\n\nFixes https://github.com/keras-team/keras/issues/17275\n\nPiperOrigin-RevId: 494221589\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(training_lib.Model):",
            "if not isinstance(self.optimizer, optimizer_v2.OptimizerV2):",
            "raise ValueError(",
            "'\"optimizer\" must be an instance of '",
            "-                    \"tf.keras.optimizers.Optimizer when a dype policy \"",
            "-                    \"with a loss scale  used, but got: %s. Using policy: \"",
            "+                    \"tf.keras.optimizers.legacy.Optimizer when a dype policy \"",
            "+                    \"with a loss scale is used, but got: %s. Using policy: \"",
            "\"%s\" % (self.optimizer, self._dtype_policy)",
            ")",
            "self.optimizer = loss_scale_optimizer.LossScaleOptimizer("
        ]
    },
    {
        "number": 5309,
        "comments": "",
        "commit_message": "fix crossentropy size in chatbot to align mask\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LuongAttnDecoderRNN(nn.Module):",
            "",
            "def maskNLLLoss(inp, target, mask):",
            "nTotal = mask.sum()",
            "-    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)))",
            "+    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))",
            "loss = crossEntropy.masked_select(mask).mean()",
            "loss = loss.to(device)",
            "return loss, nTotal.item()"
        ]
    },
    {
        "number": 5310,
        "comments": "",
        "commit_message": "fix test for dtype\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CTCPrefixScorer(PartialScorerInterface):",
            "def score_partial(self, y, ids, state, x):",
            "prev_score, state = state",
            "presub_score, new_st = self.impl(y.cpu(), ids.cpu(), state)",
            "-        tscore = torch.as_tensor(presub_score - prev_score, device=y.device)",
            "+        tscore = torch.as_tensor(presub_score - prev_score, device=x.device, dtype=x.dtype)",
            "return tscore, (presub_score, new_st)"
        ]
    },
    {
        "number": 5311,
        "comments": "",
        "commit_message": "PointConv fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PointConv(MessagePassing):",
            "N, M = edge_index[0].max().item() + 1, edge_index[1].max().item() + 1",
            "return self.propagate(edge_index, size=(N, M), x=x, pos=pos)",
            "",
            "-    def message(self, x_i, pos_i, pos_j):",
            "-        msg = pos_i - pos_j",
            "-        if x_i is not None:",
            "-            msg = torch.cat([x_i, msg], dim=1)",
            "+    def message(self, x_j, pos_j, pos_i):",
            "+        msg = pos_j - pos_i",
            "+        if x_j is not None:",
            "+            msg = torch.cat([x_j, msg], dim=1)",
            "if self.local_nn is not None:",
            "msg = self.local_nn(msg)",
            "return msg"
        ]
    },
    {
        "number": 5312,
        "comments": "",
        "commit_message": "EMA bug fix 2 (#2330)\n\n* EMA bug fix 2\n\n* update\n",
        "label": "",
        "answer": "no",
        "change": [
            "def attempt_load(weights, map_location=None):",
            "model = Ensemble()",
            "for w in weights if isinstance(weights, list) else [weights]:",
            "attempt_download(w)",
            "-        model.append(torch.load(w, map_location=map_location)['model'].float().fuse().eval())  # load FP32 model",
            "+        ckpt = torch.load(w, map_location=map_location)  # load",
            "+        model.append(ckpt['ema' if ckpt.get('ema') else 'model'].float().fuse().eval())  # FP32 model",
            "",
            "# Compatibility updates",
            "for m in model.modules():"
        ]
    },
    {
        "number": 5313,
        "comments": "",
        "commit_message": "small device handling fixes.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "from ivy.core.container import Container",
            "",
            "",
            "def variable(x):",
            "-    with _tf.device('/' + ivy.dev(x).upper()):",
            "+    with _tf.device('/' + ivy.dev(x, as_str=True).upper()):",
            "return _tf.Variable(x, trainable=True)"
        ]
    },
    {
        "number": 5314,
        "comments": "",
        "commit_message": "SRL eval script (#220)\n\n* script for SRL evaluation\n\n* rename script\n\n* script tweaks\n\n* fix for sentences with no verbal predicates\n\n* change name and remove perl script bit\n\n* script for SRL evaluation\n\n* rename script\n\n* script tweaks\n\n* fix for sentences with no verbal predicates\n\n* change name and remove perl script bit\n\n* make evaluation faster by doing batch prediction\n\n* spacing\n\n* fix viterbi decoding in SRL model, edit script to use model.decode\n\n* make forward_on_instance call decode\n\n* pylint\n\n* fix out of date docstring\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Model(torch.nn.Module, Registrable):",
            "add_batch_dimension=True,",
            "cuda_device=cuda_device,",
            "for_training=False)",
            "-        outputs = self.forward(**model_input)",
            "+        outputs = self.decode(self.forward(**model_input))",
            "",
            "for name, output in list(outputs.items()):",
            "output = output[0]"
        ]
    },
    {
        "number": 5315,
        "comments": "",
        "commit_message": "Use f-strings in the dataset scripts (#3291)\n\n* Finishes #3257\n\nUsed f-strings to format the .py files in the dataset folder\n\n* Fix style\n\n* Fix hkcancor dataset\n\nCo-authored-by: Mario \u0160a\u0161ko <mario@huggingface.co>\nCo-authored-by: mariosasko <mariosasko777@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Reclor(datasets.GeneratorBasedBuilder):",
            "",
            "if not os.path.exists(data_dir):",
            "raise FileNotFoundError(",
            "-                \"{} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('wikihow', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {}\".format(",
            "-                    data_dir, self.manual_download_instructions",
            "-                )",
            "+                f\"{data_dir} does not exist. Make sure you insert a manual dir via `datasets.load_dataset('wikihow', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: {self.manual_download_instructions}\"",
            ")",
            "return [",
            "datasets.SplitGenerator("
        ]
    },
    {
        "number": 5316,
        "comments": "",
        "commit_message": "misc fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def build_GAN_losses(vecpos, vecneg):",
            "tf.histogram_summary('sigmoid-neg', sigmneg)",
            "",
            "d_loss_pos = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(",
            "-        vecpos, tf.ones_like(vecpos)), name='d_loss_pos')",
            "+        vecpos, tf.ones_like(vecpos)), name='d_CE_loss_pos')",
            "d_loss_neg = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(",
            "-        vecneg, tf.zeros_like(vecneg)), name='d_loss_neg')",
            "+        vecneg, tf.zeros_like(vecneg)), name='d_CE_loss_neg')",
            "",
            "d_pos_acc = tf.reduce_mean(tf.cast(sigmpos > 0.5, tf.float32), name='pos_acc')",
            "d_neg_acc = tf.reduce_mean(tf.cast(sigmneg < 0.5, tf.float32), name='neg_acc')",
            "",
            "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(",
            "-        vecneg, tf.ones_like(vecneg)), name='g_loss')",
            "-    d_loss = tf.add(d_loss_pos, d_loss_neg, name='d_loss')",
            "+        vecneg, tf.ones_like(vecneg)), name='g_CE_loss')",
            "+    d_loss = tf.add(d_loss_pos, d_loss_neg, name='d_CE_loss')",
            "add_moving_summary(d_loss_pos, d_loss_neg,",
            "g_loss, d_loss,",
            "d_pos_acc, d_neg_acc)"
        ]
    },
    {
        "number": 5320,
        "comments": "",
        "commit_message": "fix: typo spelling, fixed indentation\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DenseFeaturesTest(keras_parameterized.TestCase):",
            "with tf.Graph().as_default():",
            "# Dynamic rank 0 should fail",
            "features = {",
            "-          'price': tf.compat.v1.placeholder(tf.float32),",
            "+        'price': tf.compat.v1.placeholder(tf.float32),",
            "}",
            "net = df.DenseFeatures([price])(features)",
            "self.assertEqual(1, net.shape[1])"
        ]
    },
    {
        "number": 5321,
        "comments": "",
        "commit_message": "lintfixbot: Auto-commit fixed lint errors in codebase\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def real(",
            "",
            "",
            "def isposinf(",
            "-        x: Union[tf.Tensor, tf.Variable],",
            "-        /,",
            "-        *,",
            "-        out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            "+    x: Union[tf.Tensor, tf.Variable],",
            "+    /,",
            "+    *,",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "return tf.experimental.numpy.isposinf(x)"
        ]
    },
    {
        "number": 5322,
        "comments": "",
        "commit_message": "small fix inplace_update to pass tf 'inv' in array api tests (#6288)\n\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def inplace_update(",
            "elif ivy.is_ivy_array(x):",
            "x.data = val_native",
            "else:",
            "-            raise ivy.exceptions.IvyException(",
            "-                \"TensorFlow does not support inplace updates of the tf.Tensor\"",
            "-            )",
            "+            x = ivy.to_ivy(x_native)",
            "return x",
            "else:",
            "return val"
        ]
    },
    {
        "number": 5325,
        "comments": "",
        "commit_message": "Fixed small issue with reshape\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def reshape(",
            "shape: Union[ivy.NativeShape, Sequence[int]],",
            "*,",
            "copy: Optional[bool] = None,",
            "-    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            "order: Optional[str] = \"C\",",
            "+    out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "ivy.assertions.check_elem_in_list(order, [\"C\", \"F\"])",
            "if copy:"
        ]
    },
    {
        "number": 5327,
        "comments": "",
        "commit_message": "[rllib] Make Pong-v0 + EvolutionStrategies work by sharing preprocessors with PPO (#848)\n\n* fix by sharing preprocessors\n\n* revert param changeg\n\n* Update evolution_strategies.py\n\n* Update catalog.py\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Agent(object):",
            "self.common_policy = self.par_opt.get_common_loss()",
            "self.variables = ray.experimental.TensorFlowVariables(",
            "self.common_policy.loss, self.sess)",
            "-        self.observation_filter = MeanStdFilter(preprocessor.shape, clip=None)",
            "+        self.observation_filter = MeanStdFilter(",
            "+            self.preprocessor_shape, clip=None)",
            "self.reward_filter = MeanStdFilter((), clip=5.0)",
            "self.sess.run(tf.global_variables_initializer())"
        ]
    },
    {
        "number": 5331,
        "comments": "",
        "commit_message": "[WIP] - Documentation Cleaning and Coding Style (#516)\n\n* Activation Cleaning Docstring Test\n\n* Requirements Pinned with range to insure tested versions are used. Range are used to prevent updating requirements all the time.\n\n* setup.cfg file added with PEP8 configuration\n\n* activation.py refactored\n\n* docstring fixed - ready for documentation unittest\n\n* Yapf correction for max_line_length: 120\n\n* test yapf refactored\n\n* Requirements conflict solved\n\n* Yapf Style modified and merged in file \"setup.cfg\"\n\n* Yapf Confiuguration Updated\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* Code Refactored with new YAPF formating style\n\n* tl.layers.pooling YAPF reformat\n\n* yapf updated\n\n* gitignore updated\n\n* YAPF Style Fixing Attempt\n\n* Space Error Fix\n\n* Style Correction\n\n* Assertion Codacy Errors Corrected\n\n* Error Fix\n\n* Assertion Refactored\n\n* YAPF Style Applied to Master\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "img, label = read_and_decode(\"train.cifar10\")",
            "",
            "## Use shuffle_batch or batch",
            "# see https://www.tensorflow.org/versions/master/api_docs/python/io_ops.html#shuffle_batch",
            "-img_batch, label_batch = tf.train.shuffle_batch([img, label], batch_size=4, capacity=50000, min_after_dequeue=10000, num_threads=1)",
            "+img_batch, label_batch = tf.train.shuffle_batch(",
            "+    [img, label], batch_size=4, capacity=50000, min_after_dequeue=10000, num_threads=1",
            "+)",
            "",
            "print(\"img_batch   : %s\" % img_batch._shape)",
            "print(\"label_batch : %s\" % label_batch._shape)"
        ]
    },
    {
        "number": 5333,
        "comments": "",
        "commit_message": "Remove AttributeError trap (#241)\n\n* Remove AttributeError trap\n\n* Trap AttributeError in run_pass\n\n* One more fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SummaryWriter(object):",
            "print('add_graph() only supports PyTorch v0.2.')",
            "return",
            "self.file_writer.add_graph(graph(model, input_to_model, verbose))",
            "-        except AttributeError:",
            "+        else:",
            "# Caffe2 models do not have the 'forward' method",
            "if not self.caffe2_enabled:",
            "# TODO (ml7): Remove when PyTorch 1.0 merges PyTorch and Caffe2"
        ]
    },
    {
        "number": 5334,
        "comments": "",
        "commit_message": "Fix issue: #1962, input's shape seem to cause error in 2.2.0 version tf_albert_model\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFAlbertModel(TFAlbertPreTrainedModel):",
            "if input_ids is not None and inputs_embeds is not None:",
            "raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")",
            "elif input_ids is not None:",
            "-            input_shape = input_ids.shape",
            "+            input_shape = tf.shape(input_ids)",
            "elif inputs_embeds is not None:",
            "input_shape = inputs_embeds.shape[:-1]",
            "else:"
        ]
    },
    {
        "number": 5337,
        "comments": "",
        "commit_message": "Fix dropout\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class RelationExtractor(flair.nn.DefaultClassifier):",
            "",
            "relation_embeddings.append(embedding)",
            "",
            "-            # stack and drop out",
            "-            all_relations = torch.stack(relation_embeddings)",
            "+            # stack and drop out (squeeze and unsqueeze)",
            "+            all_relations = torch.stack(relation_embeddings).unsqueeze(1)",
            "",
            "all_relations = self.dropout(all_relations)",
            "all_relations = self.locked_dropout(all_relations)",
            "all_relations = self.word_dropout(all_relations)",
            "",
            "+            all_relations = all_relations.squeeze(1)",
            "+",
            "# send through decoder",
            "if self.non_linear_decoder:",
            "sentence_relation_scores = self.decoder_2(self.nonlinearity(self.decoder_1(all_relations)))"
        ]
    },
    {
        "number": 5340,
        "comments": "",
        "commit_message": "* Clean up documentation of Hull-White and HJM.\n* Fix treatment of initial_discount_rate_fn by Hull-White model.\n* Reduce test size for hjm/swaption_pricing_test\n\nPiperOrigin-RevId: 379671870\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class HullWhiteBermudanSwaptionTest(parameterized.TestCase, tf.test.TestCase):",
            "self.float_leg_end_times) - np.array(self.float_leg_start_times)",
            "self.fixed_leg_daycount_fractions = self.float_leg_daycount_fractions",
            "self.fixed_leg_coupon = 0.011 * np.ones_like(self.fixed_leg_payment_times)",
            "-    self.zero_rate_fn = lambda x: 0.01 * tf.ones_like(x)",
            "+    zero_rate_fn = lambda x: 0.01 * tf.expand_dims(tf.ones_like(x), axis=-1)",
            "+    self.zero_rate_fn = zero_rate_fn",
            "",
            "super(HullWhiteBermudanSwaptionTest, self).setUp()"
        ]
    },
    {
        "number": 5341,
        "comments": "",
        "commit_message": "fix build\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CenterModel(EmbeddingModel):",
            "",
            "# tag the embedding of 'input' with name 'emb', just for inference later on",
            "with tf.variable_scope(tf.get_variable_scope(), reuse=True):",
            "-            tf.identity(self.embed(inputs[0]), name=\"emb\")",
            "+            tf.identity(self.embed(x), name=\"emb\")",
            "",
            "# compute the embedding loss",
            "emb_cost = center_loss(x, label, 10, 0.01)"
        ]
    },
    {
        "number": 5344,
        "comments": "",
        "commit_message": "small fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class HypernetworkModule(torch.nn.Module):",
            "if add_layer_norm:",
            "linears.append(torch.nn.LayerNorm(int(dim * layer_structure[i+1])))",
            "",
            "-            # Add dropout",
            "-            if use_dropout:",
            "-                p = 0.5 if 0 <= i <= len(layer_structure) - 3 else 0.2",
            "-                linears.append(torch.nn.Dropout(p=p))",
            "+            # Add dropout expect last layer",
            "+            if use_dropout and i < len(layer_structure) - 3:",
            "+                linears.append(torch.nn.Dropout(p=0.3))",
            "",
            "self.linear = torch.nn.Sequential(*linears)"
        ]
    },
    {
        "number": 5345,
        "comments": "",
        "commit_message": "Fix bug in Graves Attn\n\nOn my machine at Graves attention the variable self.J ( self.J = torch.arange(0, inputs.shape[1]+2).to(inputs.device) + 0.5) is a LongTensor, but it must be a float tensor. So I get the following error:\n\nTraceback (most recent call last):\n  File \"train.py\", line 704, in <module>\n    main(args)\n  File \"train.py\", line 619, in main\n    global_step, epoch)\n  File \"train.py\", line 170, in train\n    text_input, text_lengths, mel_input, speaker_embeddings=speaker_embeddings)\n  File \"/home/edresson/anaconda3/envs/TTS2/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 489, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/mnt/edresson/DD/TTS/voice-clonning/TTS/tts_namespace/TTS/models/tacotron.py\", line 121, in forward\n    self.speaker_embeddings_projected)\n  File \"/home/edresson/anaconda3/envs/TTS2/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 489, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/mnt/edresson/DD/TTS/voice-clonning/TTS/tts_namespace/TTS/layers/tacotron.py\", line 435, in forward\n    output, stop_token, attention = self.decode(inputs, mask)\n  File \"/mnt/edresson/DD/TTS/voice-clonning/TTS/tts_namespace/TTS/layers/tacotron.py\", line 367, in decode\n    self.attention_rnn_hidden, inputs, self.processed_inputs, mask)\n  File \"/home/edresson/anaconda3/envs/TTS2/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 489, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/mnt/edresson/DD/TTS/voice-clonning/TTS/tts_namespace/TTS/layers/common_layers.py\", line 180, in forward\n    phi_t = g_t.unsqueeze(-1) * (1.0 / (1.0 + torch.sigmoid((mu_t.unsqueeze(-1) - j) / sig_t.unsqueeze(-1))))\nRuntimeError: expected type torch.cuda.FloatTensor but got torch.cuda.LongTensor\n\n\nIn addition the + 0.5 operation is canceled if it is a LongTensor.\nTest: \n>>> torch.arange(0, 10) \ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n>>> torch.arange(0, 10) + 0.5\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n>>> torch.arange(0, 10.0) + 0.5\ntensor([0.5000, 1.5000, 2.5000, 3.5000, 4.5000, 5.5000, 6.5000, 7.5000, 8.5000,\n        9.5000])\n\nTo resolve this I forced the arrange range to float:\nself.J = torch.arange(0, inputs.shape[1]+2.0).to(inputs.device) + 0.5\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GravesAttention(nn.Module):",
            "",
            "def init_states(self, inputs):",
            "if self.J is None or inputs.shape[1]+1 > self.J.shape[-1]:",
            "-            self.J = torch.arange(0, inputs.shape[1]+2).to(inputs.device) + 0.5",
            "+            self.J = torch.arange(0, inputs.shape[1]+2.0).to(inputs.device) + 0.5",
            "self.attention_weights = torch.zeros(inputs.shape[0], inputs.shape[1]).to(inputs.device)",
            "self.mu_prev = torch.zeros(inputs.shape[0], self.K).to(inputs.device)"
        ]
    },
    {
        "number": 5347,
        "comments": "",
        "commit_message": "upgrade to pytorch 0.4.0 (#1126)\n\n* bump pytorch to 0.4 + fix sanitize\n\n* remove check for Variable in block_orthogonal\n\n* rename parameter split_size=\n\n* remove checks for Variable\n\n* fix more tests\n\n* fixes\n\n* get tests to pass\n\n* fix warnings\n\n* get rid of some of the Variables\n\n* more tests passing\n\n* more elimination of variables\n\n* finish removing all Variables\n\n* pylint and such\n\n* a few fixes\n\n* move torch.no_grad into model.forward_on_instances\n\n* more pytorch 0.4 changes\n\n* detach() -> data\n\n* pylint\n\n* fix bad tensor creation\n\n* fix types\n\n* remove print statement\n\n* more 0.4 goodness\n\n* factor out is_tensor\n\n* add no_grad to elmo command\n\n* cleanup\n\n* remove TODO\n\n* address PR feedback\n\n* replace all() with item()\n\n* more cleanup\n\n* further cleanup\n\n* remove Variable\n\n* really fix merge conflict\n\n* fix pylint\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TestSparseClipGrad(AllenNlpTestCase):",
            "# Now try to clip the gradients.",
            "_ = sparse_clip_norm([embedding.weight], 1.5)",
            "# Final norm should be 1.5",
            "-        grad = embedding.weight.grad.data.coalesce()",
            "-        self.assertAlmostEqual(grad._values().norm(2.0), 1.5, places=5) # pylint: disable=protected-access",
            "+        grad = embedding.weight.grad.coalesce()  # pylint: disable=no-member",
            "+        self.assertAlmostEqual(grad._values().norm(2.0).item(), 1.5, places=5) # pylint: disable=protected-access"
        ]
    },
    {
        "number": 5348,
        "comments": "",
        "commit_message": "[Testing] Fix LINT/sphinx errors. (#8874)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DQNTorchModel(TorchModelV2):",
            "sigma0 (float): initial value of noisy nets",
            "add_layer_norm (bool): Enable layer norm (for param noise).",
            "\"\"\"",
            "-",
            "+        nn.Module.__init__(self)",
            "super(DQNTorchModel, self).__init__(obs_space, action_space,",
            "num_outputs, model_config, name)"
        ]
    },
    {
        "number": 5349,
        "comments": "",
        "commit_message": "Fix code form\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestHlsToRgb:",
            "[8., 8.]]])  # 3x2x2",
            "",
            "data = torch.tensor([[[0.0641, 0.07138],",
            "-                                  [0.07138, 0.07138]],",
            "+                                [0.07138, 0.07138]],",
            "",
            "-                                 [[0.0569, 0.0588],",
            "-                                  [0.0588, 0.0588]],",
            "+                                [[0.0569, 0.0588],",
            "+                                 [0.0588, 0.0588]],",
            "",
            "-                                 [[0.4483, 0.4667],",
            "-                                  [0.4667, 0.4667]]])  # 3x2x2",
            "+                                [[0.4483, 0.4667],",
            "+                                 [0.4667, 0.4667]]])  # 3x2x2",
            "",
            "f = kornia.color.HlsToRgb()",
            "data = data.repeat(2, 1, 1, 1)  # 2x3x2x2"
        ]
    },
    {
        "number": 5352,
        "comments": "",
        "commit_message": "fixing failing tests of miscellaneous_ops\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def diagonal(",
            "axis2: int = -1,",
            "out: Optional[Union[tf.Tensor, tf.Variable]] = None,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "-    return tf.experimental.numpy.diagonal(x, offset, axis1=axis1, axis2=axis2)",
            "+    return tf.experimental.numpy.diagonal(x, offset=offset, axis1=axis1, axis2=axis2)",
            "",
            "",
            "@with_unsupported_dtypes({\"2.9.1 and below\": (\"float16\", \"bfloat16\")}, backend_version)"
        ]
    },
    {
        "number": 5353,
        "comments": "",
        "commit_message": "drop duplicate metrics (#5014)\n\n* drop duplicate metrics\n\n* keep\n\n* fix\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _explained_variance_compute(preds: torch.Tensor,",
            "return torch.sum(denominator / denom_sum * output_scores)",
            "",
            "",
            "-def explained_variance(preds: torch.Tensor,",
            "-                       target: torch.Tensor,",
            "-                       multioutput: str = 'uniform_average',",
            "-                       ) -> Union[torch.Tensor, Sequence[torch.Tensor]]:",
            "+def explained_variance(",
            "+        preds: torch.Tensor,",
            "+        target: torch.Tensor,",
            "+        multioutput: str = 'uniform_average',",
            "+) -> Union[torch.Tensor, Sequence[torch.Tensor]]:",
            "\"\"\"",
            "Computes explained variance."
        ]
    },
    {
        "number": 5355,
        "comments": "",
        "commit_message": "Fix examples and tutorials to use the updated tensor API (#886)\n\n* Fix examples and tutorials to use the updated tensor API\n\n* small fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ConcatModule(nn.Module):",
            "input_args = input_args[0]",
            "",
            "# don't concat things that are just single objects",
            "-        if torch.is_tensor(input_args) or isinstance(input_args, torch.autograd.Variable):",
            "+        if torch.is_tensor(input_args):",
            "return input_args",
            "else:",
            "return torch.cat(input_args, dim=-1)"
        ]
    },
    {
        "number": 5357,
        "comments": "",
        "commit_message": "py2.7 fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SplineGCN(Module):",
            "self.kernel_size = kernel_size",
            "self.spline_degree = spline_degree",
            "",
            "-        self.weight = Parameter(",
            "-            torch.Tensor(*kernel_size, in_features, out_features))",
            "+        weight_size = kernel_size + (in_features, out_features)",
            "+        self.weight = Parameter(torch.Tensor(*weight_size))",
            "",
            "if bias:",
            "self.bias = Parameter(torch.Tensor(out_features))"
        ]
    },
    {
        "number": 5359,
        "comments": "",
        "commit_message": "[RLlib] Attention Net prep PR #1: Smaller cleanups. (#12447)\n\n* WIP.\n\n* Fix.\n\n* Fix.\n\n* Fix.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TorchPolicy(Policy):",
            "",
            "all_grads = []",
            "for i, opt in enumerate(self._optimizers):",
            "+            # Erase gradients in all vars of this optimizer.",
            "opt.zero_grad()",
            "# Recompute gradients of loss over all variables.",
            "loss_out[i].backward(retain_graph=(i < len(self._optimizers) - 1))"
        ]
    },
    {
        "number": 5363,
        "comments": "",
        "commit_message": "[Flax] Example scripts - correct weight decay  (#12409)\n\n* fix_torch_device_generate_test\n\n* remove @\n\n* finish\n\n* finish\n\n* correct style\n",
        "label": "",
        "answer": "no",
        "change": [
            "if __name__ == \"__main__\":",
            "# to bias and LayerNorm scale parameters. decay_mask_fn returns a",
            "# mask boolean with the same structure as the parameters.",
            "# The mask is True for parameters that should be decayed.",
            "+    # Note that this mask is specifically adapted for FlaxBERT-like models.",
            "+    # For other models, one should correct the layer norm parameter naming",
            "+    # accordingly.",
            "def decay_mask_fn(params):",
            "flat_params = traverse_util.flatten_dict(params)",
            "flat_mask = {path: (path[-1] != \"bias\" and path[-2:] != (\"LayerNorm\", \"scale\")) for path in flat_params}"
        ]
    },
    {
        "number": 5367,
        "comments": "",
        "commit_message": "Scope TokenIndexer output by indexer name (#3597)\n\n* Indexer tests are now passing, at least\n\n* Fixed some masking issues, and padding keys for one config file\n\n* TextFieldEmbedder tests pass\n\n* Fixing fixtures, some more tests passing\n\n* Fixed weird ordering bug\n\n* All TokenEmbedder tests passing?\n\n* Update fixtures\n\n* Fix more hard-coded references\n\n* fix field tests\n\n* fix dataset reader tests\n\n* Fix iterator tests\n\n* More tests passing\n\n* fix hotflip and some other tests\n\n* more tests\n\n* more test fixes\n\n* more tests\n\n* most tests passing; I think the remaining ones are spacy model changes\n\n* hard-code POS tag test\n\n* last test, I think\n\n* black\n\n* updated black\n\n* flake8\n\n* mypy\n\n* black again\n\n* fix training configs\n\n* remove reference to embedder_to_indexer_map\n\n* Other fixes from PR comments\n\n* fix breakage from incorrect merge during rebase\n\n* flake, some docstring formatting\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BertForClassification(Model):",
            "loss : torch.FloatTensor, optional",
            "A scalar loss to be optimised.",
            "\"\"\"",
            "-        input_ids = tokens[self._index]",
            "-        token_type_ids = tokens[f\"{self._index}-type-ids\"]",
            "+        inputs = tokens[self._index]",
            "+        input_ids = inputs[\"input_ids\"]",
            "+        token_type_ids = inputs[\"token_type_ids\"]",
            "input_mask = (input_ids != 0).long()",
            "",
            "_, pooled = self.bert_model("
        ]
    },
    {
        "number": 5370,
        "comments": "",
        "commit_message": "vpg/trpo/dqn tests, cat policy fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CategoricalOneHotPolicy(StochasticPolicy):",
            "",
            "def __init__(self, network, session, state, random, action_count=1, scope='policy'):",
            "with tf.variable_scope(scope):",
            "-            action_layer = linear(layer_input=network.output, config={'num_outputs': action_count}, scope='outputs')",
            "-",
            "-            distribution = tf.nn.softmax(action_layer)",
            "-            sample = tf.map_fn(lambda t: tf.multinomial(logits=t, num_samples=1), elems=distribution, dtype=tf.int64)",
            "+            logits = linear(layer_input=network.output, config={'num_outputs': action_count}, scope='outputs')",
            "+            distribution = tf.nn.softmax(logits)",
            "+            sample = tf.map_fn(lambda t: tf.multinomial(logits=t, num_samples=1), elems=logits, dtype=tf.int64)",
            "",
            "super(CategoricalOneHotPolicy, self).__init__(network, [distribution, sample], session, state, random, action_count)",
            "self.dist = Categorical(random)"
        ]
    },
    {
        "number": 5371,
        "comments": "",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "label": "",
        "answer": "no",
        "change": [
            "def conv_quad_interp3d(",
            "raise ValueError(f\"Invalid input shape, we expect BxCxDxHxW. Got: {input.shape}\")",
            "",
            "B, CH, D, H, W = input.shape",
            "-    dev: torch.device = input.device",
            "grid_global: torch.Tensor = create_meshgrid3d(D, H, W, False, device=input.device).permute(0, 4, 1, 2, 3)",
            "grid_global = grid_global.to(input.dtype)"
        ]
    },
    {
        "number": 5377,
        "comments": "",
        "commit_message": "fix TF deprecation of concat/split/pack/unpack\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "b5 = branch('branch5', l, 16)",
            "",
            "final_map = Conv2D('convfcweight',",
            "-                           tf.concat(3, [b1, b2, b3, b4, b5]), 1, 1,",
            "+                           tf.concat_v2([b1, b2, b3, b4, b5], 3), 1, 1,",
            "W_init=tf.constant_initializer(0.2),",
            "use_bias=False, nl=tf.identity)",
            "costs = []"
        ]
    },
    {
        "number": 5380,
        "comments": "",
        "commit_message": "fixed a bug in the case of multi-gpu\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Decoder(torch.nn.Module):",
            "if torch_is_old:",
            "vy = Variable(h.data.new(1).zero_().long(), volatile=True)",
            "else:",
            "-            torch.set_grad_enabled(False)",
            "vy = h.new_zeros(1).long()",
            "",
            "if recog_args.maxlenratio == 0:"
        ]
    },
    {
        "number": 5384,
        "comments": "",
        "commit_message": "fix some bugs\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def start_train(config):",
            "raise",
            "finally:",
            "coord.request_stop()",
            "-            queue.close(cancel_pending_enqueues=True)",
            "+            input_queue.close(cancel_pending_enqueues=True)",
            "callbacks.after_train()",
            "sess.close()"
        ]
    },
    {
        "number": 5388,
        "comments": "",
        "commit_message": "Fixes tests still using old layout\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_recognition_results_with_lm(etype, m_str, text_idx1):",
            "",
            "",
            "@pytest.mark.parametrize((\"etype\", \"m_str\"), [",
            "-    (\"blstmp\", \"espnet.nets.e2e_asr\"),",
            "-    (\"blstmp\", \"espnet.nets.e2e_asr_th\"),",
            "-    (\"vggblstmp\", \"espnet.nets.e2e_asr\"),",
            "-    (\"vggblstmp\", \"espnet.nets.e2e_asr_th\"),",
            "+    (\"blstmp\", \"espnet.nets.chainer.e2e_asr\"),",
            "+    (\"blstmp\", \"espnet.nets.pytorch.e2e_asr_th\"),",
            "+    (\"vggblstmp\", \"espnet.nets.chainer.e2e_asr\"),",
            "+    (\"vggblstmp\", \"espnet.nets.pytorch.e2e_asr_th\"),",
            "])",
            "def test_batch_beam_search(etype, m_str):",
            "const = 1e-4"
        ]
    },
    {
        "number": 5390,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestBiMPMMatching(AllenNlpTestCase):",
            "mask1 = []",
            "for w in seq_len1:",
            "mask1.append([1] * w.item() + [0] * (len1 - w.item()))",
            "-        mask1 = torch.FloatTensor(mask1)",
            "+        mask1 = torch.BoolTensor(mask1)",
            "mask2 = []",
            "for w in seq_len2:",
            "mask2.append([1] * w.item() + [0] * (len2 - w.item()))",
            "-        mask2 = torch.FloatTensor(mask2)",
            "+        mask2 = torch.BoolTensor(mask2)",
            "",
            "d = 200  # hidden dimension",
            "n = 20  # number of perspective"
        ]
    },
    {
        "number": 5392,
        "comments": "",
        "commit_message": "Fix broken readthedocs (#1990)\n\n* override rtd settings\n\n* change torch version\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def setup(app):",
            "",
            "# @jpchen's hack to get rtd builder to install latest pytorch",
            "if 'READTHEDOCS' in os.environ:",
            "-    os.system('pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp27-cp27mu-linux_x86_64.whl')",
            "+    os.system('pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp36-cp36m-linux_x86_64.whl')"
        ]
    },
    {
        "number": 5394,
        "comments": "",
        "commit_message": "added Winogrande debiased subset (#655)\n\n* added Winogrande debiased subset\n\n* fixed dymmy data\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Winogrande(datasets.GeneratorBasedBuilder):",
            "# TODO(winogrande): Set up version.",
            "VERSION = datasets.Version(\"1.1.0\")",
            "BUILDER_CONFIGS = [",
            "-        WinograndeConfig(name=\"winogrande_\" + size, description=\"AI2 dataset\", data_size=size) for size in _SIZES",
            "+        WinograndeConfig(name=\"winogrande_\" + data_size, description=\"AI2 dataset\", data_size=data_size)",
            "+        for data_size in _FORMATS",
            "]",
            "",
            "def _info(self):"
        ]
    },
    {
        "number": 5398,
        "comments": "",
        "commit_message": "Fixed some of the model definitions\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def main(args):",
            "",
            "# Get input and output tensors",
            "images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")",
            "+            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")",
            "embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")",
            "tpr, fpr, accuracy, val, val_std, far = lfw.validate(sess, paths,",
            "actual_issame, args.seed, 60,",
            "-                images_placeholder, embeddings, nrof_folds=args.lfw_nrof_folds)",
            "+                images_placeholder, phase_train_placeholder, embeddings, nrof_folds=args.lfw_nrof_folds)",
            "print('Accuracy: %1.3f+-%1.3f' % (np.mean(accuracy), np.std(accuracy)))",
            "print('Validation rate: %2.5f+-%2.5f @ FAR=%2.5f' % (val, val_std, far))"
        ]
    },
    {
        "number": 5399,
        "comments": "",
        "commit_message": "[T5, examples] replace heavy t5 models with tiny random models (#3556)\n\n* replace heavy t5 models with tiny random models as was done by sshleifer\n\n* fix isort\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestT5Examples(unittest.TestCase):",
            "output_file_name = Path(tempfile.gettempdir()) / \"utest_output_t5_sum.hypo\"",
            "score_file_name = Path(tempfile.gettempdir()) / \"utest_score_t5_sum.hypo\"",
            "",
            "-        testargs = [\"evaluate_cnn.py\", \"t5-small\", str(tmp), str(output_file_name), str(tmp), str(score_file_name)]",
            "+        testargs = [",
            "+            \"evaluate_cnn.py\",",
            "+            \"patrickvonplaten/t5-tiny-random\",",
            "+            str(tmp),",
            "+            str(output_file_name),",
            "+            str(tmp),",
            "+            str(score_file_name),",
            "+        ]",
            "",
            "with patch.object(sys, \"argv\", testargs):",
            "run_generate()"
        ]
    },
    {
        "number": 5401,
        "comments": "",
        "commit_message": "Bugfix test_parameter (missing clone()) and test_pointer (test previously commented on torch_1)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_local_param_in_nn_module_linear():",
            "",
            "",
            "def test_remote_param_in_nn_module_linear(workers):",
            "-    model = nn.Linear(2, 1)",
            "+    model = nn.Linear(2, 1, bias=False)",
            "tensor = torch.tensor([1.0, -1.0])",
            "model_ptr = model.send(workers[\"bob\"])",
            "tensor_ptr = tensor.send(workers[\"bob\"])"
        ]
    },
    {
        "number": 5404,
        "comments": "",
        "commit_message": "[rllib] Fix PPO regression\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LocalMultiGPUOptimizer(PolicyOptimizer):",
            "# all of the device copies are created.",
            "with self.local_evaluator.tf_sess.graph.as_default():",
            "with self.local_evaluator.tf_sess.as_default():",
            "-                main_scope = tf.get_variable_scope()",
            "-                with tf.variable_scope(main_scope, reuse=tf.AUTO_REUSE):",
            "+                with tf.variable_scope(\"default\", reuse=tf.AUTO_REUSE):",
            "self.par_opt = LocalSyncParallelOptimizer(",
            "tf.train.AdamOptimizer(self.sgd_stepsize),",
            "self.devices,"
        ]
    },
    {
        "number": 5405,
        "comments": "",
        "commit_message": "More parser improvements (#934)\n\n* catch singleton words for labels, refactor span extractor internals\n\n* add ability to use embedded pos tags\n\n* use a text field embedder for pos tags\n\n* use pos tags in fixtures to test more complex case\n\n* update to use original Embedding\n\n* remove postag indexers\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SpanConstituencyParserTest(ModelTestCase):",
            "output_dict = self.model(**training_tensors)",
            "decode_output_dict = self.model.decode(output_dict)",
            "assert set(decode_output_dict.keys()) == {'spans', 'class_probabilities', 'trees',",
            "-                                                  'tokens', 'num_spans', 'loss'}",
            "+                                                  'tokens', 'pos_tags', 'num_spans', 'loss'}",
            "metrics = self.model.get_metrics(reset=True)",
            "metric_keys = set(metrics.keys())",
            "assert \"evalb_precision\" in metric_keys"
        ]
    },
    {
        "number": 5408,
        "comments": "",
        "commit_message": "fix conv1d in backends\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def conv1d(",
            "dilations: int = 1,",
            ") -> Union[tf.Tensor, tf.Variable]:",
            "if data_format == \"NCW\":",
            "-        x = tf.transpose(x, (0, 1, 2))",
            "+        x = tf.transpose(x, (0, 2, 1))",
            "res = tf.nn.conv1d(x, filters, strides, padding, \"NWC\", dilations)",
            "if data_format == \"NCW\":",
            "-        res = tf.transpose(res, (0, 1, 2))",
            "+        res = tf.transpose(res, (0, 2, 1))",
            "return res"
        ]
    },
    {
        "number": 5409,
        "comments": "",
        "commit_message": "Type promotion fixes (#2620)\n\n* casting fixes\n\n* lint fixes\n\n* changes\n\n* more changes\n\n* lint fixes\n\n* Fixed matrix_Rank\n\n* update matrix_rank return dtype\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def matrix_rank(",
            ") -> torch.Tensor:",
            "# ToDo: add support for default rtol value here, for the case where None is provided",
            "ret = torch.linalg.matrix_rank(x, rtol=rtol, out=out)",
            "-    ret = torch.tensor(ret, dtype=ivy.default_float_dtype(as_native=True))",
            "+    ret = torch.tensor(ret, dtype=ivy.default_int_dtype(as_native=True))",
            "return ret"
        ]
    },
    {
        "number": 5417,
        "comments": "",
        "commit_message": "[Enhance] Added better augmentation support for backpropagation (#826)\n\n* Avoid breaking gradients with a new tensor\n\n* Moved param init into generators\n\n* Removed deprecation warnings.\n\n* Fixed 3D ones\n\n* Fixed device / dtype error\n\n* Fixed mypy\n\n* Fixed sharpness bug\n\n* Added backward tests\n\n* Added more backward tests & Make motion blur angle/direction differentiable\n\n* Fixed RandomResizedCrop\n\n* Bug fixed & Added sharpness tests.\n\n* Updated 3D augmentations\n\n* Added RandomAffine tests\n\n* Added 3D augmentation backpropagation tsets\n\n* Updated 3D tests\n\n* Fixed broken tests\n\n* Fixed some bugs\n\n* Fixed typo\n\n* Fixed broken 3D tests\n\n* cuda fix\n\n* Skipped nearest mode under cuda devices.\n\n* Fixed linting\n",
        "label": "",
        "answer": "no",
        "change": [
            "def bbox_to_mask(boxes: torch.Tensor, width: int, height: int) -> torch.Tensor:",
            "m = m.index_fill(0, torch.arange(box[1, 1].item(), box[2, 1].item() + 1, dtype=torch.long), torch.tensor(1))",
            "m = m.unsqueeze(dim=0)",
            "m_out = (m == 1).all(dim=1) * (m == 1).all(dim=2).T",
            "+        m_out = m_out[1:-1, 1:-1]",
            "mask_out.append(m_out)",
            "",
            "return torch.stack(mask_out, dim=0).float()"
        ]
    },
    {
        "number": 5418,
        "comments": "",
        "commit_message": "fix softmax dim of Residual MoE in moe/layer.py (#2110)\n\nThanks a lot for finding this issue and fixed it :)\nCo-authored-by: Zhewei Yao <zheweiyao@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MoE(torch.nn.Module):",
            "if type(output_mlp) is tuple:",
            "output_mlp = output_mlp[0]  # Ignore the bias term for now",
            "coef = self.coefficient(hidden_states)",
            "-            coef = torch.nn.functional.softmax(coef, dim=1)",
            "+            coef = torch.nn.functional.softmax(coef, dim=-1)",
            "output = output * coef[..., 0:1] + output_mlp * coef[..., 1:]",
            "return output, self.deepspeed_moe.l_aux, self.deepspeed_moe.exp_counts"
        ]
    },
    {
        "number": 5420,
        "comments": "",
        "commit_message": "[see_memory_usage] fix deprecation (#1234)\n\nCo-authored-by: Olatunji Ruwase <olruwase@microsoft.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def see_memory_usage(message, force=False):",
            "logger.info(",
            "f\"MA {round(torch.cuda.memory_allocated() / (1024 * 1024 * 1024),2 )} GB \\",
            "Max_MA {round(torch.cuda.max_memory_allocated() / (1024 * 1024 * 1024),2)} GB \\",
            "-        CA {round(torch.cuda.memory_cached() / (1024 * 1024 * 1024),2)} GB \\",
            "-        Max_CA {round(torch.cuda.max_memory_cached() / (1024 * 1024 * 1024))} GB \")",
            "+        CA {round(torch_memory_reserved() / (1024 * 1024 * 1024),2)} GB \\",
            "+        Max_CA {round(torch_max_memory_reserved() / (1024 * 1024 * 1024))} GB \")",
            "",
            "vm_stats = psutil.virtual_memory()",
            "used_GB = round(((vm_stats.total - vm_stats.available) / (1024**3)), 2)"
        ]
    },
    {
        "number": 5422,
        "comments": "",
        "commit_message": "Fix support for dynamic schedules.\n\nPiperOrigin-RevId: 330394792\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DateTensor(tensor_wrapper.TensorWrapper):",
            "",
            "if period_type == constants.PeriodType.YEAR:",
            "y = self._years + period_tensor.quantity()",
            "-      m = tf.broadcast_to(self._months, y.shape)",
            "+      # Use tf.shape to handle the case of dynamically shaped `y`",
            "+      m = tf.broadcast_to(self._months, tf.shape(y))",
            "d = adjust_day(y, m, self._days)",
            "return from_year_month_day(y, m, d, validate=False)"
        ]
    },
    {
        "number": 5424,
        "comments": "",
        "commit_message": "Gh-109: Fix bug in single label method.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TextClassifier(nn.Module):",
            "return labels",
            "",
            "def _get_single_label(self, label_scores) -> List[Label]:",
            "-        conf, idx = torch.max(label_scores[0], 0)",
            "+        conf, idx = torch.max(label_scores, 0)",
            "label = self.label_dictionary.get_item_for_index(idx.item())",
            "",
            "return [Label(label, conf.item())]"
        ]
    },
    {
        "number": 5425,
        "comments": "",
        "commit_message": "Fix deprecated warning message and docstring (#2100)\n\n* Use deprecated Sphinx directive in docstrings\n\n* Fix deprecation message format\n\n* Use warnings.warn instead of logger.warning for deprecation\n\n* Remove trailing blank in deprecated warning message\n\n* Fix deprecated in dictionary_encode_column_\n",
        "label": "",
        "answer": "no",
        "change": [
            "def deprecated(help_message: Optional[str] = None):",
            "def wrapper(*args, **kwargs):",
            "func_hash = hash(deprecated_function)",
            "if func_hash not in _emitted_deprecation_warnings:",
            "-                logger.warning(warning_msg)",
            "+                warnings.warn(warning_msg, category=FutureWarning, stacklevel=2)",
            "_emitted_deprecation_warnings.add(func_hash)",
            "return deprecated_function(*args, **kwargs)"
        ]
    },
    {
        "number": 5426,
        "comments": "",
        "commit_message": "fix #806\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def generate_rpn_proposals(boxes, scores, img_shape,",
            "(-1, 4), name='nms_input_boxes')",
            "nms_indices = tf.image.non_max_suppression(",
            "topk_valid_boxes_y1x1y2x2,",
            "-        # TODO use exp to work around a bug in TF1.9: https://github.com/tensorflow/tensorflow/issues/19578",
            "-        tf.exp(topk_valid_scores),",
            "+        topk_valid_scores,",
            "max_output_size=post_nms_topk,",
            "iou_threshold=cfg.RPN.PROPOSAL_NMS_THRESH)"
        ]
    },
    {
        "number": 5427,
        "comments": "",
        "commit_message": "pylint -> flake8 (#3288)\n\n* pylint\n\n* update pylint\n\n* undo a lot of the raise / else\n\n* add bound on typed-ast\n\n* first mypy fixes\n\n* new flag\n\n* fix mypy errors\n\n* requirements.txt\n\n* pylint -> flake8\n\n* mypy 0.720 -> mypy 0.730\n\n* add back erroneously removed initial newline\n\n* remove .pylintrc\n\n* remove pylintrc from Dockerfile\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestExpectedRiskMinimization(AllenNlpTestCase):",
            "self.initial_state = SimpleState([0], [[0]], [torch.Tensor([0.0])])",
            "self.decoder_step = SimpleTransitionFunction()",
            "# Cost is the number of odd elements in the action history.",
            "-        self.supervision = lambda state: torch.Tensor([sum([x%2 != 0 for x in",
            "+        self.supervision = lambda state: torch.Tensor([sum([x % 2 != 0 for x in",
            "state.action_history[0]])])",
            "# High beam size ensures exhaustive search.",
            "self.trainer = ExpectedRiskMinimization(beam_size=100,"
        ]
    },
    {
        "number": 5428,
        "comments": "",
        "commit_message": "fix is tensor\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(object):",
            "\"It should be either Tensor or a list of Tensor.\"",
            ")",
            "for idx in range(len(check_argu)):",
            "-                        if not isinstance(check_argu[idx], [tf.Tensor, tf.SparseTensor, tf.Variable]) or not tf_ops.is_dense_tensor_like(",
            "+                        if not isinstance(check_argu[idx], (tf.Tensor, tf.SparseTensor, tf.Variable)) or not tf_ops.is_dense_tensor_like(",
            "check_argu[idx]):",
            "raise TypeError(",
            "\"The argument `%s` should be either Tensor or a list of Tensor \" % (check_order[co]) +"
        ]
    },
    {
        "number": 5434,
        "comments": "",
        "commit_message": "add compat.is_gpu_available(); fix test failure\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def run(fn, tf_args, cluster_meta, tensorboard, log_dir, queues, background):",
            "os.environ['TF_CONFIG'] = tf_config",
            "",
            "# reserve GPU(s) again, just before launching TF process (in case situation has changed)",
            "-    if tf.test.is_built_with_cuda():",
            "+    if compat.is_gpu_available():",
            "# compute my index relative to other nodes on the same host (for GPU allocation)",
            "my_addr = cluster_spec[job_name][task_index]",
            "my_host = my_addr.split(':')[0]"
        ]
    },
    {
        "number": 5438,
        "comments": "",
        "commit_message": "decay LR, little bugfix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def train_cifar():",
            "optimizer = optim.Adam(get_parameters(model), lr=3e-4)",
            "else:",
            "#optimizer = optim.SGD(get_parameters(model), lr=0.001)",
            "-    optimizer = optim.SGD(get_parameters(model), lr=0.003, momentum=0.85, nesterov=True)",
            "+    optimizer = optim.SGD(get_parameters(model), lr=Tensor([0.003]).realize(), momentum=0.85, nesterov=True)",
            "",
            "# 97 steps in 2 seconds = 20ms / step",
            "# step is 1163.42 GOPS = 56 TFLOPS!!!, 41% of max 136"
        ]
    },
    {
        "number": 5440,
        "comments": "",
        "commit_message": "fix warnings and failures\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "for epoch in range(opt.niter):",
            "netD.zero_grad()",
            "real_cpu = data[0].to(device)",
            "batch_size = real_cpu.size(0)",
            "-        label = torch.full((batch_size,), real_label, device=device)",
            "+        label = torch.full((batch_size,), real_label,",
            "+                           dtype=real_cpu.dtype, device=device)",
            "",
            "output = netD(real_cpu)",
            "errD_real = criterion(output, label)"
        ]
    },
    {
        "number": 5443,
        "comments": "",
        "commit_message": "Fix a few issues loading pretrained vit/bit npz weights w/ num_classes=0 __init__ arg. Missed a few other small classifier handling detail on Mlp, GhostNet, Levit. Should fix #713\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GhostNet(nn.Module):",
            "self.conv_head = nn.Conv2d(prev_chs, out_chs, 1, 1, 0, bias=True)",
            "self.act2 = nn.ReLU(inplace=True)",
            "self.flatten = nn.Flatten(1) if global_pool else nn.Identity()  # don't flatten if pooling disabled",
            "-        self.classifier = Linear(out_chs, num_classes)",
            "+        self.classifier = Linear(out_chs, num_classes) if num_classes > 0 else nn.Identity()",
            "",
            "def get_classifier(self):",
            "return self.classifier"
        ]
    },
    {
        "number": 5445,
        "comments": "",
        "commit_message": "Fix unused variable found (#1250)\n\n* Fix Unused variable found\n\n* more fixes\n\n* Update kornia/enhance/histogram.py\n",
        "label": "",
        "answer": "no",
        "change": [
            "def DepthWarperApp():",
            "# load the data",
            "root_dir = os.path.join(root_path, 'training')",
            "img_ref, depth_ref, cam_ref = load_data(root_dir, args.sequence_name, args.frame_ref_id)",
            "-    img_i, depth_i, cam_i = load_data(root_dir, args.sequence_name, args.frame_i_id)",
            "+    img_i, _, cam_i = load_data(root_dir, args.sequence_name, args.frame_i_id)",
            "",
            "# instantiate the homography warper from `kornia`",
            "warper = dgm.DepthWarper(cam_i)"
        ]
    },
    {
        "number": 5447,
        "comments": "",
        "commit_message": "Update to PyTorch 1.9.0 (#2887)\n\n* Update to PyTorch 1.9.0\n\n* Update torch.cholesky, torch.symeig\n\n* Fix torch.tensordot, torch.qr, funsor dependency\n\n* Fix torch import, AutoGuideList usage\n\n* Ignore bug in PyTorch jit\n\n* Ignore tracer warnings\n\n* Replace torch.solve with torch.linalg.solve\n\n* Xfail vectorized markov funsor tests\n\n* Update funsor version\n\n* Switch MNIST mirrors\n\n* Pin pillow version\n\n* Bump torchvision version\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_lower_cholesky_transform(batch_shape, dim):",
            "x = torch.randn(batch_shape + (dim, dim))",
            "y = t(x)",
            "assert y.shape == x.shape",
            "-    actual = y.matmul(y.transpose(-1, -2)).cholesky()",
            "+    actual = torch.linalg.cholesky(y.matmul(y.transpose(-1, -2)))",
            "assert_close(actual, y)",
            "x2 = t.inv(y)",
            "assert x2.shape == x.shape"
        ]
    },
    {
        "number": 5448,
        "comments": "",
        "commit_message": "Implement ProjectedNormal distribution and reparametrizer (#2736)\n\n* Implement ProjectedNormal distribution and reparametrizer\n\n* Attempt to fix import error\n\n* Attempt to implement .log_prob() for dims 2,3\n\n* Add test fixture; fix bugs\n\n* Fix some bugs in log_prob implementations\n\n* Get _log_prob_2 to pass gof tests\n\n* Get _log_prob_2 to pass gof tests\n\n* Get remaining gof tests to pass\n\n* Add test for constraints.sphere\n\n* Add test for ProjectedNormalReparam\n\n* Add helpful errors to autoguides\n\n* Support PyTorch 1.6\n\n* Fix docs\n\n* Clarify error message\n\n* Use non-centered reparameterizer\n",
        "label": "",
        "answer": "no",
        "change": [
            "class InitMessenger(Messenger):",
            "def _pyro_sample(self, msg):",
            "if msg[\"done\"] or msg[\"is_observed\"] or type(msg[\"fn\"]).__name__ == \"_Subsample\":",
            "return",
            "-        with torch.no_grad():",
            "+        with torch.no_grad(), helpful_support_errors(msg):",
            "value = self.init_fn(msg)",
            "if is_validation_enabled() and msg[\"value\"] is not None:",
            "if not isinstance(value, type(msg[\"value\"])):"
        ]
    },
    {
        "number": 5449,
        "comments": "",
        "commit_message": "bug fix: transfer lengths to cpu\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Encoder(torch.nn.Module):",
            "",
            "\"\"\"",
            "xs = x.unsqueeze(0)",
            "-        ilens = [x.size(0)]",
            "+        ilens = torch.tensor([x.size(0)])",
            "",
            "return self.forward(xs, ilens)[0][0]"
        ]
    },
    {
        "number": 5451,
        "comments": "",
        "commit_message": "small fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class NCSNpp(ModelMixin, ConfigMixin):",
            "for i_level in reversed(range(self.num_resolutions)):",
            "for i_block in range(num_res_blocks + 1):",
            "out_ch = nf * ch_mult[i_level]",
            "+                in_ch = in_ch + hs_c.pop()",
            "modules.append(",
            "ResnetBlock(",
            "-                        in_channels=in_ch + hs_c.pop(),",
            "+                        in_channels=in_ch,",
            "out_channels=out_ch,",
            "temb_channels=4 * nf,",
            "output_scale_factor=np.sqrt(2.0),"
        ]
    },
    {
        "number": 5452,
        "comments": "",
        "commit_message": "Add comment on reason for torch fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TorchHook:",
            "",
            "if torch.__version__ < \"1.0.2\":",
            "# Hard fix for PyTorch versions < 1.0.2",
            "+            # usage of torch.jit requires a torch version < torch 1.1, so we still need to support this torch version",
            "syft.torch.apply_fix16922(self.torch)",
            "",
            "torch_modules = syft.torch.torch_modules"
        ]
    },
    {
        "number": 5453,
        "comments": "",
        "commit_message": "Replace some fs operations by tf.gfile for other fs support. (fix #416)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def TryResumeTraining():",
            "if not logger.LOG_DIR:",
            "return JustCurrentSession()",
            "path = os.path.join(logger.LOG_DIR, 'checkpoint')",
            "-    if not os.path.isfile(path):",
            "+    if not tf.gfile.Exists(path):",
            "return JustCurrentSession()",
            "return SaverRestore(path)"
        ]
    },
    {
        "number": 5455,
        "comments": "",
        "commit_message": "CI: precommit - docformatter (#8584)\n\n* CI: precommit - docformatter\n* fix deprecated\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DeviceAssertCallback(Callback):",
            "@pytest.mark.parametrize([\"dst_device\"], [pytest.param(torch.device(\"cpu\")), pytest.param(torch.device(\"cuda\", 0))])",
            "@RunIf(min_gpus=1)",
            "def test_submodules_device_and_dtype(dst_device, dst_dtype):",
            "-    \"\"\"",
            "-    Test that the device and dtype property updates propagate through mixed nesting of regular",
            "-    nn.Modules and the special modules of type DeviceDtypeModuleMixin (e.g. Metric or LightningModule).",
            "-    \"\"\"",
            "+    \"\"\"Test that the device and dtype property updates propagate through mixed nesting of regular nn.Modules and",
            "+    the special modules of type DeviceDtypeModuleMixin (e.g. Metric or LightningModule).\"\"\"",
            "",
            "model = TopModule()",
            "assert model.device == torch.device(\"cpu\")"
        ]
    },
    {
        "number": 5456,
        "comments": "",
        "commit_message": "L2 nofm fix for tensorflow 1.0 (#810)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def l2_normalize(incoming, dim, epsilon=1e-12, name=\"l2_normalize\"):",
            "A `Tensor` with the same shape as `x`.",
            "\"\"\"",
            "with tf.name_scope(name) as name:",
            "-        x = tf.ops.convert_to_tensor(incoming, name=\"x\")",
            "+        x = tf.convert_to_tensor(incoming, name=\"x\")",
            "square_sum = tf.reduce_sum(tf.square(x), [dim], keep_dims=True)",
            "x_inv_norm = tf.rsqrt(tf.maximum(square_sum, epsilon))"
        ]
    },
    {
        "number": 5461,
        "comments": "",
        "commit_message": "Fix dropout by temporarily replacing with nn.Dropout\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class HighwayLSTM(nn.Module):",
            "self.lstm = nn.ModuleList()",
            "self.highway = nn.ModuleList()",
            "self.gate = nn.ModuleList()",
            "-        self.drop = Dropout(dropout, dims=[1] if batch_first else [0])",
            "+        self.drop = nn.Dropout(dropout)",
            "",
            "in_size = input_size",
            "for l in range(num_layers):"
        ]
    },
    {
        "number": 5462,
        "comments": "",
        "commit_message": "bugfix: div-->dim (#18135)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class PerceiverBasicDecoder(PerceiverAbstractDecoder):",
            "if self.concat_preprocessed_input:",
            "if inputs_without_pos is None:",
            "raise ValueError(\"Value is required for inputs_without_pos if concat_preprocessed_input is True\")",
            "-            pos_emb = torch.cat([inputs_without_pos, pos_emb], div=-1)",
            "+            pos_emb = torch.cat([inputs_without_pos, pos_emb], dim=-1)",
            "",
            "return pos_emb"
        ]
    },
    {
        "number": 5464,
        "comments": "",
        "commit_message": "keras: fix MetricAverageCallbackImpl\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MetricAverageCallbackImpl(object):",
            "self.device = device",
            "",
            "def _make_variable(self, metric, value):",
            "-        with tf.name_scope('MetricAverageCallback'):",
            "+        with tf.name_scope('MetricAverageCallback') as scope:",
            "var = tf.Variable(value, name=metric)",
            "self.backend.get_session().run(var.initializer)",
            "-            push_pull_op = bps.push_pull(var, device_dense=self.device)",
            "+            push_pull_op = bps.push_pull(var, scope, device_dense=self.device)",
            "return var, push_pull_op",
            "",
            "def _average_metrics_in_place(self, logs):"
        ]
    },
    {
        "number": 5465,
        "comments": "",
        "commit_message": "fix #771 and flake8\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Conv2dSubsampling(torch.nn.Module):",
            "torch.nn.ReLU()",
            ")",
            "self.out = torch.nn.Sequential(",
            "-            torch.nn.Linear(odim * ((idim - 1)// 4), odim),",
            "+            torch.nn.Linear(odim * ((idim - 1) // 4), odim),",
            "PositionalEncoding(odim, dropout_rate)",
            ")"
        ]
    },
    {
        "number": 5467,
        "comments": "",
        "commit_message": "[RLlib] Minor cleanup in preparation to tf2.x support. (#9130)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* Fixes.\n\n* Fixes and LINT.\n\n* WIP.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ParametricActionsModel(DistributionalQTFModel):",
            "action_logits = tf.reduce_sum(avail_actions * intent_vector, axis=2)",
            "",
            "# Mask out invalid actions (use tf.float32.min for stability)",
            "-        inf_mask = tf.maximum(tf.log(action_mask), tf.float32.min)",
            "+        inf_mask = tf.maximum(tf.math.log(action_mask), tf.float32.min)",
            "return action_logits + inf_mask, state",
            "",
            "def value_function(self):"
        ]
    },
    {
        "number": 5469,
        "comments": "",
        "commit_message": "Rename .reshape(s,n) -> .expand_by(s).independent(n) (#1016)\n\n* Start to rename .reshape()\n\n* Remove the .reshape() method entirely\n\n* Add .reshape() with informative error message\n\n* Fix test\n\n* Fix failing test\n\n* Fix failing test\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NormalNormalTests(TestCase):",
            "requires_grad=True))",
            "sig_q = torch.exp(log_sig_q)",
            "Normal = dist.Normal if reparameterized else fakes.NonreparameterizedNormal",
            "-            pyro.sample(\"loc_latent\", Normal(loc_q, sig_q).reshape(extra_event_dims=1))",
            "+            pyro.sample(\"loc_latent\", Normal(loc_q, sig_q).independent(1))",
            "",
            "adam = optim.Adam({\"lr\": .001})",
            "svi = SVI(model, guide, adam, loss=\"ELBO\", trace_graph=False)"
        ]
    },
    {
        "number": 5470,
        "comments": "",
        "commit_message": "Support various BERT relative position embeddings (2nd) (#8276)\n\n* Support BERT relative position embeddings\n\n* Fix typo in README.md\n\n* Address review comment\n\n* Fix failing tests\n\n* [tiny] Fix style_doc.py check by adding an empty line to configuration_bert.py\n\n* make fix copies\n\n* fix configs of electra and albert and fix longformer\n\n* remove copy statement from longformer\n\n* fix albert\n\n* fix electra\n\n* Add bert variants forward tests for various position embeddings\n\n* [tiny] Fix style for test_modeling_bert.py\n\n* improve docstring\n\n* [tiny] improve docstring and remove unnecessary dependency\n\n* [tiny] Remove unused import\n\n* re-add to ALBERT\n\n* make embeddings work for ALBERT\n\n* add test for albert\n\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LongformerEmbeddings(nn.Module):",
            "else:",
            "position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)",
            "",
            "-        # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.forward",
            "if input_ids is not None:",
            "input_shape = input_ids.size()",
            "else:"
        ]
    },
    {
        "number": 5473,
        "comments": "",
        "commit_message": "[Versatile Diffusion] Add versatile diffusion model (#1283)\n\n* up\n\n* convert dual unet\n\n* revert dual attn\n\n* adapt for vd-official\n\n* test the full pipeline\n\n* mixed inference\n\n* mixed inference for text2img\n\n* add image prompting\n\n* fix clip norm\n\n* split text2img and img2img\n\n* fix format\n\n* refactor text2img\n\n* mega pipeline\n\n* add optimus\n\n* refactor image var\n\n* wip text_unet\n\n* text unet end to end\n\n* update tests\n\n* reshape\n\n* fix image to text\n\n* add some first docs\n\n* dual guided pipeline\n\n* fix token ratio\n\n* propose change\n\n* dual transformer as a native module\n\n* DualTransformer(nn.Module)\n\n* DualTransformer(nn.Module)\n\n* correct unconditional image\n\n* save-load with mega pipeline\n\n* remove image to text\n\n* up\n\n* uP\n\n* fix\n\n* up\n\n* final fix\n\n* remove_unused_weights\n\n* test updates\n\n* save progress\n\n* uP\n\n* fix dual prompts\n\n* some fixes\n\n* finish\n\n* style\n\n* finish renaming\n\n* up\n\n* fix\n\n* fix\n\n* fix\n\n* finish\n\nCo-authored-by: anton-l <anton@huggingface.co>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class UNet2DModel(ModelMixin, ConfigMixin):",
            "num_groups_out = norm_num_groups if norm_num_groups is not None else min(block_out_channels[0] // 4, 32)",
            "self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[0], num_groups=num_groups_out, eps=norm_eps)",
            "self.conv_act = nn.SiLU()",
            "-        self.conv_out = nn.Conv2d(block_out_channels[0], out_channels, 3, padding=1)",
            "+        self.conv_out = nn.Conv2d(block_out_channels[0], out_channels, kernel_size=3, padding=1)",
            "",
            "def forward(",
            "self,"
        ]
    },
    {
        "number": 5474,
        "comments": "",
        "commit_message": "adopt torch.testing.assert_close (#1031)\n\n* adopt torch.testing.assert_close\n\n* use torch.testing.assert_close\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add TODO\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestSolveCast:",
            "error = torch.dist(B, A.matmul(X))",
            "",
            "tol_val: float = 1e-1 if dtype == torch.float16 else 1e-4",
            "-        assert_allclose(error, torch.zeros_like(error), atol=tol_val, rtol=tol_val)",
            "+        assert_close(error, torch.zeros_like(error), atol=tol_val, rtol=tol_val)"
        ]
    },
    {
        "number": 5476,
        "comments": "",
        "commit_message": "Change V2.0 coors (#2380)\n\n* Refactor (all): change coordinate system\n\n* Fix (mask_head): fix cat -1 bug in mask_paste\n\n* Fix (unittest)\n: modify unittest and pass CI\n\n* reformat to pass CI\n\n* Fix round coordinates bugs\n\n* clean file\n\n* Fix (test): use cpu version of aligned roi_align in tests\n\n* Refactor (mask): clean np.stack\n\n* Refactor (head): reformat code and fix missing -1\n\n* Reformat: reformat and add doc strings\n\n* Refactor (mask_head): more clea docstring\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def ga_loc_target(gt_bboxes_list,",
            "all_ignore_map.append(ignore_map)",
            "for img_id in range(img_per_gpu):",
            "gt_bboxes = gt_bboxes_list[img_id]",
            "-        scale = torch.sqrt((gt_bboxes[:, 2] - gt_bboxes[:, 0] + 1) *",
            "-                           (gt_bboxes[:, 3] - gt_bboxes[:, 1] + 1))",
            "+        scale = torch.sqrt((gt_bboxes[:, 2] - gt_bboxes[:, 0]) *",
            "+                           (gt_bboxes[:, 3] - gt_bboxes[:, 1]))",
            "min_anchor_size = scale.new_full(",
            "(1, ), float(anchor_scale * anchor_strides[0]))",
            "# assign gt bboxes to different feature levels w.r.t. their scales"
        ]
    },
    {
        "number": 5478,
        "comments": "",
        "commit_message": "Implement Lightning module for GraphGym (#4511)\n\n* add LitModule\n\n* format\n\n* add pl dep\n\n* add pl to min deps\n\n* add type hint\n\n* apply suggestions\n\n* apply suggestions\n\n* Update setup.py\n\n* fix tests\n\n* graphgym_requires\n\n* graphgym install\n\nCo-authored-by: Matthias Fey <matthias.fey@tu-dortmund.de>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def create_model(to_device=True, dim_in=None, dim_out=None):",
            "if 'classification' in cfg.dataset.task_type and dim_out == 2:",
            "dim_out = 1",
            "",
            "-    model = network_dict[cfg.model.type](dim_in=dim_in, dim_out=dim_out)",
            "+    model = GraphGymModule(dim_in, dim_out, cfg)",
            "if to_device:",
            "model.to(torch.device(cfg.device))",
            "return model"
        ]
    },
    {
        "number": 5479,
        "comments": "",
        "commit_message": "fix multi-gpu bug for cer/wer report\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def nsc_beam_search(decoder, h, recog_args, rnnlm=None):",
            "for i, hyp in enumerate(hyps):",
            "i_topk = (",
            "torch.cat((beam_topk[0][i], beam_logp[i, 0:1])),",
            "-                    torch.cat((beam_topk[1][i], torch.LongTensor([0]))),",
            "+                    torch.cat((beam_topk[1][i], blank_tensor)),",
            ")",
            "",
            "for logp, k in zip(*i_topk):"
        ]
    },
    {
        "number": 5484,
        "comments": "",
        "commit_message": "[AIR - Datasets] Hide tensor extension from UDFs. (#27019)\n\nWe previously added automatic tensor extension casting on Datasets transformation outputs to allow the user to not have to worry about tensor column casting; however, this current state creates several issues:\n\n1. Not all tensors are supported, which means that we\u2019ll need to have an opaque object dtype (i.e. ndarray of ndarray pointers) fallback for the Pandas-only case. Known unsupported tensor use cases:\na. Heterogeneous-shaped (i.e. ragged) tensors\nb. Struct arrays\n2. UDFs will expect a NumPy column and won\u2019t know what to do with our TensorArray type. E.g., torchvision transforms don\u2019t respect the array protocol (which they should), and instead only support Torch tensors and NumPy ndarrays; passing a TensorArray column or a TensorArrayElement (a single item in the TensorArray column) fails.\nImplicit casting with object dtype fallback on UDF outputs can make the input type to downstream UDFs nondeterministic, where the user won\u2019t know if they\u2019ll get a TensorArray column or an object dtype column.\n3. The tensor extension cast fallback warning spams the logs.\n\nThis PR:\n\n1. Adds automatic casting of tensor extension columns to NumPy ndarray columns for Datasets UDF inputs, meaning the UDFs will never have to see tensor extensions and that the UDF input column types will be consistent and deterministic; this fixes both (2) and (3).\n2. No longer implicitly falls back to an opaque object dtype when TensorArray casting fails (e.g. for ragged tensors), and instead raises an error; this fixes (4) but removes our support for (1).\n3. Adds a global enable_tensor_extension_casting config flag, which is True by default, that controls whether we perform this automatic casting. Turning off the implicit casting provides a path for (1), where the tensor extension can be avoided if working with ragged tensors in Pandas land. Turning off this flag also allows the user to explicitly control their tensor extension casting, if they want to work with it in their UDFs in order to reap the benefits of less data copies, more efficient slicing, stronger column typing, etc.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def convert_ndarray_batch_to_tf_tensor_batch(",
            "f\"should be given, instead got: {dtypes}\"",
            ")",
            "dtypes = next(iter(dtypes.values()))",
            "-        batch = tf.convert_to_tensor(ndarrays, dtype=dtypes)",
            "+        batch = convert_ndarray_to_tf_tensor(ndarrays, dtypes)",
            "else:",
            "# Multi-tensor case.",
            "batch = {",
            "-            col_name: tf.convert_to_tensor(",
            "+            col_name: convert_ndarray_to_tf_tensor(",
            "col_ndarray,",
            "dtype=dtypes[col_name] if isinstance(dtypes, dict) else dtypes,",
            ")"
        ]
    },
    {
        "number": 5492,
        "comments": "",
        "commit_message": "[Past CI] \ud83d\udd25 Leave Past CI failures in the past \ud83d\udd25  (#20861)\n\n* torch.jit._state\n\n* Fix past CI\n\n* Fix for perceiver\n\n* Fix REALM\n\n* Fix for Bloom\n\n* Fix for SwinMode\n\n* Fix for TrajectoryTransformerModel\n\n* Fix for test_wav2vec2_with_lm\n\n* make style\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RealmScorer(RealmPreTrainedModel):",
            "# [batch_size, num_candidates, retriever_proj_size]",
            "candidate_score = candidate_score.view(-1, self.config.num_candidates, self.config.retriever_proj_size)",
            "# [batch_size, num_candidates]",
            "-        relevance_score = torch.einsum(\"BD,BND->BN\", query_score, candidate_score)",
            "+        relevance_score = torch.einsum(\"bd,bnd->bn\", query_score, candidate_score)",
            "",
            "if not return_dict:",
            "return relevance_score, query_score, candidate_score"
        ]
    },
    {
        "number": 5496,
        "comments": "",
        "commit_message": "fix linter errors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class HomographyWarper(nn.Module):",
            "raise TypeError(\"Patch and homography must be on the same device. \\",
            "Got patch.device: {} dst_H_src.device: {}.\"",
            ".format(patch.device, dst_homo_src.device))",
            "-        return torch.nn.functional.grid_sample(patch,",
            "-            self.warp_grid(dst_homo_src), mode='bilinear',",
            "+        return torch.nn.functional.grid_sample(",
            "+            patch, self.warp_grid(dst_homo_src), mode='bilinear',",
            "padding_mode=padding_mode)",
            "",
            "# functional api"
        ]
    },
    {
        "number": 5507,
        "comments": "",
        "commit_message": "[RLlib] Tf2x preparation; part 2 (upgrading `try_import_tf()`). (#9136)\n\n* WIP.\n\n* Fixes.\n\n* LINT.\n\n* WIP.\n\n* WIP.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* Fixes.\n\n* WIP.\n\n* Fixes.\n\n* Test\n\n* Fix.\n\n* Fixes and LINT.\n\n* Fixes and LINT.\n\n* LINT.\n",
        "label": "",
        "answer": "no",
        "change": [
            "class KLCoeffMixin:",
            "# KL Coefficient",
            "self.kl_coeff_val = config[\"kl_coeff\"]",
            "self.kl_target = config[\"kl_target\"]",
            "-        self.kl_coeff = tf.get_variable(",
            "+        self.kl_coeff = tf1.get_variable(",
            "initializer=tf.constant_initializer(self.kl_coeff_val),",
            "name=\"kl_coeff\",",
            "shape=(),"
        ]
    },
    {
        "number": 5511,
        "comments": "",
        "commit_message": "fix use of deprecated TF functions.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            "log_qc = tf.reduce_sum(prior_prob * zc, 1, name='logQc')",
            "Elog_qc_given_x = tf.reduce_mean(log_qc_given_x, name='ElogQc_x')",
            "Hc = tf.reduce_mean(-log_qc, name='Hc')",
            "-        MIloss = tf.mul(Hc + Elog_qc_given_x, -1.0, name='neg_MI')",
            "+        MIloss = tf.multiply(Hc + Elog_qc_given_x, -1.0, name='neg_MI')",
            "",
            "self.g_loss, self.d_loss = build_GAN_losses(vecpos, vecneg)",
            "self.g_loss = tf.add(self.g_loss, MIloss, name='total_g_loss')"
        ]
    },
    {
        "number": 5512,
        "comments": "",
        "commit_message": "small fix for `to_numpy`.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def array_equal(",
            "",
            "def to_numpy(x: Union[tf.Tensor, tf.Variable], copy: bool = True) -> np.ndarray:",
            "# TensorFlow fails to convert bfloat16 tensor when it has 0 dimensions",
            "-    if get_num_dims(x) == 0 and ivy.as_native_dtype(x.dtype) is tf.bfloat16:",
            "+    if (",
            "+        ivy.is_array(x)",
            "+        and get_num_dims(x) == 0",
            "+        and ivy.as_native_dtype(x.dtype) is tf.bfloat16",
            "+    ):",
            "x = tf.expand_dims(x, 0)",
            "if copy:",
            "return np.squeeze(np.array(tf.convert_to_tensor(x)), 0)"
        ]
    },
    {
        "number": 5513,
        "comments": "",
        "commit_message": "Fix for `tensordot()` linalg.py (#7350)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def tensordot(",
            "# type casting to float32 which is acceptable for tf.tensordot",
            "x1, x2 = tf.cast(x1, tf.float32), tf.cast(x2, tf.float32)",
            "",
            "-    ret = tf.cast(tf.tensordot(x1, x2, axes), dtype)",
            "+    ret = tf.cast(tf.tensordot(x1, x2, axes=axes), dtype)",
            "return ret"
        ]
    },
    {
        "number": 5514,
        "comments": "",
        "commit_message": "Fixes warnings and add compatibility stub in torch solve (#1235)\n\n* add a compatibiliy stub for torch solve\n\n* change missing _torch_solve_cast calls\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* bump pytorch version to 1.7.1\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def conv_quad_interp3d(",
            "",
            "nms_mask: torch.Tensor = kornia.feature.nms3d(input, (3, 3, 3), True)",
            "x_solved: torch.Tensor = torch.zeros_like(b)",
            "-    x_solved_masked, _ = torch.solve(b[nms_mask.view(-1)], Hes[nms_mask.view(-1)])",
            "+    x_solved_masked, _ = _torch_solve_cast(b[nms_mask.view(-1)], Hes[nms_mask.view(-1)])",
            "x_solved.masked_scatter_(nms_mask.view(-1, 1, 1), x_solved_masked)",
            "dx: torch.Tensor = -x_solved"
        ]
    },
    {
        "number": 5516,
        "comments": "",
        "commit_message": "FReLU bias=False bug fix (#1666)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MemoryEfficientMish(nn.Module):",
            "class FReLU(nn.Module):",
            "def __init__(self, c1, k=3):  # ch_in, kernel",
            "super().__init__()",
            "-        self.conv = nn.Conv2d(c1, c1, k, 1, 1, groups=c1)",
            "+        self.conv = nn.Conv2d(c1, c1, k, 1, 1, groups=c1, bias=False)",
            "self.bn = nn.BatchNorm2d(c1)",
            "",
            "def forward(self, x):"
        ]
    },
    {
        "number": 5523,
        "comments": "",
        "commit_message": "fix colocation problems\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class MapGradient(GradientProcessor):",
            "for grad, var in grads:",
            "if re.match(self.regex, var.op.name):",
            "matched = True",
            "-                with tf.device(grad.device):",
            "-                    grad = self.func(grad, var)",
            "+                grad = self.func(grad, var)",
            "if grad is not None:",
            "ret.append((grad, var))",
            "else:"
        ]
    },
    {
        "number": 5525,
        "comments": "",
        "commit_message": "layer tests and model tests bug fixed\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Layer_Embed_Test(CustomTestCase):",
            "except AttributeError as e:",
            "print(e)",
            "self.assertEqual(embed_tensor.get_shape().as_list(), [batch_size, embedding_size])",
            "-        model = tl.models.Model(inputs=inputs, outputs=embed_tensor, name=\"word2vec_model\")",
            "+        model = tl.models.Model(inputs=inputs, outputs=embed_tensor)",
            "",
            "",
            "if __name__ == '__main__':"
        ]
    },
    {
        "number": 5526,
        "comments": "",
        "commit_message": "Full GraphGym + PyG integration (#3076)\n\n* Full GraphGym + PyG integration\n\n* fix torch.allclose\n",
        "label": "",
        "answer": "no",
        "change": [
            "sns.set_context('poster')",
            "",
            "",
            "def view_emb(emb, dir):",
            "+    '''",
            "+    Visualize a embedding matrix.",
            "+",
            "+    Args:",
            "+        emb (torch.tensor): Embedding matrix with shape (N, D). D is the",
            "+        feature dimension.",
            "+        dir (str): Output directory for the embedding figure.",
            "+",
            "+    '''",
            "if emb.shape[1] > 2:",
            "pca = PCA(n_components=2)",
            "emb = pca.fit_transform(emb)"
        ]
    },
    {
        "number": 5527,
        "comments": "",
        "commit_message": "Add a new intro tutorial (#2991)\n\n* Add a new language introduction tutorial derived from the Bayesian regression tutorials\n\n* Address coments\n\n* Address comments 2\n\n* Address more comments, add intro and conclusion\n\n* Fix some conclusion text and links\n\n* smoke test\n\n* Address comments, add table of contents\n\n* Fix seaborn warnings\n\n* tweak text and fix ylim\n\n* nits\n\n* rename\n\n* remove all references to old intro tutorials\n\n* update index\n\n* Add notes about deprecation to old intros\n\n* fix code links\n\n* Fix more sphinx errors\n\n* fix yet another sphinx error...\n\n* add a couple sentences about elbo estimators\n\n* fix weird commit??\n\n* Speed up inference\n\n* Remove obsolete comment\n\n* regenerate plots\n\n* address comments\n\nCo-authored-by: Fritz Obermeyer <fritz.obermeyer@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "\"# Modules in Pyro\\n\",",
            "\"\\n\",",
            "\"\\n\",",
            "-    \"This tutorial introduces [PyroModule](http://docs.pyro.ai/en/stable/nn.html#pyro.nn.module.PyroModule), Pyro's Bayesian extension of PyTorch's [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) class. Before starting you should understand the basics of Pyro [models](http://pyro.ai/examples/intro_part_i.html) and [inference](http://pyro.ai/examples/intro_part_ii.html), understand the two primitives [pyro.sample()](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.sample) and [pyro.param()](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.param), and understand the basics of Pyro's [effect handlers](http://pyro.ai/examples/effect_handlers.html) (e.g. by browsing [minipyro.py](https://github.com/pyro-ppl/pyro/blob/dev/pyro/contrib/minipyro.py)).\\n\",",
            "+    \"This tutorial introduces [PyroModule](http://docs.pyro.ai/en/stable/nn.html#pyro.nn.module.PyroModule), Pyro's Bayesian extension of PyTorch's [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) class. Before starting you should understand the basics of Pyro [models and inference](http://pyro.ai/examples/intro_long.html), understand the two primitives [pyro.sample()](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.sample) and [pyro.param()](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.param), and understand the basics of Pyro's [effect handlers](http://pyro.ai/examples/effect_handlers.html) (e.g. by browsing [minipyro.py](https://github.com/pyro-ppl/pyro/blob/dev/pyro/contrib/minipyro.py)).\\n\",",
            "\"\\n\",",
            "\"#### Summary:\\n\",",
            "\"\\n\","
        ]
    },
    {
        "number": 5530,
        "comments": "",
        "commit_message": "[Deepspeed] adapt multiple models, add zero_to_fp32 tests (#12477)\n\n* zero_to_fp32 tests\n\n* args change\n\n* remove unnecessary work\n\n* use transformers.trainer_utils.get_last_checkpoint\n\n* document the new features\n\n* cleanup\n\n* wip\n\n* fix fsmt\n\n* add bert\n\n* cleanup\n\n* add xlm-roberta\n\n* electra works\n\n* cleanup\n\n* sync\n\n* split off the model zoo tests\n\n* cleanup\n\n* cleanup\n\n* cleanup\n\n* cleanup\n\n* reformat\n\n* cleanup\n\n* casing\n\n* deepspeed>=0.4.3\n\n* adjust distilbert\n\n* Update docs/source/main_classes/deepspeed.rst\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* style\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TrainingArguments:",
            "device = torch.device(\"cuda\", self.local_rank)",
            "self._n_gpu = 1",
            "elif self.deepspeed:",
            "-            # deepspeed performs its own DDP internally, and requires the program to be started with:",
            "-            # deepspeed  ./program.py",
            "-            # rather than:",
            "-            # python -m torch.distributed.launch --nproc_per_node=2 ./program.py",
            "+            # deepspeed inits torch.distributed internally",
            "from .deepspeed import is_deepspeed_available",
            "",
            "if not is_deepspeed_available():"
        ]
    },
    {
        "number": 5532,
        "comments": "",
        "commit_message": "Add fp16 support (#963)\n\n* Update imgwarp.py\n\n* update fix\n\n* move _torch_inverse_cast under utils\n\n* fix some tests in pyramid\n\n* fix issues\n\n* implement other autocasting functions\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def _compute_luts(tiles_x_im: torch.Tensor, num_bins: int = 256, clip: float = 4",
            "histos: torch.Tensor = torch.empty((tiles.shape[0], num_bins), device=tiles.device)",
            "if not diff:",
            "for i in range(tiles.shape[0]):",
            "-            histos[i] = torch.histc(tiles[i], bins=num_bins, min=0, max=1)",
            "+            histos[i] = _torch_histc_cast(tiles[i], bins=num_bins, min=0, max=1)",
            "else:",
            "bins: torch.Tensor = torch.linspace(0, 1, num_bins, device=tiles.device)",
            "histos = histogram(tiles, bins, torch.tensor(0.001)).squeeze()"
        ]
    },
    {
        "number": 5533,
        "comments": "",
        "commit_message": "[Train] Torch Prepare utilities (#20254)\n\n* update\n\n* formatting\n\n* fix failures\n\n* fix session tests\n\n* address comments\n\n* add to api docs\n\n* package refactor\n\n* wip\n\n* wip\n\n* wip\n\n* finish\n\n* finish\n\n* fix\n\n* comment\n\n* fix\n\n* install horovod for docs\n\n* address comment\n\n* Update python/ray/train/session.py\n\nCo-authored-by: matthewdeng <matthew.j.deng@gmail.com>\n\n* Update python/ray/train/torch.py\n\nCo-authored-by: matthewdeng <matthew.j.deng@gmail.com>\n\n* address comments\n\n* try fix docs\n\n* fix doc build failure\n\n* fix\n\n* fix\n\n* fix\n\n* try fix doc highlighting\n\n* fix docs\n\nCo-authored-by: matthewdeng <matthew.j.deng@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def train_func():",
            "",
            "# __torch_distributed_begin__",
            "",
            "-from torch.nn.parallel import DistributedDataParallel",
            "+from ray import train",
            "",
            "def train_func_distributed():",
            "num_epochs = 3",
            "model = NeuralNetwork()",
            "-    model = DistributedDataParallel(model)",
            "+    model = train.torch.prepare_model(model)",
            "loss_fn = nn.MSELoss()",
            "optimizer = optim.SGD(model.parameters(), lr=0.1)"
        ]
    },
    {
        "number": 5535,
        "comments": "",
        "commit_message": "Fixing slow pipeline tests (#14260)\n\n* Fiixng slow pipeline tests\n\n* Remove the image-segmentaiton override.\n\n* Fixing clamping only in training.\n\n* Wav2vec2.\n\n* Remove last mention of `no_grad`.\n\n* Fixing copies.\n\n* Rename.\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class DetrEncoderLayer(nn.Module):",
            "hidden_states = residual + hidden_states",
            "hidden_states = self.final_layer_norm(hidden_states)",
            "",
            "-        if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():",
            "-            clamp_value = torch.finfo(hidden_states.dtype).max - 1000",
            "-            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)",
            "+        if self.training:",
            "+            if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():",
            "+                clamp_value = torch.finfo(hidden_states.dtype).max - 1000",
            "+                hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)",
            "",
            "outputs = (hidden_states,)"
        ]
    },
    {
        "number": 5536,
        "comments": "",
        "commit_message": "Fix some criterion code after recent API change (fixes #1866) (#1874)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/1874\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D20565473\n\nPulled By: myleott\n\nfbshipit-source-id: 25edef9f41f28a41f1c573dcc3da48b674b678e4\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class WSCTask(FairseqTask):",
            "prefix + leading_space + txt + trailing_space + suffix,",
            "append_eos=True,",
            ")",
            "-        mask = torch.zeros_like(toks, dtype=torch.uint8)",
            "+        mask = torch.zeros_like(toks, dtype=torch.bool)",
            "mask_start = len(self.binarize(prefix))",
            "mask_size = len(self.binarize(leading_space + txt))",
            "mask[mask_start:mask_start + mask_size] = 1"
        ]
    },
    {
        "number": 5537,
        "comments": "",
        "commit_message": "Fix grid index to prvent deprication warning\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class YOLOLayer(nn.Module):",
            "",
            "@staticmethod",
            "def _make_grid(nx=20, ny=20):",
            "-        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])",
            "+        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)], indexing='ij')",
            "return torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)).float()"
        ]
    },
    {
        "number": 5541,
        "comments": "",
        "commit_message": "embed sized setting in default lm bug fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class RNNLM(nn.Module):",
            "self.embed = nn.Embedding(n_vocab, n_embed)",
            "if typ == \"lstm\":",
            "self.rnn = nn.ModuleList(",
            "-                [nn.LSTMCell(n_embed, n_units)] + [nn.LSTMCell(n_units, n_units) for _ in range(n_layers - 1)]",
            "-                )",
            "+                [nn.LSTMCell(n_embed, n_units)] + [nn.LSTMCell(n_units, n_units) for _ in range(n_layers - 1)])",
            "else:",
            "self.rnn = nn.ModuleList(",
            "-                [nn.GRUCell(n_embed, n_units)] + [nn.GRUCell(n_units, n_units) for _ in range(n_layers - 1)]",
            "-                )",
            "+                [nn.GRUCell(n_embed, n_units)] + [nn.GRUCell(n_units, n_units) for _ in range(n_layers - 1)])",
            "",
            "self.dropout = nn.ModuleList(",
            "[nn.Dropout(dropout_rate) for _ in range(n_layers + 1)])"
        ]
    },
    {
        "number": 5542,
        "comments": "",
        "commit_message": "potential fix for embeddings no loading on AMD cards\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class StableDiffusionModelHijack:",
            "if len(emb.shape) == 1:",
            "emb = emb.unsqueeze(0)",
            "",
            "-            self.word_embeddings[name] = emb.detach()",
            "+            self.word_embeddings[name] = emb.detach().to(device)",
            "self.word_embeddings_checksums[name] = f'{const_hash(emb.reshape(-1)*100)&0xffff:04x}'",
            "",
            "ids = tokenizer([name], add_special_tokens=False)['input_ids'][0]"
        ]
    },
    {
        "number": 5545,
        "comments": "",
        "commit_message": "[RLlib] DDPG PyTorch actor-model was missing sigmoid layer (#8188)\n\nFix DDPG PyTorch (missing sigmoid layer (to squash action outputs) after deterministic action outputs).\n",
        "label": "",
        "answer": "no",
        "change": [
            "def ddpg_actor_critic_loss(policy, model, _, train_batch):",
            "def make_ddpg_optimizers(policy, config):",
            "# Create separate optimizers for actor & critic losses.",
            "policy._actor_optimizer = torch.optim.Adam(",
            "-        params=policy.model.policy_variables(), lr=config[\"actor_lr\"])",
            "+        params=policy.model.policy_variables(),",
            "+        lr=config[\"actor_lr\"],",
            "+        eps=1e-7)  # to match tf.keras.optimizers.Adam's epsilon default",
            "policy._critic_optimizer = torch.optim.Adam(",
            "-        params=policy.model.q_variables(), lr=config[\"critic_lr\"])",
            "+        params=policy.model.q_variables(), lr=config[\"critic_lr\"],",
            "+        eps=1e-7)  # to match tf.keras.optimizers.Adam's epsilon default",
            "return policy._actor_optimizer, policy._critic_optimizer"
        ]
    },
    {
        "number": 5546,
        "comments": "",
        "commit_message": "Saver/summaries/distributed handling updated to using MonitoredSession, hooks, etc; custom save problems fixed; added saver/summary/distributed_spec config entries; added batched_observe config entry to address performance problem; modified episode/timestep counter handling in runner; various other and related fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LayerBasedNetwork(Network):",
            "return network_variables + layer_variables",
            "",
            "def get_summaries(self):",
            "-        return super(LayerBasedNetwork, self).get_summaries() + \\",
            "-            [summary for layer in self.layers for summary in layer.get_summaries()]",
            "+        network_summaries = super(LayerBasedNetwork, self).get_summaries()",
            "+        layer_summaries = [summary for layer in self.layers for summary in layer.get_summaries()]",
            "+",
            "+        return network_summaries + layer_summaries",
            "",
            "",
            "class LayeredNetwork(LayerBasedNetwork):"
        ]
    },
    {
        "number": 5548,
        "comments": "",
        "commit_message": "Drop support for PyTorch 1.10 (#16492)\n\n* Drop support for PyTorch 1.10\n\n* CHANGELOG\n\n* READMEs\n\n* mypy\n\n* ls\n\n* New poplar version\n\n* Fixed tests\n\n* links\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* skip azure badges\n\n* Table\n\n* Matching dockerfiles\n\n* Drop unnecessary channels and packages\n\n* Push nightly\n\n* Undo unrelated changes\n\n* Revert \"Push nightly\"\n\nThis reverts commit 9618f737c4dc65331fef4bb11fe46a61513d220a.\n\n---------\n\nCo-authored-by: Jirka <jirka.borovec@seznam.cz>\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_sharded_tensor_state_dict(single_process_pg):",
            "",
            "m_0 = BoringModelWithShardedTensor(spec)",
            "m_0.sharded_tensor.local_shards()[0].tensor.fill_(1)",
            "-    name_st = \".sharded_tensor\" if _TORCH_GREATER_EQUAL_1_11 and not _TORCH_GREATER_EQUAL_1_13 else \"sharded_tensor\"",
            "+    name_st = \".sharded_tensor\" if not _TORCH_GREATER_EQUAL_1_13 else \"sharded_tensor\"",
            "assert name_st in m_0.state_dict(), 'Expect \"sharded_tensor\" to appear in the state dict'",
            "",
            "m_1 = BoringModelWithShardedTensor(spec)"
        ]
    },
    {
        "number": 5549,
        "comments": "",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class CascadeRCNNHead(object):",
            "labels_per_box = tf.gather(self.gt_labels, best_iou_ind)",
            "fg_mask = max_iou_per_box >= iou_threshold",
            "fg_inds_wrt_gt = tf.boolean_mask(best_iou_ind, fg_mask)",
            "-                labels_per_box = tf.stop_gradient(labels_per_box * tf.to_int64(fg_mask))",
            "+                labels_per_box = tf.stop_gradient(labels_per_box * tf.cast(fg_mask, tf.int64))",
            "return BoxProposals(boxes, labels_per_box, fg_inds_wrt_gt)",
            "else:",
            "return BoxProposals(boxes)"
        ]
    },
    {
        "number": 5552,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class MaskedLayerNorm(torch.nn.Module):",
            "self.size = size",
            "self.eps = eps",
            "",
            "-    def forward(self, tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:",
            "+    def forward(self, tensor: torch.Tensor, mask: torch.BoolTensor) -> torch.Tensor:",
            "",
            "-        broadcast_mask = mask.unsqueeze(-1).float()",
            "+        broadcast_mask = mask.unsqueeze(-1)",
            "num_elements = broadcast_mask.sum() * self.size",
            "mean = (tensor * broadcast_mask).sum() / num_elements",
            "masked_centered = (tensor - mean) * broadcast_mask"
        ]
    },
    {
        "number": 5554,
        "comments": "",
        "commit_message": "Make an `ObjectStore` something a `Worker` has (instead of is) (#3484)\n\n* [WIP] Make an `ObjectStore` something a `Worker` has (instead of is)\n\n* Update tests to look for objects in `worker.object_store._objects`\n\n* Fix references to `_tensors` and `rm_obj()`\n\n* Remove `owner` attribute from `ObjectStorage`\n\n* Revert \"Remove `owner` attribute from `ObjectStorage`\"\n\nThis reverts commit 7d687fbaebb402b889c6ed90a837e514d305e258.\n\n* Fix loose ends with setting `owner` attribute\n\n* Fix `ObjectStorage` tests\n\n* Fix `test_share_get`\n\n* Fix the Udacity backwards compatibility tests\n\n* Clean up `AbstractObject.tag()`\n\n* Fix `get_obj` references in `FederatedClient`\n\n* Replace direct access to `_objects` with `get_obj()`\n\n* Fix tag registration\n\n* Remove stray comment\n\n* Use `clear_objects()` method instead of setting to `{}`\n\n* Add a missing `NodeClient` import to `AbstractGrid`\n\n* Rename `ObjectStorage` to `ObjectStore`\n\n* Tone down triple exclamation points in error messages\n\n* Remove unnecessary guard clause and exception from `Object.tag()`\n\n* Clean up and expand the Udacity course compatibility comments\n\n* Add `get_obj` to `FederatedClient`\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_set_obj_takes_ownership(workers):",
            "",
            "me.set_obj(x)",
            "",
            "-    objs = me._objects",
            "+    objs = me.object_store._objects",
            "",
            "assert objs[x.id] == x",
            "assert objs[x.id].owner == workers[\"me\"]"
        ]
    },
    {
        "number": 5556,
        "comments": "",
        "commit_message": "Improve snt.Embed performance in distributed training.\n\nAdds a fix to avoid excess computation on parameter servers.\n\nPiperOrigin-RevId: 180674688\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Embed(base.AbstractModule):",
            "regularizer=self._regularizers.get(self.EMBEDDINGS, None),",
            "trainable=self._trainable)",
            "",
            "-    # On the backwards pass, we want to convert the gradient from",
            "-    # indexed-slices to a regular tensor before sending it back to the",
            "-    # parameter server. This avoids excess computation on the parameter server.",
            "-",
            "-    embeddings = util.convert_gradient_to_tensor(self._embeddings)",
            "-",
            "# Lookup embeddings",
            "-    return tf.nn.embedding_lookup(embeddings, ids, name=\"embedding_lookup\")",
            "+    return tf.nn.embedding_lookup(",
            "+        self._embeddings, ids, name=\"embedding_lookup\")",
            "",
            "@property",
            "def vocab_size(self):"
        ]
    },
    {
        "number": 5557,
        "comments": "",
        "commit_message": "[contrib] Fix the reference implementation of multihead_attn (#1423)\n\n* follow the current signature\n\nSigned-off-by: Masaki Kozuki <mkozuki@nvidia.com>\n\n* call .backward on outputs\n\nSigned-off-by: Masaki Kozuki <mkozuki@nvidia.com>\n\n* update the other caller of _softmax_backward_data\n\nSigned-off-by: Masaki Kozuki <mkozuki@nvidia.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class EncdecAttnFunc(torch.autograd.Function):",
            "dropout_grads = torch._masked_scale(matmul2_dgrad1, dropout_mask, 1.0 / (1.0 - dropout_prob_t[0]))",
            "",
            "# Softmax Grad (not a publically documented op)",
            "-        softmax_grads = torch._softmax_backward_data(dropout_grads, softmax_results, -1, softmax_results)",
            "+        softmax_grads = torch._softmax_backward_data(dropout_grads, softmax_results, -1, softmax_results.dtype)",
            "",
            "# Matmul1 - DGRAD1",
            "# Input1: (data grads)  [seqs*heads, seql_q, seql_k]"
        ]
    },
    {
        "number": 5558,
        "comments": "",
        "commit_message": "bug fix loss\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class L1LossMasked(nn.Module):",
            "# target_flat: (batch * max_len, dim)",
            "target_flat = target.view(-1, target.shape[-1])",
            "# losses_flat: (batch * max_len, dim)",
            "-        losses_flat = functional.l1_loss(input, target, size_average=False,",
            "+        losses_flat = functional.l1_loss(input, target_flat, size_average=False,",
            "reduce=False)",
            "# losses: (batch, max_len, dim)",
            "losses = losses_flat.view(*target.size())",
            "+",
            "# mask: (batch, max_len, 1)",
            "mask = _sequence_mask(sequence_length=length,",
            "max_len=target.size(1)).unsqueeze(2)"
        ]
    },
    {
        "number": 5564,
        "comments": "",
        "commit_message": "Fix build and tests of zmq op (#362)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def compile():",
            "# https://github.com/uber/horovod/blob/10835d25eccf4b198a23a0795edddf0896f6563d/horovod/tensorflow/mpi_ops.py#L30-L40",
            "def get_ext_suffix():",
            "\"\"\"Determine library extension for various versions of Python.\"\"\"",
            "+    return '.so'    # TODO",
            "ext_suffix = sysconfig.get_config_var('EXT_SUFFIX')",
            "if ext_suffix:",
            "return ext_suffix"
        ]
    },
    {
        "number": 5566,
        "comments": "",
        "commit_message": "fix more cuda bugs in RL tutorial\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "for i_episode in range(num_episodes):",
            "break",
            "",
            "print('Complete')",
            "+env.render(close=True)",
            "env.close()",
            "plt.ioff()",
            "plt.show()"
        ]
    },
    {
        "number": 5568,
        "comments": "",
        "commit_message": "Expose VQ VAE modules to as snt.nets.VectorQuantizer{EMA}\n\nAdded goldens.\n\nAlso fixed checkpoint_test & saved_model_test to support nested output.\n\nPiperOrigin-RevId: 256566131\nChange-Id: Ic26fe1785741ebac9b4a6a790c3585a64fd2f882\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class SavedModelTest(test_utils.TestCase, parameterized.TestCase):",
            "if golden.deterministic:",
            "# The output from both the saved and restored model should be close.",
            "y1 = saved_model.inference(x)",
            "-      self.assertAllEqual(y1, y2)",
            "+      tf.nest.map_structure(self.assertAllEqual, y1, y2)",
            "",
            "for a, b in zip(v1, v2):",
            "self.assertEqual(a.name, b.name)"
        ]
    },
    {
        "number": 5570,
        "comments": "",
        "commit_message": "upgrade detection / some v2 compat fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def maskrcnn_upXconv_head(feature, num_category, num_convs, norm=None):",
            "with argscope([Conv2D, Conv2DTranspose], data_format='channels_first',",
            "kernel_initializer=tf.variance_scaling_initializer(",
            "scale=2.0, mode='fan_out',",
            "-                      distribution='untruncated_normal' if get_tf_version_tuple() >= (1, 12) else 'normal')):",
            "+                      distribution='untruncated_normal')):",
            "# c2's MSRAFill is fan_out",
            "for k in range(num_convs):",
            "l = Conv2D('fcn{}'.format(k), l, cfg.MRCNN.HEAD_DIM, 3, activation=tf.nn.relu)"
        ]
    },
    {
        "number": 5572,
        "comments": "",
        "commit_message": "[Doctests] Move doctests to new GPU & Fix bugs (#15969)\n\n* test\n\n* up\n\n* up\n\n* Empty test commit\n\n* up\n\n* update tests\n\n* up\n\n* fix some vision models\n\n* correct\n\n* correct docs\n\n* Trigger notification\n\n* finalize\n\n* check\n\n* correct quicktour\n\n* Apply suggestions from code review\n\n* improve doctests\n\n* Trigger Build\n\n* next try\n\n* next try\n\n* and again\n\n* Output current clone information\n\n* Output current clone information\n\n* Correct path\n\n* add tf round again\n\n* revert to daily job\n\nCo-authored-by: Lysandre <lysandre.debut@reseau.eseo.fr>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DeiTForImageClassification(DeiTPreTrainedModel):",
            ">>> # model predicts one of the 1000 ImageNet classes",
            ">>> predicted_class_idx = logits.argmax(-1).item()",
            ">>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])",
            "+        Predicted class: maillot",
            "```\"\"\"",
            "return_dict = return_dict if return_dict is not None else self.config.use_return_dict"
        ]
    },
    {
        "number": 5574,
        "comments": "",
        "commit_message": "fix shift_rgb stack dimension (#1930)\n\n* fix shift_rgb stack dimension\n\n* update RandomRGBShift tests\n",
        "label": "",
        "answer": "no",
        "change": [
            "def shift_rgb(image: torch.Tensor, r_shift: torch.Tensor, g_shift: torch.Tensor,",
            "",
            "shifts = [r_shift, g_shift, b_shift]",
            "",
            "-    shifted = (image + torch.stack(shifts).view(-1, 3, 1, 1).to(image)).clamp_(min=0, max=1)",
            "+    shifted = (image + torch.stack(shifts, dim=1).view(-1, 3, 1, 1).to(image)).clamp_(min=0, max=1)",
            "",
            "return shifted"
        ]
    },
    {
        "number": 5579,
        "comments": "",
        "commit_message": "fix AdaLAM crash (#1881)\n\n* fix AdaLAM crash\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class AdalamFilter:",
            "\"Please either provide orientations or set 'orientation_difference_threshold' to None to disable orientations filtering\"  # noqa: E501",
            ")",
            "k1, k2, d1, d2, o1, o2, s1, s2 = self.__to_torch(k1, k2, d1, d2, o1, o2, s1, s2)",
            "+        if len(d2) <= 1:",
            "+            return _no_match(d1)",
            "distmat = dist_matrix(d1, d2, is_normalized=False)",
            "dd12, nn12 = torch.topk(distmat, k=2, dim=1, largest=False)  # (n1, 2)"
        ]
    },
    {
        "number": 5582,
        "comments": "",
        "commit_message": "[rllib] format with yapf (#2427)\n\n* initial yapf\n\n* manual fix yapf bugs\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(object):",
            "self.outputs, self.last_layer = self._build_layers(",
            "inputs, num_outputs, options)",
            "if options.get(\"free_log_std\", False):",
            "-            log_std = tf.get_variable(name=\"log_std\", shape=[num_outputs],",
            "-                                      initializer=tf.zeros_initializer)",
            "+            log_std = tf.get_variable(",
            "+                name=\"log_std\",",
            "+                shape=[num_outputs],",
            "+                initializer=tf.zeros_initializer)",
            "self.outputs = tf.concat(",
            "[self.outputs, 0.0 * self.outputs + log_std], 1)"
        ]
    },
    {
        "number": 5583,
        "comments": "",
        "commit_message": "fix device in longformer onnx path (#20419)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LongformerSelfAttention(nn.Module):",
            "hidden_states.size(2),",
            "]",
            "",
            "-        overlapping_chunks = torch.empty(chunk_size)",
            "+        overlapping_chunks = torch.empty(chunk_size, device=hidden_states.device)",
            "for chunk in range(chunk_size[1]):",
            "overlapping_chunks[:, chunk, :, :] = hidden_states[",
            ":, chunk * window_overlap : chunk * window_overlap + 2 * window_overlap, :"
        ]
    },
    {
        "number": 5590,
        "comments": "",
        "commit_message": "ConvBERT fix torch <> tf weights conversion (#10314)\n\n* convbert conversion test\n\n* fin\n\n* fin\n\n* fin\n\n* clean up tf<->pt conversion\n\n* remove from_pt\n\nCo-authored-by: patrickvonplaten <patrick.v.platen@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class GroupedLinearLayer(tf.keras.layers.Layer):",
            "def call(self, hidden_states):",
            "batch_size = shape_list(hidden_states)[0]",
            "x = tf.transpose(tf.reshape(hidden_states, [-1, self.num_groups, self.group_in_dim]), [1, 0, 2])",
            "-        x = tf.matmul(x, self.kernel)",
            "+        x = tf.matmul(x, tf.transpose(self.kernel, [2, 1, 0]))",
            "x = tf.transpose(x, [1, 0, 2])",
            "x = tf.reshape(x, [batch_size, -1, self.output_size])",
            "x = tf.nn.bias_add(value=x, bias=self.bias)"
        ]
    },
    {
        "number": 5593,
        "comments": "",
        "commit_message": "`vecdot()` fix for torch backend.\n",
        "label": "",
        "answer": "no",
        "change": [
            "def vecdot(",
            "out: Optional[torch.Tensor] = None,",
            ") -> torch.Tensor:",
            "dtype = torch.promote_types(x1.dtype, x2.dtype)",
            "-    x1, x2 = x1.type(torch.float32), x2.type(torch.float32)",
            "-    return torch.tensordot(x1, x2, dims=([axis], [axis]), out=out).type(dtype)",
            "+    x1, x2 = x1.to(torch.float32), x2.to(torch.float32)",
            "+    return torch.tensordot(x1, x2, dims=([axis], [axis]), out=out)",
            "",
            "",
            "vecdot.unsupported_dtypes = ("
        ]
    },
    {
        "number": 5595,
        "comments": "",
        "commit_message": "[Fix] Missing device initializations (#672)\n\n* Fix device initialization in several variables\n\n* Fix device initialization in several variables\n\n* Skip CUDA tests for #666\n\n* Lint\n\n* Fix device initialization in several variables\n\n* Skip CUDA tests for #666\n\n* Lint\n\n* Remove half-precision tests (#649) and add a jit test\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestRandomCutMix:",
            "",
            "assert_allclose(out_image, expected, rtol=1e-4, atol=1e-4)",
            "assert (out_label[:, :, 0] == label).all()",
            "-        assert (out_label[:, :, 1] == torch.tensor([[1, 0], [1, 0], [1, 0], [1, 0], [0, 1]])).all()",
            "+        assert (out_label[:, :, 1] == torch.tensor([[1, 0], [1, 0], [1, 0], [1, 0], [0, 1]], device=device)).all()",
            "assert_allclose(out_label[:, :, 2], torch.tensor([[0., 0.], [0., 0.], [0., 0.0833], [0., 0.], [0.5, 0.3333]],",
            "device=device, dtype=dtype), rtol=1e-4, atol=1e-4)"
        ]
    },
    {
        "number": 5597,
        "comments": "",
        "commit_message": "[TFGPT2] - Fix flaky past_key_values test (#9460)\n\n* fix tf flakey\n\n* remove test files\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFGPT2ModelTester:",
            "",
            "# create hypothetical next token and extent to next_input_ids",
            "next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)",
            "-        next_token_types = ids_tensor((self.batch_size, 3), self.type_vocab_size)",
            "next_attn_mask = ids_tensor((self.batch_size, 3), 2)",
            "+        next_token_types = ids_tensor((self.batch_size, 3), self.type_vocab_size)",
            "",
            "# append to next input_ids and token_type_ids",
            "next_input_ids = tf.concat([input_ids, next_tokens], axis=-1)",
            "-        next_token_type_ids = tf.concat([token_type_ids, next_token_types], axis=-1)",
            "next_attention_mask = tf.concat([input_mask, next_attn_mask], axis=-1)",
            "+        next_token_type_ids = tf.concat([token_type_ids, next_token_types], axis=-1)",
            "",
            "output_from_no_past = model(",
            "next_input_ids, token_type_ids=next_token_type_ids, attention_mask=next_attention_mask"
        ]
    },
    {
        "number": 5598,
        "comments": "",
        "commit_message": "minor fix\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Model:",
            "return ppgs, preds_ppg, logits_ppg, pred_spec, pred_mel",
            "",
            "def loss_net2(self):",
            "-        loss_spec = tf.reduce_mean(tf.abs(self.pred_spec - self.y_spec))",
            "-        loss_mel = tf.reduce_mean(tf.abs(self.pred_mel - self.y_mel))",
            "+        loss_spec = tf.reduce_mean(tf.squared_difference(self.pred_spec, self.y_spec))",
            "+        loss_mel = tf.reduce_mean(tf.squared_difference(self.pred_mel, self.y_mel))",
            "loss = loss_spec + loss_mel",
            "return loss"
        ]
    },
    {
        "number": 5600,
        "comments": "",
        "commit_message": "version initiatlizer fix\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NormalizedAdvantageFunctions(ValueFunction):",
            "'outputs_target')",
            "self.create_training_operations()",
            "self.saver = tf.train.Saver()",
            "-        self.session.run(tf.initialize_all_variables())",
            "+        self.session.run(tf.tf.global_variables_initializer())",
            "",
            "def get_action(self, state, episode=1, total_states=0):",
            "\"\"\""
        ]
    },
    {
        "number": 5602,
        "comments": "",
        "commit_message": "Release 1.8.6rc0 (#660)\n\n* Release 1.8.6rc0\n\n* Update CHANGELOG.md\n\n* Typo Fix\n\n* Changelog Updated\n\n* tl_logging more readable\n\n* Typo Fixed\n\n* https enforced for badges\n\n* RTD badges pointing to Latest Commit\n\n* Decorators API Refactored\n\n* extra_requires `all`, `all_cpu` and `all_gpu` added\n\n* Error fix\n\n* YAPF Formating Correction\n\n* Test for private method decorator added\n\n* Test Logging Verbosity Fixed to Debug when runned individually\n\n* YAPF corrections applied\n\n* Changelog Added\n\n* Changelog updated\n\n* PR number changed\n\n* Changelog updated\n\n* Update .travis.yml\n\n* Update CONTRIBUTING.md\n\n* Update CONTRIBUTING.md\n\n* Update CHANGELOG.md\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Layer_Core_Test(unittest.TestCase):",
            "",
            "if __name__ == '__main__':",
            "",
            "-    # tl.logging.set_verbosity(tl.logging.INFO)",
            "+    tf.logging.set_verbosity(tf.logging.DEBUG)",
            "tl.logging.set_verbosity(tl.logging.DEBUG)",
            "",
            "-    unittest.main()",
            "+    unittest.main()",
            "\\ No newline at end of file"
        ]
    },
    {
        "number": 5603,
        "comments": "",
        "commit_message": "Add Pre-commit bot fixes (#3722)\n\n* flake8\n\n* flake8\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* fix\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_polar():",
            "assert len(data) == 3",
            "assert data.pos.tolist() == pos.tolist()",
            "assert data.edge_index.tolist() == edge_index.tolist()",
            "-    assert torch.allclose(",
            "-        data.edge_attr, torch.Tensor([[1, 0], [1, PI]]), atol=1e-04)",
            "+    assert torch.allclose(data.edge_attr, torch.Tensor([[1, 0], [1, PI]]),",
            "+                          atol=1e-04)",
            "",
            "data = Data(edge_index=edge_index, pos=pos, edge_attr=edge_attr)",
            "data = Polar(norm=True)(data)",
            "assert len(data) == 3",
            "assert data.pos.tolist() == pos.tolist()",
            "assert data.edge_index.tolist() == edge_index.tolist()",
            "-    assert torch.allclose(",
            "-        data.edge_attr, torch.Tensor([[1, 1, 0], [1, 1, 0.5]]), atol=1e-04)",
            "+    assert torch.allclose(data.edge_attr,",
            "+                          torch.Tensor([[1, 1, 0], [1, 1, 0.5]]), atol=1e-04)"
        ]
    },
    {
        "number": 5614,
        "comments": "",
        "commit_message": "Fix pylint errors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "from __future__ import print_function",
            "import tensorflow as tf",
            "",
            "def templatemethod(name_):",
            "+  \"\"\"This decorator wraps a method with `tf.make_template`. For example,",
            "+",
            "+  @templatemethod",
            "+  def my_method():",
            "+    # Create variables",
            "+  \"\"\"",
            "def template_decorator(func):",
            "+    \"\"\"Inner decorator function\"\"\"",
            "def func_wrapper(*args, **kwargs):",
            "-      templated_func =  tf.make_template(name_, func)",
            "+      \"\"\"Inner wrapper function\"\"\"",
            "+      templated_func = tf.make_template(name_, func)",
            "return templated_func(*args, **kwargs)",
            "return func_wrapper",
            "return template_decorator"
        ]
    },
    {
        "number": 5615,
        "comments": "",
        "commit_message": "lint fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def pow(",
            "return ret",
            "",
            "",
            "-def round(",
            "-    x: torch.Tensor,",
            "-    *,",
            "-    out: Optional[torch.Tensor] = None",
            "-) -> torch.Tensor:",
            "+def round(x: torch.Tensor, *, out: Optional[torch.Tensor] = None) -> torch.Tensor:",
            "if \"int\" in str(x.dtype):",
            "if ivy.exists(out):",
            "return ivy.inplace_update(out, x)"
        ]
    },
    {
        "number": 5616,
        "comments": "",
        "commit_message": "fixed wrong fn_tree of test experimental gcd\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_nansum(",
            "",
            "# gcd",
            "@handle_test(",
            "-    fn_tree=\"functional.experimental.nansum\",",
            "+    fn_tree=\"functional.experimental.gcd\",",
            "dtype_and_x=helpers.dtype_and_values(",
            "available_dtypes=helpers.get_dtypes(\"integer\"),",
            "num_arrays=2,"
        ]
    },
    {
        "number": 5618,
        "comments": "",
        "commit_message": "Misc fixes (#2448)\n\nSummary: Pull Request resolved: https://github.com/pytorch/fairseq/pull/2448\n\nReviewed By: ngoyal2707\n\nDifferential Revision: D23011193\n\nPulled By: myleott\n\nfbshipit-source-id: 1a29481707108e4465aca78ec1581fb79f05efba\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class Trainer(object):",
            "",
            "def is_consistent(tensor):",
            "max_abs_diff = torch.max(torch.abs(tensor - tensor[0]))",
            "-                return (max_abs_diff / (tensor[0] + 1e-6) < 1e-6).all()",
            "+                return (",
            "+                    not torch.isfinite(tensor).any()",
            "+                    or (max_abs_diff / (tensor[0] + 1e-6) < 1e-6).all()",
            "+                )",
            "",
            "if not is_consistent(self._grad_norm_buf):",
            "pretty_detail = \"\\n\".join("
        ]
    },
    {
        "number": 5622,
        "comments": "",
        "commit_message": "Add Buildkite CI support (#984)\n\n* Buildkite support\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Incorporate recent changes\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Bugfix env variable\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Improved update-alternatives\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Move cache cutover to a point right before framework installation\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Bump build timeout to 20 minutes\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Increase # of push retries to 5\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Fix cpu-gpu error tests\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Bump timeout further\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Downgrade NCCL to 2.3.7\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Add more exclusions to TensorFlow mixed mode tests\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Pin tf-nightly to a version before the breaking change\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Update cache condition\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Replace torchvision_nightly with torchvision\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Add Pillow for torchvision\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n* Fix test_horovod_allreduce_multi_gpu\n\nSigned-off-by: Alex Sergeev <alsrgv@users.noreply.github.com>\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TfKerasTests(tf.test.TestCase):",
            "initial_epoch=1)",
            "",
            "def test_sparse_as_dense(self):",
            "-        hvd.init()",
            "-",
            "-        with self.test_session() as sess:",
            "+        with self.test_session(config=self.config) as sess:",
            "K.set_session(sess)",
            "",
            "opt = keras.optimizers.RMSprop(lr=0.0001)"
        ]
    },
    {
        "number": 5626,
        "comments": "",
        "commit_message": "Various test fixes (#15068)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def sync_ddp(result: Tensor, group: Optional[Any] = None, reduce_op: Optional[Un",
            "",
            "# WA for HPU. HPU doesn't support Long types, forcefully set it to float",
            "if module_available(\"habana_frameworks.torch.utils.library_loader\"):",
            "-        from habana_frameworks.torch.utils.library_loader import is_habana_available",
            "+        from habana_frameworks.torch.utils.library_loader import is_habana_avaialble",
            "",
            "if (",
            "-            is_habana_available()",
            "+            is_habana_avaialble()",
            "and os.environ.get(\"HCCL_DISTRIBUTED_BACKEND\") == \"1\"",
            "and result.type() in (\"torch.LongTensor\", \"torch.hpu.LongTensor\")",
            "):"
        ]
    },
    {
        "number": 5627,
        "comments": "",
        "commit_message": "change dist to torch.distributed to fix bug in assert. (#638)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def mpi_discovery(distributed_port=TORCH_DISTRIBUTED_DEFAULT_PORT, verbose=True)",
            "os.environ['MASTER_PORT']))",
            "",
            "if torch.distributed.is_initialized():",
            "-        assert dist.get_rank() == rank, \"MPI rank {} does not match torch rank {}\".format(rank, dist.get_rank())",
            "-        assert dist.get_world_size() == world_size, \"MPI world size {} does not match torch world size {}\".format(",
            "-            world_size, dist.get_world_size())",
            "+        assert torch.distributed.get_rank() == rank, \"MPI rank {} does not match torch rank {}\".format(rank, dist.get_rank())",
            "+        assert torch.distributed.get_world_size() == world_size, \"MPI world size {} does not match torch world size {}\".format(",
            "+            world_size, torch.distributed.get_world_size())",
            "",
            "",
            "def in_aml():"
        ]
    },
    {
        "number": 5628,
        "comments": "",
        "commit_message": "fix global_step scope problem\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def get_global_step_var():",
            "scope = tf.get_variable_scope()",
            "assert scope.name == '', \\",
            "\"Creating global_step_var under a variable scope would cause problems!\"",
            "-        var = tf.Variable(",
            "-            0, trainable=False, name=GLOBAL_STEP_OP_NAME)",
            "+        var = tf.get_variable(GLOBAL_STEP_OP_NAME, shape=[],",
            "+                initializer=tf.constant_initializer(), trainable=False)",
            "return var",
            "",
            "def get_global_step():"
        ]
    },
    {
        "number": 5632,
        "comments": "",
        "commit_message": "Change all masks to type torch.bool (#3890)\n\n* get_text_field_mask\n\n* masked_softmax\n\n* masked_log_softmax\n\n* masked_max/mean\n\n* get_lengths_from_binary_sequence_mask\n\n* get_final_encoder_states\n\n* replace_masked_values\n\n* batched_span_select\n\n* sequence_cross_entropy_with_logits\n\n* add_sentence_boundary_token_ids/remove_sentence_boundaries\n\n* scalar mix\n\n* More changes\n\n* Fix tests\n\n* More changes, mostly in tests\n\n* Test fix\n\n* black\n\n* More changes\n\n* More cahnges\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestStackedSelfAttentionDecoderNet(AllenNlpTestCase):",
            "batch_size = 5",
            "time_steps = 10",
            "encoded_state = torch.rand(batch_size, time_steps, decoder_inout_dim)",
            "-        source_mask = torch.ones(batch_size, time_steps)",
            "+        source_mask = torch.ones(batch_size, time_steps).bool()",
            "source_mask[0, 7:] = 0",
            "source_mask[1, 5:] = 0",
            "prev_timesteps = 3"
        ]
    },
    {
        "number": 5637,
        "comments": "",
        "commit_message": "Add fp16 support (#963)\n\n* Update imgwarp.py\n\n* update fix\n\n* move _torch_inverse_cast under utils\n\n* fix some tests in pyramid\n\n* fix issues\n\n* implement other autocasting functions\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def normalize_homography3d(dst_pix_trans_src_pix: torch.Tensor,",
            "# compute the transformation pixel/norm for src/dst",
            "src_norm_trans_src_pix: torch.Tensor = normal_transform_pixel3d(",
            "src_d, src_h, src_w).to(dst_pix_trans_src_pix)",
            "-    src_pix_trans_src_norm = torch.inverse(src_norm_trans_src_pix)",
            "+",
            "+    src_pix_trans_src_norm = _torch_inverse_cast(src_norm_trans_src_pix)",
            "dst_norm_trans_dst_pix: torch.Tensor = normal_transform_pixel3d(",
            "dst_d, dst_h, dst_w).to(dst_pix_trans_src_pix)",
            "# compute chain transformations"
        ]
    },
    {
        "number": 5643,
        "comments": "",
        "commit_message": "fix conv1d flatten bug; update reccurent layers implementation from 0(deprecated) to 1\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class NetGraph(object):",
            "self.remove_skip_layers(_KERAS_SKIP_LAYERS) # done 1 pass",
            "self.insert_1d_permute_layers()",
            "self.insert_permute_for_spatial_bn()",
            "-            self.insert_permute_for_embed_flatten()",
            "self.defuse_activation()",
            "self.remove_internal_input_layers()"
        ]
    },
    {
        "number": 5647,
        "comments": "",
        "commit_message": "[Datasets] [AIR] Fixes label tensor squeezing in to_tf() (#25553)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Dataset(Generic[T]):",
            "):",
            "if label_column:",
            "targets = convert_pandas_to_tf_tensor(batch[[label_column]])",
            "-                    assert targets.ndim == 2",
            "-                    targets = tf.squeeze(targets, axis=1)",
            "+                    if targets.ndim == 2 and targets.shape[1] == 1:",
            "+                        targets = tf.squeeze(targets, axis=1)",
            "batch.pop(label_column)",
            "",
            "features = None"
        ]
    },
    {
        "number": 5648,
        "comments": "",
        "commit_message": "Data Input Refactoring\n\n- Data input pipeline can now deal properly with missing target data.\nFixes #101\n- Moved some of the featurizer functionality into the data reader and\nmade featurizer implicit part of the model.\n- Misc: Replace concat with concat_v2\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class BidirectionalRNNEncoder(GraphModule):",
            "**kwargs)",
            "",
            "# Concatenate outputs and states of the forward and backward RNNs",
            "-    outputs_concat = tf.concat(2, outputs)",
            "+    outputs_concat = tf.concat_v2(outputs, 2)",
            "",
            "return RNNEncoderOutput(outputs=outputs_concat, final_state=states)"
        ]
    },
    {
        "number": 5650,
        "comments": "",
        "commit_message": "fix actor update problem\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Critic(object):",
            "",
            "with tf.variable_scope('Critic'):",
            "# Input (s, a), output q",
            "-            self.a = a",
            "+            self.a = tf.stop_gradient(a)    # stop critic update flows to actor",
            "self.q = self._build_net(S, self.a, 'eval_net', trainable=True)",
            "",
            "# Input (s_, a_), output q_ for q_target"
        ]
    },
    {
        "number": 5652,
        "comments": "",
        "commit_message": "[Fix] MotionBlur bug fix and doctest update (#782)\n\n* Fixed #779\n\n* Added tests for _extract_device_dtype\n\n* Fixed broken tests\n\n* Added tests against functional\n\n* Fixed doctests\n\n* bug fix\n",
        "label": "",
        "answer": "no",
        "change": [
            "def motion_blur3d(",
            ">>> torch.allclose(out_1[0], out_1[1])",
            "True",
            ">>> # perform element-wise motion blur accross the batch",
            "-        >>> out_1 = motion_blur3d(input, 5, torch.tensor([[0., 90., 90.], [90., 180., 0.]]), torch.tensor([1, -1]))",
            "+        >>> out_1 = motion_blur3d(input, 5, torch.tensor([[0., 90., 90.], [90., 180., 0.]]), torch.tensor([1., -1.]))",
            ">>> torch.allclose(out_1[0], out_1[1])",
            "False",
            "\"\"\""
        ]
    },
    {
        "number": 5654,
        "comments": "",
        "commit_message": "Fix softmax_rgb_blend() when mesh is outside zfar\n\nSummary:\nThis fixes two small issues with blending.py:softmax_rgb_blend():\n  1) zfar and znear attributes are propagated from the camera settings instead of just using default settings of znear=1.0 and zfar=100.0\n  2) A check is added to prevent arithmetic overflow in softmax_rgb_blend()\n\nThis is a fix in response to https://github.com/facebookresearch/pytorch3d/issues/334\nwhere meshes rendererd using a SoftPhongShader with faces_per_pixel=1 appear black.  This only occurs when the scale of the mesh is large (vertex values > 100, where 100 is the default value of zfar).  This fix allows the caller to increase the value of cameras.zfar to match the scale of her/his mesh.\n\nReviewed By: nikhilaravi\n\nDifferential Revision: D23517541\n\nfbshipit-source-id: ab8631ce9e5f2149f140b67b13eff857771b8807\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def softmax_rgb_blend(",
            "z_inv = (zfar - fragments.zbuf) / (zfar - znear) * mask",
            "# pyre-fixme[16]: `Tuple` has no attribute `values`.",
            "# pyre-fixme[6]: Expected `Tensor` for 1st param but got `float`.",
            "-    z_inv_max = torch.max(z_inv, dim=-1).values[..., None]",
            "+    z_inv_max = torch.max(z_inv, dim=-1).values[..., None].clamp(min=eps)",
            "# pyre-fixme[6]: Expected `Tensor` for 1st param but got `float`.",
            "weights_num = prob_map * torch.exp((z_inv - z_inv_max) / blend_params.gamma)"
        ]
    },
    {
        "number": 5655,
        "comments": "",
        "commit_message": "Replicate optimizer hyperparams; Fix tagger scorer; Add word dropout\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def harmonic_mean(a, weights=None):",
            "return sum(weights) / sum(w/x for x, w in zip(a, weights))",
            "",
            "# torch utils",
            "-def get_optimizer(name, parameters, lr, betas=(0.9, 0.999)):",
            "+def get_optimizer(name, parameters, lr, betas=(0.9, 0.999), eps=1e-8):",
            "if name == 'sgd':",
            "return torch.optim.SGD(parameters, lr=lr)",
            "elif name == 'adagrad':",
            "return torch.optim.Adagrad(parameters, lr=lr)",
            "elif name == 'adam':",
            "-        return torch.optim.Adam(parameters, lr=lr, betas=betas) # use default lr",
            "+        return torch.optim.Adam(parameters, lr=lr, betas=betas, eps=eps)",
            "elif name == 'adamax':",
            "return torch.optim.Adamax(parameters) # use default lr",
            "else:"
        ]
    },
    {
        "number": 5662,
        "comments": "",
        "commit_message": "fix baseline dim\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class FlatVarHelper(object):",
            "self.session = session",
            "shapes = map(get_shape, variables)",
            "total_size = sum(np.prod(shape) for shape in shapes)",
            "-        self.theta = theta = tf.placeholder(tf.float32, [total_size])",
            "+        self.theta = tf.placeholder(tf.float32, [total_size])",
            "start = 0",
            "assigns = []",
            "",
            "for (shape, variable) in zip(shapes, variables):",
            "size = np.prod(shape)",
            "-            assigns.append(tf.assign(variable, tf.reshape(theta[start:start + size], shape)))",
            "+            assigns.append(tf.assign(variable, tf.reshape(self.theta[start:start + size], shape)))",
            "start += size",
            "",
            "self.set_op = tf.group(*assigns)"
        ]
    },
    {
        "number": 5663,
        "comments": "",
        "commit_message": "Fix typo in contrib FusedLamb. (#1172)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class FusedLAMB(torch.optim.Optimizer):",
            "continue",
            "if p.dtype == torch.float32:",
            "g_all_32.append(p.grad.data)",
            "-                elif p.dytpe == torch.float16:",
            "+                elif p.dtype == torch.float16:",
            "g_all_16.append(p.grad.data)",
            "else:",
            "raise RuntimeError('FusedLAMB only support fp16 and fp32.')"
        ]
    },
    {
        "number": 5664,
        "comments": "",
        "commit_message": "Fixed problem with ivy.matmul where the transpose operation wasn't a matrix transpose in the backends.\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def matmul(",
            "dtype_from = tf.as_dtype(x1.dtype)",
            "",
            "if transpose_a:",
            "-        x1 = tf.transpose(x1)",
            "+        x1 = tf.linalg.matrix_transpose(x1)",
            "if transpose_b:",
            "-        x2 = tf.transpose(x2)",
            "+        x2 = tf.linalg.matrix_transpose(x2)",
            "",
            "if adjoint_a:",
            "x1 = tf.linalg.adjoint(x1)"
        ]
    },
    {
        "number": 5667,
        "comments": "",
        "commit_message": "fix deconv2d None error (#5093)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def _preprocess_deconv_output_shape(x, shape, dim_ordering):",
            "",
            "if shape[0] is None:",
            "shape = (tf.shape(x)[0], ) + tuple(shape[1:])",
            "+        shape = tf.stack(list(shape))",
            "return shape"
        ]
    },
    {
        "number": 5669,
        "comments": "",
        "commit_message": "Fix longformer onnx broken export (#20292)\n\n* fix controlflow for onnx export\n\n* fix warning\n\n* fix the case padding_len = 0, explicit the recorded control flows\n\n* style\n\n* style\n\n* fix bug\n\n* fix copy\n\n* nits\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class LongformerOnnxConfig(OnnxConfig):",
            ")",
            "import torch",
            "",
            "+        # for some reason, replacing this code by inputs[\"global_attention_mask\"] = torch.randint(2, inputs[\"input_ids\"].shape, dtype=torch.int64)",
            "+        # makes the export fail randomly",
            "inputs[\"global_attention_mask\"] = torch.zeros_like(inputs[\"input_ids\"])",
            "# make every second token global",
            "inputs[\"global_attention_mask\"][:, ::2] = 1",
            "+",
            "return inputs"
        ]
    },
    {
        "number": 5671,
        "comments": "",
        "commit_message": "update to pytorch 1.3 and fix small test issues\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestHomographyWarper:",
            "batch_size, channels, height, width = batch_shape",
            "patch_src = torch.rand(batch_size, channels, height, width)",
            "# rotation of 90deg",
            "-        dst_homo_src = utils.create_eye_batch(batch_size, 3)",
            "+        dst_homo_src = torch.eye(3)",
            "dst_homo_src[..., 0, 0] = 0.0",
            "dst_homo_src[..., 0, 1] = 1.0",
            "dst_homo_src[..., 1, 0] = -1.0",
            "dst_homo_src[..., 1, 1] = 0.0",
            "+        dst_homo_src = dst_homo_src.expand(batch_size, -1, -1)",
            "",
            "# instantiate warper and warp from source to destination",
            "warper = kornia.HomographyWarper(height, width)"
        ]
    },
    {
        "number": 5672,
        "comments": "",
        "commit_message": "More unit test fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main(args):",
            "if args.distributed_port > 0 \\",
            "or args.distributed_init_method is not None:",
            "distributed_main(args)",
            "-    elif torch.cuda.device_count() > 1:",
            "+    elif args.distributed_world_size > 1:",
            "multiprocessing_main(args)",
            "else:",
            "singleprocess_main(args)"
        ]
    },
    {
        "number": 5673,
        "comments": "",
        "commit_message": "`check_fonts()` download to `CONFIG_DIR` fix (#7489)\n\nFollows https://github.com/ultralytics/yolov5/pull/7488. Correct bug where fonts were downloading to current working directory rather than global CONFIG_DIR\n",
        "label": "",
        "answer": "no",
        "change": [
            "def check_file(file, suffix=''):",
            "def check_font(font=FONT, progress=False):",
            "# Download font to CONFIG_DIR if necessary",
            "font = Path(font)",
            "-    if not font.exists() and not (CONFIG_DIR / font.name).exists():",
            "+    file = CONFIG_DIR / font.name",
            "+    if not font.exists() and not file.exists():",
            "url = \"https://ultralytics.com/assets/\" + font.name",
            "-        LOGGER.info(f'Downloading {url} to {CONFIG_DIR / font.name}...')",
            "-        torch.hub.download_url_to_file(url, str(font), progress=progress)",
            "+        LOGGER.info(f'Downloading {url} to {file}...')",
            "+        torch.hub.download_url_to_file(url, str(file), progress=progress)",
            "",
            "",
            "def check_dataset(data, autodownload=True):"
        ]
    },
    {
        "number": 5674,
        "comments": "",
        "commit_message": "use rmsprop, fix typo in asyncrunner\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class DeepQNetwork(Model):",
            "self.target_output = self.target_model.get_output()",
            "",
            "# Create training operations",
            "-        self.optimizer = tf.train.AdamOptimizer(self.alpha)",
            "+        self.optimizer = tf.train.RMSPropOptimizer(self.alpha, momentum=0.95, epsilon=0.01)",
            "self.create_training_operations()",
            "self.saver = tf.train.Saver()",
            "writer = tf.train.SummaryWriter('logs', graph=tf.get_default_graph())"
        ]
    },
    {
        "number": 5675,
        "comments": "",
        "commit_message": "fix deprecations about casting & initializers in tf1.13\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Model(ModelDesc):",
            ".FullyConnected('linear', units=10)())",
            "tf.nn.softmax(logits, name='output')",
            "",
            "-        accuracy = tf.to_float(tf.nn.in_top_k(logits, label, 1))",
            "+        accuracy = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32)",
            "add_moving_summary(tf.reduce_mean(accuracy, name='accuracy'))",
            "",
            "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)"
        ]
    },
    {
        "number": 5676,
        "comments": "",
        "commit_message": "Summaries completely changed, distributed mode incompletely changed, various fixes\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Gaussian(Distribution):",
            "definite = mean",
            "",
            "# Non-deterministic: sample action using default normal distribution",
            "-        normal = tf.random_normal(shape=tf.shape(input=mean))",
            "-        sampled = mean + stddev * normal",
            "+        normal_distribution = tf.random_normal(shape=tf.shape(input=mean))",
            "+        sampled = mean + stddev * normal_distribution",
            "",
            "return tf.where(condition=deterministic, x=definite, y=sampled)"
        ]
    },
    {
        "number": 5678,
        "comments": "",
        "commit_message": "Supports for exporting DETR to onnx with dynamic shapes and batch inference (#5168)\n\n* support exporting `DETR` to ONNX with dynamic shapes and competitive performance\n\n* support exporting `DETR` to ONNX with dynamic shapes and competitive performance\n\n* add onnx performance docs for `DETR`\n\n* fix lint error\n\n* fix\n\n* refactor the onnx export for detr\n\n* fix doc\n\n* supports batch inference for detr\n\n* inherit config for batch inference\n\n* fix type\n\n* support batch inference for ONNX\n\n* fix dynamically clip bboxes\n\n* remove batch inference config\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_sine_positional_encoding(num_feats=16, batch_size=2):",
            "",
            "module = SinePositionalEncoding(num_feats)",
            "h, w = 10, 6",
            "-    mask = torch.rand(batch_size, h, w) > 0.5",
            "+    mask = (torch.rand(batch_size, h, w) > 0.5).to(torch.int)",
            "assert not module.normalize",
            "out = module(mask)",
            "assert out.shape == (batch_size, num_feats * 2, h, w)"
        ]
    },
    {
        "number": 5680,
        "comments": "",
        "commit_message": "bugfix VirtualWorker\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def test_ellipsis_simplify():",
            "def test_pointer_tensor_simplify():",
            "\"\"\"Test the simplification of PointerTensor\"\"\"",
            "",
            "-    alice = syft.VirtualWorker(id=\"alice\")",
            "+    alice = syft.VirtualWorker(syft.torch.hook, id=\"alice\")",
            "input_tensor = PointerTensor(id=1000, location=alice, owner=alice)",
            "",
            "output = _simplify(input_tensor)"
        ]
    },
    {
        "number": 5685,
        "comments": "",
        "commit_message": "fix TFLogSTFTMagnitude.\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFLogSTFTMagnitude(tf.keras.layers.Layer):",
            "Returns:",
            "Tensor: Spectral convergence loss value.",
            "\"\"\"",
            "-        return tf.math.log(tf.abs(y_mag) + 1e-9) - tf.math.log(tf.abs(x_mag) + 1e-9)",
            "+        return tf.abs(tf.math.log(y_mag + 1e-9) - tf.math.log(x_mag + 1e-9))",
            "",
            "",
            "class TFSTFT(tf.keras.layers.Layer):"
        ]
    },
    {
        "number": 5687,
        "comments": "",
        "commit_message": "addressd CR comments, fixed merge conflict\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Uniform(Distribution):",
            "else:",
            "# x is 2-d",
            "if x.le(_a).data[0, 0] or x.ge(_b).data[0, 0]:",
            "-                return Variable(torch.Tensor([[-float(\"inf\")]]))",
            "+                return Variable(torch.Tensor([[-np.inf]]))",
            "return torch.sum(-torch.log(_b - _a))",
            "",
            "def batch_log_pdf(self, x, a=None, b=None, batch_size=1, *args, **kwargs):"
        ]
    },
    {
        "number": 5689,
        "comments": "",
        "commit_message": "Fix output types of augmentations on autocast regions (#2168)\n\n* update `.type` casts on base augmentation class\n\n- remove all casts around the bases augmentations classes\n- add cast on `transform_inputs` of `_AugmentationBase` when autocast region is enabled\n\n* add autocast test to `GeometricAugmentationBase2D`\n\n* add `is_autocast_enabled` with old torch compat\n\n- add to docs the map_location_to_cpu\n\n* add aug 2d and 3d base tests\n\n* Fix autocast for geometric 2d\n\n* update `eye_like` to cast type after construction\n\n* add test sequential container 2D\n\n* fix autocast for AugmentationSequential container\n\n* fix AugmentationSequential container\n\n- add `.type` method to Keypoints and Boxes\n\n* add test for `VideoSequential`, `PatchSequential`\n\n* Update `is_autocast_enabled`\n\n* revert typechecking statement on is_autocast_enabled\n",
        "label": "",
        "answer": "no",
        "change": [
            "def eye_like(n: int, input: torch.Tensor, shared_memory: bool = False) -> torch.",
            "if len(input.shape) < 1:",
            "raise AssertionError(input.shape)",
            "",
            "-    identity = torch.eye(n, device=input.device, dtype=input.dtype)",
            "+    identity = torch.eye(n, device=input.device).type(input.dtype)",
            "return identity[None].expand(input.shape[0], n, n) if shared_memory else identity[None].repeat(input.shape[0], 1, 1)"
        ]
    },
    {
        "number": 5690,
        "comments": "",
        "commit_message": "fix no printing\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class Entities(InMemoryDataset):",
            "edge = torch.tensor(edge_list, dtype=torch.long).t().contiguous()",
            "edge_index, edge_type = edge[:2], edge[2]",
            "",
            "-        oh = F.one_hot(",
            "-            edge_type, num_classes=2 * len(relations)).to(torch.float)",
            "+        oh = F.one_hot(edge_type,",
            "+                       num_classes=2 * len(relations)).to(torch.float)",
            "deg = scatter_add(oh, edge_index[0], dim=0, dim_size=len(nodes))",
            "index = edge_type + torch.arange(len(edge_list)) * 2 * len(relations)",
            "edge_norm = 1 / deg[edge_index[0]].view(-1)[index]"
        ]
    },
    {
        "number": 5700,
        "comments": "",
        "commit_message": "Fix unit test errors\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_forward_with_beamformer_net(",
            "# `mask_type` has no effect when `loss_type` is not \"mask...\"",
            "return",
            "if not is_torch_1_9_plus and use_builtin_complex:",
            "-        # builtin complex support is only available in PyTorch 1.8+",
            "+        # builtin complex support is only well supported in PyTorch 1.9+",
            "return",
            "",
            "ch = 3",
            "inputs = random_speech[..., :ch].float()",
            "ilens = torch.LongTensor([16, 12])",
            "-    speech_refs = [torch.randn(2, 16, ch).float() for spk in range(num_spk)]",
            "+    speech_refs = [torch.randn(2, 16, ch, dtype=torch.float) for spk in range(num_spk)]",
            "noise_ref1 = torch.randn(2, 16, ch, dtype=torch.float)",
            "dereverb_ref1 = torch.randn(2, 16, ch, dtype=torch.float)",
            "encoder = STFTEncoder("
        ]
    },
    {
        "number": 5702,
        "comments": "",
        "commit_message": "Empty assert hunt (#6056)\n\n* Fixed empty asserts\n\n* black-reformatted stragglers in templates\n\n* More code quality checks\n\n* Update src/transformers/convert_marian_to_pytorch.py\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\n\n* Update src/transformers/convert_marian_to_pytorch.py\n\nCo-authored-by: Sam Shleifer <sshleifer@gmail.com>\n\n* removed unused line as per @sshleifer\n\nCo-authored-by: Sylvain Gugger <35901082+sgugger@users.noreply.github.com>\nCo-authored-by: Sam Shleifer <sshleifer@gmail.com>\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TFFlaubertMainLayer(TFXLMMainLayer):",
            "position_ids = tf.expand_dims(tf.range(slen), axis=0)",
            "else:",
            "# assert shape_list(position_ids) == [bs, slen]  # (slen, bs)",
            "-            tf.debugging.assert_equal(shape_list(position_ids), [bs, slen])",
            "+            tf.debugging.assert_equal(",
            "+                shape_list(position_ids), [bs, slen]",
            "+            ), f\"Position id shape {shape_list(position_ids)} and input shape {[bs, slen]} mismatched\"",
            "# position_ids = position_ids.transpose(0, 1)",
            "",
            "# langs",
            "if langs is not None:",
            "# assert shape_list(langs) == [bs, slen]  # (slen, bs)",
            "-            tf.debugging.assert_equal(shape_list(langs), [bs, slen])",
            "+            tf.debugging.assert_equal(",
            "+                shape_list(langs), [bs, slen]",
            "+            ), f\"Lang shape {shape_list(langs)} and input shape {[bs, slen]} mismatched\"",
            "# langs = langs.transpose(0, 1)",
            "",
            "# Prepare head mask if needed"
        ]
    },
    {
        "number": 5704,
        "comments": "",
        "commit_message": "Fix missing to(device) for DataParallel in detection.py\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "def get_detector(trained_model, device='cpu'):",
            "net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))",
            "else:",
            "net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))",
            "-        net = torch.nn.DataParallel(net)",
            "+        net = torch.nn.DataParallel(net).to(device)",
            "cudnn.benchmark = False",
            "",
            "net.eval()"
        ]
    },
    {
        "number": 5706,
        "comments": "",
        "commit_message": "Improve reproduceability 2/3 (#1906)\n\n* [Repro] Correct reproducability\n\n* up\n\n* up\n\n* uP\n\n* up\n\n* need better image\n\n* allow conversion from no state dict checkpoints\n\n* up\n\n* up\n\n* up\n\n* up\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* check tensors\n\n* next try\n\n* up\n\n* up\n\n* better name\n\n* up\n\n* up\n\n* Apply suggestions from code review\n\n* correct more\n\n* up\n\n* replace all torch randn\n\n* fix\n\n* correct\n\n* correct\n\n* finish\n\n* fix more\n\n* up\n",
        "label": "",
        "answer": "no",
        "change": [
            "class KarrasVeScheduler(SchedulerMixin, ConfigMixin):",
            "gamma = 0",
            "",
            "# sample eps ~ N(0, S_noise^2 * I)",
            "-        eps = self.config.s_noise * torch.randn(sample.shape, generator=generator).to(sample.device)",
            "+        eps = self.config.s_noise * randn_tensor(sample.shape, generator=generator).to(sample.device)",
            "sigma_hat = sigma + gamma * sigma",
            "sample_hat = sample + ((sigma_hat**2 - sigma**2) ** 0.5 * eps)"
        ]
    },
    {
        "number": 5710,
        "comments": "",
        "commit_message": "fix get_attr and new tensorflow saver\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class tensorflow_extractor(base_extractor):",
            "",
            "init = tf.global_variables_initializer()",
            "with tf.Session() as sess:",
            "-            # tf.train.export_meta_graph(\"kit.meta\", as_text=True)",
            "-            # writer = tf.summary.FileWriter('./graphs', sess.graph)",
            "-            # writer.close()",
            "sess.run(init)",
            "saver = tf.train.Saver()",
            "saver.restore(sess, path + cls.architecture_map[architecture]['filename'])"
        ]
    },
    {
        "number": 5718,
        "comments": "",
        "commit_message": "More TF int dtype fixes (#20384)\n\n* Add a test to ensure int dummy inputs are int64\n\n* Move the test into the existing int64 test and update a lot of existing dummies\n\n* Fix remaining dummies\n\n* Fix remaining dummies\n\n* Test for int64 serving sigs as well\n\n* Update core tests to use tf.int64\n\n* Add better messages to the assertions\n\n* Update all serving sigs to int64\n\n* More sneaky hiding tf.int32s\n\n* Add an optional int32 signature in save_pretrained\n\n* make fixup\n\n* Add Amy's suggestions\n\n* Switch all serving sigs back to tf.int32\n\n* Switch all dummies to tf.int32\n\n* Adjust tests to check for tf.int32 instead of tf.int64\n\n* Fix base dummy_inputs dtype\n\n* Start casting to tf.int32 in input_processing\n\n* Change dtype for unpack_inputs test\n\n* Add proper tf.int32 test\n\n* Make the alternate serving signature int64\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFGPTJPreTrainedModel(TFPreTrainedModel):",
            "Returns:",
            "`Dict[str, tf.Tensor]`: The dummy inputs.",
            "\"\"\"",
            "-        dummy = {\"input_ids\": tf.constant(DUMMY_INPUTS)}",
            "+        dummy = {\"input_ids\": tf.constant(DUMMY_INPUTS, dtype=tf.int32)}",
            "return dummy",
            "",
            "@tf.function(",
            "input_signature=[",
            "{",
            "-                \"input_ids\": tf.TensorSpec((None, None), tf.int64, name=\"input_ids\"),",
            "-                \"attention_mask\": tf.TensorSpec((None, None), tf.int64, name=\"attention_mask\"),",
            "+                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),",
            "+                \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),",
            "}",
            "]",
            ")"
        ]
    },
    {
        "number": 5720,
        "comments": "",
        "commit_message": "fix layer norm test\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_layer_norm(affine):",
            "torch.jit.script(norm)",
            "out1 = norm(x)",
            "assert out1.size() == (100, 16)",
            "-    assert torch.allclose(norm(x, batch), out1)",
            "+    assert torch.allclose(norm(x, batch), out1, atol=1e-6)",
            "",
            "out2 = norm(torch.cat([x, x], dim=0), torch.cat([batch, batch + 1], dim=0))",
            "-    assert torch.allclose(out1, out2[:100])",
            "-    assert torch.allclose(out1, out2[100:])",
            "+    assert torch.allclose(out1, out2[:100], atol=1e-6)",
            "+    assert torch.allclose(out1, out2[100:], atol=1e-6)"
        ]
    },
    {
        "number": 5722,
        "comments": "",
        "commit_message": "fix legacy creation (#16282)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def main_train(dir_path, max_epochs: int = 20):",
            "seed_everything(42)",
            "stopping = EarlyStopping(monitor=\"val_acc\", mode=\"max\", min_delta=0.005)",
            "trainer = pl.Trainer(",
            "+        accelerator=\"auto\",",
            "default_root_dir=dir_path,",
            "-        devices=int(torch.cuda.is_available()),",
            "precision=(16 if torch.cuda.is_available() else 32),",
            "callbacks=[stopping],",
            "min_epochs=3,"
        ]
    },
    {
        "number": 5723,
        "comments": "",
        "commit_message": "[Community] Fix merger (#2006)\n\n* [Community] Fix merger\n\n* finish\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class CheckpointMergerPipeline(DiffusionPipeline):",
            "theta_0 = theta_0()",
            "",
            "update_theta_0 = getattr(module, \"load_state_dict\")",
            "-                    theta_1 = torch.load(checkpoint_path_1)",
            "+                    theta_1 = torch.load(checkpoint_path_1, map_location=\"cpu\")",
            "",
            "-                    theta_2 = torch.load(checkpoint_path_2) if checkpoint_path_2 else None",
            "+                    theta_2 = torch.load(checkpoint_path_2, map_location=\"cpu\") if checkpoint_path_2 else None",
            "",
            "if not theta_0.keys() == theta_1.keys():",
            "print(\"SKIPPING ATTR \", attr, \" DUE TO MISMATCH\")"
        ]
    },
    {
        "number": 5730,
        "comments": "",
        "commit_message": "fix tests\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "TF2_WEIGHTS_NAME = 'tf_model.h5'",
            "TF_WEIGHTS_NAME = 'model.ckpt'",
            "CONFIG_NAME = \"config.json\"",
            "",
            "-logger = logging.getLogger(__name__)  # pylint: disable=invalid-name",
            "-",
            "def is_torch_available():",
            "return _torch_available"
        ]
    },
    {
        "number": 5734,
        "comments": "",
        "commit_message": "Fix warning about deprecated `volatile` kwarg for Variables\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LinearizedConvolution(ConvTBC):",
            "self.input_buffer[:, :-1, :] = self.input_buffer[:, 1:, :].clone()",
            "# append next input",
            "self.input_buffer[:, -1, :] = input[:, -1, :]",
            "-            input = torch.autograd.Variable(self.input_buffer, volatile=True)",
            "+            input = utils.volatile_variable(self.input_buffer)",
            "output = F.linear(input.view(bsz, -1), weight, self.bias)",
            "return output.view(bsz, 1, -1)"
        ]
    },
    {
        "number": 5735,
        "comments": "",
        "commit_message": "GH-2534: fix language model training\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class LanguageModelTrainer:",
            "# not really sure what this does",
            "ntokens = len(self.corpus.dictionary)",
            "",
            "-                    total_loss = torch.zeros(1)",
            "+                    total_loss = torch.zeros(1, device=flair.device)",
            "start_time = time.time()",
            "",
            "for batch, i in enumerate("
        ]
    },
    {
        "number": 5736,
        "comments": "",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "from kornia.augmentation.random_generator import DistributionWithMapper",
            "",
            "",
            "class TestDistMapper:",
            "-",
            "-    def test_mapper(self,):",
            "+    def test_mapper(self):",
            "_ = torch.manual_seed(0)",
            "-        dist = DistributionWithMapper(Normal(0., 1.,), map_fn=nn.Sigmoid())",
            "+        dist = DistributionWithMapper(Normal(0.0, 1.0), map_fn=nn.Sigmoid())",
            "out = dist.rsample((8,))",
            "-        exp = torch.tensor([",
            "-            0.8236, 0.4272, 0.1017, 0.6384, 0.2527, 0.1980, 0.5995, 0.6980",
            "-        ])",
            "+        exp = torch.tensor([0.8236, 0.4272, 0.1017, 0.6384, 0.2527, 0.1980, 0.5995, 0.6980])",
            "assert_allclose(out, exp, rtol=1e-4, atol=1e-4)"
        ]
    },
    {
        "number": 5738,
        "comments": "",
        "commit_message": "[Speech Examples] Add pytorch speech pretraining (#13877)\n\n* adapt wav2vec2\n\n* add example\n\n* add files\n\n* adapt\n\n* remove bogus file\n\n* Apply suggestions from code review\n\n* adapt files more\n\n* upload changes\n\n* del old files\n\n* up\n\n* up\n\n* up\n\n* up\n\n* up\n\n* correct gradient checkpoitning\n\n* add readme\n\n* finish\n\n* finish\n\n* up\n\n* more fixes\n\n* up\n\n* up\n\n* add demo run to readme\n\n* up\n",
        "label": "",
        "answer": "no",
        "change": [
            "class HubertUtilsTest(unittest.TestCase):",
            "mask_prob = 0.5",
            "mask_length = 4",
            "",
            "-        mask = _compute_mask_indices((batch_size, sequence_length), mask_prob, mask_length, torch_device)",
            "+        mask = _compute_mask_indices((batch_size, sequence_length), mask_prob, mask_length)",
            "+        mask = torch.from_numpy(mask).to(torch_device)",
            "",
            "# because of overlap mask don't have to add up exactly to `mask_prob * sequence_length`, but have to be smaller or equal",
            "for batch_sum in mask.sum(axis=-1):"
        ]
    },
    {
        "number": 5744,
        "comments": "",
        "commit_message": "always reuse when not training. otherwise reuse can be set back to False in TF<1.1 (fix #277)\n\n",
        "label": "",
        "answer": "yes",
        "change": [
            "class TowerContext(object):",
            "self._ctxs.append(tf.variable_scope(self._name))",
            "else:",
            "# use existing variable scope",
            "+                reuse = self.index > 0 or (not self.is_training)",
            "self._ctxs.append(tf.variable_scope(",
            "-                    tf.get_variable_scope(), reuse=self.index > 0))",
            "+                    tf.get_variable_scope(), reuse=reuse))",
            "self._ctxs.append(tf.name_scope(self._name))",
            "self._ctxs.append(tf.device(self._device))",
            "for c in self._ctxs:"
        ]
    },
    {
        "number": 5745,
        "comments": "",
        "commit_message": "fix serving export (fix #1449)\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ModelExporter(object):",
            "\"\"\"",
            "if tags is None:",
            "tags = (tf.saved_model.SERVING if get_tf_version_tuple() >= (1, 12)",
            "-                    else tf.saved_model.tag_constants.SERVING)",
            "+                    else tf.saved_model.tag_constants.SERVING, )",
            "",
            "self.graph = self.config._maybe_create_graph()",
            "with self.graph.as_default():"
        ]
    },
    {
        "number": 5749,
        "comments": "",
        "commit_message": "enable black in the precommit (#1777)\n\n* enable black in the precommit\n\n* [pre-commit.ci] auto fixes from pre-commit.com hooks\n\nfor more information, see https://pre-commit.ci\n\n* add some fixes\n\n* update libface detection url\n\n* added url from kornia checkpoint\n\nCo-authored-by: pre-commit-ci[bot] <66853113+pre-commit-ci[bot]@users.noreply.github.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFeat(nn.Module):",
            "# use torch.hub to load pretrained model",
            "if pretrained:",
            "storage_fcn: Callable = lambda storage, loc: storage",
            "-            pretrained_dict = torch.hub.load_state_dict_from_url(",
            "-                urls['liberty'], map_location=storage_fcn",
            "-            )",
            "+            pretrained_dict = torch.hub.load_state_dict_from_url(urls['liberty'], map_location=storage_fcn)",
            "self.load_state_dict(pretrained_dict, strict=True)",
            "self.eval()"
        ]
    },
    {
        "number": 5755,
        "comments": "",
        "commit_message": "Fix mtf broadcast error when lowering\n\nWe don't need to add the temporary dimensions to tensors before broadcasting them\n\nThis fixes:\nValueError: No new dimensions allowed in output input_shapes = [[Dimension(name='batch', size=256), Dimension(name='sequence', size=128)]] output_shape= [Dimension(name='dummy_batch', size=1), Dimension(name='sequence', size=128)]\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def expand_tile(value, newdim):",
            "print(value)",
            "print('############')",
            "",
            "-    return mtf.broadcast(mtf_expand_dims(value, 'dummy_batch', 0),",
            "+    return mtf.broadcast(mtf_expand_dims(value, newdim, 0),",
            "[newdim] + value.shape.dims)  # shape.dims gets us a list which we need in order to concat"
        ]
    },
    {
        "number": 5757,
        "comments": "",
        "commit_message": "Fixes to enable dev branch to move to pytorch-1.2 (#2001)\n\n* Fixes to enable dev branch to move to pytorch-1.2\n\n* change name for published cpu wheels\n\n* fix failing unit tests\n\n* change docs conf\n\n* change tracking example\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_unconstrained_to_corr_cholesky_transform(y_shape):",
            "log_det = transform.log_abs_det_jacobian(y, x)",
            "assert log_det.shape == y_shape[:-1]",
            "if len(y_shape) == 1:",
            "-        triu_index = x.new_ones(x.shape).triu(diagonal=1).to(torch.uint8)",
            "+        triu_index = x.new_ones(x.shape).triu(diagonal=1).to(torch.bool)",
            "x_tril_vector = x.t()[triu_index]",
            "assert_tensors_equal(_autograd_log_det(x_tril_vector, y), log_det, prec=1e-4)"
        ]
    },
    {
        "number": 5758,
        "comments": "",
        "commit_message": "Fix variable name typo in error message for launch.py\n\nCOMMANDINE_ARGS -> COMMANDLINE_ARGS\n",
        "label": "",
        "answer": "no",
        "change": [
            "if not is_installed(\"torch\") or not is_installed(\"torchvision\"):",
            "run(f'\"{python}\" -m {torch_command}', \"Installing torch and torchvision\", \"Couldn't install torch\")",
            "",
            "if not skip_torch_cuda_test:",
            "-    run_python(\"import torch; assert torch.cuda.is_available(), 'Torch is not able to use GPU; add --skip-torch-cuda-test to COMMANDINE_ARGS variable to disable this check'\")",
            "+    run_python(\"import torch; assert torch.cuda.is_available(), 'Torch is not able to use GPU; add --skip-torch-cuda-test to COMMANDLINE_ARGS variable to disable this check'\")",
            "",
            "if not is_installed(\"k_diffusion.sampling\"):",
            "run_pip(f\"install {k_diffusion_package}\", \"k-diffusion\")"
        ]
    },
    {
        "number": 5761,
        "comments": "",
        "commit_message": ":elephant: Remove warnings during testing (#1401)\n\n* fix warnings during testing\n\n* Update test/geometry/transform/test_imgwarp.py\n\n* remove warning in clahe due to //\n\n* Apply suggestions from code review\n\nCo-authored-by: Luis Ferraz <luisferrazc@gmail.com>\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestDivergenceLoss:",
            "target = torch.randn((2, 4, 10, 16), dtype=dtype, device=device)",
            "args = (input, target)",
            "op = kornia.losses.js_div_loss_2d",
            "-        op_jit = torch.jit.script(op, args)",
            "+        op_jit = torch.jit.script(op)",
            "assert_close(op(*args), op_jit(*args), rtol=0, atol=1e-5)"
        ]
    },
    {
        "number": 5762,
        "comments": "",
        "commit_message": "fixed code formatting\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TrainingTest(test_combinations.TestCase):",
            "@test_combinations.run_all_keras_modes(always_skip_v1=True)",
            "def test_distribution_reduction_method_sum(self):",
            "",
            "-        strategy = tf.distribute.MirroredStrategy([\"/cpu:1\", \"/cpu:2\", \"/cpu:3\", \"/cpu:4\"])",
            "+        strategy = tf.distribute.MirroredStrategy(",
            "+            [\"/cpu:1\", \"/cpu:2\", \"/cpu:3\", \"/cpu:4\"]",
            "+        )",
            "BATCH_SIZE = 10",
            "",
            "class MyModel(training_module.Model):"
        ]
    },
    {
        "number": 5763,
        "comments": "",
        "commit_message": "Fixed nightly errors (#886)\n\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TestCenterCropGen3D(RandomGeneratorBaseTests):",
            "[0, 0, 119],",
            "[99, 0, 119],",
            "[99, 149, 119],",
            "-                 [0, 149, 119]]], device=device, dtype=torch.long),",
            "+                 [0, 149, 119]]], device=device, dtype=torch.long).repeat(2, 1, 1),",
            ")",
            "assert res.keys() == expected.keys()",
            "assert_allclose(res['src'].to(device=device), expected['src'], atol=1e-4, rtol=1e-4)"
        ]
    },
    {
        "number": 5766,
        "comments": "",
        "commit_message": "Fixes input layer naming bugs via explicit naming propagation, including unit tests for TypeSpec naming, saving, and exporting.\n\nPiperOrigin-RevId: 521005984\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "class ReloadedLayer(base_layer.Layer):",
            "",
            "",
            "def _make_tensor_spec(x):",
            "-    return tf.TensorSpec(x.shape, dtype=x.dtype)",
            "+    return tf.TensorSpec(x.shape, dtype=x.dtype, name=x.name)",
            "",
            "",
            "def _print_signature(fn, name):"
        ]
    },
    {
        "number": 5767,
        "comments": "",
        "commit_message": "fixing some compatibility with PT 1.8 (#5864)\n\n* change default\n\n* .\n\n* p\n\n* 0.21.2\n\n* .\n\n* fix\n\n* .\n",
        "label": "",
        "answer": "no",
        "change": [
            "def test_metrics_mod(second_operand, expected_result):",
            "final_mod = first_metric % second_operand",
            "",
            "assert isinstance(final_mod, CompositionalMetric)",
            "-",
            "-    assert torch.allclose(expected_result, final_mod.compute())",
            "+    # prevent Runtime error for PT 1.8 - Long did not match Float",
            "+    assert torch.allclose(expected_result.to(float), final_mod.compute().to(float))",
            "",
            "",
            "@pytest.mark.parametrize("
        ]
    },
    {
        "number": 5769,
        "comments": "",
        "commit_message": "Modify librispeech run.sh for improved LM training and fix some LM-related bugs\n\n",
        "label": "",
        "answer": "no",
        "change": [
            "def recog(args):",
            "if args.rnnlm:",
            "rnnlm_args = get_model_conf(args.rnnlm, args.rnnlm_conf)",
            "rnnlm = lm_pytorch.ClassifierWithState(",
            "-            lm_pytorch.RNNLM(len(train_args.char_list), rnnlm_args.layer, rnnlm_args.unit))",
            "+            lm_pytorch.RNNLM(",
            "+                len(train_args.char_list), rnnlm_args.layer, rnnlm_args.unit))",
            "torch_load(args.rnnlm, rnnlm)",
            "rnnlm.eval()",
            "else:",
            "rnnlm = None",
            "",
            "if args.word_rnnlm:",
            "-        rnnlm_args = get_model_conf(args.word_rnnlm, args.rnnlm_conf)",
            "+        rnnlm_args = get_model_conf(args.word_rnnlm, args.word_rnnlm_conf)",
            "word_dict = rnnlm_args.char_list_dict",
            "char_dict = {x: i for i, x in enumerate(train_args.char_list)}",
            "word_rnnlm = lm_pytorch.ClassifierWithState(lm_pytorch.RNNLM("
        ]
    },
    {
        "number": 5771,
        "comments": "",
        "commit_message": "Update modeling_tf_deberta.py (#13654)\n\nFixed expand_dims axis\n",
        "label": "",
        "answer": "no",
        "change": [
            "class TFDebertaDisentangledSelfAttention(tf.keras.layers.Layer):",
            "if len(shape_list_pos) == 2:",
            "relative_pos = tf.expand_dims(tf.expand_dims(relative_pos, 0), 0)",
            "elif len(shape_list_pos) == 3:",
            "-            relative_pos = tf.expand_dims(relative_pos, 0)",
            "+            relative_pos = tf.expand_dims(relative_pos, 1)",
            "# bxhxqxk",
            "elif len(shape_list_pos) != 4:",
            "raise ValueError(f\"Relative position ids must be of dim 2 or 3 or 4. {len(shape_list_pos)}\")"
        ]
    }
]